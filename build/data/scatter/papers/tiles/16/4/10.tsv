id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
9a3866dfebbe67e6facbe2784d3635367dbe0769	a theoretical study: the impact of cloning failed test cases on the effectiveness of fault localization	fault localization;program diagnostics;software reliability information analysis program debugging program diagnostics program testing;class imbalance software debugging fault localization;class imbalance;program testing;cloning failed test cases dynamic program information analysis software debugging statistical fault localization techniques fault localization effectiveness;software debugging;program debugging;cloning software software algorithms software testing educational institutions algorithm design and analysis software engineering;software reliability;information analysis	Statistical fault localization techniques analyze the dynamic program information provided by executing a large number of test cases to predict fault positions in faulty programs. Related studies show that the extent of imbalance between the number of passed test cases and that of failed test cases may reduce the effectiveness of such techniques, while failed test cases can frequently be less than passed test cases in practice. In this study, we propose a strategy to generate balanced test suite by cloning the failed test cases for suitable number of times to catch up with the number of passed test cases. We further give an analysis to show that by carrying out the cloning the effectiveness of two representative fault localization techniques can be improved under certain conditions and impaired at no time.	executable;failure;jaccard index;test case;test suite	Yichao Gao;Zhenyu Zhang;Long Zhang;Cheng Gong;Zheng Zheng	2013	2013 13th International Conference on Quality Software	10.1109/QSIC.2013.23	reliability engineering;real-time computing;fault coverage;computer science;engineering;software engineering;software construction;test suite;software testing;data analysis;test case;test management approach;software quality;software fault tolerance;computer engineering;test harness	SE	-60.789662827034135	35.451079996482754	54842
03ca97e551fc54a477e6ca4581fb2d69a69af032	mutation analysis with coverage discounting	sensitivity;registers;system on chip;benchmark testing	Mutation testing is an established technique for evaluating validation thoroughness, but its adoption has been limited by the manual effort required to analyze the results. This paper describes the use of coverage discounting for mutation analysis, where undetected mutants are explained in terms of functional coverpoints, simplifying their analysis and saving effort. Two benchmarks are shown to compare this improved flow against regular mutation analysis. We also propose a confidence metric and simulation ordering algorithm optimized for coverage discounting, potentially reducing overall simulation time.	algorithm;mutation testing;simulation	Peter Lisherness;Nicole Lesperance;Kwang-Ting Cheng	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		simulation;computer science;data mining;statistics	EDA	-61.58137421791965	36.17406087372027	54900
3d121b1f8390adb8b1b8631fa7a58b5f21be8a73	semantic metrics for software testability	developpement logiciel;test programa;detection panne;failure detection;ingenieria logiciel;specification programme;test;software engineering;ensayo;essai;desarrollo logicial;software development;genie logiciel;fiabilite logiciel;fiabilidad logicial;test programme;deteccion falla;program specification;program test;software reliability;especificacion programa	Abstract   Software faults that infrequently affect output cause problems in most software and are dangerous in safety-critical systems. When a software fault causes frequent software failures, testing is likely to reveal the fault before the software is released; when the fault “hides” from testing, the hidden fault can cause disaster after the software is installed. During the design of safety-critical software, we can isolate certain subfunctions of the software that tend to hide faults. A simple metric, derivable from semantic information found in software specifications, indicates software subfunctions that tend to hide faults. The metric is the domain/range ratio (DRR): the ratio of the cardinality of the possible inputs to the cardinality of the possible outputs. By isolating modules that implement a high DRR function during design, we can produce programs that are less likely to hide faults during testing. The DRR is available early in the software lifecycle; when code has been produced, the potential for hidden faults can be further explored using empirical methods. Using the DRR during design and empirical methods during execution, we can better plan and implement strategies for enhancing testability. For certain specifications, testability considerations can help produce modules that require less additional testing when assumptions change about the distribution of inputs. Such modules are good candidates for software reuse.	software testability	Jeffrey M. Voas;Keith W. Miller	1993	Journal of Systems and Software	10.1016/0164-1212(93)90064-5	reliability engineering;embedded system;regression testing;real-time computing;software sizing;computer science;software reliability testing;software development;software design description;operating system;software engineering;software testing;software maintenance;software regression;software quality;software fault tolerance;software metric;software quality analyst;avionics software	SE	-62.40616126529578	34.82906088403568	54930
2398dc4913fc899dc3407e82bc9df8dc8f0aff61	open-ended exploration of the program repair search space with mined templates: the next 8935 patches for defects4j		In this paper our goal is to perform an open-ended exploration of the program repair search space. Our idea is to collect the largest number of test-suite adequate patches, independently of whether they are fully correct or overfitting. For that, we design Cardumen, a repair approach based mined templates that has an ultra-large search space. We evaluate the capacity of Cardumen to discover test-suite adequate patches (aka plausible patches) over the 356 real bugs from Defects4J by Just et al. (2014). Cardumen finds 8935 patches over 77 bugs of Defects4J. This is the largest number of automatically synthesized patches ever reported, all patches being available in an openscience repository. Moreover, Cardumen identifies 8 unique patches, that are patches for Defects4J bugs that were never repaired in the whole history of program repair.	mined;nonlinear gameplay;overfitting;software bug;test suite	Matias Martinez;Martin Monperrus	2017	CoRR		overfitting;template;aka;pattern recognition;computer science;artificial intelligence	SE	-59.71136838168362	37.969331604486115	55032
59db5c2dd06e45ba7da994f0369bd55a6bf9adfc	inferring design patterns using the rep graph	design pattern	Periodic refactoring of a large source code often becomes a necessity especially for long-lived projects. In order to increase maintainability and extensibility of such projects, design pattern based refactoring can be seen as an emerging alternative. Manual inspection of source code to find candidate spots where patterns can be introduced is time consuming. Therefore automated tools can help in identifying candidate spots where patterns can be introduced. The level of source code abstraction plays an important role for building such tools. We propose a new abstraction for object oriented source code that is named as “Refactoring Pattern (ReP) Graph” to realize an effective design pattern based refactoring tool. The ReP graph abstracts the source code information thereby making the process of design pattern inference easier. The proposed tool identifies candidate spots in a given source code to introduce design patterns.	code refactoring;extensibility;software design pattern	Tushar Sharma;D. Janaki Ram	2010	Journal of Object Technology	10.5381/jot.2010.9.5.a5	computer science;data mining;design pattern;programming language;engineering drawing;code refactoring	SE	-56.41475696821814	35.09397333212175	55200
1476f744be511ac89482ecf238248113d97d5ca4	a demand-driven analyzer for data flow testing at the integration level	data-flow analysis;integration level;demand-driven analysis;traditional exhaustive analyzer;data-flow testing;incremental analyzer;exhaustive analysis;demand-driven analyzer;integration testing;static analysis;unit testing;application software;computer science;data flow analysis;overhead;software testing;cost benefit analysis;performance;data flow;software engineering;data analysis	Data-flow testing relies on static analysis for computing the definition-use pairs that serve as the test case requirements for a program. When testing large programs, the individual procedures are first tested in isolation during unit testing. Integration testing is performed to specifically test the procedure interfaces. The procedures in a program are integrated and tested in several steps. Since each integration step requires data-flow analysis to determine the new test requirements, the accumulated cost of repeatedly analyzing a program can contribute considerably to the overhead of testing. Data-flow analysis is typically computed using an exhaustive approach or by using incremental data-flow updates. This paper presents a new and more efficient approach to data-flow integration testing that is based on demand-driven analysis. We developed and implemented a demand-driven analyzer and experimentally compared its performance during integration testing with the performance of (i) a traditional exhaustive analyzer, and (ii) an incremental analyzer. Our experiments show that demand-driven analysis is faster than exhaustive analysis by up to a factor of 25. The demand-driven analyzer also outperforms the incremental analyzer in 80% of the test programs by up to a factor of 5.	data-flow analysis;dataflow architecture;experiment;incremental compiler;integration testing;overhead (computing);requirement;static program analysis;test case;unit testing	Evelyn Duesterwald;Rajiv Gupta;Mary Lou Soffa	1996			non-regression testing;keyword-driven testing;data flow diagram;application software;real-time computing;orthogonal array testing;white-box testing;performance;integration testing;computer science;cost–benefit analysis;software engineering;data-flow analysis;functional testing;database;software testing;unit testing;data analysis;programming language;overhead;static analysis;test management approach	SE	-60.01546923541403	35.811259690132175	55202
8dfca4cdc763ad79f305bf90722d8fe1a35adb9b	scalability-first pointer analysis with self-tuning context-sensitivity		Context-sensitivity is important in pointer analysis to ensure high precision, but existing techniques suffer from unpredictable scalability. Many variants of context-sensitivity exist, and it is difficult to choose one that leads to reasonable analysis time and obtains high precision, without running the analysis multiple times.   We present the Scaler framework that addresses this problem. Scaler efficiently estimates the amount of points-to information that would be needed to analyze each method with different variants of context-sensitivity. It then selects an appropriate variant for each method so that the total amount of points-to information is bounded, while utilizing the available space to maximize precision.   Our experimental results demonstrate that Scaler achieves predictable scalability for all the evaluated programs (e.g., speedups can reach 10x for 2-object-sensitivity), while providing a precision that matches or even exceeds that of the best alternative techniques.	context-sensitive grammar;pointer (computer programming);pointer analysis;scalability;self-tuning;software engineering;static program analysis	Yue Li;Tian Tan;Anders Møller;Yannis Smaragdakis	2018		10.1145/3236024.3236041	real-time computing;computer science;computer engineering;scalability;pointer analysis;bounded function;static analysis;java;self-tuning	SE	-58.82466640659114	38.64051850360699	55604
3814bd1a4dfc46fcb54a2eb23b2394fd968f663b	mining coding patterns to detect crosscutting concerns in java programs	databases;copy and pasted code;software;filtering;fung pattern mining tool;crosscutting concern detection;prefixspan algorithm;java programming;software maintenance;program interpreters;frequent method call sequence;data mining;fung pattern mining tool open source java program sequential coding pattern mining crosscutting concern detection frequent method call sequence copy and pasted code duplicated code fragment software maintenance java source code translation sequence database prefixspan algorithm;cloning;pattern mining;public domain software;java source code translation;graphical user interfaces;sequential coding pattern mining;aspect mining;databases encoding java software cloning filtering graphical user interfaces;crosscutting concerns;software tools;source code;duplicated code fragment;sequential pattern mining;java aspect mining sequential pattern mining software maintenance static analysis;static analysis;sequence database;encoding;open source java program;software tools data mining java program interpreters public domain software software maintenance;java;open source	A coding pattern is a frequent sequence of method calls and control statements to implement a particular behavior. Coding patterns include copy-and-pasted code, crosscutting concerns and implementation idioms. Duplicated code fragments and crosscutting concerns that spread across modules are problematic in software maintenance. In this paper, we propose a sequential pattern mining approach to capture coding patterns in Java programs. We have defined a set of rules to translate Java source code into a sequence database for pattern mining, and applied PrefixSpan algorithm to the sequence database. As a case study, we have applied our tool to six open-source programs and manually investigated the resultant patterns. We report coding patterns that are candidates of aspects and several logging patterns that are well-known crosscutting concerns but hard to modularize.		Takashi Ishio;Hironori Date;Tatsuya Miyake;Katsuro Inoue	2008	2008 15th Working Conference on Reverse Engineering	10.1109/WCRE.2008.28	filter;sequential pattern mining;real-time computing;computer science;software engineering;sequence database;cloning;graphical user interface;database;programming language;software maintenance;java;public domain software;static analysis;encoding;source code	SE	-56.838413150010346	36.20439707240015	55631
bef6f21fccdcac24ebcd6b4287801ff1c9734966	automatic inference of code transforms for patch generation		We present a new system, Genesis, that processes human patches to automatically infer code transforms for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the complete Genesis patch generation system working with real-world patches and defects collected from 372 Java projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from previous successful patches.	automatic bug fixing;belief revision;column (database);genesis;init;java;patch (computing);search algorithm;software bug;test case	Fan Long;Peter Amidon;Martin C. Rinard	2017		10.1145/3106237.3106253	theoretical computer science;computer science;inference;java	SE	-59.2264051584258	37.765974241651726	55977
8f1ea8afe9fe88e40af9a5545acb062d67e017e0	scalable isolation of failure-inducing changes via version comparison	program diagnostics;failure inducing changes;automated debugging;public domain software configuration management parallel processing program debugging program diagnostics;software tests;large scale projects;software tests automated debugging version comparison failure inducing changes thin slicing large scale projects;public domain software;version comparison;instruments software debugging libraries runtime testing java;program debugging;configuration management;parallel processing;thin slicing;software test automated debugging methods version comparison failure inducing code changes code commits static analysis dynamic analysis apache hadoop open source project	Despite of indisputable progress, automated debugging methods still face difficulties in terms of scalability and runtime efficiency. To reach large-scale projects, we propose an approach which reports small sets of suspicious code changes. Its essential strength is that size of these reports is proportional to the amount of changes between code commits, and not the total project size. In our method we combine version comparison and information on failed tests with static and dynamic analysis. We evaluate our method on real bugs from Apache Hadoop, an open source project with over 2 million LOC1. In 2 out of 4 cases, the set of suspects produced by our approach contains exactly the location of the defective code (and no false positives). Another defect could be pinpointed by small approach extensions. Moreover, the time overhead of our approach is moderate, namely 3-4 times the duration of a failed software test.	apache hadoop;code coverage;continuous integration;debugging;dynamic program analysis;failure;jenkins;left 4 dead 2;open-source software;overhead (computing);programmer;run time (program lifecycle phase);scalability;software bug;software testing;static program analysis	Mohammadreza Ghanavati;Artur Andrzejak;Zhen Dong	2013	2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)	10.1109/ISSREW.2013.6688895	reliability engineering;parallel processing;program slicing;real-time computing;thin-slicing;computer science;operating system;software engineering;software construction;database;configuration management;algorithmic program debugging;programming language;public domain software;static program analysis	SE	-61.18662808735924	36.92414866080792	56180
3165082fe8df605e90e7d888265fe47b2a232633	towards a one-stop-shop for analysis, transformation and visualization of software	interactive visualization framework;effective language;meta-programming task;advanced layout;visualization framework;software visualization;current framework;domain-specific language;mming language rascal;seamlessly integrated visualization facility	Over the last two years we have been developing the metaprogramming language RASCAL that aims at providing a concise and effective language for performing meta-programming tasks such as the analysis and transformation of existing source code and models, and the implementation of domainspecific languages. However, meta-programming tasks also require seamlessly integrated visualization facilities. We are therefore now aiming at a ”One-Stop-Shop” for analysis, transformation and visualization. In this paper we give a status report on this ongoing effort and sketch the requirements for an interactive visualization framework and describe the solutions we came up with. In brief, we propose a coordinate-free, compositional, visualization framework, with fully automatic placement, scaling and alignment. It also provides user interaction. The current framework can handle various kinds of charts, trees, and graphs and can be easily extended to more advanced layouts. This work can be seen as a study in domain engineering that will eventually enable us to create a domain-specific language for software visualization. We conclude with examples that emphasize the integration of analysis and visualization.	abstract data type;chart;digital subscriber line;domain analysis;domain engineering;domain-specific language;graph (discrete mathematics);high- and low-level;image scaling;interactive visualization;metaprogramming;parsing;requirement;software visualization	Paul Klint;Bert Lisser;Atze van der Ploeg	2011		10.1007/978-3-642-28830-2_1	software visualization;visual analytics;information visualization;computer science;engineering;theoretical computer science;software engineering;data mining;programming language	PL	-52.950982252354	35.01383370971031	56206
7942a912a519e67736f4cc345b70dae241ceec28	a large scale empirical study on user-centric performance analysis	software performance metrics;software performance evaluation program testing software metrics;scenario centric performance analysis;software metrics;software;histograms;empirical study;dvd;measurement;software performance evaluation;user perception;metric;system operator;large scale system;user centric performance analysis;time factors measurement performance analysis dvd servers histograms software;software performance measurement;software performance;large scale;servers;load test software performance user metric;time factors;program testing;user centric software performance analysis;telecommunication system;software development;performance analysis;system tester;time factor;open source performance benchmark;usage log;user;use case;scenario centric performance analysis user centric performance analysis software performance measurement software development large scale system usage log user perception software performance metrics system tester system operator telecommunication system open source performance benchmark user centric software performance analysis;load test;open source	Measuring the software performance under load is an important task in both test and production of a software development. In large scale systems, a large amount of metrics and usage logs are analyzed to measure the performance of the software. Most of these metrics are analyzed by aggregating across all users to get general results for the scenario, i.e., how individual users have perceived the performance is typically not considered in software performance research and practice. To analyze a software's performance, user's perception of software performance metrics should be considered along with the scenario-centric perspective of system tester or operator. In our empirical study, we analyzed the impact of performance on individual users to see if performance analysis results based on the user's perception is really different from the scenario-centric (aggregated) one. Case studies on common use case scenarios in two commercial large telecommunication systems and one open source performance benchmark show scenarios where user-centric software performance analysis was able to identify performance issues that would be invisible in a scenario-centric analysis. We find that the user-centric approach does not replace the existing scenario-centric performance analysis approaches, but complements them by identifying more performance issues.	benchmark (computing);open-source software;profiling (computer programming);software development;software performance testing;software reliability testing;software system;usability;vii	Shahed Zaman;Bram Adams;Ahmed E. Hassan	2012	2012 IEEE Fifth International Conference on Software Testing, Verification and Validation	10.1109/ICST.2012.121	use case;reliability engineering;user;load testing;software performance testing;performance engineering;metric;computer science;systems engineering;engineering;software development;operating system;software engineering;database;histogram;empirical research;software metric;server;measurement	SE	-55.271901037351896	43.35428262793387	56495
254be86257aa37294569d0b07a12f2b9e5dc513f	mining application-specific coding patterns for software maintenance	java programming;software maintenance;pattern mining;aspect mining;crosscutting concerns;sequential pattern mining;reverse engineering	A crosscutting concern is often implemented based on a coding pattern, or a particular sequence of method calls and control statements. We have applied a sequential pattern mining algorithm to capture coding patterns in Java programs. We have manually investigated the resultant patterns that involve both crosscutting concerns and implementation idioms. This paper discusses the detail of our pattern mining algorithm and reports detected crosscutting concerns.	algorithm;control flow;cross-cutting concern;data mining;java;resultant;sequential pattern mining;software maintenance	Takashi Ishio;Hironori Date;Tatsuya Miyake;Katsuro Inoue	2008		10.1145/1404953.1404956	reliability engineering;real-time computing;engineering;data mining	SE	-56.8500039741726	35.65761570703074	56559
a7aa60a58df4f90ea444b3038ef59a5b03130706	security testing for operating system and its system calls	linux;operating systems;security testing;system calls;test automation	It is very important but quite difficult to test the security of an operating system. In this paper, some essential problems about security testing of an operating system are discussed, including conception and extension of security testing, feasibility and technical scheme for automated security testing of an operating system, security of system calls, testing sequence for system calls, and etc. Thereafter, a prototype system (i.e. a series of testing tools) for automated security testing of system calls is designed and implemented based on Fedora 9 and Linux kernel 2.6.25-14.fc9.i686, which is made up of control module, objects setup module, standard test module, special test module and test configuration database for each system call. Furthermore, test cases as well as test results for systems calls such as creat, access and etc are discussed and analyzed. Finally, the research work in this paper is summarized while further research directions are pointed out.	operating system;security testing	Gaoshou Zhai;Hanhui Niu;Na Yang;Minli Tian;Chengyu Liu;Hengsheng Yang	2009		10.1007/978-3-642-10847-1_15	non-regression testing;computer security model;test strategy;embedded system;black-box testing;real-time computing;orthogonal array testing;white-box testing;manual testing;integration testing;computer science;functional testing;system under test;security testing;system testing;computer security;test management approach	OS	-55.802577368691956	44.5200467570789	56715
cac5209edb438679601e808768a922add1616ac4	an approach for regression test case selection using object dependency graph	graph theory;object dependency graph;regression testing;regression test case selection;object oriented programming;regression test case selection object dependency graph regression testing;program testing;program testing graph theory object oriented programming;object oriented software regression test case selection object dependency graph software development activities change identifier;testing object recognition databases software maintenance object oriented programming computers	Regression testing is one important step in software development activities to ensure a new change does not have a negative impact to unchanged parts. Regression test case selection is an approach to reduce time and resource consumption in regression testing. We present a framework of regression test case selection by using object dependency graph as a change identifier and identifying the test cases which are worthwhile to be rerun in object-oriented software.	identifier;regression testing;software development;test case	Adipat Larprattanakul;Taratip Suwannasart	2013	2013 5th International Conference on Intelligent Networking and Collaborative Systems	10.1109/INCoS.2013.115	regression testing;method;computer science;graph theory;machine learning;pattern recognition;programming language;object-oriented programming;software regression	SE	-61.9937507856509	35.2605690362768	56802
2aa48e6d215a775e452892dc397228a149efd65b	a language-independent software renovation framework	hand held device;hierarchical clustering;refactoring;geographic information system;software systems;hybrid approach;software evolution;clustering;code clone;genetic algorithm;software renovation;genetic algorithms;source code;hill climbing;open source	One of the undesired effects of software evolution is the proliferation of unused components, which are not used by any application. As a consequence, the size of binaries and libraries tends to grow and system maintainability tends to decrease. At the same time, a major trend of today s software market is the porting of applications on hand-held devices or, in general, on devices which have a limited amount of available resources. Refactoring and, in particular, the miniaturization of libraries and applications are therefore necessary. We propose a Software Renovation Framework (SRF) and a toolkit covering several aspects of software renovation, such as removing unused objects and code clones, and refactoring existing libraries into smaller more cohesive ones. Refactoring has been implemented in the SRF using a hybrid approach based on hierarchical clustering, on genetic algorithms and hill climbing, also taking into account the developers feedback. The SRF aims to monitor software system quality in terms of the identified affecting factors, and to perform renovation activities when necessary. Most of the framework activities are language-independent, do not require any kind of source code parsing, and rely on object module analysis. The SRF has been applied to GRASS, which is a large open source Geographical Information System of about one million LOCs in size. It has significantly improved the software organization, has reduced by about 50% the average number of objects linked by each application, and has consequently also reduced the applications memory requirements. 2004 Elsevier Inc. All rights reserved.	binary file;cluster analysis;code refactoring;genetic algorithm;geographic information system;hierarchical clustering;hill climbing;language-independent specification;library (computing);mobile device;object file;open-source software;parsing;requirement;software evolution;software system	Massimiliano Di Penta;Markus Neteler;Giuliano Antoniol;Ettore Merlo	2005	Journal of Systems and Software	10.1016/j.jss.2004.03.033	real-time computing;genetic algorithm;software sizing;computer science;systems engineering;engineering;backporting;software framework;software development;operating system;software engineering;software rot;software construction;geographic information system;programming language;software maintenance;software deployment;code refactoring;software system	SE	-57.58348409119117	34.451779058565876	56894
e8cf0149ad0cfb4bf3e3c4baf052f60be2343b10	coverage-based software testing: beyond basic test requirements		Code coverage is one of the core quality metrics adopted by software testing practitioners nowadays. Researchers have devised several coverage criteria that testers use to assess the quality of test suites. A coverage criterion operates by: (1) defining a set of test requirements that need to be satisfied by the given test suite and (2) computing the percentage of the satisfied requirements, thus yielding a quality metric that quantifies the potential adequacy of the test suite at revealing program defects. What differentiates one coverage criterion from another is the set of test requirements involved. For example, function coverage is concerned with whether every function in the program has been called, and statement coverage is concerned with whether every statement in the program has executed.#R##N##R##N#The use of code coverage in testing is not restricted to assessing the quality of test suites. For example, researchers have devised test suite minimization and test case generation techniques that also leverage coverage.#R##N##R##N#Early coverage-based software testing techniques involved basic test requirements such as functions, statements, branches, and predicates, whereas recent techniques involved (1) test requirements that are complex code constructs such as paths, program dependences, and information flows or (2) test requirements that are not necessarily code constructs such as program properties and user-defined test requirements. The focus of this chapter is to compare these two generations of techniques in regard to their effectiveness at revealing defects. The chapter will first present preliminary background and definitions and then describe impactful early coverage techniques followed by selected recent work.	requirement;software testing	Wes Masri;Fadi A. Zaraket	2016	Advances in Computers	10.1016/bs.adcom.2016.04.003	modified condition/decision coverage;test data generation;simulation;computer science;test suite;code coverage;test case;test management approach	SE	-60.48981775434058	34.060995953158276	56964
1b9961cb606c46d0424d8ce5eec22d82cb0c8e2a	test case prioritization for compilers: a text-vector based approach	testing;test case prioritization search strategy adaptive random strategy greedy strategy fault relevant characteristic token extraction bug detection text vector based approach c compiler testing;principal component analysis;transforms;optimization;testing search problems computer bugs principal component analysis program processors transforms optimization;vectors feature extraction greedy algorithms program compilers program debugging program testing search problems software fault tolerance;search problems;computer bugs;program processors	Test case prioritization aims to schedule the execution order of test cases so as to detect bugs as early as possible. For compiler testing, the demand for both effectiveness and efficiency imposes challenge to test case prioritization. In the literature, most existing approaches prioritize test cases by using some coverage information (e.g., statement coverage or branch coverage), which is collected with considerable extra effort. Although input-based test case prioritization relies only on test inputs, it can hardly be applied when test inputs are programs. In this paper we propose a novel text-vector based test case prioritization approach, which prioritizes test cases for C compilers without coverage information. Our approach first transforms each test case into a text-vector by extracting its tokens which reflect fault-relevant characteristics and then prioritizes test cases based on these text-vectors. In particular, in our approach we present three prioritization strategies: greedy strategy, adaptive random strategy, and search strategy. To investigate the efficiency and effectiveness of our approach, we conduct an experiment on two C compilers (i.e., GCC and LLVM), and find that our approach is much more efficient than the existing approaches and is effective in prioritizing test cases.	code coverage;gnu compiler collection;greedy algorithm;llvm;requirement prioritization;software bug;terabyte;test case;vii	Junjie Chen;Yanwei Bai;Dan Hao;Yingfei Xiong;Hongyu Zhang;Lu Zhang;Bing Xie	2016	2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)	10.1109/ICST.2016.19	real-time computing;software bug;computer science;theoretical computer science;operating system;software engineering;data mining;software testing;test case;test management approach;principal component analysis	SE	-60.35937268524684	36.30241647467035	57020
33036c9fb05962c9345a19fa14bd696d82b78425	test case automatic generation research based on aadl behavior annex	aadl behavior annex;automatic test case generation;subsection gradient descent algorithm;software test	Test case generation is essential to software test. Software test efficiency can be greatly improved through test case automatic generation. AADL Behavior Annex is an extension of AADL which can describe detailed behavior of AADL component. In this paper, we discuss a test case automatic generation method based on the AADL Behavior Annex. This method contains two parts: automatic generation of test sequences and automatic generation of test data. The former generates test sequences through dominator analysis; the later uses subsection gradient descent algorithm to generate test data.	algorithm;architecture analysis & design language;behavioral modeling;chumby;dominator (graph theory);gradient descent;mathematical optimization;programmable logic array;software development;software project management;software testing;subroutine;test case;test data	Yu-Lan Yang;Hong-Bing Qian;Yang-Zhao Li	2011		10.1007/978-3-642-23881-9_18	real-time computing;computer science;programming language	SE	-56.38113987021813	32.55709089524475	57036
b5186d3024aca61f5fb5447f935844c37e365e09	wsim: detecting clone pages based on 3-levels of similarity clues	software;target entities;complexity theory;web pages;dynamic selection;web applications;cloning;static selection;accuracy;visualization;internet;web application;static selection code clones web applications target entities clone page detection similarity clue web page passed parameter dynamic selection;similarity;code clone;web page;passed parameter;cloning accuracy web pages software visualization noise complexity theory;page clone;noise;similarity clue;similarity web application page clone;clone page detection;code clones	Code clones often result in code inconsistencies, which eventually increase cost and degrade quality. Web applications have higher rate of clones than normal software and it is more and more necessary to detect clones in web applications. In this paper, three levels of views in detecting clone pairs are suggested for a web application. The proposed technique utilizes relationships between web pages, passed parameters, and target entities as similarity clues. The results of the experiments also represent the trade-off between recall rate and accuracy. And then, two approaches, static and dynamic selection, are suggested for deciding candidates of clone pairs. As a result, the combined strategy of three levels of methods and two approaches of candidate selection is recommended. Finally, applicability of the proposed approach is shown from the experiments.	artificial neural network;entity;experiment;identifier;machine learning;open-source software;qr code;sensitivity and specificity;sensor;usability;web application;web page	Woosung Jung;Chisu Wu;Eunjoo Lee	2010	2010 IEEE/ACIS 9th International Conference on Computer and Information Science	10.1109/ICIS.2010.102	computer science;bioinformatics;database;world wide web	SE	-60.52072720565228	39.704248893775954	57305
b4df0166787838970da24ea92b3c9aec98ad1b6a	refinement and test case generation in unifying theory of programming	mutation testing;fault based testing;integration testing;programming language;fault based technique;programming language test case generation programming unifying theory denotational semantics fault based testing fault based technique mutation testing design specifications;design specifications;denotational semantic;programming unifying theory;test case generation;program testing;denotational semantics;source code;programming languages program testing;programming languages;unifying theories of programming	This talk presents a theory of testing that integrates into Hoare and Hepsilas Unifying Theory of Programming (UTP). We give test cases a denotational semantics by viewing them as specification predicates. This reformulation of test cases allows for relating test cases via refinement to specifications and programs. Having such a refinement order that integrates test cases, we develop a testing theory for fault-based testing. Fault-based testing uses test data designed to demonstrate the absence of a set of pre-specified faults. A well-known fault-based technique is mutation testing. In mutation testing, first, faults are injected into a program by altering (mutating) its source code. Then, test cases that can detect these errors are designed. The assumption is that other faults will be caught, too. We apply the mutation technique to both specifications and programs. Using our theory of testing, two new test case generation laws for detecting injected (anticipated) faults are presented: one is based on the semantic level of design specifications, the other on the algebraic properties of a programming language.	test case	Jifeng He	2008		10.1109/ICSM.2008.4658048	random testing;model-based testing;white-box testing;integration testing;computer science;equivalence partitioning;theoretical computer science;functional testing;mutation testing;programming language;denotational semantics;algorithm;source code	NLP	-58.33134923048074	35.37389320363369	57455
57a409581e1b7222f9229eb0ca742c10f9b340a9	improving fault-based conformance testing	mutation testing;industrial case study;labelled transition system;input output conformance;ioco;session initiation protocol;cadp tgv;input output;conformance testing;session initiation protocol sip;protocol specification;fault model;state space explosion;labelled transition systems	Fault-based conformance testing is a conformance testing strategy that relies on specific fault models. Previously, this mutation testing technique has been applied to protocol specifications. Although a practical case study of web-server testing has been conducted, we observed several issues when applying this method in a large industrial project. In this paper, we discuss the foundations, techniques and tools to overcome these shortcomings. More specifically, we show a solution to the problem of state-space explosion in generating mutation tests for industrial scale applications. Furthermore, the previous approach used the counterexamples of a bisimulation check (between the original and the mutant) as test purposes. With respect to input-output conformance (ioco), this is an over-approximation resulting in more tests than are necessary. Hence, we propose to use an ioco-checker in order to generate less test cases. An industrial case study demonstrates these improvements.	approximation;bisimulation;conformance testing;correctness (computer science);experiment;fault model;model checking;mutation testing;out of memory;server (computing);state space;test case;web server	Bernhard K. Aichernig;Martin Weiglhofer;Franz Wotawa	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.11.006	input/output;real-time computing;simulation;computer science;conformance testing;fault model;mutation testing;session initiation protocol	SE	-55.06966916320041	40.637340977133654	57526
721eb8186fd0a42862a9bc339b8c58f82013b0e6	automatic detection of potential layout faults following changes to responsive web pages (n)	web design fault diagnosis human computer interaction program testing;software testing;responsive web;web pages;cascading style sheets;empirical studies software testing responsive web;layout;html;inserted layout faults automatic layout fault detection web pages mobile devices world wide web functional user friendly web sites web enabled devices responsive web design rwd complex cascading style sheets css web site appearance responsive web site testing manual screenshot inspection viewport widths redecheck tool;layout cascading style sheets web pages html mobile handsets;mobile handsets;empirical studies	Due to the exponential increase in the number ofmobile devices being used to access the World Wide Web, it iscrucial that Web sites are functional and user-friendly across awide range of Web-enabled devices. This necessity has resulted in the introduction of responsive Web design (RWD), which usescomplex cascading style sheets (CSS) to fluidly modify a Web site's appearance depending on the viewport width of the device in use. Although existing tools may support the testing of responsive Web sites, they are time consuming and error-prone to use because theyrequire manual screenshot inspection at specified viewport widths. Addressing these concerns, this paper presents a method thatcan automatically detect potential layout faults in responsively designed Web sites. To experimentally evaluate this approach, weimplemented it as a tool, called ReDeCheck, and applied itto 5 real-world web sites that vary in both their approach toresponsive design and their complexity. The experiments revealthat ReDeCheck finds 91% of the inserted layout faults.	breakpoint;cascading style sheets;cognitive dimensions of notations;experiment;html;javascript;responsive web design;screenshot;sensor;time complexity;usability;viewport;web 2.0;web application development;web page;world wide web	Thomas A. Walsh;Phil McMinn;Gregory M. Kapfhammer	2015	2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1109/ASE.2015.31	web service;layout;web application security;static web page;web development;web modeling;data web;web mapping;web-based simulation;html;web design;computer science;engineering;web navigation;web page;database;responsive web design;software testing;client-side scripting;cascading style sheets;empirical research;web 2.0;world wide web;web design program;engineering drawing;web server	SE	-58.68150253321928	40.741025260440864	57645
a3c1ce0c5b0aa57b7988e754a8d5a552c3b074db	practical selective regression testing with effective redundancy in interleaved tests		As software systems evolve and change over time, test suites used for checking the correctness of software typically grow larger. Together with size, test suites tend to grow in redundancy. This is especially problematic for complex highly-configurable software domains, as growing the size of test suites significantly impacts the cost of regression testing.  In this paper we present a practical approach for reducing ineffective redundancy of regression suites in continuous integration testing (strict constraints on time-efficiency) for highly-configurable software. The main idea of our approach consists in combining coverage based redundancy metrics (test overlap) with historical fault-detection effectiveness of integration tests, to identify ineffective redundancy that is eliminated from a regression test suite. We first apply and evaluate the approach in testing of industrial video conferencing software. We further evaluate the approach using a large set of artificial subjects, in terms of fault-detection effectiveness and timeliness of regression test feedback. We compare the results with an advanced retest-all approach and random test selection. The results show that regression test selection based on coverage and history analysis can: 1) reduce regression test feedback compared to industry practice (up to 39%), 2) reduce test feedback compared to the advanced retest-all approach (up to 45%) without significantly compromising fault-detection effectiveness (less than 0.5% on average), and 3) improve fault detection effectiveness compared to random selection (72% on average).	code coverage;continuous integration;correctness (computer science);expect;fault detection and isolation;feature interaction problem;hcs clustering algorithm;integration testing;random testing;regression testing;requirement;software system;test suite	Dusica Marijan;Marius Liaaen	2017	2018 IEEE/ACM 40th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)	10.1145/3183519.3183532	software system;regression testing;real-time computing;redundancy (engineering);correctness;software;computer science;integration testing;suite;fault detection and isolation	SE	-60.878247792252935	34.88854268092765	57864
021b698f162ce5c169ebdc20a8a7b95768c266ac	poster: retro: an automated, application-layer record and replay for android	debugging;record replay;conference paper;android apps	Today's mobile applications operate in a diverse set of environments, where it is difficult for a developer to know beforehand what conditions his or her application will be put under. For example, once deployed on an online application store, an application can be downloaded on different types of hardware, ranging from budget smartphones to high-end tablets. In addition, network conditions can vary widely from Wi-Fi to 3G to 4G. Mobile applications also need to co-exist with other applications that compete for resources at different times.  Due to this diverse set of operating conditions, it is difficult to understand what problems are occurring in the wild for mobile applications. Moreover, it is even more difficult to reproduce problems in a lab environment where developers can debug the problems. Some platforms support bug reports and stack traces, but they are inadequate in scenarios when operating conditions and inputs are not consistent.  To address these issues, we propose Retro, an automated, application-layer record and replay system for Android. Unlike previous record and replay systems, Retro aims to support mobile Android applications with three features.  First, Retro provides an automated instrumentation framework that transforms a regular Android application into a traceable application. This means that Retro does not require any change in the Android platform; thus, it enables developers to distribute instrumented applications via online application stores. Through the instrumentation, Retro records application-layer events such as click events, sensor readings, method calls, and return values. In order to reduce the overhead of logging, Retro also uses a selective logging mechanism that decides which event types to log at runtime.  Second, Retro provides a replayer that a developer can use in a lab environment to faithfully replay a recorded run. To maximize the ease of use, Retro seamlessly integrates this replay functionality into Android's existing development workflow by adding the replayer into the Android platform. This means that a developer can replay using a regular phone as well as an emulator. Also, Retro provides a VCR-like interface for replaying that is capable of forwarding and rewinding executions.  Third, Retro examines Android-specific issues in enabling record and replay and incorporates design choices that are tailored towards Android. The goal for doing this is efficiency and faithfulness; by examining Android-specific issues, Retro can provide efficient recording and replaying functionalities as well as faithfulness in replaying.	android;debugging;emulator;mobile app;overhead (computing);run time (program lifecycle phase);smartphone;stack trace;traceability;tracing (software);usability;videocassette recorder;web application	Taeyeon Ki;Satyaditya Munipalle;Karthik Dantu;Steven Y. Ko;Lukasz Ziarek	2014		10.1145/2594368.2601453	embedded system;real-time computing;simulation;computer science;operating system;debugging;computer network	Mobile	-54.58212337080631	41.51442138011656	57883
2623fc96b682b0fcbaf5e106a0437112d0f06601	consistency techniques for interprocedural test data generation	institutional repositories;software testing;fedora;unit testing;automated test data generation;test data generation;control flow graph;constraint satisfaction;vital;arrays;procedures;vtls;control dependence;consistency;ils	This paper presents a novel approach for automated test data generation of imperative programs containing integer, boolean and/or float variables. It extends our previous work to programs with procedure calls and arrays. A test program (with procedure calls) is represented by an Interprocedural Control Flow Graph (ICFG). The classical testing criteria (statement, branch, and path coverage), widely used in unit testing, are extended to the ICFG. For path coverage, the specified path is transformed into a path constraint. Our previous consistency techniques, the core idea behind the solving of path constraints, have been extended to handle procedural calls and operations with arrays. For statement (and branch) coverage, paths reaching the specified node or branch are dynamically constructed. The search for suitable paths is guided by the interprocedural control dependences of the program. The search is also pruned by a new specialized consistency filter. Finally, test data are generated by the application of the proposed path coverage algorithm. A prototype has been implemented. Experiments show the feasibility of the approach.	algorithm;code coverage;control flow graph;imperative programming;linear programming relaxation;prototype;test automation;test data generation;unit testing	Nguyen Tran Sy;Yves Deville	2003		10.1145/940071.940087	procedure;basis path testing;test data generation;real-time computing;constraint satisfaction;computer science;theoretical computer science;database;software testing;unit testing;consistency;programming language;control flow graph	SE	-59.26490984573029	35.83675074288994	58325
30a0eedb198645a30c6e88e7de3f8b94d74fb600	analyzing regression test selection techniques	software testing;regression test selection;test programa;regression testing;performance evaluation;software measurement;information science;software maintenance;reutilizacion;software management;software engineering;reuse;statistical analysis;program testing;software reusability;genie logiciel;program test reuse;production;analyse regression;regression analysis;program debugging;computer science;software testing software maintenance costs production performance evaluation computer science information science computational efficiency software measurement;fiabilite logiciel;software reusability program testing program debugging software maintenance statistical analysis;fiabilidad logicial;test programme;computational efficiency;test suite;selective retest;program test;software reliability;regression test selection techniques;gestion logiciel;reutilisation;framework;selective retest regression test selection techniques software maintenance program test reuse test suite framework	Regression testing is a necessary but expensive maintenance activity aimed at showing that code has not been adversely affected by changes. Regression test selection techniques reuse tests from an existing test suite to test a modified program. Many regression test selection techniques have been proposed; however, it is difficult to compare and evaluate these techniques because they have different goals. This paper outlines the issues relevant to regression test selection techniques, and uses these issues as the basis for a framework within which to evaluate the techniques. We illustrate the application of our framework by using it to evaluate existing regression test selection techniques. The evaluation reveals the strengths and weaknesses of existing techniques, and highlights some problems that future work in this area should address.	regression testing;test suite	Gregg Rothermel;Mary Jean Harrold	1996	IEEE Trans. Software Eng.	10.1109/32.536955	reliability engineering;regression testing;information science;computer science;systems engineering;software framework;software engineering;test suite;reuse;software testing;software maintenance;software measurement;software quality;regression analysis	SE	-62.36254211113501	33.16507110176314	58338
7260382b0041f6ec751f891dec027247fe91183d	conservation of software science parameters across modularization	external routine usages;common file specification;fortran systems;software systems;standard names;structured data interfacing;common files;software reliability	Current results in software science research provide a potentially powerful tool for software engineering management. Software science parameters including time required to write a program and program length can be estimated from parameters available at the time of program design specification. The application of these results to modularized programs is not straightforward since the derived parameters are nonlinear in vocabulary size. We define an integrated vocabulary for a modularized program. A parameter is said to be conserved across modularization if the parameter value derived from the integrated vocabulary equals the sum of the parameter values derived from the modules: three parameters have been found to be conserved across modularization in well-modularized programs. Failure to exhibit conservation of length can be used to detect excessive or insufficient intermodule communication.	nonlinear system;software engineering;vocabulary	Lawrence Hunter;Jose C. Ingojo	1977		10.1145/800179.810200	computer science;software engineering;data mining;database;software system	SE	-51.344789505032495	35.75537021749369	58804
a755d34953ead5a10bf4c0447fadc95798b99ded	an empirical investigation of inheritance trends in java oss evolution	open source systems;thesis;object oriented;inheritance;java;evolution	................................................................................................2 Declaration of Authorship...........................................................................3 Dedication...............................................................................................4 Acknowledgment......................................................................................5 Table of	acknowledgment index;java	Emal Nasseri	2009			real-time computing;computer science;software engineering;programming language	PL	-53.18293899220351	33.16260699919565	59157
44818b62108b296ec14abaeeee8c42475b66ef21	implementing the factory pattern with the help of reflection		Reflection, reflection-based programming and metaprogramming are valuable tools for many programming tasks, like the implementation of persistence and serialization-like operations, object-relational mapping, remote method invocation, automatic generation of user-interfaces, etc., and also for the implementation of several design patterns. C++ as one of the most prevalent programming languages still lacks support for standardised, compiler-assisted reflection. In this paper we introduce in short the Mirror reflection library which is a part of an ongoing effort to add reflection to C++ and we will show how reflection can be used to greatly simplify the implementation of object factories - classes constructing instances of other classes from various external data representations.		Matús Chochlík	2016	Computing and Informatics		theoretical computer science;factory method pattern;computer science;programming language;software design pattern;persistence (computer science);metaprogramming;generic programming	HCI	-52.71118539402526	34.22439438732807	59288
80620faeaac8fa4004245f9b19b7f72ce2b844af	ecity: a tool to track software structural changes using an evolving city	software evolution software architecture visualization software maintenance;software maintenance;software systems;maintenance engineering;object oriented programming;layout;software architecture visualization ecity software structural change tracking evolving city large scale software system maintenance software structure evolution analysis component insertion component removal component modification interactive visualization;data visualisation;software architecture;software evolution;image color analysis;software architecture visualization;cities and towns;cities and towns software systems layout image color analysis software maintenance maintenance engineering;software maintenance data visualisation object oriented programming software architecture	One of the main challenges in the maintenance of large-scale software systems is to ascertain the underlying software structure and to analyze its evolution. In this paper we present a tool to assist software architects and developers in not only understanding the software structure of their system but more importantly to track the insertion, removal, or modification of components over time. The tool is based on the idea that the above-mentioned stakeholders should have an intuitive, efficient, and effective means to detect when, where, and what structural changes took place. The main components include an interactive visualization that provides an overview of these changes. The usefulness of this approach is highlighted through a summary of a user study we conducted.	interactive visualization;software architect;software system;usability testing	Taimur Ahmed Khan;Henning Barthel;Achim Ebert;Peter Liggesmeyer	2013	2013 IEEE International Conference on Software Maintenance	10.1109/ICSM.2013.80	maintenance engineering;layout;software visualization;software architecture;long-term support;verification and validation;software sizing;computer science;systems engineering;engineering;package development process;backporting;software evolution;social software engineering;software framework;component-based software engineering;software development;software design description;software engineering;software construction;software walkthrough;object-oriented programming;software analytics;resource-oriented architecture;software maintenance;software deployment;software system;computer engineering	SE	-55.52591833345731	34.86496587690835	59395
a2b334ce11935abb11b7298d1f8027a96d80b73e	medic: a static analysis framework for equivalent mutant identification		Context: The equivalent mutant problem is a well-known impediment to the adoption of mutation testing in practice. In consequence of its undecidable nature, a complete automated solution is unattainable. To worsen the situation, the manual analysis of the generated mutants of a program under test is prohibitive due to their vast number and the complexity of determining their equivalence. Objective: This paper focuses on the automated identification of equivalent and partially equivalent mutants, i.e. mutants that are equivalent to the original program for a specific subset of paths. To this end, the utilisation of a series of previously proposed data flow patterns is investigated. This study also examines the cross-language nature of these patterns and the killability of the detected partially equivalent mutants. Method: A tool, named MEDIC (Mutants’ Equivalence DIsCovery), incorporating the aforementioned patterns was developed. Its efficiency and effectiveness were evaluated based on a set of manually analysed mutants from real-world programs, written in the Java programming language. Furthermore, MEDIC was employed to test subjects written in the JavaScript programming language. Results: MEDIC managed to detect 56 per cent of the examined equivalent mutants in 125 seconds, providing strong evidence regarding both its effectiveness and efficiency. Additionally, MEDIC was able to identify equivalent mutants in the JavaScript test subjects, lending colour to the cross-language nature of the implemented patterns. Finally, the identified partially equivalent mutant set consisted largely of killable mutants, 16 per cent of which were stubborn ones. ∗Corresponding author Email addresses: kintism@aueb.gr (Marinos Kintis), ngm@aueb.gr (Nicos Malevris) Conclusion: It can be concluded that pattern-based equivalent mutant identification forms a viable approach for combating the equivalent mutant problem. MEDIC automatically detected a considerable number of the manually identified equivalent mutants and was successfully applied to test subjects in all examined programming languages.	dataflow;email;java;javascript;mutation testing;programming language;static program analysis;turing completeness;undecidable problem	Marinos Kintis;Nicos Malevris	2015	Information & Software Technology	10.1016/j.infsof.2015.07.009	simulation;engineering;engineering drawing;algorithm	SE	-57.33995100604802	38.65417463288173	59586
4a0aaa6a893e9313042f96281db4a8f6a2cf8fc4	a proposed design patterns extension for the bluej ide	design pattern;design patterns;bluej;teaching;java	In this tip we describe a proposed extension to the BlueJ IDE to encapsulate knowledge of patterns in the IDE and to help students explore them and learn how to make use of them.	bluej;design pattern	James H. Paterson;John Haddow	2004		10.1145/1007996.1008117	education;software design pattern;real-time computing;computer science;design pattern;programming language;java;world wide web	HCI	-51.9320186833824	33.28630747132718	59699
7d7b23b92c9c87bb6ce5a6757c033d68aaf59ff4	selsus: towards a reference architecture for diagnostics and predictive maintenance using smart manufacturing devices	bayes methods cognition object oriented modeling automation maintenance engineering engines manufacturing;bayes methods;maintenance engineering;conference contribution;engines;cognition;component based design diagnostics prognostics maintenance manufacturing;manufacturing;smart automation devices selsus reference architecture predictive maintenance diagnostics maintenance smart manufacturing devices self sustaining manufacturing systems prognostic capabilities diagnostic capabilities;preventive maintenance fault diagnosis manufacturing systems;object oriented modeling;automation	We propose a reference architecture, SelSus (SELf-SUStaining Manufacturing Systems) that aims to enable the provisioning of diagnostic and prognostic capabilities in manufacturing systems that utilize the notions of “smart” automation devices.	bayesian network;coupling (computer programming);provisioning;reference architecture;resultant	Mohamed S. Sayed;Niels Lohse;Nicolaj Søndberg-Jeppesen;Anders L. Madsen	2015	2015 IEEE 13th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2015.7281990	reliability engineering;process development execution system;systems engineering;engineering;computer-integrated manufacturing;manufacturing engineering	Robotics	-48.32775700530791	41.176350040161154	60111
ac118f5cf88a1cfb6d5f8c195aa635d81f892ebb	extracting a knowledge from source code comprehesion using data mining methods	coupling;software maintenance;data mining;cohesion;clustering;source code comprehension	Abstract: In software maintenance, source code comprehension is a very vital task. The comprehension of the source code is performed by different tools for various purposes. Data mining is one of the important and versatile methods in this context. Data mining methods and tools have been widely used in software engineering in general and software maintenance in particular. We present in this paper a methodology to extract knowledge using data mining methods which would be very much useful for software maintenance. The data mining methods for clustering, classification and association rules have been deployed for source code comprehension. Our approach is holistic in nature that covers many aspects required for software maintenance whereas approaches by other researchers cover a partial aspect in this context. We have made a qualitative comparison of our approach with others and have derived the conclusion on the basis.	association rule learning;cluster analysis;data mining;holism;software engineering;software maintenance	Ashutosh Mishra;Vinayak Srivastava	2012	IJKEDM	10.1504/IJKEDM.2012.051240	kpi-driven code analysis;software visualization;software mining;computer science;software development;cohesion;machine learning;software construction;data mining;database;cluster analysis;coupling;programming language;software analytics;software maintenance;static program analysis	SE	-56.97589374289482	34.26566814545983	60236
fed19e1fb10afa2deb24c131d4eecda99a1447d6	improving predictive models of cognitive complexity using an evolutionary computational approach: a case study	program understanding;software maintenance;program comprehension;search based software engineering;mobile applications;machine learning;software component;genetic algorithm;feature selection;source code;prediction model;repackaging;evolutionary computing;cognitive complexity	The development of software is a human endeavor and program comprehension is an important factor in software maintenance. Predictive models can be used to identify software components as potentially problematic for the purpose of future maintenance. Such modules could lead to increased development effort, and as such, may be in need of mitigating actions such as refactoring or assigning more experienced developers.  Source code metrics can be used as input features to classifiers, however, there exist a large number of structural measures that capture different aspects of coupling, cohesion, inheritance, complexity and size. In machine learning, feature selection is the process of identifying a subset of attributes that improves a classifier's performance. This paper presents initial results when using a genetic algorithm as a method of improving a classifier's ability to discover cognitively complex classes that degrade program understanding.	code refactoring;cognitive complexity;cohesion (computer science);component-based software engineering;existential quantification;feature selection;genetic algorithm;machine learning;predictive modelling;program comprehension;software maintenance;software metric	Rodrigo A. Vivanco;Dean Jin	2007		10.1145/1321211.1321223	genetic algorithm;software sizing;search-based software engineering;computer science;software framework;component-based software engineering;software development;machine learning;cognitive complexity;software construction;data mining;database;predictive modelling;software maintenance;feature selection;source code	SE	-62.58454193483359	34.302569907585685	60418
495cda8ecc5405953df87d966987e06bac9e9c6f	sieve: a tool for automatically detecting variations across program versions	dynamic programming;program diagnostics;program binaries;sieve tool;software maintenance;program version variation detection;software systems;dynamic program;longest common subsequence;impact analysis;program testing;automatic detection;sequences dynamic programming computer bugs genetic mutations software testing software maintenance automatic testing computer science software systems instruments;open source c programs;software revisions;software tools;software maintenance sieve tool program version variation detection software systems software revisions impact analysis code block change identification dynamic programming program binaries open source c programs program testing;configuration management;software tools configuration management dynamic programming program diagnostics program testing software maintenance;code block change identification;open source	Software systems often undergo many revisions during their lifetime as new features are added, bugs repaired, abstractions simplified and refactored, and performance improved. When a revision, even a minor one, does occur, the changes it induces must be tested to ensure that invariants assumed in the original version are not violated unintentionally. In order to avoid testing components that are unchanged across revisions, impact analysis is often used to identify code blocks or functions that are affected by a change. In this paper, we present a novel solution to this general problem that uses dynamic programming on instrumented traces of different program binaries to identify longest common subsequences in strings generated by these traces. Our formulation allows us to perform impact analysis and also to detect the smallest set of locations within the functions where the effect of the changes actually manifests itself. Sieve is a tool that incorporates these ideas. Sieve is unobtrusive, requiring no programmer or compiler intervention to guide its behavior. Our experiments on multiple versions of op ensource C programs shows that Sieve is an effective and scalable tool to identify impact sets and can locate regions in the affected functions where the changes manifest. These results lead us to conclude that Sieve can play a beneficial role in program testing and software maintenance	code refactoring;code::blocks;compiler;dynamic programming;experiment;invariant (computer science);operational amplifier;programmer;scalability;sieve (mail filtering language);software bug;software maintenance;software system;software testing;tracing (software);unobtrusive javascript	Murali Krishna Ramanathan;Ananth Grama;Suresh Jagannathan	2006	21st IEEE/ACM International Conference on Automated Software Engineering (ASE'06)	10.1109/ASE.2006.61	real-time computing;computer science;operating system;software engineering;dynamic programming;longest common subsequence problem;configuration management;programming language;software maintenance;software system	SE	-57.83515568441525	38.39088369826158	60604
27db752b549171489f4a4971961399745f2ac8d3	integrating and scheduling an open set of static analyses	design guideline violation detection;type system extension;software analysis;program diagnostics;type theory java program debugging program diagnostics scheduling software engineering;software engineering;bug pattern finding;pattern detection;eclipse ide;design guideline;scheduling;type theory;software development;java scheduling static analysis software development eclipse ide bug pattern finding design guideline violation detection type system extension;program debugging;static analysis;java information analysis pattern analysis guidelines data models data analysis productivity software tools buffer overflow application software;type system;java	To improve the productivity of the development process, more and more tools for static software analysis are tightly integrated into the incremental build process of an IDE. If multiple interdependent analyses are used simultaneously, the coordination between the analyses becomes a major obstacle to keep the set of analyses open. We propose an approach to integrating and scheduling an open set of static analyses which decouples the individual analyses and coordinates the analysis executions such that the overall time and space consumption is minimized. The approach has been implemented for the Eclipse IDE and has been used to integrate a wide range of analyses such as finding bug patterns, detecting violations of design guidelines, or type system extensions for Java	eclipse;emoticon;integer programming;interdependence;java;open platform;parallel computing;requirement;scheduling (computing);sensor;specification language;static program analysis;type system	Michael Eichberg;Mira Mezini;Sven Kloppenburg;Klaus Ostermann;Benjamin Rank	2006	21st IEEE/ACM International Conference on Automated Software Engineering (ASE'06)	10.1109/ASE.2006.43	real-time computing;type system;computer science;software development;operating system;software analysis pattern;programming language;java;scheduling;type theory;static analysis	SE	-55.19045171257563	37.09032727314578	60638
7383fd695814cf2f3097c5e12e239077bba43f5d	web services composition approach based on trust computing mode	trust reputation evolution model;interactive services composition;service composition;fraudulent services entity;credibility evolution;wsc performance;malicious service nodes;service selection;quality of service web services algorithm design and analysis heuristic algorithms computational modeling equations;quality of services;trust computing;two tier model;web service composition;interentity relations;trusted computing;scheduling algorithm;computational modeling;internet;trust reputation computing;web services composition;web services scheduling;scheduling;heuristic algorithms;web services;quality of services services compositon trust computing intuitive reputation;intuitive reputation evaluation model;intuitive reputation;indirect interaction;quality of service;low quality services web services composition trust computing mode malicious service nodes web service composition wsc performance internet two tier model credibility evolution interentity relations trust reputation computing interactive services composition intuitive reputation evaluation model services scheduling trust reputation evolution model fraudulent services entity;trust computing mode;services compositon;services scheduling;low quality services;algorithm design and analysis;evaluation model	The influence of the uncertain or malicious service nodes on the Web service composition (WSC) performance is generally fatal in the Internet, so the problems of services selecting for WSC can not be completely solved by the perspective of performance. In the paper, the two-tier model of reputation computing, which describes the credibility evolution mechanism of the inter-entity relations in the course of services compistion, has been proposed. A trust reputation computing mode is builded through interactive services composition submitted by the parties, then an intuitive reputation evaluation model is formed through the direct or indirect interaction between services entities. On that basis, the services scheduling algorithm based on trust reputation evolution model has been proposed, which can effectively inhibit the interference of fraudulent services entity and reduce the influence of low-quality services in WSC. Experimental results show that the method proposed in the paper is more superior in credibility and security, comparing to the traditional services scheduling methods.	algorithm;entity;interference (communication);internet;multitier architecture;scheduling (computing);service composability principle;web service;world sudoku championship;world wide web	Chunhua Hu;Jibo Liu	2010	2010 IEEE Asia-Pacific Services Computing Conference	10.1109/APSCC.2010.98	computer science;knowledge management;operating system;internet privacy;law;scheduling;world wide web	AI	-50.10041254138587	44.17728651820686	60724
d88c60186aed638822ba7fb47ddb36533419fa67	how open source projects use static code analysis tools in continuous integration pipelines		"""Static analysis tools are often used by software developers to entail early detection of potential faults, vulnerabilities, code smells, or to assess the source code adherence to coding standards and guidelines. Also, their adoption within Continuous Integration (CI) pipelines has been advocated by researchers and practitioners. This paper studies the usage of static analysis tools in 20 Java open source projects hosted on GitHub and using Travis CI as continuous integration infrastructure. Specifically, we investigate (i) which tools are being used and how they are configured for the CI, (ii) what types of issues make the build fail or raise warnings, and (iii) whether, how, and after how long are broken builds and warnings resolved. Results indicate that in the analyzed projects build breakages due to static analysis tools are mainly related to adherence to coding standards, and there is also some attention to missing licenses. Build failures related to tools identifying potential bugs or vulnerabilities occur less frequently, and in some cases such tools are activated in a """"softer"""" mode, without making the build fail. Also, the study reveals that build breakages due to static analysis tools are quickly fixed by actually solving the problem, rather than by disabling the warning, and are often properly documented."""	code smell;continuous integration;java;list of tools for static code analysis;open-source software;pipeline (computing);software bug;software developer;static program analysis;travis ci;vulnerability (computing)	Fiorella Zampetti;Simone Scalabrino;Rocco Oliveto;Gerardo Canfora;Massimiliano Di Penta	2017	2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)		code smell;empirical research;data mining;source code;static program analysis;computer science;software;vulnerability;java	SE	-62.744226887749555	37.00440744234758	60769
63aa83a0a9474c9a882f139389bbb9adf1609054	an empirical study on clustering for isolating bugs in fault localization	pattern clustering;bugs isolation spectrum based fault localization techniques risk evaluation formulas multi bugs sbfl techniques k means clustering algorithm;multi bugs;multi bugs spectrum based fault localization cluster algorithms;program debugging pattern clustering;clustering algorithms computer bugs partitioning algorithms flexible printed circuits educational institutions sociology statistics;cluster algorithms;program debugging;spectrum based fault localization	Spectrum-based Fault Localization (SBFL) techniques use risk evaluation formulas to calculate each statement's likelihood of having a bug based on test results. SBFL can not only be used in statement level, but also can be used with other program entities such as branches, functions and so on. Most previous studies have been conducted under the assumption of a single bug. However, software always contains multi-bugs in practice. A natural idea of debugging is to isolate bugs and then use SBFL techniques to locate one bug for each group. In this paper, we conduct an empirical study on clustering for isolating bugs in fault localization. We analyze the effects of six fault localization techniques and two cluster algorithms. The main observations are: (1) ER5 (Wong1) achieves the best results of fault localization with clustering; (2) K-means outperforms hierarchical clustering for isolating bugs in fault localization.	algorithm;cluster analysis;debugging;entity;hierarchical clustering;k-means clustering;software bug	Yanqin Huang;Junhua Wu;Yang Feng;Zhenyu Chen;Zhihong Zhao	2013	2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)	10.1109/ISSREW.2013.6688893	real-time computing;computer science;theoretical computer science;data mining	SE	-60.85808575414316	35.78751341512638	60826
370fa2f4dc1b2c6cb5f6e6f4b3cd89cce12ec5e2	knowledge-base semantic gap analysis for the vulnerability detection	web security;optimal control;internet computing;sql injection;semantic gap;source code;semantic analysis;knowledge base	Web security became an alert in internet computing. To cope with ever-rising security complexity, semantic analysis is proposed to fill-in the gap that the current approaches fail to commit. Conventional methods limit their focus to the physical source codes instead of the abstraction of semantics. It bypasses new types of vulnerability and causes tremendous business loss. For this reason, the semantic structure has been studied. Our novel approach introduces token decomposition and semantic abstraction. It aims to solve the issues by using metadata code structure to envision the semantic gap. In consideration of the optimized control and vulnerability rate, we take SQL injection as an example to demonstrate the approach. For how the syntax abstraction be decomposed to token, and how the semantic structure is constructed by using metadata notation. As the new type of vulnerability can be precisely specified, business impact can be eliminated.	code;gap analysis;sql injection;semantic analysis (compilers);semantic data model	Raymond Wu;Keisuke Seki;Ryusuke Sakamoto;Masayuki Hisada	2010		10.1007/978-3-642-13365-7_27	semantic interoperability;knowledge base;semantic computing;semantic integration;sql injection;optimal control;semantic grid;computer science;theoretical computer science;social semantic web;data mining;semantic web stack;semantic compression;database;semantic technology;world wide web;computer security;semantic gap;source code	OS	-52.14946479636106	46.04962104396549	60947
ff255d5e781ddf626baf0d93bdd9c5c76753e517	fault-based identification and inspection of fault developments to enhance availability in industrial automation systems	market research;complexity theory;maintenance engineering;inspection;productivity fault diagnosis fault tolerant control industrial control inspection;automation fault diagnosis inspection complexity theory maintenance engineering adaptation models market research;adaptation models;high productivity requirements fault based identification fault based inspection fault development industrial automation systems fault prevention downtime avoidance fault occurrence system availability;fault diagnosis;increasing of system availability fault prevention fault development identification fault development inspection;automation	In this paper an innovative concept for a fault prevention in industrial automation systems is presented. The goal of this concept is the avoidance of downtimes, by means of the early identification of fault developments during runtime. Additionally, in comparison to common fault prevention solutions, today's complexity of prevention processes and the increasing amount of data is aimed to be reduced in this concept. In order to fulfill these requirements, a methodology has been developed, which helps to identify fault development characteristics after a single fault occurrence. By means of this information, the same system or similar systems can be inspected for this definite fault development in the future in order to avoid reoccurrence of this particular fault or failure. Thus, the fault-based identification and inspection helps to increase systems' availability, which is demanded amongst other things due to the high productivity requirements in industry.	automation;complexity;microsoft outlook for mac;operation time;prototype;requirement	Manuel Bordasch;Christian Brand;Peter Göhner	2015	2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)	10.1109/ETFA.2015.7301515	market research;maintenance engineering;reliability engineering;real-time computing;fault coverage;inspection;systems engineering;engineering;automation;software fault tolerance	Robotics	-53.025828125276135	41.936643820356046	61016
e14cc04d5608719c17a7a706993330ec7ef9cfd8	enabling testing of large scale highly configurable systems with search-based software engineering: the case of model-based software product lines	configuration generation;multi objective;search based software engineering;configuration prioritization;highly configurable systems;software product lines	Complex situations formed by mixes of versatile environments, various user needs and time-to-market constraints led to the development of highly configurable systems. In line with the emergence of such systems, software development is increasingly moving from the production of a single, yet configurable software to the development of families of software products. Such families of related software are called Software Product Lines (SPLs), and they allow the automation of the configuration, deployment and management of tailored software products through the combination of software features. These features and the constraints defining their legal combinations are usually encoded in a feature model (FM), which is used to represent a SPL. One main challenge with SPLs is testing them, a task which is even more difficult as the number of features proposed is important. Ideally, all the possible products that can be configured from a SPL should be tested. This, however, is unfeasible in practice since only 270 optional features allows configuring more products than the number of atoms in the universe. Considering that realistic SPLs involve thousands of features and that testing capabilities are limited by time and budget constraints, only a subset of all the configurable products can actually be tested, introducing the needs for strategies to test such SPLs. To reduce the testing effort, techniques using combinatorial interaction testing (CIT) have been proposed and proven to be successful. However, they fail at scaling to large and heavily constrained SPLs. In addition, CIT is costly to apply due to the combinatorial explosion induced by calculating the feature combinations. Besides, existing approaches do not consider multiple and potentially conflicting testing objectives such as minimizing the number of configurations and their cost. In this respect, this dissertation introduces scalable techniques for both generating and prioritizing relevant SPL product configurations for CIT by using a similarity heuristic which avoids the combinatorial explosion. In a second step, methods for handling multiple testing objectives are presented. The following part of this thesis focuses on the quality assessment of given product configurations prior to testing. The objective here is to evaluate how good is a given set of configurations according to different testing criteria, whatever the way these configurations have been selected. This situation arises when configurations that have to be tested are already available. Since testing these software products individually is a costly and time consuming task, methodologies to evaluate them prior testing are introduced, thus allowing to discard unnecessary ones and save testing sessions. In particular, an approach based on mutation of the SPL FM which can form viable and cheaper alternative to CIT is presented. The next part of this dissertation investigates the reverse-engineering of a SPL and its FM from existing source code of software product variants. Since SPLs allows us to reduce development costs and quickly derive tailored products for specific market share, automated techniques to migrate similar product variants into a whole SPL are necessary. In particular, the challenge of reverse-engineering a SPL which is concordant with the underlying software products is tackled by a fully automated approach. In addition, since reverse-engineering approaches (whether manually or automatically performed) are inherently error-prone, a methodology for evaluating and fixing reverse-engineered SPL FM is presented. The final part of this dissertation describes the application of the introduced theoretical advances to an industrial case with the CETREL company. In this project, a credit card authorization system is tested by using credit card authorizations. The testing process is optimized by modeling credit card authorizations as a SPL, enabling the application of the above-mentioned generation and evaluation approaches. All the proposed approaches use search-based techniques combined with constraint solvers and have been validated through rigorous experiments performed on moderate to large scale SPLs.		Christopher Henard	2015			reliability engineering;verification and validation;software sizing;software verification;search-based software engineering;computer science;systems engineering;package development process;component-based software engineering;software development;software construction;software testing;resource-oriented architecture;software deployment;software requirements;software system;computer engineering	SE	-60.738380101836455	33.039804955482964	61303
41b4e85621cd16192a76dd2114f02b184e173a11	experimental-analytic approach to derive software performance	software performance;performance analysis;software design	An experimental-analytic approach to derive software performance is presented. The approach unifies static and dynamic performance analysis techniques. The goal is to develop practical means to obtain precise, indepth, and relevant performance information, especially execution time, for guiding design decisions. First, computation cost functions of a program are derived. This is done by modelling the program using the computation structure model and then applying a flow analysis technique to derive the program's cost function. In addition, program instrumentation techniques are employed to measure control flows and to trace program variables. Finally, precise execution-time information is computed by evaluating the cost function against the control flows measured. The three classes of parameters (execution time, control flow, and program variables) can be analysed selectively and interactively; this gives an insight into the design's behaviour. To cope with tedious and frequent analyses of large and complex software, a tool (COPES) is built that automates the static and dynamic techniques required. The implementation and the use of this tool are illustrated.		Reda A. Ammar	1992	Information & Software Technology	10.1016/0950-5849(92)90079-5	reliability engineering;personal software process;verification and validation;software sizing;software performance testing;software verification;computer science;systems engineering;package development process;software design;social software engineering;software reliability testing;software development;software design description;software engineering;software construction;software testing;resource-oriented architecture;software measurement;software deployment;software metric;software system	SE	-55.33478096148567	36.86816079276963	61352
cb3ff53fd399ea52263476fb9290e525c1af0ad7	is the derivation of a model easier to understand than the model itself?	graph theory;complete complex model software architectures graph presentation edge connectors edge nodes expert domain knowledge;software architecture;software architecture graph theory;servers mathematical model computer crashes equations educational institutions computer architecture load modeling	Software architectures can be presented by graphs with components as nodes and connectors as edges. These graphs, or models, typically encode expert domain knowledge, which makes them difficult to understand. Hence, instead of presenting a complete complex model, we can derive it from a simple, easy-to-understand model by a set of easy-to-understand transformations. In two controlled experiments, we evaluate whether a derivation of a model is easier to understand than the model itself.	encode;experiment;graph (discrete mathematics);software architecture	Janet Siegmund;Don S. Batory;Taylor L. Riché	2012	2012 20th IEEE International Conference on Program Comprehension (ICPC)	10.1109/ICPC.2012.6240508	reference architecture;software architecture;computer science;graph theory;theoretical computer science;software engineering;domain model;distributed computing;programming language	Robotics	-54.051706177847635	32.36292324080368	61409
060c5faf123861e0f6576e2219e815aa2c37a8f4	experiments with pro-active declarative meta-programming	query language;software engineering tools;software tool;temporal logic;object oriented programming;source code history;language development;smalltalk;source code;meta programming;program querying	Program querying has become a valuable asset in the programmer's toolbox. Using dedicated querying languages, developers can reason about their source code in order to find errors, refactoring opportunities and so on. Within Smalltalk, the SOUL language has been proposed as one such language that offers a declarative and expressive means to query the source code of object-oriented programs.  Ever since its inception, SOUL has been used as the underlying technique for a number of academic software engineering tools. Despite its success, one of the problems of SOUL is that, due to its backward chained implementation, it is less suited as a basis for such pro-active software tools. Using SOUL, a developer has to launch the queries over the system manually, rather than automatically receiving feedback whenever the underlying source code is changed. In this paper we present PARACHUT, an alternative logic query language that is based on forward chaining and temporal logic and that allows developers to express queries over the change history of the system. Furthermore, PARACHUT's data-driven nature makes it possible to provide instant feedback to developers when the source code is changed, thus providing better support for pro-active software tools.	code refactoring;declarative programming;forward chaining;metaprogramming;programmer;query language;smalltalk;software engineering;soul;temporal logic	Verónica Uquillas Gómez;Andy Kellens;Kris Gybels;Theo D'Hondt	2009		10.1145/1735935.1735947	kpi-driven code analysis;code review;computer science;theoretical computer science;software development;database;programming language;static program analysis;source code	SE	-53.47917020998708	34.98104282603348	61802
a44a2af329ced925eaa3109df2a52b3c3bdd3e67	reverse engineering variability from natural language documents: a systematic literature review		Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering & machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.	algorithm;cluster analysis;emoticon;feature extraction;heart rate variability;information extraction;legacy code;machine learning;nl (complexity);natural language processing;open road tolling;preprocessor;requirement;reverse engineering;software product line;software system;sparse matrix;spatial variability;spl (unix);systematic review;video post-processing;warez	Yang Li;Sandro Schulze;Gunter Saake	2017		10.1145/3106195.3106207	software system;data mining;natural language;cluster analysis;feature extraction;software;reverse engineering;systematic review;computer science;legacy code	SE	-57.493657494512775	32.88117698981818	62007
0edec5e25569bd6089c4f392a38d13888ae5e1a9	an authorization framework resilient to policy evaluation failures	language use;optimal policy;policy language;distributed computing system;policy evaluation	In distributed computer systems, it is possible that the evaluation of an authorization policy may suffer unexpected failures, perhaps because a sub-policy cannot be evaluated or a sub-policy cannot be retrieved from some remote repository. Ideally, policy evaluation should be resilient to such failures and, at the very least, fail “gracefully” if no decision can be computed. We define syntax and semantics for an XACML-like policy language. The semantics are incremental and reflect different assumptions about the manner in which failures can occur. Unlike XACML, our language uses simple binary operators to combine sub-policy decisions. This enables us to characterize those few binary operators likely to be used in practice, and hence to identify a number of strategies for optimizing policy evaluation and policy representation.	authorization;distributed computing;fault tolerance;graceful exit;xacml	Jason Crampton;Michael Huth	2010		10.1007/978-3-642-15497-3_29	computer science;data mining;database;computer security	Security	-52.118752735055274	45.84787975386324	62501
36d9268ef14e44164690769589cc34d3f22ae383	dom transactions for testing javascript	web pages;unit testing;side effect;software transactional memory	Unit testing in the presence of side e ects requires the construction of a suitable test xture before each test run. We consider the problem of providing test xtures for unit testing of client-side JavaScript code that manipulates its underlying web page. We propose using techniques from software transactional memory to restore the test xture after each test run.	client-side;javascript;software transactional memory;unit testing;web page	Phillip Heidegger;Annette Bieniusa;Peter Thiemann	2010		10.1007/978-3-642-15585-7_24	real-time computing;computer science;operating system;software testing;world wide web;test harness	SE	-57.993012882195224	39.64605953067202	62590
0fbab8d0f0805476fd86b357be2c4547246baf95	combinatorial software testing	computers;software;software testing;protocols;combinatorial software testing;software testing practitioner;software fault detection;software fault tolerance;software technologies;testing;program verification;data mining;verificacion programa;software testing life testing system testing linux protocols automatic testing application software operating systems personal communication networks manufacturing automation;design method;program testing;combinatorial testing;pairwise testing;software fault tolerance program testing;software testing practitioner combinatorial testing software fault detection manual test case selection method;efficient test design methods software technologies design and test combinatorial software testing pairwise testing;verification programme;design and test;manual test case selection method;efficient test design methods	Combinatorial testing can detect hard-to-find software faults more efficiently than manual test case selection methods. While the most basic form of combinatorial testing-pairwise-is well established, and adoption by software testing practitioners continues to increase, industry usage of these methods remains patchy at best. However, the additional training required is well worth the effort.	software testing;test case	D. Richard Kuhn;Raghu Kacker;Yu Lei;Justin Hunter	2009	Computer	10.1109/MC.2009.253	black-box testing;white-box testing;computer science;software reliability testing;software engineering;software construction;software testing	SE	-61.69243673958396	33.12841982305332	62885
5c317218fb04e611c047ae32373b72ec75f2e6b0	identifying components in object-oriented programs using dynamic analysis and clustering	call graph;object oriented programming;functional dependency;point of view;use case;formal concept analysis;dynamic analysis	We propose an approach for component candidate identification as a first step towards the extraction of component-based architectures from object-oriented programs. Our approach uses as input dynamic call graphs, built from execution traces corresponding to use cases. This allows to better capture the functional dependencies between classes. The component identification is treated as a clustering problem. To this end, we use formal concept analysis and design heuristics.  We evaluate the feasibility of our approach on two programs. The obtained results are very satisfactory from both the performance and qualitative points of view.	cluster analysis;component-based software engineering;formal concept analysis;functional dependency;heuristic (computer science);tracing (software)	Simon Allier;Houari A. Sahraoui;Salah Sadou	2009		10.1145/1723028.1723045	use case;call graph;computer science;formal concept analysis;theoretical computer science;machine learning;data mining;database;dynamic program analysis;functional dependency;programming language;object-oriented programming	SE	-56.28820277777936	34.782690959413294	63038
b1e4354630117d4992d875b385c3893afd1a93cb	configuration coverage in the analysis of large-scale system software	large scale system;type checking;operating system;source code;static analysis	"""System software, especially operating systems, tends to be highly configurable. Like every complex piece of software, a considerable amount of bugs in the implementation has to be expected. In order to improve the general code quality, tools for static analysis provide means to check for source code defects without having to run actual test cases on real hardware. Still, for proper type checking a specific configuration is required so that all header include paths are available and all types are properly resolved.   In order to find as many bugs as possible, usually a """"full configuration"""" is used for the check. However, mainly because of alternative blocks in form of #else-blocks, a single configuration is insufficient to achieve full coverage. In this paper, we present a metric for configuration coverage (CC) and explain the challenges for (properly) calculating it. Furthermore, we present an efficient approach for determining a sufficiently small set of configurations that achieve (nearly) full coverage and evaluate it on a recent Linux kernel version."""		Reinhard Tartler;Daniel Lohmann;Christian Dietrich;Christoph Egger;Julio Sincero	2011		10.1145/2039239.2039242	real-time computing;bebugging;computer science;operating system;distributed computing;code coverage	SE	-58.261862275211364	38.904764143922655	63049
b3eba28df62d0c843ba91c989d84dd0175bcaed6	a clustering-bayesian network based approach for test case prioritization	bayesian network;regression testing;measurement;fault detection measurement testing bayes methods software quality java clustering algorithms;source code software belief networks fault diagnosis java program testing regression analysis software metrics software quality;bayes methods;testing;会议论文;test case prioritization tcp;test case prioritization;bna clustering bayesian network based approach regression testing source code change information software quality metrics test coverage data bayesian networks based test case prioritization bntcp fault detection capability hybrid regression test case prioritization technique code coverage based clustering approach java project fault detection performance bayesian networks based approach;clustering;fault detection;clustering algorithms;bayesian network bn;software quality;java	Test case prioritization can effectively reduce the cost of regression testing by executing test cases with respect to their contributions to testing goals. Previous research has proved that the Bayesian Networks based technique which uses source code change information, software quality metrics and test coverage data has better performance than those methods merely depending on only one of the items above. Although the former Bayesian Networks based Test Case Prioritization (BNTCP) focusing on assessing the fault detection capability of each test case can utilize all three items above, it still has a deficiency that ignores the similarity between test cases. For mitigating this problem, this paper proposes a hybrid regression test case prioritization technique which aims to achieve better prioritization by incorporating code coverage based clustering approach with BNTCP to depress the impact of those similar test cases having common code coverage. Experiments on two Java projects with mutation faults and one Java project with hand-seeded faults have been conducted to evaluate the fault detection performance of the proposed approach against Additional Greedy approach, Bayesian Networks based approach (BNTCP), Bayesian Networks based approach with feedback (BNA) and code coverage based clustering approach. The experimental results showed that the proposed approach is promising.	angular defect;bayesian network;cluster analysis;code coverage;coverage data;fault coverage;fault detection and isolation;greedy algorithm;java;regression testing;software metric;software quality;test case	Xiaobin Zhao;Zan Wang;Xiangyu Fan;Zhenhua Wang	2015	2015 IEEE 39th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2015.154	reliability engineering;computer science;machine learning;data mining;cluster analysis;test management approach	SE	-60.96496702161842	35.43184131183549	63113
0efb1595b8722a3f257590752ec0eb14592b8b1e	move code refactoring with dynamic analysis	program diagnostics;refactoring;software maintenance;software maintenance refactoring dynamic analysis move method refactoring;system monitoring;data visualization software maintenance cloning image color analysis conferences performance analysis;cloning;image color analysis;data visualization;performance analysis;system monitoring program diagnostics software maintenance software quality;code quality dynamic analysis program source code refactoring static analysis runtime information dynamic dispatch method traces;move method refactoring;software quality;conferences;dynamic analysis	In order to reduce coupling and increase cohesion, we refactor program source code. Previous research efforts for suggesting candidates of such refactorings are based on static analysis, which obtains relations among classes or methods from source code. However, these approaches cannot obtain runtime information such as repetition count of loop, dynamic dispatch and actual execution path. Therefore, previous approaches might miss some refactoring opportunities. To tackle this problem, we propose a technique to find refactoring candidates by analyzing method traces. We have implemented a prototype tool based on the proposed technique and evaluated the technique on two software systems. As a result, we confirmed that the proposed technique could detect some refactoring candidates, which increase code quality.	code refactoring;dynamic dispatch;dynamic program analysis;open-source software;prototype;run time (program lifecycle phase);software quality;software system;static program analysis;test case;tracing (software);xojo	Shuhei Kimura;Yoshiki Higo;Hiroshi Igaki;Shinji Kusumoto	2012	2012 28th IEEE International Conference on Software Maintenance (ICSM)	10.1109/ICSM.2012.6405324	system monitoring;real-time computing;computer science;engineering;software engineering;cloning;dynamic program analysis;programming language;software maintenance;data visualization;code refactoring;software quality	SE	-58.107186946538775	35.80059405738789	63305
4e22d9dde2073868e0f0007dc17344ac84aaf1b0	trust-based fusion of classifiers for static code analysis	static code analysis classifer fusion trust based fusion alert classification industrial case study;trust based classifier fusion method static code analysis automatic alert generation software fault software failure software development false positives alert inspection alert ranking alert classification critical fault reporting likelihood machine learning technique artifact characteristics digital tv software;accuracy inspection digital tv mathematical model middleware software systems;system recovery digital television learning artificial intelligence multimedia systems pattern classification program diagnostics sensor fusion	Static code analysis tools automatically generate alerts for potential software faults that can lead to failures. However, developers are usually exposed to a large number of alerts. Moreover, some of these alerts are subject to false positives and there is a lack of resources to inspect all the alerts manually. To address this problem, numerous approaches have been proposed for automatically ranking or classifying the alerts based on their likelihood of reporting a critical fault. One of the promising approaches is the application of machine learning techniques to classify alerts based on a set of artifact characteristics. The effectiveness of many different classifiers and artifact characteristics have been evaluated for this application domain. However, the effectiveness of classifier fusion methods have not been investigated yet. In this work, we evaluate several existing classifier fusion approaches in the context of an industrial case study to classify the alerts generated for a digital TV software. In addition, we employ a trust-based classifier fusion method. We observed that our approach can increase the accuracy of classification by up to 4%.	application domain;benchmark (computing);list of tools for static code analysis;machine learning;naive bayes classifier;oracle fusion middleware;software project management;static program analysis	Ulas Yuksel;Hasan Sözer;Murat Sensoy	2014	17th International Conference on Information Fusion (FUSION)		real-time computing;computer science;data mining;computer security	SE	-62.07199844092065	38.026707886692456	63361
b6b9530c6709390d9a6e47223aee2a8bd31aaa08	a topology-shape-metrics approach for the automatic layout of uml class diagram	class diagram;graph drawing;object oriented software;uml diagrams;uml class diagram;reverse engineering	Class diagrams are among the most popular visualizations for object oriented software systems and have a broad range of applications. In many settings it is desirable that the placement of the diagram elements is determined automatically, especially when the diagrams are generated automatically which is usually the case in reverse engineering. For this reason the automatic layout of class diagram gained importance in the last years. Current approaches for the automatic layout of class diagrams are based on the hierarchic graph drawing paradigm. These algorithms produce good results for class diagrams with large and deep structural information, i.e., diagrams with a large and deep inheritance hierarchy. However, they do not perform satisfactorily in absence of this information. We propose in this work a new algorithm for automatic layout of class diagram which is based on the topology-shape-metrics approach. The algorithm is an adaption of sophisticated graph drawing algorithms which have proven their effectiveness in many applications. The algorithm works as well for class diagrams with rich structural information as for class diagrams with few or no structural information. It improves therefore the existing algorithms significantly. An implementation of the algorithm is used in the reverse engineering tool JarInspector.	algorithm;automatic layout;class diagram;graph drawing;programming paradigm;reverse engineering;software system;unified modeling language	Markus Eiglsperger;Michael Kaufmann;Martin Siebenhaller	2003		10.1145/774833.774860	block diagram;communication diagram;flowchart;interaction overview diagram;computer science;theoretical computer science;class diagram;story-driven modeling	SE	-55.806830186482095	34.33560290004613	63451
4328db7644799bc35f722d8a03bbedcb21660d69	a joinpoint coverage measurement tool for evaluating the effectiveness of test inputs for aspectj programs	java bytecode;aspectj;measurement tool;instruments;test input generation aspectj aspect oriented programming java bytecode joinpoints test coverage;software performance evaluation java object oriented programming program testing;java software testing software reliability reliability engineering software measurement computer science software systems costs automatic testing libraries;software performance evaluation;aspectj programs;testing;object oriented programming;test input generation;joinpoints;program testing;aspect oriented programming;test coverage;control flow;xml;java bytecode joinpoint coverage measurement tool aspectj programs aspect oriented programs program testing coverage metrics;driver circuits;source code;weaving;aspect oriented programs;joinpoint coverage measurement tool;benchmark testing;coverage metrics;java	Testing aspect-oriented programs is challenging in part because of the interactions between the aspects and the base classes with which the aspects are woven. Coverage metrics, such as joinpoint coverage, address faults resulting from the changes in base class control flow that may be introduced by the woven advices. Definitions of joinpoint coverage in the literature typically require counting the execution of aspects at each joinpoint. We present a tool for measuring joinpoint coverage from two perspectives: per advice, which measures the execution of the advice at each joinpoint it is woven into, and per class, which measures the execution of all the advices in each joinpoint in the class. This gives a more detailed measurement of joinpoint coverage and helps in identifying what more needs to be tested in both the base class and the aspect. The tool is based on AspectJ and Java bytecode, and thus, does not require any source code. We demonstrate the use of our tool to measure the joinpoint coverage of test inputs generated by Xie and Zhao's Aspectra framework.	aspect-oriented software development;aspectj;benchmark (computing);control flow;dataflow;fault coverage;fault detection and isolation;interaction;java bytecode;x image extension	Fadi Wedyan;Sudipto Ghosh	2008	2008 19th International Symposium on Software Reliability Engineering (ISSRE)	10.1109/ISSRE.2008.12	reliability engineering;benchmark;real-time computing;xml;aspect-oriented programming;computer science;software testing;code coverage;programming language;object-oriented programming;control flow;java;weaving;source code	SE	-60.514883630858364	35.45336748826914	63917
d8ec5f4a408210e7a44f0cfb9634e521ea819f40	heuristic search with reachability tests for automated generation of test programs	test program generator;heuristic search;single test program;test programs;effective heuristic search algorithm;reachability test;test program;test result;test execution;test validation;automated generation;reachability tests;test case;depth first search;software verification and validation;partial order;software testing;object oriented;test cases	Our research complements the current research on automated specification-based testing by proposing a scheme that combines the setup process, test execution, and test validation into a single test program for testing the behavior of object-oriented classes. The test program can be generated automatically given the desired test cases and closed algebraic specifications of the classes. The core of the test program generator is a partial-order planner which plans the sequence of instructions required in the test program. A first cut implementation of the planner has been presented by Leow et al. (2004) based on simple depth-first search. This paper presents a more efficient and effective heuristic search algorithm that performs reachability tests using the Omega calculator. Test results show that heuristic search with reachability tests significantly reduce the search time required to generate a valid sequence of instructions	characterization test;depth-first search;disc filing system;heuristic;linear algebra;modifier key;omega;reachability;search algorithm;test case	Wee Kheng Leow;Siau-Cheng Khoo;Tiong Hoe Loh;Vivy Suhendra	2004	Proceedings. 19th International Conference on Automated Software Engineering, 2004.	10.1109/ASE.2004.10024	partially ordered set;verification and validation;heuristic;breadth-first search;computer science;theoretical computer science;software engineering;test validity;test suite;software testing;programming language;test script;object-oriented programming;test case;test management approach;algorithm	SE	-58.24637038928705	36.74063195363721	64032
33fa0334d9943859536f32523758c3bfec7de55f	multi-agent based softswitch	telecommunication switching;complicated system;main function;mabs prototype;protocols;ethernet;different scale;multiagent based softswitch;softswitch system;best efficiency;multi-agent systems;telecommunication services;next generation network;telecommunication computing;communication protocol;cost ratio;good performance;telecom service provider;large enterprise;ideal softswitch system;local area networks;multi agent systems;service provider	Softswitch is the most complicated system in next generation networks. An ideal softswitch system can produce the best efficiency/cost ratio for telecom service provider of different scale and large enterprise. It is also easy to be extended and upgraded. Multi-agent based softswitch (MABS) is given to reach the target. The architecture of MABS is presented. The cooperation of agents in MABS is described. The protocol of communication among agents over Ethernet is designed as well. An implementation of MABS prototype is used to verify the design of this paper. The results of experiment verified the main functions of MABS and it shows good performance.	agent-based model;next-generation network;prototype;softswitch	Xiantai Gou;Weidong Jin;Duo Zhao	2004	Proceedings. IEEE/WIC/ACM International Conference on Intelligent Agent Technology, 2004. (IAT 2004).	10.1109/IAT.2004.1342966	embedded system;business;computer security;computer network	Robotics	-48.603952016048844	45.73167756420006	64033
4a198242643b793ac238ce2826cf14420f3444d0	accelerating the discovery of data quality rules: a case study		Poor quality data is a growing and costly problem that affects many enterprises across all aspects of their business ranging from operational efficiency to revenue protection. In this paper, we present an application – Data Quality Rules Accelerator (DQRA) – that accelerates Data Quality (DQ) efforts (e.g. data profiling and cleansing) by automatically discovering DQ rules for detecting inconsistencies in data. We then present two evaluations. The first evaluation compares DQRA to existing solutions; and shows that DQRA either outperformed or achieved performance comparable with these solutions on metrics such as precision, recall, and runtime. The second evaluation is a case study where DQRA was piloted at a large utilities company to improve data quality as part of a legacy migration effort. DQRA was able to discover rules that detected data inconsistencies directly impacting revenue and operational efficiency. Moreover, DQRA was able to significantly reduce the amount of effort required to develop these rules compared to the state of the practice. Finally, we describe ongoing efforts to deploy DQRA.	data quality;electronic billing;sensor	Peter Z. Yeh;Colin A. Puri;Mark Wagman;Ajay K. Easo	2011			data quality;data mining;computer security	SE	-53.440662207301436	43.39584790584669	64320
1e44393f75bddfab6cf42fd85813da51ceba8102	comprehensive multiplatform dynamic program analysis for java and android	libraries;dynamic programming;dynamic programming java performance analysis software development androids humanoid robots virtual machining;virtual machines dynamic program analysis java android software engineering software development shadowvm;instruments;androids;virtual machining;android;shadowvm;software engineering android operating system java program debugging program diagnostics;software engineering;dynamic program analysis;humanoid robots;shadowvm multiplatform dynamic program analysis java android program profiling program tracing bug finding tool software engineering;virtual machines;software development;performance analysis;java	Dynamic program analysis, such as with profiling, tracing, and bug-finding tools, is essential for software engineering. Unfortunately, implementing dynamic analysis for managed languages such as Java is unduly difficult and error prone because the runtime environments provide only complex low-level mechanisms. Programmers writing custom tooling must expend great effort in tool development and maintenance, while still suffering substantial limitations such as incomplete code coverage or lack of portability. Ideally, programmers should have a framework that lets them express dynamic-analysis tools at a high level, robustly, with high coverage and supporting alternative runtimes such as Android. To satisfy these requirements, ShadowVM, an all-in-one dynamic-program-analysis framework, uses a combination of techniques.	android;code coverage;cognitive dimensions of notations;dynamic program analysis;high- and low-level;high-level programming language;java;multi-function printer;profiling (computer programming);programmer;requirement;runtime system;software engineering	Yudi Zheng;Stephen Kell;Lubomír Bulej;Haiyang Sun;Walter Binder	2016	IEEE Software	10.1109/MS.2015.151	real-time computing;computer science;software development;operating system;software engineering;dynamic programming;dynamic program analysis;programming language;java;android	SE	-57.21663326714996	37.15784426337343	64323
0dfa188fbd9df2dd10a3a1321e03e85d6039f911	automatic recovery from runtime failures	object-oriented programming;software libraries;software maintenance;software reusability;system recovery;automatic recovery;faulty application maintenance;field failures;intrinsic redundancy;reusable component libraries;runtime failures;workaround identification	We present a technique to make applications resilient to failures. This technique is intended to maintain a faulty application functional in the field while the developers work on permanent and radical fixes. We target field failures in applications built on reusable components. In particular, the technique exploits the intrinsic redundancy of those components by identifying workarounds consisting of alternative uses of the faulty components that avoid the failure. The technique is currently implemented for Java applications but makes little or no assumptions about the nature of the application, and works without interrupting the execution flow of the application and without restarting its components. We demonstrate and evaluate this technique on four mid-size applications and two popular libraries of reusable components affected by real and seeded faults. In these cases the technique is effective, maintaining the application fully functional with between 19% and 48% of the failure-causing faults, depending on the application. The experiments also show that the technique incurs an acceptable runtime overhead in all cases.	experiment;general-purpose modeling;interrupt;java;library (computing);overhead (computing);redundancy (information theory);refinement (computing);run time (program lifecycle phase);software bug;software system;triple modular redundancy;usability;workaround	Antonio Carzaniga;Alessandra Gorla;Andrea Mattavelli;Nicolò Perino;Mauro Pezzè	2013	2013 35th International Conference on Software Engineering (ICSE)		reliability engineering;embedded system;real-time computing;computer science;engineering;software framework;component-based software engineering;software development;operating system;software engineering;software construction;programming language;object-oriented programming;software maintenance	SE	-53.399868935472725	40.88318381366161	64324
7453c6d6d8ca1faf9938c0a1dd26ce55253218c7	using coverage criteria on repok to reduce bounded-exhaustive test suites	test input;different valid test input;coverage criterion;representation invariant;test case candidate;present case study;test generation;bounded-exhaustive test suite;test suite;repok code;test case	Bounded-exhaustive exploration of test case candidates is a commonly employed approach for test generation in some contexts. Even when small bounds are used for test generation, executing the obtained tests may become prohibitive, despite the time for test generation not being prohibitive. In this paper, we propose a technique for reducing the size of bounded-exhaustive test suites. This technique is based on the application of coverage criteria on the representation invariant of the structure for which the suite was produced. More precisely, the representation invariant (which is often implemented as a repOK routine) is executed to determine how its code is exercised by (valid) test inputs. Different valid test inputs are deemed equivalent if they exercise the repOK code in a similar way according to a white-box testing criterion. These equivalences between test cases are exploited for reducing test suites by removing from the suite those tests that are equivalent to some test already present in the suite. We present case studies that evaluate the effectiveness of our technique. The results show that by reducing the size of bounded-exhaustive test suites up to two orders of magnitude, we obtain test suites whose efficacy measured as their mutant-killing ability is comparable to that of bounded-exhaustive test suites.	black box;code coverage;code refactoring;common criteria;compiler;data structure;design by contract;design rationale;eiffel;executable;experiment;imperative programming;java modeling language;test case;test suite;turing completeness;white box (software engineering);white-box testing	Valeria S. Bengolea;Nazareno Aguirre;Darko Marinov;Marcelo F. Frias	2012		10.1007/978-3-642-30473-6_4	simulation;automatic test pattern generation;test suite;test case;algorithm	SE	-58.951367578924064	36.941089607132355	64349
3286169f25e535faad1b15069709b0a0a145bf93	combined static and dynamic analysis for inferring program dependencies using a pattern language	control system;pattern language;static analysis tools;static analysis;static and dynamic analysis;dynamic analysis	One of the challenges when examining enterprise applications is the ability to understand the dependencies of these applications on external and internal resources such as database access or transaction activation. Inferring dependencies can be achieved using a static approach, a dynamic one or a combination of the two. Static analysis tools detect dependencies based on code investigation while dynamic tools detect dependencies based on runtime execution. The combination of these two approaches is essential for a complete and precise analysis. In this paper we present and illustrate a technique for inferring application dependencies on resources. The technique is based on a combined dynamic and static analysis. A pattern language is defined to enable the specification of dependencies as sequences of method invocations in the application code. Specifically, the sequences are patterns that constitute access to resources, e.g. databases, message queues, and control systems. We propose an algorithm for inferring application dependencies based on hybrid dynamic and static analysis that propagates information provided by dynamic analysis into the static analysis and back to the dynamic analysis. Empirical results from our implemented prototype are presented.	algorithm;control system;database;enterprise software;message queue;pattern language;prototype;static program analysis	Inbal Ronen;Nurit Dor;Sara Porat;Yael Dubinsky	2006		10.1145/1188966.1188970	real-time computing;dependency theory;computer science;control system;theoretical computer science;data mining;pattern language;dynamic program analysis;programming language;static analysis;static program analysis	SE	-54.02212929047531	40.408682079318865	64494
6961138fa7cfaeef61442eb2046a2b145f1445ff	supervisors for testing non-deterministically specified systems	automatic testing;automatic test equipment;system under test;electronic equipment testing;testing supervisor monitors legal behavior failure detection path detection module nondeterministically specified systems;telecommunication equipment testing;design and implementation;computational complexity;telecommunication equipment testing electronic equipment testing fault location automatic testing automatic test equipment computational complexity;system testing law legal factors software reliability laboratories computer displays telephony condition monitoring instruments system software;fault location	An approach to automate detection of behavioral failures during system testing is described. A supervisor monitors the inputs and outputs of a system under test. It reports discrepancies between observed and specified behaviors as failures. Failures due to both design and implementation are detectable. The approach presented is able to tolerate legal behavioral alternatives arising out of specification non-determinism.	failure;nondeterministic algorithm;system testing;system under test	Tony Savor;Rudolph E. Seviora	1997		10.1109/TEST.1997.639710	non-regression testing;test strategy;recovery testing;keyword-driven testing;reliability engineering;embedded system;automatic test equipment;black-box testing;real-time computing;orthogonal array testing;software performance testing;white-box testing;manual testing;integration testing;computer science;engineering;acceptance testing;software reliability testing;functional testing;smoke testing;software testing;real-time testing;system under test;computational complexity theory;system testing;test management approach;algorithm	SE	-48.93436440410263	36.51964872022533	64591
5f0806e999691dc61a013064b71f4c16bd41cb52	on extracting unit tests from interactive live programming sessions	interactive systems;program testing;software engineering;exploratory testing;interactive live programming sessions;live programming research direction;scripted testing;software engineering methodologies;unit testing;user-issued functions calls	Software engineering methodologies, such as unit testing, propose that any effort made to ensuring that programs run correctly should be captured in repeatable and automated artifacts. However, when looking at developer activities on a spectrum from exploratory testing to scripted testing we find that many engineering activities include bursts of exploratory testing. In this paper we propose to leverage these exploratory testing bursts by automatically extracting scripted tests from a recording of live programming sessions. In order to do so, we wiretap the development environment so we can record all program input, all user-issued functions calls, and all program output of an exploratory testing session. We propose to then use clustering to extract scripted test cases from these recordings. We outline two early-stage prototypes, one for a static and one for a dynamic language. And we outline how this idea fits into the bigger research direction of live programming.	cluster analysis;exploratory testing;fits;interactive programming;software engineering;test case;test script;unit testing	Adrian Kuhn	2013	2013 35th International Conference on Software Engineering (ICSE)		non-regression testing;test strategy;keyword-driven testing;exploratory testing;real-time computing;simulation;software performance testing;white-box testing;manual testing;system integration testing;computer science;engineering;acceptance testing;software reliability testing;software engineering;software construction;session-based testing;smoke testing;software testing;unit testing;programming language	SE	-58.811594344638635	34.986556653671684	65019
b0447dd6f74f11551c04c950fc34904a1c8a4cfd	an assembler and disassembler framework for java™ programmers	automated testing;unit testing;crossed product;source code;domain specificity;java language	The JavaTM programming language is primarily used for platform-independent programming. Yet it also offers many productivity, maintainability and performance benefits for platform-specific functions, such as the generation of machine code. We have created reliable assemblers for SPARCTM , AMD64, IA32 and PowerPC which support all user mode and privileged instructions and with 64 bit mode support for all but the latter. These assemblers are generated as Java source code by our extensible assembler framework, which itself is written in the Java language. The assembler generator also produces javadoc comments that precisely specify the legal values for each operand. Our design is based on the Klein Assembler System written in Self. Assemblers are generated from a specification, as are table-driven disassemblers and unit tests. The specifications that drive the generators are expressed as Java language objects. Thus no extra parsers are needed and developers do not need to learn any new syntax to extend the framework for additional ISAs. Every generated assembler is tested against a preexisting assembler by comparing the output of both. Each instruction’s test cases are derived from the cross product of its potential operand values. The majority of tests are positive (i.e., result in a legal instruction encoding). The framework also generates negative tests, which are expected to cause an error detection by an assembler. As with the Klein Assembler System, we have found bugs in the external assemblers as well as in ISA reference manuals. Our framework generates tens of millions of tests. For symbolic operands, our tests include all applicable predefined constants. For integral operands, the important boundary values, such as the respective minimum, maximum, 0, 1 and −1, are tested. Full testing can take hours to run but gives us a high degree of confidence regarding correctness. c © 2007 Sun Microsystems Inc. Published by Elsevier B.V. All rights reserved.	64-bit computing;assembly language;correctness (computer science);decision table;disassembler;error detection and correction;ia-32;java;javadoc;machine code;operand;parsing;platform-specific model;powerpc;programming language;self;software bug;test case;unit testing;user space;x86-64	Bernd Mathiske;Doug Simon;Dave Ungar	2008	Sci. Comput. Program.	10.1016/j.scico.2007.07.007	inline assembler;disassembler;computer science;theoretical computer science;unit testing;programming language;algorithm;assembly language;source code	PL	-56.1127659273124	39.89492360242112	65269
344ee12bca2f37b0c2ef2a2cbb1a7ecd98934423	ava: supporting debugging with failure interpretations	software;anomalies;debugging;log file debugging failure interpretation anomalies;anomalies failure interpretation debugging technique code fragment runtime event software development ava technique automatic debugging;system recovery program debugging;debugging production facilities software data models automata context classification algorithms;automata;system recovery;settore ing inf 05 sistemi di elaborazione delle informazioni;classification algorithms;production facilities;failure interpretation;program debugging;context;log file;data models	Several debugging techniques can be used to automatically identify the code fragments or the runtime events likely responsible of a failure. These techniques are useful, but can help reducing the debugging effort only to a given extent. In fact, even when these techniques are successful, software developers still have to invest a lot of effort in understanding if and why something detected as suspicious is really wrong. In this paper we present the tool implementing the AVA technique. AVA, compared to other approaches dedicated to automatic debugging, in addition to automatically identifying the events likely responsible of a failure, generates an explanation about why these events have been considered suspicious. This explanation can be used by developers to quickly discard imprecise outputs and more effectively work on the relevant anomalies.	ava radio company;application programming interface;data logger;debugging;download;eclipse;plug-in (computing);software developer	Fabrizio Pastore;Leonardo Mariani	2013	2013 IEEE Sixth International Conference on Software Testing, Verification and Validation	10.1109/ICST.2013.58	statistical classification;data modeling;real-time computing;computer science;operating system;database;automaton;programming language;debugging	SE	-61.61384296811708	40.03709497571045	65410
11f8c51cf54e0ea2dec517a3635771e4bb946fb4	characterizing data structures for volatile forensics	libraries;computer forensics;life cycle;evidence gathering process data structure characterization volatile memory forensic tools memory dumps deep analysis techniques latent program data life cycle latent program data recoverability cafegrind statistics forensic analysts;resource manager;resource management;browsers;arrays;resource management forensics fires libraries browsers arrays;real world application;data structures;statistics;statistics computer forensics data structures;expert knowledge;fires;data structure;forensics	Volatile memory forensic tools can extract valuable evidence from latent data structures present in memory dumps. However, current techniques are generally limited by a lack of understanding of the underlying data without the use of expert knowledge. In this paper, we characterize the nature of such evidence by using deep analysis techniques to better understand the life-cycle and recoverability of latent program data in memory. We have developed Cafegrind, a tool that can systematically build an object map and track the use of data structures as a program is running. Statistics collected by our tool can show which data structures are the most numerous, which structures are the most frequently accessed and provide summary statistics to guide forensic analysts in the evidence gathering process. As programs grow increasingly complex and numerous, the ability to pinpoint specific evidence in memory dumps will be increasingly helpful. Cafegrind has been tested on a number of real-world applications and we have shown that it can successfully map up to 96% of heap accesses.	algorithm;baseline (configuration management);compiler;data structure;debugging;peripheral;random-access memory;serializability;volatile memory	Ellick Chan;Shivaram Venkataraman;Nadia Tkach;Kevin Larson;Alejandro Gutierrez;Roy H. Campbell	2011	2011 Sixth IEEE International Workshop on Systematic Approaches to Digital Forensic Engineering	10.1109/SADFE.2011.5	engineering;data science;data mining;world wide web	SE	-60.95736394047857	42.721309496806974	65565
8ce9b1355408c4c14cbfac469e99f5b038154165	particle swarm based evolution and generation of test data using mutation testing		Adequate test data generation is a vital task involved in the process software testing. Process of mutation testing, a fault-based testing technique, generates mutants of the program under test (PUT) by applying mutation operators. These mutants can assist in finding test cases that have the potential to detect faults in the PUT. Particle Swarm Optimisation (PSO) share similar working characteristics with Genetic Algorithm (GA) which has already been applied to test data generation using mutation testing. In this paper, applicability of PSO for the generation of test data with mutation testing is explored. The results obtained by empirical evaluation of the proposed approach on benchmark C programs are presented. The evaluated results show that the test cases generated from the technique proposed kills substantial number of mutants and therefore, has a scope of exploring its performance in the area of search based test case generation.	mutation testing;swarm;test data	Nishtha Jatana;Bharti Suri;Sanjay Misra;Prateek Kumar;Amit Roy Choudhury	2016		10.1007/978-3-319-42092-9_44	mathematical optimization;genetic algorithm;operator (computer programming);machine learning;computer science;test case;mutation testing;software;test data generation;artificial intelligence;test data;particle swarm optimization	SE	-59.518779153285934	35.028766357926486	65895
c6da75353039b2c32cfa1f005cf18660c7de21d5	scoped: visualising the scope chain within source code		This paper presents an interactive visualisation tool that encodes the scope chain, and information related to the scope chain, within source code. The main goal of the tool is to support programmers when dealing with issues related to scope and to provide answers to questions such as to which scope does a specific variable or function belong to and can I access a specific variable from the scope I am currently located in. The design guidelines followed during the implementation of the tool, as well as the design rationale behind the main features of the tool are described. Finally, the results of a pilot user experience evaluation study are presented where an interesting observation was that the tool seemed to support programmers in verifying and correcting their assumptions when asked questions about specific scoping issues within a source code document.	design rationale;eurographics;game demo;interactive visualization;javascript syntax;positive feedback;programmer;scope (computer science);source code editor;usability;user experience evaluation;verification and validation	Ivan Bacher;Brian Mac Namee;John D. Kelleher	2017		10.2312/eurovisshort.20171143	software visualization;source code;visualization;theoretical computer science;computer architecture;computer science	SE	-53.776450047885504	35.24072396597582	65922
28028458d75bf9281200389a880741eb6d06a3a4	recovering software specifications with inductive logic programming	software systems;inductive logic programming;machine learning;software specification;off the shelf	We consider using machine learning techniques to help understand a large software system. In particular, we describe how learning techniques can be used to reconstruct abstract Datalog specifications of a certain type of database software from examples of its operation. In a case study involving a large (more than one million lines of C) real-world software system, we demonstrate that off-the-shelf inductive logic programming methods can be successfully used for specification recovery; specifically, Grende12 can extract specifications for about one-third of the modules in a test suite with high rates of precision and recall. We then describe two extensions to Grende12 which improve performance on this task: one which allows it to output a set of candidate hypotheses, and another which allows it to output specifications containing determinations. In combination, these extensions enable specifications to be extracted for nearly two-thirds of the benchmark modules with perfect recall, and precision of better than 60%.	benchmark (computing);database;datalog;inductive logic programming;machine learning;precision and recall;software system;test suite	William W. Cohen	1994			software requirements specification;verification and validation;real-time computing;computer science;design by contract;software design;theoretical computer science;software framework;software development;machine learning;software construction;inductive programming;programming language;software system	AI	-58.681726696428676	37.221618264491745	66226
0bcc2971dc03ffaf0b977917e46c4e0b7d1f4e45	aspect mining for large systems	software engineering;scaling up;aspect oriented programming;aspect mining;eclipse;cvs;mining version archives;formal concept analysis;reverse engineering;java	The Eclipse plugin HAM identifies potential aspects in large programs. It analyzes the program's history and obtains sets of function calls that are likely to be cross-cutting. Later during programming, HAM informs the programmer when she is about to extend or change such a problematic concern.	eclipse;hold-and-modify;institute for operations research and the management sciences;programmer	Silvia Breu;Thomas Zimmermann;Christian Lindig	2006		10.1145/1176617.1176651	eclipse;aspect-oriented programming;computer science;formal concept analysis;database;programming language;java;reverse engineering	PL	-56.811871812888995	35.913882118752134	66851
d51b25cc051ada48ff6758c9acfa480391d657d5	reasoning and improving on software resilience against unanticipated exceptions	exception;contract;resilience;exception handling;test suite	In software, there are the errors anticipated at specification and design time, those encountered at development and testing time, and those that happen in production mode yet never anticipated. In this paper, we aim at reasoning on the ability of software to correctly handle unanticipated exceptions. We propose an algorithm, called short-circuit testing, which injects exceptions during test suite execution so as to simulate unanticipated errors. This algorithm collects data that is used as input for verifying two formal exception contracts that capture two resilience properties. Our evaluation on 9 test suites, with 78% line coverage in average, analyzes 241 executed catch blocks, shows that 101 of them expose resilience properties and that 84 can be transformed to be more resilient.	algorithm;exception handling;program transformation;short-circuit evaluation;simulation;test suite;verification and validation	Benoit Cornu;Lionel Seinturier;Martin Monperrus	2013	CoRR		contract;exception handling;reliability engineering;real-time computing;computer science;test suite;programming language;computer security;psychological resilience	SE	-60.18551569503068	37.2914171356641	66949
35a98b5f26e5e97e1a3b0f1298632bc620a8eb4f	uncertainty-driven black-box test data generation	machine learning algorithms;uncertainty;software systems;testing;software engineering cs se;genetic programming;journal article;subspace constraints;data models	"""We can never be certain that a software system is correct simply by testing it, but with every additional successful test we become less uncertain about its correctness. In absence of source code or elaborate specifications and models, tests are usually generated or chosen randomly. However, rather than randomly choosing tests, it would be preferable to choose those tests that decrease our uncertainty about correctness the most. In order to guide test generation, we apply what is referred to in Machine Learning as """"Query Strategy Framework"""": We infer a behavioural model of the system under test and select those tests which the inferred model is """"least certain"""" about. Running these tests on the system under test thus directly targets those parts about which tests so far have failed to inform the model. We provide an implementation that uses a genetic programming engine for model inference in order to enable an uncertainty sampling technique known as """"query by committee"""", and evaluate it on eight subject systems from the Apache Commons Math framework and JodaTime. The results indicate that test generation using uncertainty sampling outperforms conventional and Adaptive Random Testing."""	algorithm;apache commons;approximation;baseline (configuration management);communications protocol;correctness (computer science);data structure;experiment;genetic programming;machine learning;mathematical optimization;mobile app;multi-objective optimization;random testing;randomness;sampling (signal processing);software engineering;software system;system under test;test case;test data generation	Neil Walkinshaw;Gordon Fraser	2017	2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)	10.1109/ICST.2017.30	genetic programming;data modeling;test data generation;uncertainty;computer science;engineering;theoretical computer science;software engineering;machine learning;data mining;dynamic testing;software testing;programming language;software system	SE	-61.25588482145927	35.032776945980885	66980
5fe13ec3d20913e8a0b3384a72eb389e2e29c7de	run-time monitoring of architecturally significant behaviors using behavioral profiles and aspects	aspectj;java programming;dynamic program;automatic generation;software architecture;monitoring;case tool;behavioral profile;dynamic behavior	Although static structures are often advocated as the main ingredient of a software architecture, dynamic program behavior forms an essential part of it. Verifying the behavior is a crucial yet often troublesome part of testing. Hence, it is of great concern to find means to facilitate the testing of dynamic behaviors. This paper studies one approach to such behavioral monitoring. The details of the approach are the following. We have used the concept of behavioral profiles to specify the desired program behavior using UML. Provided with a behavioral profile created with a CASE tool, we are able to automatically generate AspectJ aspects that are woven into Java program code, thus adding a monitoring concern to the system. This results in the opportunity to monitor architecturally significant behaviors defined with architectural profiles at code level. Towards the end of the paper, we study the applicability of the approach in industrial use.	aspectj;computer-aided software engineering;java;software architecture;unified modeling language	Kimmo Kiviluoma;Johannes Koskinen;Tommi Mikkonen	2006		10.1145/1146238.1146259	software architecture;real-time computing;simulation;systems engineering;engineering;software engineering	SE	-52.909763295729086	32.65175058598223	66986
616d3f7e831c725b51220a34fbee3ca6ac1d711c	a structured experiment of test-driven development	external validity;control group;computacion informatica;test driven development;java programming;unit testing;grupo de excelencia;software engineering;extreme programming;statistical analysis;agile methodologies;ciencias basicas y experimentales;software development;software industry;black box testing	Test Driven Development (TDD) is a software development practice in which unit test cases are incrementally written prior to code implementation. We ran a set of structured experiments with 24 professional pair programmers. One group developed a small Java program using TDD while the other (control group), used a waterfall-like approach. Experimental results, subject to external validity concerns, tend to indicate that TDD programmers produce higher quality code because they passed 18% more functional black-box test cases. However, the TDD programmers took 16% more time. Statistical analysis of the results showed that a moderate statistical correlation existed between time spent and the resulting quality. Lastly, the programmers in the control group often did not write the required automated test cases after completing their code. Hence it could be perceived that waterfall-like approaches do not encourage adequate testing. This intuitive observation supports the perception that TDD has the potential for increasing the level of unit testing in the software industry. q 2003 Elsevier B.V. All rights reserved.	black box;black-box testing;conformance testing;experiment;external validity;java;programmer;software development;software engineering;software industry;software quality;test automation;test case;test-driven development;unit testing	Boby George;Laurie A. Williams	2004	Information & Software Technology	10.1016/j.infsof.2003.09.011	test-driven development;test double;regression testing;real-time computing;simulation;extreme programming;external validity;white-box testing;computer science;systems engineering;engineering;software development;software engineering;software construction;agile software development;database;software testing;unit testing;programming language;test management approach;scientific control	SE	-61.83810251339411	35.25267351883683	68179
ce17d3b0cb01b297a66c70a237ae18b37e032cbd	automated training-set creation for software architecture traceability problem	architecture traceability;dataset generation;architecturally significant requirements;automation	Automated trace retrieval methods based on machine-learning algorithms can significantly reduce the cost and effort needed to create and maintain traceability links between requirements, architecture and source code. However, there is always an upfront cost to train such algorithms to detect relevant architectural information for each quality attribute in the code. In practice, training supervised or semi-supervised algorithms requires the expert to collect several files of architectural tactics that implement a quality requirement and train a learning method. Establishing such a training set can take weeks to months to complete. Furthermore, the effectiveness of this approach is largely dependent upon the knowledge of the expert. In this paper, we present three baseline approaches for the creation of training data. These approaches are (i) Manual Expert-Based, (ii) Automated Web-Mining, which generates training sets by automatically mining tactic’s APIs from technical programming websites, and lastly (iii) Automated Big-Data Analysis, which mines ultra-large scale code repositories to generate training sets. We compare the trace-link creation accuracy achieved using each of these three baseline approaches and discuss the costs and benefits associated with them. Additionally, in a separate study, we investigate the impact of training set size on the accuracy of recovering trace links. The results indicate that automated techniques can create a reliable training set for the problem of tracing architectural tactics.	algorithm;baseline (configuration management);machine learning;requirement;semi-supervised learning;semiconductor industry;software architecture;supervised learning;test set;traceability;web mining	Waleed Zogaan;Ibrahim Mujhid;Joanna C. S. Santos;Danielle Gonzalez;Mehdi Mirakhorli	2016	Empirical Software Engineering	10.1007/s10664-016-9476-y	simulation;computer science;systems engineering;engineering;software engineering;data mining	SE	-61.60932580455582	39.416325057383645	68704
b60b117502a0e8c8b9345a414c4fdbc441ba3b0a	language processing in program editors	debugging;programming environments;programming profession programming environments synthesizers debugging pressing software development management software maintenance software systems error analysis feedback;software maintenance;software systems;pressing;error analysis;feedback;language processing;programming profession;synthesizers;software development management	One of the most pressing practical problems facing computer 0 programmers is managing the development and maintenance of large software systems. This poses a challenge for computer scientists: How can computers best be applied to the softwaredevelopment process? An important aspect of meeting this challenge is to design and implement new interactive tools to aid programmers. Language-based programming environments show promise for enhancing the power of these tools. Recent work is directed toward effective use of specificlanguage knowledge in tools for programming in that language. Such knowledge can serve several purposes: * Knowledge of the language's syntax can be used to assess whether a program contains syntax errors and to determine where such errors occur. Syntactic analysis can be either local (to detect contextfree errors) or global (to detect contextsensitive errors). Analysis results displayed on the screen provide immediate feedback to the programmer developing or modifying the program. By performing syntactic analysis and type checking during editing rather than compilation, errors may be diagnosed more nearly at the time they occur. * Knowledge of a language's semantics can be used to generate and update code incrementally during editing. By incorMuch of the capacity of advanced hardware goes to waste because of inadequate software. Languagebased editors using immediate computation can put this wasted capacity to work.	compiler;computation;computer scientist;parsing;programmer;software system;syntax error;type system	Thomas W. Reps;Tim Teitelbaum	1987	Computer	10.1109/MC.1987.1663414	verification and validation;computing;real-time computing;extreme programming practices;search-based software engineering;computer science;package development process;backporting;software design;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;feedback;pressing;software walkthrough;programming language;resource-oriented architecture;debugging;software maintenance;system programming;software deployment;software development process;software quality;software system;software peer review	PL	-51.84463654188332	36.843718126937844	68870
4ee7b8f71004bc13021b2da97fb9ba4d3d063adc	incremental analysis of side effects for c software systems	software testing;global solution;software systems;software systems information analysis data analysis algorithm design and analysis flow graphs computer science educational institutions partitioning algorithms software testing debugging;fixed point iteration;semantic information;dataflow analysis;hybrid method;side effect;non structural;source code;profitability;dataflow analysis incremental analysis;incremental analysis;static analysis;data flow;hybrid algorithm	Incremental static analysis seeks to efficiently update semantic information about an evolving software system, without recomputing “from scratch.” Interprocedural modification side effect analysis (MOD) calculates the set of variables possibly modified by execution of a procedure or a statement. We introduce a partial incrementalization of MOD for C systems using the hybrid method and present results of a study of 27 C programs, that predicts that our incremental MOD analysis will be substantially cheaper than exhaustive analysis for many program changes.	software system;static program analysis	Jyh-Shiarn Yur;Barbara G. Ryder;William Landi;Phil Stocks	1997	Proceedings of the (19th) International Conference on Software Engineering	10.1145/253228.253369	fixed-point iteration;data flow diagram;mathematical optimization;hybrid algorithm;computer science;theoretical computer science;operating system;software engineering;software testing;programming language;engineering drawing;static analysis;side effect;profitability index;software system;source code	SE	-59.825542368963184	36.32498751413864	69184
62b6e249a276be60f93f927292fda1be4305ae51	how are functionally similar code clones syntactically different? an empirical study and a benchmark	empirische studie;empirical study;benchmark;analyse;codeklon;quellcode;empirie;code clone;funktional ahnlicher klon;article;functionally similar clone	Background. Today, redundancy in source code, so-called ‘‘clones’’ caused by copy&paste can be found reliably using clone detection tools. Redundancy can arise also independently, however, not caused by copy&paste. At present, it is not clear how only functionally similar clones (FSC) differ from clones created by copy&paste. Our aim is to understand and categorise the syntactical differences in FSCs that distinguish them from copy&paste clones in a way that helps clone detection research.#R##N#Methods. We conducted an experiment using known functionally similar programs in Java and C from coding contests. We analysed syntactic similarity with traditional detection tools and explored whether concolic clone detection can go beyond syntax. We ran all tools on 2,800 programs and manually categorised the differences in a random sample of 70 program pairs.#R##N#Results. We found no FSCs where complete files were syntactically similar. We could detect a syntactic similarity in a part of the files in <16% of the program pairs. Concolic detection found 1 of the FSCs. The differences between program pairs were in the categories algorithm, data structure, OO design, I/O and libraries. We selected 58 pairs for an openly accessible benchmark representing these categories.#R##N#Discussion. The majority of differences between functionally similar clones are beyond the capabilities of current clone detection approaches. Yet, our benchmark can help to drive further clone detection research.	benchmark (computing)	Stefan Wagner;Asim Abdulkhaleq;Ivan Bogicevic;Jan-Peter Ostberg;Jasmin Ramadani	2016	PeerJ Computer Science	10.7717/peerj-cs.49	benchmark;computer science;bioinformatics;data mining;empirical research;world wide web	Logic	-60.60280351884208	39.50480576525216	69396
faec3cfd79d4f517179a6762c04b2e7736bd9231	automatic feature learning for vulnerability prediction		Code flaws or vulnerabilities are prevalent in software systems and can potentially cause a variety of problems including deadlock, information loss, or system failure. A variety of approaches have been developed to try and detect the most likely locations of such code vulnerabilities in large code bases. Most of them rely on manually designing features (e.g. complexity metrics or frequencies of code tokens) that represent the characteristics of the code. However, all suffer from challenges in sufficiently capturing both semantic and syntactic representation of source code, an important capability for building accurate prediction models. In this paper, we describe a new approach, built upon the powerful deep learning Long Short Term Memory model, to automatically learn both semantic and syntactic features in code. Our evaluation on 18 Android applications demonstrates that the prediction power obtained from our learned features is equal or even superior to what is achieved by state of the art vulnerability prediction models: 3%–58% improvement for within-project prediction and 85% for cross-project prediction.	android;c++;component-based software engineering;deadlock;deep belief network;deep learning;end-to-end principle;feature learning;long short-term memory;numerical weather prediction;php;programming language;qr code;software metric;software system;web application	Khanh Hoa Dam;Truyen Tran;Trang Pham;Shien Wee Ng;John Grundy;Aditya K. Ghose	2017	CoRR		computer science;software system;deadlock;syntax;source code;deep learning;android (operating system);artificial intelligence;machine learning;vulnerability;feature learning	SE	-61.87951030389748	39.52227383373968	69471
8c12495d644c9dfbd82a0bf973862a6064bd89bb	refinement in object-sensitivity points-to analysis via slicing		"""Object sensitivity analysis is a well-known form of context-sensitive points-to analysis. This analysis is parameterized by a bound on the names of symbolic objects associated with each allocation site. In this paper, we propose a novel approach based on object sensitivity analysis that takes as input a set of client queries, and tries to answer them using an initial round of inexpensive object sensitivity analysis that uses a low object-name length bound at all allocation sites. For the queries that are answered unsatisfactorily, the approach then pin points """"bad"""" points-to facts, which are the ones that are responsible for the imprecision. It then employs a form of program slicing to identify allocation sites that are potentially causing these bad points-to facts to be generated. The approach then runs object sensitivity analysis once again, this time using longer names for just these allocation sites, with the objective of resolving the imprecision in this round. We describe our approach formally, prove its completeness, and describe a Datalog-based implementation of it on top of the Petablox framework. Our evaluation of our approach on a set of large Java benchmarks, using two separate clients, reveals that our approach is more precise than the baseline object sensitivity approach, by around 29% for one of the clients and by around 19% for the other client. Our approach is also more precise on most large benchmarks than a recently proposed approach that uses SAT solvers to identify allocation sites to refine."""	array slicing;baseline (configuration management);benchmark (computing);boolean satisfiability problem;context-sensitive grammar;datalog;information retrieval;interactivity;java;pointer analysis;program slicing;refinement (computing);scalability;sorting;time complexity	Girish Maskeri Rama;Raghavan Komondoor;Himanshu Sharma	2018	PACMPL	10.1145/3276512	theoretical computer science;computer science;slicing	PL	-57.024097423657516	39.827046544863016	69512
51668ca2a1b480d89136a3ef6a89ea04e7c8342a	maintaining unit tests during refactoring	structural alignment;refactoring;test driven development;ast rewrite;eclipse plug in;api coverage;unit tests;junit	The meaning of source code is often described by unit tests, as is for example the case in Test-Driven Software Development. Test-driven development is a principle in software engineering that requires developers to write tests for each method before implementing the method itself. This ensures that for (at least) all public methods tests exist. When performing a refactoring, existing code is changed or restructured according to a predefined scheme. After a refactoring is applied, the alignment between the structure of source code and corresponding unit tests can be broken.  In this paper we describe different ways in which refactorings can impact the API coverage of unit tests. We present our approach of tracking the modifications made by refactorings, analyzing their influence on the existing test suite and giving advice to developers on how to update the test suite to migrate it. For example, tests may need to be moved or new tests developed in case a refactoring introduced new public methods. Our approach is applicable to all refactorings. We conclude this paper by discussing the potential of the presented approach and of the preliminary tool support in the Eclipse IDE.	application programming interface;code refactoring;eclipse;software development;software engineering;test suite;test-driven development;unit testing	Harrie Passier;Lex Bijlsma;Christoph Bockisch	2016		10.1145/2972206.2972223	test-driven development;structural alignment;computer science;dynamic testing;unit testing;programming language;code refactoring	SE	-54.52183443809597	34.256347095181326	69719
8ae68ce30555415002217b6f590349b7c8c1090e	from harvesting to cultivating: transformation of a web collecting system into a robust curation environment	interoperable repositories;internal structure;collection lifecycle	"""Much has been written about the lifecycle of digital objects. This study is instead concerned with the lifecycle of collections and associated services. Online collection environments are built to fulfill specific collecting objectives and constraints. If a collection proves useful within its original hosting environment, it will often be necessary or desirable to move the collection to new environments, in order to support new forms of use and re-aggregation or extract resources from legacy data environments. Such a transformation can be extremely expensive, challenging and prone to error, especially if the collections include complex internal structures and services. When """"services make the repository"""" [1], moving raw data from one location to another will often not be sufficient. Digital curators can pre-empt costly and problematic system migration efforts by integrating collections into environments specifically designed to support long-term preservation, scalability and interoperability [2]. We report on an integration of content and functionality of a feature-rich collecting environment (ContextMiner) into a robust data curation environment (iRODS).  ContextMiner is a web-based service for building collections, through the execution and management of """"campaigns"""" (i.e. sets of associated queries and parameters to harvest content over time). As a part of the VidArch project, we have been using the ContextMiner framework and services for harvesting YouTube videos and associated contextual information on a variety of topics. In July 2008, we released a public beta of ContextMiner, allowing anyone to run similar crawls. There are now more than 100 users. The current implementation - based on a single MySQL database and associated code - has served its intended purposes very well, but it is not a scalable or sustainable basis for offering wide-scale collecting services in support of the diverse array of potential users and use cases.  iRODS (integrated Rule-Oriented Data System), is adaptive policy-driven data grid middleware, which addresses aspects of growth, evolution, openness, and closure - fundamental requirements for digital preservation [3]. iRODS currently scales to hundreds of millions of files, tens of thousands of users, and petabytes of data. It operates in a highly distributed environment with heterogeneous storage resources and allows for growth through federation. It supports evolution through the virtualization of the underlying technology and supports changing business requirements through customization of repository behaviors. It supports openness through a data type agnostic treatment of content. iRODS can be instrumented with policies that support the management of the lifecycle of digital assets and will serve as a unique platform to study repository integration. One key feature is the automation of policy enforcement across distributed data that have been organized into a shared collection. The coupling of other open repositories and iRODS can create greater efficiencies and new types of repository services.  We discuss various repository integration scenarios, their potential benefits, and implications for collection life cycles. The approaches co-locate metadata and content in varied ways and rely on efficiencies found in one repository only, or on the ability to combine policies in both spaces: (1) iRODS to ContexMiner data migration, (2) Policy-based data management for ContextMiner collections, and (3) Policy interchange between ContextMiner and iRODS collections."""	business requirements;case preservation;data curation;data system;digital asset;digital curation;federation (information technology);interoperability;middleware;mysql;openness;petabyte;requirement;scalability;software feature;software release life cycle;storage resource broker;system migration;web application	Christopher A. Lee;Richard Marciano;Chien-Yi Hou;Chirag Shah	2009		10.1145/1555400.1555494	computer science;data mining;database;world wide web	Web+IR	-50.550248806228716	40.9928439403583	69738
8b5998dbfa8472c020b2bf03699be1b67b0039f7	exploring the impact of context sensitivity on blended analysis	context benchmark testing algorithm design and analysis resource management heuristic algorithms sensitivity scalability;program diagnostics;web based applications;call graph;resource management;dynamic program;trees mathematics;software engineering;blended escape analysis;context sensitive calling context tree;sensitivity;blended escape analysis context sensitivity blended static dynamic program analysis framework intensive web based application context insensitive call graph context sensitive calling context tree context sensitive code pruning;technical report departmental;trees mathematics internet program diagnostics;internet;heuristic algorithms;context sensitive code pruning;scalability;blended static dynamic program analysis;context insensitive call graph;context;algorithm design and analysis;benchmark testing;context sensitivity;framework intensive web based application	This paper explores the use of context sensitivity both intra- and interprocedurally in a blended (static/dynamic) program analysis for identifying source of object churn in framework-intensive Web-based applications. Empirical experiments with an existing blended analysis algorithm [10] compare combinations of (i) use of a context-insensitive call graph with a context-sensitive calling context tree, and (ii) use (or not) of context-sensitive code pruning within methods. These experiments demonstrate achievable gains in scalability and performance in terms of several metrics designed for blended escape analysis, and report results in terms of object instances created, to allow more realistic conclusions from the data than were possible previously.	algorithm design;benchmark (computing);call graph;computational complexity theory;context-sensitive grammar;control flow;data structure;dataflow;decision problem;dynamic program analysis;enterprise javabeans;escape analysis;experiment;ibm notes;image scaling;instance (computer science);open collaboration;scalability;software propagation;static program analysis;web application	Marc Fisher;Bruno Dufour;Shrutarshi Basu;Barbara G. Ryder	2010	2010 IEEE International Conference on Software Maintenance	10.1109/ICSM.2010.5609695	call graph;algorithm design;benchmark;web application;real-time computing;scalability;the internet;simulation;sensitivity;computer science;resource management;theoretical computer science;programming language	SE	-58.85336401122836	41.84749473309746	69755
d5ae0a5f99125f71e797fddb3805f75d2cf52cd5	toward a defect prediction model of exception handling method call structures	mobile device;security vulnerabilities;software assurance;secure coding practices	The ability to predict where faults are likely to arise in the source code can help guide test plans, reduce effort and cost, narrow the test space, and improve software quality. Our preliminary results show that exception handling code can be more risky than normal code. Therefore, in order to support more efficient testing of exception handling code, this extended abstract proposes a framework to predict faults from annotated exception handling method call structures. This framework will generate annotated exception call graphs of the whole system and calculate property-based software engineering measurement values. The framework will then predict the high risk area of the system by applying statistical modeling techniques to perform fault prediction.	exception handling;method (computer programming);software bug;software engineering;software quality;statistical model;test plan	Puntitra Sawadpong	2014		10.1145/2638404.2638513	reliability engineering;computer science;data mining;computer security	SE	-62.56232871905434	34.78637848228744	69778
6dc846fc561669eb4d349aaa94f3c8bf5b7dad2a	the causes and effects of infeasible paths in computer programs	computer program;software testing;path selection;structural testing;numerical algorithm;computer program testing;predicate testing	An analysis is presented of infeasible paths found in the NAG library of numerical algorithms. The construction of program paths designed to maximise structural testing measures is shown to be impossible without taking infeasibilities into account. Methods for writing programs which do not contain infeasible paths are also discussed.	algorithm;nag numerical library;numerical analysis;white-box testing	David Hedley;Michael A. Hennell	1985			orthogonal array testing;white-box testing;computer science;engineering;theoretical computer science;software engineering;software testing;programming language;algorithm	SE	-59.41812329636807	35.819768704076495	69824
0c18747cbd5b5e9d6a5150b41e6cffb2806ebd32	the impact of equivalent, redundant and quasi mutants on database schema mutation analysis	mutation analysis;postgresql equivalent mutant redundant mutant quasimutant database schema mutation analysis relational database database quality relational schema testing quality assessment dbms independent abstract representation static analysis dbms independent representation mutation adequate test suite hypersql;sql program diagnostics relational databases;abstracts relational databases testing syntactics accuracy software;mutation analysis relational databases;relational databases	Since the relational database is an important component of real-world software and the schema plays a major role in ensuring the quality of the database, relational schema testing is essential. This paper presents methods for improving both the efficiency and accuracy of mutation analysis, an established method for assessing the quality of test cases for database schemas. Using a DBMS-independent abstract representation, the presented techniques automatically identify and remove mutants that are either equivalent to the original schema, redundant with respect to other mutants, or undesirable because they are only valid for certain database systems. Applying our techniques for ineffective mutant removal to a variety of schemas, many of which are from real-world sources like the U.S. Department of Agriculture and the Stack Overflow website, reveals that the presented static analysis of the DBMS-independent representation is multiple orders of magnitude faster than a DBMS-specific method. The results also show increased mutation scores in 75% of cases, with 44% of those uncovering a mutation-adequate test suite. Combining the presented techniques yields mean efficiency improvements of up to 33.7%, with averages across schemas of 1.6% and 11.8% for HyperSQL and PostgreSQL, respectively.	best, worst and average case;database schema;foreign key;hsqldb (hypersql database);mutation testing;postgresql;query language;relational database;sql;sensor;stack overflow;static program analysis;test case;test suite;turing completeness;two-phase commit protocol	Chris J. Wright;Gregory M. Kapfhammer;Phil McMinn	2014	2014 14th International Conference on Quality Software	10.1109/QSIC.2014.26	schema migration;data definition language;sql;information schema;nested set model;relational model;semi-structured model;relational database;computer science;theoretical computer science;database model;data mining;database;mutation testing;candidate key;view;database schema;object-relational impedance mismatch;database design	SE	-60.78763360153031	38.266209165700005	70129
178dd09a7bf2256166269433e3486148101ae9c3	fine-grained incremental learning and multi-feature tossing graphs to improve bug triaging	triaging accuracy improvement fine grained incremental learning multifeature tossing graphs bug triaging improvement software bug fixing bug tossing reduction machine learning based prediction single attribute tossing graph;software engineering;ranking function;incremental learning;machine learning;software engineering learning artificial intelligence program debugging;prediction accuracy;program debugging;learning artificial intelligence;cumulant;computer bugs accuracy training training data history software machine learning;competence development	Software bugs are inevitable and bug fixing is a difficult, expensive, and lengthy process. One of the primary reasons why bug fixing takes so long is the difficulty of accurately assigning a bug to the most competent developer for that bug kind or bug class. Assigning a bug to a potential developer, also known as bug triaging, is a labor-intensive, time-consuming and fault-prone process if done manually. Moreover, bugs frequently get reassigned to multiple developers before they are resolved, a process known as bug tossing. Researchers have proposed automated techniques to facilitate bug triaging and reduce bug tossing using machine learning-based prediction and tossing graphs. While these techniques achieve good prediction accuracy for triaging and reduce tossing paths, they are vulnerable to several issues: outdated training sets, inactive developers, and imprecise, single-attribute tossing graphs. In this paper we improve triaging accuracy and reduce tossing path lengths by employing several techniques such as refined classification using additional attributes and intra-fold updates during training, a precise ranking function for recommending potential tossees in tossing graphs, and multi-feature tossing graphs. We validate our approach on two large software projects, Mozilla and Eclipse, covering 856,259 bug reports and 21 cumulative years of development. We demonstrate that our techniques can achieve up to 83.62% prediction accuracy in bug triaging. Moreover, we reduce tossing path lengths to 1.5–2 tosses for most bugs, which represents a reduction of up to 86.31% compared to original tossing paths. Our improvements have the potential to significantly reduce the bug fixing effort, especially in the context of sizable projects with large numbers of testers and developers.	eclipse;machine learning;open-source software;ranking (information retrieval);software bug	Pamela Bhattacharya;Iulian Neamtiu	2010	2010 IEEE International Conference on Software Maintenance	10.1109/ICSM.2010.5609736	computer science;theoretical computer science;software engineering;machine learning;data mining;software regression;cumulant	SE	-62.267627089487355	39.04843931844479	70631
4e703511cb4ff2fe0fae6ff0b268dad7a06fc4d3	an interactive debugging tool for c based on dynamic slicing and dicing	dynamic slicing;program slicing;data structure	Static Program Slicing has gained wide recognition in both academic and practical arenas. Several debugging tools have been developed that utilize static program slicing. Dynamic slicing has also gained considerable popularity in recent years. Due to the several advantages of dynamic slicing over static slicing, the objective of this work was to develop a debugging tool for C programs, called C-Debug, using dynamic slicing and dicing techniques. This paper reports the design considerations of C-Debug and the data structures used in the implementation of C-Debug. Based on the usage experiments with the C-Debug debugging tool, limitations and possible future enhancements are also outlined.	data structure;debug;debugger;debugging;experiment;program slicing;static program analysis	Mansur H. Samadzadeh;Winai Wichaipanitch	1993		10.1145/170791.170798	program slicing;real-time computing;data structure;computer science;operating system;programming language	SE	-58.13167234232439	36.481902012345756	71140
4f6698d50678cd63c4839264e45a7429bc6949c1	towards testing the implementation of graph transformations	software testing;testing;graph transformation;test case generation;pattern matching;test generation;combinational circuit;fault model	We present a method for testing the implementation of graph transformation specifications focusing on test case generation for graph pattern matching. We propose an extensible fault model for the implementation of transformations based on common programmer faults and the technicalities of graph transformations. We integrate traditional hardware testing (combinational circuits) and software testing techniques (mutant generation) for generating test cases.	combinational logic;control flow;device under test;fault detection and isolation;fault model;graph rewriting;mathematical optimization;pattern matching;postcondition;programmer;software testing;star catalogue;test case;test set;usability	Andrea Darabos;András Pataricza;Dániel Varró	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.04.031	model-based testing;orthogonal array testing;white-box testing;computer science;theoretical computer science;software testing;programming language;algorithm	PL	-56.69237812240624	32.75140355853232	71380
c922b25fa0e19f921d4d9b23d0d236c3caf83c4a	an approach for experimentally evaluating effectiveness and efficiency of coverage criteria for software testing	coverage criteria;variabilidad;software testing;preuve programme;mutation operators;program proof;haute performance;test programa;analisis estadistico;test cubierta;branching;mutation analysis;eficacia test;program verification;coverage based testing;verificacion programa;test case generation;statistical analysis;generation test;ramificacion;test coverage;analyse statistique;code commande;control flow;prueba programa;control code;alto rendimiento;ramification;test generation;experimental evaluation;variability;test programme;verification programme;variabilite;program test;high performance;generacion prueba;couverture test;test efficiency;efficacite test;generic programming;codigo control	Experimental work in software testing has generally focused on evaluating the effectiveness and effort requirements of various coverage criteria. The important issue of testing efficiency has not been sufficiently addressed. In this paper, we describe an approach for comparing the effectiveness and efficiency of test coverage criteria using mutation analysis. For each coverage criterion under study, we generate multiple coverage-adequate minimal test suites for a test-program from a test-pool, which are then executed on a set of systematically generated program mutants to obtain the fault data. We demonstrate the applicability of the proposed approach by describing the results of an experiment comparing the three code-based testing criteria, namely, block coverage, branch coverage, and predicate coverage. Our results suggest that there is a trade-off between effectiveness and efficiency of a coverage criterion. Specifically, the predicate coverage criterion was found to be most effective but least efficient whereas the block coverage criterion was most efficient but least effective. We observed high variability in the performance of block test suites whereas branch and predicate test suites were relatively consistent. Overall results suggest that the branch coverage criterion performs consistently with good efficiency and effectiveness, and it appears to be the most viable option for code-based control flow testing.	bayesian information criterion;code coverage;control flow;eisenstein's criterion;experiment;fault coverage;fault detection and isolation;heart rate variability;java;monadic predicate calculus;mutation testing;procedural generation;requirement;software testing;spatial variability;test case;test suite	Atul Gupta;Pankaj Jalote	2007	International Journal on Software Tools for Technology Transfer	10.1007/s10009-007-0059-5	modified condition/decision coverage;simulation;branching;computer science;mutation testing;software testing;ramification;code coverage;programming language;generic programming;control flow;algorithm	SE	-60.360030152707374	33.89589936752774	71563
2b23caa76ff9708289122740c482e1685e2af376	analysis of faults in an n-version software experiment	tolerancia falta;developpement logiciel;software engineering fault tolerant computing program testing;computer languages;analisis estadistico;formal model;programming language;vax computers;application software;statistical independence;ingenieria logiciel;software engineering;failure analysis;large scale;fault tolerant computing;applications programs computers;aircraft propulsion;descripcion;statistical analysis;program testing;programming profession;desarrollo logicial;software development environment;fault tolerance;software development;analyse statistique;defaillance;computer systems programs;genie logiciel;statistical correlation;production;educational institutions application software computer science large scale systems fault tolerance aircraft propulsion programming profession computer languages software reliability production;failures;computer science;fiabilite logiciel;fiabilidad logicial;software development environment failure analysis fault location statistical correlation n version programming;description;pascal programming language;software reliability;fallo;multiversion;tolerance faute;n version programming;large scale systems;fault location	We have conducted a large-scale experiment in N-version programming. A total of 27 versions of a program were prepared independently from the same specification at two universities. The results of executing the versions revealed that the versions were individually extremely reliable but that the number of input cases in which more than one failed was substantially more than would be expected if they were statistically independent. After the versions had been executed, the failures of each version were examined and the associated faults located. In this paper we present an analysis of these faults. Our goal in undertaking this analysis was to understand better the nature of the faults. We found that in some cases the programmers made equivalent logical errors, indicating that some parts of the problem were simply more difficult than others. We also found cases in which apparently different logical errors yielded faults that caused statistically correlated failures, indicating that there a re special cases in the input space that present difficulty in various parts of the solution. A formal model is presented to explain this phenomenon. It appears that minor differences in the software development environment, such as the use of different programming languages for the different versions, would not have a major impact in reducing the incidence of faults that cause correlated failures.	incidence matrix;integrated development environment;logical framework;mathematical model;n-version programming;programmer;programming language;software bug;software development;strongly correlated material	Susan S. Brilliant;John C. Knight;Nancy G. Leveson	1990	IEEE Trans. Software Eng.	10.1109/32.44387	independence;reliability engineering;failure analysis;fault tolerance;application software;simulation;n-version programming;computer science;engineering;software development;operating system;software engineering;development environment;programming language;software quality;pascal	SE	-61.910902286881054	32.56714563972328	71694
e46ac00dfcdf0e7f4717c6e7439194687f271360	defending against the attack of the micro-clones	software;history;dsl;data mining;cloning;rapid turnaround times redundant code automated microclone removal automated microclone detection boa software mining infrastructure java repositories data set github repositories repository inactivity;source code software data mining java redundancy security of data software maintenance;security;cloning java software data mining dsl security history;java	Micro-clones are small pieces of redundant code, such as repeated subexpressions or statements. In this paper, we establish the considerations and value toward automated detection and removal of micro-clones at scale. We leverage the Boa software mining infrastructure to detect micro-clones in a data set containing 380,125 Java repositories, and yield thousands of instances where redundant code may be safely removed. By filtering our results to target popular Java projects on GitHub, we proceed to issue 43 pull requests that patch micro-clones. In summary, 95% of our patches to active GitHub repositories are merged rapidly (within 15 hours on average). Moreover, none of our patches were contested; they either constituted a real flaw, or have not been considered due to repository inactivity. Our results suggest that the detection and removal of micro-clones is valued by developers, can be automated at scale, and may be fixed with rapid turnaround times.	flaw hypothesis methodology;java;patch (computing);sensor;software mining;statement (computer science);supercomputer education research centre;systems engineering	Rijnard van Tonder;Claire Le Goues	2016	2016 IEEE 24th International Conference on Program Comprehension (ICPC)	10.1109/ICPC.2016.7503736	real-time computing;digital subscriber line;computer science;information security;operating system;software engineering;cloning;database;programming language;java;java annotation	SE	-60.28948010337387	39.78087803735901	71704
5cf87a3fac2b93340eded8c3c85b7c7abda9168b	dhtml accessibility checking based on static javascript analysis	javascript;static program analysis;dhtml accessibility	DHTML accessibility is being standardized by W3C, which provides metadata for UI widgets commonly implemented by HMTL and JavaScript. However it is difficult to check that webpages always have correct metadata according to the standards of DHTML accessibility since UI widgets can be updated by JavaScript programs. Thus we propose a technique for checking accessibility of UI widgets. In this check, we use static program analysis techniques so that we can check accessibility without executing a program. In addition, we developed a prototype system based on the proposed technique and applied it to a simple DHTML application.	accessibility;dojo toolkit;dynamic html;javascript;library (computing);prototype;static program analysis;user interface;verification and validation	Takaaki Tateishi;Hisashi Miyashita;Naoshi Tabuchi;Shin Saito;Kouichi Ono	2007		10.1007/978-3-540-73283-9_20	computer science;unobtrusive javascript;database;programming language;world wide web	SE	-53.95459113146464	38.09380024868239	71869
0acaf335348ba39fd989fb75b745279e59caf29c	test generation via dynamic symbolic execution for mutation testing	test inputs quality;software;mutation testing;wrapping;instruments;dynamic symbolic execution;instrumented metaprogram;testing;software engineering;connectors;structural testing;automatic structural testing tool dynamic symbolic execution mutation testing test inputs quality mutant killing ratio test generation techniques pexmutator instrumented metaprogram mutant killing constraints;symbolic execution;engines;program testing;syntactics;mutant killing constraints;testing engines instruments wrapping syntactics connectors software;mutant killing ratio;test generation;test generation techniques;software tools;technical report;software tools program testing software engineering;meta programming;pexmutator;automatic structural testing tool	Mutation testing has been used to assess and improve the quality of test inputs. Generating test inputs to achieve high mutant-killing ratios is important in mutation testing. However, existing test-generation techniques do not provide effective support for killing mutants in mutation testing. In this paper, we propose a general test-generation approach, called PexMutator, for mutation testing using Dynamic Symbolic Execution (DSE), a recent effective test-generation technique. Based on a set of transformation rules, PexMutator transforms a program under test to an instrumented meta-program that contains mutant-killing constraints. Then PexMutator uses DSE to generate test inputs for the meta-program. The mutant-killing constraints introduced via instrumentation guide DSE to generate test inputs to kill mutants automatically. We have implemented our approach as an extension for Pex, an automatic structural testing tool developed at Microsoft Research. Our preliminary experimental study shows that our approach is able to strongly kill more than 80% of all the mutants for the five studied subjects. In addition, PexMutator is able to outperform Pex, a state-of-the-art test-generation tool, in terms of strong mutant killing while achieving the same block coverage.	experiment;microsoft research;mutation testing;scalability;symbolic execution;test automation;white-box testing	Lingming Zhang;Tao Xie;Lu Zhang;Nikolai Tillmann;Jonathan de Halleux;Hong Mei	2010	2010 IEEE International Conference on Software Maintenance	10.1109/ICSM.2010.5609672	metaprogramming;reliability engineering;simulation;computer science;engineering;technical report;software engineering;mutation testing;software testing;algorithm	SE	-59.331308683079065	35.384582708828745	72021
4aae9e4fba739e4711b9433b196042df533e9757	nlp2code: code snippet content assist via natural language tasks		Developers increasingly take to the Internet for code snippets to integrate into their programs. To save developers the time required to switch from their development environments to a web browser in the quest for a suitable code snippet, we introduce NLP2Code, a content assist for code snippets. Unlike related tools, NLP2Code integrates directly into the source code editor and provides developers with a content assist feature to close the vocabulary gap between developers' needs and code snippet meta data. Our preliminary evaluation of NLP2Code shows that the majority of invocations lead to code snippets rated as helpful by users and that the tool is able to support a wide range of tasks.	algorithm;application programming interface;content assist;context switch;documentation;eclipse;internet;natural language;source code editor;stack overflow;vocabulary	Brock Angus Campbell;Christoph Treude	2017	2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2017.56	world wide web;computer science;the internet;snippet;content assist;source code;metadata;natural language;software;vocabulary	SE	-53.61947992007038	38.15938330190855	72046
2decf483e15809b0678ccfc582ea6d37074dc36e	on the use of specification-based assertions as test oracles	software testing;software cost reduction specification based assertion test oracles software testing software faults formal specification specification translation;software cost estimation;formal specification;test oracles;automatic testing;software cost reduction;evaluation method;formal specifications;software systems;software fault tolerance formal specification program testing program verification software cost estimation;software fault tolerance;program verification;specification translation;program testing;impedance matching;system testing;cost effectiveness;computer science;specification based assertion;software faults;automatic testing software testing costs fault diagnosis system testing computer science educational institutions formal specifications software systems impedance matching;test oracle;fault diagnosis	"""The """"oracle problem' is a well-known challenge for software testing. Without some means of automatically computing the correct answer for test cases, testers must instead compute the results by hand, or use a previous version of the software. In this paper, we investigate the feasibility of revealing software faults by augmenting the code with complete, specification-based assertions. Our evaluation method is to (1) develop a formal specification, (2) translate this specification into assertions, (3) inject or identify existing faults, and (4) for each version of the assertion-enhanced system containing a fault, execute it using a set of test inputs and check for assertion violations. Our goal is to determine whether specification-based assertions are a viable method of revealing faults, and to begin to assess the extent to which their cost-effectiveness can be improved. Our evaluation is based on two case studies involving real-world software systems. Our results indicate that specification-based assertions can effectively reveal faults, as long as they adversely affect the program state. We describe techniques that we used for translating high-level specifications into code-level assertions. We also discuss the costs associated with the approach, and potential techniques for reducing these costs"""	assertion (software development);formal specification;high- and low-level;software bug;software system;software testing;state (computer science);test case	David Coppit;Jennifer M. Haddox-Schatz	1996	29th Annual IEEE/NASA Software Engineering Workshop	10.1109/SEW.2005.33	reliability engineering;computer science;engineering;software engineering;formal specification;programming language	SE	-60.14747242831863	36.97301573518462	72060
6a0bdfb237d069c3efeb7f8c4e08c54e9dab1978	test coverage of impacted code elements for detecting refactoring faults: an exploratory study		Refactoring validation by testing is critical for quality in agile development. However, this activity may be misleading when a test suite is insufficiently robust for revealing faults. Particularly, refactoring faults can be tricky and difficult to detect. Coverage analysis is a standard practice to evaluate fault detection capability of test suites. However, there is usually a low correlation between coverage and fault detection. In this paper, we present an exploratory study on the use of coverage data of mostly impacted code elements to identify shortcomings in a test suite. We consider three real open source projects and their original test suites. The results show that a test suite not directly calling the refactored method and/or its callers increases the chance of missing the fault. Additional analysis of branch coverage on test cases shows that there are higher chances of detecting a refactoring fault when branch coverage is high. These results give evidence that a combination of impact analysis with branch coverage could be highly effective in detecting faults introduced by refactoring edits. Furthermore, we propose a statistic model that evidences the correlation of coverage over certain code elements and the suite’s capability of revealing	agile software development;code coverage;code refactoring;coverage data;fault detection and isolation;open-source software;sensor;test case;test suite	Everton L. G. Alves;Tiago Massoni;Patrícia Duarte de Lima Machado	2017	Journal of Systems and Software	10.1016/j.jss.2016.02.001	reliability engineering;real-time computing;fault coverage;engineering;computer security	SE	-62.080480522871056	35.723437932983764	72069
31220c4c49cde7c66f83ecda04e16df39acd5507	distributed environment integrating tools for software testing	automated testing;distributed system;mutation testing;software testing;integration testing;client server architecture;test automation;statistical analysis;distributed environment;object oriented;experimental evaluation;open source	  This work is devoted to problems of testing many programs using various testing tools of different origin. We present a distributed  system, called Tester, which manages and automates testing process. The system is based on client-server architecture and  integrates testing tools, including commercial tools, open-source and own applications. It uses repository for storing projects  to be tested and database with test results. Different performance issues concerning test automation are discussed and experimentally  evaluated. The system was used in mutation testing of C# programs using object-oriented mutation operators, in experiments  investigating the relation between line and assembly instruction coverage on the set of C++ programs, and statistic analysis  of different program characteristics.    		Anna Derezinska;Krzysztof Sarba	2008		10.1007/978-90-481-3660-5_93	non-regression testing;test strategy;keyword-driven testing;regression testing;software performance testing;manual testing;system integration testing;integration testing;computer science;acceptance testing;software reliability testing;software engineering;software construction;cloud testing;database;distributed computing;mutation testing;software testing;real-time testing;object-oriented programming;system testing;test management approach;client–server model;distributed computing environment	SE	-55.642140014028996	32.3171129576673	72573
7c7056f1ba9c27ee520e50c47e54124e0190f70c	dependence clusters in source code	software testing;systems;empirical study;partition method;effet echelle;computacion informatica;tranchage;methode empirique;scale effect;measurement;tool;software maintenance;reutilizacion;program comprehension;metodo empirico;empirical method;retroingenierie;program transformation;test data generation;paralelisacion;dependence;program verification;transformation programme;software engineering;effet dimensionnel;testability;reuse;analisis programa;maintenance logiciel;large scale;verificacion programa;slicing;transformacion programa;methode partition;slices;ciencias basicas y experimentales;size effect;efecto escala;parallelisation;support;parallelization;genie logiciel;chapeado;algorithms;java programs;source code;metodo particion;program analysis;efecto dimensional;analyse programme;program slicing;change impact analysis;grupo a;verification programme;ingenieria informatica;languages;ingeniera inversa;reutilisation;reverse engineering;evolution	A dependence cluster is a set of program statements, all of which are mutually inter-dependent. This article reports a large scale empirical study of dependence clusters in C program source code. The study reveals that large dependence clusters are surprisingly commonplace. Most of the 45 programs studied have clusters of dependence that consume more than 10% of the whole program. Some even have clusters consuming 80% or more. The widespread existence of clusters has implications for source code analyses such as program comprehension, software maintenance, software testing, reverse engineering, reuse, and parallelization.	computer cluster;parallel computing;program comprehension;reverse engineering;software maintenance;software testing	Mark Harman;David W. Binkley;Keith Brian Gallagher;Nicolas E. Gold;Jens Krinke	2009	ACM Trans. Program. Lang. Syst.	10.1145/1596527.1596528	parallel computing;computer science;operating system;empirical research;algorithm	PL	-55.76858493790999	41.43756137804377	72684
97ffef61d147f0535ac95e97180e200b0ff241a5	analysis of mutation operators for the python language		A mutation introduced into a source code of a dyn amically typed program can generate an incompetent mutant. Such a mut nt manifests a typerelated error that cannot be detected before the mu tant execution. To avoid this problem, a program mutation can be provided at runtime, or incompetent mutants should be automatically detected and eliminat ed. We showed that the latter solution can effectively be applied providing s elected mutation operators. This paper discusses mutation operators to be used for mutation testing of Python programs. Standard and object-oriented mutatio n operators were applied to the Python language. Python-related operators deali ng w th decorators and collection slices were proposed. The operators were im ple ented in MutPy, the tool for mutation testing of Python programs, and e xperimentally evaluated.	experiment;intermediate representation;mutation (genetic algorithm);mutation testing;overhead (computing);python;run time (program lifecycle phase);static program analysis;strong and weak typing;teenage mutant ninja turtles;type system;verification and validation	Anna Derezinska;Konrad Halas	2014		10.1007/978-3-319-07013-1_15	computer science;artificial intelligence;algorithm	SE	-58.176789153576465	36.98172734980058	72803
6d4f0764e6a5510cc789017db582d3ff4bf89492	dynaria: a tool for ajax web application comprehension	ajax application rich internet applications dynamic analysis program comprehension tool reverse engineering;user sessions;negative affect;ajax application;rich internet application;program comprehension;software techniques;run time behaviour;browsers;communication model;run time behaviour dynaria ajax web application comprehension rich internet applications web 2 0 heterogeneous technologies communication models software techniques software tools dynamic analysis user sessions;dynaria;heterogeneous technologies;java internet runtime user interfaces search engines us department of transportation delay communications technology software tools visualization;servers;internet;engines;user interfaces internet program visualisation software tools;ajax web application comprehension;feature extraction;user experience;communication models;unified modeling language;web 2 0;software tools;rich internet applications;user interfaces;program visualisation;program comprehension tool;dynamic analysis;reverse engineering	Thanks to Rich Internet Applications (RIAs) with their enhanced interactivity, responsiveness and dynamicity, the user experience in the Web 2.0 is becoming more and more appealing and user-friendly. At the same time, the dynamic nature of RIAs, and the heterogeneous technologies, frameworks, communication models used for implementing them negatively affect their analyzability and understandability, so that specific software techniques and tools are needed for supporting their comprehension. This paper presents DynaRIA, a tool for the comprehension of RIAs implemented in Ajax that is based on dynamic analysis and provides functionalities for recording and analyzing user sessions from several perspectives, and producing various types of abstractions and visualizations about the run-time behaviour of the application.	ajax (programming);interactivity;list comprehension;responsiveness;rich internet application;run time (program lifecycle phase);usability;user experience;web 2.0;web application;world wide web	Domenico Amalfitano;Anna Rita Fasolino;Armando Polcaro;Porfirio Tramontana	2010	2010 IEEE 18th International Conference on Program Comprehension	10.1109/ICPC.2010.16	user experience design;rich internet application;models of communication;computer science;operating system;database;multimedia;programming language;world wide web	SE	-55.17833371740473	32.71149665786541	72859
0e2860ebe5bf1fe3fadb17dfe0c4c6d08ab13014	a probabilistic analysis of the efficiency of automated software testing	software testing;systematics;color;input variables;systematics software testing software engineering input variables random variables color;random variables;random testing;software engineering;testing theory;testing theory partition testing random testing error based partitioning efficient testing;hybrid strategy probabilistic analysis automated software testing sampling strategies random testing systematic testing exponential curve;efficient testing;partition testing;error based partitioning;program testing probability	We study the relative efficiencies of the random and systematic approaches to automated software testing. Using a simple but realistic set of assumptions, we propose a general model for software testing and define sampling strategies for random (R) and systematic (S0) testing, where each sampling is associated with a sampling cost: 1 and c units of time, respectively. The two most important goals of software testing are: (i) achieving in minimal time a given degree of confidence x in a program's correctness and (ii) discovering a maximal number of errors within a given time bound n̂. For both (i) and (ii), we show that there exists a bound on c beyond which R performs better than S0 on the average. Moreover for (i), this bound depends asymptotically only on x. We also show that the efficiency of R can be fitted to the exponential curve. Using these results we design a hybrid strategy H that starts with R and switches to S0 when S0 is expected to discover more errors per unit time. In our experiments we find that H performs similarly or better than the most efficient of both and that S0 may need to be significantly faster than our bounds suggest to retain efficiency over R.	algorithmic efficiency;correctness (computer science);experiment;hybrid testing;maximal set;network switch;probabilistic analysis of algorithms;program analysis;r language;random testing;robustness (computer science);sampling (signal processing);scalability;simulation;software engineering;software testing;test automation;test case;time complexity;usability testing	Marcel Böhme;Soumya Paul	2016	IEEE Transactions on Software Engineering	10.1109/TSE.2015.2487274	random testing;random variable;software engineering;systematics;software testing;test theory;algorithm;statistics	SE	-61.38449870633141	34.01284951828903	72878
d77514a85df4f5e1875b8459f24543ff1fb788e0	linking source code to untangled change intents		Previous work [13] suggests that tangled changes (i.e., different change intents aggregated in one single commit message) could complicate tracing to different change tasks when developers manage software changes. Identifying links from changed source code to untangled change intents could help developers solve this problem. Manually identifying such links requires lots of experience and review efforts, however. Unfortunately, there is no automatic method that provides this capability. In this paper, we propose AutoCILink, which automatically identifies code to untangled change intent links with a pattern-based link identification system (AutoCILink-P) and a supervised learning-based link classification system (AutoCILink-ML). Evaluation results demonstrate the effectiveness of both systems: the pattern-based AutoCILink-P and the supervised learning-based AutoCILink-ML achieve average accuracy of 74.6% and 81.2%, respectively.	baseline (configuration management);experiment;f1 score;linked data;open-source software;supervised learning	Xiaoyu Liu;LiGuo Huang;Chuanyi Liu;Vincent Ng	2018	2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2018.00047	systems engineering;supervised learning;task analysis;software bug;computer science;source code;cognition;machine learning;commit;software;artificial intelligence;tracing	SE	-61.79524257430109	39.46109826349864	72885
c22d107ba06bf21b981871e0a363a5c1784f1a95	ide plugins for detecting input-validation vulnerabilities		Many vulnerabilities in products and systems could be avoided if better secure coding practices were in place. There exist a number of Integrated Development Environment (IDE) plugins which help developers check for security flaws while they code. In this work, we present a review of these plugins. We specifically focus on the plugins that detect input-validation-related vulnerabilities. We list salient features such as their supported IDEs, applicable languages and specific types of vulnerability checks. We believe this work synthesizes information useful for future research on IDE plugins for detecting input-validation-related vulnerabilities.	data validation;integrated development environment;plug-in (computing);secure coding;sensor;vulnerability (computing)	Aniqua Z. Baset;Tamara Denning	2017	2017 IEEE Security and Privacy Workshops (SPW)	10.1109/SPW.2017.37	documentation;computer science;computer security;database;plug-in;secure coding;data validation;development environment;vulnerability;java	Security	-56.3984641832988	37.772759177526005	73140
0859c2eedf4936649603f1b894dbd1c515553a79	error detection using path testing and static analysis	software testing;time measurement;materials testing;error correction;system testing;algorithms;software testing computer errors error correction materials testing system testing documentation algorithms time measurement;error detection;static analysis;computer errors;documentation	How many types of errors can be detected through static analysis and branch testing? How many man-hours and machine hours do these techniques require? Here are some empirically determined answers.	error detection and correction;model-based testing;static program analysis	Carolyn Gannon	1979	Computer	10.1109/MC.1979.1658851	non-regression testing;keyword-driven testing;regression testing;real-time computing;error detection and correction;orthogonal array testing;white-box testing;manual testing;computer science;software reliability testing;theoretical computer science;dynamic testing;software testing;statistics	SE	-61.9042902280288	33.94291712556906	73199
4af162d548b1a93cff9ac396dca255b7a360f181	online detection of operator errors in cloud computing using anti-patterns		IT services are subject of several maintenance operations like upgrades, reconfigurations or redeployments. Monitoring those changes is crucial to detect operator errors, which are a main source of service failures. Another challenge, which exacerbates operator errors is the increasing frequency of changes, e.g. because of continuous deployments like often performed in cloud computing. In this paper, we propose a monitoring approach to detect operator errors online in real-time by using complex event processing and anti-patterns. The basis of the monitoring approach is a novel business process modelling method, combining TOSCA and Petri nets. This model is used to derive pattern instances, which are input for a complex event processing engine in order to analyze them against the generated events of the monitored applications.		Arthur Vetter	2017		10.1007/978-3-030-11638-5_1	cloud computing;operator (computer programming);real-time computing;it service management;business process modeling;petri net;anti-pattern;anomaly detection;complex event processing;computer science	HPC	-52.99594582187351	42.0318284824095	73436
23c5259e64b01c1a63f71b0b69ea0f3aefbfb67e	a meta heuristic for effectively detecting concurrency errors	automatic control;concurrent programs;depth first search;static analysis tools;static analysis;point of interest	Mainstream programming is migrating to concurrent architectures to improve performance and facilitate more complex computation. The state of the art static analysis tools for detecting concurrency errors are imprecise, generate a large number of false error warnings, and require manual verification of each warning. In this paper we present a meta heuristic to help reduce the manual effort required in the verification of warnings generated by static analysis tools. We manually generate a small sequence of program locations that represent points of interest in checking the feasibility of a particular static analysis warning; then we use a meta heuristic to automatically control scheduling decisions in a model checker to guide the program along the input sequence to test the feasibility of the warning. The meta heuristic guides a greedy depth-first search based on a two-tier ranking system where the first tier considers the number of program locations already observed from the input sequence, and the second tier considers the perceived closeness to the next location in the input sequence. The error traces generated by this technique are real and require no further manual verification. We show the effectiveness of our approach by detecting feasible concurrency errors in benchmarked concurrent programs and the JDK 1.4 concurrent libraries based on warnings generated by the Jlint static analysis tool.	centrality;computation;concurrency (computer science);data dependency;depth-first search;greedy algorithm;heuristic;java development kit (jdk);library (computing);metaheuristic;model checking;multitier architecture;multiversion concurrency control;nondeterministic algorithm;point of interest;randomized algorithm;scheduling (computing);sensor;static program analysis;thread (computing);tracing (software)	Neha Rungta;Eric Mercer	2008		10.1007/978-3-642-01702-5_8	real-time computing;point of interest;breadth-first search;computer science;theoretical computer science;automatic control;data mining;programming language;static analysis;algorithm;static program analysis	SE	-58.81098725027976	38.00718636466291	73696
1be9fb6a39bb075066e4a43f333655e0d27da6be	quality cloudcrowd: a crowdsourcing platform for qos assessment of saas services		The adoption of Software as a Service (SaaS) has grown rapidly since 2010, and the need for Quality of Service (QoS) information is a significant factor in selecting a trustworthy SaaS service. In the existing literature, little attention has been given to providing QoS information with the SaaS service offering. SaaS providers offer a description of the overall QoS and service performance when they make their service offer; however service user satisfaction is a crucial factor in service selection decision-making. Crowd sourcing has grown in popularity over the last few years for performing tasks such as product design and consumer feedback, in particular, attracts the researchers in the field of client feedback on services or products. In this paper, we propose a novel framework for providing missing QoS values to the cloud marketplace called “Quality CloudCrowd”. Our proposed framework comprises of several parts; however, the development of the QCC platform for collecting missing QoS values is the core element of this structure and is the focus of this paper.		Asma Alkalbani;Farookh Khadeer Hussain	2017		10.1007/978-3-319-69835-9_22	business;quality of service;trustworthiness;internet privacy;cloud computing;computer network;software as a service;popularity;product design;crowdsourcing	Web+IR	-48.601073936422104	43.291028315115184	73718
a5e66d639d3b8cfb6755ae958dc4476d5d8e6614	source file set search for clone-and-own reuse analysis		Clone-and-own approach is a natural way of source code reuse for software developers. To assess how known bugs and security vulnerabilities of a cloned component affect an application, developers and security analysts need to identify an original version of the component and understand how the cloned component is different from the original one. Although developers may record the original version information in a version control system and/or directory names, such information is often either unavailable or incomplete. In this research, we propose a code search method that takes as input a set of source files and extracts all the components including similar files from a software ecosystem (i.e., a collection of existing versions of software packages). Our method employs an efficient file similarity computation using b-bit minwise hashing technique. We use an aggregated file similarity for ranking components. To evaluate the effectiveness of this tool, we analyzed 75 cloned components in Firefox and Android source code. The tool took about two hours to report the original components from 10 million files in Debian GNU/Linux packages. Recall of the top-five components in the extracted lists is 0.907, while recall of a baseline using SHA-1 file hash is 0.773, according to the ground truth recorded in the source code repositories.	android;baseline (configuration management);code reuse;computation;control system;cryptographic hash function;database;debian;directory (computing);firefox;gnu;ground truth;linux;organizational behavior;repository (version control);sha-1;software bug;software developer;software ecosystem;version control;vulnerability (computing)	Takashi Ishio;Yusuke Sakaguchi;Kaoru Ito;Katsuro Inoue	2017	2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)		computer science;data mining;database;programming language;world wide web	SE	-60.07577896732072	39.80748273948769	73764
a04d399c6e6dad46dfea49ee0e1a564fa6f5e570	mutant reduction based on dominance relation for weak mutation testing		Context: As a fault-based testing technique, mutation testing is effective at evaluating the quality of existing test suites. However, a large number of mutants result in the high computational cost in mutation testing. As a result, mutant reduction is of great importance to improve the efficiency of mutation testing. Objective: We aim to reduce mutants for weak mutation testing based on the dominance relation between mutant branches. Method: In our method, a new program is formed by inserting mutant branches into the original program. By analyzing the dominance relation between mutant branches in the new program, the nondominated one is obtained, and the mutant corresponding to the non-dominated mutant branch is the mutant after reduction. Results: The proposed method is applied to test ten benchmark programs and six classes from opensource projects. The experimental results show that our method reduces over 80% mutants on average, which greatly improves the efficiency of mutation testing. Conclusion: We conclude that dominance relation between mutant branches is very important and useful in reducing mutants for mutation testing. © 2016 Elsevier B.V. All rights reserved.	algorithmic efficiency;benchmark (computing);mutation testing;open-source software	Dun-Wei Gong;Gongjie Zhang;Xiangjuan Yao;Fanlin Meng	2017	Information & Software Technology	10.1016/j.infsof.2016.05.001	bioinformatics;algorithm	SE	-59.45201024438734	35.46440721290773	73768
a625b26fee4b262b6d5902d16805f2feb85c64a1	debugging meets testing in erlang		We propose a bidirectional collaboration between declarative debugging and testing for detecting errors in the sequential subset of the programming language Erlang. In our proposal, the information obtained from the user during a debugging session is stored in form of unit tests. These test cases can be employed afterwards to check, through testing, if the bug has been actually corrected. Moreover, the debugger employs already existing tests to determine the correctness of some subcomputations, helping the user to locate the error readily. The process, contrarily to usual debugger frameworks is cumulative: if later we find a new bug we have more information from the previous debugging and testing iterations that can contribute to find the error readily.	algorithmic program debugging;correctness (computer science);debugger;erlang (programming language);iteration;programming language;sensor;test case;unit testing	Salvador Tamarit;Adrián Riesco;Enrique Martin-Martin;Rafael Caballero	2016		10.1007/978-3-319-41135-4_10	parallel computing;operating system;programming language	SE	-59.83968278888955	37.023617971190596	73927
d0d54c5b9bbe6428cd197cc8c55840bb7fec729e	icon: inferring temporal constraints from natural language api descriptions	semantics;contracts;natural languages;syntactics;dictionaries;documentation;tagging	Temporal constraints of an Application Programming Interface (API) are the allowed sequences of method invocations in the API governing the secure and robust operation of client software using the API. These constraints are typically described informally in natural language API documents, and therefore are not amenable to existing constraint-checking tools. Manually identifying and writing formal temporal constraints from API documents can be prohibitively time-consuming and error-prone. To address this issue, we propose ICON: an approach based on Machine Learning (ML) and Natural Language Processing (NLP) for identifying and inferring formal temporal constraints. To evaluate our approach, we use ICON to infer and formalize temporal constraints from the Amazon S3 REST API, the PayPal Payment REST API, and the java.io package in the JDK API. Our results indicate that ICON can effectively identify temporal constraint sentences (from over 4000 human annotated API sentences) with the average 79.0% precision and 60.0% recall. Furthermore, our evaluation demonstrates that ICON achieves an accuracy of 70% in inferring 77 formal temporal constraints from these APIs.	amazon simple storage service;application programming interface;client (computing);cognitive dimensions of notations;documentation;java development kit (jdk);java platform, standard edition;machine learning;natural language processing;representational state transfer;weatherstar	Rahul Pandita;Kunal Taneja;Laurie A. Williams;Teresa Tung	2016	2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2016.59	documentation;computer science;data mining;database;semantics;programming language	SE	-57.496691120905595	42.406409942127446	73943
d54ae49265cd1838806a281abc0f59ca77870e00	a study to improving anomaly detection in a ts workflow	artifact analysis anomaly detection ts workflow temporal structured workflow sequence control structures and control structures xor control structures loop control structures;filtering transforms periodic structures system recovery algorithm design and analysis switches software;component;temporal structured workflow;temporal structured workflow anomalous behavior;anomalous behavior;artifact anomaly detection component;proceedings paper;artifact anomaly detection;security of data	The analysis for the behavior of the artifacts adopted in a workflow can help preventing the execution errors. A temporal (structured) workflow, TS Workflow, is described with the min and max execution time intervals for each process and modeled by control structures (Sequence, AND, XOR, Loop). In the past, the anomalous behaviors of artifacts in a workflow are redefined to simplify artifact analysis in a workflow. However, some anomalies detected in the previous work might not occur if the temporal factors are considered in each process, because their corresponding processes cannot run continuously or concurrently due to temporal factors. In this work, the analysis work is improved by filtering out above anomalies in a TS workflow. Our work also handles the loop structure that complicates the analysis work and was not concerned in conventional analysis works.	amd k5;algorithm;anomaly detection;artifact (software development);business process;control flow;definition;exclusive or;like button;maxima and minima;run time (program lifecycle phase);time complexity	Feng-Jian Wang;Thanh-Thuy Thi Nguyen;Tennyson Lu	2016	2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2016.228	real-time computing;computer science;data mining;component;database	SE	-52.50298727115314	40.56796609598044	73983
bc47bd94a162d45b10aee69d8a5a047492611161	on the analytical comparison of testing techniques	subdomain based testing;software testing;random testing;necessary and sufficient condition;majorization;failure rate	We introduce necessary and sufficient conditions for comparing the expected values of the number of failures caused by applications of software testing techniques. Our conditions are based only on the knowledge of a total or even a hierarchical order among the failure rates of the subdomains of a program's input domain. We also prove conditions for comparing the probability of causing at least one failure in three important special cases.	failure cause;software testing	Sandro Morasca;Stefano Serra Capizzano	2004		10.1145/1007512.1007533	random testing;reliability engineering;mathematical optimization;orthogonal array testing;white-box testing;engineering;software engineering;failure rate;functional testing;risk-based testing;software testing;stress testing	SE	-60.99045656734088	33.34218813807713	74125
43bd344c720e15565edb65e1a39b3b94f2d99396	adaptive regression testing strategy: an empirical study	software;analytical hierarchy process regression testing adaptive regression testing strategy test case prioritization;analytic hierarchy process;empirical study;regression testing;software process improvement;test case prioritization technique adaptive regression testing strategy software systems code modifications cost effective technique art strategy;costs and benefits;software systems;testing;test case prioritization;subspace constraints;program testing;fault detection;testing software mathematical model equations subspace constraints java fault detection;mathematical model;cost effectiveness;regression analysis;adaptive regression testing strategy;analytical hierarchy process;md junaid;java;computer science adaptive regression testing strategy an empirical study north dakota state university hyunsook do arafeen;software process improvement program testing regression analysis	When software systems evolve, different amounts and types of code modifications can be involved in different versions. These factors can affect the costs and benefits of regression testing techniques in different ways, and thus, there may be no single regression testing technique that is the most cost-effective technique to use on every version. To date, many regression testing techniques have been proposed, but no research has been done on the problem of helping practitioners systematically choose appropriate techniques on new versions as systems evolve. To address this problem, we propose adaptive regression testing (ART) strategies that attempt to identify the regression testing techniques that will be the most cost-effective for each regression testing session considering organization's situations and testing environment. To assess our approach, we conducted an experiment focusing on test case prioritization techniques. Our results show that prioritization techniques selected by our approach can be more cost-effective than those used by the control approaches.	heuristic (computer science);regression testing;sensor;software system;test case	Md. Junaid Arafeen;Hyunsook Do	2011	2011 IEEE 22nd International Symposium on Software Reliability Engineering	10.1109/ISSRE.2011.29	reliability engineering;econometrics;regression testing;analytic hierarchy process;white-box testing;computer science;software engineering;data mining;risk-based testing;statistics	SE	-62.56839084753275	33.380820336446355	74183
62172675b3280348a83977a4c33ba0c5ac3d22de	effective regression test case selection: a systematic literature review		Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results.  The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible.  There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage.	acm computing surveys;categorization;code coverage;control flow graph;digital single-lens reflex camera;experiment;fault detection and isolation;fuzzy logic;genetic algorithm;heuristic;industrial robot;input/output;java;mathematical optimization;model-based testing;program slicing;programming language;regression testing;run time (program lifecycle phase);software engineering;software testing;swarm;systematic review;test case;test suite;trusted computer system evaluation criteria	Rafaqut Kazmi;Dayang N. A. Jawawi;Radziah Mohamad;Imran Ghani	2017	ACM Comput. Surv.	10.1145/3057269	data mining;empirical research;regression testing;fault detection and isolation;computer science;cost measures;systematic review;java	SE	-61.13287541237404	34.80330983356746	74373
e96249c2ed5fa4cc6e848b867852cadd3fa9a7c2	a formalism to automate mapping from program features to code	set operations automated mapping program features execution traces set difference set intersection input set partitioning non invoking sets invoking sets feature syntax grammar feature names parse tree annotation;program diagnostics;software fault diagnosis;software maintenance;program comprehension;automatic programming;set theory;test cases;grammars;reverse engineering program diagnostics software maintenance automatic programming grammars set theory;feature;reverse engineering	How does one locate the segments of code that implement a particular feature? N. Wilde and M.C. Scully (WS) (1995) pioneered the use of execution traces to map program features to code. Using their technique to locate the implementation of a particular feature, a program is executed with two sets of inputs; one set invokes the feature of interest and the other set does not. Operations such as set-difference and set-intersection, amongst others, are then applied on the execution traces to obtain answers for various questions related to a feature and its implementation. Previous researchers have automated the tasks of computing the execution traces and performing operations on the execution traces. We present a formalism to automate the most time-consuming aspect of this approach for locating code, namely, the partitioning of the input sets into invoking and non-invoking sets. A collection of input sets is partitioned using feature syntax, a grammar of the program's input annotated with feature names. An input set is placed in the invoking set if and only if its parse tree is annotated with that feature. WS' technique solely applies set operations on the execution traces of inputs. In our technique, we also apply the set operations among the set of features used by these inputs. By doing so, we can precisely determine the features whose implementation is identified when applying the operations on the execution traces.		Jean-Christophe Deprez;Arun Lakhotia	2000		10.1109/WPC.2000.852481	feature;computer science;engineering;theoretical computer science;operating system;software engineering;programming language;software maintenance;test case;algorithm;reverse engineering;set theory	SE	-52.51972560122271	37.440026005326835	74637
4fa8060e461a2a694febb4d914885e18a993a948	an automated functional testing framework for context-aware applications		In the modern era of mobile computing, context-aware computing is an emerging paradigm due to its widespread applications. Context-aware applications are gaining increasing popularity in our daily lives since these applications can determine and react according to the situational context and help users to enhance usability experience. However, testing these applications is not straightforward since it poses several challenges, such as generating test data, designing context-coupled test cases, and so on. However, the testing process can be automated to a greater extent by employing model-based testing technique for context-aware applications. To achieve this goal, it is necessary to automate model transformation, test data generation, and test case execution processes. In this paper, we propose an approach for behavior modeling of context-aware application by extending the UML activity diagram. We also propose an automated model transformation approach to transform the development model, i.e., extended UML activity diagram into the testing model in the form of function nets. The objective of this paper is to automate the context-coupled test case generation and execution. We propose a functional testing framework for automated execution of keyword-based test cases. Our functional testing framework can reduce the testing time and cost, thus enabling the test engineers to execute more testing cycles to attain a higher degree of test coverage.	activity diagram;behavior model;context awareness;fault coverage;functional testing;mobile computing;model transformation;model-based testing;programming paradigm;test case;test data generation;test engineer;unified modeling language;usability	Aamir Mehmood Mirza;Muhammad Naeem Ahmed Khan	2018	IEEE Access	10.1109/ACCESS.2018.2865213	real-time computing;test case;code coverage;model transformation;test data generation;computer science;functional testing;usability;test data;distributed computing;context model	SE	-49.23448814410349	38.67632890408304	74868
523c20d904ca481680c0065dabd6708fc149bbcd	software system performance debugging with kernel events feature guidance	kernel;training;software systems;servers;distributed systems performance debugging system management black box monitoring kernel events monitoring;monitoring;feature extraction;monitoring kernel servers feature extraction training software systems	To diagnose performance problems in production systems, many OS kernel-level monitoring and analysis tools have been proposed. Using low level kernel events provides benefits in efficiency and transparency to monitor application software. On the other hand, such approaches miss application-specific semantic information which can be effective to differentiate the trace patterns from distinct application logic. This paper introduces new trace analysis techniques based on event features to improve kernel event based performance diagnosis tools. Our prototype, AppDiff, is based on two analysis features: system resource features convert kernel events to resource usage metrics, thereby enabling the detection of various performance anomalies in a unified way; program behavior features infer the application logic behind the low level events. By using these features and conditional probability, AppDiff can detect outliers and improve the diagnosis of application performance.	anomaly detection;black box;business logic;debugging;enterprise software;execution pattern;kernel (operating system);operating system;performance tuning;production system (computer science);prototype;software system;tracing (software)	Junghwan Rhee;Hui Zhang;Nipun Arora;Guofei Jiang;Kenji Yoshihira	2014	2014 IEEE Network Operations and Management Symposium (NOMS)	10.1109/NOMS.2014.6838353	kernel;real-time computing;feature extraction;computer science;operating system;data mining;server;software system	Embedded	-53.48006437358213	41.318134247160486	74912
17d9b889a04f0584b25ec2cd8be73b6cbd5376d2	towards the automated recovery of complex temporal api-usage patterns		Despite the many advantages, the use of external libraries through their APIs remains difficult because of the usage patterns and constraints that are hidden or not properly documented. Existing work provides different techniques to recover API usage patterns from client programs in order to help developers understand and use those libraries. However, most of these techniques produce basic patterns that generally do not involve temporal properties. In this paper, we discuss the problem of temporal usage patterns recovery and propose a genetic-programming algorithm to solve it. Our evaluation on different APIs shows that the proposed algorithm allows to derive non-trivial temporal usage patterns that are useful and generalizable to new API clients.	algorithm;application programming interface;genetic programming;library (computing)	Mohamed Aymen Saied;Houari A. Sahraoui;Edouard Batot;Michalis Famelis;Pierre-Olivier Talbot	2018		10.1145/3205455.3205622	artificial intelligence;machine learning;computer science;mutation testing	SE	-58.34595145732188	38.58777988535655	75298
23dfd3091a3fe25cbb400d58c9314562740d1be7	spectral debugging with weights and incremental ranking	software testing costs fault diagnosis software engineering australia programming computer bugs software debugging instruments automatic testing;software;debugging;software faults spectral debugging weights ranking incremental ranking;complexity theory;software fault diagnosis;weights;software fault tolerance;data mining;runtime;spectral debugging;incremental ranking software fault diagnosis spectral debugging weights;incremental ranking;weights ranking;software fault tolerance program debugging;program debugging;software faults;computer bugs;benchmark testing;fault diagnosis	Software faults can be diagnosed using program spectra. The program spectra considered here provide information about which statements are executed in each one of a set of test cases. This information is used to compute a value for each statement which indicates how likely it is to be buggy, and the statements are ranked according to these values. We present two improvements to this method. First, we associate varying weights with failed test cases --- test cases which execute fewer statements are given more weight and have more influence on the ranking. This generally improves diagnosis accuracy, with little additional cost. Second, the ranking is computed incrementally. After the top-ranked statement is identified, the weights are adjusted in order to compute the rest of the ranking. This further improves accuracy. The cost is more significant, but not prohibitive.	cryptanalysis of the lorenz cipher;debugging;software bug;test case	Lee Naish;Hua Jie Lee;Kotagiri Ramamohanarao	2009	2009 16th Asia-Pacific Software Engineering Conference	10.1109/APSEC.2009.32	reliability engineering;benchmark;real-time computing;software bug;computer science;theoretical computer science;programming language;debugging;software fault tolerance	SE	-61.00148896822405	35.77758261406032	75601
7a43ff05e5789a5f3b286dd4fed0835d53dcb063	"""review of """"geekonomics: the real cost of insecure software by david rice, """" addison-wesley, 2008, 362 pp, 0-321-47789-8 (hardback)"""	software testing;database testing;controlled experiment;sql testing;empirical validation	Controlled experiments are a powerful way to assess and compare the effectiveness of different techniques. In this paper we present the experimental results of the evaluation of the effectiveness of a structural test coverage criterion developed for SQL queries when used by a tester to guide the selection of database test cases. We describe a controlled experiment designed for comparing this criterion with other conventional criteria such as equivalence partitioning and boundary value analysis. The results show that 1) the use of the structural coverage allows the tester to develop more effective test cases, 2) the effectiveness is higher when considering the kind of faults that are more specifically related to SQL than other kinds of faults, and 3) the results give us some insight into how to improve the coverage criterion.		Will Tracz	2008	ACM SIGSOFT Software Engineering Notes	10.1145/1344452.1344464	computer science;engineering;software engineering;database;software testing;database testing	SE	-60.56202070672069	34.76233498958191	76239
5f101f64d0b70488554d506a215531cabead3437	refactoring meets spreadsheet formulas	microprocessors;text;refactoring;spreadsheet programs;software maintenance;euses spreadsheet corpus spreadsheet formulas spreadsheet programs professional software hardcoded constants duplicated expressions unnecessary complexity unsanitized input automated refactoring object oriented domain refbook microsoft excel;spreadsheets;computer architecture;computational complexity;microsoft excel;spreadsheet programs computational complexity software maintenance;computer architecture microprocessors conferences software maintenance spreadsheet programs productivity;productivity;conferences;end user;end user programming;code smells	The number of end-users who write spreadsheet programs is at least an order of magnitude larger than the number of trained programmers who write professional software. We studied a corpus of 3691 spreadsheets and we found that their formulas are riddled with the same smells that plague professional software: hardcoded constants, duplicated expressions, unnecessary complexity, and unsanitized input. These make spreadsheets difficult to read and expensive to maintain. Like automated refactoring in the object-oriented domain, spreadsheet refactoring can be transformative. In this paper we present seven refactorings for spreadsheet formulas implemented in RefBook, a plugin for Microsoft Excel. To evaluate the usefulness of RefBook, we employed three kinds of empirical methods. First, we conducted a User Survey with 28 Excel users to find out whether they preferred the refactored formulas. Second, we conducted a Controlled Experiment with the same 28 participants to measure their productivity when doing manual refactorings. Third, we performed a Retrospective Case Study on the EUSES Spreadsheet Corpus with 3691 spreadsheets to determine how often we could apply the refactorings supported by RefBook. The results show: (i) users prefer the improved quality of refactored formulas, (ii) RefBook is faster and more reliable than manual refactoring, and (iii) the refactorings are widely applicable. On average RefBook is able to apply the refactorings in less than half the time that users performed the refactorings manually. 92.54% of users introduced errors or new smells into the spreadsheet or were unable to complete the task.	capacitor plague;code refactoring;code smell;hard coding;programmer;programming productivity;spreadsheet;technical debt;text corpus	Sandro Badame;Danny Dig	2012	2012 28th IEEE International Conference on Software Maintenance (ICSM)	10.1109/ICSM.2012.6405299	productivity;end user;computer science;software engineering;database;programming language;computational complexity theory;software maintenance;code refactoring;code smell	SE	-52.26275173344183	36.454778178665116	76842
0ccd82071b0eaa3ee6c1d21807b98e4237923ff6	decor: a method for the specification and detection of code and design smells	phase detection;detection algorithms vocabulary domain specific languages algorithm design and analysis metamodeling java design engineering object oriented programming phase detection costs;vocabulaire;open source systems code specification code detection design smells decor detex antipatterns blob functional decomposition spaghetti code swiss army knife empirical validation domain specific language;formal specification;detection algorithms;design engineering;logiciel;antipatterns blob;couteau;vocabulary;specification;langage;open source systems;object oriented programming;detection;program verification;revocation populaire;design smells;decor;recherche;software quality formal specification program verification;services des bibliotheques;code specification;functional decomposition;uqam;antipatterns;empirical validation;sciences de l information;armee;technologie de l information;code detection;java antipatterns design smells code smells specification metamodeling detection;domain specific language;swiss army knife;metamodeling;detex;algorithm design and analysis;software quality;code informatique;spaghetti code;domain specific languages;java;code smells;sciences de l information et de la communication	Code and design smells are poor solutions to recurring implementation and design problems. They may hinder the evolution of a system by making it hard for software engineers to carry out changes. We propose three contributions to the research field related to code and design smells: (1) DECOR, a method that embodies and defines all the steps necessary for the specification and detection of code and design smells, (2) DETEX, a detection technique that instantiates this method, and (3) an empirical validation in terms of precision and recall of DETEX. The originality of DETEX stems from the ability for software engineers to specify smells at a high level of abstraction using a consistent vocabulary and domain-specific language for automatically generating detection algorithms. Using DETEX, we specify four well-known design smells: the antipatterns Blob, Functional Decomposition, Spaghetti Code, and Swiss Army Knife, and their 15 underlying code smells, and we automatically generate their detection algorithms. We apply and validate the detection algorithms in terms of precision and recall on XERCES v2.7.0, and discuss the precision of these algorithms on 11 open-source systems.	algorithm;apache xerces;code smell;design smell;domain-specific language;high-level programming language;open-source software;precision and recall;refactoring software, architectures, and projects in crisis;software engineer;spaghetti code;vocabulary	Naouel Moha;Yann-Gaël Guéhéneuc;Laurence Duchien;Anne-Françoise Le Meur	2010	IEEE Transactions on Software Engineering	10.1109/TSE.2009.50	computer science;domain-specific language;theoretical computer science;operating system;software engineering;programming language;algorithm	SE	-53.38485238616855	33.610853850296124	77122
be07e5a92c357b2917065d2095e5b187f341a986	data flow analysis and testing of java server pages	hypermedia markup languages;client server systems;html statement data flow analysis java server page testing server side script;internet;program testing;java server pages;hypermedia markup languages data flow analysis java program testing client server systems internet;data flow analysis;dynamic content;data flow;data analysis java software testing computer applications data flow computing application software;java	Web applications often rely on server-side scripts to handle HTTP requests, to generate dynamic contents, and to interact with other components. The server-side scripts usually mix with HTML statements and are difficult to understand and test. In particular, these scripts do not have any compiling check and could be error-prone. Thus, it becomes critical to test the server-side scripts for ensuring the quality and reliability of Web applications. We adapt traditional dataflow testing techniques into the context of Java Server Pages (JSP), a very popular server-side script for developing Web applications with Java technology. We point out that the JSP implicit objects and action tags can introduce several unique dataflow test artifacts which need to be addressed. A test model is presented to capture the dataflow information of JSP pages with considerations of various implicit objects and action tags. Based on the test model, we describe an approach to compute the intraprocedural and interprocedural data flow test paths for uncovering the data anomalies of JSP pages.	cognitive dimensions of notations;compiler;data-flow analysis;dataflow architecture;html;java;javaserver pages;server (computing);server-side scripting;web application	Chien-Hung Liu	2004	Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.	10.1109/CMPSAC.2004.1342689	data flow diagram;java api for xml-based rpc;scriptlet;the internet;jsr 94;computer science;operating system;dynamic web page;software engineering;data-flow analysis;database;real time java;programming language;java;world wide web;application server;generics in java;java applet;java annotation	SE	-57.36299445421047	41.77006197737526	77198
05f44a9f3f5a9c4734774ca56333c1342c338878	vizslice: visualizing large scale software slices		Program slicing has long been used to facilitate program understanding. Several approaches have been suggested for computing slices based on different perspectives, including forward slicing, backward slicing, static slicing, and dynamic slicing. The applications of slicing are numerous, including testing, effort estimation, and impact analysis. Surprisingly, given the maturity of slicing, few approaches exist for visualizing slices. In this paper, we present our tool for visualizing large systems based on program slicing and through two visualization idioms: treemaps and bipartite graphs. In particular, we use treemaps to facilitate slicing-based navigation, and we use bipartite graphs to facilitate visual impact analysis by displaying relationships among system decomposition slices showing the relevant computations involving a given slicing variable. We believe our tool will support various software maintenance tasks, including providing analysts an interactive visualization of the impact of potential changes, thus allowing them to plan maintenance accordingly. Finally, we show that, through the use of both existing scalable slicing and scalable visualization approaches, our tool can facilitate analysis of large software systems.	artifact (error);browsing;capability maturity model;code refactoring;computation;cost estimation in software engineering;debugging;interactive visualization;pointer (computer programming);pointer aliasing;program comprehension;program slicing;reverse engineering;scalability;software maintenance;software system;software testing;treemapping	Hakam W. Alomari;Rachel A. Jennings;Paulo Virote de Souza;Matthew Stephan;Gerald C. Gannod	2016	2016 IEEE Working Conference on Software Visualization (VISSOFT)	10.1109/VISSOFT.2016.22	program slicing;computer science;theoretical computer science;database	SE	-55.133863971602594	35.68693115838079	77224
21f3ca726a0b33a7ad0072c67d9007d15e646463	mining execution relations for crosscutting concerns	software;program graph;object oriented programming data mining;programme commande;recurring execution patterns;graph flow;orientado aspecto;aspect oriented design;software systems;concern separation;flux donnee;flujo datos;object oriented programming;data mining;flow graphs;flujo grafo;control flow graphs;data mining software systems performance analysis software flow graphs encoding security;graphe programme;control program;flot graphe;separation preoccupation;performance analysis;graphe flux;separacion preocupacion;semi automatic static aspect mining;programa mando;crosscutting concerns;aspect oriented;graphe fluence;recurring execution patterns crosscutting concerns aspect oriented design semi automatic static aspect mining control flow graphs;data flow;grafo programa;security;encoding;grafo fluencia;oriente aspect;fluence graph	Aspect mining tries to identify crosscutting concerns in the code of existing systems and thus supports their adaption to an aspect-oriented design. A semi-automatic static aspect mining approach is described, where the program’s control flow graphs are investigated for recurring execution patterns based on different constraints, such as the requirement that the patterns have to exist in different calling contexts. Two case studies done with the implemented tool show that many discovered candidates for crosscutting concerns are instances of delegation and should not be refactored into aspects. More generally, it is shown that aspect mining techniques need a way to distinguish between delegation and superimposed behaviour.	aspect-oriented software development;code refactoring;control flow graph;cross-cutting concern;execution pattern;semiconductor industry	Jens Krinke	2008	IET Software	10.1049/iet-sen:20070005	data flow diagram;real-time computing;aspect-oriented programming;computer science;information security;software engineering;database;programming language;object-oriented programming;encoding;software system	SE	-56.710590703019214	35.57008030834334	77685
4e37d7038de9426ceb0f57473a04ab6866e2de9e	a novel approach for slicing of object oriented programs	control flow graph;d u chain;object oriented program slicing	This paper extends the graph-less technique proposed by Beszedes for slicing Object Oriented Programs. The proposed approach computes the dynamic slices of the OOPs, especially, in case of polymorphism. The approach generates the defined-used (d-u) chains of the objects and variables used in the program and computes the slice using the generated d-u chains; it then debugs the program by detecting the various possible bugs and generates messages suggesting that a bug may be present. A GUI tool has been developed to compute and display the computed slices. The tool allows the user to browse the program and see the generated advice.	browsing;debugging;graphical user interface;linux kernel oops;sensor;software bug	Paritosh Jain;Nitish Garg	2013	ACM SIGSOFT Software Engineering Notes	10.1145/2492248.2492266	program slicing;real-time computing;computer science;programming language;engineering drawing;control flow graph	SE	-54.935165982838676	36.34156067533064	77711
571a4bb49c0205e1db36cff8c57028eb6c752b69	enhancing adaptive random testing in high dimensional input domains	software testing;high dimensionality;swinburne;random testing;fix sized candidate set art;adaptive random testing;high dimension problem;high dimension	Adaptive random testing (ART) is an enhancement of random testing (RT). It can detect failures more effectively than RT when failure-causing inputs are clustered. Having test cases both randomly selected and evenly spread is the key to the success of ART. Recently, it has been found that the dimensionality of the input domain could have an impact on the effectiveness of ART. The effectiveness of some ART methods may deteriorate when the dimension is high. In this paper, we work on one particular ART method, namely Fixed-Sized-Candidate-Set ART (FSCS-ART) and show how it can be enhanced for high dimensional domains. Since the cause of the problems for FSCS-ART may also be valid for some other ART methods, our solutions to the high dimension problems of FSCS-ART may be applicable for improving other ART methods.	random testing;randomness;test case	Fei-Ching Kuo;Tsong Yueh Chen;Huai Liu;Wing Kwong Chan	2007		10.1145/1244002.1244316	random testing;simulation;artificial intelligence;software engineering;software testing;algorithm	AI	-60.91437202930128	35.22911536337497	77854
79eb07f19e5d447368c2050df339816f57d9b7d7	automatic generation of path covers based on the control flow analysis of computer programs	software analysis tool;path cover;outil logiciel;unconstrained arcs;automatic;implication;software analysis;computer program;program diagnostics;recursive iteration;selection problem;dominance;program flowgraph;software tool;selection strategy;problema seleccion;program control structures program testing software tools trees mathematics program diagnostics flow graphs;algorithm analysis;generic algorithm;program control structures;dominator tree;estrategia;ejecucion programa;automatico;program verification;trees mathematics;satisfiability;flow graphs;automatic generation;program execution;analisis programa;automated testing tool;strategy;verificacion programa;iteraccion;program testing;dominancia;herramienta controlada por logicial;execution programme;infeasible paths;program inputs;automated testing tool path covers control flow analysis program branch testing program flowgraph program inputs ddgraph graph theoretic principles dominance implication arc trees unconstrained arcs dominator tree implied tree flexibility simplicity recursive iteration infeasible paths selection strategy software analysis tool software testing tool;program branch testing;software testing tool;ddgraph;iteration;automatique;control flow analysis;analyse algorithme;software tools;program analysis;analyse programme;implied tree;arc trees;path covers;verification programme;graph theoretic principles;programa computador;strategie;infeasible path;simplicity;analisis algoritmo;programme ordinateur;flexibility;branch testing;automatic generation control software testing performance analysis algorithm design and analysis tree graphs software algorithms embedded software software tools councils costs;probleme selection	Abstmct-Branch testing a program involves: 1) generating a set of paths that will cover every arc in the program flowgraph, called a path cover; 2) finding a set of program inputs that will execute every path in the path cover. The generation of path covers can be performed through the analysis of program control flow and can be automated, but, inevitably, statically generated path covers may include infeasible paths. The method used for path generation should attempt to reduce the incidence of this problem. The paper presents a generalized algorithm that finds a path cover for a given program flowgraph. The analysis is conducted on a reduced flowgraph, called a ddgraph, and uses graph theoretic principles differently than previous approaches. In particular, the relations of dominance and implication which form two trees of the arcs of the ddgraph are exploited. These relations make it possible to identify immediately a subset of ddgraph arcs, called unconstrained arcs, having the property that a set of paths exercising all the unconstrained arcs will also cover all the arcs in the ddgraph. In fact, the algorithm has been designed to cover all the unconstrained arcs of a given ddgraph: the paths are derived one at a time, each path covering a t least one as yet uncovered unconstrained arc. The greatest merits of the algorithm are its simplicity and its flexibility. It consists in just visiting recursively in combination the dominator and the implied trees and is flexible in the sense that it can derive a path cover to satisfy different requirements, according to the strategy adopted for the selection of the unconstrained arc to be covered at each recursive iteration. This feature of the algorithm can be employed to address the problem of infeasible paths, by adopting the most suitable selection strategy for the problem at hand. Embedding of the algorithm into a software analysis and testing tool is recommended.	algorithm;computer program;control flow analysis;dominator (graph theory);graph theory;incidence matrix;iteration;path cover;recursion;requirement;signal-flow graph;test automation	Antonia Bertolino;Martina Marré	1994	IEEE Trans. Software Eng.	10.1109/32.368137	program analysis;dominator;genetic algorithm;iteration;strategy;computer science;theoretical computer science;operating system;software analysis pattern;machine learning;dominance;programming language;automatic transmission;algorithm;control flow analysis;statistics;satisfiability	SE	-59.4909664144555	35.01979448742467	77881
1e110877904379b5906220a6f04c300456864cf2	towards automatic discovery of deviations in binary implementations with applications to error detection and fingerprint generation	approach work;protocol specification;error detection;fingerprint generation;novel approach;automatic discovery;different protocol;binary implementation;implementation error;different implementation;different interpretation;specification check;multiple implementation;towards automatic discovery	Different implementations of the same protocol specification usually containdeviations, i.e., differences in how they check and process some of their inputs. Deviations are commonly introduced as implementation errors or as different interpretations of the same specification. Automatic discovery of these deviations is important for several applications. In this paper, we focus on automatic discovery of deviations for two particular applications: error detection and fingerprint generation. We propose a novel approach for automatically detecting deviations in the way different implementations of the same specification check and process their input. Our approach has several advantages: (1) by automatically building symbolic formulas from the implementation, our approach is precisely faithful to the implementation; (2) by solving formulas created from two different implementations of the same specification, our approach significantly reduces the number of inputs needed to find deviations; (3) our approach works on binaries directly, without access to the source code. We have built a prototype implementation of our approach and have evaluated it using multiple implementations of two different protocols: HTTP and NTP. Our results show that our approach successfully finds deviations between different implementations, including errors in input checking, and differences in the interpretation of the specification, which can be used as fingerprints.	error detection and correction;fingerprint;hypertext transfer protocol;prototype;sensor	David Brumley;Juan Caballero;Zhenkai Liang;James Newsome	2007			computer science;theoretical computer science;operating system;data mining;algorithm	Security	-56.58632193237914	44.062097068291386	77978
03b84b789cb342587db621c7e88eeb005cc21578	mining console logs for large-scale system problem detection	application development;decision tree;text mining;anomaly detection;large scale system;file system;principal component analysis;source code analysis	The console logs generated by an application contain messag that the application developers believed would be useful in debugging or monitoring the application. Despite the ubiquit y and large size of these logs, they are rarely exploited in a syste matic way for monitoring and debugging because they are not readily machine-parsable. In this paper, we propose a novel meth od for mining this rich source of information. First, we combin e log parsing and text mining with source code analysis to extract structure from the console logs. Second, we extract fe atures from the structured information in order to detect ano malous patterns in the logs using Principal Component Analysi s (PCA). Finally, we use a decision tree to distill the results of PCA-based anomaly detection to a format readily understand able by domain experts (e.g. system operators) who need not be familiar with the anomaly detection algorithms. As a case study, we distill over one million lines of console logs from the Hadoop file system to a simple decision tree that a domain expert can readily understand; the process requires no operat r intervention and we detect a large portion of runtime anomal ies that are commonly overlooked.	algorithm;anomaly detection;apache hadoop;debugging;decision tree;field electron emission;information source;machine-readable medium;parsing;static program analysis;subject-matter expert;sysop;text mining	Wei Xu;Ling Huang;Armando Fox;David A. Patterson;Michael I. Jordan	2008			anomaly detection;text mining;computer science;operating system;decision tree;data mining;database;rapid application development;world wide web;principal component analysis	OS	-61.14714661829953	42.14551319151838	78012
409d2e92b131b07d89d6f0a9f3d0cea3677eda7f	verification of architectural constraints on sequences of method invocations	verification;call graph;symbolic execution;architecture	The importance of correspondence between the architectural prescription and implementation has been long recognized. This paper presents an approach to verification of constraints on method invocation chains prescribed by an architectural style. It consists of two key steps. One, static backward and forward search is applied on the call graph of the system, to find all potential paths between the initial method and the final method prescribed in the architecture. Two, symbolic execution is applied to check the feasibility of those potential paths and generate tests for all feasible ones to check the correspondence. We implement our approach in a prototype based on Soot and Symbolic PathFinder (SPF), and demonstrate the usefulness of our approach using a case study.	call graph;method (computer programming);prototype;qualitative comparative analysis;subroutine;symbolic execution;test case	Stuart Siroky;Rodion M. Podorozhny;Guowei Yang	2015	ACM SIGSOFT Software Engineering Notes	10.1145/2693208.2693246	call graph;real-time computing;verification;computer science;engineering;theoretical computer science;architecture;programming language;algorithm	SE	-58.43057108108319	36.72843791453558	78051
90491a9b4331b4b19af1f53e1d36ffbd300937a4	a combined testing and verification approach for software reliability	outil logiciel;software testing;model specification;verificacion experimental;metodo matematico;software tool;mathematical method;metodologia;capacidad canal;software verification;capacite canal;satisfiability;methodologie;codificacion;model error;robot control;channel capacity;herramienta controlada por logicial;coding;consistency checking;verification experimentale;methode mathematique;fiabilite logiciel;methodology;fiabilidad logicial;software reliability;codage;experimental test	Automatic and manual software verification is based on applying mathematical methods to a model of the software. Modeling is usually done manually, thus it is prone to modeling errors. This means that errors found in the model may not correspond to real errors in the code, and that if the model is found to satisfy the checked properties, the actual code may still have some errors. For this reason, it is desirable to be able to perform some consistency checks between the actual code and the model. Exhaustive consistency checks are usually not possible, for the same reason that modeling is necessary. We propose a methodology for improving the throughput of software verification by performing some consistency checks between the original code and the model, specifically, by applying software testing. In this paper we present such a combined testing and verification methodology and demonstrate how it is applied using a set of software reliability tools. We introduce the notion of a neighborhood of an error trace, consisting of a tree of execution paths, where the original error trace is one of them. Our experience with the methodology shows that traversing the neighborhood of an error is extremely useful in locating its cause. This is crucial not only in understanding where the error stems from, but in getting an initial idea of how to redesign the code. We use as a case study a robot control system, and report on several design and modeling errors found during the verification and testing process.	algorithm;conformance testing;debugging;eventual consistency;finite-state machine;formal methods;model checking;revision control system;robot control;spin;software quality;software reliability testing;software testing;software verification;state space;test automation;throughput	Natasha Sharygina;Doron A. Peled	2001		10.1007/3-540-45251-6_35	simulation;white-box testing;software verification;computer science;software reliability testing;errors-in-variables models;methodology;robot control;software testing;coding;specification;channel capacity;algorithm;software quality;statistics;satisfiability	SE	-59.58124435380668	32.33748019336708	78200
ae7d2c7038af1a7536605d5b219705ec10d6f967	integrating program analyses with programmer productivity tools	programmer interaction;analysis communication;program analyses	Because software continues to grow in size and complexity, programmers increasingly rely on productivity tools to understand, debug, and modify their programs. These tools typically use program analyses to produce information for the programmer. This is problematic because it is based on the assumption that the programmer and program analyses all use the same vocabulary. If the programmer and analyses did not use the same vocabulary then the results of the analyses will be meaningless to the programmer. For example, ‘v124 may be NULL’ does not mean much to the programmer but ‘myStack may be NULL’ is meaningful. Often, the programmer and analyses prefer different vocabularies. While the programmer prefers his programs' source code, an analysis will prefer a simplified representation. Unfortunately, writing an analysis that works on the source code is difficult because the analysis must deal with the idiosyncracies of the source language (e.g. nested classes). In comparison, writing an analysis on SSA form is easy but the output of the analysis is not meaningful to the programmer; it must somehow be translated into something the programmer understands. We present a system, RTalk, that makes it easy to support both the programmers' and the analysis' needs. RTalk generates a translator between the programmers' and the analysis' vocabulary. Thus both the programmer and the analysis can use the vocabulary most natural to them. We demonstrate the effectiveness of RTalk by describing program understanding and program optimization tools that we have already built using RTalk. Copyright © 2011 John Wiley & Sons, Ltd.	program analysis;programmer;programming productivity	Daniel von Dincklage;Amer Diwan	2011	Softw., Pract. Exper.	10.1002/spe.1035	computer science;software engineering;algorithmic program debugging;programming language	PL	-52.384713636793705	36.4706962427369	78361
14780df9063a54eef5a4e1bca6f3d315035fef44	relevant inputs analysis and its applications	program diagnostics;security holes detection relevant input analysis dynamic analysis program execution delta debugging algorithm;testing value dependence address dependence role of inputs strength of inputs delta debugging;security of data program debugging program diagnostics;program debugging;security of data;debugging computer crashes performance analysis algorithm design and analysis security indexes power capacitors	In this paper we develop a dynamic analysis, named relevant input analysis, that characterizes the role and strength of inputs in the computation of different values during a program execution. The role indicates whether a computed value is derived from an input value or its computation is simply influenced by an input value. The strength indicates if role (derived or influenced) relied upon the precise value of the input or it is among one of many values that can play a similar role. While it is clear that the results of our analysis can be very useful for the programmer in understanding relationships between inputs and program behavior, we also demonstrate the usefulness of the analysis by developing an efficient delta debugging algorithm. Other applications of relevant input analysis include assisting in generating test inputs and detection of security holes.	algorithm;computation;delta debugging;parameter (computer programming);programmer;sensor;vulnerability (computing)	Yan Wang;Rajiv Gupta;Iulian Neamtiu	2013	2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)	10.1109/ISSRE.2013.6698926	reliability engineering;real-time computing;computer science;theoretical computer science;operating system;distributed computing;algorithmic program debugging;programming language	SE	-60.80027909691732	36.88104994197837	78662
98ed354eb49b08af8f519be4e7763f466c20d4e9	executable source code and non-executable source code: analysis and relationships	projection slicing theory;and forward;backward slicing projection slicing theory dynamic program slicing forward program slicing;backward slicing;dynamic program slicing;dynamic slicing;educational institutions informatics amorphous materials testing conferences;program slicing;forward program slicing	The concept of source code, understood as the source components used to obtain a binary, ready to execute version of a program, comprises currently more than source code written in a programming language. Specially when we move apart from systems-programming and enter the realm of end-user applications, we find source files with documentation, interface specifications, internationalization and localization modules, multimedia files, etc. All of them are source code in the sense that the developer works directly with them, and the application is built automatically using them as input. This work discusses the relationship between 'classical' source code (usually written in a programming language) and these other files by analyzing a publicly-available software versioning repository. Aspects that have been studied include the nature of the software repository, the different mixtures of source code found in several software projects stored in it, the specialization of developers to the different tasks, etc.	apl;documentation;executable;internationalization and localization;partial template specialization;programming language;software repository;software versioning	Gregorio Robles;Jesús M. González-Barahona	2004	Source Code Analysis and Manipulation, Fourth IEEE International Workshop on	10.1109/SCAM.2004.12	program slicing;real-time computing;computer science;theoretical computer science;programming language	SE	-52.28519441621609	35.46920729110161	78699
b3c3ad617a38aaf7b1d003f248d2d7335f9a97f2	design by contract: the eiffel method	contracts software engineering application software software reusability software libraries robustness q factor software quality software systems production systems;software libraries;application software;software systems;contracts;software engineering;software reusability;production systems;robustness;design by contract;software quality;q factor	It is indeed possible to use object-oriented technology to produce, almost routinely, software systems that reach a degree of reliability without any equivalent in conventional methods, languages and tools. This requires a strict application of pure object-oriented concepts, in particular seamlessness (the use of a consistent method and notation throughout the software lifecycle), information hiding, automatic garbage collection, static typing, and the combination of static typing and dynamic binding. Another key component of this approach is the notion of design by contract, which leads to the production of system whose correctness is built-in rather than ensured ex post facto.	correctness (computer science);design by contract;eiffel;garbage collection (computer science);late binding;software development process;software system;type system	Bertrand Meyer	1998		10.1109/TOOLS.1998.711043	reliability engineering;personal software process;long-term support;verification and validation;software sizing;software verification;computer science;systems engineering;package development process;backporting;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;software walkthrough;resource-oriented architecture;software measurement;software deployment;software requirements;software quality;software system;software peer review	PL	-50.92170786616238	32.734187796439926	78888
480998d2bc6c9ae250920c0fd2d0ec8bb96968e1	a proposal for transaction-level verification with component wrapper language	component wrapper language;protocol checker;protocols;signal-level verification;verification periods;signal level language;simulation-coverage analyzer;test patterns;transaction-level verification;digital simulation;signal-level verification suites;formal verification;verification suites	We propose a new approach to accelerate transaction level verification by raising the productivity of the verification suites including test patterns, protocol checker, andsimulation-coverage analyzer. This approach combines the conventional transaction level language such as C and the signal level language based on our previously developed Component Wrapper Language (CWL). This approach is based on two concepts. The first one is a complete separation between transaction-level verification and signal-level verification for generating suitable verification suites in each design phase. The second one is the quick generation of signal-level verification suites from the original specification written in CWL. Experimental results show that our approach should yield much shorter verification periods versus conventional methods.	signal-to-noise ratio;test card	Koji Ara;Kei Suzuki	2003			reliability engineering;embedded system;communications protocol;productivity;real-time computing;formal verification;software verification;computer science;signal processing;software testing;high-level verification;runtime verification;programming language;intelligent verification;functional verification;signal generator	Logic	-48.60058850028838	34.67050177716999	79227
7c02f5f461e8de32ac59a94f8182b8a76fb48aee	mankala: a traditional game developed for mobile use	mobile;programming language;java programming;game development;mobile phone;grid;uml class diagram;development tool;mobile games;job scheduling;mobile application;mobile user	The market for mobile games is expanding rapidly, and several games market in the advanced world are booming with sales, that has led us to the use of latest tools for mobile application development to modify and enhance an existing and traditional game to make it available for mobile users, Congkak game is developed for the latest mobile phones in the market. This version has been modified to an eight holes, one store, and one player game for a start. Each player hole contains four seeds, which are sown continuously until the seeds are exhausted. The main objectives of the game are: to make it available for mobile phones as well as extend/modify the games' functionality.  Major development tools used were the Sony Ericsson KToolBar, Netbeans IDE, Scite text editor, and Adobe Photoshop. UML class diagrams have also been modeled for the classes. The programming language used in this game development is java, programming in the micro edition platform (J2ME).	adobe photoshop;code;diagram;java platform, micro edition;mobile app;mobile game;mobile phone;netbeans ide;programming language;programming tool;scite;seeds (cellular automaton);text editor;traditional game;unified modeling language;video game development	Aloysius Akpanobong;Oras F. Baker	2009		10.1145/1821748.1821830	game design;game development tool;embedded system;mobile search;simulation;mobile web;level design;computer science;job scheduler;operating system;mobile technology;class diagram;game developer;multimedia;game design document;grid;mobile computing;video game development;computer security;game programming;mobile payment	HCI	-51.06113155499024	42.10811653832022	79352
7e1a4cb783087523275bad2c742f14b1cf4fb311	conflict resolution support for parallel software development	parallel version merging;large scale software systems;source coding configuration management cost benefit analysis parallel programming recommender systems software maintenance;industrial strength legacy system;software merging;conflicting software entities;software merging parallel software development software maintenance parallel source code version merging parallel source code variant merging software engineering software conflict resolution recommender cost benefit approach conflict software entity ranking semantic information structural information scorerec industrial strength legacy system recommendation systems;parallel changes;software conflict resolution recommender;development lines separation;source code variants;parallel software development;recommendation systems;cost benefit approach;software engineering activity;scorerec	Parallel changes, in which separate lines of development are carried out by different developments, are a basic fact of developing and maintaining large-scale software systems. Merging parallel versions and variants of source code is a common and essential software engineering activity. When a non-trivial number of conflicts is detected, there is a need to support the maintainer in investigating and resolving these conflicts. In this study, the authors present software conflict resolution recommender (scoreRec), a cost–benefit approach to ranking the conflicting software entities. The contributions of scoreRec lie in the leverage of both structural and semantic information of the source code to generate conflict resolution recommendations, as well as the hierarchical presentation of the recommendations with detailed explanations. The authors evaluate scoreRec through an industrial-strength legacy system developed by computational scientists. The results show that scoreRec offers relevant and insightful information and sound engineering support for conflict resolution. The authors’ work also sheds light on the future development of recommendation systems in the context of software merging.	audio engineer;computation;entity;legacy system;recommender system;software development;software engineering;software maintainer;software system	Nan Niu;Fangbo Yang;Jing-Ru C. Cheng;Sandeep Reddivari	2013	IET Software	10.1049/iet-sen.2012.0089	computer science;theoretical computer science;software framework;software development;software engineering;data mining;database;software walkthrough;programming language;recommender system	SE	-57.55462026360965	34.13208447502013	79454
c8643b036afc5d30d726e401531143a0738d1af8	a method for file valuation in information lifecycle management		ILM is based on the idea that in an enterprise different information have different values. Valuable information is stored on systems with a high quality of service (QoS). The value changes over time and therefore migration of information is required to cheaper storage systems with a lower QoS. Automated migration makes ILM dynamic. Such automation requires storage systems to understand what files are important at what time so that right policies can be applied. In this point ILM nowadays lacks information valuation methods. This paper looks at how the value of a file can be measured. Different from traditional methods using metadata leading to a classical decimal-value we show how the value can be derived using a probabilistic method. Here the value of a file is calculated from usage information and expressed as a “probability of further use”. This is a new method which allows valuation depending on the future importance of a file. Feasibility of the new method is verified by generating file migration rules for ILM.	display resolution;fits;microsoft outlook for mac;quality of service;simulation;value (ethics)	Lars Arne Turczyk;Marcel Groepl;Nicolas Liebau;Ralf Steinmetz	2007			systems engineering;data mining;database;application lifecycle management	HPC	-48.75310377124185	41.727522702522165	79678
df0a3ce273d7848257de5950df8dcc92df8e4f87	automated test data generation for programs with procedures	fault localization;debugging;unit testing;mutation analysis;automated test data generation;critical slicing;test data generation;testing;satisfiability;faults;dynamic program slicing;data dependence;static program slicing;control flow;failures	Test data generation in program testing is the process of identifying a set of test data that satisfies a selected testing criterion, such as, statement coverage or branch coverage. The existing methods of test data generation are limited to unit testing and may not efficiently generate test data for programs with procedures. In this paper we present an approach for automated test data generation for programs with procedures. This approach builds on the current theory of execution-oriented test data generation. In this approach, test data are derived based on the actual execution of the program under test. For many programs, the execution of the selected statement may require prior execution of some other statements that may be part of some procedures. The existing methods use only control flow information of a program during the search process and may not efficiently generate test data for these types of programs because they are not able to identify statements that affect execution of the selected statement. Our approach uses data dependence analysis to guide the process of test data generation. Data dependence analysis automatically identifies statements (or procedures) that affect the execution of the selected statement and this information is used to guide the search process. The initial experiments have shown that this approach may improve the chances of finding test data.	code coverage;control flow;data dependency;dependence analysis;experiment;software testing;test automation;test data generation;unit testing	Bogdan Korel	1996		10.1145/229000.226319	keyword-driven testing;reliability engineering;program slicing;test data generation;real-time computing;test data;fault coverage;computer science;test suite;mutation testing;software testing;unit testing;fault;programming language;test script;control flow;debugging;test management approach;test harness;satisfiability	SE	-60.14091322082896	35.738751134764726	80408
f137aaacf44e7df294d3557874ce2db38ea19425	codeexchange: supporting reformulation of internet-scale code queries in context (t)	field deployment codeexchange internet scale code queries source code google specialized code search engine searchcode ohloh github iterative process code search engine;google;manuals;query reformulation;search engines;internet scale;source code software internet iterative methods query processing search engines;interface;face;programming;code search;context;search engines context google manuals java programming face;java;internet scale code search query reformulation context interface	Programming today regularly involves searching for source code online, whether through a general search engine such as Google or a specialized code search engine such as SearchCode, Ohloh, or GitHub. Searching typically is an iterative process, with develop-ers adjusting the keywords they use based on the results of the previous query. However, searching in this manner is not ideal, because just using keywords places limits on what developers can express as well as the overall interaction that is required. Based on the observation that the results from one query create a con-text in which a next is formulated, we present CodeExchange, a new code search engine that we developed to explicitly leverage this context to support fluid, expressive reformulation of queries. We motivate the need for CodeExchange, highlight its key design decisions and overall architecture, and evaluate its use in both a field deployment and a laboratory study.	(formerly ohloh);dial-up internet access;download;function overloading;iteration;naruto shippuden: clash of ninja revolution 3;software bug;software deployment;web search engine	Lee Martie;Thomas D. LaToza;André van der Hoek	2015	2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1109/ASE.2015.51	face;kpi-driven code analysis;programming;query expansion;computer science;interface;database;programming language;search analytics;web search query;java;world wide web;information retrieval;code generation;search engine;source code	SE	-59.32987932754035	39.93634215464548	80471
0c6bac59280c7521f6cb474faf05229c40f372f6	early capacity testing of an enterprise service bus	enterprise service bus	An enterprise service-oriented architecture is typically done with a messaging infrastructure called an Enterprise Service Bus (ESB). An ESB is a bus which delivers messages from service requesters to service providers. Since it sits between the service requesters and providers, it is not appropriate to use any of the existing capacity planning methodologies for servers, such as modeling, to estimate the capacity of an ESB. There are programs that run on an ESB called mediation modules. Their functionalities vary and depend on how people use the ESB. This creates difficulties for capacity planning and performance evaluation. This article proposes a capacity planning methodology and performance evaluation techniques for ESBs, to be used in the early stages of the system development life cycle. The authors actually run the ESB on a real machine while providing a pseudo-environment around it. In order to simplify setting up the environment they provide ultra-light service requestors and service providers for the ESB under test. The authors show that the proposed mock environment can be set up with practical hardware resources available at the time of hardware resource assessment. Their experimental results showed that the testing results with our mock environment correspond well with the results in the real environment.	enterprise service bus	Ken Ueno;Michiaki Tatsubori	2009	Int. J. Web Service Res.	10.4018/jwsr.2009070104	real-time computing;computer science;computer security	Web+IR	-50.53208041431327	40.200693301998314	80493
501c9e97c85130833be0fad8bac9072d9623c7d8	static security analysis based on input-related software faults	libraries;software metrics;software;grammatech codesurfer tool;program diagnostics;security analysis;grammatech codesurfer tool static security analysis input related software faults reliable software complex systems software development c source code;software maintenance;static security analysis;input related software faults;software systems;reliable software;data mining;software engineering;computer security;distance measurement;c source code;complex system;buffer overflow;data security buffer overflow software maintenance open source software software algorithms computer security software systems performance analysis fault diagnosis software engineering;fault detection;software development;performance analysis;security analysis static analysis;software algorithms;complex systems;source code;book reviews;static analysis;software reliability program diagnostics security of data software metrics;security;software reliability;security of data;fault diagnosis;open source software;open source;data security	It is important to focus on security aspects during the development cycle to deliver reliable software. However, locating security faults in complex systems is difficult and there are only a few effective automatic tools available to help developers. In this paper we present an approach to help developers locate vulnerabilities by marking parts of the source code that involve user input. We focus on input-related code, since an attacker can usually take advantage of vulnerabilities by passing malformed input to the application. The main contributions of this work are two metrics to help locate faults during a code review, and algorithms to locate buffer overflow and format string vulnerabilities in C source code. We implemented our approach as a plugin to the Grammatech CodeSurfer tool. We tested and validated our technique on open source projects and we found faults in software that includes Pidgin and cyrus-imapd.	algorithm;buffer overflow;byzantine fault tolerance;c++;complex systems;fault detection and isolation;item unique identification;java;open-source software;printf format string;sensor;software engineering;uncontrolled format string;vulnerability (computing)	Csaba Levente Nagy;Spiros Mancoridis	2009	2009 13th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2009.51	reliability engineering;complex systems;code review;buffer overflow;computer science;software development;operating system;software engineering;data security;security analysis;secure coding;software maintenance;static analysis;fault detection and isolation;software quality;software metric;software system;source code	SE	-58.579039098946254	41.45638020593882	80506
0bb58c68127f1e9f7a94ca80e61881acd60f7bf5	dbug: systematic testing of unmodified distributed and multi-threaded systems	systematic testing;multi-threaded system	In order to improve quality of an implementation of a distributed and multithreaded system, software engineers inspect code and run tests. However, the concurrent nature of such systems makes these tasks challenging. For testing, this problem is addressed by stress testing, which repeatedly executes a test hoping that eventually all possible outcomes of the test will be encountered. In this paper we present the dBug tool, which implements an alternative method to stress testing called systematic testing. The systematic testing method implemented by dBug controls the order in which certain concurrent function calls occur. By doing so, the method can systematically enumerate possible interleavings of function calls in an execution of a concurrent system. The dBug tool can be thought of as a light-weight model checker, which uses the implementation of a distributed and multi-threaded system and its test as an implicit description of the state space to be explored. In this state space, the dBug tool performs a reachability analysis checking for a number of safety properties including the absence of 1) deadlocks, 2) conflicting non-reentrant function calls, and 3) system aborts and runtime assertions inserted by the user.	concurrency (computer science);deadlock;download;enumerated type;linux;model checking;open-source software;programmer;reachability;reentrancy (computing);software engineer;state space;stress testing;thread (computing);web page	Jirí Simsa;Randy Bryant;Garth A. Gibson	2011		10.1007/978-3-642-22306-8_14	real-time computing;stress testing;software;computer science;computational thinking	SE	-57.09075676236149	38.90608425449051	81009
0693d74012eec246400a1cea0ed157849ed1c87c	enabling automatic adaptation in systems with under-specified elements	anomaly detection;software systems;self adaptation;dynamic data;concurrency;component architecture;requirement specification	Software that people use for everyday purposes is usually not mission critical---some failures can be tolerated. However, this software should be dependable enough for its intended use, even when users change expectations. Software systems that could adapt to accommodate both failures and changing user expectations could significantly improve the dependability of such everyday software. Many adaptation techniques require specifications of proper behavior (for detecting improper behavior) and problem severity, alternatives and their selection (for mitigation and for repair).However, the specifications of everyday software are usually incomplete and imprecise. This makes it difficult to determine the dependability of the software and even more difficult to adapt.We address the problem of detecting anomalies---deviations from expected behavior---when specifications of expected behavior are missing. Setting up anomaly detection depends on human participation, yielding predicates that can serve as proxies for missing specifications.We propose a template mechanism to lower the demands on human attention when setting up detection. We show how this mechanism may be used in our framework for enhancing dynamic data feeds with automatic adaptation. We discuss how the same mechanism may be used in repair. Our emphasis is on detecting semantic anomalies: cases in which the data feed is responsive and delivers well-formed results, but these results are unreasonable.	anomaly detection;data feed;dependability;dynamic data;mission critical;sensor;software system;well-formed element	Orna Raz;Philip Koopman;Mary Shaw	2002		10.1145/582128.582139	reliability engineering;real-time computing;engineering;data mining	SE	-53.63445753788747	41.25313926727599	81852
50f9cf9f6e4455408e72ee54ce2e29778951fe20	draco: discovering refactorings that improve architecture using fine-grained co-change dependencies		Co-change dependencies arise whenever two source code entities, such as classes, methods, or fields, change frequently together. Similar to other kinds of software dependencies, it is possible to build software clusters from co-change relationships and, as such, previous studies explored the use of this kind of dependency in several software engineering tasks, such as predicting software faults, recommending related source code changes, and assessing software modularity. In this ongoing work, our goal is to provide tools to discover refactoring opportunitiesâsuch as move method, move field, split class, or merge classes-that are revealed when comparing the co-change clusters of fine-grained source code entities (methods, fields, constructors) to the original class decomposition; specifically when a source code entity is in the same class but in different clusters (or vice-versa). Our approach, named Draco, aims to produce minimal refactoring sequences that improve architecture decomposition.	code refactoring;control system;draco;entity;software engineering	Marcos César de Oliveira	2017		10.1145/3106237.3119872	architecture;theoretical computer science;computer science;merge (version control);code refactoring;cluster analysis;software;source code;dependency theory (database theory);modularity	SE	-57.04834850790038	34.56812133174461	81930
0dc675cb925d58ba076a6bc46868bed683736c2b	software safety goal verification using fault tree techniques: a critically ill patient monitor example	biomedical monitoring;product safety;software safety fault trees patient monitoring biomedical monitoring medical tests hazards product safety computerized monitoring control systems hardware;control systems;fault tree;hazard avoidance verification software safety goal verification fault tree techniques patient monitoring system mapping product test documentation;medical tests;hazard avoidance verification;hazards;fault tree techniques;program verification;medical computing;computerized monitoring;software safety;software safety goal verification;patient monitoring system;test generation;patient monitoring;mapping;program verification medical computing patient monitoring;critically ill patient;product test documentation;hazard analysis;fault tree analysis;hardware;fault trees	Fault tree analysis techniques as applied to a patient monitoring system are discussed. This process provides mapping for test procedures to stated safety goals, a concise archive of the safety subset of the product test documentation, and hazard avoidance verification as indicated by a preliminary hazard analysis. The process as applied to one product is explained. The fault tree for this instrument produced a total of 42 test procedures for final system test use. The time for tree and test generation was six man-weeks. The tests required two man weeks to complete. These tests were instrumental in finding 16% of the total defects and 36% of the critical defects for this product. >	fault tree analysis	Brian Connolly	1989		10.1109/CBMSYS.1989.47367	real-time computing;fault tree analysis;control system	Logic	-48.722999914565634	36.02498326005024	82410
4581ff117f253bc6b9636691f5a8a4fd8550c64c	precisely detecting runtime change interactions for evolving software	software;software testing;regression testing;history;performance evaluation;regression test suites;program merging;articulo;semantics;precisely detecting runtime change interactions for evolving software;runtime change interactions detection;software engineering java program testing;interference;software development tasks;runtime;software engineering;software performance;runtime software testing java merging performance evaluation educational institutions interference software performance;slicing;impact analysis;java subjects runtime change interactions detection evolving software software development tasks regression test suites;program testing;side effect;image edge detection;syntactics;heuristic algorithms;java subjects;software development;merging;impact analysis change analysis slicing regression testing program merging;change analysis;java;evolving software	Developers often make multiple changes to software. These changes are introduced to work cooperatively or to accomplish separate goals. However, changes might not interact as expected or may produce undesired side effects. Thus, it is crucial for software-development tasks to know exactly which changes interact. For example, testers need this information to ensure that regression test suites test the combined behaviors of changes. For another example, teams of developers must determine whether it is safe to merge variants of a program modified in parallel. Existing techniques can be used to detect at runtime potential interactions among changes, but these reports tend to be coarse and imprecise. To address this problem, in this paper, we first present a formal model of change interactions at the code level, and then describe a new technique, based on this model, for detecting at runtime such interactions with accuracy. We also present the results of a comparison of our technique with other techniques on a set of Java subjects. Our results clearly suggest that existing techniques are too inaccurate and only our technique, of all those studied, provides acceptable confidence in detecting real change interactions occurring at runtime.	algorithm;application checkpointing;formal language;ibm notes;interaction;java;mathematical model;program slicing;regression testing;run time (program lifecycle phase);sensor;side effect (computer science);software development;test suite	Raúl A. Santelices;Mary Jean Harrold;Alessandro Orso	2010	2010 Third International Conference on Software Testing, Verification and Validation	10.1109/ICST.2010.29	regression testing;real-time computing;software performance testing;computer science;software development;operating system;software engineering;semantics;interference;software testing;runtime verification;programming language;java;side effect	SE	-59.431304521981396	36.64014771540347	82494
0a169f93ef4b73107d2b8e4d93f853964a338aa8	evaluation metric for multiple-bug localization with simple and complex predicates	software debugging evaluation metric multiple bug localization software execution program statement program bug program predicate compilation program execution statistical relevancy predicate based statistical debugger simple predicate complex predicate;software metrics;statistical debugging;conference_paper;program execution;statistical analysis program debugging software metrics;debuggers;statistical analysis;program bugs;evaluation metrics;program debugging;computer bugs measurement instruments debugging labeling software educational institutions	Statistical debugging is a technique that mines data obtained from software executions in order to identify the program statements that are relevant to program bugs. Specifically, program predicates are injected into the program during compilation and statistics about those predicates are collected during the program execution. When bugs are found but the developers have no clue where the bugs are, they may call such a statistical debugger for help. The debugger ranks the injected predicates according to their statistical relevancy to bugs and presents the suspicious ones to the developers. When a bug is found and fixed, but the updated program still contains (some other) bugs, the preceding procedure is iterated until all bugs are fixed. There are two types of predicate-based statistical debugger: one type returns only simple predicates, another type returns only complex predicates. We envision that the next wave of statistic debuggers should be able to return both, depending on the kinds of bugs manifested in the software. In this paper, we take the first step and study the metrics for evaluating the effectiveness of statistical debuggers that can return both types of predicate predictors (simple or complex).	compiler;debugger;debugging;iteration;precision and recall;program transformation;relevance;software bug	Yiwei Zhang;Eric Lo;Ben Kao	2012	2012 19th Asia-Pacific Software Engineering Conference	10.1109/APSEC.2012.37	real-time computing;software bug;bebugging;computer science;database;algorithmic program debugging;programming language;debugging;software metric	SE	-59.827046862319605	37.948107376110485	82796
cdcb0fd7db9e8d727f3f2cbc24df4efcb7248682	formal analysis of the probability of interaction fault detection using random testing	software;software cost estimation;probability;lower bound combinatorial testing random testing interaction testing theory constraint feature diagram;cost reduction;software fault tolerance;software context fault detection feature extraction scalability benchmark testing;random testing;program verification;feature diagram;customer satisfaction;feature constraints interaction fault detection probability formal analysis random testing customer satisfaction user satisfaction software product lines software development cost reduction large scale reusability combinatorial testing computational overhead;constraint;program testing;combinatorial testing;feature extraction;theory;fault detection;random processes;scalability;context;benchmark testing;lower bound;software fault tolerance cost reduction customer satisfaction probability program testing program verification random processes software cost estimation;interaction testing	Modern systems are becoming highly configurable to satisfy the varying needs of customers and users. Software product lines are hence becoming a common trend in software development to reduce cost by enabling systematic, large-scale reuse. However, high levels of configurability entail new challenges. Some faults might be revealed only if a particular combination of features is selected in the delivered products. But testing all combinations is usually not feasible in practice, due to their extremely large numbers. Combinatorial testing is a technique to generate smaller test suites for which all combinations of t features are guaranteed to be tested. In this paper, we present several theorems describing the probability of random testing to detect interaction faults and compare the results to combinatorial testing when there are no constraints among the features that can be part of a product. For example, random testing becomes even more effective as the number of features increases and converges toward equal effectiveness with combinatorial testing. Given that combinatorial testing entails significant computational overhead in the presence of hundreds or thousands of features, the results suggest that there are realistic scenarios in which random testing may outperform combinatorial testing in large systems. Furthermore, in common situations where test budgets are constrained and unlike combinatorial testing, random testing can still provide minimum guarantees on the probability of fault detection at any interaction level. However, when constraints are present among features, then random testing can fare arbitrarily worse than combinatorial testing. As a result, in order to have a practical impact, future research should focus on better understanding the decision process to choose between random testing and combinatorial testing, and improve combinatorial testing in the presence of feature constraints.	combinatorial optimization;fault detection and isolation;overhead (computing);random testing;software development;software product line;software testing;test suite	Andrea Arcuri;Lionel C. Briand	2012	IEEE Transactions on Software Engineering	10.1109/TSE.2011.85	random testing;reliability engineering;stochastic process;benchmark;real-time computing;scalability;orthogonal array testing;white-box testing;feature extraction;all-pairs testing;computer science;theoretical computer science;probability;constraint;upper and lower bounds;customer satisfaction;theory;fault detection and isolation;software fault tolerance;statistics	SE	-61.03925059622339	33.61297995477688	82813
36c6ac6d220a0d571739781b8748b8819012ed4b	beyond documents: sharing work	encapsulation;internet organizing companies business environmental management runtime environment encapsulation java technology planning remuneration;remuneration;runtime environment;companies;technology planning;internet;organizing;business;environmental management;java	First Page of the Article		Charles J. Petrie;Sunil K. Sarin	2000	IEEE Internet Computing	10.1109/MIC.2000.845388	the internet;encapsulation;computer science;knowledge management;programming language;java	Visualization	-50.74848195921791	45.421518390057166	82838
5a474f19c918dd6dfd37b304e2760f36a1219bb6	a parallel evolutionary algorithm for prioritized pairwise testing of software product lines	feature models;pairwise testing;software product lines;combinatorial interaction testing	Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.	evolutionary algorithm;evolutionary computation;genetic algorithm;greedy algorithm;software product line;software system;software testing;solver	Roberto Erick Lopez-Herrejon;Javier Ferrer;Francisco Chicano;Evelyn Nicole Haslinger;Alexander Egyed;Enrique Alba	2014		10.1145/2576768.2598305	all-pairs testing;computer science;bioinformatics;theoretical computer science;machine learning	SE	-59.46117386260164	34.174208802791014	83009
b7f4ca99b35980b25f2f8807f2670f3d7d370c20	absolute scoring scheme for interoperability testing of advanced metering infrastructure on demand side management		This paper proposes an Absolute Scoring Scheme for interoperability testing purposes. The scheme has been created in the context of a Demand Side Management test case aiming to examine the capability of different components of interest to interoperate with each other. Here, the interoperability of smart meters with data concentrators has been studied. The Scoring Scheme proves to be more precise with a larger set of devices. The SGAM framework has been followed for the proper mapping of components, communication protocols and information models used. This work can contribute to enhancing the quality of interoperability testing activities in order to support industrial stakeholders.	information model;interoperability;smart meter;test case	Ioannis Poursanidis;Nikoleta Andreadou;Evangelos Kotsakis;Marcelo Masera	2018		10.1145/3208903.3212035	information model;metering mode;computer network;interoperability;communications protocol;computer science	SE	-54.59255923309693	46.090594242024004	83114
1158a3afa30525ea505e64930ef2e732b7ce3b19	enterprise software service emulation: constructing large-scale testbeds	quality assurance;lean software development;swinburne;emulation;service virtualization;kaluta;software development lifecycle	Constructing testbeds for systems which are interconnected with large networks of other software services is a challenging task. It is particularly difficult to create testbeds facilitating evaluation of the non-functional qualities of a system, such as scalability, that can be expected in production deployments. Software service emulation is an approach for creating such testbeds where service behaviour is defined by emulate-able models executed in an emulation runtime environment. We present (i) a meta-modelling framework supporting emulate-able service modelling (including messages, protocol, behaviour and states), and (ii) Kaluta, an emulation environment able to concurrently execute large numbers (thousands) of service models, providing a testbed which mimics the behaviour and characteristics of large networks of interconnected software services. Experiments show that Kaluta can emulate 10,000 servers using a single physical machine, and is a practical testbed for scalability testing of a real, enterprise-grade identity management suite. The insights gained into the tested enterprise system were used to enhance its design.	communication endpoint;computational complexity theory;emulator;enterprise software;enterprise system;identity management;lightweight directory access protocol;runtime system;scalability testing;service (systems architecture);software system;testbed	Cameron M. Hine;Jean-Guy Schneider;Jun Han;Steve Versteeg	2016	2016 IEEE/ACM International Workshop on Continuous Software Evolution and Delivery (CSED)	10.1145/2896941.2896947	quality assurance;emulation;real-time computing;simulation;computer science;systems engineering;engineering;software engineering;software as a service;service virtualization;lean software development;software development process	SE	-48.47261328607774	37.2327393042056	83211
c58f8397f700ed01021dd8bc6dca831b186486a4	on the statistical properties of the f-measure	sampling distributions;worst case scenario;program testing;debug testing;previous study;distinct test case;random processes;future study;software metrics;geometric distribution;statistical properties;testing effectiveness metric;program debugging;debug testing strategy;program failure;quality measurement;software testing;adaptive random testing method;simulation study;random testing;f-measure;adaptive random testing;sampling methods;sampling distribution;f measure	The F-measure - the number of distinct test cases to detect the first program failure - is an effectiveness measure for debug testing strategies. We show that for random testing with replacement, the F-measure is distributed according to the geometric distribution. A simulation study examines the distribution of two adaptive random testing methods, to study how closely their sampling distributions approximate the geometric distribution, revealing that in the worst case scenario, the sampling distribution for adaptive random testing is very similar to random testing. Our results have provided an answer to a conjecture that adaptive random testing is always a more effective alternative to random testing, with reference to the F-measure. We consider the implications of our findings for previous studies conducted in the area, and make recommendations to future studies.	approximation algorithm;f1 score;futures studies;random testing;sampling (signal processing);simulation;test case;worst-case scenario	Tsong Yueh Chen;Fei-Ching Kuo;Robert G. Merkel	2004	Fourth International Conference onQuality Software, 2004. QSIC 2004. Proceedings.	10.1109/QSIC.2004.1357955	random testing;random variate;reliability engineering;stochastic process;sampling;geometric distribution;orthogonal array testing;computer science;theoretical computer science;random function;risk-based testing;software testing;convolution random number generator;sampling distribution;f1 score;software metric	SE	-61.31399638556514	34.5096824206017	83224
2f9045d41182baacc589658ac05c93b67a51c090	efficient points-to analysis for partial call graph construction		Many static analysis tools provide whole-program analysis to generate call graphs. However, the whole-program analysis suffers from scalability issue. The increasing size of the libraries exacerbates the issue. For many Web applications, the libraries (e.g. Servlet containers) are even not available for whole-program analysis. We present HyPta, a points-to analysis approach, to construct partial call graphs for Java programs. HyPta extends the standard points-to analysis by establishing a hybrid heap model. Since our approach does not analyze the method bodies of the library classes, the heap model distinguishes between the abstract memory locations in the application and those in the library. HyPta infers the pointer information in the library from the interactions between the application and the library. We implement HyPta based on Spark framework and evaluate it on 14 widely cited Java benchmarks. The evaluation shows that HyPta is faster than Averroes and Spark by a factor of 4.9x and 13.7x, respectively. Meanwhile, it constructs sound and precise partial call graphs.	apache spark;call graph;interaction;interprocedural optimization;java servlet;library (computing);pointer (computer programming);pointer analysis;reflection (computer programming);scalability;static program analysis;web application	Zhiyuan Wan;Bo Zhou;Ye Wang;Yuanhong Shen	2014			data mining;graph power;computer science;strength of a graph;theoretical computer science;voltage graph;windmill graph;distance-hereditary graph;call graph;java;heap (data structure)	PL	-56.31940800090122	37.971653457764994	83987
53d85e1dd2d949970da6ec498439d4b8b38adf12	interactive fault localization using test information	fault localization;debugging;software development;interactive approach	Debugging is a time-consuming task in software development. Although various automated approaches have been proposed, they are not effective enough. On the other hand, in manual debugging, developers have difficulty in choosing breakpoints. To address these problems and help developers locate faults effectively, we propose an interactive fault-localization framework, combining the benefits of automated approaches and manual debugging. Before the fault is found, this framework continuously recommends checking points based on statements' suspicions, which are calculated according to the execution information of test cases and the feedback information from the developer at earlier checking points. Then we propose a naive approach, which is an initial implementation of this framework. However, with this naive approach or manual debugging, developers' wrong estimation of whether the faulty statement is executed before the checking point (breakpoint) may make the debugging process fail. So we propose another robust approach based on this framework, handling cases where developers make mistakes during the fault-localization process. We performed two experimental studies and the results show that the two interactive approaches are quite effective compared with existing fault-localization approaches. Moreover, the robust approach can help developers find faults when they make wrong estimation at some checking points.	breakpoint;code refactoring;code reuse;component-based software engineering;computer science;debugging;dependability;distributed object;electronic engineering;microsoft customer care framework;modeling language;program comprehension;programming language;randomness;reverse engineering;software development;software testing;sun one;test case;x image extension	Dan Hao;Lu Zhang;Tao Xie;Hong Mei;Jiasu Sun	2009	Journal of Computer Science and Technology	10.1007/s11390-009-9270-z	real-time computing;computer science;software development;operating system;machine learning;database;distributed computing;algorithmic program debugging;programming language;debugging;computer security;algorithm	SE	-60.02693419955737	36.8063389004101	84254
542083839044cb95541dac0b37ac753efbc3ed72	synthesizing framework models for symbolic execution	analytical models;sketch;androids;framework model;observers;program synthesis;symbolic execution;humanoid robots;tutorials;synthesizers;article;java	Symbolic execution is a powerful program analysis technique, but it is difficult to apply to programs built using frameworks such as Swing and Android, because the framework code itself is hard to symbolically execute. The standard solution is to manually create a framework model that can be symbolically executed, but developing and maintaining a model is difficult and error-prone. In this paper, we present Pasket, a new system that takes a first step toward automatically generating Java framework models to support symbolic execution. Pasket's focus is on creating models by instantiating design patterns. Pasket takes as input class, method, and type information from the framework API, together with tutorial programs that exercise the framework. From these artifacts and Pasket's internal knowledge of design patterns, Pasket synthesizes a framework model whose behavior on the tutorial programs matches that of the original framework. We evaluated Pasket by synthesizing models for subsets of Swing and Android. Our results show that the models derived by Pasket are sufficient to allow us to use off-the-shelf symbolic execution tools to analyze Java programs that rely on frameworks.	android;application programming interface;cognitive dimensions of notations;conformity;design pattern;executable;java;method (computer programming);mutator method;program analysis;sketch;swing (java);symbolic execution;unified model	Jinseong Jeon;Xiaokang Qiu;Jonathan Fetter-Degges;Jeffrey S. Foster;Armando Solar-Lezama	2016	2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)	10.1145/2884781.2884856	real-time computing;computer science;humanoid robot;theoretical computer science;operating system;software engineering;programming language;java	SE	-55.772682878544764	37.93493544082326	84389
085c0626a0efb0159d4c27fab8b17656c3516c34	the effect of testing on reliability of fault-tolerant software	reliability;fault tolerant;software testing reliability testing fault tolerant software software debugging;software fault tolerance;software testing fault tolerance fault tolerant systems battery powered vehicles software reliability debugging stochastic processes parameter estimation computer industry hardware;null;program testing;program debugging program testing software fault tolerance reliability;program debugging;qa76 computer software	Previous models have investigated the impact upon diversity - and hence upon the reliability of fault-tolerant software built from 'diverse' versions - of the variation in 'difficulty' of demands over the demand space. These models are essentially static, taking a single snapshot view of the system. In this paper, we consider a generalisation in which the individual versions are allowed to evolve - and their reliability to grow - through debugging. In particular, we examine the trade-off that occurs in testing between, on the one hand, the increasing reliability of individual versions, and on the other hand the possible diminution of diversity.	debugging;fault-tolerant software;snapshot (computer storage)	Peter T. Popov;Bev Littlewood	2004	International Conference on Dependable Systems and Networks, 2004	10.1109/DSN.2004.1311896	non-regression testing;recovery testing;reliability engineering;embedded system;development testing;long-term support;fault tolerance;verification and validation;regression testing;real-time computing;fuzz testing;n-version programming;software performance testing;system integration testing;computer science;backporting;software reliability testing;software development;operating system;software construction;reliability;software testing;stress testing;software fault tolerance;software metric;statistics;software system	SE	-62.309100023877875	32.496541157865394	84569
4464b4a660c023a76642496211ed8d3ec370b57d	x9: an obfuscation resilient approach for source code plagiarism detection in virtual learning environments		In computer programming courses programming assignments are almost mandatory, especially in a virtual classroom environment. However, the source code plagiarism is a major issue in evaluation of students, since it prevents a fair assessment of their programming skills. This paper proposes an obfuscation resilient approach based on the static and dynamic source code analysis in order to detect and discourage plagiarized solutions. Rather than focusing on the programming language syntax which is susceptible to lexical and structural refactoring, an instruction and an execution flow semantic analysis is performed to compare the behavior of source code. Experiments were based on case studies from real graduation projects and automatic obfuscation methods, showing a high accuracy and robustness in plagiarism assessments.	algorithm;code refactoring;computer programming;experiment;lexicon;programming language;semantic analysis (compilers);sensor;static program analysis;system call;tracing (software);web service	Bruno Prado;Kalil A. Bispo;Raul Andrade	2018		10.5220/0006668705170524	data mining;source code;computer science;plagiarism detection;virtual learning environment;obfuscation	SE	-57.657087491194076	40.14062905044562	84947
913148e1f2b8df525d5a5c5d3bb96daa2e0132aa	identifying implicitly declared self-tuning behavior through dynamic analysis	software;self tuning behavior identification;programming paradigm;non autonomic element;system monitoring;actuators;data mining;programming model;adaptation model;tuning;autonomic computing programming;self management property;taxonomy;non autonomic element self tuning behavior identification dynamic analysis autonomic computing programming self management property self tuning parameter identification;tuning dynamic programming condition monitoring actuators control systems logic programming computer science pattern matching performance analysis reverse engineering;self tuning parameter identification;false positive;static analysis;programming;autonomic computing;dynamic analysis	Autonomic computing programming models explicitly address self management properties by introducing the notion of “Autonomic Element. However, most of currently developed systems do not employ autonomic self-managing programming paradigms. Thus, a current challenge is to find mechanisms to identify the self-tuning behavior and self-tuning parameters which have implicitly been declared using non-autonomic elements, and to expose them for monitoring or to an analysis framework. Static analysis, although it shows a good potential, it results in many false positives. In this paper, we provide a mechanism to identify the tuning parameters more accurately through dynamic analysis.	autonomic computing;dynamic program analysis;programming paradigm;self-management (computer science);self-tuning;static program analysis	Hamoun Ghanbari;Marin Litoiu	2009	2009 ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems	10.1109/SEAMS.2009.5069073	control engineering;real-time computing;simulation;computer science;programming paradigm;programming language;taxonomy;autonomic computing	SE	-53.80428238139184	40.68879890609864	84962
0f6400997f3c973dc530e88a031f2d631b0bc4db	a technique for mutation of java objects	software testing;software libraries;mutation analysis;real world software testing suites java object mutation mutation analysis fault insertion plausible errors standard transformations scalar values character data object semantics mutation operators support tools java library items commercial software reusable libraries mutation components common java libraries;program testing;software reusability;genetic mutations java testing programming profession software libraries runtime object oriented modeling computer science computer errors software tools;java;software reusability java program testing software libraries	Mutation analysis inserts faults into a program to create test sets that distinguish the mutant from the original program. Inserted faults must represent plausible errors. Standard transformations can mutate scalar values such as integers, floats, and character data. Mutating objects is an open problem, because object semantics are defined by the programmer and can vary widely. We develop mutation operators and support tools that can mutate Java library items that are heavily used in commercial software. Our mutation engine can support reusable libraries of mutation components to inject faults into objects that instantiate items from these common Java libraries. Our technique should be effective for evaluating real-world software testing suites.	commercial software;fault model;java;library (computing);mutation testing;mutator method;polymorphic engine;programmer;software testing;test set	James M. Bieman;Sudipto Ghosh;Roger T. Alexander	2001		10.1109/ASE.2001.989824	jsr 94;java concurrency;jar;computer science;theoretical computer science;software framework;component-based software engineering;software development;software engineering;java modeling language;strictfp;software construction;real time java;mutation testing;software testing;programming language;java;generics in java;scala;java applet;java annotation	PL	-56.87543408009886	37.1834038766661	85334
2fafa1ec5ee902d4f2f6b6835908fc50a3bdcbf8	securing opensource code via static analysis	computer program;software testing;program diagnostics;kernel;complexity theory;linux kernel;software development stage;secure computation;commercial software development process;automated tool;software development process;open source code security;linux kernel complexity theory programming security computer bugs;computer programs;public domain software;static code analysis;software development environment;software development;programming error elimination;sca tools;software development life cycle;linux;software tools;opensource;programming error detection;static analysis;security;computer bugs;software tools linux program diagnostics public domain software security of data software quality;programming;security of data;software quality;open source software;open source;programming error detection open source code security static code analysis computer programs automated tool software development life cycle software development stage sca tools software quality linux kernel commercial software development process programming error elimination;software development life cycle static code analysis opensource software testing	Static code analysis (SCA) is the analysis of computer programs that is performed without actually executing the programs, usually by using an automated tool. SCA has become an integral part of the software development life cycle and one of the first steps to detect and eliminate programming errors early in the software development stage. Although SCA tools are routinely used in proprietary software development environment to ensure software quality, application of such tools to the vast expanse of open source code presents a forbidding albeit interesting challenge, especially when open source code finds its way into commercial software. Although there have been recent efforts in this direction, in this paper, we address this challenge to some extent by applying static analysis on a popular open source project, i.e., Linux kernel, discuss the results of our analysis and based on our analysis, we propose an alternate workflow that can be adopted while incorporating open source software in a commercial software development process. Further, we discuss the benefits and the challenges faced while adopting the proposed alternate workflow.	commercial software;computer program;floor and ceiling functions;linux;open-source software;software development process;software quality;software release life cycle	Raghudeep Kannavara	2012	2012 IEEE Fifth International Conference on Software Testing, Verification and Validation	10.1109/ICST.2012.123	kpi-driven code analysis;development testing;verification and validation;codebase;software sizing;computer science;information security;package development process;backporting;software framework;software development;operating system;software engineering;software construction;software testing;software walkthrough;programming language;software development process;linux kernel;software quality;static program analysis	SE	-57.88802551259981	36.571704182905094	86068
71141c2f318950c2b4685d89787201ddfab58549	an extensive analysis of search-based techniques for predicting defective classes		Abstract In spite of constant planning, effective documentation and proper implementation of a software during its life cycle, many defects still occur. Various empirical studies have found that prediction models developed using software metrics can be used to predict these defects. Researchers have advocated the use of search-based techniques and their hybridized versions in literature for developing software quality prediction models. This study conducts an extensive comparison of 20 search-based techniques, 16 hybridized techniques and 17 machine-learning techniques amongst each other, to develop software defect prediction models using 17 data sets. The comparison framework used in the study is efficient as it (i) deals with the stochastic nature of the techniques (ii) provides a fair comparison amongst the techniques (iii) promotes repeatability of the study and (iv) statistically validates the results. The results of the study indicate promising ability of search-based techniques and their hybridized versions for predicting defective classes.		Ruchika Malhotra	2018	Computers & Electrical Engineering	10.1016/j.compeleceng.2018.08.017	real-time computing;data mining;documentation;empirical research;software bug;software;software quality;spite;computer science;software metric;data set	AI	-62.78269567021117	34.608903312953004	86627
4b310ce91d35561d37ef05fb8cfa7c2931a0d55c	program mining augmented with empirical properties		Due to the need to reengineer and migrating aging software and legacy systems, reverse engineering has started to receive some attention. It has now been established as an area in software engineering to understand the software structure, to recover or extract design and features from programs mainly from source code. The inference of design and feature from codes has close similarity with data mining that extracts and infers information from data. In view of their similarity, reverse engineering from program codes can be called as program mining. Traditionally, the latter has been mainly based on invariant properties and heuristics rules. Recently, empirical properties have been introduced to augment the existing methods. This article summarizes some of the work in this area.	code;data mining;heuristic (computer science);legacy system;reverse engineering;software engineering	Minh Ngo	2009			software;theoretical computer science;source code;reverse engineering;control flow graph;inference;invariant (mathematics);legacy system;heuristics;computer science	SE	-57.23915554007262	34.81641719508722	86998
09f7ae69eeb4b3bb98aa85fbb7b9f1501f65bd4f	design decisions in aspectmaps	visualization design decisions;visualization design decisions aspect oriented programming;aspectmaps tool design decisions aspectual source code aspectmaps visualization;program visualisation aspect oriented programming data visualisation;data visualisation;aspect oriented programming;program visualisation;visualization color software mice programming navigation complexity theory	AspectMaps is a visualization that shows the structure of aspectual source code. In its design and implementation we made a number of design decisions that we present and discuss in this text. This in the light of more than two years of using, extending and maintaining the AspectMaps visualization and tool. The purpose of this paper is to share our experience with other visualization designers and implementers, as an aid in the making of their design decisions.	documentation;open-source software	Johan Fabry;Alexandre Bergel	2013	2013 First IEEE Working Conference on Software Visualization (VISSOFT)	10.1109/VISSOFT.2013.6650548	software visualization;visual analytics;information visualization;visualization;aspect-oriented programming;computer science;multimedia;programming language;data visualization	Visualization	-54.66179702339111	35.22635604073304	87479
c2ecce7e6639abb08029930350911d56fc834dfc	mining process models and architectural components from test cases	program testing data mining learning artificial intelligence;automatic classification mining process models architectural components it service providers independent testing services test scripts behavioral architectural components supervised learning techniques;navigation testing uniform resource locators browsers registers entropy accuracy	Independent Testing of business applications in the enterprise is largely a manual exercise. Automation, if any, is observed in test management and to a lesser degree in test automation. Test design comprising test architecture, test strategy, test procedure and test data is largely a manual activity. It is a common practice to express test cases manually as test scripts that lay down the test procedure in terms of instructions to testers. As systems evolve, test scripts are modified and if need be, new test scripts written. During maintenance, comprehension of test scripts for the underlying architecture and test strategies is important to affect changes. The problem is acute for IT service providers, providing independent testing services, as test scripts are inherited from client organizations and original authors may no longer be available. In this paper we propose a novel idea of mining process models and behavioral architectural components from test scripts. We have carried out preliminary investigations into mining process models from test scripts and present a set of challenges that need further investigations. We also present a scheme to classify test steps as a first step to mine architectural components. Experimental results of automatic classification using supervised learning techniques indicate accuracy between 88% to 94% motivating us to carry out further investigations.	comparison and contrast of classification schemes in linguistics and metadata;mike lesser;process modeling;software system;subject matter expert turing test;subject-matter expert;supervised learning;system testing;test automation;test case;test data;test design;test management;test script;test strategy;test suite	Vipul Shah;Chetan Khadke;Sunjit Rana	2015	2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)	10.1109/ICSTW.2015.7107428	reliability engineering;simulation;engineering;operating system;software engineering;test suite;data mining;programming language;test script;test case;test management approach;test harness;random test generator	SE	-57.29653619277211	32.45412289804545	87489
0224dcf420c8eeca169867d448ac029f96fc12a7	coverage-biased random exploration of large models and application to testing	uniform exploration;random testing	This paper presents several randomised algorithms for generating paths in large models according to a given coverage criterion. Using methods for counting combinatorial structures, these algorithms can efficiently explore very large models, based on a graphical representation by an automaton or by a product of several automata. This new approach can be applied to random exploration in order to optimise path coverage and can be generalised to take into account other coverage criteria, via the definition of a notion of randomised coverage satisfaction. Our main contributions are a method for drawing paths uniformly at random in composed models, i.e. models that are given as products of automata, first without and then with synchronisation; a new efficient approach to draw paths at random taking into account some other coverage criterion. Experimental results show promising agreement with theoretical predictions and significant improvement over previous randomised approaches. This work opens new perspectives for future studies of statistical testing and model checking, mainly to fight the combinatorial explosion problem.	approximation;architectural pattern;automata theory;automaton;bayesian information criterion;brute-force search;circuit complexity;code coverage;european joint conferences on theory and practice of software;futures studies;gouraud shading;icse;ieee transactions on software engineering;issre;j.w. graham medal;lecture notes in computer science;model checking;on the fly;original chip set;random testing;randomized algorithm;randomness;reachability;satisfiability modulo theories;simulation;software quality assurance;software testing;solver;state space search;traverse;test data generation;text corpus;z3 (computer)	Alain Denise;Marie-Claude Gaudel;Sandrine-Dominique Gouraud;Richard Lassaigne;Johan Oudinet;Sylvain Peyronnet	2011	International Journal on Software Tools for Technology Transfer	10.1007/s10009-011-0190-1	random testing;computer science;theoretical computer science;algorithm	Logic	-50.2895331058453	37.18694785523423	87513
61f37c8c894d88371a7c6888dd2c18117b9a6a8c	hdpv: interactive, faithful, in-vivo runtime state visualization for c/c++ and java	program understanding;java programming;object viewer;data structure;use case;software visualization	Data structure visualization can increase program understanding in an educational context and help with visual debugging. Existing data structure visualization tools are limited in interactivity, providing mostly static views; flexibility, by restricting the layout strategies users can apply; scope, by focusing on only a single language; and fidelity, by abstracting away the actual runtime layout and size of a program's data. This paper presents the design and implementation of HDPV, a system for interactive, faithful, in-vivo runtime state visualization for native C/C++ programs and Java programs. We discuss how HDPV can be used for a number of use cases ranging from understanding simple, recursive programs, to understanding the visual effect of programming errors such as buffer overflows.	buffer overflow;c++;data structure;debugging;interactivity;java;program comprehension;recursion;video-in video-out;visual effects	Jaishankar Sundararaman;Godmar Back	2008		10.1145/1409720.1409729	use case;software visualization;information visualization;data structure;computer science;theoretical computer science;software engineering;database;programming language;entry point	HCI	-54.38804240534811	35.966305894106384	87664
ed4a5d1681cc7a4175287fdc3494723b0367b7ef	demomatch: api discovery from demonstrations		We introduce DemoMatch, a tool for API discovery that allows the user to discover how to implement functionality using a software framework by demonstrating the functionality in existing applications built with the same framework. DemoMatch matches the demonstrations against a database of execution traces called Semeru and generates code snippets explaining how to use the functionality. We evaluated DemoMatch on several case studies involving Java Swing and Eclipse RCP.	application programming interface;eclipse;java;software framework;swing (java);tracing (software)	Kuat Yessenov;Ivan Kuraj;Armando Solar-Lezama	2017		10.1145/3062341.3062386	software framework;computer science;eclipse;database;java	PL	-53.80781074857769	34.95276051595052	87698
b88bfdfc4af03a384b4d0b6e4332da34d1cb9786	a new software maintenance scenario based on refactoring techniques	program comprehension and maintenance;and forward;software maintenance;program comprehension;program transformation;maintenance engineering equations particle separators radiation detectors mathematical model context reverse engineering;program comprehension and maintenance program transformation reverse and forward refactorings;reverse and forward refactorings;software comprehension software maintenance scenario understanding oriented refactoring technique efficiency oriented refactoring technique software artifact maintenance execution program comprehension	This research line proposes the classification of refactoring techniques according to two opposite program properties: understanding and efficiency, being the former useful for maintenance while the latter for executing. Understanding-oriented refactoring and efficiency-oriented refactoring are considered the inverses of each other. Thus, through the application of the first sort of refactoring, understanding can be improved but efficiency can be affected. On the other hand, by applying the second sort of refactoring, efficiency can be improved but understanding can be damaged. So, the challenge to be faced here is to transform a software artifact through the application of a sequence of understanding-oriented refactoring, and to execute maintenance with the most appropriate version obtained. After that, we plan to restore its original efficiency by the application of the opposite sequence of refactorings, i.e. efficiency-oriented refactoring. In this way, a new maintenance scenario is outlined. Up to now, this ongoing research is being carried out in the functional setting.	artifact (software development);code refactoring;insertion sort;programmer;programming paradigm;reverse turing test;software maintenance;way to go	Gustavo Villavicencio	2012	2012 16th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2012.41	reliability engineering;computer science;systems engineering;engineering;software engineering;programming language;software maintenance;code refactoring	SE	-57.15842990704273	33.70192151775109	88089
49e8fc25bdb7e7e277c572b9fd24cd9711c1c457	managing code clones using dynamic change tracking and resolution	dynamic change;clipboard activity monitoring;source code maintainability;software maintenance;dynamic change tracking;user study;data mining;cloning;management strategy;code cloning management;cloning usability software maintenance technology management monitoring software performance software tools computer bugs;clipboard activity monitoring code cloning management dynamic change tracking dynamic resolution source code maintainability cloneboard program;dynamic resolution;robots;code clone;source code;software maintenance software development management;usability;computer bugs;programming;software development management;cloneboard program	Code cloning is widely recognized as a threat to the maintainability of source code. As such, many clone detection and removal strategies have been proposed. However, some clones can often not be removed easily so other strategies, based on clone management need to be developed. In this paper we describe a clone management strategy based on dynamically inferring clone relations by monitoring clipboard activity. We introduce CloneBoard, our Eclipse plug-in implementation that is able to track live changes to clones and offers several resolution strategies for inconsistently modified clones. We perform a user study with seven subjects to assess the adequacy, usability and effectiveness of CloneBoard, the results of which show that developers actually see the added value of such a tool but have strict requirements with respect to its usability.	clipboard (computing);code refactoring;duplicate code;eclipse;paste;plug-in (computing);qr code;requirement;usability testing;video game clone	Michiel de Wit;Andy Zaidman;Arie van Deursen	2009	2009 IEEE International Conference on Software Maintenance	10.1109/ICSM.2009.5306336	robot;programming;real-time computing;software bug;usability;computer science;systems engineering;software engineering;cloning;programming language;software maintenance;source code	SE	-57.392992136867406	37.10060624900035	88097
dbc0c771c539ab77179f47b83783c95124cd258f	a high level language-based computing environment to support production and execution of reliable programs	software;debugging;programming environment computing environment production execution reliable programs debugging tool logic errors spring spring architecture high level debugging;programming environments;programming language;ada;software reliability ada debugging high level language architecture programming language;runtime;computer architecture;springs;program debugging;software reliability program debugging programming environments;high level language;software reliability;springs debugging software reliability computer architecture software runtime;high level language architecture	The authors present an environment which involves a debugging tool to aid in the detection and removal at logic errors. The debugging tool is supported by a special architecture, named SPRING, which was originally developed for reliable execution of Ada or Pascal programs. Following an overview of the system backed up by SPRING, the details of SPRING architecture are described, and the implementation of high level debugging on the SPRING architecture is discussed. In conclusion, the trial could be seen as a step toward providing an advanced programming environment for the development of reliable software.	ada;backup;debugger;debugging;high-level programming language;integrated development environment;pascal;spring	Hideaki Tsubotani;Noriaki Monden;Minoru Tanaka;Tadao Ichikawa	1986	IEEE Transactions on Software Engineering	10.1109/TSE.1986.6312927	computer architecture;real-time computing;ada;computer science;operating system;algorithmic program debugging;programming language;debugging;high-level programming language;software quality	SE	-49.73145191205478	32.385762856820485	88433
ad8648022fc2aaeefdf32e0d1f52bcb590b1d76c	uml model execution via code generation		Simulating design models makes early verification of the software’s business logic possible. Model simulators can be implemented using an interpreter, but it provides limited runtime performance. This is only acceptable if the only use case of the tool is interactive model execution and debugging. If the model executor tool is to be used for automated regression testing, execution time becomes an important factor. In such cases generating code is a better option compared to interpretation. This paper documents our experience from an ongoing project which supports both the interactive and the automated model simulation use cases via code generation. It proposes an architecture and shows techniques we found useful in this setup, and reports on a freely available UML model simulator implemented along these lines.	business logic;code generation (compiler);code refactoring;compiler;debugger;debugging;executable uml;http 404;interactivity;interoperability;interpretation (logic);interpreter (computing);java;regression testing;run time (program lifecycle phase);simulation;toolchain;unified modeling language;user experience;version control;virtual machine	Gergely Dévai;Máté Karácsony;Boldizsár Németh;Róbert Kitlei;Tamás Kozsik	2015			programming language;code generation;uml tool;unified modeling language;applications of uml;computer science	SE	-53.14529877622765	35.53071023968716	88802
442fe472c80e7f8c2ae1b58680ae1f3a4f5a07d3	studying permission related issues in android wearable apps		Wearable devices are becoming increasingly popular; these devices host software that is known as wearable apps. Wearable apps could be packaged alongside handheld apps, hence they must be installed on the accompanying device (e.g., smartphone). This device dependency causes both apps to be also tightly coupled. Most importantly, when a wearable app is distributed by embedded it in a handheld app, Android Wear platform requires to include the wearable permission also in the handheld app which is error-prone. In this paper, we defined two permission issues related to wearable apps-namely permission mismatches and superfluous features. To study the permission related issues, we propose a technique to detect permission issues in wearable apps. We implement our technique in a tool called Permlyzer, which automatically detects these permission issues from an app's APK. We run Permlyzer on a dataset of 2,724 apps that have embedded wearable version and 339 standalone wearable app. Our result shows that I) 6% of wearable apps that request permissions are suffering from the permission mismatching problem; II) out of the apps that requires underlying features, 523 (52.4%) of handheld apps and 66 (80.5%) of standalone wearable apps have at least one superfluous feature; III) all the studied apps missed a declaration of underlying features for one or more of their permissions, which shows that developers may not know the mapping between the permissions they request and the hardware features. Additionally, in a survey of wearable app developers, all of the developers that responded mention that having a tool like Permlyzer, that detect permission related issues would be useful to them. Our results contribute to the understanding of permissions related issues in wearable apps, in particular, proposing a technique to detect permission mismatch and superfluous features.	android wear;app store;cognitive dimensions of notations;declaration (computer programming);embedded system;handheld game console;play store;smartphone;wearable computer;wearable technology	Suhaib Mujahid;Rabe Abdalkareem;Emad Shihab	2018	2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2018.00043	computer science;android wear;declaration;embedded system;systems engineering;wearable computer;software;android (operating system);wearable technology;permission;mobile device	SE	-54.66599774745091	42.629742810217316	89037
f52ba1313b4640e055b92c8067fdf3226ef4e2a4	incremental benchmarks for software verification tools and techniques	software verification;benchmark problem;software systems;object oriented software;tools and techniques	This paper proposes an initial catalog of easy-to-state, relatively simple, and incrementally more and more challenging benchmark problems for the Verified Software Initiative. These benchmarks support assessment of verification tools and techniques to prove total correctness of functionality of sequential object-based and object-oriented software. The problems are designed to help evaluate the state-of-the-art and the pace of progress toward verified software in the near term, and in this sense, they are just the beginning. They will allow researchers to illustrate and explain how proposed tools and techniques deal with known pitfalls and well-understood issues, as well as how they can be used to discover and attack new ones. Unlike currently available benchmarks based on “real-world” software systems, the proposed challenge problems are expected to be amenable to “push-button” verification that leverages current technology.	benchmark (computing);correctness (computer science);object-based language;push-button;software system;software verification	Bruce W. Weide;Murali Sitaraman;Heather K. Harton;Bruce M. Adcock;Paolo Bucci;Derek Bronish;Wayne D. Heym;Jason Kirschenbaum;David Frazier	2008		10.1007/978-3-540-87873-5_10	verification and validation;software sizing;software verification;computer science;package development process;backporting;software design;software framework;component-based software engineering;software development;software design description;software construction;data mining;software walkthrough;software analytics;resource-oriented architecture;software deployment;computer-aided software engineering;software metric;software system	PL	-58.814039720024205	36.85804365875642	89089
57b8f1b0d24e80d269cf7104c930d50f5da1a997	slicing web service-based software	dependence analysis;program control structures;slicing algorithm web services bpel ecfg dependence analysis bpdg;web service;control flow graph;web services service oriented architecture debugging flow graphs xml concurrent computing finance web and internet services forward contracts algorithm design and analysis;web service composition;internet;web services internet program control structures program debugging program slicing;bpel program dependence graph web service based software slicing internet program debugging static slicing bpel programs web service compositions extended control flow graph;web services;bpel;bpdg;ecfg;program debugging;program slicing;slicing algorithm;program dependence graph	Web services offer a brand-new mechanism for program interactions over the Internet. However, the new features such as heterogeneous, loose-coupling and distributed bring great challenge to its comprehension and debugging. In the paper, the method of static slicing BPEL programs in Web service compositions is proposed. At first, an extended control flow graph (ECFG) is constructed through a in-depth analysis on the new activity elements such as flow and pick. Then, the def-use relations between Web service units are computed. Based on the above results, BPEL program dependence graph is built by introducing the concept of synchronized edge. Subsequently, the static (backward / forward) slicing algorithm is discussed in details. While considering the slicing effects on a real-world Web service application, the proposed slicing technique can reduce the number of statements needing to be checked in understanding or debugging activities.	algorithm;business process execution language;control flow graph;debugging;interaction;internet;loose coupling;program dependence graph;program slicing;web service	Chengying Mao	2009	2009 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)	10.1109/SOCA.2009.5410460	web service;program slicing;computer science;database;programming language;law;world wide web	SE	-56.13382196300115	37.27959649801466	89327
2ab4c021e43d948c877f9d5b907ae13f647dca9e	a minimum proportional time redundancy based checkpoint selection strategy for dynamic verification of fixed-time constraints in grid workflow systems	fixed time constraint verification minimum proportional time redundancy checkpoint selection strategy dynamic verification grid workflow systems;fixed time;swinburne;checkpointing;formal verification;workflow system;redundancy runtime communications technology australia middleware large scale systems astrophysics finance insurance grid computing;middleware;grid computing;formal verification checkpointing grid computing middleware	In grid workflow systems, existing typical checkpoint selection strategies, which are used to select checkpoints for verifying fixed-time constraints at run-time execution stage, are not effective and/or efficient for fixed-time constraint verification because they often ignore some necessary checkpoints and select some unnecessary checkpoints. To improve such status, in this paper, we develop a new checkpoint selection strategy. Specifically, we first address a new concept of minimum proportional time redundancy which can be used to tolerate certain time deviation incurred by abnormal grid workflow execution. Then, we discuss relationships between minimum proportional time redundancy and fixed-time constraint consistency. Based on the relationships, we present our new strategy. With the strategy, we can avoid the omission of necessary checkpoints and the selection of excess unnecessary checkpoints. Consequently, our strategy is more effective and efficient for fixed-time constraint verification than the existing typical strategies. The final evaluation further demonstrates this result.	application checkpointing;exception handling;linkage (software);offset binary;software engineering;time deviation;transaction processing system;verification and validation	Jinjun Chen;Yun Yang	2005	12th Asia-Pacific Software Engineering Conference (APSEC'05)	10.1109/APSEC.2005.11	real-time computing;formal verification;computer science;operating system;middleware;database;distributed computing;programming language;grid computing	EDA	-49.80134338967884	39.7748630770234	89660
8c7d25ec69d4121841f1949c2512619297e3cf00	generating fully executable test suite automatically	protocols;telecommunication computing;automatic testing system testing protocols algorithm design and analysis computer science writing humans computer errors iso standards iec standards;predicate satisfiability fully executable test suite generation automatic test suite generation tugen system executable parametrizing algorithm satiability problem transition problem protocol testing conformance test;conformance testing;telecommunication computing protocols conformance testing	"""Only executable test suite generated automatically has practical usage. The authors devise and implement the algorithm of """"parametrizing and executizing"""" in the TUGEN system and discuss the results, limitations and its reason. Based on this work and the study in the executability of the transition and the satiability problem of the predicate, the authors further present and implement another algorithm called """"executable parametrizing"""" to overcome the deficiency of the previous one and further improve the practicability and efficiency of TUGEN. After analysis and comparison, the authors outline the focus of the future research."""	test suite	Yixin Zhao;Xia Yin;Jianping Wu	2001		10.1109/ICC.2001.937270	communications protocol;model-based testing;computer science;theoretical computer science;operating system;conformance testing;programming language;algorithm	NLP	-49.52507334154515	34.858508273015346	89971
042598a6a1d152efaa70f547aa54d7363a2f4a35	asserting reliable convergence for configuration management scripts	convergence;idempotence;system configuration scripts;testing;declarative language;devops;configuration management;puppet	The rise of elastically scaling applications that frequently deploy new machines has led to the adoption of DevOps practices across the cloud engineering stack. So-called configuration management tools utilize scripts that are based on declarative resource descriptions and make the system converge to the desired state. It is crucial for convergent configurations to be able to gracefully handle transient faults, e.g., network outages when downloading and installing software packages. In this paper we introduce a conceptual framework for asserting reliable convergence in configuration management. Based on a formal definition of configuration scripts and their resources, we utilize state transition graphs to test whether a script makes the system converge to the desired state under different conditions. In our generalized model, configuration actions are partially ordered, often resulting in prohibitively many possible execution orders. To reduce this problem space, we define and analyze a property called preservation, and we show that if preservation holds for all pairs of resources, then convergence holds for the entire configuration. Our implementation builds on Puppet, but the approach is equally applicable to other frameworks like Chef, Ansible, etc. We perform a comprehensive evaluation based on real world Puppet scripts and show the effectiveness of the approach. Our tool is able to detect all idempotence and convergence related issues in a set of existing Puppet scripts with known issues as well as some hitherto undiscovered bugs in a large random sample of scripts.	ansible;cloud engineering;configuration management;converge;devops;download;graceful exit;idempotence;image scaling;problem domain;software bug;state transition table;subject reduction	Oliver Hanappi;Waldemar Hummer;Schahram Dustdar	2016		10.1145/2983990.2984000	declarative programming;simulation;convergence;computer science;devops;database;distributed computing;software testing;configuration management;programming language;idempotence	PL	-55.97958625971662	40.67847022112291	90863
98d07321ad7feb02bae60d06dd34f33476a98e55	blog in web application: a software engineering perspective	logical view;weighted requirements;blog design;checklists;software estimation;page;web application;post;actors;blog software components;blog software integration;requirement checklists;blog software	Blogs are being extensively integrated in web applications, to facilitate communication and interaction with the users. Usually, freely available blog software is adapted and integrated into web application, with blog software being installed on web application server or server of blog software provider. Even though several blog software exist for integration, academically, there is no mention of requirement specification or design document for blog software for use during integration. Since not all the features are relevant for the user, there is a need for some kind of mechanism, so that, the user can identify the features that are to be included in blog software, during integration. In this paper, we present for blog software in web application: 1) a design; 2) weighted requirement checklists. The design helps the developer during creating and updating of blog software. The logical view of design displays interaction of entities and sub-entities with actors. For easing requirement selection for user, a weighted requirement checklist is presented here. A metric, software estimation, is defined for quantifying selected requirements. A case study of freely available blog software is presented to which estimation and design is applied.	application server;blog;cost estimation in software engineering;entity;requirement;server (computing);software design description;software publisher;web application	Karan Gupta;Anita Goel	2014	Int. J. Web Eng. Technol.	10.1504/IJWET.2014.064789	long-term support;verification and validation;web application;computer science;package development process;backporting;software design;social software engineering;software framework;component-based software engineering;software development;software design description;operating system;software construction;data mining;database;page;software walkthrough;programming language;resource-oriented architecture;software deployment;world wide web;software requirements;software metric;software system;software peer review	SE	-55.16836342262532	33.34941778083162	90955
716a28845ca3aa706f6e18b4bf752fc01aa9687d	splat: lightweight dynamic analysis for reducing combinatorics in testing configurable systems	automated testing;efficiency;configurable systems;software product lines	Many programs can be configured through dynamic and/or static selection of configuration variables. A software product line (SPL), for example, specifies a family of programs where each program is defined by a unique combination of features. Systematically testing SPL programs is expensive as it can require running each test against a combinatorial number of configurations. Fortunately, a test is often independent of many configuration variables and need not be run against every combination. Configurations that are not required for a test can be pruned from execution. This paper presents SPLat, a new way to dynamically prune irrelevant configurations: the configurations to run for a test can be determined during test execution by monitoring accesses to configuration variables. SPLat achieves an optimal reduction in the number of configurations and is lightweight compared to prior work that used static analysis and heavyweight dynamic execution. Experimental results on 10 SPLs written in Java show that SPLat substantially reduces the total test execution time in many cases. Moreover, we demonstrate the scalability of SPLat by applying it to a large industrial code base written in Ruby on Rails.	java;out-of-order execution;relevance;ruby on rails;run time (program lifecycle phase);scalability;software product line;static program analysis;texture splatting	Chang Hwan Peter Kim;Darko Marinov;Sarfraz Khurshid;Don S. Batory;Sabrina Souto;Paulo Barros;Marcelo d'Amorim	2013		10.1145/2491411.2491459	embedded system;real-time computing;computer science;efficiency;engineering drawing	SE	-58.74379136721056	38.48312200798206	91047
7b18c0b67c80f1210f354df19f3f872885c2f3b2	revisiting ai and testing methods to infer fsm models of black-box systems		Machine learning in the form of inference of state machine models has gained popularity in model-based testing as a means of retrieving models from software systems. By combining an old idea from machine inference with methods from automata testing in a heuristic approach, we propose a new promising direction for inferring black box systems that cannot be reset. Preliminary experiments show that this heuristic approach scales up well and outperforms more systematic approaches.	automata theory;black box;experiment;finite-state machine;heuristic;machine learning;model-based testing;software system	Roland Groz;Adenilso da Silva Simão;Nicolas Brémond;Catherine Oriat	2018	2018 IEEE/ACM 13th International Workshop on Automation of Software Test (AST)	10.1145/3194733.3194736	finite-state machine;real-time computing;computer science;software system;inference;black box (phreaking);machine learning;heuristic;popularity;artificial intelligence	SE	-61.22071949522819	37.34526268046341	91328
211a4d5582dba6dccd78e9d049507e4b3df17fba	a hybrid approach for control flow graph construction from binary code	program diagnostics;symbolic execution;program testing;control flow graph construction binary code analysis static analysis dynamic analysis smt symbolic execution;binary code analysis;jakstab tool hybrid approach control flow graph construction binary code binary code analysis cfg dynamic jump instructions instruction rewriting static analysis dynamic testing indirect jumps processing;binary codes abstracts testing flow graphs performance analysis educational institutions cities and towns;smt;control flow graph construction;static analysis;program testing program diagnostics;dynamic analysis	Binary code analysis has attracted much attention. The difficulty lies in constructing a Control Flow Graph (CFG), which is dynamically generated and modified, such as mutations. Typical examples are handling dynamic jump instructions, in which destinations may be directly modified by rewriting loaded instructions on memory. In this paper, we describe a PhD project proposal on a hybrid approach that combines static analysis and dynamic testing to construct CFG from binary code. Our aim is to minimize false targets produced when processing indirect jumps during CFG construction. To evaluate the potential of our approach, we preliminarily compare results between our method and Jakstab, a state-of-the-art tool in this field.	approximation;binary code;context-free grammar;control flow graph;dynamic testing;expect;mutation testing;rewriting;static program analysis	Minh Hai Nguyen;Thien Binh Nguyen;Thanh Tho Quan;Mizuhito Ogawa	2013	2013 20th Asia-Pacific Software Engineering Conference (APSEC)	10.1109/APSEC.2013.132	dead code;computer science;theoretical computer science;dynamic program analysis;programming language;static analysis;algorithm;control flow analysis;control flow graph	SE	-59.24288635035165	36.28258612480937	91475
8befeeb435ae12fbae1149db6ff984a1f8a5340f	special issue on information retrieval for program comprehension	information retrieval;program comprehension	Welcome to the special issue on information retrieval for program comprehension (IR4PC). IR4PC employs various interdisciplinary information search techniques to examine the properties of both existing (legacy) and newly created software. IR4PC is important for software reuse, software maintenance and evolution, and reverse engineering, just to mention a few areas. Back in the 1980s and early 1990s, much program comprehension involved representing program code as control and data flow graphs. Recognizing program constructs was performed by comparing flow graphs to a plan library of known constructs (e.g. Rich and Waters 1989). However, formal non-heuristic approaches to program comprehension have been shown to be NP-hard and their success was often illustrated only in toy domains (Woods and Yang 1996). For this reason, heuristic approaches acquired new importance. In the 21st century, much program comprehension research has focused on applying various information retrieval techniques (e.g. text mining, LSI, knowledge-based NL understanding) to software. These new IR4PC semantic measures examine informal information in the tokens within the software itself (e.g. identifier names, function names and variable names, code comments) as well as the natural language content in external documentation such as software requirements documents or software design documents. In the past, IR4PC techniques have been successfully applied to (among other areas) static concept location (using information derived from informal tokens together with structural information such as call graphs to locate code sections that are related to given concepts), to determining whether a particular software component is reusable, to dynamic search or software reconnaissance (examining informal tokens along execution traces of program executed with and without a particular feature), to developer identification (determining which developer is the best one to perform a particular task), to bug location Empir Software Eng (2009) 14:1–4 DOI 10.1007/s10664-008-9097-1	code reuse;comment (computer programming);component-based software engineering;dataflow;documentation;heuristic;identifier;information retrieval;integrated circuit;nl (complexity);np-hardness;natural language;program comprehension;requirement;reverse engineering;software design;software maintenance;software requirements;text mining;tracing (software);yang	Letha H. Etzkorn;Tim Menzies	2008	Empirical Software Engineering	10.1007/s10664-008-9097-1	computer science;programming language;information retrieval;human–computer information retrieval	SE	-59.0702002303879	32.42298503792556	91532
f5b9a30b5c3d43b39863244fe65fe57af8758067	a performance prediction model for google app engine using colored petri net		Recently, PaaS (Platform as a Service) type cloud services are widely accepted as platforms for various web applications. Google App engine (GAE) is one of the most popular ones of such services. However, as for mission critical applications, there are several obstacles to migrate into these cloud services like GAE. One of the crucial obstacles is that, while such applications require predictable stable response time, it is difficult to predicate or estimate it in these services, since only a little performance information on these cloud services is available. In addition, the structure of them is not opened to general public. Therefore, it seems difficult to build a performance estimation model based on the system structure. This paper proposes a Colored Petri Net (CPN) based performance prediction model or framework for GAE, based on the performance parameters obtained through the measurement by user written programs. The framework is build focusing on the application structure, which consists of a series of GAE APIs, and GAE works as a mechanism to produce the probabilistic process delays. These delays are modeled using the queuing theory which is embedded in the CPN model. The framework has high modularity to plug-in any kinds of applications easily.	google app engine;performance prediction;petri net	Sachi Nishida;Yoshiyuki Shinkawa	2014		10.1007/978-3-319-25579-8_15	simulation;computer science;data science;world wide web	Vision	-49.25313220363083	38.98785120900668	91586
702ac88b55150095a6cd63890e0069de04e1ef5d	refactoringng: a flexible java refactoring tool	refactoring;programming language;java programming;abstract syntax tree;semantic information;api evolution;software evolution;source code;java;competitive advantage	The Java programming language and the Java API evolve and this evolution certainly will continue in future. Upgrade to a new version of programming language or API is nowadays usually done manually. We describe a new flexible refactoring tool for the Java programming language that can upgrade the code almost automatically. The tool performs refactoring rules described in the special language based on the abstract syntax trees. Each rule consists of two abstract syntax trees: the pattern and the rewrite. First, we search for the pattern and then replace each pattern occurrence with the rewrite. Searching and replacement is performed on the abstract syntax trees that are built and fully attributed by the Java compiler. Complete syntactic and semantic information about the source code and flexibility in refactoring rules give the tool competitive advantage over most similar tools.	abstract syntax tree;application programming interface;code refactoring;java compiler;list of java apis;programming language;rewrite (programming)	Zdenek Tronícek	2012		10.1145/2245276.2231959	java api for xml-based rpc;jsr 94;java concurrency;application programming interface;computer science;software evolution;strategy pattern;theoretical computer science;java modeling language;strictfp;database;real time java;programming language;homoiconicity;java;java syntax;code refactoring;competitive advantage;generics in java;abstract syntax tree;scala;java applet;java annotation;source code	PL	-53.4043161636076	34.85350427728026	91806
efd7fe30276518b3fda85868f3673d963d5e2325	dependability of safety-critical systems: contribution of the synchronous approach	security of data software reliability safety critical software software tools electricity supply industry;development strategy;operational safety safety critical systems software dependability synchronous approach programmed protection systems software tools industrial development strategy critical software schneider electric;development tool;safety critical software;safety critical system;software tools;electricity supply industry;point of view;software reliability;software safety application software fault detection computer industry chemical industry software design hardware electronics industry industrial electronics statistical analysis;security of data	"""In order to develop 'kritica1""""programmedprotection systems, dedicated development tools are needed. The formalisms used should guarantee a high level of safety for the process being considered. The synchronous approach has numerous advantages which help it meet this target. This article presents the synchronous approach JFom an industrial point of view. This is the point of view behind the industrial development strategy for critical software within Schneider-Electric's Safety Electronics and Systems department. The article gives a historical outline and a simple definition of the synchronous approach (see section 3), followed by a (very simple) example of an application which demonstrates the advantages of the synchronous approach (see section 4). It does not give a detailed description of the techniques for proving Operational Safety (SEEA [l], statistical testing [2] and proof [3]), but rather aims to describe, as simply as possible, the characteristics of the synchronous approach related to Operational Safety."""	dependability;high-level programming language;point of view (computer hardware company);programming tool	D. Pérez	1995		10.1109/ISSRE.1995.497670	software security assurance;reliability engineering;personal software process;long-term support;verification and validation;software sizing;systems engineering;engineering;package development process;backporting;social software engineering;software development;software design description;software engineering;software construction;software walkthrough;software analytics;software deployment;software development process;software quality;software metric;software system;computer engineering;avionics software;software peer review	Embedded	-48.730950689406015	32.59674538704727	92142
282b087129cecad0b7877f8ebe0b1daa3f3b1fe3	deeptrans - a model-based approach to functional verification of address translation mechanisms	computer bugs operating systems hardware engines software testing laboratories system testing packaging protection mechanical factors;software testing;formal specification;functional verification;functional verification address translation mechanism deeptrans software package model based test generation modeling language ibm test generator;automatic testing;ibm test generator;model based approach;packaging;modeling language;mechanical factors;protection;formal verification;test case generation;engines;address translation mechanism;storage management chips formal verification simulation languages formal specification virtual storage automatic testing;storage management chips;system testing;test generation;model based testing;simulation languages;deeptrans software package;model based test generation;computer bugs;virtual storage;operating systems;hardware;generalization capability	We present a new test case generation technology, specifically targeted at verifying systems that include address translation mechanisms. The ever-growing demand for performance makes these mechanisms more complex, thereby increasing the risk of bugs and increasing the need for such technology. DeepTrans is a package that provides model-based test generation capabilities to verify translation mechanisms based on a modeling language. The modeling language includes constructs for describing the address translation process, commonly used translation resources, and architecture rules related to translation. DeepTrans is currently used by two different IBM test generators.		Allon Adir;Roy Emek;Yoav Katz;Anatoly Koyfman	2003		10.1109/MTV.2003.1250255	embedded system;packaging and labeling;computer architecture;model-based testing;software bug;formal verification;computer science;operating system;software engineering;formal specification;software testing;modeling language;programming language;system testing;functional verification	Logic	-49.03025212151111	33.06225207052917	92210
38870f5da46d653828c494982b6ef3552f231e90	vermeer: a tool for tracing and explaining faulty c programs		We present VERMEER, a new automated debugging tool for C. VERMEER combines two functionalities: (1) a dynamic tracer that produces a linearized trace from a faulty C program and a given test input; and (2) a static analyzer that explains why the trace fails. The tool works in phases that simplify the input program to a linear trace, which is then analyzed using an automated theorem prover to produce the explanation. The output of each phase is a valid C program. VERMEER is able to produce useful explanations of non trivial traces for real C programs within a few seconds. The tool demo can be found at http://youtu.be/E5lKHNJVerU.	automated theorem proving;debugger;debugging;static program analysis;tracing (software)	Daniel Schwartz-Narbonne;Chanseok Oh;Martin Schäf;Thomas Wies	2015	2015 IEEE/ACM 37th IEEE International Conference on Software Engineering			SE	-57.08938984441285	38.24323511026047	92235
1abba281f5397ee45e017c1411b3f03a9755c13f	mining internet-scale software repositories	software engineering;probabilistic model;lines of code;machine learning;source code;power law;information theoretic;open source software	Large repositories of source code create new challenges and opportunities for statistical machine learning. Here we first develop Sourcerer, an infrastructure for the automated crawling, parsing, and database storage of open source software. Sourcerer allows us to gather Internet-scale source code. For instance, in one experiment, we gather 4,632 java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, SLOC, and lexical containment distributions. We then develop and apply unsupervised author-topic, probabilistic models to automatically discover the topics embedded in the code and extract topic-word and author-topic distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the AUC metric to 0.84– roughly 10-30% better than previous approaches based on text alone. Supplementary material may be found at: http://sourcerer.ics.uci.edu/nips2007/nips07.html.	algorithm;code refactoring;database storage structures;embedded system;entity;information retrieval;information theory;internet;java;machine learning;open-source software;parsing;patch (computing);software engineering;software repository;source lines of code;sourceforge;structured programming;unsupervised learning	Erik Linstead;Paul Rigor;Sushil Krishna Bajracharya;Cristina V. Lopes;Pierre Baldi	2007			kpi-driven code analysis;statistical model;power law;computer science;software development;machine learning;software construction;data mining;database;source lines of code;world wide web;statistics;static program analysis;source code	ML	-60.68314570135425	39.43864503677396	92260
dab97f04928b7df3e56b722daf95496fa959763f	using lstms to model the java programming language		Recurrent neural networks (RNNs), specifically long-short term memory networks (LSTMs), can model natural language effectively. This research investigates the ability for these same LSTMs to perform next “word” prediction on the Java programming language. Java source code from four different repositories undergoes a transformation that preserves the logical structure of the source code and removes the code’s various specificities such as variable names and literal values. Such datasets and an additional English language corpus are used to train and test standard LSTMs’ ability to predict the next element in a sequence. Results suggest that LSTMs can effectively model Java code achieving perplexities under 22 and accuracies above 0.47, which is an improvement over LSTM’s performance on the English language which demonstrated a perplexity of 85 and an accuracy of 0.27. This research can have applicability in other areas such as syntactic template suggestion and automated bug patching.	java;programming language	Brendon Boldt	2017		10.1007/978-3-319-68612-7_31	first-generation programming language;very high-level programming language;perplexity;programming language;real time java;source code;natural language;scala;computer science;java	PL	-59.43917312336454	40.05490907783828	92380
f5b3ec92454623408ea5a53221e2222d13f17f76	experiences in testing automation of a family of functional- and gui-similar programs	regression testing;process integration	This article presents experiences in the automation of a testing process. The main goal is the unified testing of not only one program, but a whole family of programs. The family is characterized by a common functionality and therefore similar GUI interfaces. The testing process integrates extraction of the application specific data from its executable and the usage of the capture and replay testing technique. The testing of various me mbers of application family is driven by a unified, common script. The approach is illustrated by a case study. As a family of applications under test a set of RSS aggregators was used. A unified test RSSscript was developed and verified in test experiments. The results of functional, performance and regression testing are presented. The benefits and limitations of the solution are discussed.	central processing unit;computer performance;executable;experiment;fastest;graphical user interface testing;identifier;interaction;personal computer;rss;regression testing;resource hacker;software performance testing;system monitor;test automation;test script;unification (computer science)	Anna Derezinska;Tomasz Malek	2007	IJCSA		non-regression testing;test strategy;keyword-driven testing;black-box testing;regression testing;model-based testing;orthogonal array testing;software performance testing;white-box testing;manual testing;system integration testing;integration testing;computer science;operating system;functional testing;database;session-based testing;risk-based testing;software testing;programming language;test script;system testing;test management approach;algorithm;process integration	SE	-55.80701328303638	32.61598766862949	92625
4228ef36fca9925f2aa93284fa4d47d72932a6c9	xfindbugs: extended findbugs for aspectj	refactoring;aspect oriented software development;software systems;large scale;aspect oriented programming;design patterns;aspect oriented;static analysis;open source	Aspect-oriented software development (AOSD) is gaining popularity with the wider adoption of languages such as AspectJ. However, though the state-of-the-art aspect-oriented programming environment (such as AJDT in the Eclipse IDE) provides powerful capabilities to check the syntactic or grammar errors in AspectJ programs, it fails to detect potential semantic defects in aspect-oriented software systems. In this paper, we present XFindBugs, an eXtended FindBugs for AspectJ, to help programmers find potential bugs in AspectJ applications through static analysis. XFindBugs supports 17 bug patterns to cover common error-prone features in an aspect-oriented system, and integrates the corresponding bug detectors into the FindBugs framework. We evaluate XFindBugs on a number of large-scale open source AspectJ projects (306,800 LOC in total). In our evaluation, XFindBugs confirms 7 reported bugs and finds 257 previously unknown defects. Our experiment also indicates that the bug patterns supported in XFindBugs exist in real-world software systems, even for mature applications by experienced programmers.	aspect-oriented programming;aspect-oriented software development;aspectj;cognitive dimensions of notations;eclipse;findbugs;integrated development environment;open-source software;programmer;sensor;software bug;software system;static program analysis	Haihao Shen;Sai Zhang;Jianjun Zhao;Jianhong Fang;Shiyuan Yao	2008		10.1145/1512475.1512490	real-time computing;aspect-oriented programming;computer science;database;programming language	SE	-57.53329172530858	37.20367534311225	92831
16b0a31be7de330027dfffc3c9521d5d4424e4cd	verifying specifications with proof scores in cafeobj	software engineering computer bugs formal specifications equations information science electronic mail application software books silver humans;program diagnostics;software bugs;formal specification;public interest;proof scores;object oriented programming;program verification;software engineering;theorem proving;software requirements;software designs;theorem proving formal specification object oriented programming program debugging program diagnostics program verification;cafeobj language specification verification proof scores software engineering software bugs software requirements software designs program codes;cafeobj language;program debugging;specification verification;program codes	Verifying specifications is still one of the most important undeveloped research topics in software engineering. It is important because quite a few critical bugs are caused at the level of domains, requirements, and/or designs. It is also important for the cases where no program codes are generated and specifications are analyzed and verified only for justifying models of problems in real world. This paper gives a survey of our research activities in verifying specifications with proof scores in CafeOBJ. After explaining fundamental issues and importance of verifying specifications, an overview of CafeOBJ language, the proof score approach in CafeOBJ including its applications to several areas are given. This paper is based on our already published books or papers (Diaconescu and Futatsugi, 1998; Futatsugi et al., 2005), and refers to many of our related publications. Interested readers are invited to look into them	book;qr code;requirement;software bug;software engineering;verification and validation	Kokichi Futatsugi	2006	21st IEEE/ACM International Conference on Automated Software Engineering (ASE'06)	10.1109/ASE.2006.73	software bug;computer science;theoretical computer science;software engineering;formal specification;automated theorem proving;programming language;object-oriented programming;software requirements	SE	-59.66466893284703	32.42366471627397	92987
4227772de9d06b05830b87133c5da57f4c1a04ea	automatic testing tool for java adts		The JUnit testing tool is widely used to support the central XP concept of test first software development. We previously introduced an informal use of formal ADT semantics for guiding JUnit test method generation [16]. Called JAX (for Junit AXioms), the method does not require the programmer to learn and use any formal notation in order to gain some benefits from a formal method; no notations are involved other than Java. Experiments showed that manual application of the JAX method produces JUnit test suites that expose more coding errors than ad hoc JUnit test case development. The previous research emphasized a manual programming procedure as a way to work the formal benefits into practice more easily. In this paper we discuss AJAX (Automated JAX), extensions we have made to the JUnit Java classes to automate the application of JAX to various degrees. The tradeoff is that for the highest degree of automation, the programmer will need to learn and develop formalisms (ADT axioms); to ease this potential burden we use a programming notation (ML) for the formalism. Our JUnit extensions will be available for download. 1 Motivation and background Regression testing has long been recognized as necessary for having confidence in the correctness of evolving software. Programmers generally do not practice thorough tool-supported regression testing, however, unless they work within a significant industrial framework. JUnit [1,2,3] was developed to support the test first principle of the XP development process [4]; it has had the side effect of bringing the benefits of regression testing to the average programmer, including independent developers and students. JUnit is small, free, easy to learn and use, and has obtained a large user base in the brief time since its introduction in the XP community. JUnit and its supporting documentation are available at http://www.junit.org . The tool is useful in any software development process (not just in XP), and since it is available for more than 20 source languages, it is useful to more than just Java programmers. The basic JUnit testing methodology is simple and effective. However, it still leaves software developers to decide if enough test methods have been written to exercise all the features of their code thoroughly. The documentation supporting JUnit does not prescribe or suggest any systematic methodology for creating complete and consistent test suites. Instead it is designed to provide automated bookkeeping, accumulation, and execution support for the manner in which a programmer is already accustomed to developing test suites. We have developed and experimented with a systematic test suite generation method we call JAX (for Junit Axioms), based on Guttags algebraic semantics of Abstract Data Types (ADTs) [5,6,7]. Following the JAX method leads to JUnit test suites that completely cover the possible behaviors of a Java class. We refer to JAX as an informal formal method because, while it is based in the formal semantics of abstract data types, the Java programmer and JUnit user need use no formalisms beyond Java itself to take advantage of the guidance provided by the method. Our approach is simple and systematic. It will tend to generate more test methods than a programmer would by following the basic JUnit practice, but our preliminary experiments show this extra work produces test suites that are more thorough and more effective at uncovering defects. We will presume the reader has some familiarity with both axiomatic ADT semantics as well as the JUnit testing tool, and will not cover them in detail here.	abstract data type;advanced audio coding;ajax (programming);algebraic semantics (computer science);correctness (computer science);documentation;download;experiment;formal methods;hoc (programming language);information needs;junit;java;programmer;regression testing;software developer;software development process;test automation;test case;test suite;tree accumulation	Miguel Ángel Quintans Rojo;Virginia Escuder Cabañas	2005			white-box testing;non-regression testing;keyword-driven testing;regression testing;real-time computing;system integration testing;package development process;software construction;software reliability testing;systems engineering;computer science	SE	-56.16551249243911	36.580320616582135	93042
993a6cb4e76c0b55217123eaaa9b86ea27da5bfc	a systems engineering approach for crown jewels estimation and mission assurance decision making	threat susceptibility analysis;system engineering;mission impact assesment mission assurance cyber risk assessment cyber risk management cyber mission models bpm process models crown jewels analysis threat susceptibility analysis;probability;systems engineering decision making probability safety critical software stochastic processes;cyber risk management;mission impact assesment;stochastic simulation;systems engineering;risk management;computational modeling context risk management quality function deployment context modeling;business process model;probabilistic model;business process analysis systems engineering approach crown jewels estimation mission assurance decision making cyber incidents probabilistic model stochastic simulation it activity models;stochastic processes;business process analysis;process models;cyber risk assessment;safety critical software;risk assessment;process model;cyber mission models;mission assurance;crown jewels analysis;bpm	Understanding the context of how IT contributes to making missions more or less successful is a cornerstone of mission assurance. This paper describes a continuation of our previous work that used process modeling to allow us to estimate the impact of cyber incidents on missions. In our previous work we focused on developing a capability that could work as an online process to estimate the impacts of incidents that are discovered and reported. In this paper we focus instead on how our techniques and approach to mission modeling and computing assessments with the model can be used offline to help support mission assurance engineering. The heart of our approach involves using a process model of the system that can be run as an executable simulation to estimate mission outcomes. These models not only contain information about the mission activities, but also contain attributes of the process itself and the context in which the system operates. They serve as a probabilistic model and stochastic simulation of the system itself. Our contributions to this process modeling approach have been the addition of IT activity models that document in the model how various mission activities depend on IT supported processes and the ability to relate how the capabilities of the IT can affect the mission outcomes. Here we demonstrate how it is possible to evaluate the mission model offline and compute characteristics of the system that reflect its mission assurance properties. Using the models it is possible to identify the crown jewels, to expose the systems susceptibility to different attack effects, and evaluate how different mitigation techniques would likely work. Being based on an executable model of the system itself, our approach is much more powerful than a static assessment. Being based on business process modeling, and since business process analysis is becoming popular as a systems engineering tool, we also hope our approach will push mission assurance analysis tasks into a framework that allows them to become a standard systems engineering practice rather than the “off to the side” activity it currently is.	activity recognition;business process;continuation;crown group;executable;mission assurance;online and offline;process modeling;risk assessment;semantics (computer science);simulation;statistical model;systems engineering	Scott Musman;Mike Tanner;Aaron Temin;Evan Elsaesser;Lewis Loren	2011	2011 IEEE Symposium on Computational Intelligence in Cyber Security (CICS)	10.1109/CICYBS.2011.5949403	reliability engineering;simulation;systems engineering;engineering	SE	-60.398687433128124	46.075742236263466	93050
037db414ecacd7100ef2a52d3d8b6948a3362751	extending rule set for static code analysis in .net platform		This paper focuses on static code analysis tools for .NET platform. Static code analysis tools typically use a certain set of rules. In this paper we have proposed to implement four rules, which we consider important from our practical experience of software development. We have analysed existing popular static analysis tools for .NET platform in order to determine whether they have the rules equivalent to our proposed new rules. We have selected an open-source tool  Gendarme  for the implementation of these rules. We also investigated existing  Gendarme  rules and discovered that some of them could be improved. Therefore, we have proposed and implemented improvements for four existing  Gendarme  rules. In order to evaluate the improvements made in  Gendarme  rule set in a real-life environment, the source code of five open-source programs from sourceforge.net was tested using new and improved rules. Experiment results indicate that the improvements of existing  Gendarme  rules and the creation of new rules enable detection of more errors and can increase the quality of source code.  DOI:  http://dx.doi.org/10.5755/j01.itc.45.1.9341	.net framework;static program analysis	Jonas Ceponis;Algimantas Venckauskas;Lina Ceponiene;Andrius Zonys	2016	ITC	10.5755/j01.itc.45.1.9341	computer science;theoretical computer science;data mining;algorithm	Logic	-57.639921658990446	37.41110373332548	93214
2ce76fe0b5fb3ee1d4649866fddf6942061e35d7	a probability-based approach for measuring external attributes of software artifacts	software reliability;probabilistic model;software measurement;probability;measure theory;software engineering;probabilistic logic	The quantification of so-called external software attributes, which are the product qualities with real relevance for developers and users, has often been problematic. This paper introduces a proposal for quantifying external software attributes in a unified way. The basic idea is that external software attributes can be quantified by means of probabilities. As a consequence, external software attributes can be estimated via probabilistic models, and not directly measured via software measures. This paper discusses the reasons underlying the proposals and shows the pitfalls related to using measures for external software attributes. We also show that the theoretical bases for our approach can be found in so-called “probability representations,” a part of Measurement Theory that has not yet been used in Software Engineering Measurement. By taking the definition and estimation of reliability as reference, we show that other external software attributes can be defined and modeled by a probability-based approach.	axiomatic semantics;relevance;software engineering;statistical model	Sandro Morasca	2009	2009 3rd International Symposium on Empirical Software Engineering and Measurement	10.1145/1671248.1671254	reliability engineering;statistical model;software sizing;measure;computer science;software analysis pattern;software engineering;probability;data mining;probabilistic logic;software measurement;goal-driven software development process;software quality;software metric	SE	-60.4055869346315	32.58746573542329	93488
07b826413626de6c591f0c9641a36a32fc5013ce	practical emulation of software defects in source code	software fault injection;software defects;software faults	Software fault injection is a fundamental technique to evaluate the behavior of systems in presence of software defects. However, in spite of the considerable number of software fault injection proposals, the lack of readily available and practical tools persists as the most serious limitation to the generalized use of software fault injection. This paper describes a fault injection tool designed to simplify the software fault injection process. The tool modifies the source code of target programs by altering the abstract syntax tree and producing software patches automatically. Several key challenges were addressed in the development of the tool. Firstly, we formally describe the software fault injection operators, which has been neglected in previous proposals. Secondly, we propose a comprehensive test suite to verify the correctness of software fault injectors. Thirdly, we improve the performance of the fault injection process by compiling only the file in which a defect is injected and linking/installing that file. Finally, we examine the implications of dealing with macros, and the limitations that source code macros impose on software fault injection. The results of injecting software faults into the Apache web server, as an example of possible target, show how these challenges have been addressed.	abstract syntax tree;algorithm;c++;compiler;correctness (computer science);emulator;experiment;failure cause;fault injection;fault model;formal specification;make;parse tree;patch (computing);quality engineering;server (computing);software bug;software deployment;test suite;verification and validation;web server	Goncalo Pereira;Raul Barbosa;Henrique Madeira	2016	2016 12th European Dependable Computing Conference (EDCC)	10.1109/EDCC.2016.19	reliability engineering;embedded system;software visualization;verification and validation;real-time computing;software sizing;software verification;computer science;package development process;backporting;software framework;software development;software design description;operating system;software engineering;software construction;distributed computing;software testing;programming language;software maintenance;computer security;software fault tolerance;software system;avionics software	SE	-57.6221081460456	37.200011039199744	93545
1d263181c65cae36988728850f6585a9e590f45a	why, when, and what: analyzing stack overflow questions by topic, type, and code	web sites;high level languages;statistical analysis;stack overflow question analysis;codes;encoding;identifiers;programming concepts;question concepts;question types;string class;topic modeling analysis;java;cascading style sheets;programming;humanoid robots	Questions from Stack Overflow provide a unique opportunity to gain insight into what programming concepts are the most confusing. We present a topic modeling analysis that combines question concepts, types, and code. Using topic modeling, we are able to associate programming concepts and identifiers (like the String class) with particular types of questions, such as, “how to perform encoding”.	documentation;identifier;integrated development environment;programming language;stack overflow;topic model	Miltiadis Allamanis;Charles A. Sutton	2013	2013 10th Working Conference on Mining Software Repositories (MSR)		embedded system;programming;entropy;computer science;humanoid robot;artificial intelligence;theoretical computer science;operating system;machine learning;mathematics;predictive modelling;cascading style sheets;programming language;java;high-level programming language;algorithm;measurement	SE	-52.74334340168719	38.25042323306675	93576
7caa46fdebfa0b619564faba1657002b40a69bb4	conceptual modeling of coincident failures in multiversion software	software reliability decision theory fault tolerant computing probability;tolerancia falta;developpement logiciel;modelizacion;probability;conceptual modeling;program choice;independently developed program versions;optimal method;simultaneous failure;conceptual model;software fault tolerance;independent failure behavior;ingenieria logiciel;indexing terms;software engineering;constraints conceptual modeling coincident failures multiversion software independently developed program versions precise duality input choice program choice diverse methodologies simultaneous failure independent failure behavior methodological diversity decision outcomes optimal method;modelisation;computer programming;fault tolerance reliability engineering operations research battery powered vehicles diversity methods stochastic processes councils glands software reliability cities and towns;multiversion software;fault tolerant computing;applications programs computers;stochastic processes;desarrollo logicial;fault tolerance;decision theory;precise duality;software development;defaillance;genie logiciel;methodological diversity;failure modes;coincident failures;input choice;failures;stochastic model;fiabilite logiciel;distribution functions;fiabilidad logicial;decision outcomes;diverse methodologies;modeling;software reliability;modelo estocastico;fallo;modele stochastique;tolerance faute;constraints	Recent work by Eckhardt and Lee shows that independently developed program versions will fail dependently: specifically that simultaneous failure of several is greater than would be the case under true independence. We show there is a precise duality between input choice and program choice in this model and consider a generalization in which different versions may be developed using diverse methodologies. The use of diverse methodologies is shown to decrease the probability of simultaneous failure of several versions. Indeed, it is theoretically possible to obtain versions which exhibit better than independent failure behavior. We try to formalize the notion of methodological diversity by considering the sequence of decision outcomes which comprises a methodology. We show that diversity of decisions implies likely diversity of behavior for the different versions developed under such forced diversity. For certain 1-out-of-n systems we obtain an optimal method for allocating diversity between versions. For 2-outof-3 systems there seem to be no simple optimality results which do not depend on constraints which cannot be verified in practice.		Bev Littlewood;Douglas R. Miller	1989	IEEE Trans. Software Eng.	10.1109/32.58771	reliability engineering;stochastic process;computer science;systems engineering;engineering;conceptual model;operating system;software engineering;statistics	SE	-62.1831746396425	32.38815194244894	93855
1411563b41b2e6bf960b4121a3956bdd4d4cfb17	towards better summarizing bug reports with crowdsourcing elicited attributes		Recent years have witnessed the growing demands for resolving numerous bug reports in software maintenance. Aiming to reduce the time testers/developers take in perusing bug reports, the task of bug report summarization has attracted a lot of research efforts in the literature. However, no systematic analysis has been conducted on attribute construction which heavily impacts the performance of supervised algorithms for bug report summarization. In this study, we first conduct a survey to reveal the existing methods for attribute construction in mining software repositories. Then, we propose a new method named Crowd-Attribute to infer new effective attributes from the crowdgenerated data in crowdsourcing and develop a new tool named Crowdsourcing Software Engineering Platform to facilitate this method. With Crowd-Attribute, we successfully construct 11 new attributes and propose a new supervised algorithm named Logistic Regression with Crowdsourced Attributes (LRCA). To evaluate the effectiveness of LRCA, we build a series of large scale data sets with 105,177 bug reports. Experiments over both the public data set SDS with 36 manually annotated bug reports and new large-scale data sets demonstrate that LRCA can consistently outperform the state-of-the-art algorithms for bug report summarization.	algorithm;bug tracking system;crowdsourcing;feature vector;information privacy;logistic regression;software engineering;software maintenance;software repository	He Jiang;Xiaochen Li;Zhilei Ren;Jifeng Xuan;Zhi Jin	2018	CoRR		task analysis;reliability engineering;automatic summarization;software bug;mathematics;software;data science;mining software repositories;software maintenance;crowdsourcing	SE	-62.43912938542877	38.170976205421745	93928
ee43d8867fdfdafedd8f191373161cc33092cf39	prioritizing junit test cases in absence of coverage information	software;regression testing;junit test cases prioritization;java programming;call graph;probability density function;software engineering java program testing;testing;data mining;test case prioritization;software engineering;jupta approach;indexes;program testing;fault detection;debugging process;coverage information;ta prioritization techniques;test ability estiimation;software testing electronic equipment testing java software debugging fault detection costs laboratories educational technology computer science education computer science;ta prioritization techniques junit test cases prioritization coverage information java software development debugging process regression testing test ability estiimation jupta approach;java software development;java;open source	Better orderings of test cases can detect faults in less time with fewer resources, and thus make the debugging process earlier and accelerate software delivery. As a result, test case prioritization has become a hot topic in the research of regression testing. With the popularity of using the JUnit testing framework for developing Java software, researchers also paid attention to techniques for prioritizing JUnit test cases in regression testing of Java software. Typically, most of them are based on coverage information of test cases. However, coverage information may need extra costs to acquire. In this paper, we propose an approach (named Jupta) for prioritizing JUnit test cases in absence of coverage information. Jupta statically analyzes call graphs of JUnit test cases and the software under test to estimate the test ability (TA) of each test case. Furthermore, Jupta provides two prioritization techniques: the total TA based technique (denoted as JuptaT) and the additional TA based technique (denoted as JuptaA). To evaluate Jupta, we performed an experimental study on two open source Java programs, containing 11 versions in total. The experimental results indicate that Jupta is more effective and stable than the untreated orderings and Jupta is approximately as effective and stable as prioritization techniques using coverage information at the method level.	debugging;eclipse;experiment;full configuration interaction;junit;java;open-source software;plug-in (computing);regression testing;rendering (computer graphics);software deployment;static program analysis;test case	Lingming Zhang;Ji Zhou;Dan Hao;Lu Zhang;Hong Mei	2009	2009 IEEE International Conference on Software Maintenance	10.1109/ICSM.2009.5306350	call graph;reliability engineering;database index;probability density function;regression testing;real-time computing;computer science;software engineering;database;software testing;programming language;java;fault detection and isolation	SE	-60.442563137775196	35.703336802624534	94280
1362437a96de121ec243e28a0f950bf1e7320185	documenting framework behavior	application development;traces of method calls;object oriented framework;object oriented programming;specifying oo frameworks;requirement specification	Frameworks [Johnson, Foote, Sparks] promise to dramatically reduce the time and effort needed to develop complete applications. A framework F for a given application area typically provides thecontrol flowamong the various methods of the various classes. A developer who wants to use F to develop a complete application A need only provide the code for the various (pure) virtual methods1 in the various abstract base classes of F . But in order for the promise of frameworks to be truly realized, the framework F must include documentation that provides the application developer with suitable information about F . In the absence of such documentation, the application developer will be forced to go through the code of F to extract the information, thereby substantially negating the advantages that the use of frameworks was supposed to provide. In the rest of this article we point out the need for a new approach to the formal specification 2 of the behavior of frameworks, sketch a possible approach, and briefly indicate how an application developer could combine such a specification of framework behavior with appropriate information about the code he supplies to arrive at a specification of the entire application.	formal specification;list of minor characters in the matrix series;software documentation	Neelam Soundarajan	2000	ACM Comput. Surv.	10.1145/351936.351950	real-time computing;computer science;programming language;object-oriented programming;rapid application development	SE	-51.69816696580657	34.87376593244821	94676
d04fd44bf21b16e6a15b3cca0c688ada1d6b38ce	code understanding through program transformation for reusable component identification	information resources;software testing;formal specification;formally reusable components;reusable component identification;software maintenance;program comprehension;program transformation;software reuse code understanding program transformation reusable component identification post delivery software activities software maintenance reverse engineering reuse redevelopment assistant r sup 3 a reverse engineering program comprehension techniques human knowledge reusable components semantic interface analysis formally reusable components;software maintenance reverse engineering information resources software tools humans computer science recycling costs hardware software testing;program comprehension techniques;formal specification reverse engineering software maintenance software reusability software tools;code understanding;software reusability;r 3 a;post delivery software activities;software tools;humans;computer science;reusable component;reusable components;software reuse;recycling;semantic interface analysis;reverse engineering reuse redevelopment assistant;reverse engineering;hardware;human knowledge	Code understanding is the most essential step in all post delivery software activities such as software maintenance and reuse. In the Reverse-engineering Reuse Redevelopment Assistant (R/sup 3/ A), a tool aimed at providing a comprehensive approach for all post delivery software activities, code understanding has been addressed by reverse engineering through program transformation. The paper proposes a method to deal with this problem and discusses in detail how program transformation techniques, program comprehension techniques and the role of human knowledge are integrated into R/sup 3/ A, i.e., how they are used during reverse engineering to recognise reusable components, and how they are used by semantic interface analysis to represent formally reusable components. The experiments conducted strongly suggest the proposed method is a practical approach to software reuse.	component-based software engineering;program transformation	Hongji Yang;Paul Luker;William C. Chu	1997		10.1109/WPC.1997.601283	computer science;systems engineering;engineering;software engineering;software construction;formal specification;software testing;knowledge;software maintenance;recycling;reverse engineering;static program analysis;computer engineering	SE	-54.73471833987943	32.386040869198254	94706
55738d0314154b7cfb1fdd1be19e017774510542	a factorial experimental evaluation of automated test input generation - - java platform testing in embedded devices	datavetenskap datalogi	Background. When delivering an embedded product, such as a mobile phone, third party products, like games, are often bundled with it in the form of Java MIDlets. To verify the compatibility between the runtime platform and the MIDlet is a labour-intensive task, if input data should be manually generated for thousands of MIDlets. Aim. In order to make the verification more efficient, we investigate four different automated input generation methods which do not require extensive modeling; random, feedback based, with and without a constant startup sequence. Method. We evaluate the methods in a factorial design experiment with manual input generation as a reference. One original experiment is run, and a partial replication. Result. The results show that the startup sequence gives good code coverage values for the selected MIDlets. The feedback method gives somewhat better code coverage than the random method, but requires real-time code coverage measurements, which decreases the run speed of the tests. Conclusion The random method with startup sequence is the best trade-off in the current setting.	embedded system;java	Per Runeson;Per Heed;Alexander Westrup	2011		10.1007/978-3-642-21843-9_18	embedded system;real-time computing;simulation;computer science;engineering;software engineering	SE	-58.826814562520184	38.37126558481315	94798
548cd2c6778c3bb3a0c913a34fb117b4fd7b80ba	automated class testing using threaded multi-way trees to represent the behaviour of state machines	software testing;automatic testing;state machine;object oriented;software development;test methods	"""Extensive test data is required to demonstrate that """" few """" errors exist in software. If the process of software testing could be carried out automatically, testing efficiency would increase and the cost of software development would be significantly reduced. In this paper, a tool for detecting errors in object oriented classes is proposed. The approach uses a state-based testing method. The method utilises state machines in order to produce threaded multi-way trees, which are referred to as inspection trees. Inspection trees can be used to generate test cases and parse test results files. This allows us to determine whether the classes under test contain errors. The algorithms for the creation of inspection trees and the examination of the test result file using an inspection tree are described in the paper."""	algorithm;file binder;finite-state machine;modal logic;parsing;regression testing;resultant;sensor;software development;software testing;test automation;test case;test data	Bor-Yuan Tsai;Simon Stobart;Norman Parrington;Ian M. Mitchell	1999	Ann. Software Eng.	10.1023/A:1018915027830	non-regression testing;keyword-driven testing;reliability engineering;regression testing;test data generation;model-based testing;orthogonal array testing;white-box testing;manual testing;classification tree method;integration testing;computer science;engineering;software reliability testing;software development;software engineering;software construction;smoke testing;software testing;system under test;finite-state machine;test method;programming language;object-oriented programming;data-driven testing;engineering drawing;test management approach	SE	-56.541960584565764	32.699921653772925	95044
74cb5002d62553922cf52ab542f3b6b6ee788bc6	automatic categorization of software libraries using bytecode		Automatic software categorization is the task of assigning categories or tags to software libraries in order to summarize their functionality. Correctly assigning these categories is essential to ensure that relevant libraries can be easily retrieved by developers from large repositories. Current categorization approaches rely on the semantics reflected in the source code, or use supervised machine learning techniques, which require a set of labeled software as a training data. These approaches fail when such information is not available. We propose a novel unsupervised approach for the automatic categorization of Java libraries, which uses the bytecode of a library in order to determine its category. We show that the approach is able to successfully categorize libraries from the Apache Foundation Repository.	categorization;java;library (computing);machine learning;software categories;software repository;supervised learning;unsupervised learning	Javier Escobar-Avila	2015	2015 IEEE/ACM 37th IEEE International Conference on Software Engineering		software visualization;computer science;package development process;software framework;software development;software construction;data mining;database;cluster analysis;programming language;software analytics;empirical research;software quality;software system	SE	-61.12356469037552	39.417415194958956	95184
37610b432bc936b276664b67e5d89db87728d7b6	a tool for clustering metamodel repositories		Over the last years, several model repositories have been proposed in response to the need of the MDE community for advanced systems supporting the reuse of modeling artifacts. Modelers can interact with MDE repositories with different intents ranging from merely repository browsing, to searching specific artifacts satisfying precise requirements. The organization and browsing facilities provided by current repositories is limited since they do not produce structured overviews of the contained artifacts, and the categorization mechanisms (if any) are based on manual activities. When dealing with large numbers of modeling artifacts, such limitations increase the effort related to both managing and reusing artifacts stored in model repositories. By focusing on metamodels management, in this paper we propose a clustering tool for automatically organizing stored metamodels and provide users with repository overviews as, for instance, the application domains covered by the available metamodels. The approach has been implemented and integrated in the MDEForge repository. I. MOTIVATION AND GOALS The increasing adoption of Model-Driven Engineering (MDE) [19] in business organizations led to the need of gathering artifacts in model repositories [11]. Several model repositories (see [12], [13], [15], [16], [11] just to mention a few) have been introduced in the past decade. Among them metamodel zoos (as for instance the Ecore Zoo2) hold metamodels, which are typically categorized to improve search and/or browse operations. However, locating relevant information in a vast repository is intrinsically difficult, because it requires domain experts to manually annotate all metamodels in the repository with accurate metadata [4]: an activity that is time consuming and prone to errors and omissions. In fact, acquiring knowledge about a software artifact is a challenging task: it is estimated that up to 60% of software maintenance is spent on comprehension [5]. In order to mitigate the difficulties related to the manual categorization of metamodels, we propose a clustering tool for metamodel repositories: an unsupervised procedure, which automatically organizes metamodels into clusters. Mutually similar artifacts are grouped together depending on a proximity measure, whose definition can be given according to specific search and browsing requirements. The tool is based on agglomerative hierarchical clustering [14] and explores well-known proximity measures as well as metamodel-specific ones, each providing different browsing characteristics. 1This research was supported by the EU through the ModelBased Social Learning for Public Administrations (Learn Pad) FP7 project (619583). 2ATLAS Ecore Zoo: http://www.emn.fr/z-info/atlanmod/index.php/Zoos elationalDBSchema MySQL	artifact (software development);browsing;categorization;cluster analysis;galaxy zoo;hierarchical clustering;metamodeling;model-driven engineering;model-driven integration;mysql;organizing (structure);requirement;software maintenance	Francesco Basciani;Davide Di Ruscio;Juri Di Rocco;Alfonso Pierantonio;Ludovico Iovino	2015			data mining;metamodeling;software mining;cluster analysis;computer science	SE	-56.30015158124823	33.590023695853766	95230
3bb1d7f5c7631502aeb4c1d058a7cfdc4bada360	collection and analysis of software metrics from the eiffel class hierarchy			class hierarchy;eiffel;software metric	Christine Mingins;Bohdan Durnota;Glen Smith	1993				SE	-53.493948338823024	32.3476443134935	95369
7660b4b5d3f3fb33547c4821403bd0a96006a0dc	test-suite prioritisation by application navigation tree mining	databases;software;calculators;testing;data mining;navigation;fault diagnosis	Software tend to evolve over time and so does the test-suite. Regression testing is aimed at assessing that the software evolution did not compromise the working of the existing software components. However, as the software and consequently the test-suite grow in size, the execution of the entire test-suite for each new build becomes infeasible. Techniques like test-suite selection, test-suite minimisation and test-suite prioritisation have been proposed in literature for regression testing. Whilst all of these techniques are essentially an attempt to reduce the testing effort, test-suite selection and minimisation reduce the test-suite size whereas test-suite prioritisation provides a priority order of the test cases without changing the test-suite size. In this work, we focus on test-suite prioritisation. Recently, techniques from data mining have been used for test-suite prioritisation which consider the frequent pairs of interaction among the application interaction patterns. We propose test-Suite prioritisation by Application Navigation Tree mining (t-SANT). First, we construct an application navigation tree by way of extracting both tester and user interaction patterns. Next, we extract frequent sequences of interaction using a sequence mining algorithm inspired from sequential pattern mining. The most frequent longest sequences are assumed to model complex and most frequently used work-flows and hence a prioritisation algorithm is proposed that prioritises the test cases based on the most frequent and longest sequences. We show the usefulness of the proposed scheme with the help of two case studies, an online book store and calculator.	algorithm;component-based software engineering;data mining;online book;regression testing;sequential pattern mining;software evolution;test case;test suite;tree (data structure);while	Muhammad Muzammal	2016	2016 International Conference on Frontiers of Information Technology (FIT)	10.1109/FIT.2016.045	reliability engineering;real-time computing;engineering;data mining	SE	-60.22245043914272	36.329958877208504	95493
a5f86455833434c5826db393e83d82eab4d4acef	instrumentation of intermediate code for runtime verification	formal specification;runtime verification;instruments runtime condition monitoring computerized monitoring aerodynamics software engineering automatic control formal specifications computer science error correction;program verification;software engineering;runtime monitoring;integrity constraints;data flow analysis;integrity constraints intermediate code runtime verification runtime monitoring runtime behavior program execution dynamic monitoring object code level automated instrumentation formal specification constraint violations software engineering;data flow analysis program verification formal specification;university of texas at el paso	Runtime monitoring is aimed at ensuring correct runtime behavior with respect to specified constraints. It provides assurance that properties are maintained during a given program execution. The Dynamic Monitoring with Integrity Constraints (DynaMICs) approach is a runtime monitoring system under development at the University of Texas at El Paso. The focus of the paper is on the identification of instructions at the object-code level that require instrumentation for monitoring. Automated instrumentation is desirable because it can reduce errors introduced by humans, it provides finer control over monitoring, and it allows greater control over instrumentation. The paper also discusses two other technologies associated with DynaMICs: the elicitation and formal specification of properties and constraint; and tracing property or constraint violations to the software engineering artifacts from which the constraints and properties were derived.	data integrity;formal specification;object code;runtime verification;software engineering	Ann Q. Gates;Oscar Mondragon;Mary Payne;Steve Roach	2003		10.1109/SEW.2003.1270727	reliability engineering;real-time computing;computer science;software engineering;data-flow analysis;data integrity;formal specification;runtime verification;programming language	SE	-48.74867154162927	35.976253593888366	95798
0fc2192b9bd3552afb24fed092e466b5422a250e	private api access and functional mocking in automated unit test generation	software;software testing;generators;search problems;tools;java	Not all object oriented code is easily testable: Dependency objects might be difficult or even impossible to instantiate, and object-oriented encapsulation makes testing potentially simple code difficult if it cannot easily be accessed. When this happens, then developers can resort to mock objects that simulate the complex dependencies, or circumvent object-oriented encapsulation and access private APIs directly through the use of, for example, Java reflection. Can automated unit test generation benefit from these techniques as well? In this paper we investigate this question by extending the EvoSuite unit test generation tool with the ability to directly access private APIs and to create mock objects using the popular Mockito framework. However, care needs to be taken that this does not impact the usefulness of the generated tests: For example, a test accessing a private field could later fail if that field is renamed, even if that renaming is part of a semantics-preserving refactoring. Such a failure would not be revealing a true regression bug, but is a false positive, which wastes the developer's time for investigating and fixing the test. Our experiments on the SF110 and Defects4J benchmarks confirm the anticipated improvements in terms of code coverage and bug finding, but also confirm the existence of false positives. However, by ensuring the test generator only uses mocking and reflection if there is no other way to reach some part of the code, their number remains small.	application programming interface;code coverage;code refactoring;download;encapsulation (networking);evosuite;experiment;fm broadcasting;fault detection and isolation;java;mock object;mockito;open-source software;reflection (computer programming);simulation;software regression;test automation;test data generation;unit testing	Andrea Arcuri;Gordon Fraser;René Just	2017	2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)	10.1109/ICST.2017.19	real-time computing;simulation;computer science;operating system;software engineering;software testing;programming language;java;computer security	SE	-60.17879080483663	37.18115577406044	95851
6bd1008213637f6c4f6ab7667af573c399467444	distributed desk checking	reading technique;software reviews;concurrency;code review;synchronization	Desk checking is known to be an effective reading technique for early detection of sequential program errors. This paper discusses how to extend desk checking for concurrent and distributed programs. In addition to exponential possible schedules, concurrent and distributed programs have execution states that include more than one process. The new distributed desk-checking technique supports the selection of schedules and execution states to be reviewed. The cross-product functional coverage technique assists in the selection process. Schedule selection guidelines that facilitate early detection and coverage are introduced. It is demonstrated that code inspection can be applied effectively to large industrial applications using the selection mechanism introduced by this technique. Industrial pilots show that distributed desk checking is an effective early error-detection review technique. Copyright c © 2006 John Wiley & Sons, Ltd.	code reading;concurrency control;control flow;distributed computing;fagan inspection;fault tolerance;forward error correction;high-level programming language;hyperlink;john d. wiley;model checking;nahum stutchkoff;program counter;real-time computing;real-time transcription;schedule (computer science);scheduling (computing);simulation;software bug;state (computer science);static program analysis;test plan;time complexity	Amiram Hayardeny;Shachar Fienblit;Eitan Farchi	2007	Concurrency and Computation: Practice and Experience	10.1002/cpe.1067	synchronization;parallel computing;real-time computing;code review;concurrency;computer science;operating system;database;distributed computing;programming language	SE	-56.76202270457563	39.63537256363119	95923
351dbefbb60a2d6eeb9d2cfbde2532834109faa0	minestrone: testing the soup		Software development using type-unsafe languages (e.g., C and C++) is a challenging task for several reasons, security being one of the most important. Ensuring that a piece of code is bug or vulnerability free is one of the most critical aspects of software engineering. While most software development life cycle processes address security early on in the requirement analysis phase and refine it during testing, it is not always sufficient. Therefore the use of commercial security tools has been widely adopted by the software industry to help identify vulnerabilities, but they often have a high false-positive rate and have limited effectiveness. In this paper we present MINESTRONE, a novel architecture that integrates static analysis, dynamic confinement, and code diversification to identify, mitigate, and contain a broad class of software vulnerabilities in Software Of Uncertain Provenance (SOUP). MINESTRONE has been tested against an extensive test suite and showed promising results. MINESTRONE showed an improvement of 34.6% over the state-of-the art for memory corruption bugs that are commonly exploited.	c++;diversification (finance);experiment;fault injection;memory corruption;requirements analysis;software bug;software development process;software engineering;software industry;software of unknown pedigree;static program analysis;test case;test suite;valgrind;vulnerability (computing)	Azzedine Benameur;Nathan S. Evans;Matthew C. Elder	2013			systems engineering;software;architecture;software development;memory corruption;test suite;diversification (marketing strategy);systems development life cycle;requirements analysis;computer science	SE	-58.430596728258664	36.14559041210985	95963
a5ae0ff1cf011c10e6c6b30593982f57b9038d00	quality assurance in performance: evaluating mono benchmark results	developpement logiciel;quality assurance;evaluation performance;performance evaluation;perforation;surveillance;evaluacion prestacion;localization;endommagement;localizacion;deterioracion;aseguracion calidad;software architecture;vigilancia;localisation;monitoring;desarrollo logicial;software development;monitorage;damaging;monitoreo;assurance qualite;qualite logiciel;software quality;architecture logiciel	Performance is an important aspect of software quality. To prevent performance degradation during software development, performance can be monitored and software modifications that damage performance can be reverted or optimized. Regression benchmarking provides means for an automated monitoring of performance, yielding a list of software modifications potentially associated with performance changes. We focus on locating individual modifications as causes of individual performance changes and present three methods that help narrow down the list of modifications potentially associated with a performance change. We illustrate the entire process on a real world project.	benchmark (computing);elegant degradation;outline of software;software development;software quality	Tomas Kalibera;Lubomír Bulej;Petr Tuma	2005		10.1007/11558569_20	quality assurance;software architecture;simulation;internationalization and localization;computer science;engineering;software development;software engineering;software quality	Metrics	-62.453114622123806	36.58405336472533	96299
3ab72a99a4a99f18e19423a9b9004e61f6709976	temporally robust software features for authorship attribution	plagiarism;software;coding style evolution;authorship attribution;adversarial information retrieval authorship attribution software forensics coding style source code programming assignment programming task;adversarial information retrieval coding style evolution authorship attribution;information retrieval;performance of systems;materials;adversarial information retrieval;accuracy;software forensics;robustness plagiarism application software software maintenance writing information retrieval algorithm design and analysis phase detection law computer applications;source code;entropy;encoding;coding style;programming;programming assignment;programming task	Authorship attribution is used to determine the creator of works among many candidates, playing a vital role in software forensics, authorship disputes and academic integrity investigations. The evolving coding style of individuals may degrade the performance of systems that attribute authorship of source code, and has not been previously studied. This paper uses a collection of six programming assignments with guaranteed relative timestamps from 272 students to examine evolution of coding style.We find that the problem domain of the software developed has a large affect on the ability to attribute authorship, and that coding style does change over time regardless of the requirements that are coded.The outcomes suggest that it takes at least three programming tasks for coding style to settle, and that at least one piece of code in the same problem domain as the code to classify is necessary for accurate authorship attribution.In the final part of the paper we analyze low level code features to discover simple features that appear immune to evolution of coding style, and use them to improve effectiveness of our system from 79% to82% (p≪0.01, z-test).	problem domain;programmer;programming style;requirement;robustness (computer science);simple features;software engineer;software forensics;stylometry;temporal logic	Steven Burrows;Alexandra L. Uitdenbogerd;Andrew Turpin	2009	2009 33rd Annual IEEE International Computer Software and Applications Conference	10.1109/COMPSAC.2009.85	programming;entropy;computer science;artificial intelligence;theoretical computer science;data mining;database;adversarial information retrieval;accuracy and precision;computer security;encoding;source code	SE	-62.71193417934846	40.50858215521519	96552
c5d6e911de7a99bfe1770e2acd4de462b730397f	interpreting coverage information using direct and indirect coverage	software;integrated circuits testing software gold measurement heuristic algorithms conferences;measurement;testing;gold;source code software program testing software quality;heuristic algorithms;directly covered code coverage information indirect coverage test suite quality uncovered code indirectly covered code insufficiently tested code direct coverage;integrated circuits;conferences	Because of the numerous benefits of tests, developers often wish their applications had more tests. Unfortunately, it is challenging to determine what new tests to add in order to improve the quality of the test suite. A number of approaches, including numerous coverage criteria, have been proposed by the research community to help developers focus their limited testing resources. However, coverage criteria often fall short of this goal because achieving 100% coverage is often infeasible, necessitating the difficult process of determining if a piece of uncovered code is actually executable, and the criteria do not take into account how the code is covered. In this paper, we propose a new approach for interpreting coverage information, based on the concepts of direct coverage and indirect coverage, that address these limitations. We also presents the results of an empirical study of 17 applications that demonstrate that indirectly covered code is common in real world software, faults in indirectly covered code are significantly less likely to be detected than faults located in directly covered code, and indirectly covered code typically clusters at the method level. This means that identifying indirectly covered methods can be effective at helping testers improve the quality of their test suites by directing them to insufficiently tested code.	code coverage;executable;test suite	Chen Huo;James Clause	2016	2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)	10.1109/ICST.2016.20	gold;development testing;real-time computing;simulation;code review;computer science;engineering;operating system;software engineering;data mining;software testing;code coverage;measurement;static program analysis	SE	-61.732801044412824	34.91481917516208	96553
65ed9dcef662be71be10b470112073b6eef15c0f	exposing complex bug-triggering conditions in distributed systems via graph mining	directed graphs;distributed system;security of data data mining directed graphs distributed processing;distributed processing;data mining;graph mining;data mining fault diagnosis software debugging;software debugging;greengps tool complex bug triggering condition distributed system graph mining software bug pop mine tool corner case bugs diagnosis minimal causal directed acyclic graph bug triggering distributed event pattern vcp tool chord tool;computer bugs software entropy data mining delay usa councils computer science;security of data;fault diagnosis	Software bugs in distributed systems are notoriously hard to find due to the large number of components involved and the non-determinism introduced by race conditions between messages. This paper introduces Pop Mine, a tool for diagnosing corner-case bugs by finding the minimal causal directed acyclic graph (DAG) of events, spanning multiple processes, which captures a bug-triggering condition. Being based on causal order, a global notion of time is not required in uncovering bug-triggering distributed event patterns. Bug triggering event DAGs can be identified by comparing execution graphs from successful runs to those where bug manifestations were observed, and exposing the minimal discriminative event DAGs that may be responsible for the problem. This is a significant extension to prior debugging tools, in that prior work considered much simpler bug-triggering conditions such as single events, event sets, or ordered chains of events. To the authors' knowledge, this is the first paper that considers bug-triggering conditions in the form of distributed event graphs. To prove the effectiveness of our approach, we applied our tool to VCP, Chord and GreenGPS and diagnosed bugs. We also present performance analysis results to demonstrate the scalability of our approach.	algorithm;causal filter;corner case;debugging;directed acyclic graph;distributed computing;embedded system;file spanning;message passing interface;profiling (computer programming);race condition;scalability;software bug;structure mining	Eunsoo Seo;Mohammad Maifi Hasan Khan;Prasant Mohapatra;Jiawei Han;Tarek F. Abdelzaher	2011	2011 International Conference on Parallel Processing	10.1109/ICPP.2011.62	parallel computing;real-time computing;directed graph;computer science;theoretical computer science;operating system;data mining;distributed computing	HPC	-59.06473145035587	37.83897623786631	96621
5abb1473aa6c5d762434279266f91fa89d6280d1	drawing the line: teaching the semantics of binary class associations	object-oriented design;patterns;customizable code pattern tool;associations;binary class association;java;uml;recent research;object oriented design	This poster presents the use of a customizable code pattern tool to provide support for teaching of binary class associations in object-oriented design and programming based on recent research on the semantics of associations.		James H. Paterson;John Haddow;Ka Fai Cheng	2008		10.1145/1384271.1384404	unified modeling language;real-time computing;computer science;object-oriented design;database;pattern;programming language;java	PL	-51.82905194361136	33.12949258602511	97254
b38df3483ad2937e685ecf9320cbca2b61bb83e1	[research paper] the case for adaptive change recommendation		As the complexity of a software system grows, it becomes increasingly difficult for developers to be aware of all the dependencies that exist between artifacts (e.g., files or methods) of the system. Change impact analysis helps to overcome this problem, as it recommends to a developer relevant source-code artifacts related to her current changes. Association rule mining has shown promise in determining change impact by uncovering relevant patterns in the system's change history. State-of-the-art change impact mining algorithms typically make use of a change history of tens of thousands of transactions. For efficiency, targeted association rule mining focuses on only those transactions potentially relevant to answering a particular query. However, even targeted algorithms must consider the complete set of relevant transactions in the history. This paper presents ATARI, a new adaptive approach to association rule mining that considers a dynamic selection of the relevant transactions. It can be viewed as a further constrained version of targeted association rule mining, in which as few as a single transaction might be considered when determining change impact. Our investigation of adaptive change impact mining empirically studies seven algorithm variants. We show that adaptive algorithms are viable, can be just as applicable as the start-of-the-art complete-history algorithms, and even outperform them for certain queries. However, more important than the direct comparison, our investigation lays necessary groundwork for the future study of adaptive techniques and their application to challenges such as the on-the-fly style of impact analysis that is needed at the GitHub-scale.	adaptive algorithm;adaptive filter;association rule learning;atari;software system	Sydney Pugh;David W. Binkley;Leon Moonen	2018	2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)	10.1109/SCAM.2018.00022	theoretical computer science;software system;data mining;change impact analysis;computer science;association rule learning;database transaction	SE	-58.80458604587387	38.70584567682834	97386
256a5977e72fb1b11ddc5487361a4ddc8a7edd73	automated detection of design patterns	software maintenance;design pattern;source code	Detection of instances of design patterns is useful for the software maintenance. This paper proposes a new framework for the automated detection of instances of design patterns. The framework uses a reengineering tool to analyze C++ source codes. Prolog is used to induce instances of design patterns, the elemental design patterns are also used as a intermediate results for the final target (design patterns). Two-phrased query makes the discovery process more efficient.	c++;code refactoring;design pattern;elemental;prolog;software maintenance	Zhixiang Zhang;Qinghua Li	2003		10.1007/978-3-540-24680-0_110	software visualization;computer architecture;computer science;software construction;design pattern;programming language;software maintenance;source code	SE	-56.610368661559505	35.3304813418715	97414
bf330a4f6dd158b84a89e7f240f495af1fbe8cf6	program concept recognition	libraries;program understanding;debugging;abstract concepts;logic programming libraries software engineering debugging displays data mining software prototyping prototypes problem solving information retrieval;programming language;software prototyping;concept model;information retrieval;programming knowledge based systems learning systems;prototypes;lower level concepts;concept recognition rules;data mining;software engineering;program code;domain knowledge;domain knowledge programming language knowledge program understanding abstract concepts program code automated concept recognition concept model concept recognition rules lower level concepts;learning systems;logic programming;programming language knowledge;displays;automated concept recognition;programming;knowledge based systems;problem solving	Program understanding can be greatly assisted by automating the recognition of abstract concepts present in the program code. This paper describes an approach to automated concept recognition and its implementation. In the approach, we use a concept model and a library of concept recognition rules to describe what the concepts are and how to recognize them from lower-level concepts. Programming language knowledge as well as domain knowledge are both used to aid the recognition of abstract concepts. 1. Program Understanding Program understanding, the understanding of program source code, is essential to many software engineering and re-engineering activities such as maintenance, debugging, verification, renovation, migration, and design recovery. Many tools have been developed to facilitate program understanding. With very few exceptions, however, these tools are little more than browsers that present different text and graphical views of the code. For example, VIFOR [Raj901 graphically displays the calling and cross reference relations in FORTRAN programs. ESW, a COBOL re-engineering toolset developed by VIASOFT, extracts structure charts from PERFORM statements and produces condensed source listings to highlight “structurally significant” parts of programs. The program understanding tool prototyped at IBM [Cle881 supports cross referencing, calling relation, and control flow and data flow Dresentations. More Browsing-based tools may assist the user in exploring hidden properties of programs. But it remains the user’s responsibility to reason about the meaning of the programs. Moreover, as the user builds understanding of individual concepts, browsing tools cannot help himher generalize the knowledge to automate the understanding of similar concepts in the future. In this paper, we present an approach to software understanding based on an automatic identification of abstract concepts in the code. The following section gives a definition of concept recognition and discusses how concepts are recognized. Then, in Section 3, we introduce a language for specifying abstract concepts. In Section 4, we describe how our prototype concept recognition system uses the concept specifications to recognize abstract concepts. The last section summarizes the approach and discusses the related work. 2. Concept Recognition What do we mean when we talk about understanding a program? Syntactically, a program is a sequence of text strings. But semantically, it contains many types of concepts, generally including language concepts and abstract concepts. Language concepts are variables, declarations, modules, statements, and etc. that are defined by the coding language. Abstract concepts represent language-independent ideas of computation and problem solving methods. Abstract concepts can be further classified into: recently, LaSSIE [DBSB91] demonstrated the need for capturing architectural and conceptual information (in addition to code-level information) to support program understanding. LaSSE provides a classification-based representation and retrievai framework for the user to encode, browse, and query the information about high-level concepts; but these concepts must be obtained and encoded manually. programming concepts that include general coding architectural concepts that are associated with intersuch as operating sysstrategies, data structures, and algorithms; faces Lo architectural tems, wansaction monitors, networks, databases, etc.; and functions implemented in the code. domain concepts that are application or business logic 0-8186-2880-4/92 SCn.00 Q 1992 IEEE 216 class MOVE-STATEMENT isa VERB-STATEMENT SOURCE-DATA-ITEM : DATA-ITEM; TARGET-IDENTIFIERS : seq(IDENTIF1ER) MOVE-STATEMENT : := “MOVE“ ( (”CORR“ I “CORRESPONDING”) 1 SOURCE-DATA-ITEM “TO” TARGET-IDENTIFIERS+ The class definition specifies that the MOVE-STATEattributes: (1) the SOURCE-DATA-ITEM which holds a IDENTIFIERS which holds a sequence of IDENTIFIER concepts. The grammar rule specifies how the MOVESTATEMENT concepts can be recognized from keywords MENT be a SUbcktSS Of the VERB-STATEMENT. It h a two Concept Of ChSS DATA-ITEM, and (2) the TARGET(MOVE, CORR, etc.) and DATA-ITEM and IDENTIFIER Concepts. The hgUage Concepts VERB-STATEMENT, DATA-ITEM, and IDENTIFIER aS Well as the rules Specifying how to recognize them are also part of the COBOL language model and its grammar. Figure 1 shows a part of an AST with a MOVE-STATEMENT concept. Triangles in the figure represent omitted AST subtrees.	algorithm;automatic identification and data capture;browsing;business logic;cobol;chart;computation;control flow;cross-reference;data structure;database;dataflow;debugging;encode;fortran;graphical user interface;high- and low-level;identifier;language model;language-independent specification;problem solving;program comprehension;programming language;prototype;software engineering;statement (computer science);string (computer science);tree (data structure);variable (computer science)	Wojtek Kozaczynski;Jim Q. Ning;Tom Sarver	1992		10.1109/KBSE.1992.252919	natural language processing;programming;computer science;theoretical computer science;knowledge-based systems;prototype;abstraction;programming language;debugging;logic programming;domain knowledge	SE	-52.430110804585354	37.39847971649228	97480
c3356a8593c46bfe08c0f8f24657f1b9115aaa7b	smartic: towards building an accurate, robust and scalable specification miner	specification mining;selected works;filtering errors;clustering traces;software evolution;bepress;time to market;information system;software specification	"""Improper management of software evolution, compounded by imprecise, and changing requirements, along with the """"short time to market"""" requirement, commonly leads to a lack of up-to-date specifications. This can result in software that is characterized by bugs, anomalies and even security threats. Software specification mining is a new technique to address this concern by inferring specifications automatically. In this paper, we propose a novel API specification mining architecture called SMArTIC Specification Mining Architecture with Trace fIltering and Clustering) to improve the accuracy, robustness and scalability of specification miners. This architecture is constructed based on two hypotheses: (1) Erroneous traces should be pruned from the input traces to a miner, and (2) Clustering related traces will localize inaccuracies and reduce over-generalizationin learning. Correspondingly, SMArTIC comprises four components: an erroneous-trace filtering block, a related-trace clustering block, a learner, and a merger. We show through experiments that the quality of specification mining can be significantly improved using SMArTIC."""	application programming interface;cluster analysis;experiment;requirement;scalability;software bug;software evolution;tracing (software)	David Lo;Siau-Cheng Khoo	2006		10.1145/1181775.1181808	software requirements specification;real-time computing;engineering;software evolution;software engineering;data mining;functional specification;database;information system	SE	-57.93178421898467	33.70497595094738	97755
fedd924955be80685253577cc3e9ed5271b33f32	towards api-specific automatic program repair		The domain of Automatic Program Repair (APR) had many research contributions in recent years. So far, most approaches target fixing generic bugs in programs (e.g., off-by-one errors). Nevertheless, recent studies reveal that about 50% of real bugs require API-specific fixes (e.g., adding missing API method calls or correcting method ordering), for which existing APR approaches are not designed. In this paper, we address this problem and introduce the notion of an API-specific program repair mechanism. This mechanism detects erroneous code in a similar way to existing APR approaches. However, to fix such bugs, it uses API-specific information from the erroneous code to search for API usage patterns in other software, with which we could fix the bug. We provide first insights on the applicability of this mechanism and discuss upcoming research challenges.	arp spoofing;algorithm;apache portable runtime;application programming interface;off-by-one error;sequential pattern mining	Sebastian Nielebock	2017	2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)		computer science;software bug;automation;theoretical computer science;data mining;maintenance engineering;software;benchmark (computing)	SE	-59.59644288107591	38.22624634066482	97812
49955c368c74bb522b1308a86e02c5817603da04	verifying networked programs using a model checker extension	cache storage;peer processes;probability density function;data stream;software fault tolerance cache storage client server systems formal verification java peer to peer computing scheduling;software fault tolerance;client server systems;testing;sockets;data mining;data streams;yarn java application software sockets web server software testing operating systems dispatching data structures libraries;servers;webdav client networked programs verification model checker extension software failures execution schedule i o cache java model checker cache module data streams peer processes;java model checker;formal verification;model checking;scheduling;webdav client;schedules;software failures;cache module;i o cache;peer to peer computing;network programming;networked programs verification;model checker extension;execution schedule;java	Model checking finds failures in software by exploring every possible execution schedule. Until recently it has been mainly applied to stand-alone applications. This paper presents the I/O-cache, an extension for a Java model checker to support networked programs. It contains a cache module, which captures data streams between a target process and its peer processes. This demonstration also shows how we found a defect in a WebDAV client with a model checker and our extension.	cpu cache;input/output;java;model checking;software bug;webdav	Watcharin Leungwattanakit;Cyrille Artho;Masami Hagiya;Yoshinori Tanabe;Mitsuharu Yamamoto	2009	2009 31st International Conference on Software Engineering - Companion Volume	10.1109/ICSE-COMPANION.2009.5071036	model checking;probability density function;real-time computing;formal verification;schedule;computer science;operating system;database;software testing;data stream mining;programming language;computer network programming;java;scheduling;software fault tolerance;server	SE	-52.86373262647395	39.117332361413716	98234
3be016a516585b565f1f2effdaba5539d652e53d	security mechanisms for multi-user collaborative cax	collaboration;multi user;computer aided applications;security	Advances in computing technologies have provided the needed tools to transform traditional single user architecture Computer Aided Applications (CAx) to multi-user collaborative CAx architectures that supports simultaneous concurrency. To allow for a successful deployment in a corporate enterprise environment, the multi-user CAx architecture will require reliable security mechanisms to ensure protection of intellectual property. In this paper we propose mechanisms for securing user access and communications in a multi-user collaborative CAx software system. The proposed security solution is currently under development and is being tested on a collaborative multi-user version of a popular commercial CAD application.	computer-aided design;concurrency (computer science);multi-user;software deployment;software system	Francis N. Mensah;Chia-Chi Teng	2013		10.1145/2512209.2512226	engineering;software engineering;computer security;computer engineering	Arch	-54.725896162269215	45.212775969366014	98313
e7f31b5e2c61c48e4c97f2ee04bf1347efdcf192	testing-based compiler validation for synchronous languages	formalism;sychronous languages;program verification computers;testing;compilers;proving;compilation;compiler validation;programming languages	In this paper we present a novel lightweight approach to validate compilers for synchronous languages. Instead of verifying a compiler for all input programs or providing a fixed suite of regression tests, we extend the compiler to generate a test-suite with high behavioral coverage and geared towards discovery of faults for every compiled artifact. We have implemented and evaluated our approach using a compiler from Lustre to C.	compiler;lustre;regression testing;test suite;verification and validation	Pierre-Loïc Garoche;Falk Howar;Temesghen Kahsai;Xavier Thirioux	2014		10.1007/978-3-319-06200-6_19	computer architecture;compiler;parallel computing;compiler correctness;interprocedural optimization;computer science;compiler construction;optimizing compiler;software testing;compilation error;programming language;inline expansion;intrinsic function;functional compiler;formalism	PL	-57.93179696055727	36.78840720980355	98425
44568fc58b6420c4027af0426c514e9fb10643d2	optimizing a structural constraint solver for efficient software checking	software testing;program diagnostics;software checking;bounded exhaustive testing;binary search trees;structural constraint solver;bounded exhaustive testing software testing;specification based testing;test cases number reduction;testing;structural complexity;arrays;indexes;dynamic analysis technique;symbolic execution;program testing;magnitude reduction structural constraint solver software checking static analysis technique dynamic analysis technique test cases number reduction;constraint solving;optimization;magnitude reduction;static analysis;static analysis technique;program testing program diagnostics;constraint optimization automatic testing software engineering software testing marine technology data structures xml constraint theory performance evaluation computer bugs;java	Several static analysis techniques, e.g., symbolic execution or scope-bounded checking, as well as dynamic analysis techniques, e.g., specification-based testing, use constraint solvers as an enabling technology. To analyze code that manipulates structurally complex data, the underlying solver must support structural constraints. Solving such constraints can be expensive due to the large number of aliasing possibilities that the solver must consider. This paper presents a novel technique to selectively reduce the number of test cases to be generated. Our technique applies across a class of structural constraint solvers. Experimental results show that the technique enables an order of magnitude reduction in the number of test cases to be considered.	aliasing;experiment;heuristic (computer science);ibm notes;image scaling;integer factorization;mathematical optimization;optimizing compiler;solver;static program analysis;symbolic execution;test case;test suite;xojo	Junaid Haroon Siddiqui;Darko Marinov;Sarfraz Khurshid	2009	2009 IEEE/ACM International Conference on Automated Software Engineering	10.1109/ASE.2009.52	computer science;theoretical computer science;software engineering;software testing;programming language;concolic testing;algorithm	SE	-59.43784833006897	36.22142454563536	98573
ce1f3e819bbf9b209afab455c2310c28f14cee7d	accurate profiling of oracles for self-checking time-constrained embedded software	verification;program diagnostics;formal specification;national instruments pxi veristand oracle profiling self checking time constrained embedded software software debugging software testing recovery operations restart operations assertion checker development process design validation out of the specification behaviors;instruments time measurement real time systems accuracy embedded software monitoring;performance profiling;program testing formal specification program debugging program diagnostics;program testing;program debugging;real time software;embedded software	One way to ensure the correct execution of embedded software is to keep debugging and testing even after shipping of the application, complemented with recovery/restart operations. In this context, the oracles, i.e., assertions and checkers, that have been widely used in the development process for design validation, can be deployed again in the final product. The application will use the oracles to monitor itself under the actual execution. In this way, erroneous out-of-the-specification behaviors can be captured at runtime. However, self-checking mechanisms come at a computational cost, which may affect time constrains of embedded software. Thus, the oracles shall be introduced while satisfying these time constraints. This work proposes a profiling approach for oracles in embedded software, which proves to be more accurate than traditional profiling approaches, e.g., statistical sampling techniques. Profiling the execution time of oracles permits to finely tune the execution rate of the application to avoid timing violation, and to increase application responsiveness. Experimental results have been carried out on an industrial deployment platform for real-time application, i.e., National Instruments PXI VeriStand.	algorithmic efficiency;debugging;embedded software;embedded system;oracle machine;pci extensions for instrumentation;profiling (computer programming);real-time computing;real-time transcription;responsiveness;run time (program lifecycle phase);sampling (signal processing);software deployment	Simone Bronuzzi;Giuseppe Di Guglielmo;Franco Fummi;Graziano Pravadelli	2012	2012 IEEE International High Level Design Validation and Test Workshop (HLDVT)	10.1109/HLDVT.2012.6418249	embedded system;verification and validation;real-time computing;verification;embedded software;computer science;software reliability testing;operating system;software construction;formal specification;software testing;programming language;avionics software	Embedded	-50.944730519567315	38.42896245033562	98629
fe259099b8789852c3d0827545e9f6490ca49285	app-bisect: autonomous healing for microservice-based apps		The microservice and DevOps approach to software design has resulted in new software features being delivered immediately to users, instead of waiting for long refresh cycles. On the downside, software bugs and performance regressions have now become an important cause of downtime. We propose app-bisect, an autonomous tool to troubleshoot and repair such software issues in production environments. Our insight is that the evolution of microservices in an application can be captured as mutations to the graph of microservice dependencies, such that a particular version of the graph from the past can be deployed automatically, as an interim measure until the problem is permanently fixed. Using canary testing and version-aware routing techniques, we describe how the search process can be sped up to identify such a candidate version. We present the overall design and key challenges towards implementing such a system.	autonomous robot;bisection (software engineering);devops;downtime;microservices;routing;software bug;software design	Shriram Rajagopalan;Hani Jamjoom	2015			real-time computing;simulation;computer science;operating system	OS	-62.021152422722004	36.968842503357436	98774
b33d4931cbd8245157f50090b0322ab19242fada	classification and coverage-based falsification for embedded control systems		Many industrial cyber-physical system (CPS) designs are too complex to formally verify system-level properties. A practical approach for testing and debugging these system designs is falsification, wherein the user provides a temporal logic specification of correct system behaviors, and some technique for selecting test cases is used to identify behaviors that demonstrate that the specification does not hold for the system. While coverage metrics are often used to measure the exhaustiveness of this kind of testing approach for software systems, existing falsification approaches for CPS designs do not consider coverage for the signal variables. We present a new coverage measure for continuous signals and a new falsification technique that leverages the measure to efficiently identify falsifying traces. This falsification algorithm combines global and local search methods and uses a classification technique based on support vector machines to identify regions of the search space on which to focus effort. We use an industrial example from an automotive fuel cell application and other benchmark models to compare the new approach against existing falsification tools.	algorithm;benchmark (computing);biasing;code coverage;control system;cyber-physical system;debugging;hydrogen;local search (optimization);monte carlo method;randomness;sampling (signal processing);software system;subdivision surface;support vector machine;temporal logic;test case;tracing (software)	Arvind S. Adimoolam;Thao Dang;Alexandre Donzé;James Kapinski;Xiaoqing Jin	2017		10.1007/978-3-319-63387-9_24	theoretical computer science;support vector machine;software system;debugging;local search (optimization);computer science;temporal logic;test case;control system	SE	-51.27508640588473	38.573423524997764	98836
61db182aa02fb8dc4ec32baa3e2b983fdcc5341e	alattin: mining alternative patterns for defect detection	frequent pattern;mining software engineering data;pattern formation;alternative patterns;false negative;source code;technical report;false positive;code search engine;static defect detection;software quality;defect detection	To improve software quality, static or dynamic defect-detection tools accept programming rules as input and detect their violations in software as defects. As these programming rules are often not well documented in practice, previous work developed various approaches that mine programming rules as frequent patterns from program source code. Then these approaches use static or dynamic defect-detection techniques to detect pattern violations in source code under analysis. However, these existing approaches often produce many false positives due to various factors. To reduce false positives produced by these mining approaches, we develop a novel approach, called Alattin, that includes new mining algorithms and a technique for detecting neglected conditions based on our mining algorithm. Our new mining algorithms mine patterns in four pattern formats: conjunctive, disjunctive, exclusive-disjunctive, and combinations of these patterns. We show the benefits and limitations of these four pattern formats with respect to false positives and false negatives among detected violations by applying those patterns to the problem of detecting neglected conditions.	algorithm;application programming interface;association rule learning;data mining;disjunctive normal form;exclusive or;ibm notes;sosi;sensor;software bug;software engineering;software quality;x image extension	Suresh Thummalapenta;Tao Xie	2011	Automated Software Engineering	10.1007/s10515-011-0086-z	type i and type ii errors;computer science;technical report;theoretical computer science;data mining;pattern formation;algorithm;software quality;source code	SE	-58.96883273554966	39.80161171543228	99143
64f1ef2e3645d5deb0808a8f8dccedcd9e0d15c8	accelerating test automation through a domain specific language	dsl;natural languages;tools;programming;selenium;java;automation	Test automation involves the automatic execution of test scripts instead of being manually run. This significantly reduces the amount of manual effort needed and thus is of great interest to the software testing industry. There are two key problems in the existing tools & methods for test automation - a) Creating an automation test script is essentially a code development task, which most testers are not trained on, and b) the automation test script is seldom readable, making the task of maintenance an effort intensive process. We present the Accelerating Test Automation Platform (ATAP) which is aimed at making test automation accessible to non-programmers. ATAP allows the creation of an automation test script through a domain specific language based on English. The English-like test scripts are automatically converted to machine executable code using Selenium WebDriver. ATAP's English-like test script makes it easy for non-programmers to author. The functional flow of an ATAP script is easy to understand as well thus making maintenance simpler (you can understand the flow of the test script when you revisit it many months later). ATAP has been built around the Eclipse ecosystem and has been used in a real-life testing project. We present the details of the implementation of ATAP and the results from its usage in practice.	domain-specific language;eclipse;ecosystem;emoticon;executable;human-readable medium;programmer;real life;selenium;software testing;test automation;test script	Anurag Dwarakanath;Dipin Era;Aditya Priyadarshi;Neville Dubash;Sanjay Podder	2017	2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)	10.1109/ICST.2017.52	programming;digital subscriber line;computer science;operating system;automation;software engineering;database;programming language;java;selenium;test harness	SE	-53.363146090615906	35.77965490269346	99530
107836ea05cf524db271d518e815b0e4a66d664a	path-based function embeddings		Identifying relationships among program elements, such as functions, is useful for program understanding, debugging, and analysis. We present func2vec, an algorithm that uses static traces to embed functions in a vector space such that related functions are close together, even if they are semantically and syntactically dissimilar. We present applications of func2vec that aid program comprehension.	algorithm;debugging;list comprehension;program comprehension;tracing (software)	Daniel DeFreez;Aditya V. Thakur;Cindy Rubio-González	2018		10.1145/3183440.3195042	debugging;real-time computing;computer science;vector space;theoretical computer science;program comprehension	SE	-56.249809198028515	36.690644286470906	99532
600dceac56597491c45e3375dbb83809b0e7e128	automated tagging of software projects using bytecode and dependencies (n)	sally closed source repository group software systems open source repository group software systems open source community term assignment software tagging software assets automatic software tagging approach maven based software projects bytecode dependency relations online repositories categorization approach;support vector machines;software systems;data mining;software management information retrieval internet project management public domain software;feature extraction tagging data mining software systems software algorithms support vector machines;feature extraction;software algorithms;tagging	Several open and closed source repositories group software systems and libraries to allow members of particular organizations or the open source community to take advantage of them. However, to make this possible, it is necessary to have effective ways of searching and browsing the repositories. Software tagging is the process of assigning terms (i.e., tags or labels) to software assets in order to describe features and internal details, making the task of understanding software easier and potentially browsing and searching through a repository more effective. We present Sally, an automatic software tagging approach that is able to produce meaningful tags for Maven-based software projects by analyzing their bytecode and dependency relations without any special requirements from developers. We compared tags generated by Sally to the ones in two widely used online repositories, and the tags generated by a state-of-the-art categorization approach. The results suggest that Sally is able to generate expressive tags without relying on machine learning-based models.	apache maven;categorization;library (computing);machine learning;open-source software;requirement;software system	Santiago Vargas-Baldrich;Mario Linares Vásquez;Denys Poshyvanyk	2015	2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1109/ASE.2015.38	software visualization;support vector machine;long-term support;verification and validation;software sizing;feature extraction;computer science;package development process;backporting;software framework;component-based software engineering;software development;software design description;software engineering;software construction;data mining;database;software walkthrough;programming language;software analytics;software deployment;world wide web;software system	SE	-61.25410164944525	39.60034638609084	99647
0bd302a00edf8cdfea117a15d75f7e40ff97dfb4	inferring types of references to gui objects in test scripts	test mantainance gui testing test automation type inference test script;software testing;performance evaluation;test automation;probability density function;automatic testing;testing;object oriented programming;data mining;test scripts;runtime;evolution biology;type inference of gui object references;type inference of gui object references test scripts testing process automation;graphical user interfaces;reasoning about programs;program testing;gui testing;programming profession;type theory;logic testing;test mantainance;source code;graphical user interfaces automatic testing software testing logic testing automation performance evaluation runtime programming profession costs java;type inference;test script;testing process automation;black box testing;type theory graphical user interfaces object oriented programming program testing reasoning about programs;java;automation	Since manual black-box testing of GUI-based APplications (GAPs) is tedious and laborious, test engineers create test scripts to automate the testing process. These test scripts interact with GAPs by performing actions on their GUI objects. Unlike conventional languages that require programmers to declare types of variables explicitly, test script statements reference GUI objects using their properties (e.g., location, color, size, etc). The absence of type information exacerbates the process of understanding test scripts, making maintenance and evolution of these scripts expensive and prohibitive, thus obliterating benefits of test automation. We offer a novel approach for Type Inference of GUI Object References (TIGOR) in test scripts. TIGOR makes types of GUI objects explicit in the source code of scripts, enabling test engineers to reason more effectively about the interactions between operations in complex test scripts and GUI objects that these operations reference. We describe our implementation and give an algorithm for automatically inferring types of GUI objects. We built a tool and evaluated it on different GAPs. Our experience suggests that TIGOR is practical and efficient, and it yields appropriate types of GUI objects.	algorithm;black-box testing;graphical user interface;interaction;programmer;test automation;test engineer;test script;type inference	Chen Fu;Mark Grechanik;Qing Xie	2009	2009 International Conference on Software Testing Verification and Validation	10.1109/ICST.2009.12	computer science;theoretical computer science;database;software testing;programming language;test script	SE	-56.928460210089675	37.19448553022157	99667
db321a5330136dfd758b89f24f0ed181d8cfb4da	improving reusability of software libraries through usage pattern mining		Abstract Modern software systems are increasingly dependent on third-party libraries. It is widely recognized that using mature and well-tested third-party libraries can improve developers’ productivity, reduce time-to-market, and produce more reliable software. Today’s open-source repositories provide a wide range of libraries that can be freely downloaded and used. However, as software libraries are documented separately but intended to be used together, developers are unlikely to fully take advantage of these reuse opportunities. In this paper, we present a novel approach to automatically identify third-party library usage patterns, i.e., collections of libraries that are commonly used together by developers. Our approach employs a hierarchical clustering technique to group together software libraries based on external client usage. To evaluate our approach, we mined a large set of over 6000 popular libraries from Maven Central Repository and investigated their usage by over 38,000 client systems from the Github repository. Our experiments show that our technique is able to detect the majority (77%) of highly consistent and cohesive library usage patterns across a considerable number of client systems.	data mining;library (computing)	Mohamed Aymen Saied;Ali Ouni;Houari A. Sahraoui;Raula Gaikovina Kula;Katsuro Inoue;David Lo	2018	Journal of Systems and Software	10.1016/j.jss.2018.08.032	systems engineering;database;software system;reuse;software;computer science;hierarchical clustering;reusability	SE	-57.542111377587496	33.7923092164642	99707
0cfd36630246c04ee2abc1a3dcc4fb3bc219cee1	techniques and tools for rich internet applications testing	complex dynamics;generators;rich internet application;test automation;testing user interfaces generators automation crawlers servers monitoring;user interface;ria testing automation;testing;communication model;automatic generation;ajax applications;servers;dynamic analysis rich internet applications testing testing automation;internet;program testing;monitoring;user interfaces internet java program testing;rich internet applications testing;crawlers;usability;ajax applications rich internet applications testing user interfaces usability ria testing automation;user interfaces;dynamic analysis;java;automation;testing automation	The User Interfaces of Rich Internet Applications (RIAs) present a richer functionality and enhanced usability than the ones of traditional Web applications which are obtained by means of a successful combination of heterogeneous technologies, frameworks, and communication models. Due to its increased complexity, dynamicity, and responsiveness, testing the user interfaces of an RIA is more complex than testing the user interfaces of a traditional Web application and requires that effective and efficient testing techniques are proposed and validated. In this paper we analyse the most critical open issues in RIA testing automation and propose a classification framework that characterizes existing RIA testing techniques from four different perspectives. Driven by this classification, we present a set of testing techniques that can be used for automatically and semi-automatically generating test cases, for executing them and evaluating their results. Some examples of applying the proposed techniques for testing real Ajax applications will also be shown in the paper.	ajax (programming);client-side;documentation;fault model;responsiveness;reverse engineering;rich internet application;semiconductor industry;server (computing);server-side scripting;test case;usability;user interface;web services enhancements;web application	Domenico Amalfitano;Anna Rita Fasolino;Porfirio Tramontana	2010	2010 12th IEEE International Symposium on Web Systems Evolution (WSE)	10.1109/WSE.2010.5623569	software performance testing;human–computer interaction;computer science;operating system;cloud testing;database;programming language;user interface;world wide web	SE	-55.41186586201211	32.54996636703071	100219
cc9626a0f5da330115e8ae33558ec5402931c810	selecting manual regression test cases automatically using trace link recovery and change coverage	regression tests;trace link recovery;software maintenance;manual system tests;test coverage;test selection	Regression tests ensure that existing functionality is not impaired by changes to an existing software system. However, executing complete test suites often takes much time. Therefore, a subset of tests has to be found that is sufficient to validate whether the system under test is still valid after it has been changed. This test case selection is especially important if regression tests are executed manually, since manual execution is time intensive and costly. To select manual test cases, many regression testing techniques exist that aim on achieving coverage of changed or even new code. Many of these techniques base on coverage data from prior test runs or logical properties such as annotated pre and post conditions in the source code. However, coverage information becomes outdated if a system is changed extensively. Also annotated logical properties are often not available in industrial software systems. We present an approach for test selection that is based on static analyses of the test suite and the system's source code. We combine trace link recovery using latent semantic indexing with the metric change coverage, which assesses the coverage of source code changes. The proposed approach works automatically without the need to execute tests beforehand or annotate source code. Furthermore, we present a first evaluation of the approach.	coverage data;latent semantic analysis;regression testing;software system;static program analysis;system under test;test case;test suite	Sebastian Eder;Benedikt Hauptmann;Maximilian Junker;Rudolf Vaas;Karl-Heinz Prommer	2014		10.1145/2593501.2593506	modified condition/decision coverage;reliability engineering;regression testing;test data generation;real-time computing;fault coverage;computer science;engineering;operating system;software engineering;test suite;data mining;dynamic testing;code coverage;test script;software maintenance;test case;test management approach	SE	-60.32650840257006	35.70718561609642	100435
104ab1aec8ec934f9ed602268f60307f890d5cc1	looper: lightweight detection of infinite loops at runtime	jedit;symbolic arguments lightweight detection infinite loops running program looper program analysis symbolic execution simple nontermination arguments program values smt solver java application open source text editor jedit java programs;program diagnostics java;infinite loops;program diagnostics;smt solver;java programming;lightweight detection;simple nontermination arguments;resource management;semantics;macro errors;program values;indexes;preprossing;looper;symbolic execution;open source text editor;abstracts;cognition;runtime surface mount technology java open source software software engineering jacobian matrices usa councils shape prototypes debugging;java programs;program analysis;off the shelf;inconsistencies;running program;java application;concrete;symbolic arguments;java;open source	When a running program becomes unresponsive, it is often impossible for a user to determine if the program is performing some useful computation or if it has entered an infinite loop. We present LOOPER, an automated technique for dynamically analyzing a running program to prove that it is non-terminating. LOOPER uses symbolic execution to produce simple non-termination arguments for infinite loops dependent on both program values and the shape of heap. The constructed arguments are verified with an off-the-shelf SMT solver. We have implemented our technique in a prototype tool for Java applications, and we demonstrate our technique’s effectiveness on several non-terminating benchmarks, including a reported infinite loop bug in open-source text editor jEdit. Our tool is able to dynamically detect infinite loops deep in the execution of large Java programs with no false warnings, producing symbolic arguments that can aid in debugging non-termination.	algorithm;automated theorem proving;computation;debugging;divergence (computer science);dynamic problem (algorithms);ibm notes;infinite loop;java;microsoft award;newman's lemma;open-source software;programming language;prototype;real life;run time (program lifecycle phase);solver;symbolic execution;text editor;jedit	Jacob Burnim;Nicholas Jalbert;Christos Stergiou;Koushik Sen	2009	2009 IEEE/ACM International Conference on Automated Software Engineering	10.1109/ASE.2009.87	program analysis;database index;parallel computing;real-time computing;cognition;concrete;computer science;infinite loop;resource management;software engineering;semantics;programming language;java;satisfiability modulo theories	SE	-57.92780137723486	38.864800544389034	100457
11916c800ae247d5b338998443c184140cd03671	structural conformance checking with design tests: an evaluation of usability and calability	program diagnostics;design test;protocols;java testing usability protocols calculators scalability;design test strucutural conformance checking;formal specification;calculators;software performance evaluation;testing;program verification;public domain software;conformance testing;program testing;open source software project structural conformance checking design tests software usability evaluation software scalability evaluation software verification functional requirements software development software quality software design specification java projects designwizard api junit testing framework think aloud protocol;application program interfaces;strucutural conformance checking;scalability;usability;software quality;software quality application program interfaces conformance testing formal specification java program diagnostics program testing program verification public domain software software performance evaluation;java	Verifying whether a software meets its functional requirements plays an important role in software development. However, this activity is necessary, but not sufficient to assure software quality. It is also important to check whether the code meets its design specification. Although there exists substantial tool support to assure that a software does what it is supposed to do, verifying whether it conforms to its design remains as an almost completely manual activity. In a previous work, we proposed design tests — test-like programs that automatically check implementations against design rules. Design test is an application of the concept of test to design conformance checking. To support design tests for Java projects, we developed DesignWizard, an API that allows developers to write and execute design tests using the popular JUnit testing framework. In this work, we present a study on the usability and scalability of DesignWizard to support structural conformance checking through design tests. We conducted a qualitative usability evaluation of DesignWizard using the Think Aloud Protocol for APIs. In the experiment, we challenged eleven developers to compose design tests for an open-source software project. We observed that the API meets most developers' expectations and that they had no difficulties to code design rules as design tests. To assess its scalability, we evaluated DesignWizard's use of CPU time and memory consumption. The study indicates that both are linear functions of the size of software under verification.	application programming interface;central processing unit;conformance testing;functional requirement;goto;junit;java;linear function;middleware;mobile data terminal;open-source software;ourgrid;scalability;software development;software project management;software quality;test automation;think aloud protocol;usability;verification and validation	João Brunet;Dalton Serey Guerrero;Jorge C. A. de Figueiredo	2011	2011 27th IEEE International Conference on Software Maintenance (ICSM)	10.1109/ICSM.2011.6080781	reliability engineering;communications protocol;scalability;usability;computer science;software design;software engineering;conformance testing;software construction;formal specification;software testing;programming language;java;public domain software;software quality	SE	-57.247300364804026	36.71989148116444	100819
1f236969768590a1cf9e21b4a63f3ea86181817e	improving casa runtime performance by exploiting basic feature model analysis.		In Software Product Line Engineering (SPLE) families of systems are designed, rather than developing the individual systems independently. Combinatorial Interaction Testing has proven to be effective for testing in the context of SPLE, where a representative subset of products is chosen for testing in place of the complete family. Such a subset of products can be determined by computing a so called t-wise Covering Array (tCA), whose computation is NP-complete. Recently, reduction rules that exploit basic feature model analysis have been proposed that reduce the number of elements that need to be considered during the computation of tCAs for Software Product Lines (SPLs). We applied these rules to CASA, a simulated annealing algorithm for tCA generation for SPLs. We evaluated the adapted version of CASA using 133 publicly available feature models and could record on average a speedup of 61.8% of median execution time, while at the same time preserving the coverage of the generated array.	algorithm;computation;computational auditory scene analysis;feature model;np-completeness;run time (program lifecycle phase);simulated annealing;software product line;speedup	Evelyn Nicole Haslinger;Roberto Erick Lopez-Herrejon;Alexander Egyed	2013	CoRR		computer science;theoretical computer science;feature model;software;simulated annealing;computation;speedup;exploit;software product line	SE	-59.24378695316854	33.914805771973654	100933
60151f7253642fa826d9f7be60164a714cd7cf54	joogie: infeasible code detection for java	java programming;control flow;error detection;static analysis	We present Joogie, a tool that detects infeasible code in Java programs. Infeasible code is code that does not occur on feasible control-flow paths and thus has no feasible execution. Infeasible code comprises many errors detected by static analysis in modern IDEs such as guaranteed null-pointer dereference or unreachable code. Unlike existing techniques, Joogie identifies infeasible code by proving that a particular statement cannot occur on a terminating execution using techniques from static verification. Thus, Joogie is able to detect infeasible code which is overlooked by existing tools. Joogie works fully automatically, it does not require user-provided specifications and (almost) never produces false warnings.	control flow;dereference operator;java;newman's lemma;pointer (computer programming);static program analysis;unreachable memory	Stephan Arlt;Martin Schäf	2012		10.1007/978-3-642-31424-7_62	dead code;code bloat;real-time computing;error detection and correction;computer science;redundant code;programming language;control flow;static analysis;algorithm;duplicate code;unreachable code	SE	-57.62325284300741	38.85841715634875	100970
88d65e00afbcf85bfcec578ff4672b11c74d3b16	providing orthogonal persistence for java extended abstract		A significant proportion of software requires persistent data, that is data that outlives the period of execution of individual programs. This is a simple consequence of the fact that most software is written to support human activities and many human activities continue for days, weeks, months or even tens of years. The design of an aircraft or the construction and maintenance of a large artefact, are typical examples. Over these longer periods it is common to find that a system must evolve to accommodate changing requirements, using additional programs and data, as well as (possibly transformed) earlier programs and data.	computation;computational model;database;gosling emacs;jdbc;java persistence api;koch snowflake;mathematical optimization;memory management;napier88;object data management group;palo;persistence (computer science);persistent object store;program optimization;programming language specification;requirement;schedule (computer science);the computer journal;type safety;type system;vldb;visual artifact	Malcolm P. Atkinson;Mick J. Jordan	1998		10.1007/BFb0054100	distributed computing;software development;database;object-oriented programming;relational database;computer science;persistence (computer science);java	PL	-52.96224914363827	33.10014257316338	101259
2451b6daebbb452a5c63c87853283e09978d6c2d	jsclassfinder: a tool to detect class-like structures in javascript		With the increasing usage of JavaScript in web applications, there is a great demand to write JavaScript code that is reliable and maintainable. To achieve these goals, classes can be emulated in the current JavaScript standard version. In this paper, we propose a reengineering tool to identify such class-like structures and to create an object-oriented model based on JavaScript source code. The tool has a parser that loads the AST (Abstract Syntax Tree) of a JavaScript application to model its structure. It is also integrated with the Moose platform to provide powerful visualization, e.g., UML diagram and Distribution Maps, and well-known metric values for software analysis. We also provide some examples with real JavaScript applications to evaluate the tool. Video: http://youtu.be/FadYE_FDVM0	abstract syntax tree;code refactoring;computation;diagram;ecmascript;emulator;javascript;moose;map;uml state machine;unified modeling language;web application	Leonardo Humberto Silva;Daniel Hovadick;Marco Tulio Valente;Alexandre Bergel;Nicolas Anquetil;Anne Etien	2016	CoRR		rich internet application;computer science;unobtrusive javascript;database;javascript;programming language;world wide web	SE	-52.985746369907055	34.33981427633794	101713
35cc45ad7797bafd7fbdf82685d42df303afcab2	static detection of event-based races in android apps		"""Event-based races are the main source of concurrency errors in Android apps. Prior approaches for scalable detection of event-based races have been dynamic. Due to their dynamic nature, these approaches suffer from coverage and false negative issues. We introduce a precise and scalable static approach and tool, named SIERRA, for detecting Android event-based races. SIERRA is centered around a new concept of """"concurrency action"""" (that reifies threads, events/messages, system and user actions) and statically-derived order (happens-before relation) between actions. Establishing action order is complicated in Android, and event-based systems in general, because of externally-orchestrated control flow, use of callbacks, asynchronous tasks, and ad-hoc synchronization. We introduce several novel approaches that enable us to infer order relations statically: auto-generated code models which impose order among lifecycle and GUI events; a novel context abstraction for event-driven programs named  action-sensitivity  and finally, on-demand path sensitivity via backward symbolic execution to further rule out false positives. We have evaluated SIERRA on 194 Android apps. Of these, we chose 20 apps for manual analysis and comparison with a state-of-the-art dynamic race detector. Experimental results show that SIERRA is effective and efficient, typically taking 960 seconds to analyze an app and revealing 43 potential races. Compared with the dynamic race detector, SIERRA discovered an average 29.5 true races with 3.5 false positives, where the dynamic detector only discovered 4 races (hence missing 25.5 races per app) -- this demonstrates the advantage of a precise static approach. We believe that our approach opens the way for precise analysis and static event race detection in other event-driven systems beyond Android."""	android;callback (computer programming);concurrency (computer science);control flow;event-driven programming;graphical user interface;hoc (programming language);race condition;reification (computer science);scalability;sensor;symbolic execution	Yongjian Hu;Iulian Neamtiu	2018		10.1145/3173162.3173173	callback;real-time computing;scalability;asynchronous communication;control flow;android (operating system);thread (computing);symbolic execution;computer science;concurrency	Arch	-55.5423191604801	39.07674005678848	101817
c3b0453c4b941c76ece1b168c6394237e72677a2	t++: a test case generator using a debugging information based technique for source code manipulation	functional testing;value analysis;automatic generation;test case generation;design and implementation;source code knowledge base t c language extension test case generator debugging information based technique source code manipulation systematic c code testing object oriented testing scheme static member function automatically default generation class implementor typicals test scripts equivalence testing boundary value analysis functional testing approaches compiler code;test methods;language extension;source code;program debugging;computer aided software engineering debugging software testing system testing automatic testing data mining gas insulated transmission lines software systems laboratories computer science;object oriented testing;knowledge base	"""T++ is a C++ language extension that aids in systematic testing of C++ code. For each type t used in the code, T++ maintains the set of """"typical"""" values of t, denoted by T(t) (typicals of t). The authors propose an object-oriented testing scheme in which each class t has a test method implemented as a static member function that runs a test script on each of the values in T(t). T++ supports this scheme by automatically generating a default T(t) based on the declaration of t and on the typicals of each of the types used in this declaration. This default can be easily tuned by the class implementor and users to reflect the particularities of the class. T++ also provides an easily tuned by the class implementor and users to reflect the particularities of the class. T++ also provides an easy-to-use mechanism for iterating over the typicals of a type. Thus, T++ gives a supporting environment for the design and implementation of test scripts with the familiar techniques of equivalence testing, boundary value analysis, or functional testing approaches. Most language extensions are realized by either a dedicated pre-processor or by a modification to the compiler. Both techniques suffer from serious drawbacks related to the complexity of C++ and the accessibility of the compiler code. In contrast, the implementation of T++ sidesteps these difficulties by relaying on a novel technique that creates a knowledge base of the source code from the debugging information."""		Joseph Gil;Beery Holstein	1997		10.1109/TOOLS.1997.654735	c++;computer science;theoretical computer science;programming language;algorithm	EDA	-51.63118452455542	35.91510758552777	102039
6f85bbca627498fa404f6f705b3000ca0155c989	css preprocessing: tools and automation techniques		Cascading Style Sheets (CSS) is a W3C specification for a style sheet language used for describing the presentation of a document written in a markup language, more precisely, for styling Web documents. However, in the last few years, the landscape for CSS development has changed dramatically with the appearance of several languages and tools aiming to help developers build clean, modular and performance-aware CSS. These new approaches give developers mechanisms to preprocess CSS rules through the use of programming constructs, defined as CSS preprocessors, with the ultimate goal to bring those missing constructs to the CSS realm and to foster stylesheets structured programming. At the same time, a new set of tools appeared, defined as postprocessors, for extension and automation purposes covering a broad set of features ranging from identifying unused and duplicate code to applying vendor prefixes. With all these tools and techniques in hands, developers need to provide a consistent workflow to foster CSS modular coding. This paper aims to present an introductory survey on the CSS processors. The survey gathers information on a specific set of processors, categorizes them and compares their features regarding a set of predefined criteria such as: maturity, coverage and performance. Finally, we propose a basic set of best practices in order to setup a simple and pragmatic styling code workflow.	adobe streamline;automation;best practice;capability maturity model;cascading style sheets;central processing unit;duplicate code;extensibility;javascript;markup language;mathematical optimization;parsing;plug-in (computing);postcss;preprocessor;sass;scalability;structured programming;style sheet (web development);stylus (computing);web development;web page	Ricardo Queirós	2018	Information	10.3390/info9010017	computer science;data mining;markup language;style sheet;modular design;ranging;structured programming;cascading style sheets;preprocessor;workflow	SE	-52.75033692106496	34.84488400716341	102225
13631e364210270930f853f69dacf26c731cb884	analyzing closeness of code dependencies for improving ir-based traceability recovery	biomedical monitoring;software;electronic mail;vocabulary;monitoring;context	Information Retrieval (IR) identifies trace links based on textual similarities among software artifacts. However, the vocabulary mismatch problem between different artifacts hinders the performance of IR-based approaches. A growing body of work addresses this issue by combining IR techniques with code dependency analysis such as method calls. However, so far the performance of combined approaches is highly dependent to the correctness of IR techniques and does not take full advantage of the code dependency analysis. In this paper, we combine IR techniques with closeness analysis to improve IR-based traceability recovery. Specifically, we quantify and utilize the “closeness” for each call and data dependency between two classes to improve rankings of traceability candidate lists. An empirical evaluation based on three real-world systems suggests that our approach outperforms three baseline approaches.	baseline (configuration management);centrality;correctness (computer science);data dependency;dependence analysis;information retrieval;interaction;traceability;vocabulary mismatch;world-system	Hongyu Kuang;Jia Nie;Hao Hu;Patrick Rempel;Jian Lu;Alexander Egyed;Patrick Mäder	2017	2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)	10.1109/SANER.2017.7884610	computer science;data mining;database;world wide web	SE	-57.703658205613536	33.51359372871565	102290
23f2ca52981bec1bcd73583d1824bbbe0cfa63b1	rapid prototyping of visualizations using mondrian	rapid prototyping software engineer feature dependency browser on the fly prototyping interactive visualization tool mondrian;software engineer;software prototyping;on the fly prototyping;interactive visualization;mondrian;prototypes visualization containers runtime model driven engineering software prototyping software tools engines buildings user interfaces;software engineering;interactive visualization tool;rapid prototyping;on the fly;feature dependency browser;interactive systems;software prototyping interactive systems program visualisation;program visualisation	Science requires tools, and computer science is no different. In a typical research context however, it is not known upfront how a tool should work. Researching the tool's design is part of the investigation process. Various designs have to be prototyped and experimented with. This paper focuses on the research process of interactive visualization tools. We present, how to improve development, so that a novel tool ca.n be tested and modified at (almost) the same time. We present the Mondrian framework, which supports on-the-fly prototyping of interactive visualizations. As an example, we present the research process of the feature dependency browser, a visualization tool which we developed to allow software engineers inspect runtime dependencies between features.	computer science;interactive visualization;mondrian olap server;rapid prototyping;software engineer	Adrian Lienhard;Adrian Kuhn;Orla Greevy	2007	2007 4th IEEE International Workshop on Visualizing Software for Understanding and Analysis	10.1109/VISSOF.2007.4290702	software visualization;interactive visualization;human–computer interaction;computer science;systems engineering;engineering;software engineering;computer engineering	Visualization	-54.04886423957084	34.88583367707126	102483
e4429b75cd7a46c1b4f1daf61475a6e83470a2ad	software quality in mobile environments: a comparative study		Evaluating Software Quality (SQ) of mobile applications is an active and challenging research topic. This is caused by the limitations of both mobile networks and mobile devices such as low bandwidth, frequent disconnection, low energy and low storage capacity. In order to help programmers, developers and SQ evaluators to deal with the limitations of mobile environments and to achieve a high level of mobile software quality, we have designed two frameworks ISO9126-FRAM and Diffserv-FRAM. ISO9126-FRAM consists in using the SQ standard ISO 9126 in mobile environments while DIFFSERV-FRAM concerns the use of the SQ standard ISO 9126 with the DiffServ Quality of Service (QoS) model to evaluate the software quality in mobile environments. Therefore, the main objective of this study is to present and compare these two frameworks of SQ evaluation to show the disadvantages and advantages of each one. This comparison is based on the Framework Mapping Comparison Method. As a result of this comparison, the correlation between the two frameworks can be strong as it can be weak depending on the studied SQ characteristics. In addition, differences have been identified at the empirical evaluation level of the two frameworks in terms of the encountered difficulties.	differentiated services;high-level programming language;iso/iec 9126;mobile app;mobile device;programmer;quality of service;software quality;sound quality	Karima Moumane;Ali Idri	2017	2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)	10.1109/CoDIT.2017.8102750	quality of service;software quality;real-time computing;mobile device;engineering	SE	-51.58939877019535	40.73075714140363	102861
02bd6c704314a6f3a085245a59e6daf8e767df16	type inference for soft-error fault-tolerance prediction	test selection and prioritization;brake by wire controller;circuit faults;fault tolerant;source level impact;high level information;software systems;software fault tolerance;memory cells;fault tolerance computer errors assembly systems software systems voltage testing error correction registers hardware prototypes;testing;type theory data flow computing program testing reasoning about programs software fault tolerance;soft error fault tolerance prediction;usage pattern;assembly;voltage spikes;reasoning about programs;cosmic radiation;program testing;control structure;fault tolerant systems;registers;test selection and prioritization fault injection assembly type system;fault tolerance;type theory;untyped assembly representation;data flow structure;data flow computing;brake by wire controller type inference soft error fault tolerance prediction software systems voltage spikes cosmic radiation source level impact soft errors fault injection black box testing high level information data flow structure usage pattern memory cells untyped assembly representation;soft errors;assembly type system;data flow;fault injection;type inference;soft error;type system;black box testing;hardware	Software systems are becoming increasingly vulnerable to a new class of soft errors, originating from voltage spikes produced by cosmic radiation. The standard technique for assessing the source-level impact of these soft errors, fault injection--essentially a black-box testing technique--provides limited high-level information. Since soft errors can occur anywhere, even control-structured white-box techniques offer little insight. We propose a type-based approach, founded on data-flow structure, to classify the usage pattern of registers and memory cells. To capture all soft errors, the type system is defined at the assembly level, close to the hardware, and allows inferring types in the untyped assembly representation. In a case study, we apply our type inference scheme to a prototype brake-by-wire controller, developed by Volvo Technology, and identify a high correlation between types and fault-injection results. The case study confirms that the inferred types are good predictors for soft-error impact.	black-box testing;cosmic;dataflow;fault injection;fault tolerance;high- and low-level;kerrison predictor;operand;prototype;soft error;software system;test suite;tracing (software);type inference;type system;voltage spike;white box (software engineering);white-box testing	Gustav Munkby;Sibylle Schupp	2009	2009 IEEE/ACM International Conference on Automated Software Engineering	10.1109/ASE.2009.61	reliability engineering;embedded system;fault tolerance;real-time computing;computer science;engineering;programming language	SE	-60.98206856229568	36.74810486588069	102902
925897dca455d20ff18c74c13f7903ed58f27dcb	t3, a combinator-based random testing tool for java: benchmarking	benchmark testing tools;random testing;automated testing java	T3 is the next generation of the light weight automated testing tool T2 for Java. In the heart T3 is still a random testing tool; but it now comes with some new features: pair-wise testing, concurrent generators, and a combinator-based approach ala QuickCheck. This paper presents the result of benchmarking of T3 on its default configuration against a set of real world classes.	java;quickcheck;random testing;test automation	I. S. W. B. Prasetya	2013		10.1007/978-3-319-07785-7_7	keyword-driven testing;real-time computing;software performance testing;manual testing;computer science;operating system;programming language	SE	-55.852936668162116	32.71088300064902	103168
64b6b34d3d2bf2e0fad2ebf480620d811b4bd73f	the package blueprint: visually analyzing and quantifying packages dependencies	software maintenance;software engineering;software comprehension;software visualization	a r t i c l e i n f o a b s t r a c t Large object-oriented applications are structured over many packages. Packages are important but complex structural entities that are difficult to understand since they act as containers of classes, which can have many dependencies with other classes spread over multiple packages. However to be able to take decisions (e.g. refactoring and/or assessment decisions), maintainers face the challenges of managing (sorting, grouping) the massive amount of dependencies between classes spread over multiple packages. To help maintainers, there is a need for at the same time understanding, and quantifying, dependencies between classes as well as understanding how packages as containers of such classes depend on each other. In this paper, we present a visualization, named Package Blueprint, that reveals in detail package internal structure, as well as the dependencies between an observed package and its neighbors, at both package and class levels. Package blueprint aims at assisting maintainers in understanding package structure and dependencies, in particular when they focus on few packages and want to take refactoring decisions and/or to assess the structure of those packages. A package blueprint is a space filling matrix-based visualization, using two placement strategies that are enclosure and adjacency. Package blueprint is structured around the notion of surfaces that group classes and their dependencies by their packages (i.e., enclosure placement); whilst surfaces are placed next to their parent node which is the package under-analysis (i.e., adjacency placement). We present two views: one stressing how an observed package depends upon the rest of the system and another stressing how the system depends upon that package. To evaluate the contribution of package blueprint for understanding packages we performed an exploratory user study comparing package blueprint with an advanced IDE. The results show that users of package blueprint are faster in analyzing and assessing package structure. The results are proved statically significant and they show that package blueprint considerably improves the experience of standard browser users. ✩ This article makes heavy use of colors. To better understand the ideas presented in this paper, please read a colored version of it.	blueprint;browsing;code refactoring;color vision;entity;existential quantification;geographical distance;human factors and ergonomics;integrated development environment;sorting;threat (computer);tree (data structure);usability testing;while	Hani Abdeen;Stéphane Ducasse;Damien Pollet;Ilham Alloui;Jean-Rémy Falleri	2014	Sci. Comput. Program.	10.1016/j.scico.2014.02.016	software visualization;package diagram;computer science;package development process;data mining;database;programming language;package;software maintenance	HCI	-55.51336750823418	34.99853664452384	103456
6f52ffd0a76e77561cc647a3dd109110af644e70	the code mini-map visualisation: encoding conceptual structures within source code		Modern source code editors typically include a code mini-map visualisation, which provides programmers with an overview of the currently open source code document. This paper proposes to add a layering mechanism to the code mini-map visualisation in order to provide programmers with visual answers to questions related to conceptual structures that are not manifested directly in the code. Details regarding the design and implementation of this scope information layer, which displays additional encodings that correspond to the scope chain and information related to the scope chain within a source code document, is presented. The scope information layer can be used by programmers to answer questions such as: to which scope does a specific variable belong, and in which scope is the cursor of the source code editor currently located in. Additionally, this paper presents a study that evaluates the effectiveness of adding the scope information layer to a code mini-map visualisation in order to help programmers understand scope relationships within source code. The results of the study show that the incorporating additional layers of information onto the code mini-map visualisation can have a positive effect on code understanding.	control flow;cursor (databases);debugging;encode;experiment;javascript syntax;open-source software;programmer;source code editor;web mapping	Ivan Bacher;Brian Mac Namee;John D. Kelleher	2018	2018 IEEE Working Conference on Software Visualization (VISSOFT)	10.1109/VISSOFT.2018.00023	task analysis;theoretical computer science;data visualization;computer science;encoding (memory);visualization;source code;cursor (user interface);software;code (cryptography)	SE	-53.70594774207123	35.57454830984232	103466
103aa63cb498678dc820ac420b5f86e1d8de21ae	recommendation system for software refactoring using innovization and interactive dynamic optimization	refactoring;articulo;search based software engineering;recommendation system for software refactoring using innovization and interactive dynamic optimization;software quality	We propose a novel recommendation tool for software refactoring that dynamically adapts and suggests refactorings to developers interactively based on their feedback and introduced code changes. Our approach starts by finding upfront a set of non-dominated refactoring solutions using NSGA-II to improve software quality, reduce the number of refactorings and increase semantic coherence. The generated non-dominated refactoring solutions are analyzed using our innovization component to extract some interesting common features between them. Based on this analysis, the suggested refactorings are ranked and suggested to the developer one by one. The developer can approve, modify or reject each suggested refactoring, and this feedback is used to update the ranking of the suggested refactorings. After a number of introduced code changes, a local search is performed to update and adapt the set of refactoring solutions suggested by NSGA-II. We evaluated this tool on four large open source systems and one industrial project provided by our partner. Statistical analysis of our experiments over 31 runs shows that the dynamic refactoring approach performed significantly better than three other search-based refactoring techniques, manual refactorings, and one refactoring tool not based on heuristic search.	code refactoring;dynamic programming;experiment;heuristic;interactivity;local search (optimization);mathematical optimization;multi-objective optimization;open-source software;recommender system;software quality	Mohamed Wiem Mkaouer;Marouane Kessentini;Slim Bechikh;Kalyanmoy Deb;Mel Ó Cinnéide	2014		10.1145/2642937.2642965	search-based software engineering;computer science;systems engineering;software engineering;programming language;world wide web;code refactoring;software quality	SE	-58.853566879093506	34.73006034712547	103900
db269c36b52b9ebfd0e1e7123d2e19bacd97d803	new architecture of the object-oriented functional coverage mechanism for digital verification	libraries;object oriented functional coverage mechanism cocotb open verification framework python coverpoint object covergroup object systemverilog language dut design under test digital integrated circuit digital verification;measurement;testing;monitoring;syntactics;measurement syntactics real time systems object oriented modeling testing monitoring libraries;object oriented methods digital integrated circuits formal verification hardware description languages integrated circuit testing;object oriented modeling;real time systems	Functional Coverage is a mechanism used in digital integrated circuits functional verification to measure whether the executed test set covered the declared functionality. It helps to examine test scenarios, by providing metrics that give information about the testcase or design-under-test (DUT) reached states (coverage points). SystemVerilog language provides dedicated syntax to make it possible, such as covergroup and coverpoint objects. However, these features have very limited functionality. It is difficult to manipulate functional coverage data on the testbench level, e.g. to make the test scenario dependent on the real-time coverage metrics. It is also not clear how to define coverage objects for complex design features. The architecture of a new, object-oriented functional coverage mechanism for digital verification, implemented in Python, is proposed in this paper. The testbench is based on the Cocotb open verification framework. The implemented solution gives more flexibility than standard SystemVerilog syntax and enables more agile creation of verification environments.	agile software development;coverage data;device under test;integrated circuit;python;real-time transcription;scenario testing;systemverilog;test bench;test case;test set	Marek Cieplucha;Witold A. Pleskacz	2016	2016 1st IEEE International Verification and Security Workshop (IVSW)	10.1109/IVSW.2016.7566600	real-time computing;verification;computer science;high-level verification;programming language;intelligent verification;functional verification;computer engineering	EDA	-48.60825482767402	34.64635617083568	103916
9a79f218fefbbb32b82d77702f5cbea74b5ae618	n-version disassembly: differential testing of x86 disassemblers	differential testing;software testing;automatic test generation;long chain;reverse engineering	The output of a disassembler is used for many different purposes (e.g., debugging and reverse engineering). Therefore, disassemblers represent the first link of a long chain of stages on which any high-level analysis of machine code depends upon. In this paper we demonstrate that many disassemblers fail to decode certain instructions and thus that the first link of the chain is very weak. We present a methodology, called N-version disassembly, to verify the correctness of disassemblers, based on differential analysis. Given a set of n - 1 disassemblers, we use them to decode fragments of machine code and we compare their output against each other. To further corroborate the output of these disassemblers, we developed a special instruction decoder, the nth, that delegates the decoding to the CPU, the ideal decoder. We tested eight of the most popular disassemblers for Intel x86, and found bugs in each of them.	central processing unit;correctness (computer science);debugging;disassembler;high- and low-level;machine code;reverse engineering;software bug;x86	Roberto Paleari;Lorenzo Martignoni;Giampaolo Fresi Roglia;Danilo Bruschi	2010		10.1145/1831708.1831741	embedded system;real-time computing;computer science;engineering;software engineering;software testing;programming language;algorithm;reverse engineering	Security	-56.2620525756606	39.82569278299893	103968
48743a1ec54fa030dc10d070013cfeb89dc7d5b7	unit testing using design by contract and equivalence partitions	test programa;tool support;unit testing;essai automatique;prueba automatica;generacion automatica prueba;extreme programming;particion;partition;generation automatique test;programmation extreme;automatic test generation;design by contract;test programme;program test;automatic test	Extreme Programming [1] and in particular the idea of Unit Testing can improve the quality of the testing process. But still programmers need to do a lot of tiresome manual work writing test cases. If the programmers could get some automatic tool support enforcing the quality of test cases then the overall quality of the software would improve significantly.	design by contract;turing completeness;unit testing	Per Madsen	2003		10.1007/3-540-44870-5_69	partition;simulation;extreme programming;computer science;design by contract;equivalence partitioning;mathematics;unit testing;programming language;algorithm	EDA	-59.15001378713443	34.57000485906653	104086
b36277310f2cbe2e96f6eaef61891951b4a54302	a half-million strong at least	embedded software embedded system embedded computing programming profession computer science management training computer languages signal processing hardware software design;microcontrollers;microprocessors design;hardware software codesign;programming language;embedded systems;microprocessor chips embedded systems hardware software codesign microcontrollers;programming languages embedded computing;microcontrollers embedded computing microprocessors design embedded software;embedded computing;programming languages;embedded software;microprocessor chips	The embedded computing application space is growing and continues to do so for quite some time as we figure out better ways to design new microprocessors and better ways to design embedded software. Furthermore, all those systems that use embedded software require maintenance. As the application platforms change - automobiles, airplanes, factory equipment, and soon - we must change the software accordingly. And as the digital platforms become obsolete, we need to redesign the embedded software for the new platform	embedded software;embedded system;microprocessor	Wayne H. Wolf	2006	Computer	10.1109/MC.2006.290	microcontroller;embedded system;embedded operating system;computer architecture;computing;embedded software;computer science;software design;software framework;component-based software engineering;software development;industrial data processing;software construction;embedded java;software deployment;software system;computer engineering;avionics software	Embedded	-49.260664593090794	32.4918852158469	104156
da41d77302a53223189741b54826376dca1089a9	automated detection of injected faults in a differential equation solver	fuzzy neural nets;reliability modeling;differential equation;data mining;fault detection differential equations system testing logic testing finite element methods data mining automatic testing software testing fault diagnosis partial differential equations;finite element;fault tolerant computing;data mining automated fault detection injected faults differential equation solver computational system software test cases software reliability info fuzzy network ifn unstructured mesh finite element solver umfes 2d elliptic partial differential equation;program testing;partial differential equations;unstructured mesh;elliptic partial differential equation;program testing fault tolerant computing fuzzy neural nets finite element analysis data mining software reliability partial differential equations;finite element analysis;fault injection;software reliability	Analysis of logical relationships between inputs and outputs of a computational system can significantly reduce the test execution effort via minimizing the number of required test cases. Unfortunately, the available specification documents are often insufficient to build a complete and reliable model of the tested system. In this paper, we demonstrate the use of a data mining method, called Info-Fuzzy Network (IFN), which can automatically induce logical dependencies from execution data of a stable software version, construct a set of non-redundant test cases, and identify faulty outcomes in new, potentially faulty releases of the same system. The proposed approach is applied to the Unstructured Mesh Finite Element Solver (UMFES) which is a general finite element program for solving 2D elliptic partial differential equations. Experimental results demonstrate the capability of the IFN-based testing methodology to detect several kinds of faults injected in the code of this sophisticated application.	data mining;finite element method;software versioning;solver;test case	Mark Last;Menahem Friedman	2004	Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings.	10.1109/HASE.2004.1281751	real-time computing;computer science;theoretical computer science;finite element method;algorithm	SE	-62.05458505776378	41.385858315310934	104158
5f03e9df7c3bc5a452be09bc5a903323075bc90e	applying testability to reliability estimation	software testing;software performance evaluation software reliability program testing;reliability estimation;executable code;reliability growth models;identity based encryption;operational profiles;software testing software reliability fault detection statistics read only memory identity based encryption information analysis programming system testing ear;software performance evaluation;software systems;testability;ear;program testing;fault detection;statistics;system testing;reliability growth models testability reliability estimation software reliability testing effectiveness operational profiles real software system executable code;software reliability;testing effectiveness;programming;information analysis;read only memory;real software system;growth model	Thepurposeof this report is to implementheideaof using testability to estimatesoftware reliability. Thebasicsteps involveestimatingtestability, evaluatinghowwell software waswritten,andassessingtherelationshipbetweentesting andusage. Resultsfromthesestepsalongwith operational profilesare usedto estimatesoftware reliability. This paper describesan applicationof this methodto evaluatethe reliability of a real software systemof about6000lines of executablecodeanddiscussestheresultsof such anestimation. Theresultsare alsocomparedwith thoseobtainedby usingtwo reliability growthmodels.	software testability;xojo	Mark C. K. Yang;W. Eric Wong;Alberto Pasquini	1998		10.1109/ISSRE.1998.730848	testability;reliability engineering;programming;real-time computing;computer science;engineering;software reliability testing;software engineering;software construction;software testing;data analysis;programming language;executable;system testing;read-only memory;fault detection and isolation;software quality;software system	Arch	-61.89859366426691	32.56521851545544	104197
9e555463b67662d1c9e78f6cfdc335108ebd7b65	detecting code clones in binary executables	software tool;labeled tree;intellectual property;binary analysis;code clone;software tools;source code;clone detection;false positive	Large software projects contain significant code duplication, mainly due to copying and pasting code. Many techniques have been developed to identify duplicated code to enable applications such as refactoring, detecting bugs, and protecting intellectual property. Because source code is often unavailable, especially for third-party software, finding duplicated code in binaries becomes particularly important. However, existing techniques operate primarily on source code, and no effective tool exists for binaries.  In this paper, we describe the first practical clone detection algorithm for binary executables. Our algorithm extends an existing tree similarity framework based on clustering of characteristic vectors of labeled trees with novel techniques to normalize assembly instructions and to accurately and compactly model their structural information. We have implemented our technique and evaluated it on Windows XP system binaries totaling over 50 million assembly instructions. Results show that it is both scalable and precise: it analyzed Windows XP system binaries in a few hours and produced few false positives. We believe our technique is a practical, enabling technology for many applications dealing with binary code.	algorithm;binary code;binary file;cluster analysis;code refactoring;component-based software engineering;cut, copy, and paste;duplicate code;executable;microsoft windows;operating system;scalability;sensor;software bug;third-party software component	Andreas Sæbjørnsen;Jeremiah Willcock;Thomas Panas;Daniel J. Quinlan;Zhendong Su	2009		10.1145/1572272.1572287	dead code;real-time computing;type i and type ii errors;computer science;theoretical computer science;redundant code;legacy code;code refactoring;algorithm;intellectual property;code generation;duplicate code;source code	SE	-59.82913490453962	39.7070671830735	104200
1b629bf5634da8d91ff49633e70ec3e48e0f60ee	what is acceptably safe for reinforcement learning?		Machine Learning algorithms are becoming more prevalent in critical systems where dynamic decision making and efficiency are the goal. As is the case for complex and safety-critical systems, where certain failures can lead to harm, we must proactively consider the safety assurance of such systems that use Machine Learning. In this paper we explore the implications of the use of Reinforcement Learning in particular, considering the potential benefits that it could bring to safety-critical systems, and our ability to provide assurances on the safety of systems incorporating such technology. We propose a high-level argument that could be used as the basis of a safety case for Reinforcement Learning systems, where the selection of ‘reward’ and ‘cost’ mechanisms would have a critical effect on the outcome of decisions made. We conclude with fundamental challenges that will need to be addressed to give the confidence necessary for deploying Reinforcement Learning within safetycritical applications.	algorithm;critical systems thinking;high- and low-level;intelligent agent;machine learning;misuse case;reinforcement learning;software deployment	John Bragg;Ibrahim Habli	2018		10.1007/978-3-319-99229-7_35	safety case;systems engineering;harm;reinforcement learning;as is;computer science;safety assurance;dynamic decision-making	ML	-48.48423387874385	38.17920016973115	104341
9e2bb1a8b1864c7c06865572223ca0d9711d0fdb	optimal software release using time and cost benefits via fuzzy multi-criteria and fault tolerance	software testing;centre of gravity;software test effort ste;fuzzy rules based;software development effort sde;fuzzy logic;confidence;kilo line of code kloc;fuzzy multi criteria approach;fault tolerance;decision makers dm	As we know every software development process is pretty large and consists of different modules. This raises the idea of prioritizing different software modules so that important modules can be tested by preference. In the software testing process, it is not possible to test each and every module regressively, which is due to time and cost constraints. To deal with these constraints, this paper proposes an approach that is based on the fuzzy multi-criteria approach for prioritizing several software modules and calculates optimal time and cost for software testing by using fuzzy logic and the fault tolerance approach. Keywords—Software Testing, Fuzzy Multi-Criteria Approach, Fuzzy Logic, Fuzzy Rules Based, Confidence, Centre of Gravity, Fault Tolerance, Kilo Line of Code (KLOC), Software Development Effort (SDE), Software Test Effort (STE), Decision Makers (DM)	fault tolerance;fuzzy logic;fuzzy rule;fuzzy set;modular programming;rule-based system;software development process;software release life cycle;software testing;test effort;vagueness	Praveen Ranjan Srivastava	2012	JIPS	10.3745/JIPS.2012.8.1.021	fuzzy logic;center of mass;fuzzy electronics;fault tolerance;verification and validation;software sizing;software verification;computer science;software design;software reliability testing;software development;software design description;operating system;machine learning;software construction;data mining;database;software testing;confidence;goal-driven software development process;software quality;software metric	SE	-60.87984491291077	32.76543629563324	104696
ce484d7a889fb3b0fa455086af0bfa453c5ec3db	identification of extract method refactoring opportunities for the decomposition of methods	module decomposition;quality improvement;extract method refactoring;program slicing;extraction method	The extraction of a code fragment into a separate method is one of the most widely performed refactoring activities, since it allows the decomposition of large and complex methods and can be used in combination with other code transformations for fixing a variety of design problems. Despite the significance of Extract Method refactoring towards code quality improvement, there is limited support for the identification of code fragments with distinct functionality that could be extracted into new methods. The goal of our approach is to automatically identify Extract Method refactoring opportunities which are related with the complete computation of a given variable (complete computation slice) and the statements affecting the state of a given object (object state slice). Moreover, a set of rules regarding the preservation of existing odule decomposition dependences is proposed that exclude refactoring opportunities corresponding to slices whose extraction could possibly cause a change in program behavior. The proposed approach has been evaluated regarding its ability to capture slices of code implementing a distinct functionality, its ability to resolve existing design flaws, its impact on the cohesion of the decomposed and extracted methods, and its ability to preserve program behavior. Moreover, precision and recall have been computed employing the refactoring depe opportunities found by in	code refactoring;computation;precision and recall;software quality	Nikolaos Tsantalis;Alexander Chatzigeorgiou	2011	Journal of Systems and Software	10.1016/j.jss.2011.05.016	reliability engineering;quality management;program slicing;real-time computing;computer science;engineering drawing;code refactoring	SE	-57.35140145625959	35.12638806575806	104954
3fcd5c4c625c19806819289a3a9aecf1576a3cac	automatic detection of interaction vulnerabilities in an executable specification	automated design;design process;human computer interaction;executable specification;automation surprise analysis;automation design;automatic detection;time use;user interaction;requirement specification	This paper presents an approach to providing designers with the means to detect Human-Computer Interaction (HCI) vulnerabilities without requiring extensive HCI expertise. The goal of the approach is to provide timely, useful analysis results early in the design process, when modifications are less expensive. The twin challenges of providing timely and useful analysis results led to the development and evaluation of computational analyses, integrated into a software prototyping toolset. The toolset, referred to as the Automation Design and Evaluation Prototyping Toolset (ADEPT) was constructed to enable the rapid development of an executable specification for automation behavior and user interaction. The term executable specification refers to the concept of a testable prototype whose purpose is to support development of a more accurate and complete requirements specification.	executable	Michael Feary	2007		10.1007/978-3-540-73331-7_53	reliability engineering;real-time computing;computer science;systems engineering	Security	-56.03986710243558	37.979307544245316	105008
249e130880c653dfedf51c23d1a4b96d9a9c624b	improving your software using static analysis to find bugs	java bytecode;java programming;boolean operation;findbugs;static analysis;java	FindBugs looks for bugs in Java programs. It is based on the concept of bug patterns. A bug pattern is a code idiom that is often an error. Bug patterns arise for a variety of reasons, such as difficult language features, misunderstood API semantics, misunderstood invariants when code is modified during maintenance, garden variety mistakes: typos, use of the wrong boolean operator and simple mistakes such as typos.FindBugs uses static analysis to inspect Java bytecode for occurrences of bug patterns. We have found that FindBugs finds real errors in most Java software. Because its analysis is sometimes imprecise, FindBugs can report false warnings, which are warnings that do not indicate true errors. In practice, the rate of false warnings reported by FindBugs is generally lower than 50%, often much lower.	application programming interface;findbugs;java bytecode;software bug;static program analysis	Brian Cole;Daniel Hakim;David Hovemeyer;Reuven Lazarus;William Pugh;Kristin Stephens	2006		10.1145/1176617.1176667	real-time computing;computer science;programming language;java;static analysis;algorithm;java annotation	PL	-57.93977857647643	38.79050644042474	105064
8b990b3a9193e051c5d971bc904cffa38e2bf5f7	towards automatic software model checking of thousands of linux modules - a case study with avinux		Modular software model checking of large real-world systems is known to require extensive manual effort in environment modelling and preparing source code for model checking. Avinux is a tool chain that facilitates the automatic analysis of Linux and especially of Linux device drivers. The tool chain is implemented as a plugin for the Eclipse IDE, using the source code bounded model checker CBMC as its backend. Avinux supports a verification process for Linux that is built upon specification annotations with SLICx (an extension of the SLIC language), automatic data environment creation, source code transformation and simplification, and the invocation of the verification backend. In this paper technical details of the verification process are presented: Using Avinux on thousands of drivers from various Linux versions lead to the discovery of six new errors. In these experiments, Avinux also reduced the immense overhead of manual code preprocessing that other projects incurred.	device driver;download;eclipse;experiment;linux;linux;microsoft windows;model checking;modular programming;operating system;overhead (computing);precondition;preprocessor;program transformation;software bug;software inspection;spurious emission;text simplification;toolchain;tracing (software);world-system	Hendrik Post;Carsten Sinz;Wolfgang Küchlin	2009	Softw. Test., Verif. Reliab.	10.1002/stvr.399		SE	-53.17621519352202	35.90675058870992	105068
7c7628a6300303ad1dd9240654cd4e81565dd515	jmctest: automatically testing inter-method contracts in java		Over the years, Design by Contract (DbC) has evolved as a powerful concept for program documentation, testing, and verification. Contracts formally specify assertions on (mostly) object-oriented programs: pre- and postconditions of methods, class invariants, allowed call orders, etc. Missing in the long list of properties specifiable by contracts are, however, method correlations: DbC languages fall short on stating assertions relating methods.	java	Paul Börding;Jan Haltermann;Marie-Christine Jakobs;Heike Wehrheim	2018		10.1007/978-3-319-99927-2_4	dbc;documentation;programming language;design by contract;java;computer science	SE	-50.370818304472145	33.10992679941461	105334
2f87e0b61d9a3393184894737ce128d0a41fcb79	refactoring clones: an optimization problem	software;optimisation;refactoring;complexity theory;maximum common subgraph;program dependence graph software clones refactoring maximum common subgraph optimization;software maintenance;reusability optimization problem software clones refactoring common functionality extraction matching process;cloning;software clones;software reusability optimisation pattern matching software maintenance;pattern matching;software reusability;process control;software algorithms;optimization;cloning software algorithms process control complexity theory algorithm design and analysis optimization software;algorithm design and analysis;program dependence graph	The refactoring of software clones is achieved by extracting their common functionality into a single method. Any differences in identifiers and literals between the clones have to become parameters in the extracted method. Obviously, a large number of differences leads to an extracted method with limited reusability due to the large number of introduced parameters. We support that minimizing the differences between the matched statements of clones is crucial for the purpose of refactoring and propose an algorithm that treats the matching process as an optimization problem.	algorithm;code refactoring;identifier;mathematical optimization;optimization problem	Giri Panamoottil Krishnan;Nikolaos Tsantalis	2013	2013 IEEE International Conference on Software Maintenance	10.1109/ICSM.2013.47	algorithm design;computer science;systems engineering;engineering;theoretical computer science;pattern matching;process control;cloning;programming language;software maintenance;code refactoring	SE	-58.4872808790846	35.36353964318579	105611
98cff4f58783e65aab273faf7d59730a8cdd019b	suppressing detection of inconsistency hazards with pattern learning	context inconsistency;pattern learning;inconsistency hazard	Context: Inconsistency detection and resolution is critical for context-aware applications to ensure their normal execution. Contexts, which refer to pieces of environmental information used by applications, are checked against consistency constraints for potential errors. However, not all detected inconsistencies are caused by real context problems. Instead, they might be triggered by improper checking timing. Such inconsistencies are ephemeral and usually harmless. Their detection and resolution is unnecessary, and may even be detrimental. We name them inconsistency hazards. Objective: Inconsistency hazards should be prevented from being detected or resolved, but it is not straightforward since their occurrences resemble real inconsistencies. In this article, we present SHAP, a pattern-learning based approach to suppressing the detection of such hazards automatically. Method: Our key insight is that detection of inconsistency hazards is subject to certain patterns of context changes. Although such patterns can be difficult to specify manually, they may be learned effectively with data mining techniques. With these patterns, we can reasonably schedule inconsistency detections. Results: The experimental results show that SHAP can effectively suppress the detection of most inconsistency hazards (over 90%) with negligible overhead. Conclusions: Comparing with other approaches, our approach can effectively suppress the detection of inconsistency hazards, and at the same time allow real inconsistencies to be detected and resolved timely.	consistency (database systems);data mining;overhead (computing);sensor	Wang Xi;Chang Xu;Wenhua Yang;Xiaoxing Ma;Ping Yu;Jian Lu	2016	Information & Software Technology	10.1016/j.infsof.2015.08.003	real-time computing;engineering;data mining;computer security	SE	-59.78290479004296	43.88619755448361	105648
3f69b09952f8fa357cb6208688c467fc7d96cb24	deviance from perfection is a better criterion than closeness to evil when identifying risky code	artificial immune system;maintenance;rule based;design defects;good practice;automatic detection;artificial immune systems;open source	We propose an approach for the automatic detection of potential design defects in code. The detection is based on the notion that the more code deviates from good practices, the more likely it is bad. Taking inspiration from artificial immune systems, we generated a set of detectors that characterize different ways that a code can diverge from good practices. We then used these detectors to measure how far code in assessed systems deviates from normality.  We evaluated our approach by finding potential defects in two open-source systems (Xerces-J and Gantt). We used the library JHotDraw as the code base representing good design/programming practices. In both systems, we found that 90% of the riskiest classes were defects, a precision far superiour to state of the art rule-based approaches.	artificial immune system;centrality;gantt chart;logic programming;open-source software;sensor	Marouane Kessentini;Stéphane Vaucher;Houari A. Sahraoui	2010		10.1145/1858996.1859015	rule-based system;computer science;engineering;artificial intelligence;computer security;artificial immune system;algorithm	SE	-61.744128858018044	38.47371459699264	105767
1cc22b90c888aed1afdcb47a1a90817965eeaae8	evolving requirements-to-code trace links across versions of a software system	software systems;feature extraction;crawlers;computer science;conferences	Trace links provide critical support for numeroussoftware engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, as the system evolves over time, there is a tendency forthe quality of trace links to degrade into a tangle of inaccurateand untrusted links. This is especially true with the links between source-code and upstream artifacts such as requirements–because developers frequently refactor and change code withoutupdating the links. We present TLE (Trace Link Evolver), asolution for automating the evolution of trace links as changesare introduced to source code. We use a set of heuristics, open source tools, and information retrieval methods to detectcommon change scenarios across different versions of software. Each change scenario is then associated with a set of linkevolution heuristics which are used to evolve trace links. Weevaluate our approach through a controlled experiment and alsothrough applying it across 27 releases of the Cassandra DatabaseSystem. Results show that the trace links evolved using ourapproach are significantly more accurate than those generatedusing information retrieval alone.	apache cassandra;code refactoring;heuristic (computer science);information retrieval;integrated circuit;open-source software;precision and recall;requirement;software system;test case;unified modeling language;viable system model	Mona Rahimi;William Goss;Jane Cleland-Huang	2016	2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2016.57	feature extraction;computer science;engineering;theoretical computer science;software engineering;data mining;world wide web;software system	SE	-58.40200946703502	35.80357484609669	105829
8c14fdfc07c7ddc98215384e3913dea808eed438	a java application framework for scientific software development	java application framework;component based software development;reusable components	This paper presents AIBench, a Java desktop application framework mainly focused on scientific software development, with the goal of improving the productivity of research groups. Following the MVC design pattern, the programmer is able to develop applications using only three types of concepts: operations, data-types and views. The framework provides the rest of the functionality present in typical scientific applications, including user parameter requests, logging facilities, multi-threading execution, experiment repeatability and graphic user interface generation, among others. The proposed framework is implemented following a plugin-based architecture which also allows assembling new applications by the reuse of modules from past development projects.	application framework;design pattern;desktop computer;graphical user interface;java desktop system;plug-in (computing);programmer;repeatability;software development;thread (computing)	Florentino Fernández Riverola;Daniel Glez-Peña;Hugo López-Fernández;Miguel Reboiro-Jato;José Ramon Méndez	2012	Softw., Pract. Exper.	10.1002/spe.1108	real-time computing;computer science;software framework;component-based software engineering;operating system;software engineering;programming language	SE	-52.319730463907526	32.546374746443455	106378
5d8e048fc9befe9becf8552845960dfdf82de4b0	predicting bugs' components via mining bug reports	software engineering;text classification;bug triage;prediction accuracy;bug reports;prediction model;support vector machine;predictive model	The number of bug reports in complex software increases dramatically. Since bugs are still triaged manually, bug triage or assignment is a labor-intensive and time-consuming task. Without knowledge about the structure of the software, testers often specify the component of a new bug incorrectly. Meanwhile, it is difficult for triagers to determine the component of the bug only by its description. For instance, we dig out the components of 28,829 bugs from the Eclipse bug project, which have been specified incorrectly and modified at least once, and indicated that these bugs have to be reassigned and the process of bug fixing has to be delayed. The average time of fixing incorrectly specified bugs is longer than that of correctly specified ones. In order to solve the problem automatically, we use historical fixed bug reports as training corpus and build classifiers based on support vector machines and Naïve Bayes to predict the component of a new bug. The best predicting precision reaches up to 81.21% on our validation corpus of Eclipse project.	assignment (computer science);eclipse;naive bayes classifier;software bug;support vector machine	Deqing Wang;Hui Zhang;Rui Liu;Mengxiang Lin;Wenjun Wu	2012	JSW	10.4304/jsw.7.5.1149-1154	computer science;machine learning;data mining;predictive modelling;software regression;world wide web	SE	-62.24679843513826	39.21446760202373	106444
18bd88b03b0e266ce0396c1a270a95a176b1740a	finding resource-release omission faults in linux	high rate;drivers directory;continual problem;false positive;resource-releasing operation;resource-release omission fault;systems code;error-handling code;false positive rate;c code;missing resource-releasing operation;error handling;memory leaks	The management of the releasing of allocated resources is a continual problem in ensuring the robustness of systems code. Missing resource-releasing operations lead to memory leaks and deadlocks. A number of approaches have been proposed to detect such problems, but they often have a high rate of false positives, or focus only on commonly used functions. In this paper we observe that resource-releasing operations are often found in error-handling code, and that the choice of resource-releasing operation may depend on the context in which it is to be used. We propose an approach to finding resource-release omission faults in C code that takes into account these issues. We use our approach to find over 100 faults in the drivers directory of Linux 2.6.34, with a false positive rate of only 16%, well below the 30% that has been found to be acceptable to developers.	algorithm;deadlock;linux;memory leak;pointer (computer programming)	Suman Saha;Julia L. Lawall;Gilles Muller	2011	Operating Systems Review	10.1145/2094091.2094094	exception handling;real-time computing;type i and type ii errors;false positive rate;computer science;programming language;computer security;algorithm;memory leak	SE	-62.33715224358466	37.97539261611927	106748
f9d77e30d4f7941c6aa77b7cf2e07ff39be472fd	a comparative evaluation of static analysis actionable alert identification techniques	actionable static analysis alert identification;automated static analysis;comparative evaluation	Automated static analysis (ASA) tools can identify potential source code anomalies that could lead to field failures. Developer inspection is required to determine if an ASA alert is important enough to fix, or an actionable alert. Supplementing current ASA tools with automated identification of actionable alerts could reduce developer inspection overhead, leading to an increase in industry adoption of ASA tools. The goal of this research is to inform the selection of an actionable alert identification technique for ranking the output of automated static analysis through a comparative evaluation of actionable alert identification techniques. We investigated six actionable alert identification techniques on three subject projects. Among these six techniques, the systematic actionable alert identification (SAAI) technique reported an average accuracy of 82.5% across the three subject projects when considering both ASA tools evaluated. Check 'n' Crash reported an average accuracy of 85.8% for the single ASA tool evaluated. The other actionable alert identification techniques had average accuracies ranging from 42.2%-78.2%.	overhead (computing);static program analysis	Sarah Smith Heckman;Laurie A. Williams	2013		10.1145/2499393.2499399	reliability engineering;engineering;data mining;computer security	SE	-62.66591045372886	37.17652692311571	106824
f2e60d0bb54070184e8415f9f525e4247ce702cb	flexible feature binding with aspectj-based idioms		In Software Product Lines (SPL), we can bind reusable features to compose a product at different times, which in general are static or dynamic. The former allows customizability without any overhead at runtime. On the other hand, the latter allows feature activation or deactivation while running the application with the cost of performance and memory consumption. To implement features, we might use aspect-oriented programming (AOP), in which aspects enable a clear separation between invariable code and variable code. In this context, recent work provides AspectJ-based idioms to implement flexible feature binding. However, we identified some design deficiencies. Thus, to solve the issues of these idioms, we incrementally create three AspectJ-based idioms. We apply these idioms to provide flexible binding for 16 features from five different product lines. Moreover, to evaluate our idioms, we quantitatively analyze them with respect to code cloning, scattering, tangling, and size by means of software metrics. Besides that, we qualitatively discuss our idioms in terms of code reusability, changeability, instrumentation overhead, behavior, and feature interaction. In conclusion, we show evidences that our idioms address the issues of those existing ones.	aspect-oriented programming;aspectj;bespoke;code reuse;feature interaction problem;neural binding;overhead (computing);run time (program lifecycle phase);software metric;software product line	Rodrigo Andrade;Henrique Rebêlo;Márcio Ribeiro;Paulo Borba	2014	J. UCS	10.3217/jucs-020-05-0692	real-time computing;computer science;operating system;programming language;algorithm	SE	-54.015975608796715	39.37097059236766	107157
3e39b1645927240328e2a64fbb780eda1e8d01c6	in-parameter-order: a test generation strategy for pairwise testing	system testing computer science reactive power independent component analysis software testing software systems;software testing;optimisation;test generation strategy;optimization technique;specification based testing;software systems;independent component analysis;local optimization;ipo strategy;computational complexity program testing optimisation;in parameter order;program testing;pairwise testing;computational complexity;system testing;test generation;np complete;computer science;test generation tool in parameter order test generation strategy pairwise testing specification based testing software testing np complete ipo strategy local optimization;test generation tool;reactive power	Pairwise testing (or 2-way testing) is a specification-based testing criterion, which requires that for each pair of input parameters of a system, every combination of valid values of these two parameters be covered by at least one test case. Empirical results show that pairwise testing is practical and effective for various types of software systems. We show that the problem of generating a minimum test set for pairwise testing is NP-complete. We propose a test generation strategy, called in-parameter-order (or IPO), for pairwise testing. For a system with two or more input parameters, the IPO strategy generates a pairwise test set for the first two parameters, extends the test set to generate a pairwise test set for the first three parameters, and continues to do so for each additional parameter. The IPO strategy allows the use of local optimization techniques for test generation and the reuse of existing tests when a system is extended with new parameters or new values of existing parameters. We present practical, IPO-based test generation algorithms. We describe the implementation of an IPO-based test generation tool and show some empirical results.		Yu Lei;Kuo-Chung Tai	1998		10.1109/HASE.1998.731623	reliability engineering;independent component analysis;np-complete;all-pairs testing;computer science;local search;software engineering;data mining;software testing;ac power;computational complexity theory;system testing;statistics;software system	EDA	-59.47916369568179	34.13908030898538	107303
71c57f0fb0c40fec82a183d74a4d2955a0568150	mbt/cpn: a tool for model-based software testing of distributed systems protocols using coloured petri nets		Model-based testing is an approach to software testing based on generating test cases from models. The test cases are then executed against a system under test. Coloured Petri Nets (CPNs) have been widely used for modeling, validation, and verification of concurrent software systems, but their application for model-based testing has only been explored to a limited extent. The contribution of this paper is to present the MBT/CPN tool, implemented through CPN Tools, to support test case generation from CPN models. We illustrate the application of our approach by showing how it can be used for model-based testing of a Go implementation of the coordinator in a two-phase commit protocol. In addition, we report on experimental results for Go-based implementations of a distributed storage protocol and the Paxos distributed consensus protocol. The experiments demonstrate that the generated test cases yield a high statement coverage.	coloured petri net;distributed computing;model-based testing;software testing	Rui Wang;Lars Michael Kristensen;Volker Stolz	2018		10.1007/978-3-030-00359-3_7	software system;paxos;system under test;petri net;software;test case;distributed computing;cpn tools;computer science;consensus	SE	-49.78913184405667	35.3936694302763	107430
103b98b3d8441701eea65803cd0b21c30741f3aa	towards the automatic evolution of reengineering tools	computer languages;programming language;lakes;code standards;software engineering;grammars;c language;automatic evolution;genetic algorithm;genetic algorithms;c grammar;software tools;parser;source code analysis grammar inference genetic algorithms;software reengineering;c grammar reverse engineering automatic evolution software reengineering programming language parser grammar maintenance genetic algorithm grammar inference;c language reverse engineering grammars genetic algorithms systems re engineering;source code analysis;grammar maintenance;grammar inference;algorithm design and analysis;open source software;reverse engineering;java;computer languages genetic algorithms java reverse engineering lakes software tools algorithm design and analysis software engineering code standards open source software;systems re engineering	Building reverse engineering or reengineering tools often requires parsers for many different programming languages. The diffusion of dialects and variants makes many available parsers almost useless. While manual grammar maintenance is feasible, it can be a long, tedious and expensive task. This paper proposes to adopt genetic algorithms to evolve existing grammars inferring changes from examples written using the dialect. Applying grammar inference from scratch may lead to a useless grammar, while the proposed approach simply applies changes to the original grammar when needed, thus producing a meaningful grammar. The paper reports some preliminary results related to the evolution of a C grammar.	code refactoring;evolution;genetic algorithm;grammar induction;parsing;programming language;reverse engineering;semiconductor industry	Massimiliano Di Penta;Kunal Taneja	2005	Ninth European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2005.52	grammar systems theory;natural language processing;synchronous context-free grammar;genetic algorithm;computer science;parsing;programming language;adaptive grammar;algorithm	SE	-53.34160605579785	35.82461130739811	107834
a626cd23921f81b091bdeb7b073b3e83a0935e9d	an industrial experience report on legacy data-intensive system migration	legacy data intensive system migration;legacy coda syl database;cobol;software maintenance;cobol system;relational databases intrusion detection protocols data models software systems application software computer architecture data mining data engineering design engineering;experience report;lines of code;legacy coda syl database legacy data intensive system migration cobol system;relational databases;software maintenance cobol relational databases	This paper presents an experience report on the migration of a COBOL system of over 2 million lines of code. The main goal of this project was to migrate a legacy CODASYL database towards a relational platform, while preserving the functionalities of the legacy application programs .	cobol;codasyl;data access;database;display resolution;iteration;legacy system;requirement;scalability;source lines of code;system migration	Jean Henrard;Didier Roland;Anthony Cleve;Jean-Luc Hainaut	2007		10.1109/ICSM.2007.4362661	software modernization;relational database;computer science;software engineering;database;cobol;programming language;software maintenance;source lines of code	DB	-52.16496326092081	32.350564761219196	108048
06b46e9f45500fb360ad9bd76f78f99d86beb978	slicing aspect-oriented software	debugging;software testing;computer languages;object oriented modeling variable speed drives application software software engineering debugging software testing computer languages software algorithms computer science flow graphs;aspect oriented software;nonaspect code;maintenance;application software;dependence arcs;software maintenance;system dependence graph;program comprehension;dependence graph;testing;software engineering;flow graphs;model checking;program testing;aspect oriented programming;variable speed drives;software algorithms;aspect oriented;program debugging;computer science;aspect oriented system dependence graph;program slicing;object oriented modeling;software maintenance program slicing program debugging reverse engineering program testing;static slice;static slice aspect oriented software program slicing software engineering program comprehension debugging testing maintenance model checking aspect oriented system dependence graph system dependence graph nonaspect code dependence arcs;reverse engineering	Program slicing has many applications in software engineering activities including program comprehension, debugging, testing, maintenance, and model checking. In this paper, we propose an approach to slicing aspect-oriented software. To solve this problem, we present a dependencebased representation called aspect-oriented system dependence graph (ASDG), which extends previous dependence graphs, to represent aspect-oriented software. The ASDG of an aspect-oriented program consists of three parts: a system dependence graph for non-aspect code, a group of dependence graphs for aspect code, and some additional dependence arcs used to connect the system dependence graph to the dependence graphs for aspect code. After that, we show how to compute a static slice of an aspect-oriented program based on the ASDG.	aspect-oriented software development;debugging;model checking;program comprehension;program slicing;software engineering;software testing	Jianjun Zhao	2002		10.1109/WPC.2002.1021346	program slicing;real-time computing;aspect-oriented programming;computer science;theoretical computer science;software engineering;software testing;programming language;static program analysis	SE	-54.316382305514196	32.37605675817144	108608
0d53635f47b304287f95ef1f37caae99d04ba29c	specifying algorithm visualizations: interesting events or state mapping?	developpement logiciel;visualizacion;algorithm visualization;analisis programa;visualization;visualisation;desarrollo logicial;software development;algorithm animation;program analysis;analyse programme	Perhaps the most popular approach to animating algorithms consists of identifying interesting events in the implementation code, corresponding to relevant actions in the underlying algorithm, and turning them into graphical events by inserting calls to suitable visualization routines. Another natural approach conceives algorithm animation as a graphical interpretation of the state of the computation of a program, letting graphical objects in a visualization depend on a program’s variables. In this paper we provide the first direct comparison of these two approaches, identifying scenarios where one might be preferable to the other. The discussion is based on examples realized with the systems Polka and Leonardo.	algorithm;bubble sort;computation;graphical user interface;leonardo (robot);whole earth 'lectronic link	Camil Demetrescu;Irene Finocchi;John T. Stasko	2001		10.1007/3-540-45875-1_2	simulation;visualization;computer science;theoretical computer science;software engineering;programming language;algorithm;computer graphics (images)	SE	-54.41476666255689	36.06246264103489	108884
22a65cad91f38074435b22f2d60c20f2db02acac	generating minimal fault detecting test suites for boolean expressions	coverage criteria;minimization;ing inf 05 sistemi di elaborazione delle informazioni;fault based testing;generators;electronic mail;boolean functions;boolean expressions;computability;logical satisfiability problem;software fault tolerance;testing;fault based testing test generation boolean expressions dnf testing;satisfiability;software fault tolerance boolean functions computability program testing;program testing;monitoring;fault detection;benchmarks;fault detection automatic testing software testing software engineering logic testing benchmark testing aerospace electronics algorithm design and analysis monitoring thyristors;test generation;sat algorithm;minimal fault detecting test suites;benchmarks minimal fault detecting test suites boolean expressions coverage criteria fault detection reduction policy logical satisfiability problem sat algorithm;context;reduction policy;dnf testing	New coverage criteria for Boolean expressions are regularly introduced with two goals: to detect specific classes of realistic faults and to produce as small as possible test suites. In this paper we investigate whether an approach targeting specific fault classes using several reduction policies can achieve that less test cases are generated than by previously introduced testing criteria. In our approach, the problem of finding fault detecting test cases can be formalized as a logical satisfiability problem, which can be efficiently solved by a SAT algorithm. We compare this approach with respect to the well-known MUMCUT and Minimal-MUMCUT strategies by applying it to a series of case studies commonly used as benchmarks, and show that it can reduce the number of test cases further than Minimal-MUMCUT.	algorithm;benchmark (computing);boolean expression;boolean satisfiability problem;fault detection and isolation;sensor;software testing;test case;test suite;whole earth 'lectronic link	Gordon Fraser;Angelo Gargantini	2010	2010 Third International Conference on Software Testing, Verification, and Validation Workshops	10.1109/ICSTW.2010.51	reliability engineering;boolean expression;computer science;stuck-at fault;theoretical computer science;software testing;computability;boolean function;test case;fault detection and isolation;algorithm;software fault tolerance;satisfiability	SE	-59.710459651335185	33.93273727851473	109053
3a62c69c0f5680211f182b85563875d1b7a92026	the optimized grouping value for precise similarity comparison of dynamic birthmark	similarity comparison;software piracy and software theft detection;software birthmark;dynamic analysis	Software birthmark is a technique for preventing and identifying plagiarism through illegal software modification using feature information from said software. Presently, research on birthmarks differentiates between static birthmarks and dynamic birthmarks according to the way of extracting feature information. Dynamic birthmarks are advantageous because such birthmarks can be extracted from a program without the source code. Because they are extracted from the executed program, however, dynamic birthmarks are susceptible to noise. Our previous research suggests that similarity comparison of dynamic birthmarks is an applied feature of dynamic birthmarks. In addition and through our previous research, similarity comparison of dynamic birthmarks were proven to have better performance than existing similarity comparison of birthmarks. Cosine similarities differed by grouping numbers, and the grouping number of the separated cosine was defined with a random number (1,000) in our previous research. In this paper, we find an optimized grouping value for similarity comparison of dynamic birthmark through experimentation.	random number generation	Daeshin Park;Youngsu Park;John Kim;Jiman Hong	2014		10.1145/2663761.2664197	computer science;artificial intelligence;computer security;engineering drawing	SE	-60.5253392395522	40.34963536993775	109496
1ad541152429bbbc78d2e58aa929b45a9fb94822	applying webmining techniques to execution traces to support the program comprehension process	cognitive science;program diagnostics;application software;software maintenance;program comprehension;webmining techniques;execution traces;object oriented programming;data mining;program diagnostics reverse engineering object oriented programming internet data mining software maintenance;software performance;internet;programming profession;object oriented modeling programming profession reverse engineering software performance cognitive science software maintenance cognition application software object oriented databases mathematical model;cognition;mathematical model;web mining;object oriented databases;program comprehension reverse engineering dynamic analysis web mining;object oriented program;object oriented modeling;program comprehension process;dynamic analysis webmining techniques program comprehension process object oriented program execution traces reverse engineering;dynamic analysis;reverse engineering	Well-designed object-oriented programs typically consist of a few key classes that work tightly together to provide the bulk of the functionality. As such, these key classes are excellent starting points for the program comprehension process. We propose a technique that uses Webmining principles on execution traces to discover these important and tightly interacting classes. Based on two medium-scale case studies - Apache Ant and Jakarta JMeter - and detailed architectural information from its developers, we show that our heuristic does in fact find a sizeable number of the classes deemed important by the developers.	algorithm;apache ant (another neat tool);apache jmeter;coupling (computer programming);data mining;heuristic;interaction;pagerank;program comprehension;scalability;software engineer;structure mining;tracing (software)	Andy Zaidman;Toon Calders;Serge Demeyer;Jan Paredaens	2005	Ninth European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2005.12	web mining;computer science;systems engineering;software engineering;programming language;object-oriented programming	SE	-55.88039108846172	35.69395605387381	109710
85475496fdf2042a87ff031c4cd76fbcf2e1df0d	conceptual entropy and its effect on class hierarchies	object oriented design;abstract data types;object oriented programming conceptual entropy class hierarchies frequent change shared structures additional subclassing existing classes entropic tendencies conceptual inconsistency consistent hierarchies object oriented design automated classification tool;object oriented programming;data structures object oriented programming abstract data types;data structures;entropy land vehicles hardware marine vehicles classification tree analysis object oriented modeling object oriented programming engines	All systems that undergo frequent change characteristically tend toward disorder. This is known as entropy and is recognized in all branches of science. Class hierarchies are shared structures which, if useful, undergo frequent change in the form of additional subclassing, modification to existing classes, and sometimes the restructuring of the hierarchy itself. Given this frequent change, we can expect class hierarchies to exhibit entropic tendencies, which we term conceptual entropy. Conceptual entropy is manifested by increasing conceptual inconsistency as we travel down the hierarchy. That is, the deeper the level of the hierarchy, the greater the probability that a subclass will not consistently extend and/or specialize the concept of its superclass. Constructing and maintaining consistent class hierarchies is one of the most difficult activities of object-oriented design. The article describes an automated classification tool that helps minimize conceptual entropy.<<ETX>>	class hierarchy;entropy (information theory)	Joseph Dvorak	1994	Computer	10.1109/2.294856	data structure;computer science;artificial intelligence;theoretical computer science;object-oriented design;machine learning;data mining;programming language;object-oriented programming;abstract data type	SE	-54.06181966634646	33.2330996697595	109753
a40f010261bd76def0dd3f0090bbcb797e3da65b	a dataset of clone references with gaps	software maintenance;code clone;dataset	This paper introduces a new dataset of clone references, which is a set of correct clones consisting of their locational information with their gapped lines. Bellon's dataset is one of widely used clone datasets. Bellon's dataset contains many clone references, thus the dataset is useful for comparing accuracies among clone detectors. However, Bellon's dataset does not have locational information of gapped lines. Thus, Bellon's benchmark does not evaluate some Type-3 clones correctly. In order to resolve the problem, we added locational information of gapped lines to Bellon's dataset. The new dataset is available at ``http://sdl.ist.osaka-u.ac.jp/~h-murakm/2014_clone_references_with_gaps/''.   This paper also shows some examples that the new dataset and Bellon's dataset yield different evaluation results. Moreover, we report an experimental result that compares Bellon's dataset and the new dataset by using three clone detectors that can detect Type-3 clones. Finally, we conclude that the new dataset can evaluate Type-3 clones more correctly than Bellon's dataset.	benchmark (computing);sensor	Hiroaki Murakami;Yoshiki Higo;Shinji Kusumoto	2014		10.1145/2597073.2597133	bioinformatics;engineering;software engineering;data mining;software maintenance;world wide web;data set	NLP	-60.76621330229351	39.3866324072495	109980
7963b0b891dbac53d2fecbfcd44a63e952bd8d7e	a structural analysis approach to the evaluation of fault coverage for protocol conformance testing.	conformance testing;fault coverage;structure analysis	In this paper, we propose a structural analysis approach to the evaluation of fault coverage of protocol conformance testing based on the finite state machine model. The attractiveness of this approach is its low computational complexity. It allows us to calculate the fault coverage of a given test suite by directly analyzing the test suite against the specification machine. Therefore, it avoids the generation and execution of mutants. The approach has been implemented and a number of experiments has been carried out. Some of the experimental results are summarized in this paper to show the accuracy of this approach compared with the mutation analysis technique.		Ming Yu Yao;Alexandre Petrenko;Gregor von Bochmann	1994			modified condition/decision coverage;fault coverage;conformance testing;structural analysis	SE	-58.779416067401314	35.24763862848127	110070
b79d91011b1986745374cc271991d24bd87e1175	evaluating cross-platform development approaches for mobile applications		The fragmented smartphone market with at least five important mobile platforms makes native development of mobile applications (apps) a challenging and costly endeavour. Cross-platform development might alleviate this situation. Several cross-platform approaches have emerged, which we classify in a first step. In order to compare concrete cross-platform solutions, we compiled a set of criteria to assess crossplatform development approaches. Based on these criteria, we evaluated Web apps, apps developed with PhoneGap or Titanium Mobile, and – for comparison – natively developed apps. We present our findings as reference tables and generalize our results. Our criteria have proven to be viable for follow-up evaluations. With regard to the approaches, we found PhoneGap viable if very close resemblance to a native look & feel can be neglected.	apache cordova;compiler;endeavour (supercomputer);look and feel;mobile app;mobile device;smartphone;table (information)	Henning Heitkötter;Sebastian Hanschke;Tim A. Majchrzak	2012		10.1007/978-3-642-36608-6_8	web application;data mining;computer science;cross-platform	HCI	-54.762937531943535	42.23950568075952	110208
87c4d4c9f685b5c7bc84aa4f8270faee5042564c	automating repetitive tasks on web-based ides via an editable and reusable capture-replay technique	capture replay technique automating repetitive tasks web based ide;software;mice;web based ide;development efficiency repetitive tasks automation web based ide editable capture replay technique reusable capture replay technique software artifacts local development tool development time system setup programming context batch test web applications user friendly approach web based capture replay technique;browsers;automating repetitive tasks;graphical user interfaces;software reusability internet program testing;capture replay technique;optimization;context;graphical user interfaces context mice browsers optimization software automation;automation	Web-based IDEs are more and more popular because developers can create or modify software artifacts in the browser without need to install any local development tool and spend valuable development time on system setup and maintenance. For those development tasks using a Web-based IDE, such as configuring programming context and batch test etc., some are frequent and repetitive because they are similar from project to project. Automating the repetitive tasks on Web-based IDEs, regardless of their complexity, would reduce the amount of work that developers must perform to complete the tasks, which would improve the development efficiency of Web applications. In this paper, we put forward a user-friendly approach to automating repetitive tasks on existing Web-based IDEs. The key to the approach is to extend the basic Web-based capture-replay technique with editable and reusable features, which are necessary for automation because some operations are redundant, as well as developers should recognize and define repetitive tasks. Moreover, we develop a supporting tool for the approach. In the case study, we introduce how the approach is used to support automating repetitive tasks on Web-based IDEs. Case studies verify that the approach can improve the development efficiency very well.	integrated development environment;requirement;usability;web application	Yanchun Sun;Dejian Chen;Chao Xin;Wenpin Jiao	2015	2015 IEEE 39th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2015.12	real-time computing;computer science;operating system;automation;graphical user interface;database;programming language;world wide web	SE	-54.257365894635946	37.78522232697093	110233
9bfa44b44860c9ccf69c72d90060fcf9de43127c	design of a modified concolic testing algorithm with smaller constraints		Concolic testing is a well-known unit test generation technique. However, bottlenecks such as constraint solving prevents concolic testers to be used in large projects. We propose a modification to a standard concolic tester. Our modification makes more but smaller queries to the constraint solver, i.e. ignores some path conditions. We show that it is possible to reach the same branch coverage as the standard concolic tester while decreasing the burden on the constraint solver. We support our claims by testing several C programs with our method. Experimental results show that our modification improves runtime performance of the standard concolic tester in half of the experiments and results in more than 5x speedup when the unit under test has many infeasible paths.	algorithm;code coverage;concolic testing;constraint satisfaction problem;device under test;experiment;nonlinear system;run time (program lifecycle phase);solver;speedup;unit testing	Yavuz Köroglu;Alper Sen	2016			concolic testing;algorithm;computer science	SE	-59.93640796825972	36.88744681309066	111234
e291ee297321552c789b1045298fb07c82ca8d85	a tool for testing liveness in distributed object systems	distributed system;debugging;software testing;program diagnostics;concurrent computing;information science;application software;program debugging distributed object management program testing program diagnostics;testing and debugging;resource management;distributed computing;sequential analysis;experience report;finite execution;reactive behavior;formal verification;program testing;testing technology;debugging liveness testing distributed object systems cidl corba developers sequential systems reactive behavior liveness violations finite execution testing technology corba based distributed systems corba idl transient testing harness;cidl;distributed object system;distributed object management;system testing software testing distributed computing information science sequential analysis debugging concurrent computing resource management formal verification application software;corba idl;testing harness;system testing;program debugging;corba based distributed systems;distributed object systems;transient;liveness testing;corba developers;sequential systems;liveness violations	The paper describes cidl, a tool that helps CORBA developers test liveness properties in distributed systems. Whereas sequential systems can be tested by examining initial states and final outcomes, distributed systems frequently exhibit reactive behavior that occurs over time. Liveness properties capture such behavior. Testing liveness, however, presents a significant challenge because liveness violations can never be detected during a finite execution. We present a testing technology for CORBA based distributed systems. We define an extension to CORBA IDL for specifying a fundamental liveness property: transient. The cidl tool uses this extension to generate a testing harness for tracking liveness. We describe how to use cidl for testing and debugging and present a preliminary experience report.	distributed object;liveness	Charles P. Giles;Paolo A. G. Sivilotti	2000		10.1109/TOOLS.2000.868982	real-time computing;computer science;distributed computing;programming language;liveness	SE	-49.920222466723544	35.73048749724835	111288
0c627b02c40189d79175ae1d7ce497b3981a02b2	bug localization by learning to rank and represent bug inducing changes		In software development, bug localization is the process finding portions of source code associated to a submitted bug report. This task has been modeled as an information retrieval task at source code file, where the report is the query. In this work, we propose a model that, instead of working at file level, learns feature representations from source changes extracted from the project history at both syntactic and code change dependency perspectives to support bug localization.  To that end, we structured an end-to-end architecture able to integrate feature learning and ranking between sets of bug reports and source code changes.  We evaluated our model against the state of the art of bug localization on several real world software projects obtaining competitive results in both intra-project and cross-project settings. Besides the positive results in terms of model accuracy, as we are giving the developer not only the location of the bug associated to the report, but also the change that introduced, we believe this could give a broader context for supporting fixing tasks.	bug tracking system;debugging;end-to-end principle;feature learning;information retrieval;learning to rank;modeling perspective;software bug;software development;software development process;software system;test suite;tracing (software)	Pablo Loyola;Kugamoorthy Gajananan;Fumiko Satoh	2018		10.1145/3269206.3271811	data mining;learning to rank;source code;software;architecture;syntax;software development;computer science;feature learning;ranking	SE	-61.45440775686671	39.25671564755256	111339
2cb3536220092d085efc6eda64ac1266d1e6dbef	on the verification of software vulnerabilities during static code analysis using data mining techniques - (short paper)		Software assurance analysts deal with thousands of potential vulnerabilities many of which could be false positives during the process of static code analysis. Manual review of all such potential vulnerabilities is tedious, time consuming, and frequently impractical. Several experiments were conducted using a production code base with the aid of a variety of static code analysis tools. A data mining process was created, which employed different classifiers for comparison. Furthermore, a selection process identified the most important features that led to significant improvements in accuracy, precision, and recall, as evidenced by the experimental data. This paper proposes machine learning algorithms to minimize false positives with a high degree of accuracy.	data mining;static program analysis	Foteini Cheirdari;George Karabatis	2017		10.1007/978-3-319-69459-7_7	software construction;kpi-driven code analysis;software assurance;data mining;software verification;static program analysis;software;false positive paradox;development testing;computer science	ML	-62.19348520944677	37.66370158848348	111653
880ef2b9b659259f0f869c0fec3f34f0afbc70a8	an analysis of scripting languages for research in applied computing	software;applied computing;software engineering authoring languages bioinformatics;scripting languages;scripting languages applied computing open source;authoring languages;software engineering;qa75 electronic computers computer science;bioinformatics software computational biology software engineering java programming;computational biology;programming;java;open source;bioinformatics;computational biology scripting languages applied computing software engineering bioinformatics	There are several scripting languages that exist today. However, some are used more frequently and popular than others. This is due to certain characteristics and features that they possess. Particularly in applied computing fields like software engineering, bioinformatics and computational biology, scripting languages are gaining popularity. This paper presents a comparative study of ten popular scripting languages that are used in the above mentioned fields/area. For making comparison, we have identified the factors against which these languages are evaluated. Accordingly, based on selected criteria we determine their suitability in the fields of software engineering, bioinformatics and computational biology research. This will serve as a guide to researchers to choose the appropriate scripting language in the various fields.	bioinformatics;computational biology;scripting language;software engineering;tiobe index	Olugbenga O. Oluwagbemi;Adewole Adewumi;Sanjay Misra;Folakemi Majekodunmi;Luis Fernández Sanz	2013	2013 IEEE 16th International Conference on Computational Science and Engineering	10.1109/CSE.2013.174	computational science;programming;computing;computer science;domain-specific language;software development;third-generation programming language;end-user computing;computer programming;scripting language;programming language;java;second-generation programming language;comparison of multi-paradigm programming languages	SE	-55.812128668837815	33.923699360251334	111677
f24456e1b59ebc700426c8a89f276b29e59bc9a3	delving source code with formal concept analysis	institutional repositories;fedora;software maintenance;source code mining;software systems;software classification;software engineering;vital;design pattern;smalltalk;source code;vtls;ils;formal concept analysis	Getting an initial understanding of the structure of a software system, whether it is for software maintenance, evolution or reengineering purposes, is a nontrivial task. We propose a lightweight approach to delve a system’s source code automatically and efficiently for relevant concepts of interest: what concerns are addressed in the code, what patterns, coding idioms and conventions have been adopted, and where and how are they implemented. We use formal concept analysis to do the actual source-code mining, and then filter, classify and combine the results to present them in a format that is more convenient to a software engineer. We applied a prototype tool that implements this approach to several small to medium-sized Smalltalk applications. For each of these, the tool uncovered several design pattern instances, coding and naming conventions, refactoring opportunities and important domain concepts. Although the tool and approach can still be improved in many ways, the tool does already provides useful results when trying to get an initial understanding of a system. The obtained results also illustrate the relevance and feasibility of using formal concept analysis as an efficient technique for source code mining.	algorithm;code refactoring;duplicate code;entity;formal concept analysis;programming idiom;prototype;relevance;smalltalk;software design pattern;software engineer;software maintenance;software system;statistical classification	Kim Mens;Tom Tourwé	2005	Computer Languages, Systems & Structures	10.1016/j.cl.2004.11.004	kpi-driven code analysis;software visualization;verification and validation;code review;computer science;formal concept analysis;package development process;software framework;software development;software construction;data mining;design pattern;software walkthrough;programming language;software maintenance;static program analysis;software system;source code	SE	-56.83788822015227	35.525307719575295	112041
a811cfdefa2ee5c87ebc863f9d56ab1a45d89c1f	fault detection and diagnosis capabilities of test sequence selection methods based on the fsm model	fault detection;test sequence selection;fsm;conformance testing	Different test sequence selection methods, namely, the Dmethod, C-method, W-method, T-method, U-method, Uvmethod and the Wp-method, are reviewed and analysed for their fault detection capabilities. We show that the C-method and the Uv-method do not provide complete fault coverage. These seven methods are formally analysed for their fault diagnosis capabilities under single fault assumption. Among these methods the W-method and the Wp-method provide the best resolution in diagnosing the fault. The test sequence selection methods are then compared based on the length of the test sequences they select, and their fault detection and diagnosis capabilities.	fault coverage;fault detection and isolation	T. Ramalingam;Anindya Das;Krishnaiyan Thulasiraman	1995	Computer Communications			AI	-58.927539143675254	34.57445661554585	112339
3923cb3d129f6887c070212dec19aa7b4a8115b9	feature-to-code traceability in a collection of software variants: combining formal concept analysis and information retrieval	lattices;abstracting;information retrieval;dp industry;software systems;context large scale integration vectors lattices formal concept analysis software systems;large scale integration;vectors;software reusability;software reusability abstracting dp industry formal concept analysis information retrieval;context;abstraction gap feature to code traceability software variants formal concept analysis information retrieval ad hoc copying software system software industry software product line spl systematic reuse source code elements product variants code ir methods;formal concept analysis	Today, developing new software variant to meet new demands of customers by ad-hoc copying of already existing variants of a software system is a frequent phenomenon in the software industry. Typically, maintaining such variants becomes difficult and expensive over the time. To re-engineer such software variants into a software product line (SPL) for systematic reuse, it is important to identify source code elements that implement a specific feature in order to understand product variants code. Information Retrieval(IR) methods have been used widely to support this purpose in a single software. This paper proposes a new approach to improve the performance of IR methods in a collection of similar software variants. Our proposal produces following two improvements. First, increasing the accuracy of IR results by exploiting commonality and variability across software variants. Secondly, increasing the number of retrieved links that are relevant by reducing the abstraction gap between feature and source code levels. We have validated our approach with a set of variants of two different systems. The experimental results showed that the proposed approach outperforms the conventional application of IR as well as the most relevant work on the subject.	formal concept analysis;hoc (programming language);identifier;information retrieval;precision and recall;software industry;software product line;software system;spatial variability;traceability;vocabulary	Hamzeh Eyal Salman;Abdelhak-Djamel Seriai;Christophe Dony	2013	2013 IEEE 14th International Conference on Information Reuse & Integration (IRI)	10.1109/IRI.2013.6642474	domain analysis;verification and validation;software sizing;computer science;formal concept analysis;package development process;backporting;theoretical computer science;software framework;component-based software engineering;software development;software design description;operating system;software engineering;domain engineering;machine learning;lattice;software construction;data mining;database;software walkthrough;programming language;software analytics;software deployment;world wide web;software quality;static program analysis;software system	SE	-57.72973848661034	33.12887022075939	112384
0db043aa49e33cc8e2232397180a1893eb8f7674	encapsulating and exploiting change with changeboxes	performance evaluation;programming language;software systems;development environment	Real world software systems change continuously to meet new demands. Most programming languages and development environments, however, are more concerned with limiting the effects of change rather than enabling and exploiting change. Various techniques and technologies to exploit change have been developed over the years, but there exists no common support for these approaches. We propose Changeboxes as a general-purpose mechanism for encapsulating change as a first-class entity in a running software system. Changeboxes support multiple, concurrent and possibly inconsistent views of software artifacts within the same running system. Since Changeboxes are first-class, they can be manipulated to control the scope of change in a running system. Furthermore, Changeboxes capture the semantics of change. Changeboxes can be used, for example, to encapsulate refactorings, or to replay or analyze the history of changes. In this paper we introduce Changeboxes by means of a prototype implementation. We illustrate the benefits that Changeboxes offer for evolving software systems, and we present the results of a preliminary performance evaluation that assesses the costs associated with Changeboxes while suggesting possible strategies for improvement.	code refactoring;first-class citizen;general-purpose markup language;performance evaluation;programming language;prototype;software system	Marcus Denker;Tudor Gîrba;Adrian Lienhard;Oscar Nierstrasz;Lukas Renggli;Pascal Zumkehr	2007		10.1145/1352678.1352681	real-time computing;simulation;computer science;systems engineering;software development	SE	-54.7147081308796	39.48408602069699	112656
89a7679fe8d1509ba23725e0b141ab7b7372e045	a computation-oriented program experimentation system (copes)	program instrumentation;structural model;experimental analysis;performance analysis computational modeling cost function computer science automatic control time measurement software systems analytical models timing application software;cost function;program testing;control flow;copes static techniques control flow measurement program variable tracing computation oriented program experimentation system software experimental analysis approach dynamic techniques execution time computation cost functions computation structure model flow analysis technique program instrumentation techniques cost function control flow program variables;software tools;software tools program testing;flow analysis	A software experimental analysis approach that unifies static and dynamic techniques is presented. Our goal is to develop practical means to obtain precise, in-depth, and relevant performance information, especially execution time, for guiding design decisions. First, we derive computation cost functions of a program. This is done hy modeling the program using the computation structure model and then applying a flow analysis technique to derive the program's cost function. In addition, we employ program instrumentation techniques to measure control flows and to trace program variables. Finally, we compute precise execution time information by evaluating the cost function against the control flows measured. The three classes of parameten (execution time, control flow and program variables) can be analyzed selectively and interactively; this gives much insight about an individual design alternative's actual behavior. To cope with tedious and frequent analyses of large and complex software, we built a tool (COPES) that automates the static and dynamic techniques required. The implementation and the usage of this tool are illustrated.	computation;control flow;data-flow analysis;interactivity;loss function;run time (program lifecycle phase)	Reda A. Ammar;Hermes Law;Howard A. Sholl;Bin Qin	1989		10.1109/ICSMC.1989.71417	program analysis;real-time computing;simulation;computer science;data-flow analysis;dynamic program analysis;control flow;experimental analysis of behavior;static program analysis	PL	-55.247370438862276	37.00021217156286	112743
309d2d439ee815f0dda6e5dc81cbf2287656c137	oracle problem in software testing		The oracle problem remains one of the key challenges in software testing, for which little automated support has been developed so far. In my thesis work we introduce a technique for assessing and improving test oracles by reducing the incidence of both false positives and false negatives. Our technique combines test case generation to reveal false positives and mutation testing to reveal false negatives. The experimental results on five real-world subjects show that the fault detection rate of the oracles after improvement increases, on average, by 48.6% (86% over the implicit oracle). Three actual, exposed faults in the studied systems were subsequently confirmed and fixed by the developers. However, our technique contains a human in the loop, which was represented only by the author during the initial experiments. Our next goal is to conduct further experiments where the human in the loop will be represented by real developers. Our second future goal is to address the oracle placement problem. When testing software, developers can place oracles externally or internally to a method. Given a faulty execution state, i.e., one that differs from the expected one, an oracle might be unable to expose the fault if it is placed at a program point with no access to the incorrect program state or where the program state is no longer corrupted. In such a case, the oracle is subject to failed error propagation. Internal oracles are in principle less subject to failed error propagation than external oracles. However, they are also more difficult to define manually. Hence, a key research question is whether a more intrusive oracle placement is justified by its higher fault detection capability.	experiment;fault detection and isolation;incidence matrix;java;mutation testing;oracle database;oracle nosql db;oracle machine;propagation of uncertainty;software propagation;software testing;state (computer science);test case	Gunel Jahangirova	2017		10.1145/3092703.3098235	oracle;computer science;real-time computing;mutation testing;software;fault detection and isolation;test case;false positive paradox;false positives and false negatives;human-in-the-loop	SE	-61.24634583390446	37.63004491679051	113142
1796b3b0a4310a4fb784290fee7c427ada7081c6	logdc: problem diagnosis for declartively-deployed cloud applications with log		Recently, as the evolution of application's development and management paradigms, the deployment declaration becomes a standard interface connecting application developers and Cloud platforms. Kuberenetes is such a system for automating deployment, scaling, and management of micro-service based applications. However, managing and operating such a cloud benefit with additional complexities from the declarative deployment. This paper proposes a log model based problem diagnosis tool for declaratively-deployed cloud applications with the full lifecycle Kubernetes logs. With the runtime logs and deployment declarations, we can pinpoint the root causes in terms of abnormal declarative items and log entries. The advantage of this approach is that we provide a precise log model of a normal deployment to help diagnose problems. The experimental results show that our approach can find out the anomalies of some real-world Kubernetes problems, some of which have been confirmed as bugs. Within the given fault types, our approach can pinpoint the root causes at 91% in Precision and at 92% in Recall.	algorithm;anomaly detection;cloud computing;cluster analysis;declaration (computer programming);declarative programming;google cloud platform;image scaling;internet;mathematical optimization;reference model;software bug;software deployment	Jingmin Xu;Pengfei Chen;Lin Yang;Fan Jing Meng;Ping Wang	2017	2017 IEEE 14th International Conference on e-Business Engineering (ICEBE)	10.1109/ICEBE.2017.52	declaration;data mining;cloud computing;software deployment;computer science	SE	-53.18793201046737	41.58510281991371	113210
a79583be480001e4adcae76667b1fdd46d67ef85	reliability analysis of on-demand service-based software systems considering failure dependencies	software fault tolerance service based software systems service correlation common cause failures reliability modeling reliability analysis;correlation software reliability encoding analytical models software systems joints	Service-based software systems (SBSSs) are widely deployed due to the growing trend of distributed computing and cloud computing. It is important to ensure high quality of an SBSS, especially in a strongly competitive market. Existing works on SBSS reliability usually assumed independence of service failures. However, the fact that resource sharing exists in different levels of SBSS operations invalidates this assumption. Ignorance of failure dependencies have been discussed as potentially affecting system reliability predictions and lowering the benefits of design diversity, as typically seen in high-reliability systems. In this paper, we propose a reliability framework that incorporates failure dependence modeling, system reliability modeling, as well as reliability analysis for individual services and for failure sources. The framework is also capable of analyzing the internal structures of popular software fault tolerant (FT) schemes. The proposed method is applied to a travel agency system based upon a real-world practice for verifying its accuracy of reliability modeling and effectiveness of varied reliability measures. The results show that failure dependence of the services is an essential factor for analyzing any valuable SBSS system. Further, a set of reliability measures with different capabilities and complexities are available for assisting SBSS engineers with system improvements.	cloud computing;display resolution;distributed computing;failure cause;fault tolerance;reliability engineering;software system;verification and validation	Kuan-Li Peng;Chin-Yu Huang	2017	IEEE Transactions on Services Computing	10.1109/TSC.2015.2473843	reliability engineering;verification and validation;real-time computing;intra-rater reliability;software verification;computer science;software reliability testing;software construction;software testing;software fault tolerance	HPC	-48.36895745921607	40.27710983137817	113688
29bad29b0f77ed208f23569319584332aff169d2	a fast assembly level reverse execution method via dynamic slicing	program debugging;program slicing;reverse engineering;bug location;code parts;debugging aids;dynamic slicing;execution history;memory overhead reduction;program debugging;program point;repetitive program restarts;reverse execution	One of the most time consuming parts of debugging istrying to locate a bug. In this context, there are two powerfuldebugging aids which shorten debug time considerably:reverse execution and dynamic slicing. Reverse executioneliminates the need for repetitive program restartsevery time a bug location is missed. Dynamic slicing, onthe other hand, isolates code parts that influence an erroneousvariable at a program point. In this paper, we presentan approach which provides assembly level reverse executionalong a dynamic slice. In this way, a programmer notonly can find the instructions relevant to a bug, but also canobtain runtime values of variables in a dynamic slice whiletraversing the slice backwards in execution history.Reverse execution along a dynamic slice skips recoveringunnecessary program state; therefore, it is potentiallyfaster than full-scale reverse execution. The experimentalresults with four different benchmarks show a wide range ofspeedups from 1.3X for a small program with few data inputsto six orders of magnitude (1,928,500X) for 400x400matrix multiply. Furthermore, our technique is very memoryefficient. Our benchmark measurements show between3.4X and 2240X memory overhead reduction as comparedto our implementation of the same features using traditionalapproaches.	benchmark (computing);debugging;full scale;overhead (computing);programmer;run time (program lifecycle phase);software bug;state (computer science)	Tankut Akgul;Vincent John Mooney;Santosh Pande	2004	Proceedings. 26th International Conference on Software Engineering		program slicing;parallel computing;real-time computing;mark and recapture;computer science;operating system;software engineering;software inspection;assembly;programming language;debugging;writing;reverse engineering	SE	-59.96510260156231	37.60220392521642	113808
9f2d6bf96fdb0c1e8167f4ee198f8392501df38c	verification patterns for rapid embedded system verification	object oriented;pattern analysis;data retrieval;data acquisition;embedded system	Test result verification is always a costly task for embedded system testing. This paper presents a systematic process to develop verification patterns and use these patterns to verify test results for state-based real-time/embedded systems. The verification patterns are organized into an object-oriented verification framework so that it can be adaptive to changes rapidly The verification patterns are reusable and thus can save significant time and effort in constructing the test execution infrastructure and generating test scripts. This paper describes a systematic process to perform rapid embedded system verification: 1) abstracts verification patterns based on requirement/scenario patterns analysis; 2) generates test cases/scripts from the verification patterns; 3) develops data acquisition component for raw data retrieval and event assemblers; 4) executes test scripts locally or remotely, and 5) collects and analyzes the test results. This process has been applied to a caralarm system and implantable device applications, which shows the framework developed can perform the verification tasks efficiently.	data acquisition;data retrieval;embedded system;formal verification;real-time clock;system testing;test case	Wei-Tek Tsai;Feng Zhu;Lian Yu;Raymond A. Paul;Chun Fan	2003			test script;raw data;embedded system;test case;computer science;scripting language;data retrieval;systematic process;object-oriented programming;data acquisition	SE	-50.67325492110179	36.09462469424605	113938
62d655560ec1312e26a882489f6009350e8d9af6	an empirical study on reliability modeling for diverse software systems	empirical study;fault tolerant;fault tolerance software reliability modeling fault correlation software testing;reliability modeling;software systems;software systems fault tolerance application software reliability engineering software testing aerospace electronics computer science data engineering design engineering programming;software architecture software reliability;software architecture;real world application;software development;software reliability	Reliability and fault correlation are two main concerns for design diversity, yet empirical data are limited in investigating these two. In previous work, we conducted a software project with real-world application for investigation on software testing and fault tolerance for design diversity. Mutants were generated by injecting one single real fault recorded in the software development phase to the final versions. In this paper, we perform more analysis and experiments on these mutants to evaluate and investigate the reliability features in diverse software systems. We apply our project data on two different reliability models and estimate the reliability bounds for evaluation purpose. We also parameterize fault correlations to predict the reliability of various combinations of versions, and compare three different fault-tolerant software architectures.	certified digital radio broadcast specialist;dependability;experiment;fault tolerance;fault-tolerant software;reliability engineering;software development;software project management;software reliability testing;software system;software testing;test case	Xia Cai;Michael R. Lyu	2004	15th International Symposium on Software Reliability Engineering	10.1109/ISSRE.2004.6	reliability engineering;software architecture;personal software process;long-term support;fault tolerance;verification and validation;software sizing;software verification;computer science;engineering;package development process;social software engineering;software reliability testing;component-based software engineering;software development;software design description;software engineering;software construction;software testing;empirical research;resource-oriented architecture;software deployment;software quality;software fault tolerance;software system;computer engineering	SE	-62.132027967069675	32.55003127671397	114434
1acb7eee0ca8ae0938344eb98d24d4a2b22b9acf	prioritizing warning categories by analyzing software history	software history analysis;software tools program debugging public domain software;control systems;history;prioritizing warning category;automatic bug finding tools;open source projects prioritizing warning category software history analysis automatic bug finding tools high false positive rates;open source projects;high false positive rates;public domain software;false positive rate;performance analysis;artificial intelligence;history software tools computer bugs java open source software software debugging control systems performance analysis computer science artificial intelligence;software tools;software debugging;program debugging;computer science;computer bugs;open source software;java;open source	"""Automatic bug finding tools tend to have high false positive rates: most warnings do not indicate real bugs. Usually bug finding tools prioritize each warning category. For example, the priority of """"overflow"""" is 1 and the priority of """"jumbled incremental"""" is 3, but the tools' prioritization is not very effective. In this paper, we prioritize warning categories by analyzing the software change history. The underlying intuition is that if warnings from a category are resolved quickly by developers, the warnings in the category are important. Experiments with three bug finding tools (FindBugs, JLint, and PMD) and two open source projects (Columba and jEdit) indicate that different warning categories have very different lifetimes. Based on that observation, we propose a preliminary algorithm for warning category prioritizing."""	algorithm;findbugs;open-source software;pmd;software bug;jedit	Sunghun Kim;Michael D. Ernst	2007	Fourth International Workshop on Mining Software Repositories (MSR'07:ICSE Workshops 2007)	10.1109/MSR.2007.26	software bug;false positive rate;computer science;control system;data mining;database;programming language;java;public domain software;world wide web	SE	-58.42227815044293	39.801159847673134	114481
2b9891c4b68bca1ed30022892eb9ada9d9da0306	a framework for measuring and evaluating program source code quality	quality metric;system performance;source code;c programming language;quantitative evaluation	The effect of the quality of program source code on the cost of development and maintenance as well as on final system performance has resulted in a demand for technology that can measure and evaluate the quality with high precision. Many metrics have been proposed for measuring quality, but none have been able to provide a comprehensive evaluation, nor have they been used widely. We propose a practical framework which achieves effective measurement and evaluation of source code quality, solves many of the problems of earlier frameworks, and applies to programs in the C programming language. The framework consists of a comprehensive quality metrics suiteC a technique for normalization of measured values, an aggregation tool which allows evaluation in arbitrary module units from the component level up to whole systemsC a visualization tool for the evaluation of resultsC a tool for deriving rating levels, and a set of derived standard rating levels. By applying this framework to a collection of embedded programs experimentally, we verified that the framework can be used effectively to give quantitative evaluations of reliability, maintainability, reusability and portability of source code.	database normalization;embedded software;embedded system;experiment;problem domain;software portability;software quality;the c programming language	Hironori Washizaki;Rieko Namiki;Tomoyuki Fukuoka;Yoko Harada;Hiroyuki Watanabe	2007		10.1007/978-3-540-73460-4_26	reliability engineering;computer science;systems engineering;database;computer performance;source code	SE	-58.35010185148458	32.743836806444996	114609
3c367e724a87d745edd06b44521d87e441eb8405	ply: visual regression pruning for web design source inspection	css;web development;reverse engineering;end user programming	Despite the ease of inspecting HTML and CSS, web developers struggle to identify the code most responsible for particular stylistic effects, due to complex DOM structures and CSS property cascades. In this paper, we introduce Ply, a DOM inspection tool which augments the Chrome Developer Tools to help developers explore complex professional web designs. To compute source code relevance, we introduce a technique called visual regression pruning, which uses pixel-level screenshot comparison to help developers locate CSS responsible for a webpage's appearance. A user selects an element, and Ply computes the visual impact of each matched CSS property. If a property does not affect the webpage, Ply hides the property from the inspector. In multiple iterations of needfinding studies, developers located relevant code more quickly using Ply. In a case study with the Airbnb homepage, Ply displays an average of 49% fewer CSS properties per element, compared to the Chrome Developer Tools as a control.	cascading style sheets;document object model;html;iteration;ply (file format);pixel;ply (game theory);relevance;screenshot;web design;web developer;web page	Sarah Lim	2017		10.1145/3027063.3048427	web development;computer science;database;cascading style sheets;world wide web;reverse engineering	HCI	-60.02439019148373	41.23264319554912	114739
4ffaf6f6ace0ed560f03e9f94f63162ef8c28478	compiler error messages: what can help novices?	programming language;introductory programming;level of detail;novice programmers;compiler error messages;novice programmer	Novices find it difficult to understand and use compiler error messages. It is useful to refine this observation and study the effect of different message styles on how well and quickly students identify errors in programs. For example, does an increased level of detail simplify the understanding of errors and their correction? We analyzed messages produced by a number of compilers for five programming languages, and grouped them into three style categories from their level of detail and presentation format, and correlated the level of experience and error type with performance and speed of response. The study involved two groups of students taking an introductory programming course at two different institutions; they used messages in these three styles to debug erroneous code. The results indicate that more detailed messages do not necessarily simplify the understanding of errors but that it matters more where information is placed and how it is structured.	compiler;error message;level of detail;programming language	Marie-Hélène Nienaltowski;Michela Pedroni;Bertrand Meyer	2008		10.1145/1352135.1352192	computer science;theoretical computer science;operating system;level of detail;programming language	HCI	-55.07331490345774	39.99119569517054	114910
2e31e9a1b96dba62a0158310d16bbb1998a3a4bf	characterizing api elements in software documentation with vector representation	software;software libraries;software documentation;semantics;natural languages;software engineering;principal component analysis;word2vec;context;documentation	In software engineering (SE), documentation for developers (e.g., developers' guide, API documentation), users' documentation, informal documentation [7, 13] (e.g., developers' forums, mailing lists, development communities' discussions, etc.), and issue reports are of much interest for software engineers. This so-called software documentation is a crucial resource for them in understanding various aspects of a software development process.	software development process;software documentation;software engineer;software engineering	Thanh Van Nguyen;Anh H. T. Nguyen;Tien N. Nguyen	2016	2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)	10.1145/2889160.2892658	look and feel;documentation;computer science;software development;operating system;software engineering;data mining;database;semantics;internal documentation;programming language;software documentation;world wide web;principal component analysis	SE	-56.71900485676462	33.138866126620265	114947
8a6872589981caa9eeb21c8ec8f18564da843102	who touched my mission: towards probabilistic mission impact assessment	bayesian network;mission impact assessment;system object dependency graph	Cyber attacks inevitably generate impacts towards relevant missions. However, concrete methods to accurately evaluate such impacts are rare. In this paper, we propose a probabilistic approach based on Bayesian networks for quantitative mission impact assessment. A System Object Dependency Graph (SODG) is first built to capture the intrusion propagation process at the low operating system level. On top of the SODG, a mission-task-asset (MTA) map can be established to associate the system objects with corresponding tasks and missions. Based on the MTA map, a Bayesian network can be constructed to leverage the collected intrusion evidence and infer the probabilities of tasks and missions being tainted. This approach is promising for effective quantitative mission impact assessment.	bayesian network;operating system;software propagation	Xiaoyan Sun;Anoop Singhal;Peng Liu	2015		10.1145/2809826.2809834	simulation;geography;data mining;remote sensing	AI	-60.56586729520845	46.154534803617565	115354
b4dbcd6c5de157102c72e55f6811d3dd6c3011a7	supporting program comprehension in large preprocessor-based software product lines	within subjects design;software engineering reverse engineering;no colours;spl engineering;treatment colours;program comprehension support;background colours;featurecommander tool;industrial sized spl;no colours program comprehension support preprocessor based software product lines spl engineering variable software implementation background colours code annotation industrial sized spl featurecommander tool within subjects design treatment colours;preprocessor based software product lines;code annotation;variable software implementation	Background: Software product line engineering provides an effective mechanism to implement variable software. However, using preprocessors to realise variability, which is typical in industry, is heavily criticised, because it often leads to obfuscated code. Using background colours to highlight code annotated with preprocessor statements to support comprehensibility has shown effective, however, scalability to large software product lines (SPLs) is questionable. Aim: Our goal is to implement and evaluate scalable usage of background colours for industrial-sized SPLs. Method: We designed and implemented scalable concepts in a tool called FeatureCommander. To evaluate its effectiveness, we conducted a controlled experiment with a large real-world SPL with over 99,000 lines of code and 340 features. We used a within-subjects design with treatments colours and no colours. We compared correctness and response time of tasks for both treatments. Results: For certain kinds of tasks, background colours improve program comprehension. Furthermore, subjects generally favour background colours compared to no background colours. Additionally, subjects who worked with background colours had to use the search functions less frequently. Conclusion: We show that background colours can improve program comprehension in large SPLs. Based on these encouraging results, we continue our work on improving program comprehension in large SPLs.	color;correctness (computer science);heart rate variability;list comprehension;obfuscation (software);preprocessor;program comprehension;programmer;prototype;response time (technology);scalability;software product line;source lines of code;web colors	Janet Siegmund;Michael Schulze;Maria Papendieck;Christian Kästner;Raimund Dachselt;Veit Köppen;Mathias Frisch;Gunter Saake	2012	IET Software	10.1049/iet-sen.2011.0172	real-time computing;computer science;engineering;theoretical computer science;programming language;algorithm	SE	-57.77594247188286	37.98375967917407	115475
3c0fe1dde9a9a2bee554ddc9da9c566957aa8960	finding refactorings via change metrics	refactoring;object oriented metrics;software systems;object oriented software;metrics;object oriented framework;object oriented frameworks;software evolution;design rationale;reverse engineering	Reverse engineering is the process of uncovering the design and the design rationale from a functioning software system. Reverse engineering is an integral part of any successful software system, because changing requirements lead to implementations that drift from their original design. In contrast to traditional reverse engineering techniques ---which analyse a single snapshot of a system--- we focus the reverse engineering effort by determining where the implementation has changed. Since changes of object-oriented software are often phrased in terms of refactorings, we propose a set of heuristics for detecting refactorings by applying lightweight, object-oriented metrics to successive versions of a software system. We validate our approach with three separate case studies of mature object-oriented software systems for which multiple versions are available. The case studies suggest that the heuristics support the reverse engineering process by focusing attention on the relevant parts of a software system.	code refactoring;design rationale;floor and ceiling functions;heuristic (computer science);requirement;reverse engineering;sensor;snapshot (computer storage);software system	Serge Demeyer;Stéphane Ducasse;Oscar Nierstrasz	2000		10.1145/353171.353183	connascence;software visualization;verification and validation;software engineering process group;software sizing;software mining;search-based software engineering;computer science;backporting;software evolution;software design;social software engineering;software framework;component-based software engineering;software development;software design description;object-oriented design;software construction;software walkthrough;metrics;design rationale;software requirements;code refactoring;reverse engineering;software system	PL	-57.127226238650756	34.724874135053234	115481
a6027d459c034a7b00adb291568d7672802ff137	"""applying """"functional scenario-based"""" test case generation method in unit testing and integration testing"""	functional scenario;testing;experiment;specification based	Specification-based testing enables us to detect errors in the implementation of functions defined in given specifications. Its effectiveness in achieving high path coverage and efficiency in generating test cases are always major concerns of testers. The automatic test case generation approach based on formal specifications proposed by Liu and Nakajima is aimed at ensuring high effectiveness and efficiency, but this approach has not been used under different testing environments. In this paper, we first present the statical analysis of the characteristics of the test case generation approach, and then show the experiments of using this approach in two different real testing environments. The two practical testing cases include a unit testing and an integration testing. We perform the testing not only for assessing Liu's approach in practice, but also trying to get some experience of using this approach in practice. The static analysis and the results of experiments indicate that this test case generation approach may not be effective in some circumstances, especially in integration testing. We discuss the results, analyze the specific causes for the ineffectiveness, namely the low path coverage, and propose some suggestions for improvement.	integration testing;test case;unit testing	Cencen Li;Mo Li;Shaoying Liu;Shin Nakajima	2012		10.1007/978-3-642-39277-1_1	non-regression testing;test strategy;keyword-driven testing;reliability engineering;black-box testing;model-based testing;simulation;orthogonal array testing;software performance testing;white-box testing;manual testing;integration testing;systems engineering;engineering;functional testing;dynamic testing;risk-based testing;software testing;system testing;test management approach	SE	-60.312855072285465	34.29665271751932	115650
76272df8acfb47fea3193e7c1bad4f0e1361df6f	monitoring software reliability in the shuttle mission simulator	smtf software reliability measurement;software reliability measurement technique;shuttle mission simulator;shooman software reliability model;software load number;software reliability;reliability problem area;software reliability model;musa model;monitoring software reliability;software reliability monitoring;real-time software	The Shuttle Mission Training Facility (SMTF) is used to train astronauts and ground controllers for Space Shuttle missions. The complex contains three simulators driven by real-time software on large mainframe host computers and supported by as many as twelve super-mini computers. The Shuttle Mission Simulators are complex systems with a large number of hardware components and many diverse pieces of software running simultaneously and interacting with each other. The real-time executable code consists of approximately 365,000 lines of FORTRAN source in 956 modules and 275,000 lines of ASSEMBLER code in 686 modules. Software reliability monitoring has been used in the SMTF for three reasons: To notify management of any changes in the reliability of the system due to new requirements implemented in the code. To understand which functions of the simulation are the reliability problem areas. To identify trends in the reliability of the software over time. Two established data bases are being used to help meet these objectives. The first is called the Problem Incident Report database. It contains information such as type of problem (hardware, software, or procedural), level of incident (critical, degraded, no lost time), time and date of occurrence and resolution, amount of lost time, software load number, and scheduled activity. The second database contains information about facility scheduling. It contains the software load number, start and stop date and time, the simulator being used, and a user identifier. Combining these two sources of information yields the data necessary to meet the three objectives. The Littlewood and Verrall, Musa, and Shooman software reliability models were examined for use in the SMTF. Results from the Littlewood and Verrall model provided a good lower bound for reliability prediction. The Musa model, with its calendar-time component, provided schedule data important to management. Because of its computational simplicity the Shooman model was used to provide “quick look” estimates and starting points for the Musa model. Since each model has attributes that make it appealing under certain circumstances, an eclectic approach was chosen; all three models were used in the SMTF monitoring effort. Software reliability monitoring has produced the desired results. Management was able to identify reliability problem areas and understand trends in the SMTF. Further, based on the application of the software reliability measurement techniques proposed by Littlewood and Verrall, Musa, and Shooman NASA has begun an upgrade to the SMTF. The upgrade consists of the consolidation of the processing done in the super-mini computers into a single multi-processing computer and acquisition of larger more efficient host computer. Along with these hardware upgrades some of the software will be redesigned to be more fault-tolerant. Also, much of the ASSEMBLER code will be rewritten in FORTRAN to make it more maintainable. An important note is that software reliability models were not available during the early phases of SMTF development and have been incorporated only recently. However, due to the benefits in the SMTF software reliability measurement has been incorporated into the proposed day-to-day operations of the new Space Station Training Facility. A plan for measuring software reliability throughout the life-cycle has been proposed using several different types of software reliability models, and requirements for this measurement have been written into the system's preliminary requirements document.	simulation;software quality;software reliability testing	G. E. Stark	1987			real-time computing;simulation;systems engineering;engineering;software reliability testing;software construction	EDA	-50.77871474783371	37.57037351465643	115660
0c3acb14e2c7190319c4421b2e019976220f2013	beyond plain video recording of gui tests: linking test case instructions with visual response documentation	graphical user interface;video;automated test;component;code tracing;gui testing;documentation;testing;graphic user interface;graphical user interfaces;information system;information systems;memory management;debugging	Information systems with sophisticated graphical user interfaces are still difficult to test and debug. As a detailed and reproducible report of test case execution is essential, we advocate the documentation of test case execution on several levels. We present an approach to video-based documentation of automated GUI testing that is linked to the test execution procedure. Viewing currently executed test case instructions alongside actual onscreen responses of the application under test facilitates understanding of the failure. This approach is tailored to the challenges of automated GUI testing and debugging with respect to technical and usability aspects. Screen recording is optimized for speed and memory consumption while all relevant details are captured. Additional browsing capabilities for easier debugging are introduced. Our concepts are evaluated by a working implementation, a series of performance measurements during a technical experiment, and industrial experience from 370 real-world test cases carried out in a large software company.	debugging;documentation;experiment;graphical user interface testing;information system;measurement in quantum mechanics;requirement;software development process;system under test;test case;usability;video	Raphael Pham;Helge Holzmann;Kurt Schneider;Christian Brüggemann	2012	2012 7th International Workshop on Automation of Software Test (AST)		computer science;operating system;test suite;graphical user interface;database;programming language;test script;test case;test management approach;test harness	SE	-54.58730101695385	36.32537576913102	115729
89396806f74872aa713804881a9320bd53abf13c	runtimedroid: restarting-free runtime change handling for android apps		Portable devices, like smartphones and tablets, are often subject to runtime configuration changes, such as screen orientation changes, screen resizing, keyboard attachments, and language switching. When handled improperly, such simple changes can cause serious runtime issues, from data loss to app crashes.  This work presents, to our best knowledge, the first formative study on runtime change handling with 3,567 Android apps. The study not only reveals the current landscape of runtime change handling, but also points out a common cause of various runtime change issues -- activity restarting. On one hand, the restarting facilitates the resource reloading for the new configuration. On the other hand, it may slow down the app, and more critically, it requires developers to manually preserve a set of data in order to recover the user interaction state after restarting.  Based on the findings of this study, this work further introduces a re starting-free runtime change handling solution -- RuntimeDroid. RuntimeDroid can completely avoid the activity restarting, at the same time, ensure proper resource updating with user input data preserved. These are achieved with two key components: an online resource loading module, called HotR and a novel UI components migration technique. The former enables proper resources loading while the activity is still live. The latter ensures that prior user changes are carefully preserved during runtime changes.  For practical use, this work proposes two implementations of RuntimeDroid: an IDE plugin and an auto-patching tool. The former allows developers to easily adopt restarting-free runtime change handling during the app developing; The latter can patch released app packages without source code. Finally, evaluation with a set of 72 apps shows that RuntimeDroid successfully fixed all the 197 reported runtime change issues, meanwhile reducing the runtime change handling delays by 9.5X on average.	android;attachments;code refactoring;desktop computer;mobile app;patch (computing);personal digital assistant;plug-in (computing);smartphone;tablet computer;user interface	Umar Farooq;Zhijia Zhao	2018		10.1145/3210240.3210327	embedded system;real-time computing;implementation;computer science;android (operating system);data loss;source code;plug-in	Mobile	-53.95916842411339	38.68376182699138	115825
232377ae8c5f2a34ba910fc1f4b57f9c60f7f95f	automatic defect categorization based on fault triggering conditions	text analysis data mining fuzzy set theory program debugging software fault tolerance;fuzzy set;text mining;mandelbug;training;f measure automatic defect categorization fault triggering conditions software system complexity bohrbug condition mandelbug condition error propagation text mining solution natural language description bug reports fuzzy set based feature selection algorithm uses algorithm fuzzy affinity scores linux dataset mysql dataset apache httpd dataset axis dataset;software systems;software fault tolerance;text analysis;data mining;fuzzy set theory;machine learning fault triggers bohrbug mandelbug feature selection fuzzy set categorization;machine learning;feature extraction;feature selection;feature extraction training software systems educational institutions text mining buildings;program debugging;buildings;bohrbug;fault triggers;categorization	Due to the complexity of software systems, defects are inevitable. Understanding the types of defects could help developers to adopt measures in current and future software releases. In practice, developers often categorize defects into various types. One common categorization is based on fault triggers of defects. Fault trigger is a set of conditions which activate a defect (i.e., Fault) and propagate the defect into a failure. In general, there are two types of defect based fault triggering conditions, Bohrbug and Mandelbug. Bohrbug refers to a bug which can be easily isolated, and its activation and error propagation is simple. Mandelbug refers to a bug whose activation and/or error propagation is complex (e.g., A time lag between the fault activation and the failure occurrence). With these category labels, developers can better perform post-mortem analysis to identify common characteristic of the defects, and design specific fault-tolerance mechanisms. However, in most software systems, these category labels are often unavailable. To address this problem, in this paper, we propose a text mining solution which categorize defects into fault trigger categories by analyzing the natural-language description of bug reports. A previous study shows that Mandelbug is more complex and needs more time to be fixed. Thus, to better identify Mandelbugs, we propose a novel Fuzzy Set based Feature Selection algorithm named USES, which selects the features (i.e., Terms) which have high ability to distinguish Mandelbugs from Bohrbugs. USES first caches a set of terms based on their fuzzy affinity scores to Bohrbug or Mandelbug. Next, it iterates many times, and in each iteration, it selects a subset of terms, and builds a classifier on these terms. USES selects the classifier and the terms which could achieve the best performance on a training data. We evaluate our solution on 4 datasets including Linux, Mysql, Apache HTTPD, and AXIS containing a total of 809 bug reports. We show that USES with naive Bayes multinomial achieves the best performance, it achieves Mandelbug F-measure scores of 0.298 - 0.615. We also compare USES with other baseline approaches. The results show that USES on average improves Mandelbug F-measure scores of the best performing baseline by 12.3%.	baseline (configuration management);categorization;f1 score;fault tolerance;feature selection;fuzzy set;heisenbug;iteration;kullback–leibler divergence;linux;multinomial logistic regression;mysql;naive bayes classifier;processor affinity;propagation of uncertainty;selection algorithm;software bug;software propagation;software system;statistical classification;text mining	Xin Xia;David Lo;Xinyu Wang;Bo Zhou	2014	2014 19th International Conference on Engineering of Complex Computer Systems	10.1109/ICECCS.2014.14	text mining;computer science;artificial intelligence;operating system;machine learning;pattern recognition;data mining;fuzzy set	SE	-61.68224671865417	39.37758461775939	115978
13cc49b3190a84eae59f2b0e43b72fce3d4ef861	node2defect: using network embedding to improve software defect prediction		Network measures have been proved to be useful in predicting software defects. Leveraging the dependency relationships between software modules, network measures can capture various structural features of software systems. However, existing studies have relied on user-defined network measures (e.g., degree statistics or centrality metrics), which are inflexible and require high computation cost, to describe the structural features. In this paper, we propose a new method called node2defect which uses a newly proposed network embedding technique, node2vec, to automatically learn to encode dependency network structure into low-dimensional vector spaces to improve software defect prediction. Specifically, we firstly construct a program's Class Dependency Network. Then node2vec is used to automatically learn structural features of the network. After that, we combine the learned features with traditional software engineering features, for accurate defect prediction. We evaluate our method on 15 open source programs. The experimental results show that in average, node2defect improves the state-of-the-art approach by 9.15% in terms of F-measure.	centrality;computation;encode;open-source software;software bug;software engineering;software system	Yu Qu;Ting Liu;Jianlei Chi;Yangxu Jin;Di Cui;Ancheng He;Qinghua Zheng	2018		10.1145/3238147.3240469	software bug;theoretical computer science;software system;dependency network;software;computation;centrality;computer science;software metric;embedding	SE	-57.55997255207004	34.776233166821775	116144
7e4d3ca41adc598e4a8b71df2d5c040ccb59be87	early detection of configuration errors to reduce failure damage		Early detection is the key to minimizing failure damage induced by configuration errors, especially those errors in configurations that control failure handling and fault tolerance. Since such configurations are not needed for initialization, many systems do not check their settings early (e.g., at startup time). Consequently, the errors become latent until their manifestations cause severe damage, such as breaking the failure handling. Such latent errors are likely to escape from sysadmins’ observation and testing, and be deployed to production at scale. Our study shows that many of today’s mature, widelyused software systems are subject to latent configuration errors (referred to as LC errors) in their critically important configurations—those related to the system’s reliability, availability, and serviceability. One root cause is that many (14.0%–93.2%) of these configurations do not have any special code for checking the correctness of their settings at the system’s initialization time. To help software systems detect LC errors early, we present a tool named PCHECK that analyzes the source code and automatically generates configuration checking code (called checkers). The checkers emulate the late execution that uses configuration values, and detect LC errors if the error manifestations are captured during the emulated execution. Our results show that PCHECK can help systems detect 75+% of real-world LC errors at the initialization phase, including 37 new LC errors that have not been exposed before. Compared with existing detection tools, it can detect 31% more LC errors.	correctness (computer science);emulator;fault tolerance;software system	Tianyin Xu;Xinxin Jin;Peng Huang;Yuanyuan Zhou;Shan Lu;Long Jin;Shankar Pasupathy	2016			real-time computing;computer science;operating system;computer security	OS	-61.9301984788221	37.94338361751496	116203
207454bc15735cbab093f7a96b19142e8c5861d9	advanced clone-analysis to support object-oriented system refactoring	reverse engineering clone analysis object oriented system refactoring source code copy functionality reuse software maintenance system redesign programming language jdk large scale system;refactoring;maintenance;software maintenance;clone analysis;object oriented programming;object oriented systems;reverse engineering object oriented programming software reusability software maintenance;redesign;software reusability;source code;cloning programming profession software systems electronic mail computer languages large scale systems software libraries information analysis;reverse engineering	Clone detection and re-factoring have grown in importance over the past 10 years. In this talk, we will briefly review the WCRE 2000 work and discuss the advances in the field. The WCRE 2000 paper presented a computer assisted clone re-factoring approach. The process was based on metric-based clone analysis that produced clone clusters. Clones in the same cluster were then compared using tokenbased dynamic programming (DP) matching. Token-based clone differences, which included insertions, deletions, and substitutions, were then projected on to the ASTs corresponding to clones. Re-factoring opportunities were evaluated using: (1) classification of differences involving su perficial differences, signature changes, and type changes , (2) number of differences, and (3) size of candidate clones. Selected clones were automatically re-factored using ”str ategy” and ”template” design patterns. Experimental evaluation was performed on JDK1.1.5 from Sun Microsystems. Significant work followed over the next years addressing problems that include matching algorithms, scalabilit y, and integration of clone detection in software engineering activities such as maintenance, evolution, and re-factori ng. Several interesting surveys can be found in the literature t ogether with a list of problems, many of which remain open today. In particular, recent work on software clones included new approaches to clone detection based on prefix and suffix trees, approaches to detection involving source code analysis based on latent semantic analysis, and clone identification techniques using analysis of program dependence graphs. In other works, a canonical representation of clones was developed and used for matching and comparison; interesting discussions about harmfulness of clo nes have also been reported; and empirical studies and evaluations of clone detection approaches can be found in several research papers. Evolution aspects have been taken into consideration in terms of evolution of clones and their life time over several versions of a system and in terms of software evolution by computing various similarity measures between versions. Clone research has also touched upon several interesting applications: intellectual property issues such as license infringement and plagiarism of source code have been addressed using software similarity concepts; in cremental approaches to clone detection have been investigated; clones and similarity between structured software a rtifacts such as trees and graphs has been introduced; detection of bugs caused by inconsistent modifications between clones in a systems and between fragments in several software releases has been investigated; domain specific clones have been studied; and approaches for clone visualization have been proposed. Finally, new specialized workshops and conferences on clones and on mining software repositories have been organized. There are many open problems that remain and possible areas for future work in CLAN (CLone Analysis) toolset including the definition of clones; addressing type III (sim ilar) and simple type IV (semantic) clones; performance and scalability aspects; taxonomies of clones; clone classific ation and statistics including frequent patterns of similar ity in large systems; inconsistent modifications of clones in one version of a system and inconsistent source code changes over several versions of a system leading to a taxonomy of identifiable bugs; clone matching by parallelizing and implementing it on a Graphical Processing Unit (GPU); intellectual property and plagiarism detection using spectr al clone analysis; increase recall while maintaining precisi on; clone maximality issues under thresholds; and more.	algorithm;code refactoring;design pattern;duplicate code;dynamic programming;graphics processing unit;integer factorization;latent semantic analysis;parallel computing;scalability;software bug;software engineering;software evolution;software repository;static program analysis;video game clone	Magdalena Balazinska;Ettore Merlo;Michel Dagenais;Bruno Laguë;Kostas Kontogiannis	2000		10.1109/WCRE.2000.891457	computer science;systems engineering;software framework;component-based software engineering;software development;software engineering;software construction;computer programming;programming language;object-oriented programming;software maintenance;code refactoring;reverse engineering;source code	SE	-60.519635170981154	39.59019593106249	116427
a992bc58527482f3cb8a58af79720368412880c7	interactive software and hardware faults diagnosis based on negative selection algorithm	detectors;program diagnostics;negative selection;matrix oriented negative selection;availability;complex faults;temperature control;software systems;matrix oriented negative selection interactive software faults diagnosis interactive hardware faults diagnosis negative selection algorithm complex faults interactive effect;matrix algebra;small samples;interactive software faults diagnosis;software performance;interactive hardware faults diagnosis;fault detection;negative selection algorithm;software algorithms;program diagnostics fault diagnosis matrix algebra;interaction effect;algorithm design and analysis;interactive effect;fault diagnosis;hardware fault diagnosis software algorithms fault detection software systems availability algorithm design and analysis software performance detectors temperature control;hardware	Both hardware and software of computer systems are subject to faults. However, traditional methods, ignoring the relationship between software fault and hardware fault, are ineffective to diagnose complex faults between software and hardware. On the basis of defining the interactive effect to describe the process of the interactive software and hardware fault, this paper present a new matrix-oriented negative selection algorithm to detect faults. Furthermore, the row vector distance and matrix distance are constructed to measure elements between the self set and detector set. The experiment on a temperature control system indicates that the proposed algorithm has good fault detection ability, and the method is applicable to diagnose interactive software and hardware faults with small samples.	control system;fault detection and isolation;selection algorithm	Zhaoxiang Yi;Xiaodong Mu;Xiang Lin;Xiongmei Zhang	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525255	algorithm design;availability;detector;interaction;real-time computing;software performance testing;computer science;theoretical computer science;temperature control;hardware architecture;control theory;distributed computing;negative selection;fault detection and isolation;software fault tolerance;software system	Robotics	-58.26830183652842	43.073920203779196	116487
a3829c1820dfdd4ab6e2b0bd100c73cc1ca05218	impact on reliability in the control-flow of programs under compiler optimizations	software fault tolerance error correction optimising compilers program control structures program diagnostics program testing;optimising compilers;program diagnostics;program control structures;software fault tolerance;mibench benchmark suite reliability program control flow compiler optimization fault coverage automatic correction of control flow errors technique llvm framework automated program analysis fault tolerance fault injection experiment;program testing;error correction;benchmark testing hardware reliability optimization program processors systems engineering and theory	This paper evaluates the impact on reliability in the control-flow of programs that compiler optimizations incur in terms of fault coverage for the Automatic Correction of Control-flow Errors technique. This technique was implemented in the LLVM framework, enabling the automated analysis of programs. In order to evaluate the efficiency of the technique of fault tolerance we performed a series of fault injection experiments using the MiBench benchmark suite as case study, measuring how individual and combined optimizations impact reliability.	benchmark (computing);control flow;experiment;fault coverage;fault injection;fault tolerance;llvm;optimizing compiler	Rafael B. Parizi;Ronaldo Rodrigues Ferreira;Álvaro F. Moreira;Luigi Carro	2012	2012 Brazilian Symposium on Computing System Engineering	10.1109/SBESC.2012.17	computer architecture;parallel computing;real-time computing;fault coverage;computer science;software fault tolerance	SE	-60.9747257009896	36.45505703091355	116664
6c9de9cce2ffb715bf801abe3e24ab604f94518b	an empirical evaluation of two user interfaces of an interactive program verifier	verification;standards;inspection;proof understanding;usability;empirical evaluation;user interfaces;java	Theorem provers have highly complex interfaces, but there are not many systematic studies of their usability and effectiveness. Specifically, for interactive theorem provers the ability to quickly comprehend intermediate proof situations is of pivotal importance. In this paper we present the (as far as we know) first empirical study that systematically compares the effectiveness of different user interfaces of an interactive theorem prover. We juxtapose two different user interfaces of the interactive verifier KeY: the traditional one which focuses on proof objects and a more recent one that provides a view akin to an interactive debugger. We carefully designed a controlled experiment where users were given various proof understanding tasks that had to be solved with alternating interfaces. We provide statistical evidence that the conjectured higher effectivity of the debugger-like interface is not just a hunch.	automated theorem proving;debugger;experience;formal verification;hunch;interactive computing;key;proof assistant;sed;usability;user interface;user interface design	Martin Hentschel;Reiner Hähnle;Richard Bubel	2016	2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1145/2970276.2970303	verification;usability;inspection;human–computer interaction;computer science;theoretical computer science;software engineering;programming language;user interface;java	SE	-55.01491743792272	35.83029262965507	116833
2cd2c05584bd7942bd527e2c6b034129789bfc93	error detection in concurrent java programs	race condition;java programming;software verification;concurrency;software verification and validation;model checking;error detection;cost of quality	Concurrency in multithreaded programs introduces additional complexity in software verification and testing, and thereby significantly increases the cost of Quality Assurance (QA). We present a case study in which a specialized model checker was used to discover concurrency errors in a large preexisting code base. The results revealed race conditions that lead to data corruption errors whose detection would have been prohibitively expensive with conventional testing and QA methods. We describe our methodology and highlight parts of the methodology that could be automated.	error detection and correction;java	Graham Hughes;Sreeranga P. Rajan;Tom Sidle;Keith D. Swenson	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2006.01.004	model checking;quality costs;verification and validation;real-time computing;error detection and correction;concurrency;software verification;computer science;software construction;database;race condition;programming language	Logic	-61.19701615010617	36.57414070237636	116973
388f8fcb6456ca33ae3e861c06e8b978ba5914cd	leveraging syntax-related code for automated program repair		We present our automated program repair technique ssFix which leverages existing code (from a code database) that is syntax-related to the context of a bug to produce patches for its repair. Given a faulty program and a fault-exposing test suite, ssFix does fault localization to identify suspicious statements that are likely to be faulty. For each such statement, ssFix identifies a code chunk (or target chunk) including the statement and its local context. ssFix works on the target chunk to produce patches. To do so, it first performs syntactic code search to find candidate code chunks that are syntax-related, i.e., structurally similar and conceptually related, to the target chunk from a code database (or codebase) consisting of the local faulty program and an external code repository. ssFix assumes the correct fix to be contained in the candidate chunks, and it leverages each candidate chunk to produce patches for the target chunk. To do so, ssFix translates the candidate chunk by unifying the names used in the candidate chunk with those in the target chunk; matches the chunk components (expressions and statements) between the translated candidate chunk and the target chunk; and produces patches for the target chunk based on the syntactic differences that exist between the matched components and in the unmatched components. ssFix finally validates the patched programs generated against the test suite and reports the first one that passes the test suite. We evaluated ssFix on 357 bugs in the Defects4J bug dataset. Our results show that ssFix successfully repaired 20 bugs with valid patches generated and that it outperformed five other repair techniques for Java.	experiment;foreach loop;java;repository (version control);software bug;test suite	Qi Xin;Steven P. Reiss	2017	2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)		computer science;theoretical computer science;software bug;syntax;codebase;semantics;database;expression (mathematics);test suite;code (cryptography);java	SE	-59.96593671288488	39.55591551093637	116979
b56c492ccc401a53e5d2486f5ca8e0f5de5b8489	codegraffiti: using hand-drawn sketches connected to code bases in navigation tasks	software;visual languages program visualisation source code software;navigation visualization software prototypes joining processes software engineering interviews;spatial documentation codegraffiti hand drawn sketches code bases navigation tasks ide text manipulation informal visual artifacts source bases examination code fragments linked sketches;prototypes;software engineering;navigation;visualization;joining processes;interviews	Current IDEs excel at text manipulation, but offer little support for sketching and capturing informal visual artifacts that developers create during their work on the code base. Such artifacts promise to help the examination of existing source bases and the orientation therein when linked up to corresponding code fragments. In this paper, we present a design and prototype how to use linked sketches to assist the the developer in orientating in the code base. Our evaluation with 32 users shows that testers adopt the navigation through linked sketches and refer to the spatial documentation significantly more.	documentation;integrated development environment;prototype;visual artifact	Leonhard Lichtschlag;Lukas Spychalski;Jan O. Borchers	2014	2014 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)	10.1109/VLHCC.2014.6883024	simulation;computer science;world wide web;computer graphics (images)	SE	-54.64214124043192	35.268769239162665	117103
d4275ca8f179c09a9d01eb9671198413b91e6a5c	study of defects in a program code in python		In the paper, defects in a program code in Python are considered. It is shown that these defects are different from those in a code in C/C++; hence, there is a need in study of defects in large-scale projects with an open source code. A classification of the defects found, which is based on whether type inference is required for finding an error, is presented. It is shown that there exists a small portion of “simple” defects; however, the determination of the majority of the defects requires type inference. The question of what constructs of the Python language are to be supported in type inference for finding real defects is discussed.	c++;code refactoring;gramps;heuristic (computer science);open-source software;prototype;python;sensor;software bug;static program analysis;type inference	I. E. Bronshteyn	2013	Programming and Computer Software	10.1134/S0361768813060017	computer science;theoretical computer science;programming language;algorithm	PL	-57.74907924758248	37.124062599275014	117211
eb9e744068b32c4dd4e50b3ce670704ef61a7fbd	f3: fast forest fire graph generation		Forest Fire (FF) generation model creates graph with heavy-tailed distribution for in- and out-degrees, the Densification Power Law, and shrinking average diameter. These properties of FF find itself important to data scientists because they correspond to real networks evolving over time. As sizes of the networks to be generated are usually large, their memory- and time-efficiencies play important role. However to the best of our knowledge there are no FF generation theoretical and empirical benchmarks presented. Therefore we describe series of experiments on generating graphs via FF model. We expect this paper to provide general comprehension about FF generation complexity and to help choose the optimal library for generating networks, sampling.	data science;experiment;fire os;sampling (signal processing)	Kuzma Leshakov	2017		10.1145/3079368.3079375	software quality;theoretical computer science;sampling (statistics);power law;comprehension;code review;computer science;graph	ML	-62.071500813708106	34.22120890693924	117447
d19a38a45343dce55fd1d2fe3fc304b8f6752412	ajats: aspectj transformation system.	tool support;aspect oriented software development;code generation;object oriented software;aspect oriented programming;aspect oriented	Refactoring is not only useful for source code of implementations, but as well for test specifications. The open source TRex tool automates the application of refactorings and the detection of refactoring opportunities for test suites that are specified using the standardised Testing and Test Control Notation (TTCN-3). Depending on the refactoring, the behaviour preserving transformations may include syntax tree transformations and direct modification of the source code; for suggesting refactorings, metrics are calculated and code smell patterns are detected. Introduction. The Testing and Test Control Notation (TTCN-3) [1] is a mature standard from the telecommunication and data communication domain that is widely used in industry and standardisation to specify and execute test suites. Just like any other software artifact, tests suffer from quality problems [2]. To remove such quality problems from TTCN-3 test suites, we use refactoring [3]. For suggesting refactorings, we use a combination of metrics and code smell detection. In the following, we first present our approach for the quality assessment and improvement of TTCN-3 test suites. Subsequently, the TTCN-3 Refactoring and Metrics Tool TRex and its implementation are described. Finally, future work is discussed in the outlook. Refactoring, Metrics, and Code Smell Detection for TTCN-3 Test Suites. Refactoring of test suites has so far only been studied in the context of JUnit [2]. Thus, we have developed a refactoring catalogue for TTCN-3 [4, 5] which includes 23 refactorings using language specific concepts of TTCN-3. Furthermore, we found 28 refactorings from Fowler’s Java refactoring catalogue [3] to be applicable to TTCN-3. For being able to automatically identify locations in source code where a refactoring is worthwhile, we investigated corresponding TTCN-3 metrics and TTCN-3 code smells. For example, a Number of References metric is used to identify definitions that are never referenced and can thus be removed or that are referenced only once and can thus be inlined using a corresponding refactoring. Even though we experienced that metrics are able to detect various issues, they are not sophisticated enough to detect more advanced problems. Therefore, we investigated pattern-analysis of source code and as a result, we have developed a catalogue of TTCN-3 code smells [6]. So far 38 TTCN-3 code smells have been identified. TRex Implementation. To automate refactoring for TTCN-3 test specifications, we have implemented the open source TTCN-3 Refactoring and Metrics tool TRex [4]. The initial version has been developed in collaboration with Motorola Labs, UK [7]. TRex implements state-of-the-art editing capabilities, assessment and improvement techniques for TTCN-3 test suites based on the calculation of metrics, automated smell detection, and refactoring. TRex is based on the Eclipse Platform [8] and thus makes use of the infrastructure offered, e.g. the Language Toolkit (LTK) or the Eclipse Test & Performance Tools Platform (TPTP) static analysis framework. The analysis infrastructure including lexer, parser, symbol table, pretty printer, etc. for TTCN-3 have been implemented using ANother Tool for Language Recognition (ANTLR) [9]. The automated refactorings we currently provide concentrate mostly on the improvement of test data descriptions (TTCN-3 templates). The refactoring implementations can be applied in two different ways: either the test developer invokes the refactoring from the code location 1st Workshop on Refactoring Tools (WRT'07)	antlr;artifact (software development);aspectj;code refactoring;code smell;eclipse;junit;java;legacy code;legacy system;lexical analysis;microsoft outlook for mac;open-source software;parse tree;prettyprint;printer (computing);screenshot;sensor;symbol table;trex;ttcn-3;test & performance tools platform;test data;test suite	Roberta Arcoverde;Sérgio Soares;Patrícia Lustosa;Paulo Borba	2007			aspect-oriented programming;computer science;programming language	SE	-53.60756734377615	33.806985713193356	117720
6c240a13c1ece372ae47091d0de0f4248e0df00f	higher order mutation testing to drive development of new test cases: an empirical comparison of three strategies		Mutation testing, which includes first order mutation (FOM) testing and higher order mutation (HOM) testing, appeared as a powerful and effective technique to evaluate the quality of test suites. The live mutants, which cannot be killed by the given test suite, make up a significant part of generated mutants and may drive the development of new test cases. Generating live higher order mutants (HOMs) able to drive development of new test cases is considered in this paper. We apply multi-objective optimization algorithms based on our proposed objectives and fitness functions to generate higher order mutants using three strategies: HOMT1 (HOMs generated from all first order mutants), HOMT2 (HOMs generated from killed first order mutants) and HOMT3 (HOMs generated from not-easy-to-kill first order mutants). We then use mutation score indicator to evaluate, which of the three approaches is better suited to drive development of new test cases and, as a result, to improve the software quality.	algorithm;clipping (computer graphics);display resolution;fitness function;higher-order function;mathematical optimization;multi-objective optimization;mutation testing;software bug;software quality;test case;test suite	Quang Vu Nguyen;Lech Madeyski	2016		10.1007/978-3-662-49381-6_23	machine learning;software quality;mutant;artificial intelligence;mutation testing;test suite;computer science;test case	SE	-59.190137411382985	35.16280326326558	117759
2537de2d9e63458730c07bc2416af0607db8df27	observability and controllability issues in conformance testing of web service compositions	web service;web service composition;conformance testing;observability and controllability;verdict testing report;black box testing	We propose a model-based black-box testing approach to test conformance of Web Service Compositions (WSC). When a WSC under test makes use of implementations of Web Services, two situations may occur: either communications between the WSC and the Web Services are observable or hidden internal actions. We show by means of an example how to generate test cases whose verdicts are provided with explanations taking into account the status of the Web Services.	black-box testing;conformance testing;observable;test case;web service;world sudoku championship;world wide web	Jose Pablo Escobedo;Christophe Gaston;Pascale Le Gall;Ana R. Cavalli	2009		10.1007/978-3-642-05031-2_15	reliability engineering;engineering;ws-policy;world wide web;computer security	SE	-48.57390542498592	38.27213639333451	117823
657133ca55289dc79dd6be868e4ea2545b594870	a parallel and resilient frontend for high performance validation suites		In any well-structured software project, a necessary step consists in validating results relatively to functional expectations. However, in the high-performance computing (HPC) context, this process can become cumbersome due to specific constraints such as scalability and/or specific job launchers. In this paper we present an original validation front-end taking advantage of HPC resources for HPC workloads. By adding an abstraction level between users and the batch manager, our tool JCHRONOSS, drastically reduces test-suite running time, while taking advantage of distributed resources available to HPC developers. We will first introduce validation work-flow challenges before presenting the architecture of our tool and its contribution to HPC validation suites. Eventually, we present results from real test-cases, demonstrating effective speed-up up to 25x compared to sequential validation time – paving the way to more thorough validation of HPC applications.		Julien Adam;Marc Pérache	2016		10.1007/978-3-319-61982-8_22	fault tolerance;scalability;software quality;computer architecture;software;computer science;architecture;real-time computing;scheduling (computing);test suite;abstraction layer	HPC	-51.77749380724879	39.36648018416577	117842
ad78b233ad38589960805ac978b2fb083d61b52e	desocore: detecting source code re-use across programming languages	programming language;source code	Source code re-use has become an important problem in academia. The amount of code available makes necessary to develop systems supporting education that could address the problem of detection of source code re-use. We present the DeSoCoRe tool based on techniques of Natural Language Processing (NLP) applied to detect source code re-use. DeSoCoRe compares two source codes at the level of methods or functions even when written in different programming languages. The system provides an understandable output to the human reviewer in order to help a teacher to decide whether a source code is re-used.	code;natural language processing;programming language;sensor	Enrique Flores;Alberto Barrón-Cedeño;Paolo Rosso;Lidia Moreno	2012			code word;kpi-driven code analysis;dead code;intentional programming;compile time;code bloat;code review;write-only language;computer science;theoretical computer science;redundant code;computer programming;programming language;source lines of code;algorithm;code generation;static program analysis;unreachable code;source code	SE	-52.54431520537046	36.50965188849421	118075
d1d30e0c6169711fcba8d35c17f2c310824b0133	identifying cycle causes with enriched dependency structural matrix	subsystems;edsm cycle causes enriched dependency structural matrix software dependency packages subsystems reengineering coloring information moose reengineering environment argouml morphic ui framework open source smalltalks squeak pharo;packages;argouml;color;dependency structure matrix;moose;ui framework;contextual information;data mining;morphic;dependency software visualization reengineering dependency structural matrix package;dependency;pharo;visualization;dependency structural matrix;packaging open source software visualization application software software packages reverse engineering europe stress computer architecture linux;open source smalltalks;software dependency;image color analysis;unified modeling language;squeak;package;smalltalk;moose reengineering environment;enriched dependency structural matrix;unified modeling language smalltalk software packages systems re engineering;coloring information;reengineering;context;cycle causes;software packages;edsm;open source software;open source;software visualization;systems re engineering	Dependency Structure Matrix (DSM) has been successfully applied to identify software dependencies among packages and subsystems. A number of algorithms were proposed to compute the matrix so that it highlights patterns and problematic dependencies between subsystems. However, existing DSM implementations often miss important information to fully support reengineering effort. For example, they do not clearly qualify and quantify problematic relationships, information which is crucial to support remediation tasks.In this paper we present enriched DSM (eDSM) where cells are enriched with contextual information about (i) the type of dependencies (inheritance, class reference...), (ii) the proportion of referencing entities, (iii) the proportion of referenced entities. We distinguish independent cycles and stress potentially simple fixes for cycles using coloring information. This work is language independent and has been implemented on top of the Moose reengineering environment. It has been applied to non-trivial case studies among which ArgoUML, and Morphic the UI framework available in two open-source Smalltalks, Squeak and Pharo. Solution to problems identified by eDSM have been performed and retrofitted in Pharo main distribution.	algorithm;argouml;code refactoring;color;complex system;cycle (graph theory);design structure matrix;entity;graph coloring;moose;morphic word;open-source software;pharo;small multiple;smalltalk;squeak;the matrix;user interface	Jannik Laval;Simon Denier;Stéphane Ducasse;Alexandre Bergel	2009	2009 16th Working Conference on Reverse Engineering	10.1109/WCRE.2009.11	dependency;unified modeling language;software visualization;visualization;business process reengineering;computer science;engineering;operating system;software engineering;database;design structure matrix;programming language;package	DB	-54.222248159435374	33.05874730420925	118138
ef87997f141b383fc3cf51cb0c93ff70d31442d7	benchmarking in the cloud: what it should, can, and cannot be		With the increasing adoption of Cloud Computing, we observe an increasing need for Cloud Benchmarks, in order to assess the performance of Cloud infrastructures and software stacks, to assist with provisioning decisions for Cloud users, and to compare Cloud offerings. We understand our paper as one of the first systematic approaches to the topic of Cloud Benchmarks. Our driving principle is that Cloud Benchmarks must consider end-to-end performance and pricing, taking into account that services are delivered over the Internet. This requirement yields new challenges for benchmarking and requires us to revisit existing benchmarking practices in order to adopt them to the Cloud.	benchmark (computing);black box;cloud computing;control theory;end-to-end principle;experiment;ibm tivoli storage productivity center;internet;provisioning;requirement;scenario planning;system under test;value network	Enno Folkerts;Alexander Alexandrov;Kai Sachs;Alexandru Iosup;Volker Markl;Cafer Tosun	2012		10.1007/978-3-642-36727-4_12	the internet;management science;service-level agreement;benchmarking;software;cloud computing;provisioning;business	Metrics	-50.22997923717064	42.86799670965675	118280
5688393e1362e4262307f126c65080422cfe6934	automated compatibility testing method for software logic by using symbolic execution	software test process symbolic execution compatibility test software test architecture;software testing automated compatibility testing method symbolic execution software logical behavior;software test architecture;symbolic execution;software test process;compatibility test;computer bugs testing software systems conferences computer architecture history;user interfaces program testing	Compatibility verification for modified software is very important. Currently this is done by executing existing test cases to the modified software. However it is often difficult to perform enough verification because it all depends on the existing test cases. In this paper, a novel compatibility testing method for software logical behavior is proposed. The method does not require existing test cases. Instead, it generates test cases which cover all possible paths in both the existing software and the new software automatically by using Symbolic Execution, and executes them to achieve exhaustive compatibility verification.	algorithm;compatibility testing;symbolic execution;test case	Keiji Uetsuki;Kazuhiko Tsuda;Tohru Matsuodani	2015	2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)	10.1109/ICSTW.2015.7107427	keyword-driven testing;computer architecture;verification and validation;regression testing;test data generation;manual testing;system integration testing;software verification;computer science;package development process;software reliability testing;software development;software engineering;software construction;software testing;programming language;software deployment;software regression;test case;concolic testing;test management approach;compatibility testing;software system;test harness	SE	-58.47948143429419	35.4559112613484	119422
07ecbcbb01b9daa05484944b14c4e5926b9fa5ee	constraint handling for the fault coverage code generation problem: an inductive evolutionary approach	real time;best practice;code generation;monitoring system;constraint handling;fault coverage	Real world problems quite often are constrained and their successful solution requires the application of an appropriate constraint handling technique. The lack of a uniform methodology for handling nonfeasible points largely predetermines the current best practice — the investigation of some problem-specific operators which search (within) the feasibility boundary in an efficient way. In this paper we apply this approach to a real world problem provided by Rolls Royce and Associates Ltd., and show how to design feasibility preserving operators that map the feasibility region onto itself. Some of our results provoke new ideas of how to modify real-time test and monitoring systems so as to increase their reliability.		George Bilchev;Ian C. Parmee	1996		10.1007/3-540-61723-X_1051	real-time computing;fault coverage;computer science;algorithm;code generation;best practice	AI	-59.997670666293914	34.365455473727415	119778
d393c7428575e5d4cc58801a65e1da51fd6f267b	dependency viewer - a tool for visualizing package design quality metrics	dependencyviewer;software metrics;java package;software quality java program visualisation software metrics software packages;design quality;design quality metrics;visualization packaging stability computer science java concrete navigation image converters routing;package graph dependencyviewer package design quality metric visualization package dependencies design quality metrics java package;package dependencies;package graph;program visualisation;software quality;software packages;package design quality metric visualization;java	DependencyViewer helps to control package dependencies to avoid degeneration of package designs. To this end it computes design quality metrics including stability and abstractness for each Java package and draws the package graph in a way, such that violations of various design principles are immediately visible. In addition DependencyViewer provides several features to inspect the packages at various levels of details including at the level of source code	java package	Michael Wilhelm;Stephan Diehl	2005	3rd IEEE International Workshop on Visualizing Software for Understanding and Analysis	10.1109/VISSOF.2005.1684321	computer architecture;package diagram;computer science;package development process;operating system;software engineering;database;programming language;package;java;user exit;software quality;software metric	EDA	-55.081731304407135	35.58174917069804	119882
89fccc4d0fd2703ace1a098d0aa3cca68a6446b1	reliable software systems using reusable software components	component replication;in house software libraries;reliable software systems;relay circuits;software libraries;reusable software modules;hardware system reliability;power system relaying;less reliable components;software systems;failure free operation;computer industry;rejuvenation;probability of failure;checkpointing;automata;commercial off the shelf;execution environment reliable software systems reusable software components reliable automata relay circuits less reliable components component replication hardware system reliability failure free operation commercial off the shelf in house software libraries checkpointing rejuvenation reusable software modules;execution environment;reliable automata;software reusability;software component;software systems software reusability hardware power system reliability buildings automata power system relaying circuits computer industry software libraries;reusable software components;circuits;physical environment;power system reliability;software reliability;buildings;software libraries software reliability software reusability;hardware	In their classic papers, Von Neumann[7], Moore and Shannon[G] discussed building reliable automata and relay circuits (hardware) using less-reliable components. They show that carefully designed replication of components in a hardware system can increase the probability of failure-free operation of that system. There is a powerful trend in the industry now to build software systems also using as many software componenfs as possible. These components might lbe commercial off-the-shelf (COTS) or in-house software libraries and modules; we call all such components reusable software components. We argue that the reliability of such a software system can be improved not only by replicating the software components, as Avizienis[l] and others[2] continue to advocate, but also by active monitoring, checkpointing and rejuvenation, and providing facilities for cold, warm and hot failover/restart of those components. These capabilities themselves can be built as reusable software modules that can be linked to the actual system components[3, 41. We will present the architecture of such a software system and a preliminary analysis to show the feasibility of this approach for building reliable software systems using reusable software components. Research into analyzing the reliability of such systems is gaining attention recently[5]. These facilit,ies provide diversity in the execution environment of a software component leading to a higher level of reliability of the software system, much as replication provides diversity in the physical environment of a hardware component giving rise to improved reliability of the hardware system that Von Neumann and others have pioneered.	application checkpointing;automata theory;component-based software engineering;failover;in-house software;library (computing);relay;software quality;software system;von neumann architecture	Chandra M. R. Kintala	1997		10.1109/RELDIS.1997.632795	software distribution;reliability engineering;embedded system;electronic circuit;verification and validation;real-time computing;software sizing;computer science;package development process;backporting;software reliability testing;software framework;component-based software engineering;software development;software design description;operating system;rejuvenation;software construction;hardware architecture;database;distributed computing;automaton;resource-oriented architecture;artificial intelligence systems integration;software deployment;computer security;software quality;software metric;software system;avionics software	SE	-62.41411370095137	32.570624109460674	120241
98bead02ec7eacf5b9461a7ae35b7eb8baec9baf	a multipurpose code coverage tool for java	software testing;regression testing;code coverage;test case prioritization;program testing java;java software testing system testing performance evaluation visualization visual databases binary codes costs fault detection information analysis;program testing;open source database multipurpose code coverage tool java branch coverage software testing regression testing test case prioritization test suite augmentation test suite minimization test coverage reporting;test coverage;java;open source	Most test coverage analyzers help in evaluating the effectiveness of testing by providing data on statement and branch coverage achieved during testing. If made available, the coverage information can be very useful for many other related activities, like, regression testing, test case prioritization, test-suite augmentation, test-suite minimization, etc. In this paper, we present a Java-based tool JavaCodeCoverage for test coverage reporting. It supports testing and related activities by recording the test coverage for various code-elements and updating the coverage information when the code being tested is modified. The tool maintains the test coverage information for a set of test cases on individual as well as test suite basis and provides effective visualization for the same. Open source database support of the tool makes it very useful for software testing research	code coverage;fault coverage;java;regression testing;software testing;test case;test suite	Raghu Lingampally;Atul Gupta;Pankaj Jalote	2007	2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)	10.1109/HICSS.2007.24	non-regression testing;modified condition/decision coverage;keyword-driven testing;regression testing;fuzz testing;software performance testing;white-box testing;manual testing;classification tree method;integration testing;computer science;operating system;software engineering;test suite;smoke testing;software testing;code coverage;programming language;test script;test management approach;test harness	SE	-59.4286642949041	35.165443141026046	120299
8a551e8ccb1292e85bcaef86d12dbbaa9f6e1862	analyzing partition testing strategies	informatica;software;metodo analisis;logiciel;subdomain modifications;subdomain modifications input domain partition testing fault detection capabilities random testing;conception;random testing;test;fault detection capabilities;ensayo;essai;methode analyse;detection defaut;program testing;analysis method;fault detection;diseno;partition testing;logicial;design;informatique;computer science;deteccion imperfeccion;software testing fault detection performance evaluation helium guidelines computer science genetic mutations;defect detection;input domain	In this paper, partition testing strategies are assessed analytically. An investigation of what conditions affect the efficacy of partition testing is performed, and comparisons of the fault detection capabilities of partition testing and random testing are made. The effects of subdomain modifications on partition testing’s ability to detect faults are also studied.	dataflow;failure cause;fault detection and isolation;mutation testing;random testing;sensor	Elaine J. Weyuker;Bingchiang Jeng	1991	IEEE Trans. Software Eng.	10.1109/32.83906	random testing;design;electronic engineering;orthogonal array testing;computer science;theoretical computer science;software engineering;algorithm	SE	-60.614226631707375	33.68004619423416	120987
e84fbd06e020f904ec3290e74842e09c2518e8e6	a class abstraction technique to support the analysis of java programs during testing	software metrics;program diagnostics;java programming;software complexity;abstract data types;object oriented programming;java taxonomy software testing concurrent computing application software computer science software tools design engineering systems engineering and theory software systems;abstract data types java object oriented programming program diagnostics program testing software metrics concurrency control exception handling;program testing;polymorphism;concurrency control;exception handling;java class cataloging class abstraction technique java program analysis program testing software complexity concurrency polymorphism exception handling;java;class group	In this paper, we describe a class abstraction technique (CAT) for Java programs that support the testing process by capturing aspects of software complexity based on the combination of class characteristics. These class characteristics relate to properties of the class features such as concurrency, polymorphism, exception handling, and accessibility as well as relationships between classes. Our taxonomy (CAT) for Java allows us to generate a finite number of possible class groups (taxa). Each class C in a Java program is cataloged into a group that summarizes the dependencies with other types realized through declarations and definitions in C. We also provide a high-level design for a tool to catalog Java classes based on our taxonomy.	accessibility;concurrency (computer science);exception handling;high- and low-level;java;level design;programming complexity;taxonomy (general)	David Crowther;Djuradj Babich;Peter J. Clarke	2005	Third ACIS Int'l Conference on Software Engineering Research, Management and Applications (SERA'05)	10.1109/SERA.2005.1	exception handling;polymorphism;java api for xml-based rpc;real-time computing;jsr 94;java concurrency;application programming interface;jar;computer science;software development;operating system;java modeling language;concurrency control;interface;strictfp;software construction;database;real time java;reflection;programming language;object-oriented programming;java;abstract data type;imagix 4d;programming complexity;generics in java;software metric;scala;java annotation	SE	-53.327510155702875	32.5265059020927	121303
09131ed4a8bd72fb18182ab4f01a704602b48d38	using mutation analysis for a model-clone detector comparison framework	software engineering;simulink model-clone detection tools;clone nature;clone report representation;model clone type search;model-clone detector comparison framework;mutation analysis;mutation operators;recall calculation	Model-clone detection is a relatively new area and there are a number of different approaches in the literature. As the area continues to mature, it becomes necessary to evaluate and compare these approaches and validate new ones that are introduced. We present a mutation-analysis based model-clone detection framework that attempts to automate and standardize the process of comparing multiple Simulink model-clone detection tools or variations of the same tool. By having such a framework, new research directions in the area of model-clone detection can be facilitated as the framework can be used to validate new techniques as they arise. We begin by presenting challenges unique to model-clone tool comparison including recall calculation, the nature of the clones, and the clone report representation. We propose our framework, which we believe addresses these challenges. This is followed by a presentation of the mutation operators that we plan to inject into our Simulink models that will introduce variations of all the different model clone types that can then be searched for by each respective model-clone detector.	clone tool;duplicate code;extensibility;mutation testing;mutator method;sensor;simulink	Matthew Stephan;Manar H. Alalfi;Andrew Stevenson;James R. Cordy	2013	2013 35th International Conference on Software Engineering (ICSE)		simulation;computer science;bioinformatics;theoretical computer science;software engineering	SE	-56.172135528664974	34.12612913941269	121309
085e3eb20350c4a7c6d79c636897e3601b21a2c4	program learning using static information and dynamic program execution slices	static information;program understanding;program maintenance;program reuse;history large scale systems application software conferences open source software information science costs programming documentation;programming environments;history;time measurement;software maintenance;program learning;open source community;learner;maintenance engineering;dynamic program;learning environment;data mining;execution history;learning systems;program reusability;public domain software;dynamic information;large scale;program execution history;feature extraction;dynamic program execution slices;software reusability;learner static information dynamic program execution slices open source community practically used programs program maintenance program reusability program understanding program learning environment dynamic information program execution history;communities;practically used programs;program learning environment;programming;software reusability learning systems programming environments public domain software software maintenance;open source;program maintenance program understanding program learning execution history program reuse	Nowadays, we can easily obtain a copy of practically any used program in our open source community for learning. However, the reality is that the level of such practically used programs is often complex and of such a large scale so that it is not as easy to understand them as one might expect. We believe that we do need some kind of environment to help the learner read and understand programs. Learning programs is necessary for other reasons such as program maintenance, reuse of programs. In this research, we built a model to use for program understanding. Using it, we examined possible ways of supporting program learning. Based on that, the objective of this paper is to present our efforts at developing a program learning environment, which helps narrow down the scope of a program to facilitate reading and understanding it while finding the differences of such dynamic information, as found in the program execution history, and also provide an environment in which the learner can use static information. We evaluate the efficiency of the proposed program learning environment through experiments.	experiment;open-source software;program comprehension;software maintenance;sticky bit	Jun Sawamoto;Eiji Sugino;Norihisa Segawa;Yuji Wada	2010	2010 IEEE 24th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2010.15	program analysis;maintenance engineering;programming;error-driven learning;feature extraction;computer science;operating system;database;programming language;software maintenance;public domain software;time	SE	-54.963418315734984	33.58376487376595	121742
725e6ae5ba776732dc9b4e3e4bc9eed39eac1049	in medio stat virtus: extract class refactoring through nash equilibria	software maintenance game theory;couplings games nash equilibrium semantics measurement software;refactoring;game theory design quality refactoring;game theory;software maintenance;medio stat virtus nash equilibria extract class refactoring ecr game theory cohesive classes;design quality	Extract Class refactoring (ECR) is used to divide large classes with low cohesion into smaller, more cohesive classes. However, splitting a class might result in increased coupling in the system due to new dependencies between the extracted classes. Thus, ECR requires that a software engineer identifies a trade off between cohesion and coupling. Such a trade off may be difficult to identify manually because of the high complexity of the class to be refactored. In this paper, we present an approach based on game theory to identify refactoring solutions that provide a compromise between the desired increment in cohesion and the undesired increment in coupling. The results of an empirical evaluation indicate that the approach identifies meaningful ECRs from a developer's point-of-view.	algorithm;code refactoring;extract class;game theory;heuristic (computer science);iteration;iterative method;marginal model;nash equilibrium;off topic;software engineer;usability;virtus (chipset)	Gabriele Bavota;Rocco Oliveto;Andrea De Lucia;Andrian Marcus;Yann-Gaël Guéhéneuc;Giuliano Antoniol	2014	2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)	10.1109/CSMR-WCRE.2014.6747173	game theory;simulation;computer science;systems engineering;engineering;software engineering;software maintenance;code refactoring	SE	-57.95846416716641	35.22103372404741	121865
5f5b211f6ac1aa84053da951b77c220832f2be9d	high-level testing and example-directed development of software specifications	specification;langage naturel;specification language;natural language;fiabilite logiciel;test programme;software specification;program test;software reliability;langage specification;cycle vie logiciel	A testing-based approach for constructing and refining very high-level software fun~tionaiity repre~ntations such as intentions, natural language assertions, and formal specifications is presented and applied to a standard line-editing problem as an illustration. The approach involves the use of specification-based (black-box) testcase generation strategies, high-level specification formalisms, redundant or parallel development and crossvalidation, and a logic pr~ramming support environment. Test-case reference sets are used as software functionality representations for the purposes of cross-validating two distinct high-level representations, and identifying ambiguities and omissions in those representations. In fact, we propose the use of successive refinements of such test reference sets as the authoritative specification throughout the software development process. Potential benefits of the approach include improvements in user/ designer communication over all life cycle phases, and an increase in the quality of specifications and designs.	black box;high- and low-level;map projection;natural language;reference implementation;software development process;test case	Robert L. Probert;Hasan Ural	1984	Journal of Systems and Software	10.1016/0164-1212(84)90031-1	reference implementation;software requirements specification;formal methods;specification language;computer science;software design;theoretical computer science;software development;software engineering;analysis effort method;software construction;formal specification;natural language;programming language;programming language specification;specification;algorithm;software quality;language of temporal ordering specification	SE	-50.618071781615036	33.13497332516622	122490
ac861bb20cf4246f6eb974c6f45c3a57e50c1f5c	formalization of a converged internet and telecommunications service environment				Niklas Blum	2010			telecommunications service;the internet;business;basic service;telecommunications control software;computer network;computer security;converged infrastructure	Networks	-48.59275582992064	45.79754466128711	122538
018f7ccea6c2259a82b222689503918525865da4	systematic testing of reactive software with non-deterministic events: a case study on lg electric oven		Most home appliance devices such as electric ovens are reactive systems which repeat receiving a user input/event through an event handler, updating their internal state based on the input, and generating outputs. A challenge to test a reactive program is to check if the program correctly reacts to various non-deterministic sequence of events because an unexpected sequence of events may make the system fail due to the race conditions between the main loop and asynchronous event handlers. Thus, it is important to systematically generate/test various sequences of events by controlling the order of events and relative timing of event occurrences with respect to the main loop execution. In this paper, we report our industrial experience to solve the aforementioned problem by developing a systematic event generation framework based on concolic testing technique. We have applied the framework to a LG electric oven and detected several critical bugs including one that makes the oven ignore user inputs due to the illegal state transition.	concolic testing;event (computing);event generator;event loop;race condition;software bug;state transition table	Yongbae Park;Shin Hong;Moonzoo Kim;Dongju Lee;Junhee Cho	2015	2015 IEEE/ACM 37th IEEE International Conference on Software Engineering		reliability engineering;embedded system;real-time computing;code review;event;engineering;operating system;software engineering;programming language	SE	-56.454126466373474	38.924263640641264	122573
15750e27991331ff55d160bf1277f40a83fe01de	evaluating feedback ratings for measuring reputation of web services	reliability;pearson correlation coefficient;service selection;user feedback;reputation;web service;correlation methods;pearson correlation coefficient web service feedback rating reputation cumulative sum method;malicious feedback detection;reputation measure method;service selection reliability;accuracy;monitoring;feedback rating;web services;malicious rating;web services correlation methods;service selection reliability feedback rating web service reputation service computing malicious rating reputation measure method malicious feedback detection cumulative sum method user feedback preference pearson correlation coefficient;web services quality of service correlation accuracy reliability monitoring;correlation;quality of service;cumulant;correlation coefficient;user feedback preference;cumulative sum method;web service reputation;service computing	In the field of service computing, reputation of a Web service is usually calculated using feedback ratings provided by service users. However, the existing of malicious ratings and different preferences of different service users often lead to a bias towards positive or negative ratings. In this paper, we propose a novel reputation measure method for Web services. The proposed method employs two phases (i.e., malicious rating detection and rating adjustment) to enhance the reputation measure accuracy. We first detect malicious feedback ratings by the Cumulative Sum Method, and then reduce the affect of different user feedback preferences by using Pearson Correlation Coefficient. Extensive experiments are conducted. Experimental results show that our proposed method is effective and can enhance the reliability of service selection.	coefficient;e-commerce;electronic business;experiment;feedback;malware;open web;reputation management;reputation system;services computing;web service	Shangguang Wang;Zibin Zheng;Qibo Sun;Hua Zou;Fangchun Yang	2011	2011 IEEE International Conference on Services Computing	10.1109/SCC.2011.32	marketing;business;world wide web;computer security	Web+IR	-50.31588619363612	44.15104725661809	122802
452189e7b268916acde8d01f32d4d23b193c2004	fault localization using visualization of test information	developmentand maintenance effort;fault localization;software development;cult component;test information;cost andquality;cant impact;debugging task;software testing;program analysis;software quality;visualization;failure analysis;software visualization;software fault tolerance;information analysis;debugging;displays;programming;software maintenance	Attempts to reduce the number of delivered faults in softwareare estimated to consume 50% to 80% of the developmentand maintenance effort [3]. Among the tasks requiredto reduce the number of delivered faults, debugging is oneof the most time-consuming [2, 12], and locating the errorsis the most dif.cult component of this debugging task (e.g.,[13]). Clearly, techniques that can reduce the time requiredto locate faults can have a signi.cant impact on the cost andquality of software development and maintenance.	debugging;software development	James A. Jones	2004	Proceedings. 26th International Conference on Software Engineering		program analysis;reliability engineering;software visualization;long-term support;verification and validation;real-time computing;software sizing;computer science;package development process;backporting;software reliability testing;software development;software engineering;software construction;software testing;programming language;software analytics;software maintenance;software deployment;software quality;software fault tolerance;software metric;software quality analyst;software system;computer engineering	SE	-62.05127579136964	33.84090597239325	122873
7382a96489af88c7a8cbba64eb5948fd1e68736a	viennatalk and assertch: building lightweight formal methods environments on pharo 4	live environment;lightweight formal methods;specification animation;validation	It is possible to make Integrated Development Environments supporting formal methods that can be as flexible as the support for dynamic programming languages. This paper contributes with a demonstration employing different support environments for the Vienna Development Method Specification Language (VDM-SL) and design by contract for visual programming language. This includes ViennaTalk developed on top of Pharo 4 providing Smalltalk-styled LIVE browsers, VDM-SL interpreters, Smalltalk code generators, UI prototyping environments and a prototype Web API server to enable rigorous and flexible modeling during exploratory phases of software development. ViennaTalk uses the Slot mechanism in Pharo to test invariant assertions on instance variables in Smalltalk objects generated from VDM-SL specifications. In addition, we present a plugin named Assertch for Phratch, a scratch-clone visual programming environment on top of Pharo 4, that provides assertion blocks for designing and debugging a series of blocks.  Both ViennaTalk and Assertch combine flexible live modeling or coding while still supporting rigorous checking. ViennaTalk has been evaluated by experienced professional engineers of VDM-SL while Assertch has been evaluated by undergraduate students of computer science. ViennaTalk and Assertch both demonstrate that Pharo and its contemporary features support rigorous modeling in formal specification languages as well as flexible prototyping in Smalltalk.	application programming interface;assertion (software development);computer science;debugging;design by contract;dynamic programming;formal methods;formal specification;integrated development environment;microsoft dynamics sl;pharo;prototype;sl (complexity);server (computing);smalltalk;software development;software prototyping;specification language;user interface;vienna development method;visual programming language;web api	Tomohiro Oda;Keijiro Araki;Peter Gorm Larsen	2016		10.1145/2991041.2991045	computer science;theoretical computer science;database;programming language	HCI	-53.04316563734146	34.751189441431194	122942
ef1288b1cc1e0b594f572d1c64666618866e7175	shrimp views: an interactive environment for exploring java programs	java visualization packaging software tools software prototyping prototypes computer science computer displays software architecture software maintenance;shrimp visualization tool;java bean components;software prototyping;java programming;software maintenance;prototypes;packaging;java program browsing;multiple views;software programs;software architecture;visualization;nested graph display;software understanding tools;application program interfaces;apis;distributed object management;computer displays;interactive environment;shrimp views;customizable environment;software tools;computer science;interactive systems program visualisation reverse engineering java distributed object management application program interfaces software architecture;software understanding tools shrimp views interactive environment java program browsing shrimp visualization tool customizable environment software programs multiple views nested graph display software architecture java bean components apis;interactive systems;program visualisation;reverse engineering;java	This paper describes a demonstration of the SHriMP visualization tool. SHriMP provides a flexible and customizable environment for exploring software programs. It supports the embedding of multiple views, both graphical and textual within a nested graph display of a program’s software architecture. SHriMP has recently been redesigned and reimplemented using Java Bean components. These APIs allow SHriMP to be easily integrated with other software understanding tools. In this demonstration, SHriMP is used for exploring and browsing Java programs.	graphical user interface;java platform, enterprise edition;software architecture	Margaret-Anne D. Storey;Casey Best;Jeff Michaud	2001		10.1109/WPC.2001.921719	software architecture;packaging and labeling;real-time computing;visualization;application programming interface;computer science;engineering;operating system;software engineering;prototype;programming language;software maintenance;java;reverse engineering	SE	-54.26272637880795	34.86981965301031	123391
15bf9a53d71b1039cc879df56c05bd0b5870c443	using conditional mutation to increase the efficiency of mutation analysis	software testing;instrumentation;compiler integrated;mutation analysis;software systems;lines of code	Assessing testing strategies and test sets is a crucial part of software testing. Mutation analysis is, among other approaches, a suitable technique for this purpose. However, compared with other methods it is rather time-consuming and applying mutation analysis to large software systems is still problematic. This paper presents a versatile approach, called conditional mutation, which increases the efficiency of mutation analysis. This new method significantly reduces the time overhead for generating and executing the mutants. Results are reported for eight investigated programs up to 373,000 lines of code and 406,000 generated mutants. Furthermore, conditional mutation has been integrated into the Java 6 Standard Edition compiler. Thus, it is widely applicable and not limited to a certain testing tool or framework.	compiler;java;mutation testing;overhead (computing);software framework;software system;software testing;source lines of code;test automation;test set	René Just;Gregory M. Kapfhammer;Franz Schweiggert	2011		10.1145/1982595.1982606	computer science;bioinformatics;engineering;theoretical computer science;software engineering;mutation testing;software testing;source lines of code;instrumentation;algorithm;software system	SE	-59.80767959312122	35.616215494301315	123564
a0a51c9e6634ca007572c25f10abc4890f73ee17	operational pattern based code generation for management information system: an industrial case study	analytical models;software;operational pattern;exploratory case study;domain engineering;metadata;code generation;skeleton;computer architecture;feature extraction;productivity;software product line	Code generation technology can significantly improve productivity and software quality. However, due to limited financial and human resources in most of small and medium software enterprises, there are many challenges when leveraging code generation approaches to large-scale software development. In this paper, an operational pattern based code generation approach is proposed for rapid development of domain-specific management information system. We demonstrate the approach with details: (I) semi-automatically extracting operational patterns from requirement documents, (II) building feature models to manage the commonalities and variability of each operational pattern, (III) mapping operational patterns into skeleton code with a template-based code generation technique, etc. Then we conduct an industrial case study in asset information management domain at CancoSoft Company for about 2 years, to analyze its feasibility and efficiency. 14 operational patterns are successfully extracted from 355 initial key phrases, and a code generator is implemented and applied to develop new Web applications. Preliminary findings show that the software development based on our approach yields a nearly 30% higher productivity as compared to traditional software development. Through code analysis, we find that around 70% of code can be automatically generated, and the generated code is also effective.	algorithm;altered level of consciousness;code generation (compiler);information management;management information system;requirement;semiconductor industry;software development;software quality;spatial variability;static program analysis;system requirements;web application	Fagui Mao;Xuyang Cai;Beijun Shen;Yong Xia;Bo Jin	2016	2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2016.7515935	kpi-driven code analysis;development testing;team software process;productivity;code review;feature extraction;computer science;package development process;software framework;software development;operating system;software engineering;domain engineering;software construction;data mining;database;programming language;metadata;skeleton;software quality;code generation;static program analysis;source code	SE	-57.160712559950504	33.21037276918289	123579
305f82f4187c1ff0111352ebf7147c663082a00f	using content-derived names for configuration management	content-derived name;configuration management;web based applications;digital signature;java;software component;distributed computing	Configuration management of compiled software artifacts (programs, libraries, icons, etc.) is a growing problem as software reuse becomes more prevalent. For an application composed from reused libraries and modules to function correctly, all of the required files must be available and be the correct version. In this paper, we present a simple scheme to address this problem: content-derived names (CDNs). Computing an object’s name automatically using digital signatures greatly eases the problem of disambiguating multiple versions of an object. By using content-derived names, developers can ensure that only those software components that have been tested together are permitted to run toget her.	antivirus software;code reuse;compiler;component-based software engineering;configuration management;digital signature;library (computing);object code;software distribution	Jeffrey K. Hollingsworth;Ethan L. Miller	1997		10.1145/258366.258399	digital signature;web application;real-time computing;software sizing;software configuration management;computer science;software framework;component-based software engineering;software development;operating system;software engineering;software construction;database;configuration management;programming language;java;software system	PL	-58.486511286500225	39.42745252031955	123822
4ea8708500851e78d87e678700b4a2f8538c0091	an integrated testing and debugging environment for parallel and distributed programs	integration testing;programming environments;testing and debugging;software tools parallel programming integrated software program testing program debugging programming environments;distributed programs;parallel programming;program testing;integrated software;software tools;program debugging;debugging software testing computer bugs sequential analysis system testing error correction interference monitoring software engineering reproducibility of results;interactive program analysis program testing environment debugging environment parallel programs distributed programs program specification program development process steps structural testing ddbg distributed debugger testing tool debugging tool stand alone tools software integration intermediate tool deipa deterministic execution;program development	To achieve a certain degree of confidence that a given program follows its specification, a testing phase must be included in the program development process, and also a complementary debugging phase to help locating the program's bugs. This paper presents an environment which results of the composition and integration of two basic tools: STEPS (Structural TEsting of Parallel Software), which is a testing tool, and DDBG (Distributed DeBuGger), which is a debugging tool. The two tools are presented individually as stand-alone tools, and we describe how they were combined through the use of another intermediate tool: DEIPA (Deterministic re-Execution and Interactive Program Analysis). We claim that the result achieved is a very effective testing and debugging environment.	breakpoint;control flow;debugging;distributed computing;dynamical system;global variable;graphical user interface;java platform debugger architecture;program analysis;relevance;software bug;test automation	João Lourenço;José C. Cunha;Henryk Krawczyk;Piotr Kuzora;Marcin Neyman;Bogdan Wiszniewski	1997	EUROMICRO 97. Proceedings of the 23rd EUROMICRO Conference: New Frontiers of Information Technology (Cat. No.97TB100167)	10.1109/EURMIC.1997.617289	non-regression testing;test strategy;computer architecture;program slicing;regression testing;real-time computing;fuzz testing;software performance testing;white-box testing;system integration testing;computer science;acceptance testing;software reliability testing;software development;software construction;dynamic program analysis;software testing;algorithmic program debugging;programming language;debugging;system testing;program animation	SE	-49.98681281078679	35.440891429143484	124032
8ddefd2a913a00896b8f6360fab9d39fe774eca2	empirically evaluating the usefulness of software visualization techniques in program comprehension activities	treemaps.;program comprehension;source code metrics;software visualization;software maintenance;source code;information visualization	Program comprehension is the basis for software maintenance and reengineering. It is possible and viable the use of information visualization techniques to improve the level of program comprehension through software visualization. This paper presents an empirical evaluation on the usefulness of software visualization techniques to program comprehension activities from the viewpoint of software maintainers. In the experiment, ten students execute a set of program comprehension activities with the assistance of a software visualization tool, called SourceMiner, over a set of two open source software systems. The results show that when using SourceMiner coupled with an IDE, as opposed to the IDE alone, the students found 45% more restructuring and bad smells opportunities in the analyzed code.	list comprehension;program comprehension;software visualization	Glauco de Figueiredo Carneiro;Angelo C. Araujo Orrico;Manoel G. Mendonça	2007			software construction;data mining;imagix 4d;software peer review;software visualization;software engineering;software analytics;program comprehension;information visualization;package development process;computer science	SE	-55.43431755840661	35.253927973679076	124329
130cc361c205fa8f9bcca0c3ddedf4999985dc5b	improved genetic algorithm to reduce mutation testing cost	control flow of program genetic algorithms mutation testing object oriented programming object s state suspicious mutant two way crossover;standards;testing genetic algorithms standards computer bugs sociology statistics java;testing;statistics;genetic algorithms;computer bugs;sociology;java	Mutation testing is a fault-based testing technique that helps generating effective test cases. Mutation testing is computationally expensive, because it requires executing hundreds and even thousands of mutants. In this situation, search-based approaches like genetic algorithm can help to automate test case generation to reduce the cost. In this paper, we present an improved genetic algorithm that can reduce computational cost of mutation testing. First, we present a novel state-based and control-oriented fitness function that efficiently uses object-oriented program features to evaluate a test case. We then empirically evaluate it using our implemented tool, eMuJava, and compare it with the standard fitness function. Results show that although our proposed fitness function provides detailed information about fitness of a test case but standard genetic algorithm is incapable of using that effectively to repair the test cases. Therefore, we propose a new two-way crossover and adaptable mutation methods that intelligently use the fitness information to generate fitter offspring. Finally, we compare the improved genetic algorithm with random testing, standard genetic algorithm, and EvoSuite. Experiment results prove that our proposed approach can find the optimal test cases in less number of attempts (reduces computational cost). Besides that it can detect software bugs from suspiciously equivalent mutants and these mutants eventually get killed (increases mutation score).	algorithmic efficiency;analysis of algorithms;computation;evosuite;fitness function;genetic algorithm;mutation testing;random testing;software bug;test case	Muhammad Bilal Bashir;Aamer Nadeem	2017	IEEE Access	10.1109/ACCESS.2017.2678200	simulation;software bug;genetic algorithm;computer science;theoretical computer science;mutation testing;software testing;java;algorithm;statistics	SE	-59.366961076330284	35.76208035441262	124375
e1ea2e45743488317a2c4aefd6f03179ae52f842	libraryguru: api recommendation for android developers		Developing modern mobile applications often require the uses of many libraries specific for the mobile platform, which can be overwhelmingly too many for application developers to find what are needed for a functionality and where and how to use them properly. This paper presents a tool, named LibraryGuru, to recommend suitable Android APIs for given functionality descriptions. It not only recommends functional APIs that can be invoked for implementing the functionality, but also recommends event callback APIs that are inherent in the Android framework and need to be overridden in the application. LibraryGuru internally builds correlation databases among various functionality descriptions and Android APIs. These correlations are extracted from Android development tutorials and SDK documents with domain-specific code parsing and natural language processing techniques adapted for functional APIs and event callback APIs separately, and are matched against functionality queries to recommend relevant APIs for developers. LibraryGuru is publicly accessible at http://libraryguru.info, and a demo video is available at https://youtu.be/f7MtjliUM-4.	android;application programming interface;callback (computer programming);database;library (computing);mobile app;mobile operating system;natural language processing;parsing;software development kit	Weizhao Yuan;Hoang H. Nguyen;Lingxiao Jiang;Yuting Chen	2018		10.1145/3183440.3195011	callback;world wide web;systems engineering;android (operating system);computer science;parsing;natural language	SE	-53.73493961762373	38.18295856624817	124529
1ab2bfb75f2360cfe66e0de3c0e9b4030e08932f	datalyzer: streaming data applications made easy		Nowadays, streaming data are continuously generated from thousands of sources, including social networks, mobile apps, sensors, e-commerce transactions, and many more. Hence, it becomes very useful to build applications able to process these data, with the purpose of filtering interesting parts, monitor their run-time evolution, persist valuable chunks, trigger events upon certain conditions are met and provide analytics. While several frameworks and systems have emerged to create this kind of applications, these systems tend to be low-level, based on complicated APIs, challenging to install and configure for end-users, and requiring from high performant hardware for their execution. Our goal is to lower the entry level to develop, deploy and run streaming applications.	stream (computing)	Mario González-Jiménez;Juan de Lara	2018		10.1007/978-3-319-91662-0_34	entry level;data mining;model-based design;real-time computing;code generation;social network;computer science;streaming data;analytics	Theory	-51.91268518958884	41.64933004500418	124554
972594dd0eee86e637edb72d3f02b0d86e08e4d4	toward rigorous object-code coverage criteria		Object-branch coverage (OBC) is often used as a measure of the thoroughness of tests suites, augmenting or substituting source-code based structural criteria such as branch coverage and modified condition/decision coverage (MC/DC). In addition, with the increasing use of third-party components for which source-code access may be unavailable, robust object-code coverage criteria are essential to assess how well the components are exercised during testing. While OBC has the advantage of being programming language independent and is amenable to non-intrusive coverage measurement techniques, variations in compilers and the optimizations they perform can substantially change the structure of the generated code and the instructions used to represent branches. To address the need for a robust object coverage criterion, this paper proposes a rigorous definition of OBC such that it captures well the semantics of source code branches for a given instruction set architecture. We report an empirical assessment of these criteria for the Intel x86 instruction set on several examples from embedded control systems software. Preliminary results indicate that object-code coverage can be made robust to compilation variations and is comparable in its bug-finding efficacy to source level MC/DC.	code coverage;compiler;control system;embedded system;internet branding;modified condition/decision coverage;object code;observable;oracle (software testing);programming language;x86	Taejoon Byun;Vaibhav Sharma;Sanjai Rayadurgam;Stephen McCamant;Mats Per Erik Heimdahl	2017	2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE)	10.1109/ISSRE.2017.33	compiler;robustness (computer science);code coverage;computer science;reliability engineering;real-time computing;x86;instruction set;source code;software;object code	SE	-60.21019729078006	35.776536727965414	124707
208a35b667ac0b17333f593f9c68cc8d8603df1e	jcrasher: an automatic robustness tester for java	software testing;random testing;time allocation;test case generation;parameter space;state re initialization;java	JCrasher is an automatic robustness testing tool for Java code. JCrasher examines the type information of a set of Java classes and constructs code fragments that will create instances of different types to test the behavior of public methods under random data. JCrasher attempts to detect bugs by causing the program under test to “crash”, that is, to throw an undeclared runtime exception. Although in general the random testing approach has many limitations, it also has the advantage of being completely automatic: no supervision is required except for off-line inspection of the test cases that have caused a crash. Compared to other similar commercial and research tools, JCrasher offers several novelties: it transitively analyzes methods, determines the size of each tested method’s parameter-space and selects parameter combinations and therefore test cases at random, taking into account the time allocated for testing; it defines heuristics for determining whether a Java exception should be considered a program bug or the JCrasher supplied inputs have violated the code’s preconditions; it includes support for efficiently undoing all the state changes introduced by previous tests; it produces test files for JUnit—a popular Java testing tool; and can be integrated in the Eclipse IDE.	batch processing;eclipse;heuristic (computer science);interactivity;junit;java;online and offline;plug-in (computing);precondition;random testing;randomness;robustness testing;software bug;software testing;test automation;test case;type system;usability	Christoph Csallner;Yannis Smaragdakis	2004	Softw., Pract. Exper.	10.1002/spe.602	random testing;real-time computing;simulation;white-box testing;manual testing;computer science;engineering;operating system;strictfp;software testing;parameter space;programming language;java;test management approach;algorithm;statistics;time allocation	SE	-58.52457992734023	38.39410265717213	124836
67db0f2eb9e9aee15cbb9232f04c696b4fc6444c	model extraction of legacy c code in sccharts		With increasing volumes of developed software and steadily growing complexity of these systems, software engineers struggle to manually maintain the vast amount of legacy code. Therefore, it is of interest to create a system which supports the documentation, maintenance, and reusability of software and its legacy code. The approach presented here automatically derives SCCharts models out of C code. These models can be used as visual documentation. By applying focus and context methods important parts of the model can be highlighted and may grant a better understanding of the overall software. Additionally, the models can also be used as a source to create new state-of-the-art code for various languages and platforms, such as C code or VHDL, using automatic code generators.		Steven Smyth;Stephan Lenga;Reinhard von Hanxleden	2017	ECEASST	10.14279/tuj.eceasst.74.1044	documentation;theoretical computer science;software;computer architecture;computer science;vhdl;legacy code;reusability	SE	-52.44007594666673	34.817097818572414	124980
1248ee7a6026d7ff3cc2d939968d70268fca1c82	a study of applying extended pie technique to software testability analysis	software metrics;software;software testing;complexity theory;information systems;application software;pie analysis software testability analysis software development software reliability software complexity program testability propagation infection and execution analysis;software complexity;probability density function;software development process;testing;computer industry;data mining;propagation infection and execution analysis;testability;computer applications;failure analysis;software reliability program testing software metrics;program testing;estimation;software testability analysis;software development;test methods;software component;system testing;software testing testability;pie analysis;computer science;and execution analysis;infection;software reliability;programming;information analysis;program testability;propagation;software testing programming application software system testing information analysis failure analysis computer applications computer science information systems computer industry	During the software development process, data that has been gained from the testing phase can help developers to predict software reliability more precisely. But the testing stage usually takes more and more effort due to the growing complexity of software. How to build software that can be tested efficiently has become an important topic in addition to enhancing and developing new testing methods. Thus, research on software testability has been developed variously. In the past, a dynamic technique for estimating program testability was proposed and called propagation, infection, and execution (PIE) analysis. Previous research studies show that PIE analysis can complement software testing. However, this technique requires a lot of computational overhead in estimating the testability of software components. In this paper, we propose an Extended PIE (EPIE) technique to accelerate the traditional PIE analysis, based on generating group testability as a substitute for location testability. This technique can be separated into three steps: breaking a program into blocks, dividing blocks into groups, and marking target statements. We developed a tool called ePAT (extended PIE Analysis Tool) to help us identify the locations which will be analyzed. The experimental results show that the number of analyzed locations can be effectively decreased and that the estimated value of testability remains acceptable and useful.	component-based software engineering;item unique identification;overhead (computing);software development process;software propagation;software reliability testing;software testability;software testing	Tsung-Han Tsai;Chin-Yu Huang;Jun-Ru Chang	2009	2009 33rd Annual IEEE International Computer Software and Applications Conference	10.1109/COMPSAC.2009.22	reliability engineering;real-time computing;computer science;software reliability testing;operating system;software engineering;software construction;database;software testing;programming language	SE	-60.15884234800946	36.30822118252388	125397
7440cfb975bfff10b1ebfc99e75add37a2555b5d	quantifying trust based on service level agreement for software as a service	trust;software computational modeling monitoring stability analysis mathematical model conferences numerical models;software service;trust model;software engineering;trust software engineering software service qualuty;quantitative service trustworthiness prediction web service level agreement trust factor similarity factor;web services;qualuty;service level agreement;software as a service	In this paper, we aim to construct quantifying trust model based on service level agreements (SLAs) as well as feedbacks from service consumers. The proposed model can aggregate the feedbacks weighted by both trust factor and similarity factor to give quantitative prediction of the target services' trustworthiness. By comparing with traditional prediction algorithm, we show that our methodology gives better performance in trust estimation.	aggregate data;algorithm;feedback;service-level agreement;software as a service;trust (emotion);web service	Lin Tan;Chi-Hung Chi;Jianming Deng	2008	2008 32nd Annual IEEE International Computer Software and Applications Conference	10.1109/COMPSAC.2008.211	web service;service level objective;computer science;knowledge management;software engineering;software as a service;data mining;database;service;trustworthy computing;world wide web	Embedded	-50.02326994481846	43.97140103771671	125414
1033e4ba559a7e2474b73d639e44ef85c203393c	real: a risk-enabled reputation model for electronic commerce	real;reliability;electronic commerce;online auctions sites;optics;probability;service provider;e commerce;probability density function;risk management;data mining;security of data electronic commerce risk management;online auctions sites real risk enabled reputation model electronic commerce reliability value e commerce;reliability value;electronic commerce ontologies software engineering computer science reliability engineering frequency risk analysis performance analysis probability social network services;clustering algorithms;risk enabled reputation model;security of data;conferences	Recently, a trend to use reputation model for selecting service providers becomes evident in the area of electronic commerce. Most of the effort have been made on providing a reliability value associated with the predicted reputation to interpret how reliable is the reputation. However, the predicted reputation will become ineffective with a low reliability value. Rather than providing a reliability of a reputation, in this paper, we propose a reputation model, called risk-enabled reputation model (REAL), by introducing the notion of risks to better interpret the recent performance of a service provider in e-commerce. The proposed approach has been validated as a better model to reflect a service provider's recent performance than given by the extant on-line auctions sites or the models using the technique of adopting mean values.	e-commerce;experiment;online and offline	Jonathan Lee;Shin-Jie Lee	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811346	e-commerce;service provider;probability density function;computer science;probability;reliability;cluster analysis;world wide web;statistics	Robotics	-50.54215721659467	44.1700693117677	125729
8c1dbc51efac18060fe1fa3389abb506cfdd6135	legacy application migration to cloud	reconstruction application migration cloud computing mashup gui recognition;application migration;graphical user interfaces html cloud computing prototypes runtime browsers mashups;gui recognition;reconstruction;application customization requirement legacy application migration cloud computing enterprises legacy applications source code application migration solution gui recognition technology gui reconstruction technology;graphical user interfaces;business data processing;mashup;graphical user interfaces business data processing cloud computing;cloud computing	Along with the increasing popularity of cloud computing, the requirement to efficiently migrate enterprises' legacy applications to cloud has become extremely intensive. Existing solutions have many limitations in use, such as the source code of legacy application is always required, or the migrated application cannot be customized or mashed up. What's more, it always overbalances the budget to perform these solutions. In this paper, we proposed a brand new Application Migration Solution (AMS) to efficiently migrate legacy applications to web applications by GUI recognition and reconstruction technology. The core technologies are presented and evaluation results are given for technology validation. Based on AMS, enterprises can deploy their legacy applications to cloud easily. This solution also fulfills advanced enterprise requirements, including application customization requirement or mashup with other applications to compose more integrated and powerful applications.	cloud computing;graphical user interface;legacy system;mashup (web application hybrid);requirement;vhdl-ams;web application	Xin Meng;Jingwei Shi;Xiaowei Liu;Huifeng Liu;Lian Wang	2011	2011 IEEE 4th International Conference on Cloud Computing	10.1109/CLOUD.2011.56	cloud computing;computer science;operating system;graphical user interface;database;world wide web;mashup	HPC	-49.88343303033585	42.08601850227022	125780
c3852d7d1552edca22c37f4a50e8531ecc3f0eec	comparing the fault detection effectiveness of n-way and random test suites	software testing;empirical study;software systems;software fault tolerance;greedy algorithms;controlled experiment;n way testing;random testing;program testing;combinatorial testing;combinatorial test suites fault detection effectiveness n way test suites random test suites software testing high quality software systems greedy algorithm combinatorial testing software faults test data combinations;fault detection;greedy algorithm;greedy algorithms program testing software fault tolerance software quality combinatorial mathematics;fault injection;fault detection software testing system testing input variables software systems greedy algorithms application software costs explosions books;combinatorial mathematics;software quality	"""Software testing plays a critical role in the timely delivery of high-quality software systems. Despite the important role that testing plays, little is known about the fault detection effectiveness of many testing techniques. We investigate """"n-way"""" test suites created using a common greedy algorithm for use in combinatorial testing. A controlled study is designed and executed to compare the fault detection effectiveness of n-way and random test suites. Combinatorial testing is conducted on target systems that have been injected with software faults. The results are that there is no significant difference in the fault detection effectiveness of n-way and random test suites for the applications studied. Analysis of the random test suite finds that they are very similar to n-way test suites from the perspective of the number of test data combinations covered. This result concurs with other hypothetical results that indicate little difference between n-way and random test suites. While we do not expect this result to apply in all combinatorial testing situations, we believe the result will lead to the design of better combinatorial test suites."""	combinatorial optimization;fault detection and isolation;greedy algorithm;random testing;software system;software testing;symmetric multiprocessing;test data;test suite	Patrick J. Schroeder;Pankaj Bolaki;Vijayram Gopu	2004	Proceedings. 2004 International Symposium on Empirical Software Engineering, 2004. ISESE '04.	10.1109/ISESE.2004.16	random testing;reliability engineering;real-time computing;fuzz testing;all-pairs testing;computer science;test suite;test case;computer engineering	SE	-61.52894086742731	34.20543940678744	125904
4fa4843efe9818829af88ff79b410249d9201429	trust-based secure information sharing between federal government agencies	protection information;intercambio informacion;administracion electronica;information communication technology;confiance;protocole transmission;reference model;securite informatique;defense nationale;service web;information access;trust model;web service;emerging technology;federal government;information sharing;computer security;vida privada;protocolo transmision;modelo;confidence;private life;proteccion informacion;confianza;echange information;information protection;information exchange;seguridad informatica;administration electronique;electronic government;national defence;vie privee;defensa nacional;modele;models;trust negotiation;enterprise architecture;nueva tecnologia informacion comunicacion;technologie information communication;servicio web;transmission protocol	The September 11 attack and the following investigations show that there is a serious information sharing problem among the relevant federal government agencies, and the problem can cause substantial deficiencies in terrorism attack detection. This paper performs a systematic analysis of the causes of this problem; and it finds that existing secure information sharing technologies and protocols cannot provide enough incentives for government agencies to share information with each other without worrying that their own interests can be jeopardized. Although trust-based information access is well studied in the literature, the existing trust models, which are based on certified attributes, cannot support effective information sharing among government agencies, which requires an interest-based trust model. To solve this information sharing problem, this paper proposes an innovative interest-based trust model and a novel information sharing protocol, where a family of information sharing policies are integrated, and information exchange and trust negotiation are interleaved with and interdependent upon each other. In addition, an implementation of this protocol is presented using the emerging technology of XML Web Services. The implementation is totally compatible with the Federal Enterprise Architecture reference models and can be directly integrated into existing EGovernment systems. 1.1. Information Sharing Problems among Government Agencies	data security;e-government;federal enterprise architecture;information access;information exchange;interdependence;ws-trust;web service	Peng Liu;Amit Chetal	2005	JASIST	10.1002/asi.20117	web service;information and communications technology;reference model;information exchange;computer science;enterprise architecture;confidence;emerging technologies;law;world wide web;computer security;information protection policy	Security	-53.25321263868118	46.35184036117611	126101
3fbaad5ea5c359cf25f1e66f0abeb4d04d6afaad	enabling sophisticated analyses of ×86 binaries with revgen	libraries;program diagnostics;legacy software;software maintenance;driver circuits registers assembly libraries instruction sets transforms;assembly;device drivers;registers;transforms;source language;driver circuits;source code;program analysis;static analysis tools;program compilers;static and dynamic analysis;off the shelf;device driver;device drivers 86 binaries revgen static analysis tools binary software ad hoc intermediate representations machine code program analysis source language compiler framework llvm source code analysis frameworks run time instrumentation tools legacy software;source coding device drivers program compilers program diagnostics software maintenance;instruction sets;source coding;intermediate representation	Current state-of-the-art static analysis tools for binary software operate on ad-hoc intermediate representations (IR) of the machine code. Therefore, even though IRs facilitate program analysis by abstracting away the source language, it is hard to reuse existing implementations of analysis tools in new endeavors. Recently, a new compiler framework — LLVM — has emerged, together with many analysis tools that use its IR. However, these tools rely on a compiler to generate the IR from source code. We propose RevGen, a tool that automatically converts existing binary programs to the standard LLVM IR, making an increasingly large number of static and dynamic analysis frameworks, as well as run-time instrumentation tools, applicable to legacy software. We show the potential of RevGen by converting several programs and device drivers to LLVM and checking the resulting code with off-the-shelf analysis tools.	compiler;device driver;hoc (programming language);intermediate representation;llvm;legacy system;machine code;microsoft windows;prototype;static program analysis;x86	Vitaly Chipounov;George Candea	2011	2011 IEEE/IFIP 41st International Conference on Dependable Systems and Networks Workshops (DSN-W)	10.1109/DSNW.2011.5958815	real-time computing;computer science;operating system;distributed computing;programming language;source code	SE	-55.18184653914714	35.76496339737004	126149
7d6c47c8011d20114e24023669febeab7c77bc28	interactive visualization of object-oriented programs	runtime analysis;software tool;program visualization;interactive execution;interactive visualization;object oriented programming;object and sequence diagrams;sequence diagram;java	We describe a novel approach to runtime visualization of object-oriented programs. Our approach features: visualizations of execution state and history; forward and reverse execution; interactive queries during program execution; and advanced drawing capabilities involving a combination of compile-time and runtime-analysis. Our methodology is realized in a software tool called <i>JIVE</i>, for <i>Java Interactive Visualization Environment</i>.	compile time;compiler;interactive visualization;java;programming tool	Paul V. Gestwicki	2004		10.1145/1028664.1028691	sequence diagram;software visualization;computer architecture;visual analytics;real-time computing;information visualization;interactive visualization;computer science;programming language;object-oriented programming;java	PL	-54.55627225875705	35.99223174984249	126352
326ef3cc8b080ab98a19324fbedebed4738dd5ee	challenges in managing dependable data systems	storage system;cost effectiveness;dependent data;business process	Recent work shows how to automatically design storage systems that meet performance and dependability requirements by appropriately selecting and configuring storage devices, and creating snapshot, remote mirror, and traditional backup copies. Although this work represents a solid foundation, users demand an even higher level of functionality: the ability to cost-effectively manage data according to application-centric (or better, business process-centric) performance, dependability and manageability requirements, as these requirements evolve over the data's lifetime. In this paper, we outline several research challenges in managing dependable data systems, including capturing users' high-level goals; translating them into storage-level requirements; and designing, deploying, and analyzing the resulting data systems.	backup;business process;data system;dependability;high- and low-level;norm (social);requirement;snapshot (computer storage)	Kimberly Keeton;Arif Merchant	2006	SIGMETRICS Performance Evaluation Review	10.1145/1138085.1138089	real-time computing;cost-effectiveness analysis;business requirements;computer science;operating system;business process	OS	-50.1293564628146	41.11611971243839	126354
e8474e634ef4836296ffd115bd786242ad4d28a1	a comparison of abstract data types and objects recovery techniques	abstract data type;software engineering;data flow analysis;source code;false positive	In the context of the authors’ research on architectural features recovery, abstract data t (ADT) and abstract data objects (ADO, also called objects) have been identified as two of the smalles ponents which are useful for building a significant architectural overview of the system. The authors named these the atomic components (AC) of an architecture. This article compares six published techniques which extract ADTs and ADOs from source code w extensive data flow analysis. A prototype tool implementing each technique has been developed and to three medium-size systems written in C (each over 30 Kloc). The results from each approach are pared with the atomic components identified by hand by a group of software engineers. This article extends previous papers by discussing how the software engineers’ AC identification wa idated and by analyzing the false positives, i.e., the atomic components identified by automatic appro which were not identified by software engineers.	abstract data type;advanced audio coding;data-flow analysis;dataflow;prototype;software engineer	Jean-Francois Girard;Rainer Koschke	2000	Sci. Comput. Program.	10.1016/S0167-6423(99)00035-0	type i and type ii errors;computer science;theoretical computer science;data-flow analysis;programming language;abstract data type;algorithm;source code	SE	-56.358690706651075	35.73462783236617	126493
2331403f40fceda9927d82a5320729fdf8a07708	microprocessor software development method using a specification language	specification language;software development	Abstract   At the start of a microprocessor project, the specification of the software is frequently quite vague. If an efficient reliable and testable piece of software is to be produced, then the specification must be defined clearly and concisely before assembly-level coding begins. A method of specification is described which uses some of the advantages of high-level languages, but which is independent of any particular microprocessor. A software driver for the popular Seiko printer mechanism is designed and specified in minute detail to illustrate the method. Implementation details for various microprocessors are discussed and a checklist of common pitfalls is given.	microprocessor;software development;specification language	Brian R. Kirk	1977	Microprocessors	10.1016/0308-5953(77)90153-2	reference implementation;embedded system;software requirements specification;computer architecture;real-time computing;specification language;computer science;software design;software development;analysis effort method;software construction;database;programming language	SE	-49.73208826668516	32.36270141109802	126635
ce4321e2248e1ec4f2048ec236c5e1a96ac1dcbe	standardization and testing of implementations of mathematical functions in floating point numbers	floating point;floating point arithmetic	Requirements definition and test suites development for implementations of mathematical functions in floating point arithmetic in the framework of the IEEE 754 standard are considered. A method based on this standard is proposed for defining requirements for such functions. This method can be used for the standardization of implementations of such functions; this kind of standardization extends IEEE 754. A method for designing test suites for the verification of those requirements is presented. The proposed methods are based on specific properties of the representation of floating point numbers and on some features of the functions under examination.	requirement;test suite	Victor V. Kuliamin	2007	Programming and Computer Software	10.1134/S036176880703005X	arithmetic;minifloat;computer science;floating point;theoretical computer science;algorithm	SE	-49.283216369316804	33.53459746475489	127120
0b99e88367684bb9723b92dbe80627dc16fd3a1b	testing concurrent programs to achieve high synchronization coverage	software testing;code coverage;random testing;concurrent programs;open source	The effectiveness of software testing is often assessed by measuring coverage of some aspect of the software, such as its code. There is much research aimed at increasing code coverage of sequential software. However, there has been little research on increasing coverage for concurrent software. This paper presents a new technique that aims to achieve high coverage of concurrent programs by generating thread schedules to cover uncovered coverage requirements. Our technique first estimates synchronization-pair coverage requirements, and then generates thread schedules that are likely to cover uncovered coverage requirements. This paper also presents a description of a prototype tool that we implemented in Java, and the results of a set of studies we performed using the tool on a several open-source programs. The results show that, for our subject programs, our technique achieves higher coverage faster than random testing techniques; the estimation-based heuristic contributes substantially to the effectiveness of our technique.	code coverage;concurrent computing;fault coverage;heuristic;ibm notes;java;meltwater entrepreneurial school of technology;open-source software;prototype;random testing;requirement;software testing	Shin Hong;Jaemin Ahn;Sangmin Park;Moonzoo Kim;Mary Jean Harrold	2012		10.1145/2338965.2336779	modified condition/decision coverage;random testing;reliability engineering;real-time computing;computer science;software engineering;distributed computing;software testing;code coverage	SE	-61.214867072333206	36.0139958983461	127246
c8d5bb33d58cb76e17fb06482cf183c1ee06f53c	using profiling to analyze algorithms (abstract)	libraries;multimedia;parallel programming;software engineering		algorithm;profiling (computer programming)	Daniel E. Nohl	1994		10.1145/191029.191335	computational science;search-based software engineering;computer science;theoretical computer science;software framework;software development;programming language	Theory	-51.51914011193396	32.499503864833024	127329
193463157bde8135a0eee8c5066064e6cee7e3d0	diagnosing faults in embedded queries in database applications	embedded language;testing;diagnosing faults;testing database applications	Diagnosing faults in embedded queries in database applications is a daunting process. When test cases fail, the traditional way of diagnosing faults is to follow possible execution paths, either mentally or step-by-step in a debugger, to locate the problematic area. The diagnosis problem becomes even harder when you have embedded language with quite different semantics and properties.  Our focus is on a specific problem: diagnosing failed test cases caused by embedded queries in database applications which are syntactically correct but semantically incorrect (i.e., they produce incomplete or incorrect results). Much research literature is available on database applications and databases but the diagnosis problem for embedded queries that cause failure of test cases has not been tackled.  We perform an experiment to see how far existing techniques could be useful in proposing a new technique for this problem. We identify the additional components that need to be developed to take us to a full solution and describe our tentative conclusions so far.	database;debugger;embedded system;experiment;scientific literature;test case	Muhammad Akhter Javid;Suzanne M. Embury	2012		10.1145/2320765.2320831	real-time computing;computer science;data mining;database	DB	-60.694189596775324	37.880071584171944	127503
b27f1360d897b8508138527566050b1b9980f96e	semantic-based interaction detection in aspect-oriented scenarios	data security unified modeling language network servers protection software engineering engineering management;analytical models;semantic annotation;industrial case study;domain specific markers;authentication;aspect oriented modeling;data mining;software engineering;feature interaction;formal method;software lifecycle semantic based interaction detection aspect oriented scenarios lightweight semantic annotations domain specific markers automated analysis;servers;software lifecycle;aspect interactions;lightweight semantic annotations;unified modeling language;semantic interactions;aspect oriented;goal modeling;feature interactions;semantic interactions aspect interactions feature interactions aspect oriented modeling goal modeling;context;domain specificity;semantic based interaction detection;aspect oriented scenarios;automated analysis	Interactions between dependent or conflicting aspects are a well-known problem with aspect-oriented development (and related paradigms). These interactions are potentially dangerous and can lead to unexpected or incorrect results when aspects are composed. To date, most aspect interaction detection methods have been based either on purely syntactic comparisons or have relied on heavyweight formal methods. We present a new approach that is based instead on lightweight semantic annotations of aspects. Each aspect is annotated with domain-specific markers and a separate influence model describes how semantic markers from different domains influence each other. Automated analysis can then be used both to highlight semantic aspect conflicts and to trade-off aspects. We apply this technique to early aspects, namely, aspect scenarios, because it is desirable to detect aspect interactions as early in the software lifecycle as possible. We evaluate the technique using an industrial case study and show that the technique detects interactions that cannot be discovered using syntactic techniques.	aspect-oriented programming;aspect-oriented software development;formal methods;interaction;software development process	Gunter Mussbacher;Jon Whittle;Daniel Amyot	2009	2009 17th IEEE International Requirements Engineering Conference	10.1109/RE.2009.13	unified modeling language;formal methods;aspect-oriented programming;goal modeling;computer science;software engineering;data mining;authentication;database;programming language;world wide web;software development process;server	SE	-48.497741988405764	39.05295831094742	127513
386bd33445249c810aafb7dec2a492f11713336f	path exploration tool	systeme temps reel;software testing;architecture systeme;sistema informatico;computer system;formal verification;programa puesta a punto;software development;verification formelle;arquitectura sistema;real time system;systeme informatique;sistema tiempo real;system architecture;programme debogage;debugging program	While veriication methods are becoming more frequently integrated into software development projects, software testing is still the main method used to search for programming errors. Software testing approaches focus on methods for covering diierent execution paths of a program, e.g., covering all the statements, or covering all the possible tests. Such coverage criteria are usually approximated using some add-hoc heuristics. We present a tool for testing execution paths in sequential and concurrent programs. The tool, path exploration tool (Pet), visualizes concurrent code as ow graphs, and allows the user to interactively select an (inter-leaved) execution path. It then calculates and displays the condition to execute such a path, and allows the user to easily modify the selection in order to cover additional related paths. We describe the design and architecture of this tool and suggest various extensions.	approximation algorithm;heuristic (computer science);hoc (programming language);interactivity;path (graph theory);software development;software testing	Elsa L. Gunter;Doron A. Peled	1999		10.1007/3-540-49059-0_28	embedded system;basis path testing;real-time computing;simulation;real-time operating system;white-box testing;formal verification;computer science;software development;operating system;software engineering;software testing;programming language;systems architecture	SE	-58.114030270675975	36.61246169456526	127591
af19b2fa4d8f024bcf4638c7a48445a7b4e6e4fb	software testing method considering the importance of factor combinations in pair-wise testing		Software testing bears a burden of software development, increasing its time and cost. The bugs appearing due to the combination of two factors are well known in the system test phase. The current system testing methods represented by pair-wise tests or orthogonal arrays tests generate test sets by the forms of factors and values. In this study we extract two problems in the system test phase, and propose a solution to solve the problems. The first type of the problems is a survival bugs by the combination of factors among test sets. The second type of the problems is a duplication of factors by extra test case in the test set. We propose a solution which considers combinations of important factors for these two problems.	software testing	Ruoan Xu;Yoshimitsu Nagai;Syohei Ishizu	2011		10.1007/978-3-642-22098-2_39	non-regression testing;software reliability testing;functional testing;risk-based testing	SE	-60.99497499156204	35.00205454004099	127656
6cbdfd5b5b748f5529f94996728f83333ced09fe	use of context for recommending code: an approach based on frequent pattern mining	frequent pattern mining;naming patterns;code recommendation	During creating a class, several times a developer must be conscious of regularities that should be complied in order to satisfy an intended architectural design. This paper presents an approach for code recommendation, which apply concepts of frequent pattern mining to take advantage of the use of naming conventions and the organization of source code in software development. The proposal has been evaluated in projects of the organizations Apache and Eclipse. The results have shown that significant terms and regularities, in form of frequent relationships, could be mined. The approach is particular in the sense that it enables using naming patterns and package hierarchies as context for recommending code.	data mining;eclipse;mined;software development	Paul Mendoza	2016		10.1145/2998626.2998671	computer science;data mining;database;world wide web	SE	-56.470201334664914	34.9097550772095	127730
f4b643ced1cbeb1fcd87fe6436171ca76f3935d8	a petri net-based metric for active rule validation	database system;cybernetics;knowledge based system;complexity theory;measurement;rule based;program testing knowledge based systems petri nets;metric;conditional colored petri net model petri net based metric active rule validation test cases;program testing;conditional colored petri net;database systems;colored petri net;measurement complexity theory ip networks knowledge based systems database systems conferences cybernetics;metric rule validation conditional colored petri net;ip networks;rule validation;petri nets;petri net;knowledge based systems;conferences	Active rules are the mechanism by which some systems can behave automatically. Rule validation is a mandatory step to guarantee those systems work properly. One of the most used validation techniques is based on test cases. In this paper we introduce a new metric through the Conditional Colored Petri Net model of the rule base, to determine the number of test cases.	event condition action;petri net;rule-based system;test case	Lorena Chavarría-Báez;Xiaoou Li	2011	2011 IEEE 23rd International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2011.156	discrete mathematics;computer science;artificial intelligence;theoretical computer science;knowledge-based systems;data mining;validation rule;petri net	Robotics	-48.90225795183619	35.28387045910883	127894
37c04a742561ac2e2fd0069a9e2f92a048df4c0e	characterizing logging practices in open-source software	public domain software;problematic logging code;current log message;real world software;tool developer;logging practice characterization;own modification;log message;software tools;compiler designer;checker;conventional programming practice;production run;large open-source software;language designer;programming practice;interesting finding;failure diagnosis;quantitative characteristic study;empirical study;log quality;open-source software;software logging;system monitoring;program compilers;history;production;programming;servers	Software logging is a conventional programming practice. As its efficacy is often important for users and developers to understand what have happened in production run, yet software logging is often done in an arbitrarily manner. So far, there have been little study for understanding logging practices in real world software. This paper makes the first attempt (to the best of our knowledge) to provide quantitative characteristic study of the current log messages within four pieces of large open-source software. First, we quantitatively show that software logging is pervasive. By examining developers’ own modifications to logging code in revision history, we find that they often do not make the log messages right in their first attempts, and thus need to spend significant amount of efforts to modify the log messages as after-thoughts. Our study further provides several interesting findings on where developers spend most of their efforts in modifying the log messages, which can give insights for programmers, tool developers, and language and compiler designers to improve the current logging practice. To demonstrate the benefit of our study, we built a simple checker based on one of our findings and effectively detected 138 new problematic logging code from studied software (24 of them are already confirmed and fixed by developers).	compiler;data logger;ibm notes;open-source software;pervasive informatics;programmer;programming language;software design pattern;gift	Ding Yuan;Soyeon Park;Yuanyuan Zhou	2012	2012 34th International Conference on Software Engineering (ICSE)		system monitoring;programming;real-time computing;computer science;operating system;software engineering;programming language;empirical research;management;public domain software;world wide web;server	SE	-57.030933377369834	38.666222199893326	128051
3cdcc41a3680ade0786cea362dde27cf002012fe	automatic test data generation targeting hybrid coverage criteria		Software used in safety critical domains such as aviation and automotive has to be rigorously tested. Since exhaustive testing is not feasible, Modified Condition/Decision Coverage (MC/DC) has been introduced as an effective structural coverage alternative. However, studies have shown that complementing the test cases satisfying MC/DC to also satisfy Boundary Value Analysis (BVA) increases the bug finding rate. Hence, the industry adopted its testing processes to accommodate both. Satisfying these coverage requirements manually is very expensive and as a result many efforts were put to automate this task. Genetic algorithms (GA) have shown their effectiveness so far in this area. We propose an approach employing GA techniques and targeting hybrid coverage criteria to increase BVA in addition to MC/DC.	test data generation	Ahmed El-Serafy;Cherif Salama;Ayman Wahba	2015		10.1007/978-3-319-22689-7_11	reliability engineering;data mining;computer security	SE	-60.41151094335007	34.37277075225506	128209
f4d6836c292e8ec2a80799e100209effe6e90476	regression testing for model transformations: a multi-objective approach	bepress selected works;multi objective optimization;model transformation;search based software engineering;testing;model transformation multi objective optimization search based software engineering testing	In current model-driven engineering practices, metamodels are modified followed by an update of transformation rules. Next, the updated transformation mechanism should be validated to ensure quality and robustness. Model transformation testing is a recently proposed effective technique used to validate transformation mechanisms. In this paper, a more efficient approach to model transformation testing is proposed by refactoring the existing test case models, employed to test previous metamodel and transformation mechanism versions, to cover new changes. To this end, a multi-objective optimization algorithm is employed to generate test case models that maximizes the coverage of the new metamodel while minimizing the number of test case model refactorings as well as test case model elements that have become invalid due to the new changes. Validation results on a widely used transformation mechanism confirm the effectiveness of our approach.	algorithm;code refactoring;database schema;mathematical optimization;metaheuristic;metamodeling;model transformation;model-driven architecture;model-driven engineering;multi-objective optimization;petri net;regression testing;test case	Jeffery Shelburg;Marouane Kessentini;Daniel R. Tauritz	2013		10.1007/978-3-642-39742-4_16	mathematical optimization;model-based testing;search-based software engineering;software engineering;multi-objective optimization;data mining;software testing;test management approach	SE	-58.58866381490493	33.69698135339837	128325
090203127dc92ba26f7e6df39e0a12aa238e7a58	lightweight control-flow instrumentation and postmortem analysis in support of debugging	shrinking stack sensitive interprocedural static slices lightweight control flow instrumentation postmortem analysis post deployment debugging program activity latent information post failure memory dumps run time tracing realistically tuned tracing scheme postmortem static slice restriction technique potentially executed code;program slicing program debugging;program debugging;program slicing;instruments core dumps debugging arrays computer crashes algorithm design and analysis production	Debugging is difficult and costly. As a programmer looks for a bug, it would be helpful to see a complete trace of events leading to the point of failure. Unfortunately, full tracing is simply too slow to use after deployment, and may even be impractical during testing. We aid post-deployment debugging by giving programmers additional information about program activity shortly before failure. We use latent information in post-failure memory dumps, augmented by low-overhead, tunable run-time tracing. Our results with a realistically-tuned tracing scheme show low enough overhead (0–5 %) to be used in production runs. We demonstrate several potential uses of this enhanced information, including a novel postmortem static slice restriction technique and a reduced view of potentially-executed code. Experimental evaluation shows our approach to be very effective. For example, our analyses shrink stack-sensitive interprocedural static slices by 53–78 % in larger applications.	control flow;debugging;overhead (computing);programmer;reliability engineering;software deployment	Peter Ohmann;Ben Liblit	2013	2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1007/s10515-016-0190-1	program slicing;parallel computing;real-time computing;tracing;computer science;operating system;software engineering;algorithmic program debugging;programming language	SE	-60.425505637712796	37.47927320990949	128341
2156b6f279bd08d2b8b27389547565299a4c9715	spectrum enhanced dynamic slicing for better fault localization		Debugging consumes a considerable amount of time in software engineering, but it is rarely automated. In this paper, we focus on improving existing fault localization techniques. Spectrum-based fault localization (SFL) and slicing-hitting-set-computation (SHSC) are two techniques based on program execution traces. Both techniques come with small computational overhead and aid programmers to faster identify possible locations of faults. However, they have disadvantages: SHSC results in an undesirable high ranking of statements which are executed in many test cases, such as constructors. SFL operates on block level. Therefore, it cannot provide fine-grained results. We combine SHSC with SFL in order to eliminate these disadvantages. Our objective is to improve the ranking of faulty statements so that they allow for better fault localization than when using the previously mentioned methods separately. We show empirically that the resulting approach reduces the number of statements a programmer needs to check manually. In particular, we gain improvements of about 50% percent for SHSC and 25 % for SFL.		Birgit Hofer;Franz Wotawa	2012		10.3233/978-1-61499-098-7-420	real-time computing;computer science;artificial intelligence;theoretical computer science;machine learning;algorithm	SE	-60.05086157301083	37.285846037786705	128575
12887b4e571dc30ebb34fd4b35af4cd60e9f2f5c	applying object-oriented construction to fault tolerant systems	libraries;dynamic programming;production costs;fault tolerant;software libraries;application software;data diversity;class libraries;software fault tolerance;object oriented programming;redundant components;fault tolerant system;fault tolerant systems;inheritance hierarchy;object oriented;production costs object oriented construction fault tolerant systems object diversity data diversity algorithm diversity inheritance hierarchy redundant components class libraries;directional data;object oriented construction;fault tolerance;software reusability;object diversity;production systems;fault tolerant systems fault tolerance application software medical control systems dynamic programming computer science libraries production systems costs humans;humans;computer science;production cost;inheritance;algorithm diversity;software reusability object oriented programming software fault tolerance inheritance software libraries;medical control systems	"""This paper investigates the application of object-oriented construction to fault tolerant systems. The resulting system provides traditional fault tolerance within objects, but also a new form of fault tolerance between objects: object diversity. Object diversity extends current practice by integrating diversity in two directions: data and algorithm. This resulting form will allow increased diversity to be incorporated within fault tolerant systems. Further benefits are derived from the use of the inheritance hierarchy as a natural source of redundant components. As class libraries (both general and application specific) grow, more and more """"free"""" redundant components will become available, yielding increasing savings on production costs. >"""	fault tolerance	James Miller;Murray Wood;Andrew Brooks;Marc Roper	1994		10.1109/APSEC.1994.465277	reliability engineering;fault tolerance;real-time computing;computer science;engineering;distributed computing;programming language	Logic	-51.201029184549576	32.95785403370694	128606
a553691124ba866925ed80243272bfaa3e76339f	a novel approach to optimize clone refactoring activity	mutation testing;evolution modeling;ant colony;refactoring effort;multi objective optimization;software engineering;effort prediction;development tool;cost effectiveness;genetic algorithm;genetic algorithms;tabu search;software quality improvement;software quality;reverse engineering	Software evolution and software quality are ever changing phenomena. As software evolves, evolution impacts software quality. On the other hand, software quality needs may drive software evolution strategies.This paper presents an approach to schedule quality improvement under constraints and priority. The general problem of scheduling quality improvement has been instantiated into the concrete problem of planning duplicated code removal in a geographical information system developed in C throughout the last 20 years. Priority and constraints arise from development team and from the adopted development process. The developer team long term goal is to get rid of duplicated code, improve software structure, decrease coupling, and improve cohesion.We present our problem formulation, the adopted approach, including a model of clone removal effort and preliminary results obtained on a real world application.	code refactoring;cohesion (computer science);duplicate code;evolution strategy;geographic information system;scheduling (computing);software evolution;software quality	Salah Bouktif;Giuliano Antoniol;Ettore Merlo;Markus Neteler	2006		10.1145/1143997.1144312	mathematical optimization;verification and validation;team software process;simulation;genetic algorithm;software engineering process group;software sizing;search-based software engineering;computer science;artificial intelligence;software development;machine learning;software construction;software testing;software maintenance;software deployment;software quality control;goal-driven software development process;software metric	SE	-58.278151200935916	34.14983178737692	128649
6b0f8ddc774bccf4b07752f81659baa72af719f0	a knowledge-based debugger for real-time software systems based on a noninterference testing architecture	real time systems software systems software testing system testing debugging monitoring timing computer errors computer architecture history;real time;testing and debugging;software systems;indexing terms;program testing;timing errors error localization knowledge based debugger real time software systems noninterference testing architecture execution behavior program execution history target system timing constraints redundant information postprocessing organize necessary information testing redundant data required data knowledge based debugging tool synchronization;software tools;program debugging;knowledge based systems;software tools knowledge based systems program debugging program testing;knowledge base;time constraint	On the basis of a general mechanism for reproducing the execution behavior of real-time software systems, a new approach is suggested for a knowledge-based debugger for real-time software systems. A description is given of the use of noninterference architecture to collect the program execution history of a target system without interfering with the execution of the target system. Thus, the timing constraints of a real-time software system can be preserved. To eliminate the redundant information in the collected execution history, a postprocessing mechanism is introduced to organize the necessary information for testing and debugging. The redundant data, which are useless for debugging certain types of error, are removed, and only the required data proceed to the debugging phase. A knowledge-based debugging tool is then used to assist users in localizing errors. Examples are used to illustrate the approach in detecting synchronization and timing errors of real-time software systems. >	debugger;non-interference (security);real-time transcription;software system	Jeffrey J. P. Tsai;Kwang-Ya Fang;Horng-Yuan Chen	1989		10.1109/CMPSAC.1989.65160	knowledge base;computer architecture;regression testing;real-time computing;index term;tracing;orthogonal array testing;system integration testing;computer science;artificial intelligence;software reliability testing;software development;operating system;software construction;database;dynamic program analysis;software testing;algorithmic program debugging;programming language;debugging;software quality;software metric;software system	SE	-54.30754768896086	33.786868967188155	128709
8e0deae12dc874b0bc5f2d7a150e4dec719c3e37	inline method considered helpful: an approach to interface evolution	outil logiciel;algoritmo paralelo;software tool;parallel algorithm;generation code;software management;generacion codigo;code generation;gestion configuration;software configuration management;algorithme parallele;herramienta controlada por logicial;next generation;source code;configuration management;extraction method;gestion logiciel	While Extract Method is generally considered one of the most useful refactorings, the inverse refactoring Inline Method is so far only documented for removing methods whose bodies are as clear as their names. This paper outlines an approach how Inline Method may be used in changing method signatures and behavior. Furthermore, it proposes how the approach may simplify evolving published interfaces as well as merging parallel source code changes in next generation software configuration management tools.		Tammo Freese	2003		10.1007/3-540-44870-5_33	software configuration management;computer science;operating system;parallel algorithm;configuration management;programming language;algorithm;code generation;source code	SE	-56.35464924833761	35.66059127802263	128768
440cf930639d5cffd093db91ff53b0b31570a119	on the test case definition for gui testing	second order;internet browser test case definition gui testing graphical user interfaces formal language;formal languages;higher order;graphical user interfaces;first order;program testing;gui testing;case definition;computer aided software engineering graphical user interfaces application software software testing formal languages performance evaluation operating systems printers automatic testing solids;formal languages program testing graphical user interfaces;formal language	GUI testing is an area of growing importance, facing a number of severe challenges. A few methods have been proposed for GUI testing. However it is still not clear how to define GUI test cases and how many actions should be comprised of a GUI test case. In this paper we propose an approach that defines GUI test cases as a sequence of primitive GUI actions and treats GUI test suites as an inner hierarchy of formal language. This is not only theoretically solid but also practically convenient. The dimension of a GUI test suite and the order of a GUI test case can be defined uniquely. A convenient procedure is available that generates higher-order test cases from lower-order test cases. Three testing experiments with a real-world Internet browser reveal that second-order test cases may significantly outperform first-order test cases in GUI testing and should be generated to perform particular GUI functions. In addition, the number of actions applied during testing should be used to replace the number of tests performed during testing to evaluate the effectiveness of GUI testing processes. This paper provides a potential link between formal language theory and GUI testing.	experiment;first-order predicate;formal language;graphical user interface testing;test case;test suite	Kai-Yuan Cai;Lei Zhao;Hai Hu;Chang-Hai Jiang	2005	Fifth International Conference on Quality Software (QSIC'05)	10.1109/QSIC.2005.45	keyword-driven testing;formal language;orthogonal array testing;white-box testing;manual testing;computer science;theoretical computer science;software engineering;arinc 661;programming language;graphical user interface testing;test management approach	SE	-56.840944096620426	37.31141427695448	129281
d78822b0649603ddd1f3bc88d443406cff9ca711	making live programming practical by bridging the gap between trial-and-error development and unit testing	debugging;testing;live programming	"""Live programming environments are powerful experimental tools that enable programmers to write programs in a trial-and-error way thanks to its quick feedback. Since the feedback includes intermediate data such as a control flow and a history of variable bindings, the live programming environments integrate debugging into editing. One of the disadvantages of such interactive systems is that tests are transient. If we wrote persistent tests using an automated testing framework like JUnit, we could not fully enjoy """"liveness."""" This is because we need to write proper parameters and expected values in advance. We develop Shiranui, a live programming environment with unit testing features. In Shiranui, the programmers can check functions' behaviors in a lively manner and then convert the results into persistent test cases. One of the features enables the programmers to make a test case from an intermediate result that are found in a debugging process. It makes constructing error-reproducing-tests easier."""	automated testing framework;bridging (networking);control flow;debugging;integrated development environment;interactive programming;junit;lively kernel;liveness;programmer;test automation;test case;transient (computer programming);unit testing	Tomoki Imai;Hidehiko Masuhara;Tomoyuki Aotani	2015		10.1145/2814189.2814193	real-time computing;simulation;computer science;operating system;software testing;programming language;debugging	SE	-54.05641399110119	37.329996837639804	129322
1581e9399edaf9f0fb3ddbf9ecfc005fabee54ab	comb: computing relevant program behaviors		The paper presents COMB, a tool to improve accuracy and efficiency of software engineering tasks that hinge on computing all relevant program behaviors. Computing all behaviors and selecting the relevant ones is computationally intractable. COMB uses Projected Control Graph (PCG) abstraction to derive the relevant behaviors directly and efficiently. The PCG is important as the number of behaviors relevant to a task is often significantly smaller than the totality of behaviors.  COMB provides extensive capabilities for program comprehension, analysis, and verification. We present a basic case study and a Linux verification study to demonstrate various capabilities of COMB and the addressed challenges. COMB is designed to support multiple programming languages. We demonstrate it for C and Java. Video url: https://youtu.be/YoOJ7avBIdk	computational complexity theory;computer security;java;lempel–ziv–stac;linux;linux;program comprehension;programming language;scalability;software engineering	Benjamin Holland;Payas Awadhutkar;Suresh Kothari;Ahmed Tamrawi;Jon Mathews	2018	2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)	10.1145/3183440.3183476	software bug;software analysis pattern;real-time computing;software verification;visualization;software;computer science;theoretical computer science;program comprehension;java;abstraction	SE	-56.19407736555644	38.24700047511326	129354
b1f3bb8000025c4db93b96ebb5f9a0bd48400075	test-case generation and bug-finding through symbolic execution	proceedings national;symbolic execution;test case generation;defect detection	In this paper we present Artemis, a tool to analyse Java bytecode and discover run-time errors. Artemis uses the method of symbolic execution to perform path-sensitive analysis on compiled Java classes, in the process building up constraints under which errors like null pointer dereferences and division-by-zero errors can occur. During the analysis, many warnings for possible errors may occur, but not all paths leading to these warnings are feasible. Artemis uses an external decision procedure---a constraint solver---to decide the feasibility of paths, and only if a path is feasible does it generate a JUnit test case for that path. It signals the possibility of a real error only if a test case manages to detect an expected exception during an actual run by the JUnit core.  Artemis allows control of the analysis in two important ways: (1) The depth to which method calls descend can be bounded, and (2) so can the number of times branch statements are executed over symbolic values.  We performed an evaluation of a small set of non-trivial benchmarking tests, averaging 450 lines of code, and we concluded that using a call depth of one and branch bound of two is optimal for the discovery of bugs. Although Artemis was written as a Java-only solution, it was constructed so that the various components connect via interfaces. Therefore it is easy to extend, and we plan to explore, amongst others, different constraint solvers in the future.	compiler;decision problem;division by zero;junit;java bytecode;pointer (computer programming);software bug;solver;source lines of code;symbolic execution;test case	Willem Bester;Cornelia P. Inggs;W. C. Visser	2012		10.1145/2389836.2389838	real-time computing;computer science;programming language;algorithm	SE	-58.32591766826211	37.83340833615431	129503
1fbb0567be28394dfc878a4bb9fc945dab06e968	evaluating how developers use general-purpose web-search for code retrieval		Search is an integral part of a software development process. Developers often use search engines to look for information during development, including reusable code snippets, API understanding, and reference examples. Developers tend to prefer general-purpose search engines like Google, which are often not optimized for code related documents and use search strategies and ranking techniques that are more optimized for generic, non-code related information.  In this paper, we explore whether a general purpose search engine like Google is an optimal choice for code-related searches. In particular, we investigate whether the performance of searching with Google varies for code vs. non-code related searches. To analyze this, we collect search logs from 310 developers that contains nearly 150,000 search queries from Google and the associated result clicks. To differentiate between code-related searches and non-code-related searches, we build a model which identifies the code intent of queries. Leveraging this model, we build an automatic classifier that detects a code and non-code related query. We confirm the effectiveness of the classifier on manually annotated queries where the classifier achieves a precision of 87%, a recall of 86%, and an F1-score of 87%. We apply this classifier to automatically annotate all the queries in the dataset. Analyzing this dataset, we observe that code related searching often requires more effort (e.g., time, result clicks, and query modifications) than general non-code search, which indicates code search performance with a general search engine is less effective.		Md Masudur Rahman;Jed Barson;Sydney Paul;Joshua Kayan;Federico Andres Lois;Sebastian Fernandez Quezada;Christopher Parnin;Kathryn T. Stolee;Baishakhi Ray	2018	2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)	10.1145/3196398.3196425	data mining;task analysis;software development process;search engine;software;requirements traceability;computer science;software maintenance;java;code (cryptography)	SE	-59.82330615424494	39.47296265548012	129522
5da1d5c29fee2140f47b9c7042f073bb27970ee1	evaluation of subsetting programming language elements in a novice's programming environment	programming language;programming environment;user interface;project manager;data capture;language subsets	In this paper, we evaluate the effects of applying programming language subsets to the programming environment used by novice (CS1) students in a closed-laboratory setting, as well as reducing the complexity of the user interface for the environment. Our goal in this effort was to assess if such interface and application-level changes adversely impact the student or hinder the later migration to a traditional professional-strength programming environment.We focus on the comparison of the quantitative data captured from the closed-laboratory sessions (assignment grades, number of syntax / semantic errors, and the number of compilation / execution attempts) involving subjects that used a new programming environment featuring a less complex interface in two forms: one lacking support of language subsets, and one supporting the application of language subsets.We found that while using the environment supporting the application of language subsets, there was no difference in the compilation-attempt rate, the number of errors did not increase, and student grades were equivalent between the experiment groups. Additionally, following a migration to the professional environment from the simplified environment, student grades were equivalent to those that had used the professional environment throughout the experiment. Student reaction to the experimental environment was very positive, especially related to the removal of unused tools and project management features.	compiler;integrated development environment;programming language;user interface	Peter DePasquale;John A. N. Lee;Manuel A. Pérez-Quiñones	2004		10.1145/971300.971392	fourth-generation programming language;interface description language;natural language programming;simulation;language primitive;reactive programming;data control language;computer science;extensible programming;operating system;automatic identification and data capture;multimedia;programming paradigm;procedural programming;symbolic programming;low-level programming language;inductive programming;fifth-generation programming language;visual programming language;programming language;user interface;programming language specification;high-level programming language	SE	-53.61042162678764	37.05864580075662	129721
4269324926a6cb31ad87e0b8fd789b5a3174b510	program transformation for time-aware instrumentation	program diagnostics;safety critical software computerised instrumentation embedded systems program diagnostics;embedded systems;safety critical software;program instrumentability time aware instrumentation program behavior safety critical real time embedded applications instrumentation techniques etp shift effectiveness metric program transformation techniques;computerised instrumentation	Instrumentation is a valuable technique to gain insight into a program's behavior. Safety-critical real-time embedded applications are time sensitive and so instrumentation techniques for this domain must especially consider timing. This work establishes the basis for measuring the effectiveness of approaches for time-aware instrumentation in addition to coverage. We define the ETP shift effectiveness metric and define its optimality criterion. We identify locations in the program where program transformation techniques can be applied to increase the instrumentability of the program. We subsequently use the proposed metric to evaluate two transformation methods that improve the effectiveness and coverage of current techniques for time-aware instrumentation by a factor of five.	correctness (computer science);embedded system;information extraction;optimality criterion;program transformation;real-time clock;run time (program lifecycle phase)	Hany Kashif;Sebastian Fischmeister	2012	Proceedings of 2012 IEEE 17th International Conference on Emerging Technologies & Factory Automation (ETFA 2012)	10.1109/ETFA.2012.6489580	embedded system;instrumentation;real-time computing;computer science;engineering;computer engineering	EDA	-61.311176195252976	36.924397773954155	130000
6e79a1336f57200856c3ddaac5323497c3af01da	unit tests as api usage examples	api unit tests;api;code example;unit testing;barium;application programming interface api usage examples api unit tests standard api documentation advanced usage scenarios;standard api documentation;noise measurement;program testing application program interfaces;code example api usability unit test documentation;program testing;application program interfaces;barium noise measurement;api usage examples;advanced usage scenarios;usability;unit test;documentation;application programming interface	This study aims to find out if API unit tests can provide good usage examples, and if so, what prevents developers from finding and using those examples. The results of an experiment we performed with two groups of developers showed that unit tests can be very helpful, especially when the task is complicated and involves multiple classes and methods. Well-written tests proved to be a good source of examples, but finding the relevant examples using the standard tools might be very difficult. We propose to supplement the standard API documentation with relevant examples taken from the unit tests. To further improve the learnability of the API, presentation of the documentation and examples has to be tailored in a way that separates or hides advanced usage scenarios from the commonly used ones.	application programming interface;documentation;learnability;unit testing	Seyed Mehdi Nasehi;Frank Maurer	2010	2010 IEEE International Conference on Software Maintenance	10.1109/ICSM.2010.5609553	embedded system;application programming interface;computer science;database;unit testing;programming language;world wide web	SE	-53.91307191206984	38.01294953235899	130110
02a182e1332c8886f21c0affea097ee721cf01c8	verifying consistency and validity of formal specifications by testing	developpement logiciel;formal specification;tolerance aux pannes logiciel;logiciel a securite critique;software fault tolerance;ingenieria logiciel;software engineering;specification formelle;especificacion formal;formal verification;desarrollo logicial;specification tests;safety critical software;software development;genie logiciel;verification formelle;verification and validation	Detecting faults in specications can help reduce the cost and risk of software development because incorrect implementation can be prevented early. This goal can be achieved by verifying the consistency and validity of specications. In this paper we put forward specication testing as a practical technique for verication and validation of formal specications. Our approach is to derive proof obligations from a specication and then test them, in order to detect faults leading to the violation of consistency or validity of the specication. We describe proof obligations for various consistency properties of a specication, and suggest the use of ve strategies for testing them. We provide a method for testing implicit specications by evaluation rather than by prototyping, and criteria for interpreting the meaning of test results.	software development;verification and validation	Shaoying Liu	1999		10.1007/3-540-48119-2_49	verification and validation;formal verification;computer science;software development;formal specification;programming language;algorithm;software fault tolerance	SE	-48.60117380434898	33.98840668799018	130125
dfe0733b80fc8d9ad198154ec0ebf4cab3a08462	hotwave: creating adaptive tools with dynamic aspect-oriented programming in java	runtime weaving;virtual machine;aspectj;profiling;java virtual machine;development tool;dynamic aspect oriented programming;aspect oriented programming;code hotswapping;bytecode instrumentation;reverse engineering	Developing tools for profiling, debugging, testing, and reverse engineering is error-prone, time-consuming, and therefore costly when using low-level techniques, such as bytecode instrumentation. As a solution to these problems, we promote tool development in Java using high-level aspect-oriented programming (AOP). We demonstrate that the use of aspects yields compact tools that are easy to develop and extend. As enabling technology, we rely on HotWave, a new tool for dynamic and comprehensive aspect weaving. HotWave reconciles compatibility with existing virtual machine and AOP technologies. It provides support for runtime adaptation of aspects and reweaving of previously loaded code, as well as the ability to weave aspects into all methods executing in a Java Virtual Machine, including methods in the standard Java class library. HotWave also features a new mechanism for efficiently passing data between advices that are woven into the same method. We demonstrate the benefits of HotWave's distinguishing features with two case studies in the area of profiling.	aspect-oriented programming;cognitive dimensions of notations;debugging;high- and low-level;java class library;java virtual machine;profiling (computer programming);reverse engineering	Alex Villazón;Walter Binder;Danilo Ansaloni;Philippe Moret	2009		10.1145/1621607.1621622	real-time computing;aspect-oriented programming;java concurrency;computer science;virtual machine;operating system;strictfp;real time java;profiling;programming language;java;reverse engineering;java annotation	PL	-55.48324525994626	37.92976598938992	130378
3492c85babed6b1213b5e0f2d06d8b2dff36825e	cleanroom: edit-time error detection with the uniqueness heuristic	dynamic languages;program diagnostics;cleanroom edit time error detection dynamic programming language features standard program analysis uniqueness heuristic;error detection dynamic languages bugs;code generation;dynamic program;software engineering;software engineering error detection program debugging program diagnostics programming languages;bugs;program analysis;program debugging;error detection;html calculators syntactics dictionaries ieee potentials semantics reflection;programming languages	Many dynamic programming language features, such as implicit declaration, reflection, and code generation, make it difficult to verify the existence of identifiers through standard program analysis. We present an alternative verification, which, rather than analyzing the semantics of code, highlights any name or pair of names that appear only once across a program’s source files. This uniqueness heuristic is implemented for HTML, CSS, and JavaScript, in an interactive editor called Cleanroom, which highlights lone identifiers after each keystroke. Through an online experiment, we show that Cleanroom detects real errors, that it helps developers find these errors more quickly than developers can find them on their own, and that this helps developers avoid costly debugging effort by reducing how many times a program is executed with potential errors. The simplicity and power of Cleanroom’s heuristic may generalize well to other dynamic languages with little support for edit-time name verification.	cascading style sheets;cleanroom;code generation (compiler);debugging;declaration (computer programming);dynamic programming;error detection and correction;event (computing);html;heuristic;identifier;javascript;program analysis;program lifecycle phase;programming language;reflection (computer programming);sensor	Andrew Jensen Ko;Jacob O. Wobbrock	2010	2010 IEEE Symposium on Visual Languages and Human-Centric Computing	10.1109/VLHCC.2010.11	computer science;theoretical computer science;database;programming language	SE	-57.59670514346146	38.80456515302219	130394
18e100793c9a42a7943faab8f05299b6bb782adc	exploring the instability of spectra based fault localization performance	spectra based fault localization;software performance evaluation program testing software fault tolerance;risk evaluation formula spectra based fault localization performance instability software fault localization testing process testing resources sbfl instability stochastic quantity stability level test suite size;instability;software fault localization;flexible printed circuits standards software software testing conferences electronic mail;instability software fault localization spectra based fault localization	Spectra Based Fault Localization (SBFL) is a technique to improve the efficiency of software fault localization. The performance of SBFL largely depends on the input information provided by an executed test suite. Due to the randomness existing in the testing process, the output of SBFL may not be stable. In practice, testers do not have the chance to run the whole testing process many times. They are not sure whether the actually obtained SBFL output has a large deviation from the ideal output (i.e. the SBFL output obtained under the assumption that the amount of testing resources is unlimited). Thus, concerning the application of SBFL in real cases, such instability of its performance (SBFL instability for short) is a challenge. In this paper, the SBFL instability is discussed and its characteristics are further explored. Specifically, we define SBFL instability as a stochastic quantity and introduce the measure of StabilityLevel to quantify it. Then, based on the definition and measurement, we conduct experimental studies to demonstrate that SBFL instability is indeed a prevalent phenomenon and also a serious problem. Besides, two factors which influence the intensity of SBFL instability, i.e. the test suite size and risk evaluation formula, are observed and analyzed.	instability;rca spectra 70;randomness;test suite	Yuanchi Guo;Xiaoyi Zhang;Zheng Zheng	2016	2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2016.113	reliability engineering;real-time computing;simulation;engineering;computer security;instability	SE	-61.54433959375404	34.068973764508925	130411
65470a3e333b29066d582a0b9b38773a8e1ace0b	identifying test conditions for software maintenance	program understanding;atomic program;automatic test condition identification;software testing;atomic measurements;software prototyping;application software;software maintenance;atomic program automatic test condition identification software system maintenance program points program dependence graph representation test suite maintenance program understanding system behaviour;automatic testing;software systems;computer industry;data flow analysis software maintenance reverse engineering program testing;impact analysis;program testing;data flow analysis;production;program points;system testing;system behaviour;software testing software maintenance system testing software systems computer industry automatic testing atomic measurements application software production software prototyping;test suite maintenance;reverse engineering;program dependence graph representation;software system maintenance;program dependence graph	As a software system goes through maintenance, it is necessary to test that the system's behaviour has changed (only) as expected. For each program change, this requires the identification of a set of test conditions that together provide some measure of confidence about the effects of the change. In this paper, we propose a technique to automatically identify the set of conditions to completely test for the effects of an atomic program change at some identified 'target' points in the program. This work builds upon our work to identify the conditions for influence between program points using a program dependence graph representation. The set of conditions thus identified can be used to either select tests from an existing test-suite, or to produce new tests that do not belong to the test-suite, thus helping in maintaining the test-suite as the system evolves. Some results of using our technique on sample programs are also reported. We also briefly discuss how the basic idea behind our technique can have potential applications in areas such as impact analysis and program understanding.	cognitive dimensions of notations;function representation;graph (abstract data type);program dependence graph;program comprehension;regression testing;software maintenance;software system;test case;test suite	Srihari Sukumaran;Ashok Sreenivas	2005	Ninth European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2005.32	program analysis;reliability engineering;application software;computer science;systems engineering;engineering;software engineering;data-flow analysis;dynamic program analysis;software testing;software maintenance;system testing;reverse engineering;software system	SE	-58.52464376988585	35.044226873375905	130426
08cb200bffbcf164713b8ee63565ba264b6456c0	toolkit to support intelligibility in context-aware applications	application development;context aware application;decision models;context awareness;user needs;context aware;generic algorithm;explanations;intelligibility;toolkits	Context-aware applications should be intelligible so users can better understand how they work and improve their trust in them. However, providing intelligibility is non-trivial and requires the developer to understand how to generate explanations from application decision models. Furthermore, users need different types of explanations and this complicates the implementation of intelligibility. We have developed the Intelligibility Toolkit that makes it easy for application developers to obtain eight types of explanations from the most popular decision models of context-aware applications. We describe its extensible architecture, and the explanation generation algorithms we developed. We validate the usefulness of the toolkit with three canonical applications that use the toolkit to generate explanations for end-users.	algorithm;intelligibility (philosophy)	Brian Y. Lim;Anind K. Dey	2010		10.1145/1864349.1864353	natural language processing;decision model;genetic algorithm;computer science;theoretical computer science;data mining;rapid application development;intelligibility	HCI	-49.32128830827311	38.67092876117646	130664
284fa3371b368af2732f12e2cc192c13eca185d2	a functional correctness model of program verification	condition dependence;trace table functional correctness model program verification elementary symbolic execution;testing watches computer errors specification languages computer science object oriented modeling software quality education computer bugs design methodology;program verification;symbolic execution	A model whose verification conditions depend only on elementary symbolic execution of a trace table is presented. The method is applied to rather simple programs. However, even in large complex implementations, the techniques can be applied informally to determine the functionality of complex interactions. The technique is easy to learn (it is used in a freshman computer science course) and lends itself to automation.<<ETX>>	computer science;correctness (computer science);formal verification;interaction;symbolic execution;trace table	Marvin V. Zelkowitz	1990	Computer	10.1109/2.60878	program analysis;verification;computer science;theoretical computer science;programming language;concolic testing;intelligent verification;symbolic trajectory evaluation;functional verification	Logic	-50.027364866762916	33.94708123979746	131015
99096a4a7972ca59bb179552375a7375e3bb6627	a configurable benchmark test management framework	system under test;stakeholders;system performance;requirement elicitation;scenarios;problem driven;reference architecture	By looking over the performance benchmarks, we found that test systems are becoming more and more complex [1]. It is expensive for academe to implement every benchmark systems from scratch, as well as the customized benchmarks, to measure particular system under test (SUT). We propose a configurable benchmark test management framework, which gives a structure intended to serve as a support or guide for the building of testbed benchmark systems that expands the reference architecture into something useful in internetware components benchmark and real system performance evaluation.	benchmark (computing);performance evaluation;reference architecture;system under test;test management;testbed;turing test	Naiqiao Du;Xiaojun Ye;Jianmin Wang	2009		10.1145/1640206.1640231	sdet;real-time computing;simulation;systems engineering;engineering	SE	-48.547799379338215	37.2062120914072	131098
411f882126b402c09a8e6fefafeb1c713e739928	functional safety certification: practice and issues	bridges usa councils automation conferences;functional safety certification;programmable controllers;automation control system;cost saving;safety systems;bridges;usa councils;programmable electronic system;control system;functional analysis;industrial control;control logic verification method functional safety certification programmable electronic system automation control system work in progress;control logic verification method;work in progress;work in progress functional analysis industrial control programmable controllers safety systems;conferences;automation	As the increasing use of programmable electronic systems to carry out safety functions of automation control systems, there is an emerging need for functional safety certification in order to protect people, equipment and work in progress (WIP) from injuries and damages. The current practice of functional safety certification and other existing control logic verification methods that could be used for functional safety certification are compared, and issues are identified. With that, a new approach and the associated technical challenges for its implementation are discussed. It aims to improve the certification process by significantly reducing setup cost, saving certification time, and improving certification quality. The paper concludes with a future research plan for solving identified challenges.	automation;control system;hybrid functional;web standards	Jing Liu;Chengyin Yuan;Fangming Gu;Stephan Biller	2008	2008 IEEE International Conference on Automation Science and Engineering	10.1109/COASE.2008.4626563	systems engineering;engineering;software engineering;functional safety;computer engineering	EDA	-56.8255538009549	45.85654528080561	131130
857fc1be23924b3ca08c320390cdfc38bddee543	isolating failure-inducing thread schedules	debugging;runtime analysis;uml;testing;junit	Consider a multi-threaded application that occasionally fails due to non-determinism. Using the DEJAVU capture/replay tool, it is possible to record the thread schedule and replay the application in a deterministic way. By systematically narrowing down the difference between a thread schedule that makes the program pass and another schedule that makes the program fail, the Delta Debugging approach can pinpoint the error location automatically---namely, the location(s) where a thread switch causes the program to fail. In a case study, Delta Debugging isolated the failure-inducing schedule difference from 3.8 billion differences in only 50 tests.	context switch;delta debugging;nondeterministic algorithm;schedule (computer science);thread (computing)	Jong-Deok Choi;Andreas Zeller	2002		10.1145/566172.566211	unified modeling language;parallel computing;real-time computing;computer science;operating system;software engineering;software testing;programming language;debugging	SE	-60.90062604347687	37.14260460819939	131258
43faf2c77a62e8a3781331612b61d38712c88235	featureide: a tool framework for feature-oriented software development	analytical models;software;computer languages;investments;mathematics;feature oriented software development;programming language;software maintenance;availability;probability density function;data mining;featureide;c language;feature extraction;xml;featurec;featurehouse feature oriented software development featureide programming language featurec;informatics;software tools;computer science;featurehouse;software tools c language software maintenance;programming computer languages computer science investments open source software java informatics mathematics education availability;programming;open source software;java	Tools support is crucial for the acceptance of a new programming language. However, providing such tool support is a huge investment that can usually not be provided for a research language. With FeatureIDE, we have built an IDE for AHEAD that integrates all phases of feature-oriented software development. To reuse this investment for other tools and languages, we refactored FeatureIDE into an open source framework that encapsulates the common ideas of feature-oriented software development and that can be reused and extended beyond AHEAD. Among others, we implemented extensions for FeatureC++ and FeatureHouse, but in general, FeatureIDE is open for everybody to showcase new research results and make them usable to a wide audience of students, researchers, and practitioners.	code refactoring;feature-oriented programming;open-source software;pl/i;programming language;software development	Christian Kästner;Thomas Thüm;Gunter Saake;Janet Siegmund;Thomas Leich;Fabian Wielgorz;Sven Apel	2009	2009 IEEE 31st International Conference on Software Engineering	10.1109/ICSE.2009.5070568	availability;programming;probability density function;xml;feature extraction;computer science;operating system;software engineering;database;programming language;software maintenance;java;informatics	SE	-54.75463445428678	33.011388140511166	131382
0875dd3bbc8eec1887a7100d7f0fca9a5c1ee297	visualizing feature interaction in 3-d	program diagnostics;software maintenance computer graphics program diagnostics program visualisation;static class hierarchy system view;3d visualization;software system;software maintenance;computer graphics;interactive 3d visualization;execution traces;feature execution;feature interaction;object interactions;maintenance change;dynamic behavior visualization;visual representations;feature implementation;object creations;software development;visual features;static analysis;static and dynamic analysis;visualization software maintenance terminology informatics software systems reverse engineering information analysis engines control systems runtime;program visualisation;dynamic analysis;feature execution feature interaction software system maintenance change feature implementation interactive 3d visualization static analysis dynamic analysis visual representations dynamic behavior visualization execution traces object creations object interactions static class hierarchy system view;dynamic behavior	Without a clear understanding of how features of a software system are implemented, a maintenance change in one part of the code may risk adversely affecting other features. Feature implementation and relationships between features are not explicit in the code. To address this problem, we propose an interactive 3D visualization technique based on a combination of static and dynamic analysis which enables the software developer to step through visual representations of execution traces. We visualize dynamic behaviors of execution traces in terms of object creations and interactions and represent this in the context of a static class-hierarchy view of a system. We describe how we apply our approach to a case study to visualize and identify common parts of the code that are active during feature execution	apply;class hierarchy;feature interaction problem;software developer;software system;tracing (software)	Orla Greevy;Michele Lanza;Christoph Wysseier	2005	3rd IEEE International Workshop on Visualizing Software for Understanding and Analysis	10.1109/VISSOF.2005.1684317	real-time computing;visualization;computer science;theoretical computer science;software development;operating system;software engineering;dynamic program analysis;programming language;computer graphics;software maintenance;static analysis;software system	SE	-54.82554224253111	35.71987827332051	131487
27dbd520a0a15f7796b40ca0e137aa9a4beef9fc	a systematic framework to optimize launch times of web apps	progressive web apps;web apps;native apps;launch time	Web Applications typically have longer launch times compared to native applications, especially upon device boot up or if the browser is not already running in the background. In this paper, we propose an approach to speed up the launch time for web applications, by considering the user's usage of web applications and pre-launching the predicted web applications. We provide implementation details of our model and perform experiments on various web applications to measure the effectiveness of the framework for fast launch of the applications after the device boots.	booting;experiment;launch time;machine code;web application	Suresh Kumar Gudla;Jitendra Kumar Sahoo;Abhishek Singh;Joy Bose;Nazeer Ahamed	2017		10.1145/3041021.3054206	web application;computer science	Web+IR	-53.95633758779265	38.519732550681745	131603
948f52d64c5fed7193c7b592e80b2d5022a7bb69	declarative testing: a paradigm for testing software applications	libraries;software;software testing;software quality application program interfaces automatic test software graphical user interfaces program testing;programming environments;test automation;declarative testing;application software;programming environment;automatic testing;information technology;testing software application;software testing automatic testing programming environments programming theory software quality;sequential analysis;graphical user interface;testing;development process;data mining;graphical user interfaces;programming theory;program testing;automatic test software;application program interfaces;software test automation;graphic user interface;system testing;software testing application software automatic testing automation graphical user interfaces system testing sequential analysis user interfaces libraries information technology;software quality declarative testing testing software application graphical user interface software test automation api set test automation code automatic testing;api set;user interfaces;context;test automation code;software quality;automation	Traditional techniques to test a software application through the application's graphical user interface have a number of weaknesses. Manual testing is slow, expensive, and does not scale well as the size and complexity of the application increases. Software test automation which exercises an application through the application's UI using an API set can be difficult to maintain. We propose a software testing paradigm called declarative testing. In declarative testing, a test scenario focuses on what to accomplish rather than on the imperative details of how to manipulate the state of an application under test and verify the final application state against an expected state. Declarative testing is a test design paradigm which separates test automation code into conceptual Answer, Executor, and Verifier entities. Preliminary experience with declarative testing suggests that the modular characteristics of the paradigm may significantly enhance the ability of a testing effort to keep pace with the evolution of a software application during the application's development process.	application programming interface;declarative programming;entity;graphical user interface;imperative programming;manual testing;programming paradigm;scalability;scenario testing;software testing;state (computer science);system under test;test automation;test design	Ed Triou;Zafar Abbas;Sravani Kothapalle	2009	2009 Sixth International Conference on Information Technology: New Generations	10.1109/ITNG.2009.85	non-regression testing;test strategy;keyword-driven testing;black-box testing;regression testing;model-based testing;software performance testing;pair testing;white-box testing;manual testing;system integration testing;computer science;software reliability testing;operating system;software engineering;software construction;graphical user interface;database;smoke testing;software testing;programming language;information technology;system testing;graphical user interface testing;test management approach	SE	-55.743895274355395	32.67421633724177	131609
a75d807d1bf5a8befb1cfe79d15604c73e5049ab	experience with regression test selection	regression testing	Regression testing is a maintenance process that attempts to validate modified software, and ensure that no new errors are introduced into previously tested code. Regression testing is expensive; it can account for as much as one-half of the cost of software maintenance (Leung and White, 1989). One approach to regression testing is selective retest , which addresses two problems: (1) the problem of selecting tests from an existing test suite, and (2) the problem of determining where additional tests may be required. We have developed a new selective retest technique that addresses the first problem (Rothermel and Harrold, 1996). Our algorithms construct control flow graphs for a procedure or program and its modified version, and use these graphs to select tests from the original test suite that execute changed code. To investigate the application of our technique in practice, we implemented one of our algorithms as a tool called DejaVu . We conducted empirical studies with our technique, by applyingDejaVu to various programs, modified versions, and test suites. Our empirical results support several conclusions about regression test selection. In particular, our results suggest that our technique can significantly reduce the cost of regression testing a modified program.	algorithm;control flow graph;regression testing;regular expression;software maintenance;test suite	Gregg Rothermel;Mary Jean Harrold	1997	Empirical Software Engineering	10.1023/A:1009765704299	regression testing;computer science;engineering;regression diagnostic	SE	-60.23233295014827	35.32531510603976	131659
af28a7168c8139dcb2164c9138bbff2efbb7d3ee	fine-grained library customization		Code bloat widely exists in production-run software. Left untackled, it not only degrades software performance but also increases its attack surface. In this work, we conduct a case study to understand this issue in statically linked libraries. To be specific, we analyze midilib, a software package enclosing statically linked libraries. We show that it is possible to leverage dependence analysis to trim the resultless code statements residing in a target library. With this observation, we believe it is possible to build a tool to automatically cut off code pertaining to resultless operations.	attack surface;code bloat;data structure;dependence analysis;executable;library (computing);software performance testing;static build;static library;static program analysis;test automation	Linhai Song;Xinyu Xing	2018	CoRR		computer science;trim;dependence analysis;leverage (finance);theoretical computer science;personalization;code bloat;attack surface;software;software performance testing	SE	-59.731720300876354	39.91156643295132	131751
d81d96eb3c0bd98015642d9a4db6ad8082d364b3	reproducibility of environment-dependent software failures: an experience report	software;debugging;software testing;random access memory;memory management;sql concurrency control failure analysis program diagnostics program testing;testing;my sql failure reproducibility environment dependent software failure reproducibility software execution environmental components my sql server software system my sql failure reports database disk usage concurrency level;servers;debugging software testing;computer bugs;memory management software computer bugs servers testing random access memory debugging	We investigate the dependence of software failure reproducibility on the environment in which the software is executed. The existence of such dependence is ascertained in literature, but so far it is not fully characterized. In this paper we pinpoint some of the environmental components that can affect the reproducibility of a failure and show this influence through an experimental campaign conducted on the My SQL Server software system. The set of failures of interest is drawn from My SQL's failure reports database and an experiment is designed for each of these failures. The experiments expose the influence of disk usage and level of concurrency on My SQL failure reproducibility. Furthermore, the results show that high levels of usage of these factors increase the probabilities of failure reproducibility.	concurrency (computer science);experiment;microsoft sql server;software bug;software system	Davide G. Cavezza;Roberto Pietrantuono;Javier Alonso;Stefano Russo;Kishor S. Trivedi	2014	2014 IEEE 25th International Symposium on Software Reliability Engineering	10.1109/ISSRE.2014.19	verification and validation;regression testing;real-time computing;computer science;package development process;software reliability testing;software development;operating system;software engineering;software construction;software release life cycle;database;software testing;programming language;software deployment;software system	SE	-61.4482270633468	36.60534850847073	131883
1a50b0c25e48ecb91d5b6d13474104ed572b20f8	systematic concurrency testing using chess	race condition;race conditions;testing;concurrency;model checking;stress testing	"""Concurrency testing should aim for systematic coverage of thread interleavings. The most common method used today is stress testing, where the program is run under load with lots of threads. While this indirectly increases the variety of thread interleavings, the coverage is neither sufficient and nor predictable---stories are legend of the so-called """"Heisenbugs"""" that rarely surface during testing and are very hard to debug.  In this talk, I will argue for a new notion of concurrency testing called scenario testing and describe CHESS, a tool we have developed towards that end. A user of CHESS provides simple concurrency scenarios and CHESS systematically enumerates all thread interleavings of these scenarios. CHESS employs model checking techniques to effectively focus on potentially bug-yielding schedules and provides sound quantifiable notions of coverage. On finding an error, CHESS has the capability to replay the erroneous interleaving, greatly simplifying the debugging process. CHESS has been successfully integrated with several codebases inside Microsoft and is used daily by the test teams. CHESS has found numerous bugs in systems that have been stress-tested for months. Additionally, CHESS has successfully reproduced many stress-test crashes that were previously hard to debug. The latter shows that many bugs that are found in stress-testing can indeed be reproduced in simple scenarios, a good validation for scenario testing.  CHESS is available for download at http://research.microsoft.com/CHESS. The talk will contain enough material to act as a tutorial for first-time users. I will also describe the challenges in applying CHESS to a new codebase and how to address them."""	concurrency (computer science);concurrency control;debugging;download;forward error correction;heisenbug;model checking;scenario testing;software bug;stress testing	Madan Musuvathi	2008		10.1145/1390841.1390851	real-time computing;simulation;computer science;distributed computing;race condition;programming language	SE	-56.91951263405727	39.16620765090513	132198
3262757dc8b3603a32599d28b9924c5ac49bf922	data-driven synthesis for object-oriented frameworks	user study;object oriented framework;dynamic program;program synthesis;software engineering;indexation;dynamic instrumentation;thin slicing	Software construction today often involves the use of large frameworks. The challenge in this type of programming is that object-oriented frameworks tend to grow exceedingly intricate; they spread functionality among numerous classes, and any use of the framework requires knowledge of many interacting components. We present a system named MATCHMAKER that from a simple query synthesizes code that interacts with the framework. The query consists of names of two framework classes, and our system produces code enabling interaction between them. MATCHMAKER relies on a database of dynamic program traces called DELIGHT that uses novel abstraction-based indexing techniques to answer queries about the evolution of heap connectivity in a matter of seconds.  The paper evaluates the performance and effectiveness of MATCHMAKER on a number of benchmarks from the Eclipse framework. The paper also presents the results of a user study that showed a 49% average productivity improvement from the use of our tool.	3delight;algorithm;application programming interface;eclipse;glue code;interaction;mit computer science and artificial intelligence laboratory;programmer;software construction;software framework;tracing (software);usability testing	Kuat Yessenov;Zhilei Xu;Armando Solar-Lezama	2011		10.1145/2048066.2048075	real-time computing;thin-slicing;computer science;database;programming language;world wide web	PL	-54.55452804773332	35.402854099097624	132215
39de9e8f51a926fa7e8390e6efb685b1616a405a	cheetah: just-in-time taint analysis for android apps		Current static-analysis tools are often long-running, which causes them to be sidelined into nightly build checks. As a result, developers rarely use such tools to detect bugs when writing code, because they disrupt their workflow. In this paper, we present Cheetah, a static taint analysis tool for Android apps that interleaves bug fixing and code development in the Eclipse integrated development environment. Cheetah is based on the novel concept of Just-in-Time static analysis that discovers and reports the most relevant results to the developer fast, and computes the more complex results incrementally later. Unlike traditional batch-style static-analysis tools, Cheetah causes minimal disruption to the developer's workflow. This video demo showcases the main features of Cheetah: https://www.youtube.com/watch?v=i_KQD-GTBdA.	daily build;denial-of-service attack;eclipse;esoteric programming language;integrated development environment;just-in-time compilation;neutral build;open-source software;software bug;software developer;static program analysis;taint checking;usability testing	Lisa Nguyen Quang Do;Karim Ali;Benjamin Livshits;Eric Bodden;Justin Smith;Emerson R. Murphy-Hill	2017	2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)		real-time computing;software bug;computer science;world wide web;taint checking;software;android (operating system);development environment;workflow;java;fuzz testing	SE	-56.52451044991657	38.54265415679211	132597
d8842d364578c7b6005329aaee28f161260b49fa	model-based automatic usability validation: a tool concept for improving web-based uis	automated validation;web usability;web pages;tool support;web engineering;integrated development environment;development environment;accessibility;usability model;usability guidelines	"""This paper describes an approach for improving automated usability tool support during the development of websites. Existing usability and accessibility validators only analyse the HTML code of a page when they create a report of possible problems. However, when a web engineering method is used to create a website, additional information is available in the form of models which describe the site. An automated validator can use these models to verify usability guidelines (such as """"text on the web page should be easy to understand by the target audience"""") with higher accuracy. It can also perform automatic validation in situations where existent tools only output instructions for manual inspection by the developer. The paper systematically analyses existent guidelines and tools, and identifies ways in which the use of a model can improve verification quality. An extension to existing web engineering models is necessary to support automated checkers. It specifies properties of the users, the technical platform and the environment of use. A flexible approach allows the models to be used by validators running inside an integrated development environment, but also at a later time, without access to the development environment. Finally, the prototype of a model-based automatic usability validator is presented. It features verification of a number of guidelines which cannot be automated by existent validation approaches."""	accessibility;html;integrated development environment;prototype;usability;validator;web application;web engineering;web page	Richard Atterer	2008		10.1145/1463160.1463163	usability goals;pluralistic walkthrough;web usability;component-based usability testing;cognitive walkthrough;usability;web design;human–computer interaction;web standards;computer science;accessibility;usability engineering;web page;data mining;development environment;web engineering;heuristic evaluation;world wide web;usability lab;usability inspection	SE	-53.809514897410686	37.96742698474173	132749
038867f57eba37473afef3ca6fdda0696e459be0	a context-sensitive coverage criterion for test suite reduction	test suite reduction technique;size reduction;context-sensitive coverage criterion;suite reduction;coverage criterion;modern software;software application;test suite reduction;automated test case generation;test suite;coverage-based test suite reduction	Modern software is increasingly developed using multi-language implementations, large supporting libraries and frameworks, callbacks, virtual function calls, reflection, multithreading, and object- and aspect-oriented programming. The predominant example of such software is the graphical user interface (GUI), which is used as a front-end to most of today's software applications. The characteristics of GUIs and other modern software present new challenges to software testing. Because recently developed techniques for automated test case generation can generate more tests than are practical to regularly execute, one important challenge is test suite reduction . Test suite reduction seeks to decrease the size of a test suite without overly compromising its original fault detection ability. This research advances the state-of-the-art in test suite reduction by empirically studying a coverage criterion which considers the context in which program concepts are covered. Conventional approaches to test suite reduction were developed and evaluated on batch-style applications and, due to the aforementioned considerations, are not always easily applicable to modern software. Furthermore, many existing techniques fail to consider the context in which code executes inside an event-driven paradigm, where programs wait for and interactively respond to user- and system-generated events. Consequently, they yield reduced test suites with severely impaired fault detection ability. The novel feature of this research is a test suite reduction technique based on the call stack coverage criterion which addresses many of the challenges associated with coverage-based test suite reduction in modern applications. Results show that reducing test suites while maintaining call stack coverage yields good tradeoffs between size reduction and fault detection effectiveness compared to traditional techniques. The output of this research includes models, metrics, algorithms, and techniques based upon this approach.	test suite	Scott McMaster	2008			test data generation;model-based testing;simulation;computer science;software engineering;code coverage;test case	EDA	-59.47290374580296	37.10888517659305	133074
6442b17d3c74ffed5bee1672e7a7a52794e8fc44	new tools for aspect-oriented programming in music and media programming environments		Media/arts programming is often experimental and exploratory in nature and requires a flexible development environment to enable continually changing requirements and to facilitate iterative design in which the development of software impacts the design of a work of art which in turn produces new requirements for the software. We discuss agile development as it relates to media/arts programming and present aspect-oriented programming and its implementation in Max/MSP using Open Sound Control and the odot library as tool for mobilizing the benefits of agile development.	agile software development;aspect-oriented programming;iteration;iterative design;max;requirement	John MacCallum;Adrian Freed;David Wessel	2014			simulation;extreme programming;human–computer interaction;extreme programming practices;agile usability engineering;reactive programming;functional reactive programming;systems engineering;engineering;software development;extensible programming;requirement;computer programming;inductive programming;software development process	HCI	-52.626092447943655	33.48579123282909	133255
7f41e459a1853294ff5a3ebc62717ba68e38467b	experimental assessment of software metrics using automated refactoring	software metrics;refactoring;info eu repo semantics conferenceobject;real world java systems experimental assessment software metrics automated refactoring experimental technique search based refactoring cohesion metrics;software maintenance;technology;search based software engineering;software engineering;science technology;cohesion;refactoring software metrics search based software engineering;design;software metrics software educational institutions abstracts java;search problems;computer science;software metrics java search problems software maintenance;java	A large number of software metrics have been proposed in the literature, but there is little understanding of how these metrics relate to one another. We propose a novel experimental technique, based on search-based refactoring, to assess software metrics and to explore relationships between them. Our goal is not to improve the program being refactored, but to assess the software metrics that guide the auto- mated refactoring through repeated refactoring experiments.  We apply our approach to five popular cohesion metrics using eight real-world Java systems, involving 300,000 lines of code and over 3,000 refactorings. Our results demonstrate that cohesion metrics disagree with each other in 55% of cases, and show how our approach can be used to reveal novel and surprising insights into the software metrics under investigation.	code refactoring;experiment;java;software metric;source lines of code	Mel Ó Cinnéide;Laurence Tratt;Mark Harman;Steve Counsell;Iman Hemati Moghadam	2012	Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement	10.1145/2372251.2372260	design;computer science;systems engineering;engineering;cohesion;software engineering;programming language;software maintenance;java;code refactoring;technology	SE	-59.13632388270793	34.78255181896343	133355
694511d65989b841ba8a62d1a84ef8d84e3a7782	safe and automated state transfer for secure and reliable live update	resource manager;mechanism design;reliability;software reliability;resource management;security;configuration management	Traditional live update systems offer little or no automated support for state transfer between two different program versions with changes in the program state. In this paper, we report our efforts to build a safe and automated state transfer framework for C programs that requires a minimal number of program state annotations and handles common structural state changes with no programmer assistance. To handle more complex state transformations, the framework includes a number of extension mechanisms designed to minimize the overall programming effort. Our experience with real-world programs suggests that our framework can handle all the standard C idioms and support safe and automated state transfer for complex state changes. We believe our approach is effective in several update scenarios and significantly raises the bar on the security and reliability of live update.	ansi c;pointer (computer programming);programmer;programming idiom;programming model;run time (program lifecycle phase);state (computer science);state management	Cristiano Giuffrida;Andrew S. Tanenbaum	2012	2012 4th International Workshop on Hot Topics in Software Upgrades (HotSWUp)		reliability engineering;computer science;database;computer security	SE	-56.11256764632337	40.6941969931243	134294
315ef13b7185d340d17d7e478a4ad31ca42231e5	diagnosing new faults using mutants and prior faults (nier track)	software;crash failure;program diagnostics;mutants decision tree execution traces faulty function;decision tree;measurement;faulty code;crashing failures;nier track;testing;execution traces;usa councils;mutants;program code;error correction;fault diagnosis decision trees software flexible printed circuits measurement testing usa councils;source code;decision trees;unix program diagnostics;unix utilities;faulty function;unix;fault diagnosis nier track program code faulty code crashing failures source code unix utilities;fault diagnosis;flexible printed circuits	"""Literature indicates that 20% of a program's code is responsible for 80% of the faults, and 50-90% of the field failures are rediscoveries of previous faults. Despite this, identification of faulty code can consume 30-40% time of error correction. Previous fault-discovery techniques focusing on field failures either require many pass-fail traces, discover only crashing failures, or identify faulty """"files"""" (which are of large granularity) as origin of the source code. In our earlier work (the F007 approach), we identify faulty """"functions"""" (which are of small granularity) in a field trace by using earlier resolved traces of the same release, which limits it to the known faulty functions. This paper overcomes this limitation by proposing a new """"strategy"""" to identify new and old faulty functions using F007. This strategy uses failed traces of mutants (artificial faults) and failed traces of prior releases to identify faulty functions in the traces of succeeding release. Our results on two UNIX utilities (i.e., Flex and Gzip) show that faulty functions in the traces of the majority (60-85%) of failures of a new software release can be identified by reviewing only 20% of the code. If compared against prior techniques then this is a notable improvement in terms of contextual knowledge required and accuracy in the discovery of finer-grain fault origin."""	error detection and correction;netbsd gzip / freebsd gzip;software release life cycle;tracing (software);unix	Syed Shariyar Murtaza;Nazim H. Madhavji;Mechelle Gittens;Zude Li	2011	2011 33rd International Conference on Software Engineering (ICSE)	10.1145/1985793.1985959	embedded system;real-time computing;computer science;operating system;decision tree;distributed computing	SE	-62.768130437656666	40.77827787445467	134445
0f95f498d2e3cb72e4ee80b186e8f0385675019d	effect of test set minimization on fault detection effectiveness	probability density function;code coverage;testing;testing fault detection software engineering;data mining;software engineering;fault detection	Size and code coverage are important attributes of a set of tests. When a program P is executed on elements of the test set T, we can observe the fault detecting capability of T for P. We can also observe the degree to which T induces code coverage on P according to some coverage criterion. We would like to know whether it is the size of T or the coverage of T on P which determines the fault detection effectiveness of T for P. To address this issue we ask the following question: While keeping coverage constant, what is the effect on fault detection of reducing the size of a test set? We report results from an empirical study using the block and all-uses criteria as the coverage measures.	code coverage;fault detection and isolation;sensor;test set	W. Eric Wong;Joseph Robert Horgan;Saul London;Aditya P. Mathur	1995	1995 17th International Conference on Software Engineering	10.1145/225014.225018	modified condition/decision coverage;reliability engineering;probability density function;real-time computing;fault coverage;computer science;engineering;stuck-at fault;software engineering;data mining;software testing;code coverage;fault detection and isolation	SE	-60.907917863072655	35.22490500486354	134506
966657b09f6f103d5bda9ae4af9e91798e827ed0	a taxonomy of code changes occurring within a statement or a signature		We propose a taxonomy of code changes at a granularity finer than the statement level. It classifies changes that occur within a statement or signature. We firstly define the changes based on the proposed operations on the tree of the statement or signature. Then, we classify the changes according to action type, entity kind, element kind and pattern kind. Based on the current implementation, we applied it to four open Java projects. Through the case study, we validated that the taxonomy can classify changes in all the modified statements and signatures. And, we checked the proportions of change patterns. Furthermore, we demonstrated that it is easy to find out the rename-induced statement updates with the help of the taxonomy. As a result, this taxonomy can be used for further change understanding and change analysis.		Chunhua Yang;E. James Whitehead	2018	2018 International Symposium on Theoretical Aspects of Software Engineering (TASE)	10.1109/TASE.2018.00020	change patterns;information retrieval;theoretical computer science;computer science;granularity;software evolution;java	SE	-56.90442122136077	35.87426635748789	134773
e2b162acaa90c750a340ae3c457bec8120aa5ed0	exception handling: a field study in java and .net	error recovery;programming language;error handling;exception handling;field study;programming languages;exception handling mechanisms	Most modern programming languages rely on exceptions for dealing with abnormal situations. Although exception handling was a significant improvement over other mechanisms like checking return codes, it is far from perfect. In fact, it can be argued that this mechanism is seriously limited, if not, flawed. This paper aims to contribute to the discussion by providing quantitative measures on how programmers are currently using exception handling. We examined 32 different applications, both for Java and .NET. The major conclusion for this work is that exceptions are not being correctly used as an error recovery mechanism. Exception handlers are not specialized enough for allowing recovery and, typically, programmers just do one of the following actions: logging, user notification and application termination. To our knowledge, this is the most comprehensive study done on exception handling to date, providing a quantitative measure useful for guiding the development of new error handling mechanisms.	.net framework;code;exception handling;field research;java;programmer;programming language	Bruno Cabral;Paulo Marques	2007		10.1007/978-3-540-73589-2_8	exception handling;real-time computing;computer science;database;programming language;field research	SE	-55.27035335067555	39.629508595706135	134839
0ba69414c2d870d7c25d3701b0505b7237174e93	evolution of prepaid payment processor's software architecture: an empirical study	empirical study;authorisation;electronic money;computer architecture program processors business databases accuracy servers;software architecture authorisation electronic money;qos requirements architectural evolution domain specific software architecture empirical study prepaid payment processor;domain specific software architecture;prepaid payment processor;software architecture;qos requirements;architectural evolution;quality of service prepaid payment processor software architecture prepaid card point of sale machine pos machine automated teller machine atm authorization merchants acquirers branded network transaction processing	Prepaid cards are the payment option for consumers who want to use an electronic means of payment but do not want to tie up the payment with a credit or debit account. When a prepaid card transaction is initiated using a Point of Sale (POS) machine or an Automated Teller Machine (ATM), it travels through multiple entities for authorization. These entities include merchants, acquirers, branded networks, and payment processors. Each of the entities has its own software solution for processing its part of the transaction. In this paper we present an empirical study of the evolution of a payment processor's software architecture. We first describe a basic architecture which acts as a baseline for further evolution. Results of transaction processing on this baseline architecture are discussed to highlight different quality of service issues. This architecture is gradually evolved into subsequent architectures resolving the encountered issues.	atm turbo;acquiring bank;authorization;baseline (configuration management);central processing unit;entity;evolution;point of sale;quality of service;scalability;software architecture;transaction processing	Abdul Haleem Qureshi;Ali Afzal Malik	2012	2012 10th International Conference on Frontiers of Information Technology	10.1109/FIT.2012.41	embedded system;payment service provider;operating system;business;computer security	SE	-53.77005402424726	44.29834272233384	134923
55de44165947d4a5a8256e80b896f185405e49f3	overfitting in semantics-based automated program repair	automated program repair;program synthesis;symbolic execution;patch overfitting	The primary goal of Automated Program Repair (APR) is to automatically fix buggy software, to reduce the manual bug-fix burden that presently rests on human developers. Existing APR techniques can be generally divided into two families: semantics- vs. heuristics-based. Semantics-based APR uses symbolic execution and test suites to extract semantic constraints, and uses program synthesis to synthesize repairs that satisfy the extracted constraints. Heuristic-based APR generates large populations of repair candidates via source manipulation, and searches for the best among them. Both families largely rely on a primary assumption that a program is correctly patched if the generated patch leads the program to pass all provided test cases. Patch correctness is thus an especially pressing concern. A repair technique may generate overfitting patches, which lead a program to pass all existing test cases, but fails to generalize beyond them. In this work, we revisit the overfitting problem with a focus on semantics-based APR techniques, complementing previous studies of the overfitting problem in heuristics-based APR. We perform our study using IntroClass and Codeflaws benchmarks, two datasets well-suited for assessing repair quality, to systematically characterize and understand the nature of overfitting in semantics-based APR. We find that similar to heuristics-based APR, overfitting also occurs in semantics-based APR in various different ways.	arp spoofing;apache portable runtime;correctness (computer science);failure;heuristic (computer science);machine learning;overfitting;population;program synthesis;satisfiability modulo theories;scalability;software bug;symbolic execution;test case;test suite	Xuan Bach Le;Ferdian Thung;David Lo;Claire Le Goues	2017	Empirical Software Engineering	10.1007/s10664-017-9577-2	data mining;overfitting;correctness;test case;program synthesis;heuristics;heuristic;software;computer science;symbolic execution	SE	-59.64905184138814	38.00229791986254	135116
168b23ba6551866c77885ca2b6e9ea1992ece61e	propane: an environment for examining the propagation of errors in software	publikationer;error propagation analysis;software systems;konferensbidrag;system under test;error propagation;software development;artiklar;software component;rapporter;software development tools;source code;error detection;fault injection;software reliability;embedded software	In order to produce reliable software, it is important to have knowledge on how faults and errors may affect the software. In particular, designing efficient error detection mechanisms requires not only knowledge on which types of errors to detect but also the effect these errors may have on the software as well as how they propagate through the software. This paper presents the Propagation Analysis Environment (PROPANE) which is a tool for profiling and conducting fault injection experiments on software running on desktop computers. PROPANE supports the injection of both software faults (by mutation of source code) and data errors (by manipulating variable and memory contents). PROPANE supports various error types out-of-the-box and has support for user-defined error types. For logging, probes are provided for charting the values of variables and memory areas as well as for registering events during execution of the system under test. PROPANE has a flexible design making it useful for development of a wide range of software systems, e.g., embedded software, generic software components, or user-level desktop applications. We show examples of results obtained using PROPANE and how these can guide software developers to where software error detection and recovery could increase the reliability of the software system.	chart;component-based software engineering;desktop computer;embedded software;embedded system;error detection and correction;experiment;fault injection;out of the box (feature);software bug;software developer;software propagation;software system;system under test;user space	Martin Hiller;Arshad Jhumka;Neeraj Suri	2002		10.1145/566172.566184	real-time computing;error detection and correction;software sizing;embedded software;computer science;software reliability testing;propagation of uncertainty;software framework;component-based software engineering;software development;software design description;operating system;software engineering;software construction;distributed computing;system under test;programming language;software quality;software metric;software system;source code;avionics software	SE	-57.20663699119666	36.959757606496076	135193
188209a52d5ea42439f6955b747cb4a25b32dbfd	input-based adaptive randomized test case prioritization: a local beam search approach	adaptive test case prioritization;regression testing;randomized algorithm	Test case prioritization assigns the execution priorities of the test cases in a given test suite. Many existing test case prioritization techniques assume the fullfledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in realworld software development projects. This paper proposes a novel family of input-based local-beam-search adaptiverandomized techniques. They make adaptive tree-based randomized explorations with a randomized candidate test set strategy to even out the search space explorations among the branches of the exploration trees constructed by the test inputs in the test suite. We report a validation experiment on a suite of four medium-size benchmarks. The results show that our techniques achieve either higher APFD values than or the same mean APFD values as the existing code-coverage-based greedy or search-based prioritization techniques, including Genetic, Greedy and ART, in both our controlled experiment and case study. Our techniques are also significantly more efficient than the Genetic and Greedy, but are less efficient than ART. Keywords-regression testing, adaptive test case prioritization, randomized algorithm, empricial comparsion	beam search;benchmark (computing);code coverage;coverage data;genetic algorithm;greedy algorithm;randomized algorithm;regression testing;scheduling (computing);software development;test case;test set;test suite	Bo Jiang;Wing Kwong Chan	2015	Journal of Systems and Software	10.1016/j.jss.2015.03.066	reliability engineering;greedy randomized adaptive search procedure;regression testing;simulation;computer science;data mining;randomized algorithm	SE	-60.57631743832645	35.296903801198745	135343
b70cb5047259928cb333cd1ec12e72b372c98569	software input space modeling with constraints among parameters	directed graphs;software;software input space modeling;software testing;parameter constraint;software quality software testing computer science application software space technology usa councils laboratories statistical analysis computer applications educational institutions;application software;graph path;testing;usa councils;dead node elimination;data mining;statistical testing software input space modeling software test case generation parameter constraint directed graph model graph path subgraph splitting dead node elimination;computer applications;input space;computational modeling;statistical analysis;program testing;subgraph splitting;directed graph;directed graph model;dependencies;space technology;statistical testing;markov processes;computer science;statistical testing directed graphs program testing;software test case generation;constraints;software quality;dependencies software testing input space constraints	This paper considers the task of software test case generation from a large space of values of input parameters. The purpose of the paper is to create a model of software input space with constraints among parameters to serve as a basis of testing. We suggest a procedure to create a directed graph model, where paths through the graph represent all valid (and only valid) input combinations. The procedure accommodates an arbitrary set of dependencies among parameters. It starts from a simple linear graph and sequentially modifies this graph for each dependency between parameters. Modifications include subgraph splitting and elimination of dead nodes and edges. A complete example of a system with six input parameters and five dependencies among them is presented to illustrate the application of the procedure. Applicability of the approach for different types of parameters and dependencies is addressed.	directed graph;graphical model;markov chain;software testing;test case	Sergiy A. Vilkomir;W. Thomas Swain;Jesse H. Poore	2009	2009 33rd Annual IEEE International Computer Software and Applications Conference	10.1109/COMPSAC.2009.27	dependency graph;directed graph;computer science;theoretical computer science;software engineering;machine learning;database;software testing;programming language;statistics	Visualization	-54.4613694910478	32.641737769030364	135455
c8d325b066173bdae8a8b8686abf1aa6295e1d60	a strategy to determine when to stop using automatic bug localization	fault localization;buglocator automatic bug localization information retrieval source code entity expected time cost etc;bug localization;computer bugs measurement information retrieval probability switches software computer science;information retrieval;evaluation metric bug localization information retrieval bug reports fault localization feature location;evaluation metric;bug reports;source code software information retrieval program debugging;feature location	Information retrieval based automatic bug localization techniques provide developers a ranked list of suspicious buggy source entities to aid locate the ones needed to be modified and to fix the bug. However, it is unavoidable that some buggy entities are ranked low in the result list using these automatic techniques. We assume a bug localization process to address this challenge. Each time a source code entity in the ranked list is examined, the developers will have the option as to whether to continue examining the automatic bug localization result, or simply switch to using a conventional localization approach. We propose a new evaluation metric called ETC (Expected Time Cost) in the localization process, which includes the time cost of using the conventional approach. Under our assumptions, we derived simple criteria to minimize ETC. We compared the time cost of a state-of-art automatic localization method, BugLocator, with and without using our strategy in two projects. The result shows that using our proposed strategy combining both automatic localization technique together with conventional approach performs better than using only either the automatic localization technique or the conventional approach.	average-case complexity;entity;information retrieval;internationalization and localization;software bug	Zhendong Shi;Jacky W. Keung;Kwabena Ebo Bennin;Nachai Limsettho;Qinbao Song	2016	2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2016.39	computer science;theoretical computer science;data mining;information retrieval	SE	-60.17352452762983	38.44772130885637	135525
fd498ee0e493b94965b297f2561994e21749b1fc	towards reconstructing architectural models of software tools by runtime analysis	software engineering	We present a method and initial results on reverse engineering the architecture of monolithic software systems. Our approach is based on analysis of system binaries resulting in a series of models, which are successively refined into a component structure. Our approach comprises the following steps: 1) instrumentation of existing binaries for dynamically generating execution traces at runtime and connected analysis, 2) static inspection of binaries, 3) interpretation using domain knowledge, and 4) identifying component boundaries using software clustering. We motivate a generic method which covers a large class of software systems, and evaluate our method on concrete software tools for industrial automation systems development, focusing on Intel x86 and Microsoft Windows-compatible applications.	analysis of algorithms;automation;binary file;cluster analysis;microsoft windows;reverse engineering;run time (program lifecycle phase);software development process;software system;tracing (software);x86	Ian Peake;Jan Olaf Blech;Lasith Fernando	2013			automation;software system;architecture;x86;cluster analysis;domain knowledge;reverse engineering;computer architecture;software;computer science	SE	-56.242925585688226	35.32033235411469	135598
f8145974e211c5a3df6f392c4b8ab2fda800efe2	test input reduction for result inspection to facilitate fault localization	fault localization;debugging;test suite reduction;testing;test generation;test collection	Testing-based fault-localization (TBFL) approaches often require the availability of high-statement-coverage test suites that sufficiently exercise the areas around the faults. However, in practice, fault localization often starts with a test suite whose quality may not be sufficient to apply TBFL approaches. Recent capture/replay or traditional test-generation tools can be used to acquire a high-statement-coverage test collection (i.e., test inputs only) without expected outputs. But it is expensive or even infeasible for developers to manually inspect the results of so many test inputs. To enable practical application of TBFL approaches, we propose three strategies to reduce the test inputs in an existing test collection for result inspection. These three strategies are based on the execution traces of test runs using the test inputs. With the three strategies, developers can select only a representative subset of the test inputs for result inspection and fault localization. We implemented and applied the three test-input-reduction strategies to a series of benchmarks: the Siemens programs, DC, and TCC. The experimental results show that our approach can help developers inspect the results of a smaller subset (less than 10%) of test inputs, whose fault-localization effectiveness is close to that of the whole test collection.	code coverage;ibm notes;test suite;theory of cryptography conference;tracing (software);x image extension	Dan Hao;Tao Xie;Lu Zhang;Xiaoyin Wang;Jiasu Sun;Hong Mei	2009	Automated Software Engineering	10.1007/s10515-009-0056-x	reliability engineering;embedded system;real-time computing;fault coverage;computer science;engineering;automatic test pattern generation;software engineering;test compression;test suite;software testing;programming language;debugging;test case;test management approach	SE	-60.322071050267475	36.34573182203134	135658
0b5c93e5cd886b1b59b05bc87cf9063539987785	a survey of code-based change impact analysis techniques	journal;source code;change impact analysis;application;survey;journal magazine article	Software change impact analysis (CIA) is a technique for identifying the effects of a change, or estimating what needs to be modified to accomplish a change. Since the 1980s, there have been many investigations on CIA, especially for code-based CIA techniques. However, there have been very few surveys on this topic. This article tries to fill this gap. And 30 papers that provide empirical evaluation on 23 code-based CIA techniques are identified. Then, data was synthesized against four research questions. The study presents a comparative framework including seven properties, which characterize the CIA techniques, and identifies key applications of CIA techniques in software maintenance. In addition, the need for further research is also presented in the following areas: evaluating existing CIA techniques and proposing new CIA techniques under the proposed framework, developing more mature tools to support CIA, comparing current CIA techniques empirically with unified metrics and common benchmarks, and applying the CIA more extensively and effectively in the software maintenance phase. Copyright © 2012 John Wiley & Sons, Ltd.	academy;benchmark (computing);computer science;debugging;dependence analysis;intermediate representation;john d. wiley;regression testing;software evolution;software maintenance;software propagation;software repository;software system;test case	Bixin Li;Xiaobing Sun;Hareton K. N. Leung;Sai Zhang	2013	Softw. Test., Verif. Reliab.	10.1002/stvr.1475	computer science;engineering;software engineering;data mining;programming language;operations research;change impact analysis;source code	SE	-57.66690469561268	34.67059561027384	135805
b691936342e0e904b15e5511087271491f569590	is text search an effective approach for fault localization: a practitioners perspective	empirical study;bug solving	There has been widespread interest in both academia and industry around techniques to help in fault localization. Much of this work leverages static or dynamic code analysis and hence is constrained by the programming language used or presence of test cases. In order to provide more generically applicable techniques, recent work has focused on devising text search based approaches that recommend source files which a developer can modify to fix a bug. Text search may be used for fault localization in either of the following ways. We can search a repository of past bugs with the bug description to find similar bugs and recommend the source files that were modified to fix those bugs. Alternately, we can directly search the code repository to find source files that share words with the bug report text. Few interesting questions come to mind when we consider applying these text-based search techniques in real projects. For example, would searching on past fixed bugs yield better results than searching on code? What is the accuracy one can expect? Would giving preference to code words in the bug report better the search results? In this paper, we apply variants of text-search on four open source projects and compare the impact of different design considerations on search efficacy.	bug tracking system;code word;dynamic program analysis;expect;mind;open-source software;preprocessor;programming language;repository (version control);result set;software bug;static program analysis;test case;text-based (computing)	Vibha Sinha;Senthil Mani;Debdoot Mukherjee	2012		10.1145/2384716.2384770	computer science;theoretical computer science;operating system;data mining;empirical research;world wide web	SE	-59.996713237833994	38.97580252693684	136049
4765884d784fde01c9c821e26172f807f8a47c73	a new fault localizing method for the program debugging process	fault localization;dynamic slicing;program debugging;program slicing;inventory control	Abstract   A large amount of effort is spent on fault localization in the program debugging process. So, it is necessary to develop effective fault-localizing methods. Dynamic slicing is a technique for narrowing down where a fault is likely to exist. However, dynamic slicing requires a large amount of run time due to the tracing information that is collected during the program's execution. Furthermore, the extracted slice may, in the worst case, be the whole program itself. This paper presents a new fault localizing method that combines dynamic slicing with forward tracking techniques. The proposed method differs from previous slicing-based fault localization techniques in that it utilizes information from the design specification during forward tracking, and thus can reduce the size of the dynamic slice constructed. This paper applies this new method to an inventory control program at a wine shop and discusses its effectiveness by comparison with an existing method.	debugging	Lin Lian;Shinji Kusumoto;Tohru Kikuno;Ken-ichi Matsumoto;Koji Torii	1997	Information & Software Technology	10.1016/S0950-5849(96)01149-4	inventory control;embedded system;program slicing;real-time computing;computer science;software engineering;programming language	SE	-60.045613652876575	37.194172392136196	136155
b902cfcb78cf48426ebb99f1ffd0edec91504d9f	a search engine for finding and reusing architecturally significant code	ibrahim jameel;computer science a search engine for finding and reusing architecturally significant code rochester institute of technology mehdi mirakhorli mujhid;model;traceability;architecture;tactics;information	A code search engine to automate the discovery, extraction and indexing of tactics.A big data compatible architecture to search through 22 million source files.Novel techniques to detect tactics and technical context in which they are used.Introducing a novel ranking algorithm to order the retrieved tactical files.Enhancing the state-of-the-art code search engines in finding tactical-code. Architectural tactics are the building blocks of software architecture. They describe solutions for addressing specific quality concerns, and are prevalent across many software systems. Once a decision is made to utilize a tactic, the developer must generate a concrete plan for writing code and implementing the tactic. Unfortunately, this is a non-trivial task even for experienced developers. Often, developers resort to using search engines, crowd-sourcing websites, or discussion forums to find sample code snippets to implement a tactic. A fundamental problem of finding implementation for architectural tactics/patterns is the mismatch between the high-level intent reflected in the descriptions of these patterns and the low-level implementation details of them. To reduce this mismatch, we created a novel Tactic Search Engine called ArchEngine (ARCHitecture search ENGINE). ArchEngine can replace this manual internet-based search process and help developers find and reuse tactical code from a wide range of open source systems. ArchEngine helps developers find implementation examples of an architectural tactic for a given technical context. It uses information retrieval and program analysis techniques to retrieve applications that implement these design concepts. Furthermore, it lists and rank the code snippets where the patterns/tactics are located. Our case study with 21 graduate students (with experience level of junior software developers) shows that ArchEngine is more effective than other search engines (e.g., Krugle and Koders) in helping programmers to quickly find implementations of architectural tactics/patterns.	crowdsourcing;high- and low-level;information retrieval;internet;krugle;open-source software;program analysis;programmer;software architecture;software developer;software system;web search engine	Ibrahim Mujhid;Joanna C. S. Santos;Raghuram Gopalakrishnan;Mehdi Mirakhorli	2017	Journal of Systems and Software	10.1016/j.jss.2016.11.034	software system;architecture;implementation;computer science;big data;search engine indexing;search engine;software;data mining;software architecture	SE	-59.80365461409682	39.721854330103895	136187
bbbc8410685821766e2227ae2cb660f9a68d76d7	runtime model checking for sla compliance monitoring and qos prediction		Sophisticated workflows, where multiple parties cooperate towards the achievement of a shared goal are today common. In a market-oriented setup, it is key that effective mechanisms be available for providing accountability within the business process. The challenge is to be able to continuously monitor the progress of the business process, ideally,anticipating contract breaches and triggering corrective actions. In this paper we propose a novel QoS prediction approach which combines runtime monitoring of the real system with probabilistic model-checking on a parametric system model. To cope with the huge amount of data generated by the monitored system, while ensuring that parameters are extracted in a timing fashion, we relied on big data analytics solutions. To validate the proposed approach, a prototype of the QoS prediction framework has been developed, and an experimental campaign has been conducted with respect to a case study in the field of Smart Grids.	big data;business process;design by contract;experiment;model checking;prism (surveillance program);prism model checker;prototype;quality of service;service-level agreement;statistical model	Giuseppe Cicotti;Luigi Coppolino;Salvatore D'Antonio;Luigi Romano	2015	JoWUA		quality of service;model checking;compliance monitoring;reliability engineering;computer science	AI	-60.05336599187562	45.87843096876144	136455
76f309b21449bfb3288ba4fe447549d3ef467136	reconstructing higher level change information from versioning data	application software;software maintenance;software maintenance configuration management;software systems;versioning systems;data mining;higher level change information;versioning systems higher level change information versioning data software development cycle;visualization;software development;change process;visualization software systems programming data mining software maintenance application software;software development cycle;programming;configuration management;versioning data	Change is an essential element of the software development cycle, certainly since systems which do not change, perish. Despite the importance of changes, most evolution studies evaluate the impact of changes by comparing two or more snapshots of a system over time instead of reconstructing and evaluating the actual change operations. Therefore we propose a number of techniques which are capable of reconstructing change-operations from the low-level data stored in versioning systems in order to learn more about the current system as well as the change process. More specifically, we show how these techniques can be used to study the use of move-operations	high- and low-level;software development process;version control	Filip Van Rysselberghe	2006	Conference on Software Maintenance and Reengineering (CSMR'06)	10.1109/CSMR.2006.44	programming;application software;real-time computing;visualization;computer science;systems engineering;engineering;software development;software engineering;database;configuration management;software maintenance;software development process;software system	SE	-56.97852475063776	34.45412646339668	136880
3a65f0f102c49955494b1c47fd26253bcc57cd8b	generating test cases to expose concurrency bugs in android applications	instruments;data race;androids;concurrent computing;record replay;android;testing;humanoid robots;message systems;computer bugs;mobile application;data models	Mobile systems usually support an event-based model of concurrent programming. This model, although advantageous to maintain responsive user interfaces, may lead to subtle concurrency errors due to unforeseen threads interleaving coupled with non-deterministic reordering of asynchronous events. These bugs are very difficult to reproduce even by the same user action sequences that trigger them, due to the undetermined schedules of underlying events and threads. In this paper, we proposed RacerDroid, a novel technique that aims to expose concurrency bugs in android applications by actively controlling event schedule and thread interleaving, given the test cases that have potential data races. By exploring the state model of the application constructed dynamically, our technique starts first to generate a test case that has potential data races based on the results obtained from existing static or dynamic race detection technique. Then it reschedules test cases execution by actively controlling event dispatching and thread interleaving to determine whether such potential races really lead to thrown exceptions or assertion violations. Our preliminary experiments show that RacerDroid is effective, and it confirms real data races, while at the same time eliminates false warnings for Android apps found in the wild.	android;assertion (software development);concurrency (computer science);concurrent computing;experiment;forward error correction;schedule (computer science);software bug;test case;user experience;user interface	Hongyin Tang;Guoquan Wu;Jun Wei;Hua Zhong	2016	2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1145/2970276.2970320	embedded system;real-time computing;concurrent computing;computer science;distributed computing;programming language;android	SE	-56.13852342798936	38.80152901620129	137238
4f2a40b1a0cb899d28ebfdb4d10011fc982836ba	physical separation of features: a survey with cpp developers		Several implementation techniques for software product lines have emerged over time. A common distinction of these techniques is whether features are annotated in the code base (virtually separated) or composed from modules (physically separated). While each approach promises different pros and cons, mainly annotations and especially the C PreProcessor (CPP) are established in practice. Thus, the question arises, which barriers prevent the adoption of composition-based approaches. In this paper, we report an empirical study among C and C++ developers in which we investigate this issue. Therefore, we ask our participants to describe how they use the CPP and how they assess the idea of moving annotated code into modules. More precisely, we use small examples based on our Feature Compositional PreProcessor (FeatureCoPP) that enables this separation while keeping annotations---avoiding divergences from the preprocessor concept. Overall, we identify different characteristics that indicate when physical separation can be useful. While most responses are skeptical towards the approach, they also emphasize its usability for source code analysis and for implementing specific use cases.	c preprocessor;c++;cohesion (computer science);compiler;complexity;consistent pricing process;route inspection problem;software product line;static program analysis;usability	Jacob Krüger;Kai Ludwig;Bernhard Zimmermann;Thomas Leich	2018		10.1145/3167132.3167351	separation of concerns;empirical research;data mining;source code;usability;use case;software;preprocessor;computer science;cons	SE	-54.191543846023535	39.50597648470749	137326
a7a82618d7cf6909a291dc1d4930ea7dd3562bf3	visual analytics of software structure and metrics	analytical models;software;software measurement;software maintenance;software data visualization analytical models object oriented modeling visualization software measurement;visual queries;software measurement visual analytics software structure software metrics software maintenance software comprehension software engineering software visualization interactive visual workflow modeling approach standard software analysis tools visual elements vimetrik visual specification of metrics;visualization;software maintenance visual analytics visual queries software measurement software comprehension;data visualization;software metrics data analysis data visualisation formal specification interactive systems software maintenance;visual analytics;object oriented modeling;software comprehension	In terms of software maintenance and comprehension, the fields of software engineering and software visualization have produced several methods and tools. However, they are typically separate tools in practice. In this paper, we present a novel methodology of combining software analysis and software visualization tools via an interactive visual workflow modeling approach. Standard software analysis tools are also limited in that they support only well-known metrics or are too complicated to use for generating custom software metrics. To address these shortcomings, our approach focuses on visual elements, their configurations, and interconnectivity rather than a data ontology and querying language. In order to test and validate our methodology, we developed a prototype tool called VIMETRIK (Visual Specification of Metrics). Our preliminary evaluation study illustrates the intuitiveness and ease-of-use of our approach with regard to understanding software measurement and analysis data.	checkstyle;data mining;database model;dependability;findbugs;full scale;interconnectedness;pmd;prototype;software development;software engineering;software maintenance;software measurement;software metric;software system;software visualization;static program analysis;usability;visual analytics	Taimur Ahmed Khan;Henning Barthel;Achim Ebert;Peter Liggesmeyer	2015	2015 IEEE 3rd Working Conference on Software Visualization (VISSOFT)	10.1109/VISSOFT.2015.7332411	software visualization;personal software process;long-term support;verification and validation;visual analytics;visualization;software sizing;computer science;package development process;backporting;software design;social software engineering;component-based software engineering;software development;software engineering;software construction;database;software walkthrough;programming language;software analytics;software maintenance;software measurement;software deployment;data visualization;software quality;software system;software peer review	SE	-55.5005380182684	33.5290972762292	137530
2b8e9979385fb07abe651efb06a8a0089caa967d	mobile games to foster the learning of history at archaeological sites	formal specification;uml;unified modeling language context modeling contracts software safety programming profession rail to rail outputs computer industry software standards standards development design engineering;unified modeling language formal specification;null;formal constraints;unified modelling language uml;g600 software engineering;counter intuitive features constraint diagram language unified modelling language uml formal constraints software models textual object constraint language;unified modelling language;unified modeling language;counter intuitive features;textual object constraint language;object constraint language;software models;constraint diagram language	This paper presents a system designed to support young students learning history at an archaeological site, by exploiting mobile technology. The approach uses game-play, since it stimulates in young students an understanding of history that would otherwise be difficult to engender, helping players to acquire historical notions and making archaeological visits more effective and exciting. A strength of the system is that, by running on the visitors own cellular phones, it requires minimal investments and small changes to the existing site exhibition.	mobile phone	Carmelo Ardito;Paolo Buono;Maria Francesca Costabile;Rosa Lanzilotti;Thomas Pederson	2007	IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2007)	10.1109/VLHCC.2007.54	concurrent constraint logic programming;natural language processing;constraint programming;communication diagram;constraint satisfaction;computer science;theoretical computer science;applications of uml;class diagram;programming language;object constraint language	Mobile	-50.00718451894628	34.00341271151379	137618
81b869b02f8e1dfb0e1f5491ef0944937c1dc8a6	evosuite: on the challenges of test case generation in the real world	junit test suites evosuite test case generation java programming language real world usability sourceforge open source platform java classes real world open source projects;search based software engineering;public domain software;test case generation;program testing;search based testing;public domain software java program testing;java testing prototypes instruction sets security instruments;testing classes;java	Test case generation is an important but tedious task, such that researchers have devised many different prototypes that aim to automate it. As these are research prototypes, they are usually only evaluated on a few hand-selected case studies, such that despite great results there remains the question of usability in the “real world”. EVOSUITE is such a research prototype, which automatically generates unit test suites for classes written in the Java programming language. In our ongoing endeavour to achieve real-world usability, we recently passed the milestone success of applying EVOSUITE on hundred projects randomly selected from the SourceForge open source platform. This paper discusses the technical challenges that a testing tool like EVOSUITE needs to address when handling Java classes coming from real-world open source projects, and when producing JUnit test suites intended for real users.	code coverage;endeavour (supercomputer);evosuite;experiment;foremost;junit;java;open-source software;programming language;prototype;randomness;sourceforge;test automation;test case;unit testing;usability	Gordon Fraser;Andrea Arcuri	2013	2013 IEEE Sixth International Conference on Software Testing, Verification and Validation	10.1109/ICST.2013.51	search-based software engineering;computer science;operating system;software engineering;java modeling language;real time java;programming language;java;public domain software;java annotation	SE	-58.8243788014446	36.07914100019081	137839
35aa6711833ff6bac1828b4ec3a2769dadea7664	automated generation of rest api specification from plain html documentation		REST is nowadays highly popular and widely adopted by Web services providers. However, most of the Web services providers only provide the documentation of their REST API in plain HTML pages, even if many specification formats exist such as WADL or OpenAPI for example. This prevents the Web Services users to benefit from all the advantages of having a machine-readable specification, such as generating client or server code, generating web services composition, checking formal properties, testing, etc. To face this issue, we provide a fully automated approach that builds a REST API specification from its corresponding plain HTML documentation. By given the root URL of the plain HTML API documentation, our approach automatically extracts the four mandatory parts that compose a specification: the base URL, the path templates, the HTTP verbs and the associated formal parameters. Our approach has been validated with topmost commercial REST based Web Services, and the validation shows that our approach achieves good precision and recall for popular Web Services.	application programming interface;documentation;html	Hanyang Cao;Jean-Rémy Falleri;Xavier Blanc	2017		10.1007/978-3-319-69035-3_32	precision and recall;application programming interface;documentation;world wide web;database;web service;computer science	SE	-57.455346505769775	42.38754255186081	137865
08f2176d652f87f6492a6b0f72f0347c29a1347a	reachability and propagation for ltl requirements testing	automated testing;requirements testing;software;software testing;ordinary code failures;property testing;temporal logic;ordinary code faults;biological system modeling;requirements testing software testing property testing automated testing test case generation;testing;satisfiability;runtime;temporal logic program testing;testing techniques;linear temporal logic software testing testing techniques ordinary code failures ordinary code faults requirements testing;cost accounting;test case generation;program testing;linear temporal logic;mathematical model;user requirements;model based testing;requirements traceability;testing equations mathematical model cost accounting runtime biological system modeling software	It is important to test software with respect to user requirements, especially when adhering to safety standards, which require traceability from requirements to test cases. While research has resulted in many different model based testing techniques, only a few consider requirement properties; this paper helps fill this gap. We identify two fundamental characteristics of a test case intended to evaluate a given requirement property. The two characteristics are adapted from the venerable Reachability, Infection, and Propagation (RIP) model for faults and failures in ordinary code. In the context of requirements testing, we propose the reachability property amounts to the property not being vacuously true on a given test case, and the propagation property amounts to a potential violation of the property on the test case being observable. In particular, we formalize these notions in the context of requirement properties expressed in linear temporal logic (LTL), and not only show how to determine reachability and propagation for given test cases, but also how to modify test cases to satisfy these properties. To demonstrate the importance of reachability and propagation we evaluate test sets for a small real-life application generated according to previously published test criteria.	linear temporal logic;model checking;model-based testing;observable;reachability;real life;requirement;runtime verification;software propagation;test case;traceability;user requirements document	Gordon Fraser;Paul Ammann	2008	2008 The Eighth International Conference on Quality Software	10.1109/QSIC.2008.21	reliability engineering;real-time computing;computer science;software engineering;software testing;test case;algorithm	SE	-60.19045226580676	32.6408358334925	138002
5bdcf7a9f4bbc1c7dba474fb9c55c6dd1c788c4e	a goal model elaboration for localizing changes in software evolution	software metrics;software;formal specification;complexity theory;control loops;program control structures;software evolution;monitoring;batteries;robots;process control;elaboration techniques;batteries software process control cleaning robots monitoring complexity theory;code complexity prevention goal model elaboration technique software evolution change localization loosely coupled component extraction requirements descriptions control loop extraction software development source code change impact analysis;control loops software evolution goal modeling elaboration techniques;goal modeling;software metrics formal specification program control structures;cleaning	Software evolution is an essential activity that adapts existing software to changes in requirements. Localizing the impact of changes is one of the most efficient strategies for successful evolution. We exploit requirements descriptions in order to extract loosely coupled components and localize changes for evolution. We define a process of elaboration for the goal model that extracts a set of control loops from the requirements descriptions as components that constitute extensible systems. We regard control loops to be independent components that prevent the impact of a change from spreading outside them. To support the elaboration, we introduce two patterns: one to extract control loops from the goal model and another to detect possible conflicts between control loops. We experimentally evaluated our approach in two types of software development and the results demonstrate that our elaboration technique helps us to analyze the impact of changes in the source code and prevent the complexity of the code from increasing.	control flow;experiment;loose coupling;requirement;software development;software evolution	Hiroyuki Nakagawa;Akihiko Ohsuga;Shinichi Honiden	2013	2013 21st IEEE International Requirements Engineering Conference (RE)	10.1109/RE.2013.6636715	robot;reliability engineering;real-time computing;goal modeling;computer science;systems engineering;engineering;software evolution;software engineering;process control;formal specification;software metric	SE	-53.045215010227885	32.55913209924309	138473
18450f539b1f3cedcd906d0ce2335c48b2dc880e	program slicing tool for effective software evolution using aspect-oriented technique	new technology;java programming;program execution software evolution program debugging program slicing tool dynamic slicing fault location procedural program object oriented program java program aspect oriented programming crosscutting aspect program analysis dynamic information;separation of concern;software systems;object oriented programming;dynamic information;fault diagnosis program debugging program slicing software tools java object oriented programming;col;software evolution;dynamic slicing;aspect oriented programming;cost effectiveness;crosscutting concerns;aspect oriented;software tools;program analysis;program debugging;program slicing;software tools java software debugging software systems fault diagnosis information analysis data analysis information science application software usability;dynamic analysis;fault diagnosis;java language;java	One of the issues in software evolution is debugging. Debugging large and complex software systems evolved requires a lot of effort since it is very difficult to localize and identify faults. Therefore, reducing the effort of debugging process is an important step towards efficient software evolution. Program slicing, especially dynamic slicing, has been proposed to efficiently localize faults in a procedural program and an object-oriented program. Although several tools have been developed for Java programs, these are difficult to maintain because of the frequent revision of Java languages. Aspect-Oriented Programming (AOP) is a new technology for the separation of concerns in program development. Using AOP, modularizing crosscutting aspects of a system is possible. One useful application of AOP is for modularizing the collecting program’s dynamic information for program analysis. Since the collection of dynamic information affects the over-all target program, this functionality is a typical crosscutting concern. In this paper, we apply AOP to develop a program debugging tool using program slicing. First, we examine the application of AOP for collecting dynamic information from program execution and for calculating program slices. Next, we develop a program slicing system using AspectJ. Finally, we describe the benefits, usability, and cost effectiveness of a module of dynamic analysis based on AOP.	aspect weaver;aspect-oriented programming;aspectj;control flow;cross-cutting concern;data dependency;debugger;debugging;dependence analysis;dynamic data;dynamic program analysis;java;local variable;program slicing;separation of concerns;software evolution;software system;usability	Takashi Ishio;Shinji Kusumoto;Katsuro Inoue	2003		10.1109/IWPSE.2003.1231204	program analysis;program slicing;real-time computing;aspect-oriented programming;computer science;operating system;dynamic program analysis;algorithmic program debugging;programming language	SE	-55.84746786128289	36.1166869826562	139024
72fc1e75f62a56d44fa5d8d88e4e2106afb3370b	investigation of logistic regression as a discriminant of software quality	software metrics;software metrics software quality forecasting theory statistical analysis boolean functions inspection space vehicles safety critical software software cost estimation aerospace computing subroutines pattern classification;threshold identification method logistic regression functions boolean discriminant functions software quality prediction quality classification ability predictive accuracy inspection cost space shuttle module quality ranking classification accuracy safety critical systems;software cost estimation;cost function;boolean functions;testing;space shuttle;logistic regression;discriminant function;inspection;software quality prediction;threshold identification method;logistics software quality inspection space shuttles cost function safety time factors predictive models testing quality control;time factors;logistics;statistical analysis;forecasting theory;aerospace computing;safety critical software;safety;safety critical system;prediction accuracy;pattern classification;predictive models;inspection cost;classification accuracy;quality control;quality classification ability;critical value;space shuttles;software quality;module quality ranking;safety critical systems;space vehicles;logistic regression functions;subroutines;predictive accuracy;boolean discriminant functions	Investigates the possibility that logistic regression functions (LRFs), when used in combination with Boolean discriminant functions (BDFs), which we had previously developed, would improve the quality classification ability of BDFs when used alone; this was found to be the case. When the union of a BDF and LRF was used to classify quality, the predictive accuracy of quality and inspection cost was improved over that of using either function alone for the Space Shuttle. Also, the LRFs proved useful for ranking the quality of modules in a build. The significance of these results is that very high-quality classification accuracy (1.25% error) can be obtained while reducing the inspection cost incurred in achieving high quality. This is particularly important for safety-critical systems. Because the methods are general and not particular to the Shuttle, they could be applied to other domains. A key part of the LRF development was a method for identifying the critical value (i.e. threshold) that could discriminate between high and low quality, and at the same time constrain the cost of inspection to a reasonable value.	discriminant;logistic regression;software quality	Norman F. Schneidewind	2001		10.1109/METRIC.2001.915540	space shuttle;reliability engineering;computer science;engineering;software engineering;machine learning;data mining;statistics	SE	-62.35842861553241	33.83334260498648	139039
05faeb5e1fe29908b8f8fc24590c2b7ce6c87188	visual design and programming for web applications	visual design;satisfiability;web applications;program generation;visual programming;prototyping;image orientation;aspect oriented programming;software development;program generator;model development;information society	With the development of the information society, it has become necessary to release software early that satisfies users. Therefore, it has become important to develop the software quickly so that the users can try it, and give the developers feedback. Recently, instead of the conventional water-fall-model development, new development techniques such as aspectoriented programming have been researched. The software development techniques that make use of graphics have also been researched in a variety of fields. This paper presents the imageoriented programming method that uses graphics as a tool of designing software, and enables users to easily develop software according to their image of what they want to develop. It also describes the BioPro system that implements this method for Web applications. The BioPro system has the following features; (1) users can develop programs according to their image, (2) they can easily verify the completeness of components that make up the program and the consistency of those relationships, and (3) they can easily confirm what they have developed, regardless of which stage of development they are currently at. r 2004 Elsevier Ltd. All rights reserved.	graphics;program transformation;software development process;web application;web design;web page	Takao Shimomura	2005	J. Vis. Lang. Comput.	10.1016/j.jvlc.2004.08.005	web application;simulation;aspect-oriented programming;orientation;computer science;package development process;theoretical computer science;software framework;component-based software engineering;software development;operating system;software engineering;software construction;database;prototype;visual programming language;programming language;software development process;satisfiability	SE	-51.538987159207906	34.90389310588967	139046
5acc2fbf724441d92c2271f22ae1cd251f80667a	lessons learned from model checking a nasa robot controller	software verification;software systems;abstraction;model checking;component oriented software development;lessons learned;software development;executable design specifications;compositional reasoning	This paper reports as a case study an attempt to model check the control subsystem of an operational NASA robotics system. Thirty seven properties including both safety properties and liveness properties were formulated for the system. Twenty two of the thirty seven properties were successfully model checked. Several significant flaws in the original software system were identified and corrected during the model checking process. The case study presents the entire process in a semi-historical mode. The goal is to provide reusable knowledge of what worked, what did not work and why.	liveness;model checking;robot;robotics;semiconductor industry;software system	Natasha Sharygina;James C. Browne;Fei Xie;Robert P. Kurshan;Vladimir Levin	2004	Formal Methods in System Design	10.1023/B:FORM.0000040029.73127.85	model checking;personal software process;verification and validation;real-time computing;software verification;computer science;package development process;software development;software engineering;software construction;abstraction;programming language;software system	AI	-48.768928504541016	32.578630871186114	139127
9bafbc7f5f57b84361a08dc03c8be8dcdb7b5b26	mysql extension automatic porting to pdo for php migration and security improvement		In software management, the upgrade of programming languages may introduce critical issues. This is the case of PHP, the fifth version of which is going towards the end of the support. The new release improves on different aspects, but removes the old deprecated MySQL extensions, and supports only the newer library of functions for the connection to the databases. The software systems already in place need to be renewed to be compliant with respect to the new language version. The conversion of the source code, to be safe against injection attacks, should involve also the transformation of the query code. The purpose of this work is the design of specific tool that automatically applies the required transformation yielding to a precise and efficient conversion procedure. The tool has been applied to different projects to provide evidence of its effectiveness.	mysql;php;portable distributed objects	Fabio Mondin;Agostino Cortesi	2018		10.1007/978-3-319-99954-8_38	porting;programming language;machine learning;software system;deprecated;artificial intelligence;source code;computer science;static analysis;software;upgrade;constructed language	NLP	-54.17340744508588	38.79027997872124	139304
1665a8e88e258e3af6874b4aa015a15fb4723238	automated program repair using genetic programming and model checking	automated software repair;automatic bug fixing;genetic programming;model checking	Automated program repair is still a highly challenging problem mainly due to the reliance of the current techniques on test cases to validate candidate patches. This leads to the increasing unreliability of the final patches since test cases are partial specifications of the software. In the present paper, an automated program repair method is proposed by integrating genetic programming (GP) and model checking (MC). Due to its capabilities to verify the finite state systems, MC is employed as an appropriate criterion for evolving programs to calculate the fitness in GP. The application of MC for the fitness evaluation, which is novel in the context of program repair, addresses an important gap in the current heuristic approaches to the program repair. Being focused on fault detection based on the desired aspects, it enables the programmers to detect faults according to the definition of properties. Creating a general method, this characteristic can be effectively customized for different domains of application and the corresponding faults. Apart from various types of faults, the proposed method is capable of handling concurrency bugs which are not the case in many general repair methods. To evaluate the proposed method, it was implemented as a tool, named JBF, to repair Java programs. To meet the objectives of the study, some experiments were conducted in which certain programs with known bugs were automatically repaired by the JBF tool. The obtained results are encouraging and remarkably promising.	automated theorem proving;backtracking;concurrency (computer science);enumerated type;experiment;fault detection and isolation;genetic programming;heuristic;infinite loop;java;model checking;programmer;prototype;run time (program lifecycle phase);scheduling (computing);software bug;test case;time complexity	Zahra Zojaji;Behrouz Tork Ladani;Alireza Khalilian	2016	Applied Intelligence	10.1007/s10489-016-0804-0	algorithm	SE	-59.48519459998932	36.893779279893316	139830
17d34c3483607ded2dc977caaab8ef742c86cd03	model-based testing service on the web	test automation;separation of concern;web interface;smart phone;gui testing;graphic user interface;test generation;model based testing;domain specificity	Model-based testing (MBT) seems to be technically superior to conventional test automation. However, MBT features some diffi culties that can hamper its deployment in industrial contexts. We are developin g a domain-specific MBT solution for graphical user interface (GUI) testing of S ymbian S60 smartphone applications. We believe that such a tailor-made solu tion can be easier to deploy than ones that are more generic. In this paper, we pres nt a service concept and an associated web interface that hide the inherent c omplexity of the test generation algorithms and large test models. The interface enables an easy-to-use MBT service based on the well-known keyword concept. With th is solution, a better separation of concerns can be obtained between the te st modeling tasks that often require special expertise, and test execution th at can be performed by testers. We believe that this can significantly speed up the i ndustrial transfer of model-based testing technologies, at least in this context .	algorithm;graphical user interface testing;mobile app;model-based testing;separation of concerns;software deployment;test automation;whole earth 'lectronic link	Antti Jääskeläinen;Mika Katara;Antti Kervinen;Henri Heiskanen;Mika Maunumaa;Tuula Pääkkönen	2008		10.1007/978-3-540-68524-1_5	embedded system;simulation;engineering;world wide web;graphical user interface testing;test management approach	SE	-49.29610615290736	38.68100723166197	140099
2d1023701b9a8fe1b2a4d26d1fecabdb5a53c9dc	information flow control for workflow management systems			information flow (information theory);non-interference (security)	Thomas Bauereiß;Dieter Hutter	2014	it - Information Technology		xpdl;document management system;structure of management information;workflow management system;workflow engine;workflow technology	Arch	-52.27432917448227	44.86491595462353	140289
39b8d0a98661730245bb4ed4583d82262157e8d4	implementation techniques for efficient data-flow analysis of large programs	optimising compilers;program understanding;software engineering tools;optimising compilers reverse engineering program slicing data flow analysis;lines of code;data flow analysis;data analysis performance analysis data engineering program processors information analysis optimizing compilers software tools programming profession computer science data mining;c programs data flow analysis large programs software engineering tools program slicers optimizing compilers space performance time performance program slicing tool;program slicing;data flow;reverse engineering	Many software engineering tools such as program slicers must perform data-flow analysis in order to extract necessary information from the program source. These tools typically borrow much of their implementation from optimizing compilers. However, since these tools are expected to analyze programs in their entirety, rather than functions in isolation, the time and space performance of the data-flow analyses are of major concern. We present techniques that reduce the time and space required to perform data-flow analysis of large programs. We have used these techniques to implement an efficient program slicing tool for C programs and have computed slices of programs with more than 100,000 lines of code.	computer data storage;data-flow analysis;dataflow;heuristic;hybrid algorithm;iterative method;memory hierarchy;openvms;optimizing compiler;program slicing;search algorithm;signal-flow graph;software engineer;software engineering;software maintenance;source lines of code	Darren C. Atkinson;William G. Griswold	2001		10.1109/ICSM.2001.972711	data flow diagram;computer architecture;program slicing;real-time computing;computer science;software engineering;data-flow analysis;programming language;source lines of code;reverse engineering	SE	-56.58993807609248	36.235241200696976	140363
253abb8f55e3d2e171074bf38bb6a2ac6b277987	hypertext support for the information needs of software maintainers	program dependencies;information need;program slicing;reverse engineering	Making changes safely to programs requires program comprehension and satisfaction of the information needs of software maintainers. In this paper we provide insights into improving hypertext-based software maintenance support by analyzing those information needs. There exists a series of four earlier, detailed-level empirical studies on the information needs of professional C program maintainers. We focus on these studies, synthesize their results and determine sources from which the required information might be attained. An experimental research tool, the HyperSoft system, is used to demonstrate the satisfaction of information needs and the system is analytically evaluated against the needs explored by the empirical studies. HyperSoft is based on our transient hypertext approach for software maintenance support. HyperSoft provides automatically generated hypertextual access structures and software visualizations. The results show that transient hypertext is a well-suited representational form of the typically required versatile information. The discussion also covers related tools and the main features for providing the information required by maintainers are identified. The results show that the focus areas of these tools vary considerably.	hypertext;information needs	Jussi Koskinen;Airi Salminen;Jukka Paakki	2004	Journal of Software Maintenance	10.1002/smr.292	information needs;program slicing;computer science;systems engineering;engineering;software engineering;programming language;world wide web;reverse engineering	SE	-54.157409575781344	35.33446293223687	140372
0551eaf835bb68c487fcda6c7aa8fe18e0152d62	mining security vulnerabilities from linux distribution metadata	debian;security of data data mining linux;vulnerability patching history security vulnerability repository mining linux distribution metadata change logs debian ubuntu version branching structure;vulnerabilities;linux;repository mining;debian vulnerabilities repository mining linux;linux security data mining software indexes history	Security vulnerability research has long been hindered by the difficulty in obtaining structured, detailed data on individual vulnerabilities in sufficient quantities for analysis. We mined vulnerabilities from historical change log data from Linux distribution packages, tapping a yet-unexplored source of security data. Change logs provide a unified view of a application's evolution, version branching structure, and vulnerability patching history, allowing for the large-scale compilation of data on susceptible (and, in some cases, non-susceptible) versions of the original application for each vulnerability. We then compiled vulnerability datasets for multiple releases of Debian and Ubuntu Linux, analyzing trends in vulnerability patching over time. Patching practices in Debian and Ubuntu were similar, and patch rates stayed constant throughout each distribution's lifetime.	archive;bug tracking system;compiler;database;debian;libressl;lineage (evolution);linux;mined;ubuntu;vulnerability (computing)	Jeff Stuckman;James Purtilo	2014	2014 IEEE International Symposium on Software Reliability Engineering Workshops	10.1109/ISSREW.2014.101	vulnerability management;vulnerability;computer science;operating system;database;world wide web;computer security;linux kernel	Arch	-61.35219393512052	43.112100690183524	140467
32b53da1c65210df455091048d3009335d13c047	on the validation of api execution-sequence to assess the correctness of application upon cots upgrades deployment	regression test selection;regression testing;system test suite api execution sequence cots upgrades deployment regression test selection software systems;software systems;program verification;program testing;api execution sequence;software systems system testing application software runtime prototypes operating systems software prototyping technological innovation software testing costs;application program interfaces;software packages application program interfaces program testing program verification;cots upgrades deployment;system test suite;test oracle;software packages	Various regression test selection strategies have been developed to establish the correctness of software systems upon deployment of COTS upgrades. These strategies recommend a smaller regression test suite to be re-executed. We present a process that identifies the patch impact on the system and localizes the source of potential errors without executing the system test suite. The impact is analyzed based on verifying the behavior of affected COTS API execution-sequences. An API execution-sequence is called affected when one or more of the altered functions of the COTS upgrade are part of COTS API execution-sequence that interfaces with application components. A wrapper-based passive test oracle has been developed to check the correctness of the behavior of these execution-sequences. The changed behavior of these execution-sequences reflects the patch impact. A case study has been conducted and the results are encouraging	application programming interface;component-based software engineering;correctness (computer science);oracle (software testing);regression testing;software deployment;software system;system testing;test suite;verification and validation	Anjaneyulu Pasala;Srinivasa Rao;Arnab Dutta Gupta;Srinivas Gunturu	2007	2007 Sixth International IEEE Conference on Commercial-off-the-Shelf (COTS)-Based Software Systems (ICCBSS'07)	10.1109/ICCBSS.2007.23	reliability engineering;regression testing;real-time computing;model-based testing;computer science;operating system;software construction;test suite;software testing;software deployment;software regression	SE	-60.40140259004533	35.64858811271695	140660
954e963a070aee415d1dc9c9fc26fa1a6672d51d	model-driven testing of restful apis	verification;measurement;languages	In contrast to the increasing popularity of REpresentational State Transfer (REST), systematic testing of RESTful Application Programming Interfaces (API) has not attracted much attention so far. This paper describes different aspects of automated testing of RESTful APIs. Later, we focus on functional and security tests, for which we apply a technique called model-based software development. Based on an abstract model of the RESTful API that comprises resources, states and transitions a software generator not only creates the source code of the RESTful API but also creates a large number of test cases that can be immediately used to test the implementation. This paper describes the process of developing a software generator for test cases using state-of-the-art tools and provides an example to show the feasibility of our approach.	application programming interface;model-based testing;representational state transfer;software development;test automation;test case	Tobias Fertig;Peter Braun	2015		10.1145/2740908.2743045	real-time computing;verification;computer science;database;resource-oriented architecture;world wide web;measurement	SE	-53.41673904940885	33.93997413002576	140707
c67e78af0121fd09777a0a4b331288e891d5f6ab	interactive recovery of requirements traceability links using user feedback and configuration management logs	interactive method;configuration management log;traceability	Traceability links can assist in software maintenance tasks. There are some automatic traceability recovery methods. Most of them are similaritybased methods recovering links by comparing representation similarity between requirements and code. They cannot work well if there are some links independent of the representation similarity. Herein to cover weakness of them and improve the accuracy of recovery, we propose a method that extends the similarity-based method using two elemental techniques: a log-based traceability recovery method using the configuration management log and a link recommendation from user feedback. These techniques are independent of the representation similarity between requirements and code. As a result of applying our method to a large enterprise system, we successfully improved both recall and precision by more than a 20 percent point in comparison with singly applying the similarity-based method (recall: 60.2% to 80.4%, precision: 41.1% to 64.8%).	configuration management;elemental;enterprise system;experiment;precision and recall;requirement;requirements traceability;software maintenance	Ryosuke Tsuchiya;Hironori Washizaki;Yoshiaki Fukazawa;Keishi Oshima;Ryota Mibe	2015		10.1007/978-3-319-19069-3_16	traceability;systems engineering;software engineering;database;world wide web;traceability matrix;requirements traceability	SE	-57.92190250028893	32.66273080457588	140885
15903af7ba6f746ae948fc609906b03be6161e02	high dimensional search-based software engineering: finding tradeoffs among 15 objectives for automating software refactoring using nsga-iii	info eu repo semantics conferenceobject;search based software engineering;many objective optimization;refactroing;software quality;code smells	There is a growing need for scalable search-based software engineering approaches that address software engineering problems where a large number of objectives are to be optimized. Software refactoring is one of these problems where a refactoring sequence is sought that optimizes several software metrics. Most of the existing refactoring work uses a large set of quality metrics to evaluate the software design after applying refactoring operations, but current search-based software engineering approaches are limited to using a maximum of five metrics. We propose for the first time a scalable search-based software engineering approach based on a newly proposed evolutionary optimization method NSGA-III where there are 15 different objectives to be optimized. In our approach, automated refactoring solutions are evaluated using a set of 15 distinct quality metrics. We evaluated this approach on seven large open source systems and found that, on average, more than 92% of code smells were corrected. Statistical analysis of our experiments over 31 runs shows that NSGA-III performed significantly better than two other many-objective techniques (IBEA and MOEA/D), a multi-objective algorithm (NSGA-II) and two mono-objective approaches, hence demonstrating that our NSGA-III approach represents the new state of the art in fully-automated refactoring.	algorithm;code refactoring;code smell;experiment;moea framework;mathematical optimization;multi-objective optimization;open-source software;scalability;search-based software engineering;software design;software metric	Mohamed Wiem Mkaouer;Marouane Kessentini;Slim Bechikh;Kalyanmoy Deb;Mel Ó Cinnéide	2014		10.1145/2576768.2598366	software visualization;verification and validation;software sizing;software verification;search-based software engineering;computer science;software framework;component-based software engineering;software development;software construction;software maintenance;code refactoring;software quality;code smell;software metric	SE	-59.17302685142582	34.593261896668665	140991
5fde4932e65c904588f197c2c0c72d9693b4c316	intentional software	intentional software;generative programming	Wysiwyg editors simplified document creation by separating the document contents from the looks and by automating the re-application of the looks to changing contents. In the same way Intentional Software simplifies software creation by separating the software contents in terms of their various domains from the implementation of the software and by enabling automatic re-generation of the software as the contents change. This way, domain experts can work in parallel with programmers in their respective areas of expertise; and the repeated intermingling can be automated. Intentional Software is supported by a Domain Workbench tool where multiple domains can be defined, created, edited, transformed and integrated during software creation. Key features include a uniform representation of multiple interrelated domains, the ability to project the domains in multiple editable notations, and simple access for a program generator.	automatic programming;c syntax;domain-specific language;executable;general-purpose programming language;integer factorization;programmer;structure editor;structured programming;wysiwyg;workbench	Charles Simonyi;Magnus Christerson;Shane Clifford	2006		10.1145/1167473.1167511	domain analysis;computer science;package development process;backporting;software design;theoretical computer science;software framework;component-based software engineering;software development;software design description;domain engineering;software construction;database;software walkthrough;programming language;software analytics;resource-oriented architecture;software system	SE	-52.052323136932465	34.39841497021534	141173
94c5d02c8010402105b322568c2a115a0fc2b6fe	a measure of the modularisation of sequential software versions using random graph theory		Software module clustering is the problem of automatically partitioning the structure of a software system using low-level dependencies in the source code to understand and improve the system’s architecture. Munch, a clustering tool based on search-based software engineering techniques, was used to modularise a unique dataset of sequential source code software versions. This paper investigates whether the dataset used for the modularisation resembles a random graph by computing the probabilities of observing certain connectivity. Modularisation will not be possible with data that resembles random graphs. Thus, this paper demonstrates that our real world time-series dataset does not resemble a random graph except for small sections where there were large maintenance activities. Furthermore, the random graph metric can be used as a tool to indicate areas of interest in the dataset, without the need to run the modularisation.	graph theory;random graph;software versioning	Mahir Arzoky;Stephen Swift;Steve Counsell;James Cain	2014		10.1007/978-3-319-14358-3_10	combinatorics;discrete mathematics;pattern recognition;mathematics	Arch	-57.40450752227173	34.96561371803794	141413
4c14a4ef5c2c6a0dfa396a4b8e8f6dd616990fbc	evaluating manual intervention to address the challenges of bug finding with klee		Symbolic execution has shown its ability to find security-relevant flaws in software, but faces significant scalability challenges. There is a commonly held belief that manual intervention by an expert can help alleviate these limiting factors. However, there has been little formal investigation of this idea. In this paper, we present our experiences applying the KLEE symbolic execution engine to a new bug corpus, and of using manual intervention to alleviate the issues encountered. Our contributions are (1) Hemiptera, a novel corpus of over 130 bugs in real world software, (2) a comprehensive evaluation of the KLEE symbolic execution engine on Hemiptera with a categorisation of frequently occurring software patterns that are problematic for symbolic execution, and (3) an evaluation of manual mitigations aimed at addressing the underlying issues of symbolic execution. Our experience shows that manual intervention can increase both code coverage and bug detection in many situations. It is not a silver bullet however, and we discuss its limitations and the challenges encountered.		John Galea;Sean Heelan;Daniel M Neville;Daniel Kroening	2018	CoRR		computer science;theoretical computer science;code coverage;scalability;software;symbolic execution;limiting	SE	-60.932859917774266	37.915421142837076	141442
77a8bcc54d8553b6f5da2917bee688f4b206393c	automatically detecting integrity violations in database-centric applications		Database-centric applications (DCAs) are widely used by many companies and organizations to perform various control and analytical tasks using large databases. Real-world databases are described by complex schemas that oftentimes contain hundreds of tables consisting of thousands of attributes. However, when software engineers develop DCAs, they may write code that can inadvertently violate the integrity of these databases. Alternatively, business analysts and database administrators can also make errors that lead to integrity violations (semantic bugs). To detect these violations, stakeholders must create assertions that check the validity of the data in the rows of the database tables. Unfortunately, creating assertions is a manual, laborious and error-prone task. Thus, a fundamental problem of testing DCAs is how to find such semantic bugs automatically. We propose a novel solution, namely DACITE, that enables stakeholders to automatically obtain constraints that semantically relate database attributes and code statements using a combination of static analysis of the source code and associative rule mining of the databases. We rely on SAT-solvers to validate if a solution to the combined constraints exists and issue warnings on possible semantic bugs to stakeholders. We evaluated our approach on eight open-source DCAs and our results suggest that semantic bugs can be found automatically with high precision. The results of the study with developers show that warnings produced by DACITE are useful and enable them to find semantic bugs faster.	boolean satisfiability problem;cognitive dimensions of notations;database;database-centric architecture;open-source software;software bug;software engineer;static program analysis;table (database)	Boyang Li;Denys Poshyvanyk;Mark Grechanik	2017	2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)		data mining;database;row;software;database administrator;deep learning;table (database);source code;associative property;schema (psychology);computer science;artificial intelligence	SE	-60.59685793963686	40.709720401901905	141580
697cafc40842fe18184efb2e8a67e1962a61e505	sequential pattern mining based test case regeneration	genetic algorithms;object-oriented software;sequential pattern;test case regeneration;test repository	Automated test generation for object-oriented programs is an essential and yet a difficult task. Many automated test generation approaches produce test cases entirely from the program under test, without considering useful information from already created test cases. This paper presents an approach to regenerate test cases via exploiting frequently-used method call sequences from test repository. Particularly, for an object-oriented program under test, a sequential pattern mining strategy is employed to obtain frequent subsequences of method invocations as sequential patterns from corresponding test repository, and then a GA-based test case regeneration strategy is used to produce new test cases on the basis of the sequential patterns. A prototype called SPM-RGN is developed and is applied to generate test cases for actual Java programs. Empirical results show that SPM-RGN can achieve 47.5%, 11.2% and 4.5% higher branch coverage than three existing automated test generators. Besides, SPM-RGN produces 85.1%, 28.1% and 27.4% shorter test cases than those test generators. Therefore, the test cases generated by SPM-RGN are more effective and easier to understand.	best practice;code coverage;data mining;java;method (computer programming);prototype;sequential pattern mining;software release life cycle;software testing;super paper mario;test automation;test case	Wei He;Ruilian Zhao	2013	JSW		test data generation;genetic algorithm;computer science;artificial intelligence;automatic test pattern generation;machine learning;test compression;test suite;data mining;system under test;test case;algorithm	SE	-59.431308565152634	35.870594316918684	141582
1cae204b881b3322a3e4b43024e2db950282ab9c	edobs - graphical debugging for eclipse		This paper presents the eDOBS tool. eDOBS is the little brother of the Fujaba environment. While Fujaba is used to create graph grammar based specifications and programs, eDOBS is used to browse graphs, to edit graphs, and to execute graph transformations.	debugging;eclipse	Leif Geiger;Albert Zündorf	2006	ECEASST	10.14279/tuj.eceasst.1.84	computer science;theoretical computer science;database;programming language	HCI	-54.23436004436151	35.33063324261172	141811
26005e642decee732bcee1f59bbbf7fb11819aa4	mlda: a multiple levels detection approach for design patterns recovery		Design patterns have a key role in the software development process. They describe both structure, behavior of classes and their relationships. During the maintenance phase, architects can benefit from knowing the underlying software design choices made during the implementation. Moreover, design patterns can improve software documentation, speed up the development process and enable large-scale reuse of software architectures. This paper presents a Multiple Levels Detection Approach (MLDA) to recover design pattern instances from Java source code. The novelty behind MLDA is its ability to extract design pattern instances based on a generated class level representation of an investigated system. Specifically, MLDA presents what is the so-called Structural Search Model (SSM) which incrementally builds the structure of each design pattern based on the generated source code model. As the experiment results illustrate, MLDA is able to extract 22 design patterns with reasonable detection accuracy.	java;software design pattern;software development process;software documentation	Mohammed Al-Obeidallah;Miltos Petridis;Stelios Kapetanakis	2017		10.1145/3093241.3093244	machine learning;software development process;real-time computing;reverse engineering;software design;software documentation;source code;software design pattern;behavioral pattern;structural pattern;computer science;artificial intelligence	SE	-56.70024242775024	35.06022939806802	141846
de773a032ae0cfa760a4f84e0b49ce459cbca3e8	selection and evaluation of test data based on genetic programming	mutation testing;genetic program;evolutionary computation;software quality assurance;testing criteria;mutation analysis;evolutionary computing	In the literature, we find several criteria that consider different aspects of the program to guide the testing, a fundamental activity for software quality assurance. They address two important questions: how to select test cases to reveal as many fault as possible and how to evaluate a test set T and end the test. Fault-based criteria, such as mutation testing, use mutation operators to generate alternatives for the program P being tested. The goal is to derive test cases capable of producing different behaviors in P and its alternatives. However, this approach usually does not allow the test of interaction between faults since the alternative differs from P by a simple modification. This work explores the use of Genetic Programming (GP), a field of Evolutionary Computation, to derive alternatives for testing P and introduces two GP-based procedures for selection and evaluation of test data. The procedures are related to the above questions, usually addressed by most testing criteria and tools. A tool, named GPTesT, is described and results from an experiment using this tool are also presented. The results show the applicability of our approach and allow comparison with mutation testing.	evolutionary computation;genetic programming;mutation testing;p (complexity);software quality assurance;test case;test data;test set	Maria Cláudia Figueiredo Pereira Emer;Silvia Regina Vergilio	2003	Software Quality Journal	10.1023/A:1023772729494	domain testing;reliability engineering;test data generation;model-based testing;orthogonal array testing;white-box testing;computer science;systems engineering;engineering;functional testing;mutation testing;risk-based testing;test management approach;algorithm;evolutionary computation	SE	-59.86109100219272	34.78215499008486	142096
a10b58e6834e99ccc626ea65600b1bff3d607c2c	automated conformance testing of java virtual machines	software testing;formal specification;java software engineering software testing formal methods;virtual machines conformance testing finite state machines formal specification java program testing;formal methods;java testing safety computational modeling virtual machining abstracts load modeling;software engineering;finite state machines;conformance testing;program testing;virtual machines;java;jvm specifications automated conformance testing java virtual machine structural constraints program execution type safety formal modeling finite state machine java standard specifications test suite oracle	We present a technique to fully automate the conformance testing of a Java virtual machine (JVM) implementation to the structural constraints it must satisfy to enforce type safety of program execution. The approach is based on formal modeling of the JVM as a finite state machine, ruled by the Java standard specifications. The model is used to derive a test suite and corresponding oracle that systematically explores the space of illegal states reachable by a JVM implementation under test. Also, a degree of conformance to the JVM specifications (i.e., too strict or too coarse) can be assessed by counting the number of false positives. Despite the huge test space, the entire proposed process need not human supervision. The technique is black box, fully automated, and can be applied for validating final products or during development i.e. for debugging purposes.	algorithm;black box;business continuance volume;byte;conformance testing;control flow;debugging;finite-state machine;java applet;java virtual machine;precondition;regression testing;test automation;test suite;type inference;type safety;vii	Andrea Calvagna;Emiliano Tramontana	2013	2013 Seventh International Conference on Complex, Intelligent, and Software Intensive Systems	10.1109/CISIS.2013.99	real-time computing;jsr 94;java concurrency;computer science;operating system;java modeling language;strictfp;conformance testing;real time java;programming language;java;scala;java annotation	SE	-56.422996117464734	40.25564502822277	142187
7614505cf9295a355504c70530273ce9784fc660	using ltl rewriting to improve the performance of model-checker based test-case generation	software testing;runtime verification;temporal logic;ease of use;automated software testing;test case generation;ltl rewriting;test case generation with model checkers	Model-checkers have recently been suggested for automated software test-case generation. Several works have presented methods that create efficient test-suites using model-checkers. Ease of use and complete automation are major advantages of such approaches. However, the use of a model-checker comes at the price of potential performance problems. If the model used for test-case generation is complex, then model-checker based approaches can be very slow, or even not applicable at all. In this paper, we identify that unnecessary, redundant calls to the model-checker are one of the causes of bad performance. To overcome this problem, we suggest the use of temporal logic rewriting techniques, which originate from runtime verification research. This achieves a significant increase in the performance, and improves the applicability of model-checker based test-case generation approaches in general. At the same time, the suggested techniques achieve a reduction of the resulting test-suite sizes without degradation of the fault sensitivity. This helps to reduce the costs of the test-case execution.	elegant degradation;model checking;rewriting;runtime verification;software testing;temporal logic;test case;test suite	Gordon Fraser;Franz Wotawa	2007		10.1145/1291535.1291542	real-time computing;usability;temporal logic;computer science;software engineering;software testing;runtime verification;programming language;algorithm	SE	-59.781216487485146	36.90360479905734	142304
a5a145abd1deae6650d57c929db939574e78101e	a test-driven approach to code search and its application to the reuse of auxiliary functionality	test driven development;development tool;software development;development tools;code search;software reuse;open source	ContextSoftware developers spend considerable effort implementing auxiliary functionality used by the main features of a system (e.g., compressing/decompressing files, encryption/decription of data, scaling/rotating images). With the increasing amount of open source code available on the Internet, time and effort can be saved by reusing these utilities through informal practices of code search and reuse. However, when this type of reuse is performed in an ad hoc manner, it can be tedious and error-prone: code results have to be manually inspected and integrated into the workspace. ObjectiveIn this paper we introduce and evaluate the use of test cases as an interface for automating code search and reuse. We call our approach Test-Driven Code Search (TDCS). Test cases serve two purposes: (1) they define the behavior of the desired functionality to be searched; and (2) they test the matching results for suitability in the local context. We also describe CodeGenie, an Eclipse plugin we have developed that performs TDCS using a code search engine called Sourcerer. MethodOur evaluation consists of two studies: an applicability study with 34 different features that were searched using CodeGenie; and a performance study comparing CodeGenie, Google Code Search, and a manual approach. ResultsBoth studies present evidence of the applicability and good performance of TDCS in the reuse of auxiliary functionality. ConclusionThis paper presents an approach to source code search and its application to the reuse of auxiliary functionality. Our exploratory evaluation shows promising results, which motivates the use and further investigation of TDCS.		Otávio Augusto Lazzarini Lemos;Sushil Krishna Bajracharya;Joel Ossher;Paulo César Masiero;Cristina V. Lopes	2011	Information & Software Technology	10.1016/j.infsof.2010.11.009	kpi-driven code analysis;test-driven development;real-time computing;computer science;software development;software engineering;data mining;database;programming language;management;world wide web;source code	SE	-59.74102388193921	39.052792620013264	142816
ccb9c91e83eb74d716fd44333b42cc227dde0b3f	vm reliability modeling and analysis for iaas could		With the popularity of cloud computing, its reliability is becoming a key concern for cloud users. As the main product of IaaS cloud, virtual machineu0027s reliability is a valuable research issue. This paper presents a modeling strategy for VM reliability based on general stochastic Petri nets. The modeling strategy decomposes VM reliability into network reliability, computing reliability and storage reliability, and considers high availability mechanism in IaaS cloud. To analyze the reliability model, we use Markov process theory and give the detailed solving process. A case study of OpenStack cloud is conducted to illustrate the applicability of the proposed modeling strategy. In addition, we discussed the reliability impact of some model parameters which can help IaaS cloud on how to improve VM reliability.	cloud computing;reliability engineering	Qingfeng Du;Huan Li;Kanglin Yin;Juan Qiu	2017		10.1109/CyberC.2017.77	real-time computing;reliability engineering;software quality;cloud computing;reliability (computer networking);high availability;virtual machine;computer science;stochastic petri net;markov process;server	SE	-49.065113802555885	41.245951887847426	142890
3e676b817a1870e5d7e8851bda69cf0988f4ccaf	test criteria for context-free grammars	software testing vocabulary terminology software engineering chaos system testing extraterrestrial measurements production computer applications application software;software testing;context free grammars;formal languages;structural testing;program testing;context free grammar;test set sizes;software testing adequacy test adequacy criterion context free grammars subsume relations test set sizes;subsume relations;program testing context free grammars;software testing adequacy;test adequacy criterion	This paper discusses the formalization of the test adequacy criterion for context-free grammars. The basic concept of grammar-based testing is introduced and the grammar-based test criterion is formally defined. This paper also proposes a family of grammar-based structural test criteria whose subsume relations and test set sizes are analyzed, which are used to assess the proposed criteria. Some basic properties based on Weyuker's axiom systems are generalized and some fundamental properties specific to grammar-based testing are proposed We believe the concepts developed in this paper will contribute to the classical theory of software testing adequacy and are very valuable for software test practice	context-free grammar;context-free language;software testing;test set	Hu Li;Maozhong Jin;Chao Liu;Zhongyi Gao	2004	Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.	10.1109/CMPSAC.2004.1342847	natural language processing;model-based testing;computer science;software engineering;functional testing;risk-based testing;software testing;context-free grammar;programming language;test management approach;algorithm	SE	-59.84769046148594	33.69390608234358	143092
949dfecc920dc55ecf17419580f7b05ea88733cf	a model for conformance analysis of software documents	conformational analysis;formal specification;software prototyping;software maintenance;system documentation;conformance model software project evolution software artifacts requirement specification design documents source code bug reports software documents semantic relationship semantic consistency conformance analysis logical relationship nonconformance detection timestamp versioned hypermedia;hypermedia;large scale;formal verification;conformance testing;text analysis software maintenance large scale systems documentation software development management software tools environmental management project management nasa engineering profession;software development;source code;program debugging;hypermedia system documentation software prototyping software maintenance formal specification formal verification conformance testing program debugging;requirement specification	During the evolution of a large-scale software project, developers produce a large variety of software artifacts such as requirement specifications, design documents, source code, documentation, bug reports, etc. These software documents are not isolated items — they are semantically related to each other. They evolve over time and the set of active semantic relationships among them is also dynamic. Their evolution makes the task of managing them and maintaining their semantic consistency a very challenging task for software developers. At times, the evolutionary changes may reduce the consistency of the software project and break semantic connections among its documents. We use the term conformance to denote the state where the network formed by software documents and their relationships is in semantic harmony. Conformance analysis is the process of determining whether software documents and their logical relationships are in agreement. In this paper, we present a representation for software documents and their logical relationships based on the hypertext model. We describe how conformance analysis can be supported by this representation and present a method to detect non-conformance using timestamps and versioned hypermedia. Then we describe a formalism, called the conformance model, that can be used to understand and combine approaches to the conformance analysis problem.	arcsde;conformance testing;hypermedia;hypertext;mathematical model;prototype;semantics (computer science);software developer;software documentation;software engineering;software project management;testbed;web page	Tien Nhut Nguyen;Ethan V. Munson	2003		10.1109/IWPSE.2003.1231206	verification and validation;formal verification;computer science;systems engineering;engineering;software development;software engineering;conformance testing;software construction;formal specification;database;programming language;software maintenance;source code	SE	-54.40402647730349	33.58874985449457	143243
96e26f36ce6230ebb7ed4bdca78e1add01624d5d	flexible method for producing static visualizations of log data	log visualization;electronic learning;pilot study;human computer interaction;log files;history;application software;system monitoring computer aided instruction data visualisation file organisation internet public domain software software tools;computer aided instruction;e learning system;system monitoring;software systems;learning environment;log data;public domain software;data visualisation;internet;visual representation;interaction pattern;static visualization;data visualization;log file visualization;software tools;web server;open source software tools static visualizations log data log file visualization e learning system web server;open source tools;open source software tools;static visualizations;open source software;open source;open source tools log visualization static visualization;file organisation;timing	The aim in this work was to develop a method to visualize log files produced by a pilot study of an experimental e-learning platform. The e-learning system running on a web server generated detailed descriptions of its usage that were analyzed. The log files were produced to allow the project's researchers to study offline how the system was utilized. We used open-source software tools to create a flexible system that processed a large amount of log data into concise visual representations. The visualizations revealed a set of different interaction patterns during the pilot study and showed that each teacher had a large effect on students' interaction with the system. Thus, the results showed that the suggested visualization method can be used to support the evaluation and training of students' proficiency in a learning environment.	data logger;effective method;interaction;online and offline;open-source software;server (computing);web server	Harri Siirtola;Kari-Jouko Räihä;Veikko Surakka;Toni Vanhala	2008	2008 12th International Conference Information Visualisation	10.1109/IV.2008.42	computer science;operating system;database;world wide web	Visualization	-53.79498250627783	37.024229875696584	143650
d2160f723fb1c01abcf2f97ce2183fd65c5307b3	failure analysis and tolerance strategies in web service ecosystems	failed service features;service ecosystem;composition patterns;期刊论文;service substitution;failure impacts	Service-oriented computing and cloud computing are playing critical roles in supporting business collaboration over the Internet. Thanks to the latest development in computing technologies, various large-scale, evolving, and rapidly growing service ecosystems emerge. However, service failures greatly hamper the usability and reputation of service ecosystems. In the previous work, service failure is not adequately studied from an ecosystem's perspective. To address this gap, we propose a service failure analysis framework based on a complex network model of service ecosystem. This framework comprises a feature model of failed services and several service failure impact indicators. By applying the framework, empirical analysis of failed service features and failure impact assessment can be implemented more easily and precisely. Moreover, to provide failure tolerance strategies for service ecosystems, a novel composition-based service substitution method is designed to replace the failed services with functional similar ones, such that the service systems are more robust when a failure occurs. As the new substitution method requires fewer structural data of services, it is more convenient to be applied in present RESTful Representational State Transfer REST service environment. Both the framework and the service substitution method are tested on real-world data set, and their usability and efficiency are demonstrated. Copyright © 2014 John Wiley & Sons, Ltd.	ecosystem;failure analysis;web service	Yi Liu;Yushun Fan;Keman Huang;Wei Tan	2015	Concurrency and Computation: Practice and Experience	10.1002/cpe.3319	mobile qos;simulation;service product management;service management;service delivery framework;operating system;service guarantee;database;distributed computing;data as a service;world wide web	OS	-48.41400965821404	40.563181550254406	143763
dbe93a5bb9e1d21123198445837ff81df1b5b049	improving side-effect analysis with lazy access path resolving	software;program diagnostics;pointer analysis;information analysis algorithm design and analysis scalability application software educational institutions information science space technology optical propagation software engineering java;inclusion based context insensitive pointer analysis side effect analysis lazy access path resolving;data mining;arrays;indexes;computational modeling;side effect;side effect analysis;must alias side effect points to analysis;scalability;algorithm design and analysis;must alias;lazy access path resolving;points to analysis;inclusion based context insensitive pointer analysis	For scalability, many side-effect analysis methods choose inclusion-based context-insensitive (IBCI) pointer analysis as their basis. However, such a pointer analysis is known to be imprecise, which often results in over-conservative side-effect sets. In this paper, we present a lightweight approach that exploits lazy access path resolving to improve the precision of side-effect analysis under IBCI pointer analysis. The approach partly represents and propagates side-effects in the access path form with the help of interstatement must aliases. All access paths can finally be resolved to the accessed locations, but during the side-effect propagation phase, an access path will never be resolved as long as it could be mapped to another access path in the caller. Since in inclusion-based points-to analysis, points-to sets of variables in the callers tend to be smaller than the ones in the callees, such lazy resolving mechanism can lead to more precision. The experimental results show that the lazy access path resolving approach is effective in improving the precision of IBCI pointer analysis based side-effect analysis methods.	algorithm;alias analysis;computation;experiment;global value numbering;heuristic (computer science);lazy evaluation;pointer (computer programming);pointer analysis;scalability;side effect (computer science);software maintenance;software propagation	Ju Qian;Yuming Zhou;Baowen Xu	2009	2009 Ninth IEEE International Working Conference on Source Code Analysis and Manipulation	10.1109/SCAM.2009.13	database index;algorithm design;scalability;computer science;theoretical computer science;database;distributed computing;programming language;computational model;pointer analysis;side effect	SE	-59.26565359245525	38.14407192511207	143959
18cc42a2b2006c57dd222d4dd8b2992a55ac43a7	a hybrid approach to propagation analysis	hybrid approach;static analysis	Propagation analysis is a dynamic code analysis technique that can be used to quantitatively assess certain software properties that are oth erwise di cult to assess such as testability safety and security The currently accepted analysis technique relies heavily on repeated execution of the code being assessed and is therefore very expensive to apply We are currently developing static analysis methods to assist in the dynamic assessment Our aim is to replace the most expensive parts of the dy namic analysis technique with less expensive and nearly as accurate static analysis techniques This paper summarizes our ideas on how this can be	dynamic program analysis;software propagation;static program analysis	David Byers;Mariam Kamkar	1997			testability;reliability engineering;software;computer science;static analysis;dynamic program analysis	SE	-60.22615213394417	36.97807173689196	143992
5728bd19dc936bbe75801ecdad75e6e3a0238115	refining clustering evaluation using structure indicators	software systems clustering algorithms software measurement software maintenance software algorithms information retrieval performance evaluation size measurement reverse engineering;software decompositions structure indicators software clustering algorithms clustering evaluation;cluster algorithm;pattern clustering;software decompositions;software measurement;structure indicators;software maintenance;search space;evaluation method;software performance evaluation;software systems;software performance evaluation pattern clustering software maintenance;data mining;software clustering algorithms;ground penetrating radar;clustering evaluation;clustering algorithms;software algorithms	The evaluation of the effectiveness of software clustering algorithms is a challenging research question. Several approaches that compare clustering results to an authoritative decomposition have been presented in the literature. Existing evaluation methods typically compress the evaluation results into a single number. They also often disagree with each other for reasons that are not well understood. In this paper, we introduce a novel set of indicators that evaluate structural discrepancies between software decompositions. They also allow researchers to investigate the differences between existing evaluation approaches in a reduced search space. Several experiments with real software systems showcase the usefulness of the introduced indicators.	algorithm;application programming interface;bunch;brian;cluster analysis;experiment;knowledge engineering;mitchell corporation;software system;usability;xojo	Mark Shtern;Vassilios Tzerpos	2009	2009 IEEE International Conference on Software Maintenance	10.1109/ICSM.2009.5306306	ground-penetrating radar;computer science;engineering;theoretical computer science;software engineering;data mining;database;cluster analysis;software maintenance;software measurement;software system	SE	-62.73102046990608	33.71299312144738	144144
e9b41424923367e904a3d0fde481810d454d1537	assessing inheritance for the multiple descendant redefinition problem in oo systems	object-oriented design;object-oriented metrics;method redefinition;class hierarchy;inheritance;smalltalk;object oriented design	Current use of inheritance has illustrated that the introduction of conceptual inconsistencies is possible in a class hierarchy. This paper discusses the reasons why complete method redefinition infringes the essence of inheritance. A redefinition metric set is proposed and practical experiments demonstrate that the results obtained permit the detection of inheritance design problems. Appropriate design decisions are suggested.	class hierarchy;experiment	Philippe Li-Thaio-Té;Jessie B. Kennedy;John Owens	1997			computer science;systems engineering;programming language;algorithm	AI	-50.54808457396115	34.76500759686787	144300
0df9b92f2ffca347d41b3ec24adc5d19cce1ecda	continuous test generation: enhancing continuous integration with automated test generation	unit testing;articulo;continuous integration;continuous test generation enhancing continuous integration with automated test generation;automated test generation;continuous testing	In object oriented software development, automated unit test generation tools typically target one class at a time. A class, however, is usually part of a software project consisting of more than one class, and these are subject to changes over time. This context of a class offers significant potential to improve test generation for individual classes. In this paper, we introduce Continuous Test Generation (CTG), which includes automated unit test generation during continuous integration (i.e., infrastructure that regularly builds and tests software projects). CTG offers several benefits: First, it answers the question of how much time to spend on each class in a project. Second, it helps to decide in which order to test them. Finally, it answers the question of which classes should be subjected to test generation in the first place. We have implemented CTG using the EvoSuite unit test generation tool, and performed experiments using eight of the most popular open source projects available on GitHub, ten randomly selected projects from the SF100 corpus, and five industrial projects. Our experiments demonstrate improvements of up to +58% for branch coverage and up to +69% for thrown undeclared exceptions, while reducing the time spent on test generation by up to +83%.	code coverage;continuous integration;evosuite;experiment;open-source software;randomness;software development;software project management;test automation;unit testing	José Campos;Andrea Arcuri;Gordon Fraser;Rui Abreu	2014		10.1145/2642937.2643002	test data generation;simulation;computer science;engineering;test suite;unit testing;programming language;test management approach;algorithm;test harness	SE	-61.939310824399804	35.5703200722685	144558
95319bbd9c8ca26181f03b43b3d33772d3cc5f1d	creating and maintaining tutorials with deft	manuals;tutorial documentation writing switches computer science costs software libraries html contracts project management;teaching application program interfaces computer aided instruction;api documentation;development environment for tutorials;project management;software libraries;computer aided instruction;maintenance cost;contracts;maintenance engineering;development environment for tutorials tutorials framework documentation api documentation;html;framework documentation;development environment;chapters;tutorials;application program interfaces;writing;computer science;switches;programming;documentation;teaching;tutorial	Good documentation is crucial for frameworks to be used correctly. One important kind of framework documentation are tutorials. They tell how to use the framework and provide additional code examples. However, tutorials are rare compared to other kinds of documentation such as API documentation due to high creation and maintenance costs. In this paper we highlight the problems of writing and maintaining tutorials. Then we present the Development Environment For Tutorials (DEFT), which provides support to address the identified problems.	application programming interface;documentation;programming language;software framework;user interface	Andreas Bartho	2009	2009 IEEE 17th International Conference on Program Comprehension	10.1109/ICPC.2009.5090072	maintenance engineering;project management;programming;html;documentation;computer science;software engineering;development environment;multimedia;writing;computer engineering	SE	-54.821337033430645	33.45283078011497	144660
45f5aaf0e988ec0cdf9c34494c3f65f4e2997a4f	the impact of concurrent coverage metrics on testing effectiveness	software metrics;software testing;testing fault detection synchronization correlation concurrent computing delays;multi threading;concurrent computing;concurrent program;testing;software metrics multi threading program testing;program testing;synchronization;fault detection;fault detection concurrent coverage metrics multithreaded program testing concurrent program behavior branch and statement coverage metrics sequential program testing;correlation;delays;concurrent program software testing coverage metric;coverage metric	When testing multithreaded programs, the number of possible thread interactions makes exploring all interactions infeasible in practice. In response, researchers have developed concurrent coverage metrics for multithreaded programs. These metrics allow them to estimate how well they have exercised concurrent program behavior, just as branch and statement coverage metrics do for sequential program testing. However, unlike sequential coverage metrics, the effectiveness of concurrent coverage metrics in testing remains largely unexamined. In this paper, we explore the relationship between concurrent coverage and fault detection effectiveness by studying the application of eight concurrent coverage metrics in testing nine concurrent programs. Our results show that existing concurrent coverage metrics are often moderate to strong predictors of concurrent testing effectiveness, and are generally reasonable targets for test suite generation. Nevertheless, their relative effectiveness as predictors and test generation targets varies across programs, and thus additional work is needed in this area.	code coverage;concurrency (computer science);concurrent computing;concurrent testing;fault detection and isolation;interaction;sequential consistency;test case;test suite;thread (computing);vii	Shin Hong;Matthew Staats;Jaemin Ahn;Moonzoo Kim;Gregg Rothermel	2013	2013 IEEE Sixth International Conference on Software Testing, Verification and Validation	10.1109/ICST.2013.32	reliability engineering;real-time computing;concurrent computing;computer science;software engineering;distributed computing;software testing	SE	-61.68894826636124	36.275685599132615	145028
9d7cb49c3f37f008e548aff262d41a0730636dfb	a new code generation method for software engineering: from requirements model to source code.				Bo Huang;Zhijun Fang;Guoqing Wu;Xiankun Sun;Yongbin Gao	2017		10.3233/978-1-61499-800-6-747	kpi-driven code analysis;code coverage;theoretical computer science;computer science;static program analysis;code refactoring;computer architecture;codebase;source code;code generation;source lines of code	SE	-56.699131591552465	36.13617101597186	145316
233a50cdc9ba4ce0b322053cebbc380ec93a728c	repeatedly-executed-method viewer for efficient visualization of execution paths and states in java	program understanding;record and replay;dynamic analysis;java	The state of a program at runtime is useful information for developers to understand a program. Omniscient debugging and logging-based tools enable developers to investigate the state of a program at an arbitrary point of time in an execution. While these tools are effective to analyze the state at a single point of time, they might be insufficient to understand the generic behavior of a method which includes various control-flow paths. In this paper, we propose REMViewer (Repeatedly-Executed-Method Viewer), or a tool that visualizes multiple execution paths of a Java method. The tool shows each execution path in a separated view so that developers can firstly select actual execution paths of interest and then compare the state of local variables in the paths.	control flow;debugging;java;local variable;run time (program lifecycle phase)	Toshinori Matsumura;Takashi Ishio;Yu Kashima;Katsuro Inoue	2014		10.1145/2597008.2597803	real-time computing;computer science;operating system;distributed computing;dynamic program analysis;programming language;java	SE	-55.04103086754399	36.578217155570414	145546
37d83650375457b2c7d233ace6dadb542f857d93	fault detection in building management system networks	performance monitoring;buildingoperations;building management system;network management tools;universitycampus;building automation;control and monitoring;configuration management;building operations;university campus	"""Building management systems that integrates control and monitoring of building operation are common in modern (""""intelligent"""") buildings. With the increasing size of the building management system size, its management becomes complex task consisting of configuration management, fault detection and prevention, performance monitoring and auditing of user actions. In large building complexes detecting faults in BMS becomes difficult and time consuming for human operators. Faults that remain undetected in the system for long time periods can cause increased operation costs or prevent reliable analysis of building operation. Some of the issues of large BMS can be resolved by use of existing network management tools or development of specific software that automatically detects faults in the system. The aim of this article is to present experience with management of large BMS at University Campus in Brno-Bohunice and to propose algorithms and techniques for detection of specific types of faults."""	management system	Adam Kucera;Tomás Pitner;Petr Glos	2013		10.3182/20130925-3-CZ-3023.00027	building management system;systems management;simulation;systems engineering;engineering;network management application;computer security	Robotics	-52.8784412226122	42.45687541656934	145617
251f6c32b8b18eeb15ff3eb63136db91b38aee90	event views and graph reductions for understanding system level c code	event views;program visualization event views graph reductions system level c code understanding program comprehension software tool interactive tool;program diagnostics;software tool;runtime kernel aggregates data structures operating systems protocols software maintenance yarn visualization linux;program visualization;program comprehension;domain knowledge;c language;system level c code understanding;operating system;reverse engineering c language interactive systems program diagnostics program visualisation;interactive tool;interactive systems;data structure;concurrent process;program visualisation;reverse engineering;graph reductions	Concurrent processing, runtime bindings, and an extensive use of aggregate data structures make system level C codes difficult to understand. This paper proposes event views and graph reductions as techniques to facilitate program comprehension. Starting with some domain knowledge, a user can apply these techniques to quickly identify and analyze exactly those parts of the program that are relevant to a given concern. We have built a tool called CVision to demonstrate applicability of the proposed techniques. CVision is an interactive tool that allows the user to: (a) quickly get to the relevant parts of the code; (b) graphically visualize relationships between program elements; and (c) interactively apply different graph reductions to eliminate irrelevant relationships. Using these capabilities, the user can quickly distill a large body of code and extract meaningful views of runtime events that capture the user's concern. The proposed program comprehension techniques are demonstrated through two case studies based on Linux and XINU operating systems	aggregate data;code;data structure;emoticon;interactivity;linux;linux;list comprehension;operating system;program comprehension;relevance;software engineer;software engineering;software maintenance;xinu	Srinivas Neginhal;Suraj C. Kothari	2006	2006 22nd IEEE International Conference on Software Maintenance	10.1109/ICSM.2006.28	data structure;computer science;theoretical computer science;operating system;software engineering;database;programming language;domain knowledge;reverse engineering	SE	-54.89230767926354	36.17425331453316	145692
29802d6110a0314a15d3b6d382de2d2338610715	recommending relevant classes for bug reports using multi-objective search	software;software maintenance search based software engineering bug reports multi objective optimization;history;software maintenance;multi objective optimization;search based software engineering;computer bugs search problems software engineering optimization software history documentation;software engineering;bug reports;optimization;software maintenance application program interfaces estimation theory java optimisation probability program debugging public domain software search problems;search problems;computer bugs;software maintenance class recommendation bug report description multiobjective search probability multiobjective optimization algorithm relevance estimation api documentation open source java project;documentation	Developers may follow a tedious process to find the cause of a bug based on code reviews and reproducing the abnormal behavior. In this paper, we propose an automated approach to finding and ranking potential classes with the respect to the probability of containing a bug based on a bug report description. Our approach finds a good balance between minimizing the number of recommended classes and maximizing the relevance of the proposed solution using a multi-objective optimization algorithm. The relevance of the recommended classes (solution) is estimated based on the use of the history of changes and bug-fixing, and the lexical similarity between the bug report description and the API documentation. We evaluated our system on 6 open source Java projects, using the version of the project before fixing the bug of many bug reports. The experimental results show that the search-based approach significantly outperforms three state-of-the-art methods in recommending relevant files for bug reports. In particular, our multi-objective approach is able to successfully locate the true buggy methods within the top 10 recommendations for over 87% of the bug reports.	algorithm;bug tracking system;documentation;java;mathematical optimization;multi-objective optimization;open-source software;relevance;software bug	Rafi Almhana;Mohamed Wiem Mkaouer;Marouane Kessentini;Ali Ouni	2016	2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1145/2970276.2970344	software bug;documentation;search-based software engineering;computer science;software development;software engineering;data mining;database;programming language;software maintenance;software regression;world wide web;software system	SE	-59.17215918395717	35.13809603482996	145831
a700fd39637ffeb26470980e04242e8016b2fe92	a study on design of fmt framework for flex quality assurance	databases;quality assurance;flexible printed circuits business maintenance engineering internet xml databases servers;rich internet application;maintenance;xml database;flex;maintenance engineering;servers;internet;maintenance flex compatibility;business;xml;compatibility;configuring user screen rich internet application fmt framework flex quality assurance flex multitier framework web application adobe;software quality;software quality internet quality assurance;flexible printed circuits	The paradigm of RIA (Rich Internet Application) emerged since users¡® tendency to use web applications was changed for the sake of convenience, and the Flex was presented as a platform to develop the RIA with the advangtage of prevalance by the Adobe. The Flex is mainly composed of configuring user screens, which has insufficient compatibility and maintenance due to its unified layer. Accordingly, this paper woudl design and implement a FMT (Flex Mlti-Tier) framework to improve compatibility and maintenance of the Flex.	multitier architecture;programming paradigm;rich internet application;web application	Yang-Hoon Kim;Guk-Boh Kim;Hangbae Chang	2011	2011 14th International Conference on Network-Based Information Systems	10.1109/NBiS.2011.96	maintenance engineering;quality assurance;embedded system;the internet;xml;rich internet application;computer science;operating system;xml database;database;compatibility;world wide web;computer security;software quality;server	Robotics	-49.96714718871502	42.05156846147109	145858
25761645412cbe2fa96efec6db9ac07ca965aecd	modeling request routing in web applications	modeling request routing;server side components;web pages;routing pipelines web pages table lookup java information analysis performance analysis information filtering information filters runtime;maintenance cost;j2ee web application modeling request routing server side components pipeline architectural pattern;software architecture;internet;pipeline architectural pattern;source code;architectural pattern;j2ee web application;software architecture internet java;java;modeling and analysis	For Web applications, determining how requests from a Web page are routed through server components can be time-consuming and error-prone due to the complex set of rules and mechanisms used in a platform such as J2EE. We define request routing to be the possible sequences of server-side components that handle requests. Many maintenance tasks require the developer to understand the request routing, so this complexity increases maintenance costs. However, viewing this problem at the architecture level provides some insight. The request routing in these Web applications is an example of a pipeline architectural pattern: each request is processed by a sequence of components that form a pipeline. Communication between pipeline stages is event-based, which increases flexibility but obscures the pipeline structure because communication is indirect. Our approach for improving the maintainability of J2EE Web applications is to provide a model that exposes this architectural information. We use Z to formally specify request routing models and analysis operations that can be performed on them, then provide tools to extract request routing information from an application's source code, create the request routing model, and analyze it automatically. We have applied this approach to a number of existing applications up to 34K LOC, showing improvement via typical maintenance scenarios. Since this particular combination of patterns is not unique to Web applications, a model such as our request routing model could provide similar benefits for these systems	altered level of consciousness;architectural pattern;cognitive dimensions of notations;java platform, enterprise edition;pipeline (computing);routing;server (computing);server-side;web application;web page	Minmin Han;Christine Hofmeister	2006	2006 Eighth IEEE International Symposium on Web Site Evolution (WSE'06)	10.1109/WSE.2006.14	policy-based routing;routing table;software architecture;routing;enhanced interior gateway routing protocol;static routing;web modeling;real-time computing;the internet;equal-cost multi-path routing;architectural pattern;computer science;operating system;software engineering;web page;database;routing protocol;link-state routing protocol;programming language;java;world wide web;application server;source code	Arch	-56.4407750543093	42.27045917746106	145903
10facbf56739eecf1de8b1e37927d9a3e9dcab26	puzzle-based automatic testing: bringing humans into the loop by solving puzzles	open source projects puzzle based automatic testing environment automatic test generation techniques randoop pex jcute object mutation problem complex constraint solving problems pat;code coverage;automatic testing;testing;public domain software;testing code coverage human computation;program testing;public domain software program testing;test coverage;automatic test generation;test generation;human computation;constraint solving;open source	Recently, many automatic test generation techniques have been proposed, such as Randoop, Pex and jCUTE. However, usually test coverage of these techniques has been around 50-60% only, due to several challenges, such as 1) the object mutation problem, where test generators cannot create and/or modify test inputs to desired object states; and 2) the constraint solving problem, where test generators fail to solve path conditions to cover certain branches. By analyzing branches not covered by state-of-the-art techniques, we noticed that these challenges might not be so difficult for humans.   To verify this hypothesis, we propose a Puzzle-based Automatic Testing environment (PAT) which decomposes object mutation and complex constraint solving problems into small puzzles for humans to solve. We generated PAT puzzles for two open source projects and asked different groups of people to solve these puzzles. It was shown that they could be effectively solved by humans: 231 out of 400 puzzles were solved by humans at an average speed of one minute per puzzle. The 231 puzzle solutions helped cover 534 and 308 additional branches (7.0% and 5.8% coverage improvement) in the two open source projects, on top of the saturated branch coverages achieved by the two state-of-the-art test generation techniques.	constraint satisfaction problem;fault coverage;humans;open-source software	Ning Chen;Sunghun Kim	2012	2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering	10.1145/2351676.2351697	test data generation;simulation;computer science;theoretical computer science;software engineering;code coverage;programming language;algorithm	SE	-59.507485030458206	36.69756923910376	146132
468bef5a3f6acae34e6756c51aa1f0885510cb9b	practical syntactic error recovery	error recovery;error detection	A substantial portion of any programmer's time is spent in debugging. One of the major services of any compiler ought to be to provide as much information as possible about compile-time errors in order to minimize the time required for debugging. A good error detection and recovery scheme should maximize the number of errors detected but minimize the number of times it reports an error when there is none. These spurious error detections and their associated error messages are usually engendered by an inappropriate recovery action.In this paper we describe a recovery scheme for syntax errors which provides high quality recovery with good diagnostic information at relatively low cost. In addition, implementation of the recovery scheme can be automated - that is, the recovery routine can be created by a parser-generator. Therefore, the compiler designer need not be burdened with the difficulties of error recovery and the programming effort necessary to design and debug a myriad of ad hoc recovery routines.	compile time;compiler;compiler-compiler;debugging;display resolution;error detection and correction;error message;hoc (programming language);programmer;sensor;syntax error	Susan L. Graham;Steven P. Rhodes	1973		10.1145/512927.512932	parallel computing;real-time computing;error detection and correction;computer science;theoretical computer science;compilation error	DB	-51.83056705217337	37.214627656545346	146264
af07767774c56ab237b524c79d5d114619ddecc3	mining and discovery of hidden relationships between software source codes and related textual documents			code	Amir Hossein Rasekh;Amir Hossein Arshia;Seyed Mostafa Fakhrahmad;Mohammad Hadi Sadreddini	2018	DSH	10.1093/llc/fqx052	software mining;source code;data mining;software;computer science	ML	-57.25351564100827	33.526800676192146	146949
0eb030fcfa06b753327b9ba9faf0cb96edcf522f	statistical fault localization using execution sequence	graph theory;execution sequence statistical fault localization predicate hypothesis testing;statistical analysis fault diagnosis graph theory program debugging software fault tolerance;software fault tolerance;execution sequence;predicate;statistical analysis;abstracts;hypothesis testing;program debugging;statistical fault localization;statistical fault localization techniques execution sequence program debugging statistical approach statistical behavior sequentially connected predicates graph vertices sequential predicates execution trace hypothesis testing edge evaluation bias fault relevance score siemens suite predicate based fault localization method;fault diagnosis	Fault localization is one of the most expensive and time consuming jobs in program debugging. Many approaches were proposed in order to locate faults effectively and efficiently. In this paper, we proposed a novel statistical approach by exploiting the statistical behavior of two sequentially connected predicates in the execution. If the predicates are regarded as the vertices of a graph, then the edges of the graph represent the transition of two sequential predicates in the execution trace of the program. The label of each edge is the frequency of each transition. For each edge, we apply hypothesis testing to evaluate the difference between edge evaluation bias in the passed runs and that in the failed runs. The edges are ranked according to the fault relevance score obtained from the hypothesis testing. The experimental results on Siemens suite show that the our proposed predicate-based fault localization method outperforms other well-used statistical fault localization techniques.	debugging;relevance	Zunwen You;Zengchang Qin;Zheng Zheng	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6359473	statistical hypothesis testing;real-time computing;predicate;computer science;stuck-at fault;graph theory;theoretical computer science;algorithm;software fault tolerance;statistics	SE	-60.708309276560186	36.13274192679812	147366
ab21d8eb98d797e9daa8bf658e2f808ea3841bcc	finding bugs in java native interface programs	typestate analysis;resource manager;java native interface;exception handling;static analysis	In this paper, we describe static analysis techniques for finding bugs in programs using the Java Native Interface (JNI). The JNI is both tedious and error-prone because there are many JNI-specific mistakes that are not caught by a native compiler. This paper is focused on four kinds of common mistakes. First, explicit statements to handle a possible exception need to be inserted after a statement calling a Java method. However, such statements tend to be forgotten. We present a typestate analysis to detect this exception-handling mistake. Second, while the native code can allocate resources in a Java VM, those resources must be manually released, unlike Java. Mistakes in resource management cause leaks and other errors. To detect Java resource errors, we used the typestate analysis also used for detecting general memory errors. Third, if a reference to a Java resource lives across multiple native method invocations, it should be converted into a global reference. However, programmers sometimes forget this rule and, for example, store a local reference in a global variable for later uses. We provide a syntax checker that detects this bad coding practice. Fourth, no JNI function should be called in a critical region. If called there, the current thread might block and cause a deadlock. Misinterpreting the end of the critical region, programmers occasionally break this rule. We present a simple typestate analysis to detect an improper JNI function call in a critical region.  We have implemented our analysis techniques in a bug-finding tool called BEAM, and executed it on opensource software including JNI code. In the experiment, our analysis techniques found 86 JNI-specific bugs without any overhead and increased the total number of bug reports by 76%.	cognitive dimensions of notations;compiler;deadlock;exception handling;global variable;grammar checker;java virtual machine;machine code;open-source software;overhead (computing);programmer;sensor;software bug;static program analysis;typestate analysis	Goh Kondoh;Tamiya Onodera	2008		10.1145/1390630.1390645	exception handling;real-time computing;computer science;resource management;software engineering;database;programming language;java;static analysis;java annotation	PL	-57.4771602459813	39.46762117864791	147374
29181f7bd7a5b182dbe443634e64632ce3167de6	a market-based bug allocation mechanism using predictive bug lifetimes	developer agent;bug repositories;machine learning algorithms;bug lifetime;sample size;multiagent system;bug repositories multiagent system market mechanism bug lifetime;software project;backlog minimization;resource management;prediction algorithms;contextual information;firefox bug repository;auction based multiagent mechanism;firefox bug repository market based bug allocation mechanism predictive bug lifetime bug assignment software project auction based multiagent mechanism backlog minimization intelligent software agent developer agent triager agent eclipse bug repository;data mining;market mechanism;triager agent;accuracy;computer viruses;multi agent systems;multi agent systems computer viruses;bug assignment;computer bugs fires data mining prediction algorithms machine learning algorithms resource management accuracy;fires;computer bugs;market based bug allocation mechanism;historical data base;predictive bug lifetime;intelligent software agent;eclipse bug repository	Bug assignment in large software projects is typically a time-consuming and tedious task, effective assignment requires that bug triagers hold significant contextual information about both the reported bugs and the pool of available developers. In this paper, we propose an auction-based multiagent mechanism for assigning bugs to developers that is intended to minimize backlogs and overall bug lifetime. In this approach, developers and triagers are both modeled as intelligent software agents working on behalf of individuals in a multiagent environment. Upon receiving a bug report, triager agents auction off the bug and collect the requests. Developer agents compute their bids as a function of the developer's profile, preferences, current schedule of assigned bugs, and estimated time-to-fix of the bug. This value is then sent to the triager agent for the final decision. We use the Eclipse and Firefox bug repositories to validate our approach, our studies suggest that the proposed auction-based multiagent mechanism can improve the bug assignment process compared to currently practised methods. In particular, we found a 16% improvement in the number of fixed bugs compared to the historic data, based on a sample size of 213,000 bug reports over a period of 6 years.	agent-based model;algorithm;bug tracking system;categorization;data mining;eclipse;firefox;intelligent agent;scheduling (computing);software agent;software bug;utility	Hadi Hosseini;Raymond Nguyen;Michael W. Godfrey	2012	2012 16th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2012.25	sample size determination;real-time computing;software bug;prediction;computer science;resource management;data mining;accuracy and precision;software regression;world wide web;computer virus	SE	-62.768795087821715	39.73662272756823	147401
5d9e88c856b33260f7c0be517d2987ed6f1dd69a	using an information retrieval system to retrieve source code samples	information retrieval system;learning curve;information retrieval;source code repository;search;software development;source code;organization development	Software developers often face steep learning curves in using a new framework, library, or new versions of frameworks for developing their piece of software. In large organizations, developers learn and explore use of frameworks, rarely realizing, several peers may have already explored the same. A tool that helps locate samples of code, demonstrating use of frameworks or libraries would provide benefits of reuse, improved code quality and faster development. This paper describes an approach for locating common samples of source code from a repository by providing extensions to an information retrieval system. The approach improves the existing approaches in two ways. First, it provides the scalability of an information retrieval system, supporting search over thousands of source code files of an organization. Second, it provides more specific search on source code by preprocessing source code files and understanding elements of the code as opposed to considering code as plain text.		Renuka Sindhgatta	2006		10.1145/1134285.1134448	kpi-driven code analysis;codebase;code review;computer science;software development;data mining;internal documentation;learning curve;legacy code;world wide web;organization development;information retrieval;code refactoring;code generation;static program analysis;source code	SE	-60.190571497264536	39.73771899842422	148095
1274243ea33d6b2db2c43c617302cfa014951fbc	abstracting plan-like program information: a demonstration	logical relation;program understanding;logically related program information;software fault diagnosis software maintenance;program diagnostics;view composition;software fault diagnosis;call graph;software maintenance;program views;graphical representations;plan like program information abstraction;abstract syntax tree;global variable occurrence pattern;program modification;software maintenance program diagnostics;graphical representation;abstract level;view refinement;program analysis;program effects;control dependence;view refinement plan like program information abstraction program understanding program modification program views program analysis graphical representations abstract level logically related program information program effects global variable occurrence pattern interactive manipulation view composition;interactive manipulation	The U.S. Department of Justice, Office of Justice Programs, National Institute of Justice is seeking applications from State and local law enforcement agencies or regional law enforcement information centers to conduct demonstrations of predictive policing methods in practice. Law enforcement agencies will be expected to develop and demonstrate predictive policing models that can be subjected to program evaluation to determine criminal justice outcomes and impacts. This program furthers the Department’s mission by improving the safety and effectiveness of criminal justice technology and law enforcement and other criminal justice agencies’ access to that chnology. te		Eng-Siong Tan;Henry G. Dietz	1994		10.1109/ICSM.1994.336768	program analysis;call graph;program evaluation;computer science;theoretical computer science;database;programming language;software maintenance;abstract syntax tree	ML	-53.8617179452679	35.893741130446806	148306
07e26a9df3ab50ae45035e5ca911b47248feb5d8	coclac - feedback generation for combined uml class and activity diagram modeling tasks		We introduce a newly developed visual programming and UML modeling tool for educational use. It’s implemented to provide students with informative feedback during exercises as well as to assess the submissions of students automatically. Tasks combine class and activity modeling aspects as well as some simple programming requirements. The system is web-based and named COCLAC. Visual programming is done through an included UML editor. Created diagrams are syntactically checked and automatically converted into Java code using several conventions. Thereafter, code is executable on a server and semantic correctness can be checked through automatic tests.	activity diagram;automatic taxonomy construction;correctness (computer science);executable;information;java;requirement;server (computing);uml tool;unified modeling language;visual programming language;web application	Philip-Daniel Beck;Thomas Mahlmeister;Marianus Ifland;Frank Puppe	2015			psychology;programming language;clinical psychology;human–computer interaction;visual programming language;applications of uml;unified modeling language;activity diagram;uml tool;class diagram;executable;java	HCI	-51.93019708684458	33.57641027876789	148364
91f81ebaffe183cf413423e0c189567321e6f517	using static analysis to find bugs	developpement logiciel;software;detectors;code quality;software development open source static analysis tool security violations sql injection runtime errors findbugs java;analyse statique;defecto;sql;software defects;open source static analysis tool;langage java;program verification;software engineering;security violations;analisis estatica;public domain software;verificacion programa;desarrollo logicial;sql injection;runtime errors;defect;logiciel libre;software development;defaut;interviews;software quality static analysis findbugs code quality bug patterns software defects;findbugs;software libre;lenguaje java;book reviews;static analysis tools;static analysis;verification programme;computer bugs;programming;security of data;software quality;sql java public domain software security of data software engineering;computer bugs java software tools testing security educational institutions open source software software quality production programming;open source software;java language;java;open source;bug patterns	Static analysis examines code in the absence of input data and without running the code. It can detect potential security violations (SQL injection), runtime errors (dereferencing a null pointer) and logical inconsistencies (a conditional test that can't possibly be true). Although a rich body of literature exists on algorithms and analytical frameworks used by such tools, reports describing experiences in industry are much harder to come by. The authors describe FindBugs, an open source static-analysis tool for Java, and experiences using it in production settings. FindBugs evaluates what kinds of defects can be effectively detected with relatively simple techniques and helps developers understand how to incorporate such tools into software development.	algorithm;dereference operator;findbugs;java;open-source software;pointer (computer programming);run time (program lifecycle phase);sql injection;software bug;software development;static program analysis	Nathaniel Ayewah;David Hovemeyer;J. David Morgenthaler;John Penix;William Pugh	2008	IEEE Software	10.1109/MS.2008.130	computer science;operating system;software engineering;database;programming language;software quality	SE	-57.342237381781835	38.368523306841674	148557
1717e32a1bf05e108b5dff527fe6bd4fb8b556d4	idiomata: direct manipulation of code through idiomatic views		Currently, front-end web developers spend countless hours overcoming programming challenges while debugging unexpected asynchronous behaviors, writing code to interact with a framework's API, or fixing faults. Such problems demand rethinking programming tools, and for that, we systematically analyzed 301 posts from Stack Overflow, and sought to identify the programming activities developers struggled with and find information barriers that were the primary cause of their challenges. Our results reveal that developers most commonly post questions to change behavior of existing code. Further, specific discussions or changes in code fragments that conceptually have the same intention (e.g. both document.querySelector(“x”) and $(“x”) retrieve references from the DOM), often presented reoccurring challenges that demanded different reasoning to obtain equivalent runtime information (e.g. check if a DOM reference matches what is intended in the output). Code idioms are those concepts underlying patterns in code and by categorizing them, we were able to associate their challenges to execution information barriers that prevent developers from observing relevant state and behavior at runtime (e.g. mapping a DOM reference in a code fragment to its element in the output), and selection barriers that prevent developers from finding or choosing the correct object, method, or parameter values to achieve a desired behavior (e.g. generating code that references a DOM element).	application programming interface;categorization;competitive programming;debugging;direct manipulation interface;document object model;programming tool;run time (program lifecycle phase);stack overflow;web developer	David I. Samudio	2017	2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)	10.1109/VLHCC.2017.8103488	visualization;theoretical computer science;debugging;stack overflow;computer science;cognition;cascading style sheets;asynchronous communication;code (cryptography)	SE	-57.016247029650266	38.10954665017562	148639
7b04b7f6a4b36fe406a1a98c1d4b66489db2f51a	detection of software anomalies using object-oriented metrics	software metrics;object oriented programming;ck metrics;error proneness;software anomalies	The development of quality software has always been the aim of many studies in past years, in which the focus was on seeking for better software production with high effectiveness and quality. In order to evaluate software quality, software metrics were proposed, providing an effective tool to analyze important features such as maintainability, reusability and testability. The Chidamber and Kemerer metrics (CK metrics) are frequently applied to analyze Object-Oriented Programming (OOP) features related to structure, inheritance and message calls. The main purpose of this article is to gather results from studies that used the CK metrics for source code evaluation, and based on the CK metrics, perform a review related to software metrics and the values obtained. Results on the mean and standard deviation obtained in all the studied papers is presented, both for Java and C++ projects. Therefore, software anomalies are identified comparing the results of software metrics described in those studies. This article contributes by suggesting values for software metrics that, according to the literature, can present high probabilities of failures. Another contribution is to analyze which CK metrics are successfully used (or not) in some activities such as to predict proneness error, analyze the impact of refactoring on metrics and examine the facility of white-box reuse based on metrics. We discovered that, in most of the studied articles, CBO, RFC and WMC are often useful and hierarchical metrics as DIT and NOC are not useful in the implementation of such activities. The results of this paper can be used to guide software development, helping to manage the development and preventing future problems.	c++;code refactoring;cognitive dimensions of notations;directory information tree;java;library (computing);multiple inheritance;network on a chip;software bug;software development;software metric;software quality;software testability;verification and validation;windows media center	Renato Correa Juliano;Bruno A. N. Travençolo;Michel S. Soares	2014		10.5220/0004889102410248	halstead complexity measures;computer science;data mining;database;programming language;object-oriented programming;software metric	SE	-62.74479308880417	34.953979396328805	148716
34a3ea35aeaa9420948ce83709cd7fea2e37b6e5	nhancing cognitive aspects of software visualization using doclike modularized graph		Understanding an existing software system to trace possible changes involved in a maintenance task can be time consuming especially if its design document is absence or out-dated. In this case, visualizing the software artefacts graphically may improve the cognition of the subject system by software maintainers. A number of tools have emerged and they generally consist of a reverse engineering environment and a viewer to visualize software artefacts such as in the form of graphs. The tools also grant structural re-documentation of existing software systems but they do not explicitly employ document-like software visualization in their methods. This paper proposes DocLike Modularized Graph method that represents the software artefacts of a reverse engineered subject system graphically, module-by-module in a document-like re-documentation environment. The method is utilized in a prototype tool named DocLike viewer that generates graphical views of a C language software system parsed by a selected C language parser. Two experiments were conducted to validate how much the proposed method could improve cognition of a subject system by software maintainers without documentation, in terms of productivity and quality. Both results deduce that the method has the potential to improve cognitive aspects of software visualization to support software maintainers in finding solutions of assigned maintenance tasks.	code;cognition;documentation;experiment;graph (abstract data type);graph drawing;graphical user interface;parsing;program comprehension;programmer;prototype;reverse engineering;software design description;software engineer;software system;software visualization;systemverilog	Shahida Sulaiman;Norbik Bashah Idris;Shamsul Sahibuddin	2005	Int. Arab J. Inf. Technol.			SE	-55.43960775705658	34.870752809570796	148728
a9ba623dc589e35a5987890f7f168c8f9ed8d533	automatic code generation: model-code semantic consistency	testing.;validation & verification;automatic code generation;software tools;cots;code generation	Automatic translation or code generation of software models to code may help alleviate problems associated with manual coding effort. This paper emphasizes the importance of attaining a high level of assurance that the process of automatically translating model to code is correct. It illustrates modeling experiments performed using Statemate (iLogix) to establish a correspondence between model elements and code constructs. The research is a step towards achieving assurance of semantic consistency between model and generated code.	automatic programming;code generation (compiler);consistency model;experiment;high-level programming language	Andrew J. Kornecki;Sona Johri	2006			kpi-driven code analysis;unreachable code;redundant code;source code;theoretical computer science;code generation;programming language;software;static program analysis;computer science;object code	SE	-50.724091035220134	33.25024391238876	148862
4e33a69a075db235467356a1ca7a2a753ed5fd87	a refactoring suggestion tool for removing clones in java code	settore inf 01 informatica;settore ing inf 05 sistemi di elaborazione delle informazioni		code refactoring;java	Francesca Arcelli Fontana;Marco Zanoni;Francesco Zanoni	2014	ERCIM News		computer science;operating system;programming language;algorithm	SE	-56.9334036662777	36.040096249567476	148992
0a2648504ba7d3d61f76ad3f798d92d243b897c6	control and data flow execution of java programs		Background and Objective: Since decades understanding of programs has become a compulsory task for the students as well as for others who are involved in the process of developing software and providing solutions to open problems. In that aspect showing the problem in a pictorial presentation in a best manner is a key advantage to better understand it. Materials and Methods: This article provides model and structure for Java programs to understand the control and data flow analysis of execution. Especially it helps to understand the static analysis of Java programs, which is an uttermost important phase for software maintenance. This article provided information and model for visualization of Java programs that may help better understanding of programs for a learning and analysis purpose. The idea provided for building visualization tool is extracting data and control analysis from execution of Java programs. Results: Theoretically, this article has shown how to extract source code and provide information of data and control graph. It is helpful for understanding of programs and may help towards software debugging and maintenance process. Conclusions: This article presented case studies to prove that our idea is most important for better understanding of Java programs which may help towards static analysis, software debugging and software maintenance.	data-flow analysis;dataflow architecture;debugging;image;java;objective-c;out-of-order execution;software maintenance;static program analysis	Safeeullah Soomro;Zainab Alansari;Mohammad Riyaz Belgaum	2017	CoRR	10.3923/ajsr.2017	systems engineering;electronic engineering;engineering;debugging;real time java;real-time computing;scala;java annotation;java concurrency;java modeling language;strictfp;java	SE	-56.281479264621204	35.98524646295355	149018
c99c5ec61bbd6a9caab4b367ae07f0d2a48ed4da	a case study in sequential pattern mining for it-operational risk	pre and post processing;software systems;operational risk;data mining;sequential pattern mining;sequential pattern	IT-operational risk management consists of identifying, assessing, monitoring and mitigating the adverse risks of loss resulting from hardware and software system failures. We present a case study in IT-operational risk measurement in the context of a network of Private Branch eXchanges (PBXs). The approach relies on preprocessing and data mining tasks for the extraction of sequential patterns and their exploitation in the definition of a measure called  expected risk .	sequential pattern mining	Valerio Grossi;Andrea Romei;Salvatore Ruggieri	2008		10.1007/978-3-540-87479-9_46	sequential pattern mining;computer science;data science;data mining;operational risk;software system	ML	-60.64945257050774	45.91729989270563	149039
4d79ad8d16a4e08bd233cb37c9d75e0241ca3bfe	promises: limited specifications for analysis and manipulation	encapsulation;distributed development;systems analysis formal specification;formal specification;formal specifications;limited specifications;software systems;data mining;structural change;promises;mechanical factors;semantic information;systems analysis;unique references limited specifications systems analysis distributed development process annotation mechanism promises automatic analysis tools;programming profession;computer science data mining us government encapsulation mechanical factors formal specifications programming profession costs information analysis software systems;automatic analysis tools;unique references;annotation mechanism;separate compilation;computer science;information analysis;us government;distributed development process	Structural change in a large system is hindered when information is missing about portions of the system, as is often the case in a distributed development process. An annotation mechanism called promises is described for expressing properties that can enable many kinds of structural change in systems. Promises act as surrogates for an actual component, and thus are analogous to “header” files, but with more specific semantic information. Unlike formal specifications, however, promises are designed to be easily extracted from systems and managed by programmers using automatic analysis tools. Promises are described for effects, unique references, and use properties. By using promises, a component developer can offer additional opportunity for change (flexibility) to clients, but at a potential cost in flexibility for the component itself. This suggests the possibility of using promises as a means to allocate flexibility among the components of a system.	formal specification;programmer;surrogates	Edwin C. Chan;John Tang Boyland;William L. Scherlis	1998		10.1109/ICSE.1998.671113	computer science;systems engineering;engineering;operating system;software engineering;formal specification;database;programming language	SE	-51.542556756446515	34.796764175491866	149228
0f78323dc7668ecf0969a1bacd5c78000d1d9866	ripple: a test-aware architecture modeling framework		Different architecture views can be used to address concerns of different stakeholders. While architecture models have been used to guide software detailed design, development, and maintenance, no existing work has incorporated information generated in testing activities into architecture models for providing testing guidance. In this paper, we present Ripple, the framework for constructing test-aware DRSpace modeling to simultaneously reveal dynamic test coupling and static structural dependencies among source files in a software system. Ripple first mines from dynamic test coverage reports to extract traceability links between source files and test cases. It then encodes testing information into DRSpaces and leverages the DRH algorithm to cluster source files into independent test modules. To evaluate Ripple, we conducted a pilot study on a component of Hadoop. The study shows that Ripple has the potential to provide guidance for various stakeholders in making test-related decisions.	algorithm;apache hadoop;benchmark (computing);computable function;extensibility;fault coverage;parallel computing;ripple;scalability;software system;software testability;test case;traceability	Lu Xiao;Tingting Yu	2017	2017 IEEE/ACM 1st International Workshop on Establishing the Community-Wide Infrastructure for Architecture-Based Software Engineering (ECASE)	10.1109/ECASE.2017.2	enterprise architecture framework;software architecture;reference architecture;architecture;real-time computing;view model;dynamic testing;software architecture description;applications architecture;engineering	SE	-56.701121815593666	33.41856057312312	149566
28c222cd0e554865ab4d78ec63014e9b1371fa9c	number of faults per line of code	software science;logic arrays;application software;vocabulary;fault prediction;programmable logic arrays;upper bound;lines of code;software science fault prediction fault rate program complexity;programming profession;computer bugs application software programming profession vocabulary arithmetic logic arrays programmable logic arrays upper bound;arithmetic;fault rate;program complexity;computer bugs	"""In this note, the number of faults or """"bugs"""" per line of code is estimated based upon Halstead's software science relationships. This number is shown to be an increasing function of the number of lines of code in a program, a result in agreement with intuition and some current theories of complexity. The form of this function is investigated and an easy-to-use approximation is developed. An application to a moderately large software project is shown in which the predicted number of faults for program modules of various sizes agrees fairly well with the actual numbers of faults discovered."""	approximation;halstead complexity measures;software bug;software project management;source lines of code;theory	M. Lipow	1982	IEEE Transactions on Software Engineering	10.1109/TSE.1982.235579	application software;software bug;computer science;theoretical computer science;operating system;software engineering;code coverage;upper and lower bounds;programming language;source lines of code;algorithm;software fault tolerance;static program analysis;computer engineering	SE	-61.97295794631426	33.69780661803569	149638
0087000cbe651299db8034028426dd774cd2c460	using evowave to analyze software evolution	software engineering;software evolution;software comprehension;software visualization	Software evolution produces large amounts of data which software engineers need to understand for their daily activities. The use of software visualization constitutes a promising approach to help them comprehend multiple aspects of the evolving software. However, portraying all the data is not an easy task as there are many dimensions to the data (e.g. time, files, properties) to be considered. This paper presents a new software visualization metaphor inspired by concentric waves, which gives information about the software evolution in different levels of detail. This new metaphor is able to portray large amount of data and may also be used to consider different dimensions of the data. It uses the concepts of the formation of concentric waves to map software evolution data generated during the waves formation life cycle. The metaphor is useful for exploring and identifying certain patterns in the software evolution. To evaluate its applicability, we conducted an exploratory study to show how the visualization can quickly answer different questions asked by software engineers when evolving their software.	bug tracking system;color;control system;correctness (computer science);palette (computing);prototype;software engineer;software evolution;software visualization;surround sound;version control	Rodrigo Chaves Magnavita;Renato Lima Novais;Manoel G. Mendonça	2015		10.5220/0005373901260136	software visualization;personal software process;software sizing;computer science;package development process;software evolution;software design;social software engineering;theoretical computer science;software framework;software development;software design description;software engineering;software construction;software walkthrough;software analytics;software metric;software system	SE	-55.57874704199533	34.5166486678251	149991
2b4b0e37e64fe7a04a979c3f6c002f84bf7ba8ee	test data combination strategy for effective test suite generation	software robot sensing systems software testing software algorithms fault detection redundancy;program testing;fault detection test data combination strategy test suite generation software testing;fault diagnosis;program testing fault diagnosis	It is important to reduce the size of a test suite in order to reduce the cost and time in software testing. Teat data combination strategy is important because the size of a test suite is different by test data combination strategies even if their fault detection rates are equal. But in many cases, combination strategies include redundancy because they do not consider relationship between inputs and outputs of software under test. Also, some combination strategies considering relationship between inputs and outputs do not reduce the size of a test suite enough. In this paper, we propose test data combination strategy that improves existing test data combination strategy considering relationship between inputs and outputs. Also, we compared our strategy with existing test data combination strategies in robot software, and the experimental result shows the advantage of our algorithm.	algorithm;fault detection and isolation;redundancy (engineering);robot software;software testing;test data;test suite	Jae Hoon Yoon;Jeong Seok Kang;Hong Seong Park	2013	2013 International Conference on IT Convergence and Security (ICITCS)	10.1109/ICITCS.2013.6717891	reliability engineering;regression testing;test data generation;real-time computing;model-based testing;fault coverage;white-box testing;manual testing;engineering;software reliability testing;automatic test pattern generation;test compression;test suite;data mining;software testing;system under test;test case;test management approach;test harness	SE	-60.53454753576394	34.87555722178829	150002
b83d8f55b0c41946c99c0828d04dfa6173e1ee79	tool support for statistical testing of software components	tool support;statistical test;object oriented programming;automatic programming;automatic generation;test case generation;automatic programming object oriented programming program testing statistical testing;program testing;software component;statistical testing;behaviour checking version tool support statistical testing software component stsc prototype tool operational profiles test oracles test case generation automatic test driver generation;test oracle;statistical analysis software tools software testing software prototyping automatic testing usability software engineering probability information technology australia	"""We describe the """"STSC"""" prototype tool that supports the statistical testing of software components. The tool supports a wide range of operational profiles and test oracles for test case generation and output evaluation. The tool also generates appropriate values for different types of input parameters of operations. STSC automatically generates a test driver from an operational profile. This test driver invokes a test oracle that is implemented as a behaviour-checking version of the implementation. To evaluate the flexibility and usability of the tool, it has been applied to several case studies using different types of operational profiles and test oracles."""	application programming interface;component-based software engineering;formal specification;human error;markov chain;markov model;oracle (software testing);oracle machine;parameter (computer programming);prototype;scalability;scientific time sharing corporation;test case;usability	Rakesh Shukla;Paul A. Strooper;David A. Carrington	2005	12th Asia-Pacific Software Engineering Conference (APSEC'05)	10.1109/APSEC.2005.105	non-regression testing;test strategy;keyword-driven testing;reliability engineering;statistical hypothesis testing;regression testing;test data generation;model-based testing;software performance testing;white-box testing;manual testing;system integration testing;computer science;systems engineering;acceptance testing;software reliability testing;software construction;test suite;smoke testing;software testing;programming language;test script;test case;test management approach;test harness	SE	-60.93202274096252	32.341583979990716	150183
f51ac8478a06b05e06f7aecbaf48a39b1f7d24f4	essential asp.net2	essential asp net with examples in visual basic net;asp;essential asp net with examples in c;onion;net;visual basic;books visual basic application specific processors programming profession web pages computer languages data security buildings;c;essential asp net with examples in visual basic net asp net c visual basic essential asp net with examples in c	A review of Essential ASP.NET with Examples in C# and Essential ASP.NET with Examples in Visual Basic .NET by Fritz Onion.	asp.net;visual basic;visual basic[.net]	Zalech Zalech	2004	IEEE Distributed Systems Online	10.1109/MDSO.2004.36	computer science;data mining;algorithm;net	EDA	-57.51424126259455	40.70278346846107	150257
0891879335d702b7101030bc55c08ef5975e0afb	asynchrony-aware static analysis of android applications	analytical models;androids;semantics;humanoid robots;computer science;benchmark testing;automation	Software applications developed for the Android platform are very popular. Due to this, static analysis of these applications has received a lot of attention recently. An Android application is essentially an asynchronous, event-driven program. The Android framework manages the state of the application by invoking callbacks, called lifecycle callbacks, in pre-defined orders. Unfortunately, the existing static analysis techniques treat the callbacks synchronously. Additionally, they do not model all possible orderings of lifecycle callbacks. These may result in unsound analysis results. In this work, we present a precise representation of control flow of Android applications called Android inter-component control flow graph (AICCFG). In this representation, the asynchronous nature of the callbacks is modeled accurately. Further, all interleavings of callbacks of different components of an Android application are modeled in AICCFG. We use this representation to design a typestate analysis of Android applications. Android applications use a rich set of resources such as camera and media player whose safe usage is governed by some state machines. Using the typestate analysis, we can verify whether an application uses a resource safely or not. We have implemented the construction of AICCFG and the typestate analysis in the Soot framework. We have also implemented a variant of typestate analysis which uses the unsound control flow model used commonly in the literature. To compare our AICCFG based analysis with this, we present a benchmark of Android applications called AsyncBench. It comprises applications that use various resources in both safe and unsafe manner. The experiments over this benchmark demonstrate the benefits of our more precise control flow model and the typestate analysis.	android;asynchrony (computer programming);benchmark (computing);callback (computer programming);control flow graph;data-flow analysis;digital camera;event-driven architecture;experiment;scalability;static program analysis;typestate analysis	Ashish Mishra;Aditya Kanade;Y. N. Srikant	2016	2016 ACM/IEEE International Conference on Formal Methods and Models for System Design (MEMOCODE)	10.1109/MEMCOD.2016.7797761	benchmark;real-time computing;simulation;computer science;humanoid robot;theoretical computer science;automation;software engineering;semantics;programming language	SE	-55.4868585938995	39.053973097544876	150794
41573c1618a7bbbd7790bea4e367d28693aabb05	incremental strategy for applying mutation operators emphasizing faults difficult to be detected by automated static analyser		To ensure software quality, we can use static and dynamic analysis techniques. Both have advantages and disadvantages and should be used together to improve their performance. In this paper, we present a strategy for applying a set of mutation operators for software testing, which represents a dynamic technique, based on the difficulty an automated static analyzer has on detecting their modeled faults. In other words, we investigated which sets of faults, represented by mutation operators, an automated static analyzer was able to recognize and prioritize the mutation testing considering only the set of mutation operators whose set of faults are difficult to be detected by such static analyzer. We compare our set of mutation operators with others, and the statical analysis shows no difference in the mutation score and costs regarding the number of generated and equivalent mutants among the different strategies. Nevertheless, we consider our proposal attractive once it uses operators with lower overlapping with faults detected by the automated static analyzer we have used.	mutation testing;sensor;software quality;software testing;static program analysis	Vinícius Barcelos Silva;Cláudio Antônio de Araújo;Edmundo Sérgio Spoto;Auri Marcelo Rizzo Vincenzi	2017		10.1145/3131151.3131169	reliability engineering;operator (computer programming);software quality;analyser;spectrum analyzer;static analysis;software;mutation testing;computer science	SE	-59.90800379634623	35.65186298411821	150917
8dbb0a69290c6c3c0ae02a76d5ffdcc0c2977cb8	an automated framework for software test oracle	automated testing;mutation testing;software testing;software test oracle;i o relationship analysis;qa75 electronic computers computer science;artificial neural networks;test oracle;artificial neural network	Context: One of the important issues of software testing is to provide an automated test oracle. Test oracles are reliable sources of how the software under test must operate. In particular, they are used to evaluate the actual results that produced by the software. However, in order to generate an automated test oracle, oracle challenges need to be addressed. These challenges are output-domain generation, input domain to output domain mapping, and a comparator to decide on the accuracy of the actual outputs. Objective: This paper proposes an automated test oracle framework to address all of these challenges. Method: I/O Relationship Analysis is used to generate the output domain automatically and MultiNetworks Oracles based on artificial neural networks are introduced to handle the second challenge. The last challenge is addressed using an automated comparator that adjusts the oracle precision by defining the comparison tolerance. The proposed approach was evaluated using an industry strength case study, which was injected with some faults. The quality of the proposed oracle was measured by assessing its accuracy, precision, misclassification error and practicality. Mutation testing was considered to provide the evaluation framework by implementing two different versions of the case study: a Golden Version and a Mutated Version. Furthermore, a comparative study between the existing automated oracles and the proposed one is provided based on which challenges they can automate. Results: Results indicate that the proposed approach automated the oracle generation process 97% in this experiment. Accuracy of the proposed oracle was up to 98.26%, and the oracle detected up to 97.7% of the injected faults. Conclusion: Consequently, the results of the study highlight the practicality of the proposed oracle in addition to the automation it offers. 2011 Elsevier B.V. All rights reserved.	artificial neural network;comparator;experiment;input/output;modular programming;mutation testing;oracle (software testing);oracle database;oracle machine;programming paradigm;regression testing;reinforcement learning;software testing;supervised learning;system under test;test automation;test case;unsupervised learning	Seyed Reza Shahamiri;Wan M. N. Wan-Kadir;Suhaimi Ibrahim;Siti Zaiton Mohd Hashim	2011	Information & Software Technology	10.1016/j.infsof.2011.02.006	oracle;computer science;data mining;database;mutation testing;software testing;oracle unified method;programming language;test case;artificial neural network;algorithm	SE	-60.710840245604246	34.366390564110546	151059
87d230a15f5705f9f9409cbb44da27eae9e4d0b1	on some characterisation problems of subdomain testing	software testing;software systems;random testing;software engineering;software quality	Subdomain testing is a very general approach to the selection of test cases. It captures the characteristics of testing strategies that require the test suite to cover some predefined testing requirements. This paper attempts to characterise precisely the failure distributions for the best and worst case of any given subdomain testing strategy. Our analysis has revealed some crucial factors and principles that affect the effectiveness of subdomain testing strategies.	best, worst and average case;requirement;test case;test suite	Tsong Yueh Chen;Yuen-Tak Yu	1996		10.1007/BFb0013485	non-regression testing;random testing;reliability engineering;verification and validation;regression testing;software sizing;software performance testing;system integration testing;software verification;computer science;systems engineering;acceptance testing;package development process;social software engineering;software reliability testing;software development;software engineering;software construction;software testing;software measurement;software deployment;software quality;software system	SE	-60.84348135958362	33.32271951280045	151087
f606b9450ef8d84c5beb492deac457ffa18a571f	tess: automated support for the evolution of persistent types	compound type changes;persistent data evolution;software maintenance;abstract data types;type evolution software system;object oriented programming;computer aided software engineering;compound type changes tess type evolution software system persistent types persistent data evolution software maintenance;tess;object oriented programming software maintenance abstract data types object oriented databases computer aided software engineering;object oriented databases;persistent types;object oriented databases relational databases object oriented modeling database systems computer languages computer science eyes java software systems	Persistent data often has a long lifetime. During its lifetime, the types that are used to structure the data may undergo evolution to support new requirements or provide more efficient services. This evolution often makes the persistent data inaccessible unless it also evolves with the types. Existing systems that support type and data evolution focus on changes isolated to individual types, thereby limiting what can be easily accomplished during maintenance. We extend this work by presenting a model of compound type changes that can also describe changes simultaneously involving multiple types and their effects on data. We then describe Tess, a system to automate the evolution of types and their associated data when the types undergo compound type changes.	accessibility;composite data type;evolution;requirement;software maintainer;software maintenance;type system	Barbara Lerner	1997		10.1109/ASE.1997.632837	computer science;systems engineering;software development;software engineering;database;programming language;object-oriented programming;software maintenance;abstract data type;computer-aided software engineering	SE	-53.34932214353792	32.79982885967685	151250
239749cb6dfbedb8bdead9a546479b86005ef478	lazy symbolic evaluation and its path constraints solution	libraries;silicon;software engineering program control structures;programming language;lazy symbolic evaluation;input variables;probability density function;program control structures;path constraints solution;lazy evaluation;data mining;software engineering;indexes;symbolic execution;input variables concrete software engineering prototypes constraint theory programming computer languages software prototyping software testing debugging;symbolic execution lazy symbolic evaluation path constraints solution program structures;artificial intelligence;constraint solving;program structures	Some program structures in modern programming languages can not be reasoned about symbolically. Lazy symbolic evaluation as proposed in this paper introduces a lazy evaluation strategy into traditional symbolic execution in order to address the issue. Constraint variables in path constraints generated by lazy symbolic evaluation may be input or intermediate variables. To eliminate the latter, concrete values for related input variables are first obtained by constraints solving or searching processes. Then, the given path is executed again using concrete and symbolic values. The procedure is repeated until the resulting path constraint is on input variables alone. We have implemented a prototype tool and performed several experiments. Preliminary results show the feasibility of our approach.	control flow graph;experiment;lazy evaluation;programming language;prototype;scalability;symbolic execution;variable (computer science);whole earth 'lectronic link	Mengxiang Lin;Yin-li Chen;Kai Yu;Guo-shi Wu	2009	2009 ICSE Workshop on Automation of Software Test	10.1109/IWAST.2009.5069044	database index;probability density function;computer science;theoretical computer science;lazy evaluation;symbolic data analysis;silicon;programming language;symbolic trajectory evaluation;algorithm;lazy loading	SE	-59.294454926072504	36.176783628223504	151671
08d41f600eef14bb33c9797a2a76239ce6fc2fb6	improving usability of fault injection	usability fault injection tools eclipse ide;eclipse ide fault injection usability software development processes software fault injection sfi tool;eclipse ide;software tools programming environments software fault tolerance;usability;fault injection tools;fault tolerance fault tolerant systems usability testing hardware libraries	The lack of tools that can fit in existing development practices and processes hampers the adoption of Software Fault Injection (SFI) in real-world projects. This paper presents an ongoing work towards an SFI tool integrated in the Eclipse IDE, and designed for usability.	eclipse;fault injection;integrated development environment;usability	Domenico Cotroneo;Luigi De Simone;Antonio Ken Iannillo;Anna Lanzaro;Roberto Natella	2014	2014 IEEE International Symposium on Software Reliability Engineering Workshops	10.1109/ISSREW.2014.37	embedded system;usability;computer science;operating system;programming language;software fault tolerance	SE	-56.915631836050565	36.70168879855346	152016
7ea53599c80377c1d94fdb70d3ab7f78e1623199	experience mining google's production console logs	early experience;log mining technique;experience mining google;log parser;hadoop data;large scale production deployment;production data;log sanitization;production google system;log parsing	We describe our early experience in applying our console log mining techniques [19, 20] to logs from production Google systems with thousands of nodes. This data set is five orders of magnitude in size and contains almost 20 times as many messages types as the Hadoop data set we used in [19]. It also has many properties that are unique to large scale production deployments (e.g., the system stays on for several months and multiple versions of the software can run concurrently). Our early experience shows that our techniques, including source code based log parsing, state and sequence based feature creation and problem detection, work well on this production data set. We also discuss our experience in using our log parser to assist the log sanitization.	anomaly detection;apache hadoop;benchmark (computing);data mining;ground truth;parsing;resource contention;sanitization (classified information);sensor	Wei Xu;Ling Huang;Michael I. Jordan	2010			computer science;data mining;database;world wide web	OS	-61.16344888692832	41.98506071603845	152117
1afd1273953f5e8e0e02cf98042ef1c5cfdf9ef7	inspectj: program monitoring for visualisation using aspectj	uml sequence diagram;object oriented programming;algorithm animation;aspect oriented;sorting algorithm;domain specificity	Software is becoming increasingly complex. Visualisation, which presents a high level view of a system, can assist programmers in constructing, debugging and maintaining programs, as well as being a useful teaching aid. To create visualisations of running programs, it is necessary to have access to program run-time information. Traditional means of collecting program monitoring information suffer from a range of problems, from not being able to collect a wide enough range of information, to being too intrusive by requiring modification of existing tools and source files. We present InspectJ, which is a program visualisation system that uses AspectJ to collect program monitoring information for visualisation. AspectJ is a Java-based aspect-oriented language. Its advantages are that it is non-intrusive, and therefore does not require modification of existing tools or source files, and that it is easy to use, by virtue of being Java-based. To explore and evaluate AspectJ for collecting object-oriented program monitoring information, we used AspectJ pointcuts and advice to create various visualisations in InspectJ. These visualisations include UML sequence diagrams of running programs and algorithm animations of sorting algorithms. With InspectJ, we also created domain specific visualisations for a Library system written in Java.	aspectj	Rilla Khaled;James W Noble;Robert Biddle	2003			real-time computing;computer science;theoretical computer science;programming language	SE	-54.88381317484866	36.31760189907289	152217
c3a7f3d65e9c3542b5083828960c383eddbfa31c	achieving lightweight trustworthy traceability	traceability;safety critical	Despite the fact that traceability is a required element of almost all safety-critical software development processes, the trace data is often incomplete, inaccurate, redundant, conflicting, and outdated. As a result, it is neither trusted nor trustworthy. In this vision paper we propose a philosophical change in the traceability landscape which transforms traceability from a heavy-weight process producing untrusted trace links, to a light-weight results-oriented trustworthy solution. Current traceability practices which retard agility are cast away and replaced with a disciplined, just-in-time approach. The novelty of our solution lies in a clear separation of trusted trace links from untrusted ones, the change in perspective from `living-with' inacurate traces toward rigorous and ongoing debridement of stale links from the trusted pool, and the notion of synthesizing available `project exhaust' as evidence to systematically construct or reconstruct purposed, highly-focused trace links.	just-in-time compilation;requirements traceability;software development;tracing (software)	Jane Cleland-Huang;Mona Rahimi;Patrick Mäder	2014		10.1145/2635868.2666612	traceability;computer science;engineering;software engineering;data mining;internet privacy;computer security	SE	-59.83807236046716	45.631435267224646	152356
41b89df4ec53fa8a52f401908be55fb55efcecc2	a case study of domain-based program understanding	program understanding;computer program;synchronized refinement;source analysis;mosaic world wide web browser;systems re engineering reverse engineering internet online front ends software tools;application software;system requirements;software systems;mosaic;strontium;application domain analysis;online front ends;computer aided software engineering;internet;web sites;performance analysis;world wide web;software tools;domain analysis;reengineering case study domain based program understanding system requirements source code analysis application domain analysis synchronized refinement reverse engineering mosaic world wide web browser domain analysis software tools;source code analysis;reengineering;information analysis;computer aided software engineering application software reverse engineering information analysis strontium software systems web sites performance analysis software tools documentation;domain based program understanding;documentation;reverse engineering;systems re engineering	Program understanding relates a computer program to the goals and requirements it is designed to accomplish. Understanding techniques that rely only on source code analysis are limited in their ability to derive this relationship. Application-domain analysis is another source of information that can aid program understanding by guiding the source analysis and providing structure to its results. This paper describes the application of a domain-based program understanding process, Synchronized Refinement, to the problem of reverse engineering the Mosaic World Wide Web browser software. It discusses the domain analysis undertaken, the corresponding source code analysis we plan to perform, and the strengths and limitations of available automated tools.	application domain;computer program;domain analysis;information source;program comprehension;requirement;reverse engineering;static program analysis;world wide web	Richard Clayton;Spencer Rugaber;Lyman Taylor;Linda M. Wills	1997		10.1109/WPC.1997.601273	domain analysis;program analysis;mosaic;application software;system requirements;domain;strontium;documentation;computer science;systems engineering;operating system;software engineering;world wide web;reverse engineering;static program analysis	SE	-54.41768902972	33.665115320742004	152403
75807839a5f42f58a36a1bfae526707700528b72	debuggers and logging frameworks	developpement logiciel;debugging;puesta a punto programa;software testing;program diagnostics;software fault diagnosis;logging framework debugging breakpoint;bug finding system;program debugger;logging;debogage;breakpoint;program testing;program testing program debugging program diagnostics;desarrollo logicial;software development;bug finding system program debugger logging framework program testing program diagnostic;software debugging;program debugging;debugging assembly hardware instruments runtime program processors programming profession java computer bugs;program diagnostic;logging framework	The testing, diagnostic, and repair equipment of many professions is horrendously expensive. Assuming that the bug-finding systems the author discussed that program code a clean bill of health, our next alternatives for productively pinpointing errors that have crept into our code are debuggers or logging instrumentation. Our toolbag is full of useful debugging tools. Being an expert user of a debugger and a logging framework is a sign of professional maturity	capability maturity model;debugger;debugging	Diomidis Spinellis	2006	IEEE Software	10.1109/MS.2006.70	real-time computing;computer science;software development;operating system;software engineering;software testing;breakpoint;programming language;debugging;program animation;logging	SE	-61.57673435277705	33.4571605384811	152533
bf97ca34ee3d7ad04940ecc427b113097622ad89	system performance analysis with an ada process model development	ada;performance evaluation;resource allocation;adpe performance model;ada process model development;ccpds-r project;cpm;performance analysis;performance model;requirements compliance;system allocations;system design	Most ADPE performance models are used to support systems engineering analysis in proposals or the early phases of projects. These models verify performance requirements at a high level to validate system designs. Use of some of these models has continued through the life of a project analyzing the performance of the system as the design evolves. The Command Center Processing and Display System-Replacement (CCPDS-R) Performance Model has enjoyed an expanded role on the CCPDS-R project. The performance model has not only influenced the system design, but system allocations of resources as well. CCPDS-R performance analysis and requirements compliance insight have been substantially enhanced over the early phase of development. This paper will reflect on the benefits of this approach drawing on real world experience gained to date on CCPDS-R. 1. PROJECT BACKGROUND The Command Center Processing and Display System Replacement (CCPDS-R) program is a command and control system that provides missile warning information to various end users. The software for this project is being developed in Ada using a demonstration based, incremental development approach---the Ada Process Model. To capitalize on this new process, traditional software engineering activities have had to adapt. The system performance estimation activity was no exception. The performance model interacted with this new approach in a unique fashion. This uniqueness is not as a result of new modeling techniques, but because the Ada Process Model provides complementary information to the performance model as an indirect result of its approach. This gives the performance model something that prior projects have not had until after CDR, empirical software performance data. 2. THE Ada PROCESS MODEL Figure 1 is an overview of a generic definition of an incremental development derived from experience on CCPDS-R [Royce 1990]. This figure shows how incremental development feeds the demonstration milestones and other project areas. Royce describes in his paper how code is developed early, through partial implementation of software functions. This provides early insight into the functionality of the design. The main objective of the Ada Process Model (APM) is early implementation of software. This concept is exemplified by the fact that of the five software builds on CCPDS-R, four are completed prior to System CD/L 3. CCPDS-R PERFORMANCE MODEL DEVELOPMENT The CCPDS-R Performance Model (CPM) is exceptional in its use in concert with the APM and its interaction with the software testbed. The purpose of the CPM is to: (1) investigate system design issues in advance of software implementation, (2) establish performance allocations to the CSCIs, (3) provide performance predictions for the operational system configuration as the design evolves, (4) provide a measure to evaluate the performance of capability demonstrations, and (5) provide insight for design tradeoffs and proposed system upgrades. Figure 2 shows the relationship of the system performance model to a performance database. The performance database centrally houses performance information describing the system design. Contributions to this database include software engineering inputs, prototyping results, commercial or off-the-shelf (COTS) hardware and software performance data, operational performance data and testbed performance results. 3.1 Relationship of CPM to Program Testbeds The CCPDS-R Performance Prediction/Measurement Process uses three performance sources: (1) the CPM simulating CCPDS-R hardware and software architecture, (2) a prototype system testbed (Host or target based), and (3) the operational system testbed, running on the target hardware environment. The CPM is a simulation based upon a collection of transaction flow descriptions and a data base of performance characteristics of the processing performed by each transaction flow and the underlying hardware. (A transaction flow is the sequence of processing that occurs in response to some system stimulus such as an incoming sensor message or an operator input) The prototype testbed is a model of the processing that uses some stubbed versions of the CCPDS-R to top level tasks. These tasks perform the same message routing logic as their counterparts in the real system without necessarily performing the same computations. Instead, they may contain simple central processing unit (CPU) resource burning procedures that correspond to the current estimates of the resource requh'ements in the real system as defined by the software designers. The operational system testbed contains the turned over software that is used to verify the functional and performance requirements at the Functional Qualification Test (FQT) milestone. The CPM and the prototypes use a common data base of estimates of software resource requirements. They are updated with new design information as it becomes available. Consequently, both the fidelity and accuracy of these techniques increase throughout the system development process. The estimation methods complement each other. The CPM provides early visibility into the performance of the design. The prototype testbed provides a target environment for executing prototyped software. This testbed can then be used to provide data for the CPM to refine the model's fidelity. The target environment also provides empirical system data to the model (e.g., Operating System Overhead Factors).	ada;advanced power management;central processing unit;computation;computer;control system;database;global variable;high-level programming language;iterative and incremental development;operating system;operational system;performance prediction;process modeling;profiling (computer programming);prototype;r language;requirement;routing;simulation;software architecture;software engineering;software performance testing;system configuration;systems design;systems engineering;testbed;winston w. royce	Joseph A. Viceroy	1990			reliability engineering;real-time computing;ada;resource allocation;computer science;systems engineering;engineering;technical report;process modeling;computer performance;programming language;world wide web;systems design	SE	-50.641998616012145	37.53196990420609	152561
23ef52f39c49a0cb9dadb82de55c033057976c19	comparing persistent computing with autonomic computing	biology computing;concurrent computing;distributed computing;persistent computing;software engineering;computer vision;biology computing computer vision concurrent computing distributed computing design methodology computer architecture protection security engineering management software engineering;computer architecture;protection;engineering management;comparative study;ubiquitous computing;security;autonomic computing persistent computing;autonomic computing;design methodology	This paper presents a comparative study to examine the relationship between autonomic computing and persistent computing from the various aspects including motivation problems, ideas, purposes, goals, underlying principles, design methodologies, and architectures	autonomic computing	Jingde Cheng	2005	11th International Conference on Parallel and Distributed Systems (ICPADS'05)	10.1109/ICPADS.2005.110	concurrent computing;design methods;computer science;comparative research;distributed computing;ubiquitous computing;autonomic computing	HPC	-49.48385757516588	46.036696632930145	152938
c5046ee7e7cf975fd044d887f748f257716f5833	lintest: a development tool for testing dialogue systems	software development;test driven development;indexing terms;agile software development;computer science;unit testing	In this paper we present a development tool for testing dialogue systems. Testing software through the specification is important for software development in general and should be as automated as possible. For dialogue systems, the corpus can be seen as one part of the specification and the dialogue system should be tested on available corpora on each new build. The testing tool is inspired from work on agile software development methods, test driven development and unit testing, and can be used in two modes and during various phases of development.	agile software development;dialog system;dialog tree;test automation;test-driven development;text corpus;unit testing	Lars Degerstedt;Arne Jönsson	2006				SE	-56.66705654088546	32.33548822816867	153166
2c873b28efaba1c536d2fbc0f48118c632c8e3cc	model-based mutation testing of reactive systems		In this paper we give an overview of our work on combining model-based testing and mutation testing. Model-based testing is a black-box testing technique that avoids the labour of manually writing hundreds of test cases, but instead advocates the capturing of the expected behaviour in a model of the system-under-test. The test cases are automatically generated from this model. The technique is receiving growing interest in the embedded-systems domain, where models are the rule rather than the exception. Mutation testing is a technique for assessing and improving a test suite. A number of faulty versions of a program-under-test are produced by injecting bugs into its source code. These faulty programs are called mutants. A tester analyses if his test suite can ”kill” all mutants. We say that a test kills a mutant if it is able to distinguish it from the original. The tester improves his test suite until all faulty mutants get killed. In model-based mutation testing, we combine the central ideas of modelbased testing and mutation testing: we inject bugs in a model and generate a test suite that will kill these bugs. In this paper, we discuss its scientific foundations and tools. The foundations include semantics and conformance relations; the supporting tools involve model checkers, constraint solvers and SMT solvers.	algorithm;black-box testing;conformance testing;dependability;embedded system;fault model;fundamental fysiks group;model checking;model-based testing;mutation testing;satisfiability modulo theories;scalability;semantics (computer science);software bug;system under test;test case;test suite	Bernhard K. Aichernig	2013		10.1007/978-3-642-39698-4_2	machine learning;test case;mutation testing;test suite;artificial intelligence;computer science;reactive system	SE	-59.072992243571086	37.25003149432833	153204
ac0eac3b2a32a347ae1e759784a6d60812191dd1	bug localization using revision log analysis and open bug repository text categorization	machine learning;support vector machine	In this paper, we present a new approach to localize a bug in the software source file hierarchy. The proposed approach uses log files of the revision control system and bug reports information in open bug repository of open source projects to train a Support Vector Machine (SVM) classifier. Our approach employs textual information in summary and description of bugs reported to the bug repository, in order to form machine learning features. The class labels are revision paths of fixed issues, as recorded in the log file of the revision control system. Given an unseen bug instance, the trained classifier can predict which part of the software source file hierarchy (revision path) is more likely to be related to this issue. Experimental results on more than 2000 bug reports of ‘UI’component of the Eclipse JDT project from the initiation date of the project until November 24, 2009 (about 8 years) using this approach, show weighted precision and recall values of about 98% on average.	algorithm;belief revision;bug tracking system;categorization;data logger;document classification;eclipse;global variable;java;log analysis;machine learning;open-source software;precision and recall;revision control system;software bug;source lines of code;stemming;support vector machine	Amir H. Moin;Mohammad Khansari	2010			support vector machine;computer science;operating system;data mining;database;software regression;world wide web	SE	-62.001955337618085	39.37403143930042	153374
8d65c8c416fe0c86de75076b565dd26cc0834fac	sql query volume performance estimation tool	performance estimation;response time;estimation;high data volume;sql query performance	Typically, applications are tested on small data size for both functional and non functional requirements. However, in production environment, the applications, having SQL queries, may experience performance violations due to increase in data volume. There is need to have tool which could test SQL query performance for large data sizes without elongating application testing phase. In this paper, we have presented a tool for estimating SQL query execution time for large data sizes without actually generating and loading the large volume of data. The model behind the working of the tool has been validated with TPC-H benchmarks and industry applications to predict within 10% average prediction error. The tool is built using underlying popular open source project CoDD with better project management and user interfaces.	benchmark (computing);deployment environment;functional requirement;ibm tivoli storage productivity center;open-source software;run time (program lifecycle phase);sql;select (sql);software testing;user interface	Rekha Singhal;Chetan Phalak	2017		10.1145/3030207.3053663	sargable;query optimization;estimation;computer science;data science;data mining;database;response time;statistics	SE	-55.168063610549105	37.32254818283095	153842
5169d75995d181dab9a8c4b99114602bb403fa9b	reducing coverage collection overhead with disposable instrumentation	parallel machines java program testing software quality benchmark testing;stopping criteria;data collection;java virtual machine;quality assessment;program testing;parallel machines;software quality;benchmark testing;instruments probes java costs virtual machining computer science data engineering quality assessment benchmark testing fluid flow measurement;specjbb2000 benchmark quality assessment disposable coverage instrumentation instrumentation probe java virtual machine specjvm98 benchmark;java	"""Testers use coverage data for test suite quality assessment, stopping criteria definition, and effort allocation. However, as the complexity of products and testing processes increases, the cost of coverage data collection may grow significantly, jeopardizing its potential application. We present two techniques to mitigate this problem based on the concept of """"disposable coverage instrumentation"""": coverage instrumentation that is removed after its usage. The idea is to reduce coverage collection overhead by removing instrumentation probes after they have been executed. We have extended a Java virtual machine to support these techniques, and show their potential through empirical studies with the Specjvm98 and Specjbb2000 benchmarks. The results indicate that the techniques can reduce coverage collection overhead between 18% and 97% over existing techniques."""	coverage data;expect;instrumentation (computer programming);java virtual machine;machine code;overhead (computing);software deployment;test suite	Kalyan-Ram Chilakamarri;Sebastian G. Elbaum	2004	15th International Symposium on Software Reliability Engineering	10.1109/ISSRE.2004.32	embedded system;benchmark;real-time computing;computer science;operating system;strictfp;programming language;java;software quality;statistics;data collection	SE	-61.319677959337454	36.46156566465882	154060
15684058d73560590931596da9208804c2e14884	analyzing neighborhoods of falsifying traces in cyber-physical systems	sensitivity analysis;hypothesis testing;black box testing	"""We study the problem of analyzing falsifying traces of cyber-physical systems. Specifically, given a system model and an input which is a counterexample to a property of interest, we wish to understand which parts of the inputs are """"responsible"""" for the counterexample as a whole. Whereas this problem is well known to be hard to solve precisely, we provide an approach based on learning from repeated simulations of the system under test.  Our approach generalizes the classic concept of """"one-at-a-time"""" sensitivity analysis used in the risk and decision analysis community to understand how inputs to a system influence a property in question. Specifically, we pose the problem as one of finding a neighborhood of inputs that contains the falsifying counterexample in question, such that each point in this neighborhood corresponds to a falsifying input with a high probability. We use ideas from statistical hypothesis testing to infer and validate such neighborhoods from repeated simulations of the system under test. This approach not only helps to understand the sensitivity of these counterexamples to various parts of the inputs, but also generalizes or widens the given counterexample by returning a neighborhood of counterexamples around it.  We demonstrate our approach on a series of increasingly complex examples from automotive and closed loop medical device domains. We also compare our approach against related techniques based on regression and machine learning."""	benchmark (computing);cyber-physical system;decision analysis;digital footprint;heuristic (computer science);machine learning;naruto shippuden: clash of ninja revolution 3;parallel computing;simulation;solver;system under test;time complexity;tracing (software);witsenhausen's counterexample	Ram Das Diwakaran;Sriram Sankaranarayanan;Ashutosh Trivedi	2017	2017 ACM/IEEE 8th International Conference on Cyber-Physical Systems (ICCPS)	10.1145/3055004.3055029	statistical hypothesis testing;discrete mathematics;white-box testing;computer science;machine learning;mathematics;sensitivity analysis;algorithm;statistics	SE	-51.252117990206536	38.660906455578456	154190
bcf3a7342648a93f46886a866415498517b3d39c	performance analysis framework for large software-intensive systems with a message passing paradigm	product life cycle;embedded system;mobile phone;software performance engineering;software architecture;performance tuning of embedded systems;performance analysis;message passing;entry barrier;performance tuning;communication service;dynamic analysis	The launch of new features for mobile phones is increasing and the product life cycle symmetrically decreasing in duration as higher levels of sophistication are reached. Therefore, the optimization of resources is particularly important in embedded systems where CPU power and memory space are limited. In this context, performance engineers must be able to predict and analyze the performance of the software architecture in order to support its evolution and its new requirements. In this paper I describe a framework for the analysis of the performance of a software architecture where the architectural elements communicate using message based communication services. Using instrumentation traces I have extracted the run-time events I considered significant for the study. I have created a set of architectural views to reconstruct the dynamic and the static views of the architecture. Understanding the connections and the relationships between them guide the performance analyst to a clear comprehension of how the architecture works and subsequently how it can be optimized. The performance analysis framework described constitute an essential set. The next challenge is to enhance the integration of the tools and the synchronization of the views and to facilitate the entry barrier to novice performance engineers.	central processing unit;dspace;embedded system;list comprehension;mathematical optimization;message passing;mobile phone;profiling (computer programming);programming paradigm;requirement;software architecture;tracing (software)	Christian Del Rosso	2005		10.1145/1066677.1066878	barriers to entry;reference architecture;embedded system;software architecture;message passing;real-time computing;computer science;product lifecycle;operating system;software engineering;database;distributed computing;dynamic program analysis;programming language;world wide web;computer security	SE	-55.28816212599976	37.26289895059986	154310
300bba5c1310c4922bdebb7697369915fa82eff4	on effectiveness of fault-seeding using interaction patterns	fault seeding;interaction patterns;sequential patterns;program testing;testing databases electronic mail data mining prediction algorithms software engineering fault diagnosis;blackboard application fault seeding effectiveness interaction pattern testing technique structural testing method user interaction guided fault seeding mechanism;sequential patterns fault seeding interaction patterns	Fault Seeding is a testing technique where faults are artificially injected into an application to assess the effectiveness, i.e. if a given test suite is capable of uncovering the injected faults, of a test suite. This is helpful in establishing confidence in the test suite and is an alternative to structural testing methods. One of the issues with fault seeding is the identification of potential areas in the application, where the faults are to be seeded. We argue that if the intended usage of the application under test could be inferred from the potential users' interactions with the application, such information could be incorporated into the fault-seeding process. This could lead to more effective fault-seeding in a test application. In this work, we study fault seeding mechanisms based on user interactions with the application, and thus give a guided fault seeding mechanism for the purpose. We show the usefulness of the guided fault seeding with the help of a case study using the blackboard application.	experiment;fault (technology);interaction;microsoft outlook for mac;scalability;system under test;test data;test suite;usage data;white-box testing	Tamim Ahmed Khan;Anas Ijaz	2015	2015 13th International Conference on Frontiers of Information Technology (FIT)	10.1109/FIT.2015.31	reliability engineering;real-time computing;simulation;fault coverage;engineering	SE	-61.13893108713775	36.91360416867042	154337
01bd433e0f4f76b0aadf1c586e6dff97a5303648	non-null references by default in java: alleviating the nullity annotation burden	empirical study;general practice;open source	With the advent of Java 5 annotations, we note a marked increase in the availability of tools that can statically detect potential null dereferences. For such tools to be truly effective, they require that developers annotate declarations in their code with nullity modifiers and have annotated API libraries. Unfortunately, it has been our experience in specifying moderately large code bases that the use of non-null annotations is more labor intensive than it should be. Motivated by this experience, we conducted an empirical study of 5 open source projects (including the Eclipse 3.3 JDT Core) totaling over 700 KLOC. The results allow us to confirm that in Java programs, at least 2/3 of declarations of reference types are meant to be non-null, by design. Guided by these results, we propose a new non-null-by-default semantics. This new default has the advantages of better matching general practice, lightening the annotation burden of developers and being safer. We describe how we have adapted the Eclipse JDT Core to support the new semantics, including the ability to read the extensive, fully annotated API library specifications written by the Java Modeling Language (JML) community. Issues of backwards compatibility are also addressed.	application programming interface;backward compatibility;circuit rank;eclipse;java modeling language;java annotation;library (computing);open-source software;pointer (computer programming);reference type;source lines of code	Patrice Chalin;Perry R. James	2007		10.1007/978-3-540-73589-2_12	computer science;data mining;database;programming language;empirical research	PL	-54.71186478680282	38.790974513972984	154344
14366bf3e7ada1ebb1da23085c87e89925583633	quantifying the effectiveness of testing via efficient residual path profiling	inter procedural;software testing;regression testing;path profiling;testing;residual;software development	Software testing is extensively used for uncovering bugs inlarge, complex software. Testing relies on well designed regression test suites that anticipate all reasonable software usage scenarios. Unfortunately, testers today have no way of knowing how much of real-world software usage was untested by their regression suite. Recent advances in low-overhead path profiling provide the opportunity to rectify this deficiency and perform residual path profiling on deployed software. Residual path profiling identifies all paths executed bydeployed software that were untested during software development. We extend prior research to perform low-overhead interprocedural path profiling. We demonstrate experimentally that low-overhead path profiling, both intraprocedural and interprocedural, provides valuable quantitative information on testing effectiveness. We also show that residual edge profiling is inadequate as a significant number of untested paths include no new untested edges.	angular defect;experiment;overhead (computing);regression testing;software bug;software development;software testing;test suite	Trishul M. Chilimbi;Aditya V. Nori;Kapil Vaswani	2007		10.1145/1287624.1287706	reliability engineering;regression testing;real-time computing;white-box testing;computer science;software reliability testing;software engineering;data mining;software testing	SE	-61.75056725912911	36.07371826877462	154371
2662b1858d88cfd2d3d557c273b28e834018b324	detection of high-level execution patterns in reactive behavior of control programs	real time;program comprehension;testing and debugging;plc applications;reactive control;reactive behavior;software development;execution patterns;diagnostics;programmable logic;real time systems	This paper presents an approach to extract high-level patterns from traces of programmable logic control (PLC) programs recorded with a deterministic replay debugging tool. Our deterministic replay debugging works by recording an application run in real-time with minimal overhead so that it can be reproduced afterwards. In a subsequent phase, the application is replayed in offline mode to produce a more detailed trace log with additional information about the application run. A software developer can replay the program in a debugger and use debugger features to analyze the program run and locate errors. However, due to the vast amount of data and the complex behavior of reactive control programs, a normal debugger is usually only a poor support in comprehending the program behavior. In this paper we present an approach to analyze recorded program runs of PLC applications. We present a technology to visualize the reactive behavior of a program run and find recurring high-level execution patterns in long-running applications. We give an overview of possible application scenarios to support program comprehension, testing, and debugging.	airplane mode;debugger;debugging;execution pattern;high- and low-level;logic control;online and offline;overhead (computing);program comprehension;real-time clock;software developer;tracing (software)	Herbert Prähofer;Roland Schatz;Christian Wirth	2010		10.1145/1868321.1868324	parallel computing;real-time computing;computer science;algorithmic program debugging;programming language	SE	-54.83819559968012	36.859462225306814	154457
f3325dd37ac156b57ffd51293c8fe29b964986d4	appx: an automated app acceleration framework for low latency mobile app		Minimizing response time of mobile applications is critical for user experience. Existing work predominantly focuses on reducing mobile Web latency, whereas users spend more time on native mobile apps than mobile Web. Similar to Web, mobile apps contain a chain of dependencies between successive requests. However, unlike Web acceleration where object dependencies can easily be identified by parsing Web documents, App acceleration is much more difficult because the dependency is encoded in the app binary.  Motivated by recent advances in program analysis, this paper presents a system that utilizes static program analysis to automatically generate acceleration proxies for mobile apps. Our framework takes Android app binary as input, performs program analysis to identify resource dependencies, and outputs an acceleration proxy that performs dynamic prefetching. Our evaluation using a user study from 30 participants and an in-depth analysis of popular commercial apps shows that an acceleration proxy reduces the median user-perceived latency by up to 64% (1,471 ms).	cpu cache;mobile app;parsing;proxy server;response time (technology);static program analysis;usability testing;user experience;world wide web	Byungkwon Choi;Jeongmin Kim;Daeyang Cho;Seong Min Kim;Dongsu Han	2018		10.1145/3281411.3281416	computer network;operating system;program analysis;http/2;android (operating system);push technology;latency (engineering);static program analysis;user experience design;mobile web;computer science	SE	-55.19322096127534	42.0297394614211	154580
2f93d7aac06b9733ed508506ea284cdf205cb8cf	beyond instrumentation: fdi for modular subsystems with proprietary protocols	topology;protocols;instruments;modular field devices field device integration electrical integration industrial subsystem engineering;standards;switchgear electronic engineering computing iec standards;prototypes;switchgear user interfaces standards topology instruments protocols prototypes;switchgear;abb mns is fdi standard iec 62769 field device integration automation industry field instruments compact electrical device modular electrical subsystem modular device device management system fdi software components dms low voltage switchgear system;user interfaces	Upon its first release, the FDI standard (field device integration, IEC 62769) [1] will be the most widely accepted standard for field device integration in automation industry [6]. Although it is a very new standard, the underlying technology re-use protects investments in customer installed base on an even wider scale. Still, FDI-related work has mostly been focused on the support of field instruments and compact electrical devices. Toward a complete coverage of all automation devices with one integration technology, we have therefore investigated the potential of FDI to also support the particularities of modular electrical subsystems. Using ABB's low-voltage switchgear MNS iS as a case study, we have attempted to practically port the relevant parts of the existing tool chain to FDI technology. While other modular devices such as remote IOs typically come from the same vendor as the device management system (DMS) and therefore may be supported in a proprietary manner, we must be able to integrate electrical subsystems into an FDI system like any compact device: by loading standard-conforming description files. Given the approval state of the standard and the still ongoing development of common FDI software components, we find the results rather encouraging. From the gaps we found in both the standard and the currently available implementation, we have concluded a set of proposals for action. We are confident that with reasonable effort the value of FDI can be enhanced even further.	component-based software engineering;fault detection and isolation;flexible display interface;toolchain;ios	Dirk Schulz;Roland Braun;Ulrich Topp;Martin Stockl	2014	Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)	10.1109/ETFA.2014.7005145	embedded system;communications protocol;telecommunications;computer science;engineering;electrical engineering;operating system;prototype;user interface;switchgear;manufacturing engineering	SE	-51.290243727258485	43.27591220919349	154973
36c70af3ad8cc547fc95fa373b6aa0b0fc1f5a04	non-parametric statistical fault localization	fault localization;parametric method;controlled experiment;statistical model;hypothesis testing;test methods;non parametric method;non parametric statistics;program debugging;article;hypothesis test	Fault localization is a major activity in program debugging. To automate this time-consuming task, many existing fault-localization techniques compare passed executions and failed executions, and suggest suspicious program elements, such as predicates or statements, to facilitate the identification of faults. To do that, these techniques propose statistical models and use hypothesis testing methods to test the similarity or dissimilarity of proposed program features between passed and failed executions. Furthermore, when applying their models, these techniques presume that the feature spectra come from populations with specific distributions. The accuracy of using a model to describe feature spectra is related to and may be affected by the underlying distribution of the feature spectra, and the use of a (sound) model on inapplicable circumstances to describe real-life feature spectra may lower the effectiveness of these fault-localization techniques. In this paper, we make use of hypothesis testing methods as the core concept in developing a predicate-based fault-localization framework. We report a controlled experiment to compare, within our framework, the efficacy, scalability, and efficiency of applying three categories of hypothesis testing methods, namely, standard non-parametric hypothesis testing methods, standard parametric hypothesis testing methods, and debugging-specific parametric testing methods. We also conduct a case study to compare the effectiveness of the winner of these three categories with the effectiveness of 33 existing statement-level fault-localization techniques. The experimental results show that the use of non-parametric hypothesis testing methods in our proposed predicate-based fault-localization ing. model is the most promis	debugging;population;real life;scalability;statistical model	Zhenyu Zhang;Wing Kwong Chan;T. H. Tse;Yuen-Tak Yu;Peifeng Hu	2011	Journal of Systems and Software	10.1016/j.jss.2010.12.048	statistical hypothesis testing;test statistic;computer science;data mining;statistics	SE	-60.91674707043478	35.88407527725696	155120
8598ea6af56f283b3dc6e2276ee0dd2e8413a2d1	hotcomments: how to make program comments more useful?	keyword search;natural language;source code;memory allocation;natural language processing;open source	Program comments have long been used as a common practice for improving inter-programmer communication and code readability, by explicitly specifying programmers’ intentions and assumptions. Unfortunately, comments are not used to their maximum potential, as since most comments are written in natural language, it is very difficult to automatically analyze them. Furthermore, unlike source code, comments cannot be tested. As a result, incorrect or obsolete comments can mislead programmers and introduce new bugs later. This position paper takes an initiative to investigate how to explore comments beyond their current usage. Specifically, we study thefeasibility andbenefitsof automatically analyzing comments to detect software bugs and bad comments. Our feasibility and benefit analysis is conducted from three aspects using Linux as a demonstration case. First, we study comments’ characteristics and found that a significant percentage of comments are about “hot topics” such as synchronization and memory allocation, indicating that the comment analysis may first focus on hot topics instead of trying to “understand” any arbitrary comments. Second, we conduct a preliminary analysis that uses heuristics (i.e. keyword searches) with the assistance of natural language processing techniques to extract information from lock-related comments and then check against source code for inconsistencies. Our preliminary method has found 12 new bugs in the latest version of Linux with 2 already confirmed by the Linux Kernel developers. Third, we examine several open source bug databases and find that bad or inconsistent comments have introduced bugs, indicating the importance of maintaining comments and detecting inconsistent comments.	algorithm;comment (computer programming);computer programming;database;heuristic (computer science);linux;natural language processing;open-source software;programmer;sensor;software bug	Lin Tan;Ding Yuan;Yuanyuan Zhou	2007			computer science;theoretical computer science;operating system;data mining;natural language;world wide web;docstring;source code;memory management	OS	-59.83093473820385	38.56339767090993	155200
221e38875c5d9b6d38222304d5ab323b52cf8a6c	towards a better collaboration of static and dynamic analyses for testing concurrent programs	debugging;search space;multi threaded programming;race conditions;happens before;lockset;concurrent programs;coarse grained;static analysis;parallel programs;static and dynamic analysis;race detection;dynamic analysis	Testing concurrent programs remains a difficult task due to the non-deterministic nature of concurrent executions. Many approaches have been proposed to combine static and dynamic analysis to reduce the complexity of uncovering potential concurrency bugs. However, the existing collaboration schemes only provide a limited mechanism for exchanging relevant information between the two analyses. For example, alias information only flows from the static analysis module to the dynamic analysis module at the beginning of the dynamic analysis. Therefore, we cannot fully exploit the advantages of each type of analysis. Motivated by this observation, in this paper we present a new testing technique which enables a tighter collaboration between static analysis and dynamic analysis. In this collaboration scheme, static analysis and dynamic analysis interact iteratively throughout the whole testing process. Static analysis uses coarse-grained analysis to guide the dynamic analysis to concentrate on the relevant search space, while dynamic analysis collects concrete runtime information during the guided exploration. The runtime information provided by the dynamic analysis helps the static analysis to refine its coarse-grained analysis and provides better guidance on dynamic analysis. Currently, our implementation consists of a static analysis module based on Soot and a dynamic analysis module based on JPF (Java PathFinder).	bi-directional text;concurrency (computer science);concurrent computing;context-free grammar;java pathfinder;out of the box (feature);run time (program lifecycle phase);software bug;static program analysis	Jun Chen;Steve MacDonald	2008		10.1145/1390841.1390849	real-time computing;simulation;computer science;dynamic testing;distributed computing;race condition;dynamic program analysis;shape analysis;programming language;debugging;static analysis	SE	-56.21744607191995	39.35423398818638	155435
47d72d1dc28c490d22cc2665e4741961562dd663	classifying software visualization tools using the bloom's taxonomy of cognitive domain	software;software visualization tools classification;bloom s cognitive taxonomy;software measurement;bloom s cognitive taxonomy software visualization tools classification bloom s taxonomy cognitive domain classification schema;computer graphics;cinematography;software systems;classification;software engineering;visualization tools;visualization;cognitive domain;animation;unified modeling language;data visualization;taxonomy;crawlers;software tools;software tools taxonomy data visualization computer graphics animation software systems cinematography humans concrete software measurement;humans;bloom s taxonomy;software engineering program visualisation;bloom s taxonomy visualization tools classification;classification schema;program visualisation;concrete;java;software visualization	There are a lot of software visualization tools existing for various purposes. Therefore, how to choose the right visualization tool for a specific task becomes an important issue. Although there are a few taxonomies for classifying visualization tools, each has its own defects. This paper proposes a new classification schema based on the widely used Bloom's cognitive taxonomy. We summarize a set of questions for each level of the Bloom taxonomy using the well defined verbs, to identify if the visualization tool belongs to the individual cognitive level, which in turn can determine the purposes of the tool. We also conduct case studies on four tools to evaluate this new approach. The result demonstrates that the new classification schema can provide good guidance for people to choose a useful visualization tool.	bloom's taxonomy;software visualization;whole earth 'lectronic link	Shaochun Xu;Xuhui Chen;Dapeng Liu	2009	2009 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2009.5090082	anime;unified modeling language;software visualization;information visualization;visualization;concrete;biological classification;computer science;theoretical computer science;database;cinematography;computer graphics;java;software measurement;world wide web;data visualization;taxonomy;software system	Visualization	-56.03914436979993	34.242477886042586	155578
3272a8c073b5c685abf8594bef5add2ef8d7a9ed	inferring web api descriptions from usage data	web services application program interfaces;standards;learning descriptions web apis;api usage server logs web api description trained classifiers;servers;uniform resource locators entropy standards servers terminology simple object access protocol;terminology;entropy;simple object access protocol;uniform resource locators	We describe a set of techniques to infer structured descriptions of web APIs from usage examples. Using trained classifiers, we identify fixed and variable segments in paths, and tag parameters according to their types. We implemented our techniques and evaluated their precision on 10 APIs for which we obtained: 1) descriptions, manually written by the API maintainers, and 2) server logs of the API usage. Our experiments show that our system is able to reconstruct the structure of both simple and complex web API descriptions, outperforming an existing tool with similar goals. Finally, we assess the impact of noise in the input data on the results of our method.	application programming interface;code;experiment;image noise;server (computing);usage data;web api	Philippe Suter;Erik Wittern	2015	2015 Third IEEE Workshop on Hot Topics in Web Systems and Technologies (HotWeb)	10.1109/HotWeb.2015.19	computer science;web api;data mining;database;world wide web	SE	-57.59368320114986	42.33815122632904	155870
f0ec25ccb1a1c88fc5fd8e4019e493eaa858df96	a self-recovery method based on mixed reboot	biomedical monitoring;software;mission critical systems application software software systems educational institutions computer science humans system recovery search engines open source software operating systems;mission critical systems;mission critical system;pediatrics;software componentization;application software;search engines;probability density function;software systems;software componentization self recovery method mixed reboot method microreboot method mission critical system autonomic recursive software reboot;autonomic recursive software reboot;data mining;self recovery method;mixed reboot;recursive reboot;fault tolerant computing;system recovery;critical system;microreboot method;monitoring;recursive reboot self recovery mixed reboot;mixed reboot method;humans;computer science;coarse grained;self recovery;system recovery computer bootstrapping fault tolerant computing;open source software;operating systems;java;computer bootstrapping	Microreboot is one of the most efficient self recovery technologies for mission-critical system. But some limitations still exist in the existing microreboot methods such as demand of componentization and loosing coupling etc. In order to eliminate these limitations, we propose a more flexible and usable microreboot method called mixed reboot in this paper, which systematically combines coarse-grained reboot with fine-grained reboot. The mixed reboot can be used to recursively restart from inner class to the whole application which has never been realized before. The result of our experiment shows that the MR can realize autonomic recursive reboot of software to provide system with higher self-recovery ability than the existing microreboot methods.	autonomic computing;critical system;inner class;mission critical;recursion	Xikun Dong;Huiqiang Wang;Jibao Lai	2009	2009 Fifth International Conference on Autonomic and Autonomous Systems	10.1109/ICAS.2009.18	embedded system;probability density function;application software;real-time computing;computer science;operating system;java;software system	SE	-53.37401208847574	41.04649608724165	155934
94c4b3a3c9fd4855465030c0f503a7b2e9712dfc	equivalence class verification and oracle-free testing using two-layer covering arrays	oracle problem;t way testing;verification and validation v v;primary covering array equivalence class verification oracle free testing two layer covering arrays unit testing module testing two layer covering array;t way testing combinatorial testing factor covering array oracle problem verification and validation v v;factor covering array;testing fault detection indexes impedance matching access control standards containers;combinatorial testing;program verification program testing	This short paper introduces a method for verifying equivalence classes for module/unit testing. This is achieved using a two-layer covering array, in which some or all values of a primary covering array represent equivalence classes. A second layer covering array of the equivalence class values is computed, and its values substituted for the equivalence class names in the primary array. It is shown that this method can detect certain classes of errors without a conventional test oracle, and an illustrative example is given.	array data structure;collision detection;oracle (software testing);sampling (signal processing);sensor;test design;test set;turing completeness;unit testing;verification and validation	D. Richard Kuhn;Raghu Kacker;Yu Lei;José Torres-Jiménez	2015	2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)	10.1109/ICSTW.2015.7107445	orthogonal array testing;equivalence partitioning;algorithm	SE	-50.04260397124775	36.56728780639955	156155
cd3b0f96bb7953c4ced032b8829b240711d0c0cd	mvt: a system for visual testing of software	software testing;visual debugging;visual testing;data visualisation;visual debugging algorithm simulation bytecode instrumentation execution history logging;execution history logging;software development;algorithm simulation;program debugging;interactive graphics;data structure;bytecode instrumentation;interaction technique	Software development is prone to time-consuming and expensive errors. Finding and correcting errors in a program (debugging) is usually done by executing the program with different inputs and examining its intermediate and/or final results (testing). The tools that are currently available for debugging (debuggers) do not fully make use of several potentially useful visualisation and interaction techniques.This article presents a prototype debugging tool (MVT--Matrix Visual Tester) based on a new interactive graphical software testing methodology called visual testing. A programmer can use a visual testing tool to examine and manipulate a running program and its data structures. The tool combines aspects of visual algorithm simulation, high-level data visualisation and visual debugging, and allows easier testing, debugging and understanding of software.	algorithm;data structure;data visualization;debugger;debugging;graphical user interface;high- and low-level;interaction technique;programmer;prototype;simulation;software development;software testing;test automation	Jan Lönnberg;Ari Korhonen;Lauri Malmi	2004		10.1145/989863.989931	computer architecture;real-time computing;data structure;human–computer interaction;computer science;software reliability testing;software development;software engineering;software construction;software testing;algorithmic program debugging;programming language;debugging;interaction technique;program animation	SE	-54.717339897791966	36.29360333511843	156158
85b1db53d3e7508b48e798435cb3affb1001a6cf	automated refactoring of legacy java software to enumerated types	software environments;software maintenance;software tools;enumerated types;java;automated refactoring	Modern Java languages introduce several new features that offer significant improvements over older Java technology. In this article we consider the new enum construct, which provides language support for enumerated types. Prior to recent Java languages, programmers needed to employ various patterns (e.g., the weak enum pattern) to compensate for the absence of enumerated types in Java. Unfortunately, these compensation patterns lack several highly-desirable properties of the enum construct, most notably, type safety. We present a novel fully-automated approach for transforming legacy Java code to use the new enumeration construct. This semantics-preserving approach increases type safety, produces code that is easier to comprehend, removes unnecessary complexity, and eliminates brittleness problems due to separate compilation. At the core of the proposed approach is an interprocedural type inferencing algorithm which tracks the flow of enumerated values. The algorithm was implemented as an open source, publicly available Eclipse plug-in and evaluated experimentally on 17 large Java benchmarks. Our results indicate that analysis cost is practical and the algorithm can successfully refactor a substantial number of fields to enumerated types. This work is a significant step towards providing automated tool support for migrating legacy Java software to modern Java technologies.	algorithm;cobol;code refactoring;compiler;eclipse;enumerated type;experiment;java;language construct;open-source software;plug-in (computing);programmer;reference type;string (computer science);type safety;user interface	Raffi Khatchadourian	2016	Automated Software Engineering	10.1007/s10515-016-0208-8	java concurrency;computer science;software engineering;java modeling language;strictfp;database;real time java;programming language;java;algorithm;generics in java;scala;java annotation	SE	-54.90029959089781	38.602277196286785	156214
847d89c7ed057e2273dceb78ae9f57d308994d3c	estimating the correctness of computer programs	computer program	Program testing will for some time remain the major means by which program designers convince themselves and others of the correctness or validity of their programs. Thus it is desirable to be able to use test results to measure the degree to which a program approaches correctness. The probability of correctness of a program can sometimes be estimated according to the success of testing. A general parameter is introduced which has the property that its value is zero for a correct program and is greater than zero for an incorrect program. Under reasonable interpretations, the parameter is related, through probability density functions, to characteristics which are observable through program testing. For instance, if the parameter is interpreted as the number of errors in a program, a related, observable characteristic is the number of errors discovered through testing. Upper bounds on the parameter value are established as a function of the value of the observed characteristic and the desired level of confidence in the result. Results are presented for cases of combinations of interpretations, observable characteristics, and probability density functions.	computer program;correctness (computer science)	Joe W. Duran;John J. Wiorkowski	1982	Inf. Sci.	10.1016/0020-0255(82)90038-X	discrete mathematics;computer science;mathematics;algorithm;statistics	DB	-61.682506361202776	33.81098201528823	156418
3d21ee7d4f9d11f625f928590f2010d500f5d207	automatic generation of valid and invalid test data for string validation routines using web searches and regular expressions	test data generation;web searches;regular expressions	Classic approaches to automatic input data generation are usually driven by the goal of obtaining program coverage and the need to solve or find solutions to path constraints to achieve this. As inputs are generated with respect to the structure of the code, they can be ineffective, difficult for humans to read, and unsuitable for testing missing implementation. Furthermore, these approaches have known limitations when handling constraints that involve operations with string data types. This paper presents a novel approach for generating string test data for string validation routines, by harnessing the Internet. The technique uses program identifiers to construct web search queries for regular expressions that validate the format of a string type (such as an email address). It then performs further web searches for strings that match the regular expressions, producing examples of test cases that are both valid and realistic. Following this, our technique mutates the regular expressions to drive the search for invalid strings, and the production of test inputs that should be rejected by the validation routine. The paper presents the results of an empirical study evaluating our approach. The study was conducted on 24 string input validation routines collected from 10 open source projects. While dynamic symbolic execution and search-based testing approaches were only able to generate a very low number of values successfully, our approach generated values with an accuracy of 34% on average for the case of valid strings, and 99% on average for the case of invalid strings. Furthermore, whereas dynamic symbolic execution and search-based testing approaches were only capable of detecting faults in 8 routines, our approach detected faults in 17 out of the 19 validation routines known to contain implementation errors.	data validation;email;identifier;internet;open-source software;regular expression;sensor;string (computer science);symbolic execution;test case;test data;web search engine;web search query	Muzammil Shahbaz;Phil McMinn;Mark Stevenson	2015	Sci. Comput. Program.	10.1016/j.scico.2014.04.008	test data generation;computer science;theoretical computer science;data mining;programming language;regular expression;algorithm	SE	-59.35954825686148	39.24722680347715	156686
ba8557bbe239e6082b9dae7ecb1de847bd4cfdfe	using linq as a universal tool for defining architectural assertions		We demonstrate that Microsoft LINQ can be used as a convenient tool to define architectural assertions. We introduce an abstract model of software based on a directed multigraph and formalize the notion of software architecture and architectural assertions. We demonstrate how Microsoft Visual Studio can be harnessed to extract the architecture of a given software project and append it with assertions using LINQ notation. In particular we explain the flow of data processing that takes place within Visual Studio engine. We follow with examples of assertions selected to demonstrate the expressive power of our approach. We conclude by showing subsequent areas of research worth following in order to deepen the research indicated in this paper.	append;dataflow;expressive power (computer science);language integrated query;microsoft visual studio;multigraph;software architecture;software project management	Bartosz Frackowiak;Robert Dabrowski	2016		10.15439/2016F587	language integrated query;database;data mining;computer science	SE	-53.29047490018558	35.37640907920908	157084
ccb6739b402d76a2f1f93c2f215c8a2541e4ab21	metamorphic fault tolerance: an automated and systematic methodology for fault tolerance in the absence of test oracle	oracle problem;metamorphic testing;swinburne;metamorphic relation;fault tolerance	A system may fail due to an internal bug or a fault in its execution environment. Incorporating fault tolerance strategies enables such system to complete its function despite the failure of some of its parts. Prior to the execution of some fault tolerance strategies, failure detection is needed. Detecting incorrect output, for instance, assumes the existence of an oracle to check the correctness of program outputs given an input. However, in many practical situations, oracle does not exist or is extremely difficult to apply. Such an oracle problem is a major challenge in the context of software testing. In this paper, we propose to apply metamorphic testing, a software testing method that alleviates the oracle problem, into fault tolerance. The proposed technique supports failure detection without the need of oracles.	collision detection;correctness (computer science);fault tolerance;metamorphic testing;oracle (software testing);oracle database;oracle machine;software bug;software testing	Huai Liu;Iman I. Yusuf;Heinz W. Schmidt;Tsong Yueh Chen	2014		10.1145/2591062.2591109	reliability engineering;fault tolerance;real-time computing;computer science;engineering;algorithm;software fault tolerance	SE	-60.509998348273605	37.35537324514039	157196
0633f3040b91a90b0a3c751eda458f34720002f2	macke: compositional analysis of low-level vulnerabilities with symbolic execution	software;memory management;symbolic execution;engines;complex systems;scalability;compositional analysis;security;computer bugs	Concolic (concrete+symbolic) execution has recently gained popularity as an effective means to uncover non-trivial vulnerabilities in software, such as subtle buffer overflows. However, symbolic execution tools that are designed to optimize statement coverage often fail to cover potentially vulnerable code because of complex system interactions and scalability issues of constraint solvers. In this paper, we present a tool (MACKE) that is based on the modular interactions inferred by static code analysis, which is combined with symbolic execution and directed inter-procedural path exploration. This provides an advantage in terms of statement coverage and ability to uncover more vulnerabilities. Our tool includes a novel feature in the form of interactive vulnerability report generation that helps developers prioritize bug fixing based on severity scores. A demo of our tool is available at https://youtu.be/icC3jc3mHEU.	buffer overflow;code coverage;complex system;concolic testing;high- and low-level;interaction;scalability;static program analysis;symbolic execution;vulnerability (computing)	Saahil Ognawala;Martín Ochoa;Alexander Pretschner;Tobias Limmer	2016	2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1145/2970276.2970281	complex systems;real-time computing;scalability;software bug;computer science;theoretical computer science;operating system;distributed computing;programming language;concolic testing;memory management	SE	-56.31013252628086	38.35350426921991	157324
a99e6bbfb8ba9f1c77c8d94ea45ef849176f877a	noms'94 distinguished experts' panel: it's a small world, isn't it - but can we manage it?	investments;communication networks;aerodynamics;technology management computer networks communication networks computer network management intelligent networks information management graphical user interfaces aerodynamics costs investments;small world;technology management;computer networks;graphical user interfaces;information management;computer network management;intelligent networks;weed management			Joseph Betser	1994		10.1109/NOMS.1994.643485	intelligent network;systems management;simulation;information technology management;network management station;human–computer interaction;telecommunications;aerodynamics;computer science;knowledge management;technology management;weed control;graphical user interface;network management application;structure of management information;information management;management;computer network	NLP	-50.760267639130795	45.4445355621152	157333
cd4d9397b93c6112a9c2ca82c7c9dfe949539291	a design, tests and considerations for improving keystroke and mouse loggers		We start by reviewing several logging tools. We then report improvements to a keystroke logger we have developed for the Mac and PC, Recording User Input (RUI). These improvements include changes to its interface, increased accuracy, and extensions to its logging ability. RUI runs in the background recording user behavior with timestamps and mouse location data across all applications—thus avoiding problems associated with video logs and instrumenting individual applications. We provide a summary and comparison of tests for loggers and and present procedures for validating logger timing that quantifies timing accuracy using an external clock. We demonstrate these tests on RUI and three other applications (Morae, Camtasia, and AppMonitor). We conclude by providing some general specifications and considerations for creating, testing, evaluating, and using keystroke and mouse loggers with respect to different experimental questions and tasks.	data logger;event (computing);instrumentation (computer programming);keystroke logging;software testing;trusted timestamping	Jonathan H. Morgan;Chen-Yang Cheng;Christopher Pike;Frank E. Ritter	2013	Interacting with Computers	10.1093/iwc/iws014	real-time computing;simulation;operating system;world wide web	HCI	-62.329904780836465	44.4441649999462	157702
1c38f2458f59ec760b796eda5d7ddc9089ed887d	visualizing the evolution of systems and their library dependencies	libraries;software;time series visualization system evolution visualization library dependencies system centric dependency plots sdp library centric dependant diffusion plot ldp;software maintenance;color;layout;evolution biology;visualization;shape;software evolution;software maintenance software evolution software reuse;software libraries data visualisation;libraries visualization shape evolution biology software color layout;software reuse	System maintainers face several challenges stemming from a system and its library dependencies evolving separately. Novice maintainers may lack the historical knowledge required to efficiently manage an inherited system. While some libraries are regularly updated, some systems keep a dependency on older versions. On the other hand, maintainers may be unaware that other systems have settled on a different version of a library. In this paper, we visualize how the dependency relation between a system and its dependencies evolves from two perspectives. Our system-centric dependency plots (SDP) visualize the successive library versions a system depends on over time. The radial layout and heat-map metaphor provide visual clues about the change in dependencies along the system's release history. From this perspective, maintainers can navigate to a library-centric dependants diffusion plot (LDP). The LDP is a time-series visualization that shows the diffusion of users across the different versions of a library. We demonstrate on real-world systems how maintainers can benefit from our visualizations through four case scenarios.	dependency relation;heat map;library (computing);radial (radio);stemming;time series;world-system	Raula Gaikovina Kula;Coen De Roover;Daniel M. Germán;Takashi Ishio;Katsuro Inoue	2014	2014 Second IEEE Working Conference on Software Visualization	10.1109/VISSOFT.2014.29	layout;software visualization;visualization;shape;computer science;software evolution;software engineering;programming language;software maintenance;world wide web	SE	-55.405772661757396	34.71191805654186	158181
263010e4b7124b5e325620eae5f8a38ddc124f9c	trends and directions in cloud service selection	user interfaces cloud computing contracts knowledge management;service selection;cloud;service level agreement cloud service selection quality of service;cloud computing quality of service google software as a service organizations elasticity;user knowledge cloud service selection cloud computing cloud users user requirements service level agreement sla academic research;service level agreement;quality of service	With the growing popularity of cloud computing the number of cloud service providers and services have significantly increased. Thus selecting the best cloud services becomes a challenging task for prospective cloud users. The process of selecting cloud services involves various factors such as characteristics and models of cloud services, user requirements and knowledge, and service level agreement (SLA), to name a few. This paper investigates into the cloud service selection tools, techniques and models by taking into account the distinguishing characteristics of cloud services. It also reviews and analyses academic research as well as commercial tools in order to identify their strengths and weaknesses in the cloud services selection process. It proposes a framework in order to improve the cloud service selection by taking into account services capabilities, quality attributes, level of user's knowledge and service level agreements. The paper also envisions various directions for future research.	capability maturity model;cloud computing;cloud research;list of system quality attributes;prospective search;provisioning;quality of service;requirement;seamless3d;service-level agreement;system migration;user requirements document;vendor lock-in	Mona Eisa;Muhammad Younas;Kashinath Basu;Hong Zhu	2016	2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)	10.1109/SOSE.2016.59	service provider;panorama9;cloud computing security;service level requirement;service level objective;service catalog;quality of service;cloud computing;differentiated service;computer science;service delivery framework;operating system;service design;cloud testing;data mining;database;data as a service;world wide web	SE	-48.564157420302465	43.33093167289322	158283
996a3bf6dd67fd6602daae5165f9caf065b3473b	test data generation considering data dependence	mutation testing;software testing;domain reduction;test data generation mutation testing constraint system automated software testing;nickel;test data generation;testing;general techniques;control flow graph;flow graphs;arrays;automated software testing;test case generation;program testing;data dependence;success rate;constraint system;search problems;path wise generation;correlation;control dependence;reduction method;high efficiency;domain reduction test data generation data dependence test case generation constraint system path wise generation control flow graph;buildings;nickel arrays search problems buildings testing flow graphs correlation	The path-wise technique of test data generation is a high-efficiency technique for test cases generation, implements test data generation by building and solving constraint systems. However, most of path-wise generation techniques only take the control dependence among statements into consideration, viz, build constraint system by analyzing the control flow graph but neglecting the data dependence among statements. Considering both of them, this paper proposes a new domain reduction method for improving the test data generation technique of domain reduction. Experimental results showed that this method improves the successful rate and execution efficiency of test data generation at a large extent, although it couldn’t find the optimum solution for test data generation.	control flow graph;data dependency;test case;test data generation;viz: the computer game	Xinzhong Liu;Gaochao Xu;Xiaodong Fu;Yushuang Dong	2010	2010 Fifth International Conference on Frontier of Computer Science and Technology	10.1109/FCST.2010.8	test data generation;computer science;theoretical computer science;software testing;algorithm	SE	-59.41038213845342	36.01028735324542	158532
364222acea9a0d687d07ec0dd6d8e0f0696429dc	the codecompass comprehension framework		CodeCompass is an open source LLVM/Clang based tool developed by Ericsson Ltd. and the Eötvös Loránd University, Budapest to help understanding large legacy software systems. Based on the LLVM/Clang compiler infrastructure, CodeCompass gives exact information on complex C/C++ language elements like overloading, inheritance, the usage of variables and types, possible uses of function pointers and the virtual functions - features that various existing tools support only partially. Steensgaard's and Andersen's pointer analysis algorithm are used to compute and visualize the use of pointers/references. The wide range of interactive visualizations extends further than the usual class and function call diagrams; architectural, component and interface diagrams are a few of the implemented graphs. To make comprehension more extensive, CodeCompass is not restricted to the source code. It also utilizes build information to explore the system architecture as well as version control information e.g. git commit history and blame view. Clang based static analysis results are also integrated to CodeCompass. Although the tool focuses mainly on C and C++, it also supports Java and Python languages.  In this demo we will simulate a typical bug-fixing work flow in a C++ system. First, we show, how to use the combined text and definition based search for a fast feature location. Here we also demonstrate our log search, which can be used to locate the code source of an emitted message. When we have an approximate location of the issue, we can start a detailed investigation understanding the class relationships, function call chains (including virtual calls, and calls via function pointers), and the read/write events on individual variables. We also visualize the pointer relationships. To make the comprehension complete, we check the version control information who committed the code, when and why.  This Tool demo submission is complementing our Industry track submission with the similar title. A live demo is also available at the homepage of the tool https://github.com/ericsson/codecompass.	approximation algorithm;c++;clang;compiler;cross-reference;diagram;function pointer;integrated development environment;interactive visualization;java;llvm;legacy system;list comprehension;open platform;open-source software;operator overloading;pointer (computer programming);pointer analysis;python;simulation;software metric;software system;static program analysis;systems architecture;version control;windows live	Zoltán Porkoláb;Tibor Brunner	2018		10.1145/3196321.3196352	subroutine;data mining;compiler;programming language;pointer (computer programming);pointer analysis;computer science;python (programming language);function pointer;source code;java	SE	-55.25497649892115	36.69661903190143	158577
9680ef6a5f8cac7730f2a8b6e9a78735890ac48c	a principled taxonomy of software visualization	software visualisation;software visualization	Abstract   In the early 1980s researchers began building systems to visualize computer programs and algorithms using newly emerging graphical workstation technology. After more than a decade of advances in interface technology, a large variety of systems has been built and many different aspects of the visualization process have been investigated. As in any new branch of a science, a taxonomy is required so that researchers can use a common language to discuss the merits of existing systems, classify new ones (to see if they really are new) and identify gaps which suggest promising areas for further development. Several authors have suggested taxonomies for these visualization systems, but they have been  ad hoc  and have relied on only a handful of characteristics to describe a large and diverse area of work. Another major drawback of these taxonomies is their inability to accommodate expansion: there is no clear way to add new categories when the need arises.  In this paper we present a detailed taxonomy of systems for the visualization of computer software. This taxonomy was derived from an established black-box model of software and is composed of a hierarchy with six broad categories at the top and over 30 leaf-level nodes at four hierarchical levels. We describe 12 important systems in detail and apply the taxonomy to them in order to illustrate its features. After discussing each system in this context, we analyse its coverage of the categories and present a research agenda for future work in the area.	software visualization	Blaine A. Price;Ronald Baecker;Ian S. Small	1993	J. Vis. Lang. Comput.	10.1006/jvlc.1993.1015	software visualization;computer science;software engineering;data mining;database;programming language	SE	-55.13637790851537	33.91666810074817	158671
4f085999e7e996fc2ff387e4d27e7c2d13145638	prevention of failures due to assumptions made by software components in real-time systems	large scale;commercial off the shelf;system integration;software component;weed management;real time systems	Large scale real-time systems consist of hundreds of commercial off-the-shelf (COTS) and custom software components. Mismatched assumptions between software components are a prime source of failures in these systems. Further, component assumptions are often implicit due to the limitations of current software interfaces. In this work, we introduce a framework to explicitly expose assumptions in software components, and automatically verify these assumptions during system integration. We manage the propagation and composition of these assumptions in the presence of changes and upgrades to individual components.	component-based software engineering;real-time clock;real-time computing;software propagation;system integration	Ajay Tirumala;Tanya L. Crenshaw;Lui Sha;Girish Baliga;Sumant Kowshik;Craig L. Robinson;Weerasak Witthawaskul	2005	SIGBED Review	10.1145/1121802.1121810	embedded system;real-time computing;software sizing;computer science;backporting;software framework;component-based software engineering;software development;operating system;weed control;software construction;programming language;software deployment;system integration	Embedded	-53.389461546559914	45.18335777480303	159311
513002d98dcc28f5aed434ce2970c754cdbeb8ce	an empirical comparison of similarity measures for abstract test case prioritization		Test case prioritization (TCP) attempts to order test cases such that those which are more important, according to some criterion or measurement, are executed earlier. TCP has been applied in many testing situations, including, for example, regression testing. An abstract test case (also called a model input) is an important type of test case, and has been widely used in practice, such as in configurable systems and software product lines. Similarity-based test case prioritization (STCP) has been proven to be cost-effective for abstract test cases (ATCs), but because there are many similarity measures which could be used to evaluate ATCs and to support STCP, we face the following question: How can we choose the similarity measure(s) for prioritizing ATCs that will deliver the most effective results? To address this, we studied fourteen measures and two popular STCP algorithms — local STCP (LSTCP), and global STCP (GSTCP). We also conducted an empirical study of five realworld programs, and investigated the efficacy of each similarity measure, according to the interaction coverage rate and fault detection rate. The results of these studies show that GSTCP outperforms LSTCP — in 61% to 84% of the cases, in terms of interaction coverage rates; and in 76% to 78% of the cases with respect to fault detection rates. Our studies also show that Overlap, the simplest similarity measure examined in this study, could obtain the overall best performance for LSTCP; and that Goodall3 has the best performance for GSTCP.	algorithm;fault detection and isolation;overlap–add method;regression testing;scheduling (computing);similarity measure;software product line;test template framework;test case	Rubing Huang;Yunan Zhou;Weiwen Zong;Dave Towey;Jinfu Chen	2017	2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2017.271	empirical research;regression testing;fault detection and isolation;statistics;similarity measure;computer science;test case;prioritization	SE	-60.88957438840733	35.32143588209265	159597
d209a6f98beca9ea2769065d2afd8ce2b56b5bd2	automated tracing and visualization of software security structure and properties	rule set visualization;firewall visualization;managing firewalls	Visualizing a program's structure and security characteristics is the intrinsic part of in-depth software security assessment. Such an assessment is typically an analyst-driven task. The visualization for security analysis is usually labor-intensive, since analysts need to read documents and source code, synthesize trace data from multiple sources (e.g., system utilities like lsof or strace). To help address this problem, we propose SecSTAR, a tool that dynamically collects the key information from a system and automatically produces the necessary diagrams to support the first steps of widely-used security analysis methodologies, such as Microsoft Threat Modeling and UW/UAB First Principles Vulnerability Assessment (FPVA). SecSTAR uses an efficient dynamic binary instrumentation technique, self-propelled instrumentation, to collect trace data from production systems during runtime then automatically produces diagrams. Furthermore, SecSTAR allows analysts to interactively view and explore diagrams in a web browser. For example, analysts can navigate the diagrams through time and at different levels of detail. We demonstrated the usefulness of using SecSTAR to produce FPVA-style diagrams for a widely used and complex distributed middleware system, the Condor high-throughput scheduling system. Compared with the original manual approach in FPVA, SecSTAR shortened the initial diagram construction time from months to hours and constructed a more accurate diagram visualizing the complete runtime structure of Condor.	application security;diagram;high-throughput computing;interactivity;level of detail;middleware;scheduling (computing);self-propelled particles;threat model;throughput;strace	Wenbin Fang;Barton P. Miller;James A. Kupsch	2012		10.1145/2379690.2379692	real-time computing;computer science;data mining;database;world wide web;computer security	SE	-53.49959168035622	36.23340692361938	159921
5e601d023575b3877b33ccee8bbe67606ef8e6d9	automatic repair of buggy if conditions and missing preconditions with smt	automatic repair;angelic fix localization;missing precondition;smt;test suite;buggy if condition	We present Nopol, an approach for automatically repairing buggy if conditions and missing preconditions. As input, it takes a program and a test suite which contains passing test cases modeling the expected behavior of the program and at least one failing test case embodying the bug to be repaired. It consists of collecting data from multiple instrumented test suite executions, transforming this data into a Satisfiability Modulo Theory (SMT) problem, and translating the SMT result -- if there exists one -- into a source code patch. Nopol repairs object oriented code and allows the patches to contain nullness checks as well as specific method calls.	failure;precondition;test case;test suite	Favio Demarco;Jifeng Xuan;Daniel Le Berre;Martin Monperrus	2014		10.1145/2593735.2593740	real-time computing;computer science;engineering;test suite;programming language;algorithm	SE	-58.11306212465902	37.71859357830061	160447
9c0d28bb8e9d6ee82f7b079f80cc88339b0860e5	some experiments toward understanding how program plan recognition algorithms scale	program understanding;plan recognition;search strategy;constraint satisfaction problem;constraint based program understanding program understanding program plan recognition algorithms scalability search strategies constraint satisfaction problems;scalability libraries computer science heart performance analysis algorithm design and analysis proposals;reverse engineering	Over the past decade, researchers in program understanding have formulated many program understanding algorithms but have published few studies of their relative scalability. Consequently, it is dificult to understand the relative limitations of these algorithms and to determine whether the f ie ld of program understanding is making progress. This paper attempts to address this deficiency b y formalizing the search strategies of several different program understanding algorithms as constraint satisfaction problems, and b y presenting some preliminary empirical scalability results for these constraint-based implementations. These initial results suggest that, at least under certain conditions, constraint-based program understanding is close to being applicable to real-world programs.	algorithm;angular defect;constraint satisfaction problem;program comprehension;scalability	Steven Woods;Alex Quilici	1996		10.1109/WCRE.1996.558813	program analysis;simulation;computer science;operating system;data mining;management science;constraint satisfaction problem;reverse engineering	SE	-58.774810959045176	34.65438579973522	160703
3ba518ce881b6b6e477ff756e4a0729fff00d5a5	[research paper] semantics-based code search using input/output examples		As the quality and quantity of open source code increase, semantics-based code search has become an emerging need for software developers to retrieve and reuse existing source code. We present an approach of semantics-based code search using input/output examples for the Java language. Our approach encodes Java methods in code repositories into path constraints via symbolic analysis and leverages SMT solvers to find the methods whose path constraints can satisfy the given input/output examples. Our approach extends the applicability of the semantics-based search technology to more general Java code compared with existing methods. To evaluate our approach, we encoded 1228 methods from GitHub and applied semantics-based code search on 35 queries extracted from Stack Overflow. Correct method code for 29 queries was obtained during the search and the average search time was just about 48 seconds.	encode;experiment;input/output;java;open-source software;software developer;software engineering;stack overflow;web search query	Renhe Jiang;Z. M. Chen;Zejun Zhang;Yu Pei;Minxue Pan;Tian Zhang	2018	2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)	10.1109/SCAM.2018.00018	theoretical computer science;programming language;reuse;source code;method code;software;semantics;computer science;symbolic data analysis;input/output;java	SE	-59.5546291037826	39.499002483765125	161001
e09e1e9e66fa12e3e57dfbb7a57de7b88900dd29	a bi-objective model inspired greedy algorithm for test suite minimization	software regression testing;fault detection effectiveness;regression testing;test suite reduction;testing criteria;satisfiability;fault detection;test suite minimization;greedy algorithm;object model	Regression testing is a critical activity which occurs during the maintenance stage of the software lifecycle. However, it requires large amounts of test cases to assure the attainment of a certain degree of quality. As a result, test suite sizes may grow significantly. To address this issue, Test Suite Reduction techniques have been proposed. However, suite size reduction may lead to significant loss of fault detection efficacy. To deal with this problem, a greedy algorithm is presented in this paper. This algorithm attempts to select a test case which satisfies the maximum number of testing requirements while having minimum overlap in requirements coverage with other test cases. In order to evaluate the proposed algorithm, experiments have been conducted on the Siemens suite and the Space program. The results demonstrate the effectiveness of the proposed algorithm by retaining the fault detection capability of the suites while achieving significant suite size reduction.	benchmark (computing);experiment;fault detection and isolation;greedy algorithm;mathematical optimization;optimization problem;regression testing;requirement;software development process;test case;test suite	Saeed Parsa;Alireza Khalilian	2009		10.1007/978-3-642-10509-8_24	reliability engineering;model-based testing;computer science;machine learning;algorithm	SE	-60.09956361690591	34.360311911494975	161282
af3314c68ee0988fb4853edc483c905b09f9d090	starvoors - episode ii - strengthen and distribute the force	runtime monitors;time;clara	Static and runtime techniques for the verification of programs are complementary. They both have their advantages and disadvantages, and a natural question is whether they may be combined in such a way as to get the advantages of both without inheriting too much from their disadvantages. In a previous contribution to ISoLA’12, we have proposed StaRVOOrS (‘Static and Runtime Verification of Object-Oriented Software’), a unified framework for combining static and runtime verification in order to check dataand control-oriented properties. Returning to ISoLA here, we briefly report on advances since then: a unified specification language for dataand control-oriented properties, a tool for combined static and runtime verification, and experiments. On that basis, we discuss two future research directions to strengthen the power, and broaden the scope, of combined static and runtime verification: (i) to use static analysis techniques to further optimise the runtime monitor, and (ii) to extend the framework to the distributed case.	experiment;runtime verification;specification language;static program analysis;unified framework	Wolfgang Ahrendt;Gordon J. Pace;Gerardo Schneider	2016		10.1007/978-3-319-47166-2_28	real-time computing;computer science;theoretical computer science;runtime verification;programming language	PL	-52.870811929906864	32.57010513676215	161297
95ea271d707fa57b5b70aadbe2064933fce453c6	how to merge program texts	programming language;software development;article	Software usually exists in multiple versions. All these versions must evolve in parallel. We propose a programmerging system that helps the programmer to manage the evolution of all the versions, which is much needed in software development and maintenance environments. The system can either combine two programs together or it can combine changes to a base program together. The system consists of three stages: a syntax-based comparator, a synchronous printer, and a merging editor. Based on the differences between the two programs that are identified by the syntax-based comparator, the synchronous printer combines the texts of two programs. Since there may be conflicts between the two programs, a merging editor provides the user with commands to resolve the conflicts. Another distinct feature of the merging system is the generator approach to producing syntactic program comparators for new programming languages.	comparator;printer (computing);programmer;programming language;software development	Wuu Yang	1994	Journal of Systems and Software	10.1016/0164-1212(94)90026-4	computer science;theoretical computer science;software development;operating system;software engineering;database;programming language	SE	-52.1360120283831	35.94391383823297	161455
abbe48acd10575b8b24e0f0040c936380cf8c7f4	an automatic tool for benchmark testing of cloud applications		The performance testing of cloud applications is a challenging research topic, due to the multiplicity of different possibilities to allocate application services to Cloud Service Providers (CSPs). Currently available benchmarks mainly focus on evaluating specific services or infrastructural resources offered by different CSPs, but are not always useful to evaluate complete cloud applications and to discover performance bugs. This paper proposes a methodology to define an evaluation performance process, particularly suited for cloud applications, and an automatic procedure to set up and to execute benchmark tests. The methodology is based on the evaluations of two performance indexes, and is validated by presenting a complete case study application, developed within the FP7-EU-SPECS project. The analysis of the measurement results, produced automatically, can help the developer to discover possible bottlenecks and to take actions to improve both the usability and the performance of a cloud application.	benchmark (computing);bottleneck (software);cloud computing;component-based software engineering;graphical user interface;programming paradigm;representational state transfer;response time (technology);software bug;software deployment;software performance testing;synthetic intelligence;usability	Valentina Casola;Alessandra De Benedictis;Massimiliano Rak;Umberto Villano	2017		10.5220/0006379507010708	sdet;cloud testing	HPC	-51.669256242060314	39.73941788181128	161500
64ca16b8d31fddf79849804a455681c689235efd	fault classification oriented spectrum based fault localization		The commonly-used software fault localization approaches mainly utilize test coverage information and test cases execution results to calculate the suspiciousness of each program entity to identify the location of faults, namely spectrum based software fault localization (SBFL). It had been argued that such techniques are not helpful in real debugging process, since the low accuracy of localization and few information provided to programmers. In this paper we consider the combination of statement based fault classification with the SBFL, aiming at increasing accuracy of fault localization and provide additional possible fault information to programmers. An improved technique, fault classification oriented SBFL (FC-SBFL), is proposed in this paper, in which the suspiciousness value is adjusted dynamically based on the probability of statement being faulty. Experimental results on real application programs show that FC-SBFL is more effective than SBFL to locate faults, and studies with Tarantula and OP2 show that more than 75% faults have been identified in a better effectiveness.	debugging;fault coverage;programmer;test case	Xiujing Liu;Yong Liu;Zheng Li;Ruilian Zhao	2017	2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2017.125	code coverage;real-time computing;fault coverage;software fault tolerance;debugging;software;computer science;test case;reliability engineering	SE	-60.68019488068612	35.92509302536038	161589
0f3144dd44ce0fa037b59f327b6e5edfba7f4779	automated software winnowing	managerial decision making;software metrics;software development projects;software metrics semantic;bayesian networks	The strong isolation guarantees of hardware virtualization have led to its widespread use. A consequence of this is that individual partitions contain much software that is designed to be used in a variety of environments and by a range of applications, while in practice only a limited subset is actually utilized. Similarly, the modular design of software has contributed greatly to the ability of application developers to quickly write sophisticated programs. However, in most instances only a small fraction of the functionality included in a particular software component is needed.  To address the resulting code bloat, we describe a tool Occam that combines techniques from partial evaluation and type theory with the goal of reducing the code in deployed applications. Occam can be used without annotating or otherwise modifying a program's source. It leverages configuration-time information to produce a version of the application that is specialized to the context in which it will be deployed. We present our algorithms, implementation, and experimental evaluation.	algorithm;alias analysis;code bloat;component-based software engineering;disk partitioning;hardware virtualization;modular design;partial evaluation;software deployment;type theory;occam	Gregory Malecha;Ashish Gehani;Natarajan Shankar	2015		10.1145/2695664.2695751	long-term support;verification and validation;team software process;software sizing;computer science;artificial intelligence;package development process;backporting;software design;theoretical computer science;software framework;component-based software engineering;software development;software design description;operating system;software engineering;machine learning;software construction;bayesian network;data mining;database;software walkthrough;programming language;software analytics;software deployment;world wide web;computer security;software metric;static program analysis;software system;avionics software	PL	-62.8443820038716	38.56563507867443	161837
d2c1d8b4a396ebcb48d1255dd6ebcd7e2cbe4705	finding discriminative weighted sub-graphs to identify software bugs	decision tree classifier;fault localization;decision tree;graph mining;objective function	Abstract. The aim has been to detect discriminative sub-graphs which are highly distinguishable between program failing and passing execution graphs resulted from different runs. In this paper, a novel approach to mine weighted-edge graphs is proposed. We also apply our efficient objective function to find most discriminative patterns between failing and passing graphs. To find bug relevant sub-graphs, a decision tree classifier is used to classify program failing and passing runs based on their discriminative sub-graphs. The experimental results on Siemens test suite reveal the effectiveness of the proposed approach specifically in finding multiple bugs. It also gives the debugger an infection path related to the discovered bug(s). Keywords: Software Fault Localization, Discriminative Graph, Graph Mining, Execution Graphs, Decision Tree, Classification. 1 Introduction Despite of many attempts to decrease the number of software bugs before its deployment, a major number of latent bugs still remain. The majority of latent bugs are occasional non-crashing ones, which do not reveal themselves, unless specific inputs are given to the program [7]. The logic structure of a program can be represented as a control flow graph, where each node represents a basic block of the program, a sequence of instructions which are executed sequentially, and each edge characterizes the possible control flow between blocks [7]. In each execution, a sub-structure of this graph is executed which could represent a faulty or successful path of the program. Therefore, each incorrect or correct execution of a program can be converted as a failing or passing execution graph, respectively. Instead of basic blocks, each node in an execution graph represents a branch statement, namely predicate [1]. Consider we have a number of failing and passing execution graphs constructed from program runs. The aim is to extract bug relevant sub-graphs by investigating the program’s execution graphs. Bug relevant sub-graph(s) may help the debugger to easily detect the faulty program path(s) and fix the existing bugs. Since, a program may have many failing and passing execution graphs, an appropriate graph mining technique should be applied to find most discriminative sub-graphs. Furthermore, to find bug relevant sub-graphs among the extracted discriminative ones, we also require a suitable classification method [4].		Saeed Parsa;Somaye Arabi Naree;Neda Ebrahimi Koopaei;Mojtaba Vahidi-Asl	2010		10.1007/978-3-642-12214-9_49	real-time computing;computer science;theoretical computer science;algorithm	NLP	-59.5803963330385	37.475872921703804	161981
4c223e7cb4b3670adc814509feac0e3f869aa114	kenyon-web: reconfigurable web-based feature extractor	mining software repository;software;mining software repositories;software maintenance;database management systems;software reusability data mining database management systems internet software architecture software maintenance;feature extraction data mining software engineering history java database systems computer architecture wheels web pages;msr;data mining;software engineering;kenyon web architecture;reconfigurable web based feature extractor;computer architecture;database management system reconfigurable web based feature extractor kenyon web architecture msr mining software repository software engineering software change comprehension bug prediction developer network recovery software reusability dbms;software architecture;internet;feature extraction;software reusability;classification algorithms;developer network recovery;source code;dbms;database management system;software change comprehension;bug prediction;java	Research on Mining Software Repositories (MSR) has yielded fruitful results in many Software Engineering areas including software change comprehension, bug prediction, and developer network recovery. When performing MSR research, the first task is to extract features corresponding to source code details from repositories. Since reusable feature extraction tools are not available, each MSR research group builds their own extraction tool, a duplication of effort. We introduce a reusable feature extractor, Kenyon-web, for MSR research. Kenyon-web is fully reconfigurable, pluggable, and serves most MSR related tasks. In this report, we show the architecture of Kenyon-web and demonstrate its utility by showcasing a sample MSR task.	feature extraction;randomness extractor;software engineering;web application	Sunghun Kim;Shivkumar Shivaji;E. James Whitehead	2009	2009 IEEE 17th International Conference on Program Comprehension	10.1109/ICPC.2009.5090061	statistical classification;software architecture;the internet;feature extraction;computer science;operating system;software engineering;database;programming language;software maintenance;java;world wide web;source code	SE	-61.36510059285345	39.19515937732501	162014
0bb05146297fd4f60052d5b33a159e6ad5166437	compiler error notifications revisited: an interaction-first approach for helping developers more effectively comprehend and resolve error notifications	visualization;ide;taxonomy;compiler error messages	Error notifications and their resolutions, as presented by modern IDEs, are still cryptic and confusing to developers. We propose an interaction-first approach to help developers more effectively comprehend and resolve compiler error notifications through a conceptual interaction framework. We propose novel taxonomies that can serve as controlled vocabularies for compiler notifications and their resolutions. We use preliminary taxonomies to demonstrate, through a prototype IDE, how the taxonomies make notifications and their resolutions more consistent and unified.	compiler;controlled vocabulary;human–computer interaction;integrated development environment;prototype;taxonomy (general)	Titus Barik;Jim Witschey;Brittany Johnson;Emerson R. Murphy-Hill	2014		10.1145/2591062.2591124	visualization;computer science;operating system;database;programming language;world wide web;taxonomy	SE	-53.889545268099084	37.583359047886134	162738
63278308db6c8092fb988bf4216185b76cce76f3	a mining approach for component abnormal information based on monitor log	implicit vulnerability;security vulnerabilities;gsp algorithm;data mining;software components;component testing;software security;explicit vulnerability;generalised sequential patterns;frequent item sets;apriori algorithm;monitor logs;abnormal information	A software component is an assembly unit that can be deployed independently in any software system. Since the source code and development documents of software components cannot be obtained, the vulnerability testing for software components is a challenge for component users. Explicit and implicit vulnerabilities are two common security vulnerabilities in the components. In this paper, in order to detect security vulnerabilities in the component under test effectively, a mining approach for component abnormal information based on monitor log is proposed. For explicit vulnerability, the monitor log is mined with the improved apriori algorithm, and the risk coefficient of each method in component is calculated with the frequent item sets algorithm based on the mining results. For implicit vulnerability, all the method execution sequences in monitor log should be extracted and stored into a database to establish the method sequence database. The vulnerability testing report will be obtained by mining the method sequence database with the improved generalised sequential patterns (GSP) algorithm after data preprocessing. An empirical study based on the proposed method is conducted, and the experimental results show that the approach to mine component abnormal information can effectively detect security exceptions of the component under test.	apriori algorithm;assembly language;code;coefficient;component-based software engineering;data mining;data pre-processing;effective method;guardian service processor;mined;preprocessor;security testing;sequence database;sequential pattern mining;software system;vulnerability (computing)	Jinfu Chen;Lili Zhu;Yuchi Guo;Saihua Cai;Xiaolei Zhao	2016	IJSPM	10.1504/IJSPM.2016.079196	vulnerability management;software security assurance;gsp algorithm;computer science;component-based software engineering;data mining;database;unit testing;apriori algorithm;computer security	SE	-61.54649390755811	41.17725898040337	162886
bd99746b1f8c85c1f1e0462b930152f8b8df5e09	lkdt: a keyword-driven based distributed test framework	automated testing;libraries;software;software testing;software testing automation;test automation;keyword driven;software reusability linux program testing;keyword driven based distributed test framework;testing;distributed test framework keyword driven;graphical user interfaces;program testing;software reusability;distributed test framework;linux;software reusability lkdt keyword driven based distributed test framework linux software testing automation;system architecture;automatic testing software testing programming automation linux software engineering application software costs documentation computer science;lkdt;automation	In order to improve the reusability for the software testing automation procedures, at the same time support the program test for Linux, keyword-driven distributed test automation framework (LKDT) is proposed in this paper, which is based on the analyze of the recent automated testing frameworks and combined with the actual project practice for software testing automation. This paper presents the LKDTpsilas system architecture, design, as well as the key technology details, finally, a concrete application and instance analysis are given to estimate the frameworkpsilas feasibility and superiority.	linux;software testing;systems architecture;test automation	Jie Hui;Lan Yu-Qing;Luo Pei;Guo Shu-Hang;Gao Jing	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1036	non-regression testing;test strategy;keyword-driven testing;model-based testing;white-box testing;manual testing;integration testing;computer science;software reliability testing;operating system;software engineering;database;software testing;real-time testing;programming language;test management approach;systems architecture;test harness	SE	-55.71565783633895	32.49730080309357	163034
0d660e6782409f0094029a48e99663e668080ee4	reverse-engineering 1-n associations from java bytecode using alias analysis	class diagram;program understanding;java bytecode;alias analysis;programming language;java programming;case tool;type inference;static program analysis;reverse engineering	1-n associations are design language constructs that represent one-to-many structural invariants for objects. To implement 1-n associations, container classes, such as Vector in Java, are frequently used as programming language constructs. Many of the current CASE tools fail to reverse-engineer 1-n associations that have been implemented via containers because sophisticated analyses are required to infer the type of elements collected in containers. This paper presents a new approach to reverse-engineering 1-n associations from Java bytecode based on alias analysis. In our approach, 1-n associations are inferred by examining the abstract heap structure that is constructed by applying an alias analysis on inter-variable relationships extracted from assignments and method invocations of containers. Our approach handles container alias problem that has been neglected by previous techniques by approximating the relationships between containers and elements at the object level rather than analyzing only the bytecode. Our prototype implementation was used with a suite of well-known Java programs. Most of the 1-n associations were successfully reverse-engineered from hundreds of class files in less than 1 minute.	alias analysis;java bytecode;reverse engineering	Yoohoon Kang;Chanjin Park;Chisu Wu	2007	Information & Software Technology	10.1016/j.infsof.2006.02.004	parallel computing;real-time computing;alias analysis;java concurrency;computer science;java modeling language;type inference;strictfp;class diagram;database;real time java;programming language;java;generics in java;reverse engineering;scala;static program analysis;java annotation	SE	-56.01610091019131	37.768197101620764	163180
1cb9f282294cbad3333072d675564ebd125be523	a relation-based method combining functional and structural testing for test case generation	software testing;software tool;functional testing;specification based testing;structural testing;test case generation;test methods;relationship based testing	Specification-based (or functional) testing enables us to detect errors in the implementation of functions defined in specifications, but since specifications are often incomplete in practice for some reasons (e.g., lack of ideas, no time to write), it is unlikely to be sufficient for testing all parts of corresponding programs. On the other hand, implementation-based (or structural) testing focuses on the examination of program structures, which allows us to test all parts of the programs, but may not be effective to show whether the programs properly implement the corresponding specifications. To perform a comprehensive testing of a program in practice, it is important to adopt both specification-based and implementation-based testing. In this paper we describe a relation-based test method that combines the specification-based and the implementation-based testing approaches. We establish a set of relations for test case generation, illustrate how the method is used with an example, and investigate the effectiveness and weakness of the method through an experiment on testing a software tool system. 2007 Elsevier Inc. All rights reserved.	functional testing;programming tool;sandy bridge;specification language;test case;white-box testing	Shaoying Liu;Yuting Chen	2008	Journal of Systems and Software	10.1016/j.jss.2007.05.036	non-regression testing;test strategy;domain testing;keyword-driven testing;reliability engineering;black-box testing;regression testing;model-based testing;orthogonal array testing;software performance testing;white-box testing;manual testing;integration testing;computer science;systems engineering;acceptance testing;software engineering;functional testing;session-based testing;risk-based testing;smoke testing;software testing;test method;system testing;test management approach;algorithm	SE	-59.168593873245534	33.70295535025446	163330
8a983ebd98c164d67de3d0fda21ba5103ea383b7	faster mutation-based fault localization with a novel mutation execution strategy	mutation cost reduction;dynamic mutation execution strategy software debugging mutation based fault localization mutation cost reduction;mutant test execution times mutation based fault localization approach mutation execution strategy program entity checking mutation analysis dynamic mutation execution strategy fault localization process faster mbfl;software fault tolerance probability program debugging program testing;dynamic mutation execution strategy;mutation based fault localization;history optimization measurement software conferences testing accuracy;software debugging	Checking program entities for finding faults is extremely tedious for developers. Fault localization techniques are designed to give a rank list of the probability that program entities incur faults to assist developers to locate faults. Mutation-based fault localization is a recently proposed fault localization approach via mutation analysis. With improved effectiveness, the mutation-based fault localization also brings huge execution cost. To reduce the execution cost of mutation-based fault localization technique, this paper proposes a dynamic mutation execution strategy which includes execution optimizations on both mutants and test cases. As fewer mutants and test cases are executed with the presented strategy, the fault localization process will be faster. The empirical studies show that mutation-based fault localization with the dynamic strategy we proposed, called Faster-MBFL, can reduce mutant-test execution times by 32.4% to 87% with keeping the fault localization accuracy unchanged; further, the additional run time required by utilizing our strategy can be ignored.	entity;mutation testing;run time (program lifecycle phase);software bug;test case	Pei Gong;Ruilian Zhao;Zheng Li	2015	2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)	10.1109/ICSTW.2015.7107448	reliability engineering;real-time computing;computer science;distributed computing;mutation testing	SE	-60.265675054435235	36.455762607525756	163559
8e91210374e55c8ddf4893fd9c4f84df89dbde2d	testing configuration knowledge-bases		Writing tests for configuration knowledge-bases is a difficult task. One not minor reason is the huge search space. For exhaustive testing, all possible combinations of configuration parameters must be considered. In practice, exhaustive testing is thus often impossible, due to the sheer, exponential, number of combinations. Consequently it becomes necessary to focus on the most important configurations first. This abstract challenge is well-known in the testing community, and can be addressed by exploiting combinatorial testing. Combinatorial testing deals with reducing the number of test inputs by aiming at exhaustive combinations of parameter subsets. That is, ensuring that a test-suite contains tests covering all value combinations for all parameter subsets for (or up to) a given size. In this paper, we formulate the configuration-testing problem and show how combinatorial testing can be used in a corresponding test case generation process, in order to achieve a huge reduction in the number of required test cases.	experiment;exploit (computer security);fault detection and isolation;knowledge-based configuration;knowledge-based systems;random testing;requirement;sensor;software testing;test case;test suite;time complexity	Franz Wotawa;Ingo Pill	2014			mathematical optimization;test case;exponential function;mathematics	SE	-60.136411482425395	34.25341948117955	163719
6c875e8a6488788c659e1ad33b1e2b848191b040	extraction of documentation from fortran 90 source code: an industrial experience	document handling;documentation generation;literate programming;literate programming documentation generation reverse engineering;fortran;fortran document handling;programming language documentation extraction fortran 90 source code software program;reverse engineering;documentation programming syntactics abstracts computer languages standards software	Software programs in scientific and engineering domains typically contain complex and extensive computations that are additionally optimized concerning running time or memory requirements. As a consequence, the actual computations are often hard to comprehend from the source code only. This paper presents tool support to extract documentation from source code that reveals the actual computation. We discuss how to generate the entire documentation from the source code automatically by using a minimum of additional annotations. This reduces the effort for writing documentation manually and ensures consistency between source code and documentation. Furthermore, the extracted documentation is given in a notation expected and comprehensible by domain experts. We applied our approach in the electrical engineering domain for software systems implemented in the Fortran 90 programming langugage.	computation;diagram;documentation;electrical engineering;fortran;literate programming;non-structured programming;requirement;software system;source-code annotation;subject-matter expert;symbolic execution;time complexity	Josef Pichler	2013	2013 17th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2013.58	common source data base;computer science;engineering;operating system;software engineering;technical documentation;computer programming;database;internal documentation;programming language;software documentation;docstring;reverse engineering;software system;source code	SE	-52.30494821810815	36.23133976399648	163758
165c9d60988b9a137c4bb13b882b735117e7bed5	codegraffiti: communication by sketching for pair programmers	pair programming;software development environment;software development;pen input;source code;code annotation	In pair programming, two software developers work on their code together in front of a single workstation, one typing, the other commenting. This frequently involves pointing to code on the screen, annotating it verbally, or sketching on paper or a nearby whiteboard, little of which is captured in the source code for later reference. CodeGraffiti lets pair programmers simultaneously write their code, and annotate it with ephemeral and persistent sketches on screen using touch or pen input. We integrated CodeGraffiti into the Xcode software development environment, to study how these techniques may improve the pair programming workflow.	integrated development environment;pair programming;programmer;software developer;software development;workstation;xcode	Leonhard Lichtschlag;Jan O. Borchers	2010		10.1145/1866218.1866260	intentional programming;code review;pair programming;human–computer interaction;computer science;software framework;software development;operating system;development environment;programming language;world wide web;static program analysis;source code	SE	-54.7272827381729	36.1050645883066	164053
3f7125601ee81af85906ba40badf5a5b536b1f8b	viscad: flexible code clone analysis support for nicad	visualization;visualization technique;code clone;analysis;clone detection;code clones	Clone detector results can be better understood with tools that support visualization and facilitate in-depth analysis. In this tool demo paper we present VisCad, a comprehensive code clone analysis and visualization tool that provides such support for the near-miss hybrid clone detection tool, NiCad. Through carefully selectedmetrics and visualization techniques VisCad can guide users to explore the cloning of a system from different perspectives.	duplicate code	Muhammad Asaduzzaman;Chanchal Kumar Roy;Kevin A. Schneider	2011		10.1145/1985404.1985425	real-time computing;computer science;bioinformatics;world wide web	Visualization	-55.4883983098915	34.921893658940604	164451
0f9e5118b6bca54fd9d723bb7b5539908a4394ec	a dynamic program analysis to find floating-point accuracy problems	program instrumentation;floating point accuracy;dynamic program;dynamic program analysis;binary translation;floating point;floating point arithmetic	Programs using floating-point arithmetic are prone to accuracy problems caused by rounding and catastrophic cancellation. These phenomena provoke bugs that are notoriously hard to track down: the program does not necessarily crash and the results are not necessarily obviously wrong, but often subtly inaccurate. Further use of these values can lead to catastrophic errors.  In this paper, we present a dynamic program analysis that supports the programmer in finding accuracy problems. Our analysis uses binary translation to perform every floating-point computation side by side in higher precision. Furthermore, we use a lightweight slicing approach to track the evolution of errors.  We evaluate our analysis by demonstrating that it catches wellknown floating-point accuracy problems and by analyzing the Spec CFP2006 floating-point benchmark. In the latter, we show how our tool tracks down a catastrophic cancellation that causes a complete loss of accuracy leading to a meaningless program result. Finally, we apply our program to a complex, real-world bioinformatics application in which our program detected a serious cancellation. Correcting the instability led not only to improved quality of the result, but also to an improvement of the program's run time.In this paper, we present a dynamic program analysis that supports the programmer in finding accuracy problems. Our analysis uses binary translation to perform every floating-point computation side by side in higher precision. Furthermore, we use a lightweight slicing approach to track the evolution of errors. We evaluate our analysis by demonstrating that it catches wellknown floating-point accuracy problems and by analyzing the SpecfiCFP2006 floating-point benchmark. In the latter, we show how our tool tracks down a catastrophic cancellation that causes a complete loss of accuracy leading to a meaningless program result. Finally, we apply our program to a complex, real-world bioinformatics application in which our program detected a serious cancellation. Correcting the instability led not only to improved quality of the result, but also to an improvement of the program's run time.	algorithm;benchmark (computing);binary translation;bioinformatics;bioinformatics;computation;dynamic program analysis;instability;loss of significance;programmer;rounding;run time (program lifecycle phase);software bug;spec#;valgrind;web application security scanner	Florian Benz;Andreas Hildebrandt;Sebastian Hack	2012		10.1145/2254064.2254118	real-time computing;simulation;computer hardware;computer science;floating point	SE	-58.778320956821666	39.06084524142692	164661
bd79bd2d723fced177586e56bad8a4b647ef2a9a	bakar alir: supporting developers in construction of information flow contracts in spark	formal specification;information retrieval;ide support;contracts;bakar alir interactive environment information flow browsing spark program ada certified safety security critical system formal specification data dependency program control dependency eclipse plug in program slicing program chopping information flow contracts;slicing;safety critical software;sparks contracts abstracts software input variables security;software tools contracts formal specification information retrieval program slicing safety critical software;spark;software tools;program slicing;chopping;ide support slicing chopping spark	This tool paper describes the design and implementation of an interactive environment for discovering and browsing information flow in SPARK programs. SPARK is a subset of Ada that has been used in a number of industrial contexts for implementing certified safety and security critical systems. SPARK requires explicit specification of information flow properties in the form of procedure contracts. To write such contracts, developers need to understand the data and control dependencies in the program. Our tool Bakar Alir, implemented as an Eclipse Plug-in, utilizes classic slicing and chopping techniques to assist developers in writing information flow contracts.	ada;altran praxis;apache spark;dependence analysis;eclipse;information assurance;information flow;tucker decomposition	Hariharan Thiagarajan;John Hatcliff;Jason Belt;Robby	2012	2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation	10.1109/SCAM.2012.25	program slicing;real-time computing;spark;computer science;software engineering;formal specification;database;programming language	SE	-51.06533994084714	33.85606125239645	165159
55e6bc0786ccf15bd24ad32e70e1b5657d62157a	on formal definition and analysis of formal verification processes		This paper suggests that there is considerable value in creating precise and formally-defined specifications of processes for carrying out formal verification, and in then subjecting those processes to rigorous analysis, and using the processes to guide the actual performance of formal verification. The paper suggests that some of the value could derive from widening the community of verifiers by having a process definition guide the performance of formal verification by newcomers or those who may be overawed by the complexities of formal verification. The paper also suggests that formally-defined process definitions can be of value both to novices and more experienced verifiers by serving as subjects of both dynamic and static analyses, with these analyses helping to build the confidence of various stakeholder groups (including the verifiers themselves) in the correct performance of the process and hence the correctness of the verification results. This paper is a status report on early work aimed at developing such processes, and demonstrating the feasibility and value of such analyses. The paper provides an example of a formally-defined verification process and suggests some kinds of dynamic and static analyses of the process. The process incorporates specification of both the nominal, ideal process as well as how the process must be iterated in response to such verification contingencies as incorrect assertions, incorrectly stated lemmas, and failed proofs of lemmas. In demonstrating how static analyses of this process can demonstrate that it assures certain kinds of desirable behaviors, the paper demonstrates an approach to providing effective verification guidance that assures sound verification results.	formal verification	Leon J. Osterweil	2014		10.1007/978-3-642-54624-2_2	formal system;formal methods;formal verification;formal semantics;formal specification;refinement	Logic	-48.68606933545659	34.12834649729475	165499
def7309a9197997994b77bc010dfbb709d970811	pex--an intelligent assistant for rigorous developer testing	program diagnostics;test driven development;unit testing;network operating systems;code coverage;automatic testing;rigorous developer testing;usa councils;automatic programming;program testing automatic programming learning artificial intelligence network operating systems program debugging program diagnostics;parameterized unit test;systems engineering and theory;bounded model checking;program behavior learning pex intelligent assistant rigorous developer testing net applications parameterized unit test automatic unit test case generation bug fix systematic program analysis;program testing;program behavior learning;performance analysis;net applications;bug fix;program analysis;program debugging;learning artificial intelligence;automatic unit test case generation;automatic testing usa councils performance analysis systems engineering and theory;systematic program analysis;pex intelligent assistant	Summary form only given. Pex takes test-driven development to the next level. Pex analyzes .NET applications. From a parameterized unit test, which serves as a specification, it automatically produces traditional unit tests cases with high code coverage. Moreover, when a generated test fails, Pex can often suggest a bug fix. To do so Pex performs a systematic program analysis (similar to path bounded model-checking). It records detailed execution traces of test cases. Pex learns the program behavior from the traces, and a constraint solver produces new test cases with different behavior. The result is a minimal test suite with maximal code coverage. For information about Pex, see http://research.microsoft.com/Pex/.	code coverage;maximal set;model checking;patch (computing);program analysis;solver;test case;test suite;test-driven development;tracing (software);unit testing	Wolfram Schulte	2007	12th IEEE International Conference on Engineering Complex Computer Systems (ICECCS 2007)	10.1109/ICECCS.2007.35	program analysis;test-driven development;simulation;computer science;engineering;artificial intelligence;theoretical computer science;operating system;software engineering;unit testing;code coverage;programming language	SE	-59.40093274138869	36.426860925610605	165543
eb291ee1754814666bf8ab135b2873dc24348e78	linux kernel developer responses to static analysis bug reports	address easy-to-fix bug;linux kernel developer response;actively-maintained code;triage bug;code quality;deeper analysis;static analysis bug report;linux kernel developer;static analysis tool;actively-maintained file;triage report;automated tool	We present a study of how Linux kernel developers respond to bug reports issued by a static analysis tool. We found that developers prefer to triage reports in younger, smaller, and more actively-maintained files ( §2), first address easy-to-fix bugs and defer difficult (but possibly critical) bugs ( §3), and triage bugs in batches rather than individually (§4). Also, although automated tools cannot find many types of bugs, they can be effective at directing developers’ attentions towards parts of the codebase that contain up to 3X more user-reported bugs ( §5). Our insights into developer attitudes towards static analysis tools allow us to make suggestions for improving their usability and effectiveness. We feel that it could be effective to run static analysis tools continuously while programming and before committing code, to rank reports so that those most likely to be triaged are shown to developers first, to show the easiest reports to new developers, to perform deeper analysis on more actively-maintained code, and to use reports as indirect indicators of code quality and importance.	linux;software bug;software quality;static program analysis;usability	Philip J. Guo;Dawson R. Engler	2009			real-time computing;software bug;computer science;operating system;world wide web	OS	-62.59744415613531	37.08844106015563	165914
f5b29201547a960ec0a4eb749bb04b8a906160e9	tree slicing: finding intertwined and gapped clones in one simple step		Most of software nowadays contain code duplication that leads to serious problems in software maintenance. A lot of different clone detection approaches have been proposed over the years to deal with this problem, but almost all of them do not consider semantic properties of the source code. We propose to reinforce traditional tree-based clone detection algorithm by using additional information about variable slices. This allows to find intertwined/gapped clones on variables; preliminary evaluation confirms applicability of our approach to real-world software.	algorithm;duplicate code;software maintenance	Marat Kh. Akhin;Vladimir M. Itsykson	2013	Automatic Control and Computer Sciences	10.3103/S0146411613070171	computer science;theoretical computer science;distributed computing;algorithm	SE	-57.568124527235526	35.84845848710745	166502
a816e3f40f6f08e6dc48bb1ebbf2fe1ab0b9b838	on automatically generating commit messages via summarization of source code changes	open source systems source code change summarization version control systems java projects descriptive messages commit message summarizing software changes software development task software maintenance task changescribe change sets automatic natural language commit message generation commit stereotype change type file rename property file changes;code changes commit message summarization;source code software configuration management public domain software software maintenance;summarization;java software abstracts context natural languages visualization production facilities;commit message;code changes	Although version control systems allow developers to describe and explain the rationale behind code changes in commit messages, the state of practice indicates that most of the time such commit messages are either very short or even empty. In fact, in a recent study of 23K+ Java projects it has been found that only 10% of the messages are descriptive and over 66% of those messages contained fewer words as compared to a typical English sentence (i.e., 15-20 words). However, accurate and complete commit messages summarizing software changes are important to support a number of development and maintenance tasks. In this paper we present an approach, coined as Change Scribe, which is designed to generate commit messages automatically from change sets. Change Scribe generates natural language commit messages by taking into account commit stereotype, the type of changes (e.g., files rename, changes done only to property files), as well as the impact set of the underlying changes. We evaluated Change Scribe in a survey involving 23 developers in which the participants analyzed automatically generated commit messages from real changes and compared them with commit messages written by the original developers of six open source systems. The results demonstrate that automatically generated messages by Change Scribe are preferred in about 62% of the cases for large commits, and about 54% for small commits.	angular defect;control system;design rationale;ibm notes;java;natural language;open-source software;rename (relational algebra);software development;stereotype (uml);truncation;version control	Luis Fernando Cortes-Coy;Mario Linares Vásquez;Jairo Aponte;Denys Poshyvanyk	2014	2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation	10.1109/SCAM.2014.14	three-phase commit protocol;commit;computer science;automatic summarization;database;programming language;world wide web	SE	-52.74797944384539	36.180938157131	166631
0a5e354dd77e99f9e82bed403c28989de3b0b28b	checking inside the black box: regression testing by comparing value spectra	software testing;empirical study;regression testing;path spectra comparison regression testing program version software maintenance black box program program behavior program spectra deviation propagation call tree c program value spectra comparison;index terms program spectra;software maintenance;program spectra;indexing terms;formal verification;program testing;regression analysis program testing software maintenance formal verification;software maintenance index terms program spectra regression testing software testing empirical studies;regression analysis;empirical studies;software testing software maintenance fault diagnosis propagation losses program processors optimizing compilers	Comparing behaviors of program versions has become an important task in software maintenance and regression testing. Black-box program outputs have been used to characterize program behaviors and they are compared over program versions in traditional regression testing. Program spectra have recently been proposed to characterize a program's behavior inside the black box. Comparing program spectra of program versions offers insights into the internal behavioral differences between versions. In this paper, we present a new class of program spectra, value spectra, that enriches the existing program spectra family. We compare the value spectra of a program's old version and new version to detect internal behavioral deviations in the new version. We use a deviation-propagation call tree to present the deviation details. Based on the deviation-propagation call tree, we propose two heuristics to locate deviation roots, which are program locations that trigger the behavioral deviations. We also use path spectra (previously proposed program spectra) to approximate the program states in value spectra. We then similarly compare path spectra to detect behavioral deviations and locate deviation roots in the new version. We have conducted an experiment on eight C programs to evaluate our spectra-comparison approach. The results show that both value-spectra-comparison and path-spectra-comparison approaches can effectively expose program behavioral differences between program versions even when their program outputs are the same, and our value-spectra-comparison approach reports deviation roots with high accuracy for most programs.	approximation algorithm;black box;code refactoring;comparison sort;heuristic (computer science);oldversion.com;rca spectra 70;regression testing;root-finding algorithm;software bug;software maintenance;software propagation;software quality	Tao Xie;David Notkin	2005	IEEE Transactions on Software Engineering	10.1109/TSE.2005.107	reliability engineering;computer science;software engineering;data mining;empirical research;algorithm	SE	-60.22061424639011	36.652100200411	166851
3073d9772a1d0f3f940c9eca2881664da47684b3	jpet: an automatic test-case generator for java	program comprehension testing test case generation;software;generators;program comprehension;testing;automatic programming;source coding automatic programming java program testing;positron emission tomography;test case generation;program testing;source code jpet whitebox test case generator software development eclipse environment tcg bytecode java program reverse engineering;java positron emission tomography programming concrete testing software generators;programming;concrete;source coding;java	We present jPET, a white box test-case generator (TCG) which can be used during software development of Java applications within the Eclipse environment. jPET builds on top of PET, a TCG which automatically obtains test-cases from the byte code associated to a Java program. jPET performs reverse engineering of the test-cases obtained at the byte code level by PET in order to yield this information to the user at the source code level. This allows understanding the information gathered at the lower level and using it to test source Java programs.	byte;eclipse;java;polyethylene terephthalate;reverse engineering;software development;white box (software engineering);white-box testing	Elvira Albert;Israel Cabanas;Antonio Flores-Montoya;Miguel Gómez-Zamalloa;Sergio Gutierrez	2011	2011 18th Working Conference on Reverse Engineering	10.1109/WCRE.2011.67	programming;real-time computing;concrete;computer science;operating system;software testing;programming language;java;source code	SE	-56.8518299115784	36.33009051942317	166911
ec991409db54863a61bda45e3b8833c1ae75e9a2	an optimal solution for test case generation using robdd graph and pso algorithm	specification based testing;reduced ordered binary decision diagram;test case generation;cause effect graph testing	Software testing is one of the most important techniques to examine the behavior of the software products to assure their quality. An effective and efficient testing approach must balance two important but conflicting requirements. One of them is the accuracy that needs a large number of test cases for testing, and the second one is reducing the time and cost, which requires a few test cases. Even for small software, the number of possible test cases is typically very large, and exhaustive testing is impractical. Hence, selecting appropriate test suite is necessary. Cause–effect graph testing is a common black-box testing technique, which is equivalently representing Boolean relations between input parameters. However, the other traditional black-box strategies cannot identify the relations that it may result in loss of some of the important test cases. Although the cause–effect graph is regarded very promising in specification testing, it is observed that most of the proposed approaches using the graph are complex or generate impossible and a large number of test cases. This observation has motivated our research to propose an efficient strategy to generate minimal test suite that simultaneously achieves high coverage of input parameters. To do so, at first, we identify major effects from the cause–effect graph using reduced ordered binary decision diagram (ROBDD). ROBDD makes the related Boolean expression of the graph concise and obtains a unique representation of the expression. Using the ROBDD, it is possible to reduce the size of the generated test suite and to perform testing faster. After that, our proposed method utilizes particle swarm optimization (PSO) algorithm to select the optimal test suite, which covers all pairwise combinations of input parameters. The experimental results show that our approach simultaneously achieves high efficacy and reduces cost of testing by selecting appropriate test cases, respectively, to both test size and coverage size. Also, it outperforms some existing state-of-the-art strategies in the black-box testing. Copyright © 2015 John Wiley u0026 Sons, Ltd.	algorithm;binary decision diagram;particle swarm optimization;test case	Akram Kalaee;Vahid Rafe	2016	Quality and Reliability Eng. Int.	10.1002/qre.1934	computer science;theoretical computer science;machine learning;algorithm	EDA	-60.19482485116856	34.37483744386631	167404
e885a2aa44b1cbefe43309ca702632d554bc12d0	automatic generation of service availability models	high availability;bridging business and it architecture;reliability;availability;computer model;information technology;biological system modeling;maintenance engineering;automatic generation;service model;computational modeling;business data processing;business;markov process;availability business biological system modeling computational modeling markov processes maintenance engineering;markov processes;services models;service oriented architecture;it service management;it service management reliability high availability services models bridging business and it architecture;systematic model based methodology it service management trustworthy service delivery business process availability service availability model investment return it architecture service level agreement;service oriented architecture business data processing information technology reliability	In the world where on-demand and trustworthy service delivery is one of the main preconditions for successful business, service and business process availability is of the paramount importance and cannot be compromised. For that reason service availability is coming into central focus of the IT operations and management research and practice. Still, our understanding of service and process availability is mostly empirical and at best, sketchy. Services are assessed using a mixture of qualitative, quantitative, and analytical methods, with results of varying quality. We introduce a systematic model-based methodology and a tool for service and business process availability assessment. The main advantage of the proposed method is the ability to automatically generate availability models, based on the service/process description and technical infrastructure it is executing on. Therefore, service level agreements can be tested/simulated or return on investment calculation can be performed, without the need for costly experiments and/or actual investments.	business process;cognitive dimensions of notations;experiment;formal language;itil;information security;markov chain;mathematical model;mean time between failures;mean time to repair;precondition;reliability engineering;service-level agreement;simulation;single point of failure;state space;technical standard;trustworthy computing;windows update	Nikola Milanovic;Bratislav Milic	2011	IEEE Transactions on Services Computing	10.1109/TSC.2010.11	maintenance engineering;service level requirement;service level objective;service product management;business service provider;computer science;service delivery framework;service design;service guarantee;database;markov process;information technology;statistics;incident management	Visualization	-49.19333703298076	41.55552598635534	167418
099028b424a7562ef51eaba9a5d3b8ad96496b86	reverse engineering of the uml class diagram from c++ code in presence of weakly typed containers	libraries;class diagram;information resources;computer languages;reverse engineering unified modeling language containers data mining libraries collaborative work information resources engines computer languages java;collaborative work;design and development;software maintenance;uml;development process;data mining;inter class relations;uml class diagram;object oriented systems;c language;engines;specification languages;unified modeling language;c code;design rationale;inter class relations reverse engineering uml class diagram c code weakly typed containers;weakly typed containers;flow analysis;containers;reverse engineering;java;c language reverse engineering software maintenance specification languages	UML diagrams, and in particular the most frequently used one, the class diagram, represent a valuable source of information even after the delivery of the system, when it enters the maintenance phase. Several tools provide a reverse engineering engine to recover it from the code. In this paper an algorithm is proposed for the improvement of the accuracy of the UML class diagram extracted from the code. Specifically, important information about inter-class relations may be missed in a reverse engineered class diagram, when weakly typed containers, i.e., containers collecting objects whose type is the top of the inheritance hierarchy, are employed. In fact, the class of the contained objects is not directly known, and therefore no relation with it is apparent from the container declaration. The proposed approach was applied to several software components developed at CERN. Experimental results highlight that a substantial improvement is achieved when the container type information is refined with the inferred data. The number of relations otherwise missed is relevant and the connectivity of the associated class diagrams is radically different when containers are considered.	c++;class diagram;reverse engineering;strong and weak typing;unified modeling language	Paolo Tonella;Alessandra Potrich	2001		10.1109/ICSM.2001.972750	unified modeling language;computer science;systems engineering;software engineering;class diagram;database;programming language	SE	-54.042862944388595	33.1344637762843	167553
b88bf636c4595380f17d24e13e7192e3c7dc19be	an integrated approach for studying architectural evolution	software metrics;integrated approach;software maintenance;search engines;software systems;software structure;software engineering;beagle architectural evolution software system metrics software visualization origin analysis structural change architectural change query engine web based visualization interface web based navigation interface software maintenance;software systems software maintenance history software architecture navigation software tools data visualization computer science engines software prototyping;user interfaces software architecture software metrics software maintenance program visualisation program compilers internet search engines;software architecture;internet;software evolution;software development;program compilers;user interfaces;program visualisation	Studying how a software system has evolved over time is difficult, time consuming, and costly; existing techniques are often limited in their applicability, are hard to extend, and provide little support for coping with architectural change. This paper introduces an approach to studying software evolution that integrates the use of metrics, software visualization, and origin analysis, which is a set of techniques for reasoning about structural and architectural change. Our approach incorporates data from various statistical and metrics tools, and provides a query engine as well as a web-based visualization and navigation interface. It aims to provide an extensible, integrated environment for aiding software maintainers in understanding the evolution of long-lived systems that have undergone significant architectural change. In this paper, we use the evolution of GCC as an example to demonstrate the uses of various functionalities of BEAGLE, a prototype implementation of the proposed environment.	beagle;comparison of command shells;gnu compiler collection;prototype;software evolution;software system;software visualization;web application	Qiang Tu;Michael W. Godfrey	2002		10.1109/WPC.2002.1021334	software architecture;the internet;architectural pattern;computer science;systems engineering;engineering;software evolution;software development;operating system;software engineering;user interface;software maintenance;software metric;software system;computer engineering	SE	-54.168116052597746	34.59704116863944	167587
1a747f7ad9a28d804c72bc0f5be3545de8129b12	a practical framework for the dataflow pointcut in aspectj	software;aspectj;computer languages;electronic mail;security aspect dataflow pointcut aspectj compiler aspect oriented programming language java programming language program execution;java programming;security aspectj dataflow;security aspect;object oriented programming;data mining;program execution;dataflow;aspect oriented programming;security of data data flow analysis java object oriented programming program compilers;java programming language;aspectj compiler;aspect oriented programming language;data flow analysis;crosscutting concerns;weaving;dataflow pointcut;data security information security open source software computer languages application software computer security design engineering java availability laboratories;program compilers;security;security of data;java	In this paper, we present the design and the implementation of the dataflow pointcut in AspectJ compiler ajc 1.5.0. Some security concerns are sensitive to flow of information in a program execution. The dataflow pointcut has been proposed by Masuhara and Kawauchi in order to easily implement such security concerns in aspect-oriented programming languages. The pointcut identifies join points based on the origins of values. The dataflow pointcut can detect and fix a lot of vulnerabilities that result from not validating input effectively, e.g., web application vulnerabilities, process injection, log forging, and path injection. AspectJ extends the Java programming language to implement crosscutting concerns modularly in general. The implementation methodology of the dataflow pointcut which depends in define-use analysis is described in detail together with case studies that demonstrate how the implemented dataflow pointcut can detect a considerable number of vulnerabilities.	aspect-oriented programming;aspectj;compiler;cross-cutting concern;data dependency;dataflow architecture;java;java bytecode;join point;modular programming;pointcut;programming language;web application	Amine Boukhtouta;Dima Alhadidi;Mourad Debbabi	2009	2009 International Conference on Availability, Reliability and Security	10.1109/ARES.2009.86	parallel computing;real-time computing;aspect-oriented programming;computer science;programming language	SE	-57.45078562063247	40.608168123713135	167990
78aac3186733c5a80665dce57739d8b1d44bc5fb	mbeddr: an extensible c-based programming language and ide for embedded systems	dsls;development environments;formal methods;language extension;embedded software	While the C programming language provides good support for writing efficient, low-level code, it is not adequate for defining higher-level abstractions relevant to embedded software. In this paper we present the mbeddr technology stack that supports extension of C with constructs adequate for embedded systems. In mbeddr, efficient low-level programs can be written using the well-known concepts from C. Higher-level domain-specific abstractions can be seamlessly integrated into C by means of modular language extension regarding syntax, type system, semantics and IDE. In the paper we show how language extension can address the challenges of embedded software development and report on our experience in building these extensions. We show that language workbenches deliver on the promise of significantly reducing the effort of language engineering and the construction of corresponding IDEs. mbeddr is built on top of the JetBrains MPS language workbench. Both MPS and mbeddr are open source software.	capability maturity model;debugger;diagram;digital subscriber line;embedded software;embedded system;enumerated type;graphical user interface;high- and low-level;integrated development environment;language workbench;legacy code;mps (format);open-source software;preprocessor;program analysis;sensor;smart meter;software development;solution stack;structural integrity and failure;the c programming language;type system;whole earth 'lectronic link	Markus Völter;Daniel Ratiu;Bernhard Schätz;Bernd Kolb	2012		10.1145/2384716.2384767	interface description language;first-generation programming language;computer architecture;formal methods;language primitive;embedded software;specification language;data control language;computer science;theoretical computer science;software development;operating system;low-level programming language;programming language;programming language specification;high-level programming language	PL	-50.86729847113019	32.46232099831282	168061
1243d56328f05025a78187a607aaee3ce595037c	source code reuse evaluation by using real/potential copy and paste	software;detectors;source code reuse reuse based software engineering teaching code clone;history;software cloning history ieee potentials software engineering detectors productivity;software engineering;cloning;software reusability software quality;quantitative assessment source code reuse evaluation real potential copy and paste software quality reusable code fragment source code reuse behavior code clone detection tool;productivity;ieee potentials	Developers often reuse existing software by copy and paste. Source code reuse improves productivity and software quality. On the other hand, source code reuse requires several professional skills to developers. In source code reuse, developers must locate reusable code fragments, and judge whether such reusable code is adequate to copy and paste into the source file under development. This paper presents extraction and analysis methods for developers' source code reuse behavior (copy and paste). Our method extracts developers' actual source code reuse (real copy and paste). Then, by using a code clone detection tool, the method extracts code fragments for (potential reuse). Our study of real and potential copy and paste provides a quantitative assessment for source code reuse by developers.	code reuse;cut, copy, and paste;duplicate code;hoc (programming language);software development;software quality;stylometry	Takafumi Ohta;Hiroaki Murakami;Hiroshi Igaki;Yoshiki Higo;Shinji Kusumoto	2015	2015 IEEE 9th International Workshop on Software Clones (IWSC)	10.1109/IWSC.2015.7069887	kpi-driven code analysis;real-time computing;codebase;code review;computer science;operating system;software engineering;software construction;software maintainer;source lines of code;static program analysis;source code	SE	-57.72702798417029	36.25559663834207	168319
514d5f20f001ed9abcf211018e922ddb811295a8	exposing unforeseen consequences of software change	call graph;dependence graph;control structure;source code;software change;source behavior relationship	Changing source code can have unintended effects on a program's behavior. Seemingly trivial changes have incurred significant cost, distress, and catastrophe: that is, the concern about the consequences of a change is not merely theoretical. At the same time, many -- perhaps most -- software changes do not cause problems in practice, instead improving the program's behavior in intended ways. The changes that improve program behavior clearly collectively outweigh those that harm it. Nevertheless, nobody would argue that programmers make changes with certainty about the future behavior of a program. This is in part due to Dijkstra's observation from decades ago about the conceptual gap between the static program and the dynamic execution process. This observation led to many aspects of structured programming, most notably the aggressive use of one-in/one-out control structures such as if-then-else statements and while loops. However, for a number of reasons, the source-behavior relationship has become more opaque rather than less so, leaving programmers with relatively little help in distinguishing between changes leading to intended versus unintended behaviors. To aid programmers facing these difficulties, we have developed an approach that takes a pair of program versions, models each version's source code as static call graphs and each version's behavior as dynamic call graphs, partitions these four dependence graphs based on their set intersections, and identifies partitions that tend to expose changes that are likely to have unforeseen consequences on the program's source-behavior relationship. This is joint work with Reid Holmes.	catastrophe theory;conditional (computer programming);control flow;inventory;out-of-order execution;programmer;structured programming;unintended consequences;while loop	David Notkin	2010		10.1145/1730874.1730875	call graph;simulation;computer science;engineering;artificial intelligence;theoretical computer science;operating system;mathematics;programming language;control flow;management;world wide web;computer security;algorithm;source code	SE	-59.71308621732575	42.532362402388124	168332
0ce102af6aede7ea773f5ef522808ae3199d8bcd	partial automation of an integrated reverse engineering environment of binary code	automation reverse engineering binary codes software tools investments software systems assembly operating systems computer science costs;tools binary translation techniques software migration binary code partially automated integrated reverse engineering environment executable code disassemblers binary translators decompilers;binary translation;disa;reverse engineering	The constant development of newer and faster machines requires software to be made available on those new machines at a rate faster than what it takes to develop the software. The use of binary translation techniques to migrate software from one machine to another is effective—it makes software available in little time without incurring reprogramming costs. However, the development of such a tool is in itself an issue, as with each new architecture, a new tool needs to be written. We present a partially automated integrated environment for the reverse engineering of binary or executable code. This environment is suitable for the development of disassemblers, binary translators and decompilers.	automation;binary code;binary file;binary translation;comparison of command shells;decompiler;disassembler;executable;powerpc;reverse engineering;sparc;system migration	Cristina Cifuentes	1996		10.1109/WCRE.1996.558851	computer architecture;computer science;engineering;software development;operating system;software engineering;software construction;programming language;reverse engineering;computer engineering	SE	-49.33969918196545	33.0684378637763	168372
477f3c5530b19278e470590c17b97fb63236f0d0	design by units: abstractions for human and compute resources for elastic systems	elasticity;resource allocation;system analysis and design;resource management;abstraction;inference mechanisms;human factors elasticity computational modeling cloud computing system analysis and design resource management;computational modeling;human factors;resource allocation cloud computing inference mechanisms;service system design human resources compute resources elastic systems complex system qualities elasticity system measurability system management units infrastructure resources;design;abstraction design elasticity resource;cloud computing;resource	Units make the usage and properties of diverse resources, including infrastructure and human resources, explicit early in system design, and allow for reasoning about complex system qualities, such as elasticity. They advance the measurability and management of systems whose quality depends largely on the resources that the system uses.	complex system;elasticity (data store);systems design	Stefan Tai;Philipp Leitner;Schahram Dustdar	2012	IEEE Internet Computing	10.1109/MIC.2012.81	design;simulation;cloud computing;resource allocation;computer science;resource management;abstraction;management science;utility computing;elasticity;human resource management system;resource	OS	-49.1225599195277	42.84783474614845	168847
f9154d44f4bea9204e6632d067b38b342f45d9b0	reliability, sampling, and algorithmic randomness	software testing;random sampling;satisfiability;reliability assessment;kolmogorov complexity	We investigate the relationship between software testing, statistical reliability modelling, and random sampling. Let P be a program whose reliability is assessed statistically based on the number of times P fails when executed on a sample S of its inputs. We show that if this assessment is grossly misleading, then either the degree of randomness (Kolmogorov complexity) of S is not high or the sum of the lengths of P and any program that satisfies the requirements for P is not much less than the size of the “population” of runs from which S was drawn. This implies that there is an a priors condition that, if satisfied, guarantees that a reliability assessment obtained by sampling cannot be grossly misleading.	algorithmically random sequence;kolmogorov complexity;monte carlo method;randomness;reliability engineering;requirement;sampling (signal processing);software testing;statistical model	Andy Podgurski	1991		10.1145/120807.120809	sampling;theoretical computer science;software engineering;randomness tests;software testing;satisfiability	Logic	-61.621654455362105	33.80303200449719	169040
4d83f83163c97d825ce37b76a2f846ca8181ccd7	constraints based approach to interactive feature location		Feature location is a maintenance task to identify the implementation of a feature within the source code. To automate or support the task, extensive studies have been conducted on feature location techniques. In this paper, we focus on certain static and dynamic constraints regarding feature additions to object-oriented programs, and construct an interactive feature location procedure based on the constraints. We manually conducted a case study for several features of a real-world program on the assumption that the user always correctly answers the questions asked by the procedure. The results show that over 75% of the feature's implementation could be efficiently covered by the procedure with relatively small number of execution traces.	correctness (computer science);semantics (computer science);tracing (software)	Daiki Fujioka;Naoya Nitta	2017	2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2017.55	feature (computer vision);computer science;feature model;data mining;software;source code;feature extraction;small number	SE	-58.080395315543605	37.8353437217328	169061
ede90235eace1ed377f52680a19fcd8a42fa06c1	a similarity-aware approach to testing based fault localization	fault localization;debugging;fuzzy set;maintenance;software development	Debugging is a time-consuming task in software development and maintenance. To accelerate this task, several approaches have been proposed to automate fault localization. In particular, testing based fault localization (TBFL), which utilizes the testing information to localize the faults, seem to be very promising. However, the similarity between test cases in the test suite has been ignored in the research on TBFL. In this paper, we investigate this similarity issue and propose a novel approach named similarity-aware fault localization (SAFL), which can calculate the suspicion probability of each statement with little impact by the similarity issue. To address and deal with the similarity between test cases, SAFL applies the theory of fuzzy sets to remove the uneven distribution of the test cases. We also performed an experimental study for two real-world programs at different size levels to evaluate SAFL together with another two approaches to TBFL. Experimental results show that SAFL is more effective than the other two approaches when the test suites contain injected redundancy, and SAFL can achieve a competitive result with normal test suites. SAFL can also be more effective than applying test suite reduction to current approaches to TBFL.	debugging;experiment;fuzzy set;software development;test case;test suite	Dan Hao;Ying Pan;Lu Zhang;Wei Zhao;Hong Mei;Jiasu Sun	2005		10.1145/1101908.1101953	reliability engineering;computer science;theoretical computer science;software development;fuzzy set;programming language;debugging;algorithm	SE	-60.4914261835986	36.07178244673332	169180
ed5fd251a6628bbcbf6af08189051b995822e524	deductive verification of legacy code		Deductive verification is about proving that a piece of code conforms to a given requirement specification. For legacy code, this task is notoriously hard for three reasons: (1) writing specifications posthoc is much more difficult than producing code and its specification simultaneously, (2) verification does not scale as legacy code is often badly modularized, (3) legacy code may be written in a way such that verification requires frequent user interaction. We give examples for which characteristics of (imperative) legacy code impede the specification and verification effort. We also discuss how to handle the challenges of legacy code verification and suggest a strategy for post-hoc verification, together with possible improvements to existing verification approaches. We draw our experience from two case studies for verification of imperative implementations (in Java and C) in which we verified legacy software, i.e., code that was provided by a third party and was not designed to be verified.	best practice;best, worst and average case;code refactoring;deductive database;design pattern;formal methods;formal verification;functional requirement;hoc (programming language);imperative programming;iteration;java;key;legacy code;legacy system;parsing;program analysis;program slicing;refinement (computing);scalability;software engineering;software verification;virtual collective consciousness	Bernhard Beckert;Thorsten Bormer;Daniel Grahl	2016		10.1007/978-3-319-47166-2_53	programming language;software product line;legacy code;computer science	SE	-52.230921579939235	33.4287008430572	169265
6575a9be2227b5d08e335913d22dadf4cd52641a	stac: software tuning panels for autonomic control	large applications;software testing;legacy software;java programming;code coverage;software systems;proof of concept;autonomous control;legacy system;autonomic computing;open source	One aspect of autonomic computing is the ability to identify, separate and automatically tune parameters related to performance, security, robustness and other properties of a software system. Often the response to events affecting these properties consists of adjusting tuneable system parameters such as table sizes, timeout limits, restart checks and so on. In many ways these tuneable parameters correspond to the switches and potentiometers on the control panel of many hardware devices. While modern software systems designed for autonomic control may make these parameters easily accessible, in legacy systems they are often scattered or deeply hidden in the software source.In this paper we introduce Software Tuning Panels for Autonomic Control (STAC), a system for automatically re-architecting legacy software systems to facilitate autonomic control. STAC works to isolate tuneable system parameters into one visible area of a system, producing a resulting architecture that can be used in conjunction with an autonomic controller for self-maintenance and tuning. A proof-of-concept implementation of STAC using source transformation is presented along with its application to the automatic re-architecting of two open source Java programs. Use of the new architecture in monitoring and autonomic control is demonstrated on these examples.	autonomic computing;java;legacy system;lempel–ziv–stac;memory controller;network switch;open-source software;plugboard;potentiometer;software system;source transformation	Elizabeth Dancy;James R. Cordy	2006		10.1145/1188966.1188982	embedded system;real-time computing;computer science;software development;operating system;software engineering;software construction;database;distributed computing;real-time control system software;programming language;legacy system;software system;autonomic computing	SE	-51.993019618333626	38.66538069330628	169404
4d56f26683487868fe36de7dd805d4521722316e	development and evaluation of a slicing-based c++ debugger	databases;debugging;incremental development;slicing and dicing;information technology;static slicing;optical computing;static slicing slicing based c debugger development c program debugging tool c debug program slicing dicing technique incremental development dynamic slicing;runtime;flow graphs;structure function;computer architecture;c language;dynamic slicing;heuristic algorithms;c program debugging tool;c debug;dicing technique;slicing based c debugger development;program debugging c language program slicing;algorithms;debugging computer science heuristic algorithms runtime computer architecture flow graphs information technology databases optical computing;evaluation;evaluation debugging slicing and dicing dynamic slicing algorithms;program debugging;computer science;program slicing	The main objective of this work was to develop a debugging tool for C++ programs called C++Debug that uses program slicing and dicing techniques. The incremental development started by including simple statements first and then adding pointers, structures, functions, and classes. In order for C++Debug to be more powerful, dynamic slicing rather than static slicing was chosen. The work included new algorithms that handle class, function, and pointer in C++. In this paper, the overall architecture of C++Debug is presented and an overview of the algorithms devised for the debugger is discussed. The limitations of the tool are also listed and the results of a small-scale evaluation of the debugger is reported.	algorithm;c++;debugger;iterative and incremental development;opaque pointer;pointer (computer programming);program slicing	Winai Wichaipanitch;Mansur H. Samadzadeh;Songsri Tangsripairoj	2005	International Conference on Information Technology: Coding and Computing (ITCC'05) - Volume II	10.1109/ITCC.2005.127	program slicing;parallel computing;real-time computing;run-time type information;c++;computer science;programming language;smart pointer	SE	-58.41188232828885	36.346228478083304	169589
743a31efa6ade80bd119aed58dd5a88847bb76cd	timestamp based execution control for c and java programs	java bytecode;java programming;executive control	Many programmers have had to deal with an overwritten variable resulting for example from an aliasing problem. The culprit is obviously the last write-access to that memory location before the manifestation of the bug. The usual technique for removing such bugs starts with the debugger by (1) finding the last write and (2) moving the control point of execution back to that time by re-executing the program from the beginning. We wish to automate this. Step (2) is easy if we can somehow mark the last write found in step (1) and control the execution-point to move it back to this time. In this paper we propose a new concept, position, that is, a point in the program execution trace, as needed for step (2) above. The position enables debuggers to automate the control of program execution to support common debugging activities. We have implemented position in C by modifying GCC and in Java with a bytecode transformer. Measurements show that position can be provided with an acceptable amount of overhead.	aliasing;breakpoint;code;control point (mathematics);debugger;debugging;gnu compiler collection;global variable;hippocampal replay;java;java platform debugger architecture;java virtual machine;line number;memory address;overhead (computing);programmer;software bug;thread (computing);tracing (software);transformer;x window system	Kazutaka Maruyama;Minoru Terada	2003	CoRR		real-time computing;computer science;operating system;real time java;programming language;java	PL	-57.44327027005431	39.38719370775483	169952
376051f500bde3961a976b74b039709156097ed0	lessons learned in migrating from swing to javafx		We describe our experience with the migration of a diagramming tool written in Java from the Swing Graphical User Interface framework to the more recent JavaFX framework, in the general context of research of software evolution. The experience led to a number of unexpected realizations about the impact of subtle differences in the design of framework features and their documentation.	automated planning and scheduling;discoverability;documentation;experiment;graphical user interface;information discovery;information needs;java;javafx;list of concept- and mind-mapping software;pixel;risk management;software evolution;software framework;stack overflow;swing (java)	Martin P. Robillard;Kaylee Kutschera	2018	CoRR			SE	-53.99110940933664	35.06534129553724	170033
6bb6b75ba7054dfaa7756507d1fdf242f52ec270	data base concepts applied to generalized programming packages	software package;generic programming	"""Software """"packages"""" are becoming more and more prevalent in both use and availability. These packages provide many useful functions, without the costly, time-consuming development and testing normally required to implement the desired operations. A disadvantage of many packages, however, is the difficulty in modifying them in order to receive the desired result, if the original resultant of the package is not totally satisfactory."""	database;error message;resultant	G. Cort Steinhorst;Barry L. Bateman;Daniel L. Curtis	1974		10.1145/1500175.1500254	real-time computing;computer science;package development process;theoretical computer science;package;engineering drawing	SE	-51.36224521122317	35.69905131387113	170112
55cf0bf854cb1c28cfec73511f3e7c3f83857276	can clone detection support test comprehension?	software testing;software maintenance;program comprehension;industrial software systems clone detection test comprehension software system central artifacts;program testing;clone detection program comprehension software testing software maintenance;clone detection;cloning manuals testing automation inspection optimization maintenance engineering	Tests are central artifacts of software systems. Therefore, understanding tests is essential for activities such as maintenance, test automation, and efficient execution. Redundancies in tests may significantly decrease their understandability. Clone detection is a technique to find similar parts in software artifacts. We suggest using this technique to gain a better understanding of tests and to provide guidance for testing activities. We show the capabilities as well as the limits of this approach by conducting a case study analyzing more than 4000 tests of seven industrial software systems.	4000 series;analysis of algorithms;cardinality (data modeling);duplicate code;mathematical optimization;natural language;redundancy (engineering);semiconductor consolidation;software development;software system;system testing;test automation;test data;test suite	Benedikt Hauptmann;Maximilian Junker;Sebastian Eder;Elmar Jürgens;Rudolf Vaas	2012	2012 20th IEEE International Conference on Program Comprehension (ICPC)	10.1109/ICPC.2012.6240490	non-regression testing;reliability engineering;long-term support;verification and validation;regression testing;real-time computing;software performance testing;system integration testing;software verification;computer science;systems engineering;engineering;acceptance testing;package development process;software reliability testing;software development;software engineering;software construction;software testing;software maintenance;software quality analyst	SE	-61.890360153590976	33.47245146097579	170421
656224cba03837e4657a9dbacd57db4484ac9a4d	a qualitative comparison of three aspect mining techniques	institutional repositories;program diagnostics;fedora;concept analysis crosscutting concerns aspect oriented programming aspect mining fan in analysis;program comprehension;object oriented software;object oriented programming;data mining;vital;reverse engineering object oriented programming data mining program diagnostics;level of detail;concept analysis;aspect oriented programming;aspect oriented programming aspect mining techniques object oriented software program comprehension fan in analysis identifier analysis dynamic analysis jhotdraw crosscutting concerns;aspect oriented program ming;aspect mining;crosscutting concerns;aspect oriented;vtls;scattering software systems impedance conferences;fan in analysis;ils;dynamic analysis;reverse engineering	The fact that crosscutting concerns (aspects) cannot be well modularized in object oriented software is an impediment to program comprehension: the implementation of a concern is typically scattered over many locations and tangled with the implementation of other concerns, resulting in a system that is hard to explore and understand. Aspect mining aims to identify crosscutting concerns in a system, thereby improving the system's comprehensibility and enabling migration of existing (object-oriented) programs to aspect-oriented ones. In this paper, we compare three aspect mining techniques that were developed independently by different research teams: fan-in analysis, identifier analysis and dynamic analysis. We apply each technique to the same case (JHotDraw) and mutually compare the individual results of each technique based on the discovered aspects and on the level of detail and quality of those aspects. Strengths, weaknesses and underlying assumptions of each technique are discussed, as well as their complementarity. We conclude with a discussion of possible ways to combine the techniques in order to achieve a better overall aspect-mining technique.	aspect-oriented software development;code refactoring;complementarity theory;cross-cutting concern;fan-in;identifier;level of detail;lexicon;program comprehension;tracing (software);unification (computer science)	Mariano Ceccato;Marius Marin;Kim Mens;Leon Moonen;Paolo Tonella;Tom Tourwé	2005	13th International Workshop on Program Comprehension (IWPC'05)	10.1109/WPC.2005.2	aspect-oriented programming;computer science;systems engineering;engineering;data mining;programming language	SE	-57.22066362220324	34.137336441089495	170602
f4409ba15ac963ba4977ca54f8520e54ae5eb60b	"""""""cyber-film"""": a visual approach that facilitates program comprehension"""	cyber film;program comprehension;learning tool;programming tool;software visualization	"""This paper presents a visual approach that will make program comprehension easy. This visual approach employs """"Cyber-Films"""" where a user can use them not only as a programming tool but also as a learning tool. This paper briefly describes how to use """"Cyber-Films"""" as a programming tool, how to use them as a learning tool, and how to guarantee the absence (or at least minimize the presence) of syntax errors in programming. In order to verify that """"Cyber-Films"""" are also useful as a learning tool, a prototype implementation was developed and an experiment was conducted. This paper presents and discusses the results of that experiment. Its goal was to evaluate whether or not the visual approach used was easy to understand even for users with no background at all. The results of that experiment showed that the majority of the respondents correctly identified each scheme of communication presented in """"Cyber-Film"""" format by just observing the set of frames in animation either once or twice and at most thrice before understanding a given scheme. Because the respondents easily identified how a given scheme worked by observing it, this approach can also facilitate the understanding of programs written by other people."""	list comprehension;program comprehension	Robert R. Roxas;Nikolay N. Mirenkov	2005	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194005002609	software visualization;simulation;computer science;artificial intelligence;software engineering;multimedia;programming language	SE	-53.95586067462026	37.15737082073812	170742
63b24e16a9456e403a0db46c4ba8bfb1b6bb5330	uml/analyzer: a tool for the instant consistency checking of uml models	uml modeling tool;design model;unified modeling language software engineering software tools;uml modeling tool instant consistency checking uml models analyzer tool;software engineering;unified modeling language displays motion pictures streaming media feedback software engineering usability data visualization decoding best practices;software testing education;web based tutorial;unified modeling language;uml models;consistency checking;software tools;instant consistency checking;analyzer tool;modeling tool	Large design models contain thousands of model elements. Designers easily get overwhelmed maintaining the consistency of such design models over time. Not only is it hard to detect new inconsistencies while the model changes but it also hard to keep track of known inconsistencies. The UML/Analyzer tool identifies inconsistencies instantly with design changes and it keeps track of all inconsistencies over time. It does not require consistency rules with special annotations. Instead, it treats consistency rules as black-box entities and observes their behavior during their evaluation. The UML/Analyzer tool is integrated with the UML modeling tool IBM Rational Rose^TM for broad applicability and usability. It is highly scalable and was evaluated on dozens of design models.	black box;entity;processor consistency;scalability;uml tool;unified modeling language;usability	Alexander Egyed	2007	29th International Conference on Software Engineering (ICSE'07)	10.1109/ICSE.2007.91	unified modeling language;model-driven architecture;real-time computing;uml tool;computer science;software engineering;applications of uml;class diagram;database;programming language;node;object constraint language	SE	-54.698421077195476	33.628443443508516	171722
a1e30a786c4cbfd9f3793ecf30998a81be4a7e7b	orlis: obfuscation-resilient library detection for android		Android apps often contain third-party libraries. For many program analyses, it is important to identify the library code in a given closed-source Android app. There are several clients of such library detection, including security analysis, clone/repackage detection, and library removal/isolation. However, library detection is complicated significantly by commonly-used code obfuscation techniques for Android. Although some of the state-of-the-art library detection tools are intended to be resilient to obfuscation, there is still room to improve recall, precision, and analysis cost.  We propose a new approach to detect third-party libraries in obfuscated apps. The approach relies on obfuscation-resilient code features derived from the interprocedural structure and behavior of the app (e.g., call graphs of methods). The design of our approach is informed by close examination of the code features preserved by typical Android obfuscators. To reduce analysis cost, we use similarity digests as an efficient mechanism for identifying a small number of likely matches. We implemented this approach in the ORLIS library detection tool. As demonstrated by our experimental results, ORLIS advances the state of the art and presents an attractive choice for detection of third-party libraries in Android apps.	android;class hierarchy;library (computing);obfuscation (software);semantic similarity;video game clone	Yan Wang;Haowei Wu;Hailong Zhang;Atanas Rountev	2018	2018 IEEE/ACM 5th International Conference on Mobile Software Engineering and Systems (MOBILESoft)	10.1145/3197231.3197248	database;obfuscation (software);android (operating system);security analysis;obfuscation;static analysis;analysis cost;computer science;graph	SE	-59.79850110797718	39.97279194787719	171797
2ec6747b2d4eec60726b09c8a9b75441311e74b9	common program similarity metric method for anti-obfuscation		Program similarity metrics, especially malware similarity metrics, have long been an area of active research. However, code obfuscation techniques bring many challenges to similarity analysis. Most prior techniques lack extensibility since they focus on problems of only one particular type, such as malicious programs on a specific platform. Moreover, some of these techniques cannot measure the similarity between the various components of two programs, and some cannot accommodate code obfuscation techniques, particularly for control flow obfuscation. To address these limitations, we present the new concept of the reductive instruction dependence graph (RIDG), which is platform-independent and stable throughout most code obfuscation processes. In addition, we propose a four-level similarity schema that is based on RIDG for measuring the similarity between two programs, which can be used to find the similarity between the components of programs. We use the proposed program similarity metric method to measure similarities among 100 different programs, and we evaluate the anti-obfuscation ability of this method by using four popular code obfuscation techniques. The experimental results show that the proposed method can overcome the limitations described above.	control flow;extensibility;malware;obfuscation (software)	Xiaochuan Zhang;Jianmin Pang;Xiaonan Liu	2018	IEEE Access	10.1109/ACCESS.2018.2867531	humanoid robot;obfuscation (software);theoretical computer science;control flow;malware;semantics;extensibility;computer science;obfuscation;distributed computing;schema (psychology)	SE	-59.72961287918129	40.47634073367921	171926
8f7d4e1ffd32958fc0621c74f11cec982ab4eb30	ixj: interactive source-to-source transformations for java	interactive environments;program transformation;program transformation languages;large scale;development environment;interactive environment;source code	Manual large-scale modification or generation of source code can be tedious and error-prone. Integrating scriptable source-to-source program transformations into development environments will assist developers with this overwhelming task. We discuss various usability issues of bringing such a<i>d-hoc</i> transformations to end-users and describe a developer-oriented interactive source code transformation tool for Java that we are building.	cognitive dimensions of notations;java;program transformation;usability	Marat Boshernitsan;Susan L. Graham	2004		10.1145/1028664.1028755	computational science;computer science;theoretical computer science;development environment;programming language;source code	PL	-53.094291581423704	35.687372063495594	172080
85152d1b85b0328c3e7f3fb35f1a9e46ad5019c1	compiler-directed program-fault coverage for highly available java internet services	software testing;web and internet services;software engineering;proof of concept;program processors java web and internet services software testing operating systems fault diagnosis system testing hardware software engineering computer science;internet services;system testing;fault coverage;computer science;fault injection;program processors;proxy server;fault diagnosis;operating systems;hardware;java	We presenta new approach that usescompilerdirected fault-injection for coverage testing of recovery code in Internetservicesto evaluatetheir robustnessto operating systemand I/O hardware faults. We definea set of program-fault coverage metricsthat enablequantification of Javacatch blocks exercisedduring fault-injection experiments.Weusecompileranalysesto instrumentapplicationcodein two ways: to direct fault injection to occur at appropriatepointsduring execution, andto measur e the resultingcoverage. As a proof of concept for theseideas, wehaveappliedour techniquesmanually to Muffin,a proxy server; we obtained a high degreeof coverage of catch blocks, with, on average, 85% of the expectedfaults per catch beingexperiencedascaught exceptions.	code coverage;compiler;experiment;fault coverage;fault injection;input/output;java;proxy server;server (computing)	Chen Fu;Richard P. Martin;Kiran Nagaraja;Thu D. Nguyen;Barbara G. Ryder;David G. Wonnacott	2003		10.1109/DSN.2003.1209969	reliability engineering;embedded system;real-time computing;fault coverage;computer science;operating system;distributed computing;software testing;code coverage;programming language;java;system testing;proof of concept;computer security	OS	-58.20360102895506	40.8166008087223	172193
9a6fa46d292a25b4545555f088c495bb4232ffcc	an evaluation of slicing algorithms for concurrent programs	syntactic violation identification;program comprehension;software quality reverse engineering;natural languages software quality dictionaries information retrieval software engineering libraries documentation educational institutions usa councils programming profession;abbreviated program identifier meaning extraction;lines of code;software quality abbreviated program identifier meaning extraction program understandability program comprehension syntactic violation identification;natural language;program comprehension software quality program identifiers;program understandability;software quality;reverse engineering;program identifiers	Program slicing is a program-reduction technique for extracting statements that may influence other statements. While there exist efficient algorithms to slice sequential programs precisely, there are only two algorithms for precise slicing of concurrent interprocedural programs with recursive procedures. We implemented both algorithms for Java, applied several new optimizations and examined their precision and runtime behavior. We compared these results with two further algorithms which trade precision for speed. We show that one algorithm may produce incorrect slices and that precise slicing of concurrent programs is very expensive in terms of computation time.	approximation algorithm;compile time;computation;concurrency (computer science);concurrent computing;context-free grammar;control flow graph;existential quantification;i2p — the anonymous network;information flow (information theory);interference (communication);java compiler;java memory model;non-interference (security);overhead (computing);precision and recall;program slicing;reachability;recursion;sensor;test suite;time complexity;tree traversal;usability	Dennis Giffhorn;Christian Hammer	2007	Seventh IEEE International Working Conference on Source Code Analysis and Manipulation (SCAM 2007)	10.1109/SCAM.2007.9	computer science;software engineering;data mining;database;natural language;programming language;source lines of code;software quality;reverse engineering	SE	-59.33221017633582	38.14876870057079	172715
065bdbc6f4d716a92e197d850014a998c56c35d3	assessing aspect-oriented programming: preliminary results	aspect oriented programming;aspect oriented	The aspect-oriented programming approach claims to make it easier to reason about, develop, and maintain certain kinds of application code while maintaining highly e cient code. To better understand the usefulness and usability of the aspect-oriented approach, we have been conducting a series of experiments. These experiments are designed to investigate such characteristics of aspect-oriented development as the creation and ease of debugging programs built in this style. This paper provides an overview of the experiments we have conducted to date.	aspect-oriented programming;debugging;experiment;usability	Robert J. Walker;Elisa L. A. Baniassad;Gail C. Murphy	1998		10.1007/3-540-49255-0_131	aspect-oriented programming;computer science;programming language	SE	-53.019916941092454	37.04125764928153	172813
a94dc8c33454cc01adc1af3d546444ac1b2eead1	towards test case reuse: a study of redundancies in android platform test libraries	towards test case reuse a study of redundancies in android platform test libraries;articulo	Similar software systems have similar test cases. We find much redundancy even within test cases of a single system. In this paper, we describe the results of similarity analysis performed on Android platform framework project’s test case libraries. The results confirm our hypothesis that reuse of test cases can boost productivity at least as much as reuse of code. We identified repetition patterns in Android platform framework test case libraries that can be represented in generic form for further reuse using variability techniques adopted from Software Product Line (SPL). By exploiting similarity among test cases, we can design generic test case libraries that are much smaller, easier to develop/evolve than existing test case libraries. In this paper, we present quantitative and qualitative findings from our study of Android platform framework test case libraries. We discuss typical patterns of repetitions and illustrate an example of how they can be treated with variability technique ‘XML-based Variant Configuration Language (XVCL)’.	test case	Suriya Priya R. Asaithambi;Stan Jarzabek	2013		10.1007/978-3-642-38977-1_4	embedded system;real-time computing;engineering;operating system	SE	-56.18774242933402	32.6744521710978	172902
cd818a469c9f3d255a7e82b4bfaea5dff341d99c	fault prediction capability of program file's logical-coupling metrics	software metrics;software;measurement couplings correlation computer bugs software predictive models principal component analysis;measurement;software maintenance;logical coupling metrics;fault prediction models logical coupling metrics;principal component analysis;software metric;software metrics fault diagnosis program debugging software maintenance;predictive models;prediction model;program debugging;correlation;couplings;bug prediction model fault prediction capability program file logical coupling metrics logically coupled source files;computer bugs;fault prediction models;historical data;fault diagnosis	Frequent changes in logically coupled source files induced bugs in software. Metrics have been used to identify source files which are logically coupled. In this paper, we propose an approach to compute a set of eight metrics, which measure logical-couplings among source files. We compute these metrics using the historical data of software changes which are related to the fixing of post release bugs. To validate that our propose set of metrics is highly correlated with the number bugs and are more capable to construct a bug prediction model, we performed an experiment. Our experimental results show that our propose set of metrics is highly correlated with the number of bugs, and hence can be used to construct a bug prediction model. In our experiment, the obtained accuracy of our bug predictor model is 97%.	kerrison predictor;protein structure prediction;software bug	Syed Nadeem Ahsan;Franz Wotawa	2011	2011 Joint Conference of the 21st International Workshop on Software Measurement and the 6th International Conference on Software Process and Product Measurement	10.1109/IWSM-MENSURA.2011.38	reliability engineering;real-time computing;computer science;database	SE	-62.76518605876308	35.66867082764027	173066
0cfa3741cee1af6fb7426ae59deca1314289215d	a manifesto for higher order mutation testing	mutation testing;software testing;optimisation;optimization technique;uncertainty;helium;research agenda;search space;search based software engineering;testing;software engineering;higher order;genetic mutations software testing software engineering educational institutions computer bugs fault detection helium;structural testing;first order;software engineering optimisation program testing search problems;program testing;syntactics;fault detection;optimization;search problems;search based optimization technique higher order mutation testing real faults search based software engineering;search based optimization technique;genetic mutations;real faults;couplings;computer bugs;higher order mutation testing;channel hot electron injection	We argue that higher order mutants are potentially better able to simulate real faults and to reveal insights into bugs than the restricted class of first order mutants. The Mutation Testing community has previously shied away from Higher Order Mutation Testing believing it to be too expensive and therefore impractical. However, this paper argues that Search Based Software Engineering can provide a solution to this apparent problem, citing results from recent work on search based optimization techniques for constructing higher order mutants. We also present a research agenda for the development of Higher Order Mutation Testing.	clipping (computer graphics);expectation propagation;fitness function;genetic programming;martin woodward;mathematical optimization;mutation testing;norm (social);search-based software engineering;simulation;software bug;x image extension;eric	Mark Harman;Yue Jia;William B. Langdon	2010	2010 Third International Conference on Software Testing, Verification, and Validation Workshops	10.1109/ICSTW.2010.13	simulation;computer science;systems engineering;engineering;software engineering;software testing;algorithm	SE	-58.65798041468656	34.99993916418478	174012
db34169bb612f0e606cd6008675d6500ad0d56cb	multi-perspective change impact analysis using linked data of software engineering	software engineering linked data;multi perspective change impact analysis;propagation assessment	Change impact analysis plays an important role in software maintenance and evolution. However existing researches mostly focus on one single artifact. Software development is usually accompanied by various types of software artifacts, such as requirement documents, software architectures, test cases, source code, etc., requiring a much more comprehensive change impact analysis. This paper presents a novel approach to multi-perspective change impact analysis that is able to address heterogeneous software artifacts. The essential idea of the novel approach is (1) to adopt semantic web to construct automatically ontology based software engineering linked data, which links requirements, classes, code, bug reports, commits, developers, test cases and others, (2) to build a weighted change impact matrix/graph using the dependency features extracted from linked data, and (3) to follow a change impact propagation algorithm to analyze the overall change impacts. We have conducted experiments on two open source projects (HtmlUnit and OpenRocket) to evaluate our approach. The experimental results show that our approach achieves better F-measure and stability than existing multi-perspective change impact analysis approaches.	algorithm;experiment;f1 score;htmlunit;linked data;open-source software;requirement;semantic web;software architecture;software development;software engineering;software maintenance;software propagation;test case	Chengcheng Wan;Zece Zhu;Yuchen Zhang;Yuting Chen	2016		10.1145/2993717.2993729	reliability engineering;change management;verification and validation;software engineering process group;software sizing;computer science;systems engineering;software design;software framework;component-based software engineering;software development;software design description;software construction;data mining;software analytics;goal-driven software development process;software metric	SE	-57.652100388997184	33.30035550063686	174033
a577d4996bec8573a6b1851931849d328c9e9622	building a research infrastructure for program comprehension observations	results aggregation research infrastructure program comprehension observations program comprehension studies systematic analysis method think aloud protocols;program comprehension;protocols encoding computer science gratings programming profession software maintenance software tools guidelines documentation size control;reverse engineering	Detailed program comprehension studies are expensive. Today, most of those studies are dificult to aggregate, as analysis methods for the data differ widely between researchers. As part of an egort to build a research infrastructure, a uniform, systematic analpsis method for analyzing think-aloud protocols is proposed. This method is shown to be compatible with other analysis methods and extensible. I t provides the possibility of aggregating results from experiments and leveraging results through such aggregation.	aggregate data;experiment;list comprehension;program comprehension;think aloud protocol	Steve Lang;Anneliese Amschler Andrews	1997		10.1109/WPC.1997.601287	computer science;systems engineering;operating system;software engineering;programming language;reverse engineering	SE	-55.22205601431595	34.06304989170125	174330
34d66f838da664cbbc390200dd1afb61c5fc4b5f	wad: a feasibility study using the wicked audio debugger	dynamic programming;debugging;wicked audio debugger;dynamic program behavior wicked audio debugger microsoft visual studio 2005 computer code;speech;testing;dynamic program;computer code;psychology;microsoft visual studio 2005;computer architecture;feasibility study;testing speech programming profession dynamic programming debugging computer architecture java joining processes psychology unified modeling language;codes;programming profession;unified modeling language;control flow;joining processes;program debugging;program debugging codes;dynamic program behavior;java	Contemporary programmers have a predominately visual experience. Computer code is read from sophisticated text editors, analyzed using visual tools, designed using UML, and debugged using a watch window. Little research has attempted to create tools for the non-sighted programmer, using either haptic or aural feedback. In this paper, we present WAD, the wicked audio debugger, a new debugger for Microsoft Visual Studio 2005 that sonifies computer code as an aid to the programmer. This paper has two primary contributions, namely the tool itself and the results of a feasibility study. We conducted this feasibility study to test participants' ability to comprehend dynamic program behavior, including tracking the value of state variables and control flow. Results of our feasibility study show that participants were able to comprehend approximately 86% of dynamic program behavior using audio only.	control flow;debugger;debugging;haptic technology;microsoft visual studio;programmer;text editor;unified modeling language;wicked	Andreas Stefik;Roger T. Alexander;Robert D Patterson;Jonathan Brown	2007	15th IEEE International Conference on Program Comprehension (ICPC '07)	10.1109/ICPC.2007.42	unified modeling language;feasibility study;computer architecture;real-time computing;computer science;speech;operating system;software engineering;dynamic programming;software testing;programming language;control flow;debugging;java;code;source code	HCI	-54.48946485843588	36.105337471726536	174462
abc4f1c18a56ea27680507bcbdfd328167bb0df0	exploring large profiles with calling context ring charts	calling context tree cct;object oriented software;dynamic metrics;visualization;control flow;performance analysis;calling context profiles;data structure	Calling context profiling is an important technique for analyzing the performance of object-oriented software with complex inter-procedural control flow. A common data structure is the Calling Context Tree (CCT), which stores dynamic metrics, such as CPU time, separately for each calling context. As CCTs may comprise millions of nodes, there is need for a condensed visualization that eases the location of performance bottlenecks. In this paper, we discuss Calling Context Ring Charts (CCRCs), a compact visualization for CCTs, where callee methods are represented in ring segments surrounding the caller's ring segment. In order to reveal hot methods, their callers, and callees, the ring segments can be sized according to a chosen dynamic metric. We describe a case study where CCRCs help detect and fix performance problems in an application. An evaluation confirms that our implementation efficiently handles large CCTs with millions of nodes.	central processing unit;chart;computational complexity theory;control flow;data structure;global variable	Philippe Moret;Walter Binder;Alex Villazón;Danilo Ansaloni	2010		10.1145/1712605.1712617	real-time computing;visualization;data structure;computer science;operating system;data mining;database;programming language;control flow;world wide web	PL	-55.85074941415203	37.14314507844971	174646
9ddcb7db1ff5836e1c3b3cc4978c9d3da013d372	concepts, operations, and feasibility of a projection-based variation control system	software;control systems;history;publikationer;printers;prototypes;konferensbidrag;syntactics;calculus;artiklar;rapporter	Highly configurable software often uses preprocessor annotations to handle variability. However, understanding, maintaining, and evolving code with such annotations is difficult, mainly because a developer has to work with all variants at a time. Dedicated methods and tools that allow working on a subset of all variants could ease the engineering of highly configurable software. We investigate the potential of one kind of such tools: projection-based variation control systems. For such systems we aim to understand: (i) what end-user operations they need to support, and (ii) whether they can realize the actual evolution of real-world, highly configurable software. We conduct an experiment that investigates variability-related evolution patterns and that evaluates the feasibility of a projection-based variation control system by replaying parts of the history of a highly configurable real-world 3D printer firmware project. Among others, we show that the prototype variation control system does indeed support the evolution of a highly configurable system and that in general, it does not degrade the code.	3d printing;control system;firmware;heart rate variability;preprocessor;printer (computing);prototype;software system;spatial variability;user interface	Stefan Stanciulescu;Thorsten Berger;Eric Walkingshaw;Andrzej Wasowski	2016	2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2016.88	real-time computing;simulation;computer science;systems engineering;engineering;control system;software engineering;prototype;programming language	SE	-53.05181410821092	33.536023922308324	174671
855f2a986ae4be4f4699584e91a4cada819bcf16	detecting data races in interrupt-driven programs based on static analysis and dynamic simulation	data race;simulation;testing;会议论文;interrupt driven;analysis	Interrupt-driven programs are often embedded in safety-critical systems to perform hardware/resource dependent data operation tasks, such as data acquisition, processing, and transformation. The interrupt programs and tasks may happen in parallel which in a result causes indeterminist concurrent problems at runtime. Data race is one of the most popular problems challenging researchers and practitioners. Various static analysis, software testing approaches have been proposed to detect data races in source code, testing, and even production run. However, static analysis may report too many false positives due to the lack of execution information. Dynamic testing may miss some important races since it could not generate adequate test cases to test all possible execution scenarios. In this paper, we propose a hybrid approach to detect data races in interrupt-driven programs based on static analysis and dynamic simulation. We implemented a prototype tool and conducted a controlled experiment to demonstrate the applicability of our approach.	data acquisition;dynamic simulation;dynamic testing;embedded system;interrupt;prototype;race condition;run time (program lifecycle phase);sensor;software testing;static program analysis;test case	Yu Wang;Junjing Shi;Linzhang Wang;Jianhua Zhao;Xuandong Li	2015		10.1145/2875913.2875943	parallel computing;real-time computing;computer science;operating system	SE	-60.93166249412364	37.54493875456318	174856
78be5b364794a935777da920203cdd54795373eb	on the right objectives of data flow testing	settore inf 01 informatica;program testing data flow analysis;articulo;false positives data flow testing static data flow technique dynamic data flow technique false negatives;settore ing inf 05 sistemi di elaborazione delle informazioni;testing data structures performance analysis runtime monitoring context computational modeling;on the right objectives of data flow testing	This paper investigates the limits of current data flow testing approaches from a radically novel viewpoint, and shows that the static data flow techniques used so far in data flow testing to identify the test objectives fail to represent the universe of data flow relations entailed by a program. This paper compares the data flow relations computed with static data flow approaches with the ones observed while executing the program. To this end, the paper introduces a dynamic data flow technique that collects the data flow relations observed during testing. The experimental data discussed in the paper suggest that data flow testing based on static techniques misses many data flow test objectives, and indicate that the amount of missing objectives (false negatives) can be more limiting than the amount of infeasible data flow relations identified statically (false positives). This opens a new area of research of (dynamic) data flow testing techniques that can better encompass the test objectives of data flow testing.	approximation algorithm;data-flow analysis;dataflow architecture;dynamic data;dynamic logic (digital electronics);reaching definition	Giovanni Denaro;Mauro Pezzè;Mattia Vivanti	2014	2014 IEEE Seventh International Conference on Software Testing, Verification and Validation	10.1109/ICST.2014.18	real-time computing;computer science;data mining;algorithm;statistics	SE	-59.71422489185046	33.841177614307234	175046
804422223263f25eae083053a16da39827e1e936	an automated object-oriented testing for c++ inheritance hierarchy	interlevel first;software testing;programming environments;c programs;c inheritance hierarchy;programming environment;automatic testing;intralevel first;formal specifications;object oriented software;object oriented programming;z notation;uri;unit repeated inheritance;automatic generation;program testing inheritance c language object oriented programming;acoustic testing;c language;program testing;programming profession;c programs object oriented testing c inheritance hierarchy inheritance unit repeated inheritance uri intralevel first interlevel first windowing tool programming environment;error correction;information management;business;system testing;source code;automatic testing software testing system testing information management business programming environments programming profession error correction acoustic testing formal specifications;inheritance;object oriented testing;windowing tool	This paper proposes a concept named unit repeated inheritance (URI) in Z notation to realize objectoriented testing of an inheritance hierarchy. Based on this unit, an inheritance level technique (ILT) method as a guide to test object-oriented software errors in the inheritance hierarchy is described. In addition, two testing criteria, intralevel rst and interlevel rst, are formed based on the proposed mechanism. Moreover, in order to make the test process automatic, we use LEX and YACC to automatically generate a lexical analyzer and a parser to demonstrate a declaration of C++ source code. And, we also construct a windowing tool used in conjunction with a conventional C++ programming environment to assist a programmer to analyze and test his/her C++ programs.	c++;declaration (computer programming);integrated development environment;lex (software);lexical analysis;multiple inheritance;programmer;uniform resource identifier;yacc;z notation	Chun-Chia Wang;Wen C. Pai;Timothy K. Shih	1997		10.1109/ASE.1997.632862	uniform resource identifier;real-time computing;c++;error detection and correction;computer science;z notation;software testing;information management;policy-based design;programming language;object-oriented programming;system testing;algorithm;source code	PL	-51.74705605761296	35.95538668933341	175542
51e46a646f9e0549658e2937b243c31aa2149304	actively comparing clones inside the code editor	tool support;software maintenance;code comparison;integrated development environment;eclipse integrated development environment;software evolution;code clone;source code;copy and paste programming;differencing tools;software quality;java	Tool support for code clones can improve software quality and maintainability. While significant research has been done in locating clones in existing source code, there has been less of a research focus on proactively tracking and supporting copy-paste-modify operations, even though copying and pasting is a major source of clone formation and the resulting clones are then often modified. We designed and implemented a programming editor, based on the Eclipse integrated development environment, named CSeR (Code Segment Reuse), which keeps a record of copy-and-paste-induced clones and then tracks and visualizes the changes made to a clone with distinct colors. The core of CSeR is an algorithm that actively compares two clones for detailed differences as a programmer edits either one of them. This edit-based comparison algorithm is unique to CSeR and produces more immediate, accurate, and natural results than other differencing tools.	algorithm;autoregressive integrated moving average;code segment;color;cut, copy, and paste;eclipse;integrated development environment;programmer;software quality	Ferosh Jacob;Daqing Hou;Patricia Jablonski	2010		10.1145/1808901.1808903	kpi-driven code analysis;real-time computing;computer science;operating system;programming language	SE	-55.55180751387043	36.235164725128904	175588
8083fca5d58c13975db7668e2f8a9f4a80d71216	automated multi-language artifact binding and rename refactoring between java and dsls used by java frameworks	refactoring;polyglot programming;program comprehension;experiment;multi language software;domain specific languages;java	C o n si st en t * lete * W ll D o c u m e n t e d * E a s y t o R e u s e * * E v a l u a t e d * E C O O P * Ar tifact * A E C Developing non-trivial software applications involves using multiple programming languages. Although each language is used to describe a particular aspect of the system, artifacts defined inside those languages reference each other across language boundaries; such references are often only resolved at runtime. However, it is important for developers to be aware of these references during development time for programming understanding, bug prevention, and refactoring. In this work, we report on a) an approach and tool for automatically identifying multi-language relevant artifacts, finding references between artifacts in different languages, and (rename-) refactoring them, and b) on an experimental evaluation of the approach on seven open-source case studies which use a total of six languages found in three frameworks. As our main result, we provide insights into the incidence of multi-language bindings in the case studies as well as the feasibility of automated multi-language rename refactorings.	algorithm;apache wicket;application programming interface;artifact (software development);batch processing;code refactoring;deductive lambda calculus;eclipse;emoticon;entity;experiment;gigabyte;glue code;html;hibernate orm;high bandwidth memory;incidence matrix;integrated development environment;java persistence query language;language binding;md5;metamodeling;needham–schroeder protocol;open-source software;parsing;plug-in (computing);programming language;regression testing;rename (relational algebra);repeatability;run time (program lifecycle phase);software system;spring framework;unit testing;user interface;virtual machine;virtualbox;workspace	Philip Mayer;Andreas Schroeder	2014		10.1007/978-3-662-44202-9_18	experiment;fourth-generation programming language;real-time computing;computer science;domain-specific language;third-generation programming language;fifth-generation programming language;programming language;java;second-generation programming language;code refactoring	SE	-54.96132972904013	38.53875022110452	175650
d9a4ff4ed5c85fcbe5c97bf3d82523601e2e4704	automated methods for proving program termination and liveness	electronic mail;programming language;imperative programming language;construction industry;proving program termination;synthetic aperture sonar;ranking function;imperative programming language proving program termination software reliability;computational modeling;tutorials;computational modeling software algorithms scientific computing software reliability programming profession computer languages safety machinery synthetic aperture sonar;program termination;ranking functions;software algorithms;transition predicate program termination ranking functions;software reliability;transition predicate;software reliability programming languages;programming languages	Proving program termination is an important step towards ensuring software reliability. The programmers expect that the majority of code fragments, including procedures, event handles, or other program components, always terminates. Unfortunately, until recently there were no viable approaches for automatically proving termination of programs written in imperative programming languages.	imperative programming;liveness;programmer;programming language;software reliability testing;termination analysis	Andrey Rybalchenko	2009	2009 11th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2009.70	imperative programming;synthetic aperture sonar;computer science;theoretical computer science;database;programming language;computational model;algorithm;software quality	Arch	-50.17469187896743	33.33480559429589	175787
063c698f518758abd140f8424cd7f9497ce7feba	developers ask reachability questions	navigation cognition software encoding concrete educational institutions libraries;empirical study;software maintenance;software maintenance code navigation developer questions empirical study program comprehension;program comprehension;software engineering;software engineering program debugging reachability analysis;code navigation;software development;developer questions;program debugging;reachability analysis;program debugging reachability questions bug developers software developers	A reachability question is a search across feasible paths through a program for target statements matching search criteria. In three separate studies, we found that reachability questions are common and often time consuming to answer. In the first study, we observed 13 developers in the lab and found that half of the bugs developers inserted were associated with reachability questions. In the second study, 460 professional software developers reported asking questions that may be answered using reachability questions more than 9 times a day, and 82% rated one or more as at least somewhat hard to answer. In the third study, we observed 17 developers in the field and found that 9 of the 10 longest activities were associated with reachability questions. These findings suggest that answering reachability questions is an important source of difficulty understanding large, complex codebases.	question answering;reachability;software bug;software developer;web search engine	Thomas D. LaToza;Brad A. Myers	2010	2010 ACM/IEEE 32nd International Conference on Software Engineering	10.1145/1806799.1806829	computer science;systems engineering;engineering;software development;software engineering;database;programming language;empirical research;software maintenance	SE	-62.48886595057883	34.36254272719545	175992
b18504d025e15ebe38368ef0a6f857ed68b1618c	from validation to automated repair & beyond with constraint solving	debugging;computer engineering;development;testing;los angeles todd millstein samimi;hesam;program repair;constraint solving;computer science;computer science from validation to automated repair beyond with constraint solving university of california;software reliability	of the Dissertation From Validation to Automated Repair & Beyond with Constraint Solving by Hesam Samimi Doctor of Philosophy in Computer Science University of California, Los Angeles, 2013 Professor Todd Millstein, Chair Tremendous amounts of software engineering efforts go into the validation of software. Developers rely on many forms of software validation, from unit tests to assertions and formal specifications, dynamic contract checking to static formal verification, to ensure the reliability of software packages. Traditionally, however, the benefits seem to stop there, at checking whether there are problems. But once problems have been detected, those spent validation efforts play no role in the challenging task of debugging those problems, a task which requires manual, time-consuming, and error-prone developer efforts. The key insight of this dissertation is that we can leverage the efforts that developers currently put into the validation of software, such as unit tests and formal verification, to get software engineering benefits that can go beyond validation, including automated software repair. Validation mechanisms can be elevated to this status using modern constraint solving, a technology that is already in use for the purpose of formal verification of software. I present three novel and practical instances of this idea, that I was able to identify by focusing on particular domains and scenarios. The first work, used in development, builds on unit testing as the most common form of validation, and exploits a constraint solving method to automatically fix a certain class of bugs in the source code (offline repair). The second builds on dynamic, specification-based validation as in assertions and contracts used during development and testing, and applies it to deployed software to make it robust to unforeseen run-time failures by falling back to constraint solving (online repair). Finally, I use specifications and constraint solving to improve an existing validation methodology in test-driven development, used to enable testing when part of the depended upon software is unavailable or hard to set up.	cognitive dimensions of notations;computer science;constraint satisfaction problem;debugging;domain theory;formal verification;online and offline;software bug;software engineering;software verification and validation;specification language;test-driven development;unit testing	Hesam Samimi	2013			reliability engineering;verification and validation;regression testing;simulation;software verification;computer science;systems engineering;package development process;software reliability testing;software development;software construction;software testing;validation rule;software quality control;software metric	SE	-59.116495000672074	36.63809816923427	176250
2114241cc1c4f71628a33d1c9fbccffbb57f6a43	(un-)covering equivalent mutants	detectors;manuals;mutation testing;software testing;measurement;software measurement;time measurement;java programming;genetic mutations software testing java detectors software quality software measurement costs time measurement;articulo;code coverage;semantics;testing;program semantics;runtime;un covering equivalent mutants;open source javalanche framework;public domain software;lines of code;program testing;open source javalanche framework equivalent mutants mutation testing measures artificial defects program semantics java programs;dynamic analysis mutation testing code coverage;java programs;public domain software java program testing;mutation testing measures;genetic mutations;artificial defects;software quality;equivalent mutants;dynamic analysis;java;open source	Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a test suite fails to detect a mutation, it may also fail to detect real defects-and hence should be improved. However, there also are mutations which keep the program semantics unchanged and thus cannot be detected by any test suite. Such equivalent mutants must be weeded out manually, which is a tedious task. In this paper, we examine whether changes in coverage can be used to detect non-equivalent mutants: If a mutant changes the coverage of a run, it is more likely to be non-equivalent. Ina sample of 140 manually classified mutations of seven Java programs with 5,000to 100,000 lines of code, we found that: (a) the problem is serious and widespread-about 45% of all undetected mutants turned out to be equivalent;(b) manual classification takes time-about 15 minutes per mutation; (c)coverage is a simple, efficient, and effective means to identify equivalent mutants-with a classification precision of 75% and a recall of 56%; and (d)coverage as an equivalence detector is superior to the state of the art, in particular violations of dynamic invariants. Our detectors have been released as part of the open source Javalanche framework; the data set is publicly available for replication and extension of experiments.	experiment;java;mutation testing;open-source software;semantics (computer science);sensor;source lines of code;test suite;turing completeness	David Schuler;Andreas Zeller	2010	2010 Third International Conference on Software Testing, Verification and Validation	10.1109/ICST.2010.30	real-time computing;computer science;engineering;software engineering;semantics;software testing;programming language;algorithm	SE	-60.10386665627533	38.772766341073755	176532
a279c07ec4a151a89301a5ec6018b1c708677818	an accurate estimation of the levenshtein distance using metric trees and manhattan distance	estimation theory;program compilers;program testing;public domain software;trees (mathematics);levenshtein distance;manhattan distance;accurate approximation;accurate estimation;frequency vectors;large scale open source software;metric trees;original clone detection technique;precision clone detection technique;precision measurement;recall measurement;research opportunity;source code;testbench;windowed-tokens;clone detection;levenshtein distance;manhattan distance;software clones	This paper presents an original clone detection technique which is an accurate approximation of the Levenshtein distance. It uses groups of tokens extracted from source code called windowed-tokens. From these, frequency vectors are then constructed and compared with the Manhattan distance in a metric tree. The goal of this new technique is to provide a very high precision clone detection technique while keeping a high recall. Precision and recall measurement is done with respect to the Levenshtein distance. The testbench is a large scale open source software. The collected results proved the technique to be fast, simple, and accurate. Finally, this article presents further research opportunities.	approximation;design rationale;distortion;duplicate code;emulator;experiment;levenshtein distance;metric tree;open-source software;precision and recall;taxicab geometry;test bench;window function	Thierry Lavoie;Ettore Merlo	2012	2012 6th International Workshop on Software Clones (IWSC)		damerau–levenshtein distance;edit distance;computer science;theoretical computer science;data mining;bitap algorithm;algorithm	SE	-60.23329746912587	38.345780633640935	176790
4bc04f87a48cf8fb5fdcb87cbfe140fdf0fc0d74	safe and automatic live update for operating systems	live update;automatic updates;state checking;state transfer;update safety;operating systems	Increasingly many systems have to run all the time with no downtime allowed. Consider, for example, systems controlling electric power plants and e-banking servers. Nevertheless, security patches and a constant stream of new operating system versions need to be deployed without stopping running programs. These factors naturally lead to a pressing demand for live update---upgrading all or parts of the operating system without rebooting. Unfortunately, existing solutions require significant manual intervention and thus work reliably only for small operating system patches.  In this paper, we describe an automated system for live update that can safely and automatically handle major upgrades without rebooting. We have implemented our ideas in Proteos, a new research OS designed with live update in mind. Proteos relies on system support and nonintrusive instrumentation to handle even very complex updates with minimal manual effort. The key novelty is the idea of state quiescence, which allows updates to happen only in safe and predictable system states. A second novelty is the ability to automatically perform transactional live updates at the process level, ensuring a safe and stable update process. Unlike prior solutions, Proteos supports automated state transfer, state checking, and hot rollback. We have evaluated Proteos on 50 real updates and on novel live update scenarios. The results show that our techniques can effectively support both simple and complex updates, while outperforming prior solutions in terms of flexibility, security, reliability, and stability of the update process.	booting;downtime;online banking;operating system;patch (computing);proteus;quiescence search	Cristiano Giuffrida;Anton Kuijsten;Andrew S. Tanenbaum	2013		10.1145/2451116.2451147	real-time computing;computer science;operating system;database;computer security	Arch	-56.086748125643446	40.87686008367837	176946
219241cd2eb39264a32ed37f75252d17dfb11663	measuring coupling and cohesion: an information-theory approach	information theory approach;software measurement software design information theory software quality entropy software engineering software systems information analysis software metrics computer science;software metrics;graph theory;coupling;design decisions;software measurement;software system;object oriented design;call graph;software metrics information theory approach coupling cohesion software design structure chart calling relationships object oriented design design decisions software quality software system subgraph information theory based measures modular system system level coupling intramodule coupling intramodule abstraction intermodule coupling information theory approach;software systems;software engineering;cohesion;intramodule coupling;intermodule coupling;modular system;structure chart;system level coupling;excess entropy;information theory based measures;entropy;intramodule abstraction;computer science;subgraph;software design;similarity measure;information analysis;software quality;information theory;information theory software metrics software quality graph theory;calling relationships	The design of software is often depicted by graphs that show components and their relationships. For example, a structure chart shows the calling relationships among components. Object-oriented design is based on various graphs, as well. Such graphs are abstractions of the software, devised to depict certain design decisions. Coupling and cohesion are attributes that summarizes the degree of interdependence or connectivity among subsystems and within subsystems, respectively. When used in conjunction with measures of other attributes, coupling and cohesion can contribute to an assessment or prediction of software quality. Let a graph be an abstraction of a software system and let a subgraph represent a module (subsystem). This paper proposes information theory-based measures of coupling and cohesion of a modular system. These measures have the properties of system-level coupling and cohesion de ned by Briand, Morasca, and Basili. Coupling is based on relationships between modules. We also propose a similar measure for intramodule coupling based on an intramodule abstraction of the software, rather than intermodule, but intramodule coupling is calculated in the same way as intermodule coupling. We de ne cohesion in terms of intramodule coupling, normalized to between zero and one. We illustrate the measures with example graphs. Preliminary analysis showed that the information-theory approach has ner discrimination than counting.	cohesion (computer science);dual total correlation;graph (discrete mathematics);information theory;interdependence;offset binary;relevance;software architecture;software developer;software engineering;software metric;software quality;software system;structure chart;victor basili	Edward B. Allen;Taghi M. Khoshgoftaar	1999		10.1109/METRIC.1999.809733	reliability engineering;information theory;computer science;systems engineering;engineering;graph theory;theoretical computer science;software engineering;software system	SE	-57.2098175542125	33.92441887366934	176977
77f453f551fdcc23119c0c9f82fc1501afdaaa89	external assessment of qos provisioning in distributed cloud services	quality of service servers time factors computational modeling cognition cloud computing fault tolerance;servers;computational modeling;time factors;web services cloud computing quality of service;system internal mechanisms qos provisioning distributed cloud services distributed service support system replicated data services qos assessment sla violations resource depletions resource outages cloud resources third party providers;fault tolerance;cognition;quality of service;cloud computing	Given a distributed service support system S realized on a cloud (e.g., replicated data services), QoS assessment captures the SLA violations that may arise under various resource depletions and outages faced by S. The issue of less-than-100% guarantee of QoS arises due to the lax control of the underlying cloud resources and components that is typical of third-party providers. Our goal is to reason about how well the system-internal mechanisms are geared to offer a required level of service to the application. We employ computational models of S to determine the optimal feasible output and verify how close is the actual behavior of S to this 'gold-standard'. Case studies from diverse application domains are described to corroborate our assessment methodology.	adaptive system;angular defect;application domain;cloud computing;computation;computational model;content delivery network;dependability;distributed computing;mv-algebra;platform as a service;provisioning;quality of service;service-level agreement;simulation;software development;structural similarity;systems management	Kaliappa Nadar Ravindran;Arun Adiththan;Michael Iannelli;Mohammad Rabby	2016	2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop (DSN-W)	10.1109/DSN-W.2016.74	fault tolerance;real-time computing;mobile qos;cognition;quality of service;cloud computing;computer science;operating system;distributed computing;computational model;computer security;server;computer network	HPC	-48.38840315407594	43.546746356094765	177120
b8a4a6b236097198eec32e3c8c939725f9d95cd5	method for operation support systems (oss) integration by using semantic web	optical fiber subscriber loops;outsourcing;access network;flexible cooperation operation support systems integration oss integration semantic web enterprise networks ip vpn large area ethernet access networks dsl providers ontology;information model;electronic mail;operations support systems;dsl;service provider;dsl providers;oss integration;enterprise networks;business communication;resource description framework;companies;semantic web ontologies electronic mail companies ethernet networks dsl resource description framework costs optical fiber subscriber loops outsourcing;information integration;computer network management semantic web business communication;computer network management;large area ethernet access networks;semantic web;operation support systems integration;flexible cooperation;ontologies;ethernet networks;ontology;ip vpn	Enterprise networks constructed in recent years have rarely composed one service offered by one company. Instead, they have typically combined various services provided by various carriers, with connections between IP-VPN, large-area Ethernet access networks, DSL providers, and so forth. Under these conditions, providing fast services according to customer needs requires information on related enterprises for flexible integration of the mechanism. To provide a method of meeting this requirement, we applied semantic Web and ontology technology to achieve information integration between enterprises. We examine the resulting system, which enables flexible cooperation.	apl;access network;digital subscriber line;open sound system;s-expression;semantic web;virtual private network	Yoichiro Yamaki;Tatsuyuki Kimura	2004	2004 IEEE/IFIP Network Operations and Management Symposium (IEEE Cat. No.04CH37507)	10.1109/NOMS.2004.1317830	service provider;digital subscriber line;telecommunications;information model;computer science;ontology;information integration;semantic web;rdf;ontology;database;ontology-based data integration;business communication;world wide web;computer security;outsourcing;computer network;access network	OS	-48.5145090364899	45.63763149462625	177191
45aa08a09f5e7fdbb7751ed43639a012d19892a8	scta tracer: a distributed environment for standardized awareness support assessments	software;distributed system;groupware;collaborative work;computer supported cooperative work;probes;error analysis;time factors;distributed environment;four quadrant system scta tracer distributed environment standardized awareness support assessments cooperative environments computer supported cooperative work grasp awareness standardized coordination task assessment;xml;time factor;tools and techniques;early development;probes java time factors error analysis xml software collaborative work;java	Awareness support in cooperative environments has been a research issue in the area of distributed systems for computer-supported cooperative work for more than two decades. Measuring its effectiveness remains a complex task since it is difficult to grasp awareness in situ. Consequently, techniques and tools are required generating results while a user's awareness is still present. The Standardized Coordination Task Assessment (SCTA) and its tool the SCTA Tracer use freeze probes to query participants at specific points in time while working on a common task measuring and recording response times, performance, and errors. The result is visualized in a four quadrant system distinguishing illusive, ineffective, inefficient, and ideal systems. The SCTA Tracer guides awareness support researchers and designers to focus their effort on essential concepts already at early development stages. This paper shows how a smart selection of tools and techniques is integrated for this complex task.	awareness;computer-supported cooperative work;distributed computing	Christoph Oemig;Tom Gross	2012	2012 20th Euromicro International Conference on Parallel, Distributed and Network-based Processing	10.1109/PDP.2012.98	real-time computing;xml;simulation;human–computer interaction;computer science;operating system;computer-supported cooperative work;distributed computing;programming language;java;distributed computing environment	SE	-50.8153504523046	39.307787755536424	177388
191202e2359df1471b5086a4dcf61dfd4ed8a4c3	dotrl: a platform for rapid reinforcement learning methods development and validation	octopus arm and half cheetah dotrl platform rapid reinforcement learning methods net framework benchmark environments;software engineering reinforcement learning evaluation platform;program verification learning artificial intelligence program testing;program verification;learning artificial intelligence visualization user interfaces protocols vectors libraries decision making;program testing;learning artificial intelligence	This paper introduces dotRL, a platform that enables fast implementation and testing of Reinforcement Learning algorithms against diverse environments. dotRL has been written under .NET framework and its main characteristics include: (i) adding a new learning algorithm or environment to the platform only requires implementing a simple interface, from then on it is ready to be coupled with other environments and algorithms, (ii) a set of tools is included that aid running and reporting experiments, (iii) a set of benchmark environments is included, with as demanding as Octopus-Arm and Half-Cheetah, (iv) the platform is available for instantaneous download, compilation, and execution, without libraries from different sources.	.net framework;algorithm;benchmark (computing);compiler;download;experiment;library (computing);machine learning;reinforcement learning	Bartosz Papis;Pawel Wawrzynski	2013	2013 Federated Conference on Computer Science and Information Systems		simulation;computer science;artificial intelligence;operating system;machine learning;database;programming language;hyper-heuristic	Robotics	-52.24515627673996	39.0072488432345	177473
1f8dc7f9fe86eee35ad212f32f6b6318efb1209b	optonet - a case study in using rigorous analysis techniques to justify a revised product assurance strategy	program diagnostics;system configuration;system testing industrial control software safety mission critical systems hardware assembly systems computer industry electrical equipment industry embedded software application software;software maintenance;embedded systems;program testing;systems analysis;scada systems program testing embedded systems safety critical software industrial control systems analysis software maintenance program diagnostics;system integration;safety critical software;industrial control;embedded software product mission critical industrial control systems safety related industrial control systems software upgrading system integrity properties revised product assurance strategy optonet rigorous system analysis system testing static analysis system configuration dynamic testing upgraded system components;system analysis;280302 software engineering;static analysis;scada systems;700199 computer software and services not elsewhere classified;embedded software	When upgrading software in mission-critical or safety-related industrial control systems, it is imperative to ensure that system integrity properties are preserved. Comprehensive system testing is one way to gain this assurance. This has limitations, however, in that the hardware may be too expensive to assemble a large test rig, or where a product upgrade is to be deployed in diversely configured systems. This paper describes a method that uses rigorous system analysis to justify the replacement of system testing with both static analysis of the system configuration and dynamic testing of the upgraded system components. The paper reports on industrial experience in applying this method to the OptoNet product, which is an embedded software product used in industrial control systems. System analysis techniques are used to develop a detailed understanding of how OptoNet components (RTUs) interact to realise OptoNet system behaviour. Based on this detailed understanding, recommendations for a revised assurance strategy are made. The lessons learnt in the trial application of this method to the OptoNet product are discussed, and possible extensions to the method are proposed.	control system;dynamic testing;dynamical system;embedded software;embedded system;imperative programming;mission critical;model checking;naruto shippuden: clash of ninja revolution 3;open road tolling;ork;real-time clock;real-time computing;remote terminal unit;software documentation;static program analysis;system analysis;system configuration;system integrity;system testing	Leesa Murray;Alena Griffiths;Paul A. Strooper	2002	Eighth IEEE International Conference on Engineering of Complex Computer Systems, 2002. Proceedings.	10.1109/ICECCS.2002.1181516	reliability engineering;systems analysis;embedded software;computer science;systems engineering;engineering;operating system;software engineering;system analysis;software maintenance;static analysis;system integration	SE	-48.85217958776406	32.569580645398254	177594
82833eb0d42d0ed265f8a1abb1bb47ac24fccee7	prototyping an inconsistency checking tool for software process models	static checking;software prototyping;inconsistency checking tool;prototypes;logic;software systems;manufacturing industries;computer industry;software engineering;alf project software prototyping inconsistency checking tool software engineering software process models;software process models;manufacturing processes;software process model;software prototyping software tools software engineering prototypes logic;constraint solving;alf project;software tools;process model;scanning probe microscopy;software tools software engineering software prototyping	Software process modeling has attracted much research effort in Software Engineering. However, there is little work reported for the verification of process models. In fact, the verification is often either performed by hand or it i s left to the enacting mechanism to detect inconsistencies during execution. Since process models are becoming more and more powerful and complex, their verification is also becoming increasingly difficult and critical. Our proposition is that we need special tools to help verifr the consistency of software process models. This paper presents part of our experience in designing and prototyping such a tool for the verification of software process models in the ALF project. The tool helps verifr the partial consistency of process models by statically detecting various inconsistencies. This prototype uses techniques developed in different fields such as compilation, constraint solving and logic. As far as we know, this is the first tool of its kind designed for the static checking for process models.	alf;compiler;constraint satisfaction problem;content reference identifier;correctness (computer science);estimation of signal parameters via rotational invariance techniques;gesellschaft für informatik;louvain modularity;process modeling;prolog;prototype;sensor;software development process;software engineering;software manufacturing	Jin-Kao Hao;F. Trousset;Jean Jacques	1992		10.1109/SEKE.1992.227924	verification and validation;verification and validation of computer simulation models;software engineering process group;scanning probe microscopy;software verification;computer science;systems engineering;engineering;package development process;software engineering;software construction;logic;goal-driven software development process;software development process;computer engineering	SE	-48.5718537074261	32.50142716641431	177801
655403d75870e72430d45d74a9ba5d8223f5dce9	tutorial: static analysis and dynamic testing of computer software	computer program;software testing;performance evaluation;software maintenance;software systems;code standards;tutorials;performance analysis;system testing;static analysis;quality control;computer errors;dynamic analysis;tutorials software testing software maintenance software systems computer errors performance evaluation performance analysis code standards system testing quality control	Computer programs can be checked for errors statically and dynamically. Static analysis looks for structural and semantic faults in programs. Dynamic analysis affirms proper operation—and helps identify errors.	computer program;dynamic testing;static program analysis	Richard E. Fairley	1978	Computer	10.1109/C-M.1978.218132	non-regression testing;development testing;quality control;verification and validation;computing;regression testing;software performance testing;system integration testing;computer science;acceptance testing;package development process;software reliability testing;software development;software engineering;software construction;dynamic testing;dynamic program analysis;software testing;software maintenance;system testing;static analysis;software quality;static program analysis;software quality analyst;software system;software peer review	SE	-61.50453860504396	33.191403576443584	177812
c0e8bcdfe571a215c635a2c5954185d85cae58f7	abstract object state and version recovery in n-version programming	abstract object state;formal specification;software reusability object oriented programming system recovery configuration management formal specification;software fault tolerance;object oriented programming;software diversity;system recovery;reusability abstract object state version recovery n version programming software diversity object oriented systems faulty version recovery nvp scheme abstract version state common general description abstract state formal description object state abstraction oo programming layom psl recovery features meta object architecture object version recovery problem protocol recoverable version objects;object oriented;fault tolerance;software reusability;metaobject protocol;object oriented programming computer bugs programming profession diversity methods object oriented modeling protocols usability software systems software design software quality;faulty version recovery;configuration management;n version programming	The paper deals with the use of software diversity, specifically, N-version programming (NVP) in object oriented (OO) systems. We formulate the problem of faulty version recovery and show how our NVP scheme, developed recently, can be extended to solve it. Our approach relies on using the abstract version state, which represents a common general description of the states of all correct version objects. The recovery consists in mapping the state of a correct version onto the state of the faulty version via the abstract state. We introduce a formal description of our model and show that many ideas related to object state abstraction can be found in the existing research on OO programming. We discuss extensions of LAYOM and PSL as promising practical approaches for developing recovery features in OO programming. As an alternative solution, we propose a meta-object architecture and a related protocol which can facilitate the solution of the object version recovery problem. The paper finishes with a brief discussion of engineering steps which have to be done for developing recoverable version objects and of some approaches which can improve the reusability of the scheme proposed.	n-version programming	Alexander Romanovsky	1999		10.1109/TOOLS.1999.779002	real-time computing;computer science;systems engineering;programming language	Robotics	-51.257616365029904	32.911641917153275	177988
a09b2bf38bb968fa4a8f6ca12c651f0a3c752d15	concernlines: a timeline view of co-occurring concerns	software;reverse engineering program visualisation;history;data visualization software systems programming history computer science feedback user interfaces displays frequency java;software systems;concernlines tool;data mining;non functional requirement;evolution biology;timeline view co occurring concern visualization software evolution system understanding software release history non functional requirement software component concernlines tool cognitive process;cognitive process;visualization;co occurring concern visualization;software release history;data visualization;software component;software evolution system understanding;functional requirement;program visualisation;conferences;reverse engineering;timeline view	Understanding the evolution of a software system requires understanding how information about the release history, non-functional requirements and project milestones relates to functional requirements on the software components. This short paper describes a new tool, called CONCERNLINES, that supports this cognitive process by visualizing co-occurring concerns over time.	cognition;component-based software engineering;functional requirement;non-functional requirement;software system;timeline	Christoph Treude;Margaret-Anne D. Storey	2009	2009 IEEE 31st International Conference on Software Engineering	10.1109/ICSE.2009.5070559	cognition;visualization;computer science;systems engineering;component-based software engineering;software engineering;database;world wide web;functional requirement;non-functional requirement;data visualization;reverse engineering;software system	SE	-55.12796583231138	33.9017745345855	178025
7d0577772fe06b773d359d1b4060fce92fd4948f	antminer: mining more bugs by reducing noise interference	databases;kernel;会议论文;data mining;code mining;linux;program slicing;bug detection;computer bugs;programming	Detecting bugs with code mining has proven to be an effective approach. However, the existing methods suffer from reporting serious false positives and false negatives. In this paper, we developed an approach called AntMiner to improve the precision of code mining by carefully preprocessing the source code. Specifically, we employ the program slicing technique to decompose the original source repository into independent sub-repositories, taking critical operations (automatically extracted from source code) as slicing criteria. In this way, the statements irrelevant to a critical operation are excluded from the corresponding sub-repository. Besides, various semantics-equivalent representations are normalized into a canonical form. Eventually, the mining process can be performed on a refined code database, and false positives and false negatives can be significantly pruned. We have implemented AntMiner and applied it to detect bugs in the Linux kernel. It reported 52 violations that have been either confirmed as real bugs by the kernel development community or fixed in new kernel versions. Among them, 41 cannot be detected by a widely used representative analysis tool Coverity. Besides, the result of a comparative analysis shows that our approach can effectively improve the precision of code mining and detect subtle bugs that have previously been missed.	interference (communication);kernel (operating system);linux;preprocessor;program slicing;qualitative comparative analysis;relevance;software bug	Bin Liang;Pan Bian;Yan Zhang;Wenchang Shi;Wei You;Yan Cai	2016	2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE)	10.1145/2884781.2884870	programming;program slicing;kernel;software bug;bebugging;computer science;theoretical computer science;operating system;data mining;world wide web;linux kernel	SE	-59.133111953961546	38.528410002422	178210
25bb5a68218681d1edcab0494f2e03da06d3d234	a tool for helping teach a programming method	institutional repositories;programming errors;formal specification;fedora;program construction;program verification;vital;student learning;vtls;invariant;ils;programming course	We present and discuss a tool that checks the correctness of simple programs constructed according to the structured programming method. The tool is intended to provide interesting feedback to students learning the programming method: it detects programming and/or reasoning errors and it provides typical counter-examples. We argue that our system is better adapted to our pedagogical context than other verification tools and we report on preliminary experiments with the tool in a third year programming course.	correctness (computer science);experiment;structured programming	Isabelle Dony;Baudouin Le Charlier	2006		10.1145/1140124.1140181	first-generation programming language;constraint programming;declarative programming;simulation;programming domain;reactive programming;computer science;theoretical computer science;extensible programming;software engineering;invariant;jackson structured programming;software construction;formal specification;programming paradigm;event-driven programming;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;programming language specification	PL	-50.058367817132705	33.80954405741459	178270
0ba33ca4d3e58cac80e32a545bc5bf88170cde7f	understanding log lines using development knowledge	program understanding;google;web search electronic mail context google software systems knowledge engineering engines;electronic mail;software maintenance;software systems;engines;software logs;web search;system monitoring internet public domain software software maintenance;context;program understanding software maintenance software logs;web search log lines development knowledge log maintenance open source systems;knowledge engineering	Logs are generated by output statements that developers insert into the code. By recording the system behaviour during runtime, logs play an important role in the maintenance of large software systems. The rich nature of logs has introduced a new market of log management applications (e.g., Splunk, XpoLog and log stash) that assist in storing, querying and analyzing logs. Moreover, recent research has demonstrated the importance of logs in operating, understanding and improving software systems. Thus log maintenance is an important task for the developers. However, all too often practitioners (i.e., operators and administrators) are left without any support to help them unravel the meaning and impact of specific log lines. By spending over 100 human hours and manually examining all the email threads in the mailing list for three open source systems (Hadoop, Cassandra and Zookeeper) and performing web search on sampled logging statements, we found 15 email inquiries and 73 inquiries from web search about different log lines. We identified that five types of development knowledge that are often sought from the logs by practitioners: meaning, cause, context, impact and solution. Due to the frequency and nature of log lines about which real customers inquire, documenting all the log lines or identifying which ones to document is not efficient. Hence in this paper we propose an on-demand approach, which associates the development knowledge present in various development repositories (e.g., code commits and issues reports) with the log lines. Our case studies show that the derived development knowledge can be used to resolve real-life inquiries about logs.	apache cassandra;apache hadoop;correctness (computer science);email;embedded system;log management;open-source software;randomness;real life;redo log;run time (program lifecycle phase);software documentation;software maintenance;software system;web search engine	Weiyi Shang;Meiyappan Nagappan;Ahmed E. Hassan;Zhen Ming Jiang	2014	2014 IEEE International Conference on Software Maintenance and Evolution	10.1109/ICSME.2014.24	computer science;engineering;software engineering;knowledge engineering;web log analysis software;data mining;database;programming language;software maintenance;world wide web;software system	SE	-61.01758919669706	41.6065481245475	178459
1370b7ec6cb56b0ff25f512bd673acbab214708c	automated concolic testing of smartphone apps	smartphone apps;android;testing event driven programs;concolic testing;gui testing;path explosion;technical report	We present an algorithm and a system for generating input events to exercise smartphone apps. Our approach is based on concolic testing and generates sequences of events automatically and systematically. It alleviates the path-explosion problem by checking a condition on program executions that identifies subsumption between different event sequences. We also describe our implementation of the approach for Android, the most popular smartphone app platform, and the results of an evaluation that demonstrates its effectiveness on five Android apps.	algorithm;android;concolic testing;mobile app;smartphone;subsumption architecture	Saswat Anand;Mayur Naik;Mary Jean Harrold;Hongseok Yang	2012		10.1145/2393596.2393666	embedded system;real-time computing;computer science;engineering;technical report;operating system;concolic testing;android	SE	-56.11812997030733	38.55992218137647	178861
fa961f37618a9c210991a96858dc4a56a2ae567f	evolutionary generation of test data for path coverage with faults detection	software testing;evolutionary computation;software fault tolerance evolutionary computation mathematical analysis program testing;test data generation;software fault tolerance;evolutionary optimization software testing path coverage faults detection test data multi objective;mathematical analysis;totinfo program fault detection software testing program under test path oriented test data generation data traversing target paths mathematical model risk level multiobjective evolutionary optimization algorithm;program testing;fault detection optimization frequency division multiplexing software testing mathematical model instruments educational institutions;fault detection;frequency division multiplexing;mathematical model;evolutionary optimization	The aim of software testing is to find faults in the program under test. Previous methods of path-oriented test data generation can generate test data traversing target paths, but they may not guarantee to find faults in the program. We present a method of evolutionary generation of test data for path coverage with faults detection in this paper. First, we establish a mathematical model of the problem considered in this paper, in which the number of faults detected in the path traversed by test data, and the risk level of faults are optimization objectives, and the approach level of the traversed path from the target one is a constraint. Then, we generate test data using a multi-objective evolutionary optimization algorithm with constraints. Finally, we apply the proposed method in a benchmark program bubble sort and an industrial program totinfo, and compare it with the traditional method. The experimental results conform that our method can generate test data that not only traverse the target path but also detect faults in it. Our achievement provides a novel way to generate test data for path coverage with faults detection.	algorithm;benchmark (computing);bubble sort;code coverage;display resolution;fault detection and isolation;mathematical model;mathematical optimization;multi-objective optimization;optimization problem;software testing;traverse;test data generation	Yan Zhang;Dun-Wei Gong;Yongjin Luo	2011	2011 Seventh International Conference on Natural Computation	10.1109/ICNC.2011.6022397	basis path testing;test data generation;real-time computing;fault coverage;computer science;theoretical computer science;algorithm	SE	-59.89797504830575	35.632316809467774	179275
743b18e8d252f4239c4f389868ca2bd1e1a742f8	practical ultra‐reliability for abstract data types	abstract data type;self checking;ultra reliability;bi directional term rewriting;term rewriting	The Term Redundancy Method (TRM) is a novel approach for obtaining ultra-reliable programs through specification-based testing. Current specification-based testing schemes need a prohibitively large number of test cases for estimating ultra-reliability. They assume the availability of an accurate program-usage distribution prior to testing, and they assume the availability of a test oracle. This paper shows how to obtain ultra-reliable abstract data types specified with equational specifications, with a practical number of test cases, without an accurate usage distribution, and without the usual test oracle. The effectiveness of the TRM in failure detection and recovery is demonstrated on the aircraft collision avoidance system TCAS. Copyright c © 2007 John Wiley & Sons, Ltd.	abstract data type;ada;airborne collision avoidance system;c++;computation;federal enterprise architecture;java;john d. wiley;oracle (software testing);test case;traffic collision avoidance system	Borislav Nikolik;Dick Hamlet	2007	Softw. Test., Verif. Reliab.	10.1002/stvr.367	computer science;theoretical computer science;database;programming language;abstract data type;algorithm	SE	-49.86540856455135	36.46944361368914	179481
c297ac259b9a73cc994bc234fb06378f2b222878	comments on “algorithmic aspectsof hardware/software partitioning:1d search algorithms”	software;theoretical description hardware software partitioning 1d search algorithms;electronic mail;search problems hardware software codesign;search methods;1d search algorithm hardware software partitioning;1d search algorithm;hardware software partitioning algorithms software algorithms educational institutions search methods electronic mail;hardware software partitioning;software algorithms;partitioning algorithms;hardware	In this paper, the work in is analyzed. An error in its theoretical description part is pointed out and illustrated by a simple example. A modification suggestion is proposed to make the theoretical description of the work more deliberate and thus being used appropriately.	algorithm;norm (social)	Hao-Jun Quan;Tao Zhang;Qiang Liu;Jichang Guo;Xiaodong Wang;Ruimin Hu	2014	IEEE Transactions on Computers	10.1109/TC.2012.174	hardware compatibility list;computer architecture;software sizing;search-based software engineering;computer science;backporting;theoretical computer science;software framework;component-based software engineering;software development;software design description;software construction;hardware architecture;distributed computing;software system	Vision	-49.732377872742916	33.41892846236947	179703
da6d6ebb1de6c5339e3fbb4c1591ef087c872f9f	scaling up the fitness function for reverse engineering feature models		Recent research on software product line engineering has led to several search-based frameworks for reverse engineering feature models. The most common fitness function utilized maximizes the number of matched products with an oracle set of products. However, to calculate this fitness each product defined by the chromosome has to be enumerated using a SAT solver and this limits scalability to product lines with fewer than 30 features. In this paper we propose (textit{SAT}_{textit{ff}}), a fitness function that simulates validity by computing the difference between constraints in the chromosome and oracle. In an empirical study on 101 feature models comparing (textit{SAT}_{textit{ff}}) with two existing fitness functions that use the enumeration technique we find that (textit{SAT}_{textit{ff}}) shows a significant improvement over one, and no significant difference with the other one. We also find that (textit{SAT}_{textit{ff}}) requires only 7 % of the runtime on average scaling to feature models with as many as 97 features.	fitness function;reverse engineering	Thammasak Thianniwet;Myra B. Cohen	2016		10.1007/978-3-319-47106-8_9	fitness approximation	DB	-59.56372270662446	34.16815785830884	179790
38b7081a6a0106aee14bccf7f16de82c8e7cf9a3	round-trip engineering with the two-tier programming toolkit	design verification;tool support;software evolution;java;codecharts	A major impediment to the long-term quality of large and complex programs is inconsistency between design and implementation. Conflicts between intent and execution are common because detecting them is laborious, error-prone, and poorly supported, and because the costs of continuously maintaining design documents outweigh immediate gains. A growing inconsistency between design and implementation results in software that is unpredictable and poorly understood. Round-trip engineering tools support an iterative process of detecting conflicts and resolving them by changing either the design or the implementation. We describe a Toolkit which supports a round-trip engineering of native Java programs without interfering with any existing practices, tools, or development environments, thereby posing a minimal barrier on adoption. The Toolkit includes a user-guided software visualization and design recovery tool, which generates Codecharts from source code. A “round-trip” process is possible because Codecharts visualizing source code can be edited to reflect the intended design, and the Verifier can detect conflicts between the intended and as-implemented design. We demonstrate each stage in this process, showing how the Toolkit effectively helps to close the gap between design and implementation, recreate design documentation, and maintaining consistency between intent and execution.	c++;call of duty: black ops;code refactoring;cognitive dimensions of notations;conformance testing;correctness (computer science);critical graph;design pattern;diagram;document;documentation;entity–relationship model;human–computer interaction;iteration;java;level of detail;maple;multitier architecture;programmer;programming language;round-trip engineering;runtime verification;semiconductor industry;sensor;software development;software visualization;technical debt;undecidable problem	Amnon H. Eden;Epameinondas Gasparis;Jon Nicholson;Rick Kazman	2017	Software Quality Journal	10.1007/s11219-017-9363-9	software visualization;documentation;systems engineering;computer science;real-time computing;source code;software;iterative and incremental development;java;round-trip engineering;software evolution	SE	-52.51629901230474	33.646954339340645	180221
2a68cac03e6fcfe1045b8ef4f4e81cf853d5cae5	the implementation of the cha-q meta-model: a comprehensive, change-centric software representation		Although contemporary software development processes have embraced the need for continuous change, most development tools still assume that they act upon a single complete release of the system. The CHA-Q project (Change-centric Quality Assurance) aims to strike a balance between agility and reliability through change-centric quality assurance tools. These tools are to share a first-class representation of changes to software artefacts. In this paper we present the CHA-Q meta-model that defines this representation and highlight important characteristics of its implementation: an object-oriented API, persistency through a graph database, and a strategy for tracking the history of artefacts in a memory-efficient manner.	abstract syntax tree;application programming interface;class browser;code refactoring;control system;data logger;eclipse;email;entity;experiment;graph database;identifier;java;language-independent specification;metamodeling;programming tool;requirement;smalltalk;software bug;software development;software system;test case;traceability;version control;visualworks	Coen De Roover;Christophe Scholliers;Viviane Jonckers;Javier Pérez;Alessandro Murgia;Serge Demeyer	2014	ECEASST	10.14279/tuj.eceasst.0.902	simulation;computer science;software engineering;software quality analyst	SE	-53.30817546787662	33.81119441519937	180491
9814e273bd95b3daaf5cb7f729116a86238312c1	an approach to aid the understanding and maintenance of input validation	software tool;software systems;software systems prototypes automatic control programming profession application software flow graphs databases software prototyping control systems guidelines;input validation maintenance;data dependence;software tools;source code;program slicing;software tools program slicing;input validation understanding;software tool input validation understanding input validation maintenance program slicing	Input validation is an essential and a very important feature in any software system that has intensive interaction with its users. In this paper, we introduce some invariant properties with regards to input validation through analyzing the control and data dependency among inputs accessed and effects raised in a program. We then propose a method for the automated recovery of input validation from program source code. Based on the information recovered, we present the techniques to aid the understanding and maintenance of the feature using program slicing. A prototype tool has been implemented to validate the approach, and the empirical results show that the proposed approach can be very useful and effective for both experienced and inexperienced programmers	data dependency;data validation;experience;program slicing;programmer;prototype;software system	Hui Liu;Hee Beng Kuan Tan	2006	2006 22nd IEEE International Conference on Software Maintenance	10.1109/ICSM.2006.12	reliability engineering;verification and validation;program slicing;software sizing;software verification;computer science;engineering;backporting;software framework;software development;software engineering;data validation;software construction;database;programming language;software analytics;validation rule;software deployment;software quality control;static program analysis;software system;source code	SE	-56.099009618238206	36.26270941097728	180590
2f53f78f31d0fbfea426b08de573bb79af1fc04c	context-awareness to improve anomaly detection in dynamic service oriented architectures	unifi;verification;reliability;firenze;dependable systems;affidabili;ricerca;resilient computing lab;dependability;validation;rcl;affidabilita;florence;sistemi;assessment	Revealing anomalies to support error detection in software-intensive systems is a promising approach when traditional detection mechanisms are considered inadequate or not applicable. The core of anomaly detection lies in the definition of the expected behavior of the observed system. Unfortunately, the behavior of complex and dynamic systems is particularly difficult to understand. To improve the accuracy of anomaly detection in such systems, in this paper we present a context-aware anomaly detection framework which acquires information on the running services to calibrate the anomaly detection. To cope with system dynamicity, our framework avoids instrumenting probes into the application layer of the observed system monitoring multiple underlying layers instead. Experimental evaluation shows that the detection accuracy is increased considerably through context-awareness and multiple layers monitoring. Results are compared to state-of-the-art anomaly detectors exercised in demanding more static contexts.	anomaly detection;context awareness;service-oriented architecture	Tommaso Zoppi;Andrea Ceccarelli;Andrea Bondavalli	2016		10.1007/978-3-319-45477-1_12	reliability engineering;embedded system;real-time computing;verification;computer science;engineering;reliability;dependability;educational assessment;statistics	ML	-48.99148664592562	39.58683430903211	181022
5c121691984ee975053fc3b5110ec6c790af34d2	making data-driven porting decisions with tuscan		Software typically outlives the platform that it was originally written for. To smooth the transition to new tools and platforms, programs should depend on the underlying platform as little as possible. In practice, however, software build processes are highly sensitive to their build platform, notably the implementation of the compiler and standard library. This makes it difficult to port existing, mature software to emerging platforms---web based runtimes like WebAssembly, resource-constrained environments for Internet-of-Things devices, or innovative new operating systems like Fuchsia.   We present Tuscan, a framework for conducting automatic, deterministic, reproducible tests on build systems. Tuscan is the first framework to solve the problem of reproducibly testing builds cross-platform at massive scale. We also wrote a build wrapper, Red, which hijacks builds to tolerate common failures that arise from platform dependence, allowing the test harness to discover errors later in the build. Authors of innovative platforms can use Tuscan and Red to test the extent of unportability in the software ecosystem, and to quantify the effort necessary to port legacy software.   We evaluated Tuscan by building an operating system distribution, consisting of 2,699 Red-wrapped programs, on four platforms, yielding a `catalog' of the most common portability errors. This catalog informs data-driven porting decisions and motivates changes to programs, build systems, and language standards; systematically quantifies problems that platform writers have hitherto discovered only on an ad-hoc basis; and forms the basis for a common substrate of portability fixes that developers can apply to their software.	c++;compiler;fuchsia;hoc (programming language);institute for operations research and the management sciences;legacy system;machine learning;mantle;operating system;runtime system;software build;software developer;software ecosystem;software portability;standard library;test harness;text corpus;toolchain;webassembly	Kareem Khazem;Earl T. Barr;Petr Hosek	2018		10.1145/3213846.3213855	compiler;test harness;software build;computer science;porting;systems engineering;real-time computing;legacy system;software ecosystem;software;software portability	SE	-59.48724621180704	41.92300361479746	181129
24b561d17a35e5a03432f0a9b803c95b6ce37ff5	spam filter based approach for finding fault-prone software modules	fault prone software module;program diagnostics;unsolicited electronic mail;electronic mail;e mail detection;text mining;application software;open source development;information filtering;software fault tolerance;data mining;spam filter based approach;source code module;postal services;spam filtering;logistics;fault detection;source code module spam filter based approach fault prone software module text mining e mail detection;source code;unsolicited electronic mail information filtering information filters electronic mail fault detection open source software application software software quality logistics postal services;unsolicited e mail;unsolicited e mail data mining information filtering information filters program diagnostics software fault tolerance software quality;information filters;software quality;open source software	Because of the increase of needs for spam e-mail detection, the spam filtering technique has been improved as a convenient and effective technique for text mining. We propose a novel approach to detect fault-prone modules in a way that the source code modules are considered as text files and are applied to the spam filter directly. In order to show the applicability of our approach, we conducted experimental applications using source code repositories of Java based open source developments. The result of experiments shows that our approach can classify more than 75% of software modules correctly.	anti-spam techniques;email filtering;experiment;java;modular programming;open-source software;repository (version control);spamming;text mining	Osamu Mizuno;Shiro Ikami;Shuya Nakaichi;Tohru Kikuno	2007	Fourth International Workshop on Mining Software Repositories (MSR'07:ICSE Workshops 2007)	10.1109/MSR.2007.29	logistics;text mining;application software;computer science;operating system;data mining;database;world wide web;fault detection and isolation;software quality;software fault tolerance;source code	SE	-61.52013100060034	40.989728849360866	181606
9db34b08fd642d4ac4801246607b28f63731e140	using code quality features to predict bugs in procedural software systems		A wide range of metrics have been used as features to build bug (or fault) predictors. However, most of the existing predictors focus mostly on object-oriented (OO) systems, either because they rely on OO metrics or were evaluated mainly with OO systems. Procedural software systems (PSS), less addressed in bug prediction research, often suffer from maintainability problems because they typically consist of low-level applications, using for example preprocessors to cope with variability. Previous work evaluated sets of features (composed of static code metrics) proposed in existing approaches in the PSS context. However, explored metrics are limited to those that are part of traditional metric suites, being often associated with structural code properties. A type of information explored to a smaller extent in this context is the output of code quality tools that statically analyse source code, providing hints of code problems. In this paper, we investigate the use of information collected from quality tools to build bug predictors dedicated to PSS. We specify four features derived from code quality tools or associated with poor programming practices and evaluate the effectiveness of these features. Our evaluation shows that our proposed features improve bug predictors in our investigated context.	heart rate variability;high- and low-level;physical symbol system;software bug;software metric;software quality;software system	Cristiano Werner Araújo;Vanius Zapalowski;Ingrid Nunes	2018		10.1145/3266237.3266271	reliability engineering;systems engineering;software system;maintainability;source code;software quality;seven basic tools of quality;computer science	SE	-58.19122591936762	33.06681627655288	181807
de684d7bec5d2a0d025ec0f7cb49a0b4dadbf699	towards the interoperability between mpeg-21 rel and creative commons licenses	rights expression language;creative commons licenses;formal languages;copyright;digital rights management;multimedia computing;value chain;formal language interoperability mpeg 21 rel creative commons licenses digital rights management multimedia content distribution rights expression language;multimedia content distribution;open systems copyright formal languages multimedia computing;interoperability;licenses formal languages multimedia systems content management control systems publishing protection raw materials permission technology management;open systems;mpeg 21 rel;digital right management;formal language	Digital rights management systems enable the distribution and use of multimedia content in a controlled way through the complete digital value chain. The terms and conditions of use, delivery and access to multimedia content are specified using a Rights Expression Language. A REL is a formal language designed to define the permissions and constraints of use of multimedia content. Creative Commons has defined different types of licenses to govern digital works when publishing them. Nevertheless, these licenses do not use a specific REL. A key issue is to express CC licenses using a REL. Then, DRM systems can control the use of the governed digital works. This paper presents two approaches to express CC licenses using the MPEG-21 REL	cc system;digital data;digital rights management;formal language;interoperability;mpeg-21;rel;rights expression language;unified expression language	Eva Rodríguez;Jaime Delgado	2006	2006 Second International Conference on Automated Production of Cross Media Content for Multi-Channel Distribution (AXMEDIS'06)	10.1109/AXMEDIS.2006.55	computer science;multimedia;world wide web;computer security	HCI	-52.368472538481576	44.05106976524341	182213
19d1d699aad1bb424def120075ab1d3d37b43699	visualising exemplary program values	runtime value;tracing;visualization;variable occurrence;exemplary value;source code;exemplary program value;software visualization;software developer;branches;variables;software development	We describe an idea of a tool to aid software developers, similar to tracing and software visualization. The tool monitors a running program and log some values of its variables. The exemplary values, chosen by the tool, are later displayed onto the source code. Each variable occurrence in the source code is visualized with a few examples of its runtime values. We are unaware of such a tool implemented already, and the question which values should be selected seems interesting.	software developer;software visualization;tracing (software)	Marcin Stefaniak	2007		10.1145/1295014.1295046	variables;kpi-driven code analysis;software visualization;simulation;visualization;tracing;computer science;theoretical computer science;software development;software engineering;branch;programming language;static program analysis;source code	SE	-55.23652520320423	36.37681074672573	182664
561fab0b60f68b60c7517f544f0f92bb2dc25338	automated test case generation based on coverage analysis	java bytecode;software testing;generators;information systems;coverage analysis;unit testing;constraint solving java bytecode test case generation test data generation symbolic execution;automatic testing;virtual machining;runtime environment;test data generation;testing;data mining;software engineering;program testing java;assembly;arrays;automated test case generation;symbolic execution;graphical user interfaces;test case generation;program testing;image edge detection;choice point generation;control flow;backtracking;system testing;constraint solving;data flow;automatic testing java software testing system testing information systems virtual machining assembly runtime environment graphical user interfaces software engineering;backtracking automated test case generation coverage analysis java bytecode constraint solving choice point generation;java	We present a tool for the automated generation of unit tests. It symbolically executes Java bytecode in order to find execution paths through a program. To efficiently accomplish this task, it uses constraint solving, choice-point generation and backtracking. As the number of test cases found might be very high and most of them are redundant, we propose a novel way to eliminate test cases based on their contribution to the global coverage of the control-flow and data-flow. Besides discussing the techniques used to achieve this, we present experimental results to prove the feasibility of our approach.	avl tree;algorithm;backtracking;constraint satisfaction problem;control flow;coverage data;dataflow;java bytecode;randomness;robustness (computer science);symbolic execution;test case;unit testing	Tim A. Majchrzak;Herbert Kuchen	2009	2009 Third IEEE International Symposium on Theoretical Aspects of Software Engineering	10.1109/TASE.2009.33	real-time computing;computer science;theoretical computer science;software engineering;software testing;programming language	SE	-58.481904216770786	36.321683311547076	182871
2fb7042a5db33653c769b190c37fd11041db0c41	improving spectral-based fault localization using static analysis	fault localization metrics;spectral debugging;static analysis;dynamic analysis	SummaryrnDebugging is crucial for producing reliable software. One of the effective bug localization techniques is spectral-based fault localization (SBFL). It helps to locate a buggy statement by applying an evaluation metric to program spectra and ranking program components on the basis of the score it computes. SBFL is an example of a dynamic analysis – an analysis of computer program that is performed by executing it with sufficient number of test cases. Static analysis, on the other hand, is performed in a non-runtime environment. We introduce a weighting technique by combining these two kinds of program analysis. Static analysis is performed to categorize program statements into different classes and giving them weights based on the likelihood of being buggy statement. Statements are finally ranked on the basis of the weights computed by statementsu0027 categorization (static analysis) and scores computed by SBFL metrics (dynamic analysis). We evaluate the performance of our technique on Siemens test suite and Flex (having seeded bugs seeded by expert developers), Sed (having mixture of real and seeded bugs), and Space (having real bugs). In our evaluation, proposed weighting technique improves the performance of a wide variety of fault localization metrics up to 20% on single bug datasets and up to 42% on multi-bug datasets. Copyright © 2017 John Wiley u0026 Sons, Ltd.	static program analysis	Neelofar;Lee Naish;Jason Lee;Kotagiri Ramamohanarao	2017	Softw., Pract. Exper.	10.1002/spe.2490	geotechnical engineering;software;debugging;program analysis;test case;computer science;data mining;static analysis;artificial intelligence;test suite;ranking;pattern recognition;sed	SE	-60.764925350033636	35.963146048698945	182938
dfe3c09525a22d02226ddde80d03fce993299299	equitas: a tool-chain for functional safety and reliability improvement in automotive systems		To support advanced features such as hybrid engine control, intelligent energy management, and advanced driver assistance systems, automotive embedded systems must use advanced technologies. As a result, systems are becoming distributed and include dozens of Electronic Control Units (ECU). On the one hand, this tendency raises the issue of robustness and reliability, due to the increase in the error ratio with the integration level and the clock frequency. On the other hand, due to a lack of automation, software Validation and Verification (V&V) tends to swallow up 40% to 50% of the total development cost. The ``Enhanced Quality Using Intensive Test Analysis on Simulators'' (EQUITAS   1   ) project aims (1) to improve reliability and functional safety and (2) to limit the impact of software V&V on embedded systems costs and time-to-market. These two achievements are obtained by (1) developing a continuous tool-chain to automate the V&V process, (2) improving the relevance of the test campaigns by detecting redundant tests using equivalence classes, (3) providing assistance for hardware failure effect analysis (FMEA) and finally (4) assessing the tool-chain under the ISO 26262 requirements.	toolchain	Réda Nouacer;Manel Djemal;Smaïl Niar;Gilles Mouchard;Nicolas Rapin;Jean-Pierre Gallois;Philippe Fiani;Francois Chastrette;Arnault Lapitre;Toni Adriano;Bryan MacEachen	2016	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2016.07.020	embedded system;simulation;electrical engineering	EDA	-56.83493489441681	45.200612772255816	183712
834c006070cc693cfc207552dd370231384a7ec7	on the use of design defect examples to detect model refactoring opportunities	search-based software engineering;design defects;detection by example;genetic algorithm	Design defects are symptoms of design decay, which can lead to several maintenance problems. To detect these defects, most of existing research is based on the definition of rules that represent a combination of software metrics. These rules are sometimes not enough to detect design defects since it is difficult to find the best threshold values; the rules do not take into consideration the programming context, and it is challenging to find the best combination of metrics. As an alternative, we propose in this paper to identify design defects using a genetic algorithm based on the similarity/distance between the system under study and a set of defect examples without the need to define detection rules. We tested our approach on four open-source systems to identify three potential design defects. The results of our experiments confirm the effectiveness of the proposed approach.	class diagram;code refactoring;correctness (computer science);domain-specific language;experiment;fitness function;genetic algorithm;information retrieval;open-source software;precision and recall;software bug;software metric;software release life cycle;structural similarity	Adnane Ghannem;Ghizlane El-Boussaidi;Marouane Kessentini	2015	Software Quality Journal	10.1007/s11219-015-9271-9	reliability engineering;systems engineering;engineering;data mining	SE	-58.733471226765516	33.60724918720863	183876
448ea08a52605be76de015e6a1bbcacdd215c026	framework information based java software architecture recovery		Software systems tend to become more and more complex as they evolve, which makes it difficult to review, understand, and maintain the source code without complete architectural information, especially in case of large-scale systems. Software architecture recovery is considered an important method contributing to solving this problem. Hierarchical clustering is one of the techniques used to extract architectural information from lower level software representations, such as the source code. This paper is aimed at improving the accuracy of existing hierarchical clustering algorithms by allowing users to parameterize and configure framework information as framework-specific features. We have implemented our approach as an Eclipse plugin and have applied it to recovering the architecture of Java programs. Experiments are carried out on our benchmark built upon Java web applications in which the Spring Framework is used. The experimental results show that our approach can improve the accuracy of the recovered architecture to some extent.	algorithm;apache struts;architectural pattern;benchmark (computing);cluster analysis;eclipse;experiment;hierarchical clustering;java;machine learning;model–view–controller;plug-in (computing);programming language;software architecture recovery;software system;spring framework;threat (computer);web application	Xiaocong Li;Xiang Lin;Ning Ge	2017	2017 24th Asia-Pacific Software Engineering Conference Workshops (APSECW)	10.1109/APSECW.2017.15	computer science;real-time computing;architecture;software system;source code;plug-in;theoretical computer science;software;software architecture;software architecture recovery;java	SE	-56.96144755186969	34.90920941339181	184376
5ad9f6d0792d36782f859d31b574104d55a9a927	the database architectures research group at cwi	flash memory ssds;sorted index scan;partitioned sort	The workhorse and focal point for our research is MonetDB, an open-source columnar database system. Its development goes back as far as the early eighties when our first relational kernel, called Troll, was shipped as an open-source product. It spread to ca. 1000 sites world-wide and became part of a software case-tool until the beginning of the nineties. None of the code of this system has survived, but ideas and experiences on how to obtain a fast relational kernel by simplification and explicit materialization found their origin during this period. The second half of the eighties was spent on building the first distributed main-memory database system in the context of the national Prisma project. A fully functional system of 100 processors and a, for that time, wealthy 1GB of main memory showed the road to develop database technology from a different perspective. Design from the processor to the slow disk, rather than the other way around. Immediately after the Prisma project, a new kernel based on Binary Association Tables (BATs) was laid out. This storage engine became accessible through MIL, a scripting language intended as a target for compiling SQL queries. The target application domain was to better support scientific databases with their (archaic) file structures. It quickly shifted to a more urgent and emerging area. Several datamining projects called for better database support. It culminated in our first spinoff company, Data Distilleries, in 1995, which based their analytical customer relationship suite on the power provided by the early MonetDB implementations. In the years following, many technical innovations were paired with strong industrial maturing of the software base. Data Distilleries became a subsidiary of SPSS in 2003, which in turn was acquired by IBM in 2009. Moving MonetDB Version 4 into the open-source domain required a large number of extensions to the code base. It became of the utmost importance to support a mature implementation of the SQL03 standard, and the bulk of application programming interfaces (PHP, JDBC, Python, Perl, ODBC, RoR). The result of this activity was the first official open-source release in 2004. A very strong XQuery front-end was developed with partners and released in 2005 [1]. MonetDB remains a product well-supported by the group. All its members carry out part of the development and maintenance work, handling user inquiries, or act as guinea pigs of the newly added features. A thorough daily regression testing infrastructure ensures that changes applied to the code base survive an attack of ca. 20 platform configurations, including several Linux flavors, Windows, FreeBSD, Solaris, and MacOS X. A monthly bugfix release and ca. 3 feature releases per year support our ever growing user community. The web portal 1 provides access to this treasure chest of modern database technology. It all helped us to create and maintain a stable platform for innovative research directions, as summarized below. The MonetDB spin-off company was set up to support its market take-up, to provide a foundation for QA, support, and development activities that are hard to justify in a research institute on an ongoing basis.	application domain;application programming interface;central processing unit;circa;column-oriented dbms;compiler;computer architecture;computer data storage;dancing pigs;data mining;database engine;focal (programming language);freebsd;gigabyte;in-memory database;jdbc;linux;microsoft windows;monetdb;open database connectivity;open-source software;php;patch (computing);perl;portal;prisma;python;regression testing;spss;sql;scripting language;software quality assurance;text simplification;virtual community;world wide web;xquery	Martin L. Kersten;Stefan Manegold;K. Sjoerd Mullender	2011	SIGMOD Record	10.1145/2094114.2094124	parallel computing;computer hardware;computer science;database	DB	-53.57220131981858	39.42046995083151	184386
b98352569ad63fe5827867fb2478ab6603c5bf79	selecting object-oriented source code metrics to improve predictive models using a parallel genetic algorithm	parallel genetic algorithm;object oriented metrics;object oriented;genetic algorithm;predictive models;feature selection;source code;prediction model;cognitive complexity	Predictive models can be used to discover potentially problematic components. Source code metrics can be used as input features to predictive models, however, there are many structural and design measures that capture related metrics of coupling, cohesion, inheritance, complexity and size. Feature selection is the process of identifying a subset of attributes that improves the performance of a predictive model. This paper presents a prototype that implements a parallel genetic algorithm as a search-based feature selection method that enhances a predictive model's ability to identify cognitively complex components in a Java application.	cohesion (computer science);feature selection;genetic algorithm;java;predictive modelling;prototype;software metric	Rodrigo A. Vivanco;Dean Jin	2007		10.1145/1297846.1297879	computer science;machine learning;pattern recognition;data mining;predictive modelling;feature selection	SE	-62.496257015763895	34.36425069270796	184409
29030f95311c65c20be2c5d4cc80138b32072b3f	jumbl: a tool for model-based statistical testing	system reliability;model generation;model analysis jumbl model based statistical testing software quality usage model test cases generation system reliability j usage model builder library java class library command line tools model construction;statistical test;statistical analysis object oriented modeling system testing automatic testing software quality software libraries java telephony electronic mail reliability;reasoning about programs;program testing;program testing statistical testing software quality reasoning about programs software reliability java;cost effectiveness;statistical testing;software reliability;software quality;java	Statistical testing of software based on a usage model is a cost-effective and efficient means to make inferences about software quality. In order to apply this method, a usage model is developed and analyzed to validate its fitness for use in testing. The model may then be used to generate test cases representing expected usage, and to reason about system reliability given the performance on the set of tests. The J Usage Model Builder Library (JUMBL) is a Java class library and set of command-line tools for working with usage models. The JUMBL supports construction and analysis of models, generation of test cases, automated execution of tests, and analysis of testing results.	command-line interface;java class library;library (computing);software quality;test case	Stacy J. Prowell	2003		10.1109/HICSS.2003.1174916	non-regression testing;test strategy;keyword-driven testing;statistical hypothesis testing;verification and validation;regression testing;model-based testing;software performance testing;white-box testing;manual testing;system integration testing;integration testing;computer science;acceptance testing;software reliability testing;functional testing;software construction;dynamic testing;risk-based testing;software testing;programming language;test management approach;software quality;statistics	SE	-61.08661669356256	32.35111148703829	184443
77d5e26f258fa4022b67b40b94a9594e76332ae2	converting specifications in a subset of object-z to skeletal spec# code for both static and dynamic analysis	formal specification;software systems;formal method;static analysis;static and dynamic analysis;dynamic analysis;program correctness	Construction of correctness is an essential issue for the implementation of a reliable software system. Formal methods based verification techniques provide programmers various ways to reason their program correctness through mathematically supported static analysis and dynamic analysis. In this paper, we introduce a tool that converts formal specifications in a subset of Object-Z to skeletal Spec# code with assertions. This tool aims at facilitating the refinement from formal specifications to Spec# and the full usage of the static and dynamic analysis techniques in Spec#.	.net framework;computer science;computer scientist;correctness (computer science);formal methods;formal specification;formal verification;information assurance;jim woodcock;object-z;programmer;programming language;refinement (computing);return statement;return type;skeletal animation;software developer;software engineering;software system;spec#	Xiufeng Ni;Cui Zhang	2008	Journal of Object Technology	10.5381/jot.2008.7.8.a6	real-time computing;formal methods;formal verification;computer science;formal specification;database;dynamic program analysis;programming language;static analysis;software system	SE	-50.86495882785033	33.7429918685052	184622
94b55b77ff702dd7c1c0631dc4adc2d85e8f4972	non-functional requirement analysis and recommendation for software services	software;hierarchical clustering;program diagnostics;noise measurement vectors software algorithm design and analysis security availability monitoring;availability;software engineering program diagnostics;software engineering;noise measurement;non functional requirement;requirement analysis;vectors;monitoring;factor analysis;hierarchical clustering non functional requirement requirement analysis recommendation factor analysis;recommendation;security;factor analysis technique nonfunctional requirement analysis software services recommendation system;algorithm design and analysis	Non-Functional (NF) requirement is very important for the success of a software service. Considering that there could be multiple services implementing a same function, it is crucial for software providers to understand the real NF demands from consumers so that they can meet these demands and attract users. It is also crucial for consumers to know what is being offered so that they can pose realistic NF requests. We address both issues here by proposing a NF requirement analysis and recommendation system which works for both providers and consumers. NF requirements from various sources are first collected, and then we apply the factor analysis technique to identify those independent latent factors which contribute to those observable NF values. Finally we use cluster analysis to summarize the popular NF demands. Our experiment result shows the effectiveness of this approach.	algorithm;cluster analysis;factor analysis;functional requirement;latent variable;new foundations;non-functional requirement;observable;recommender system;requirements analysis;service (systems architecture);software as a service	Xiao-Lin Zhang;Chi-Hung Chi;Chen Ding;Raymond K. Wong	2013	2013 IEEE 20th International Conference on Web Services	10.1109/ICWS.2013.80	algorithm design;availability;requirements analysis;computer science;noise measurement;information security;operating system;data mining;hierarchical clustering;factor analysis;world wide web;non-functional requirement	SE	-54.97389010479737	44.071611909457495	184629
a252f8d2f358eb991cbb2d5ef03d8859416e7cc6	developers' perception of co-change patterns: an empirical study	package structure cochange pattern crosscutting cluster octopus cluster;software packages object oriented programming pattern clustering public domain software;maintenance engineering;pattern matching;clustering algorithms;open source software;clustering algorithms maintenance engineering pattern matching partitioning algorithms java open source software;partitioning algorithms;java	Co-change clusters are groups of classes that frequently change together. They are proposed as an alternative modular view, which can be used to assess the traditional decomposition of systems in packages. To investigate developer's perception of co-change clusters, we report in this paper a study with experts on six systems, implemented in two languages. We mine 102 co-change clusters from the version history of such systems, which are classified in three patterns regarding their projection to the package structure: Encapsulated, Crosscutting, and Octopus. We then collect the perception of expert developers on such clusters, aiming to ask two central questions: (a) what concerns and changes are captured by the extracted clusters? (b) do the extracted clusters reveal design anomalies? We conclude that Encapsulated Clusters are often viewed as healthy designs and that Crosscutting Clusters tend to be associated to design anomalies. Octopus Clusters are normally associated to expected class distributions, which are not easy to implement in an encapsulated way, according to the interviewed developers.	c++;computer cluster;emoticon;encapsulation (networking);programming language;software architecture	Luciana Lourdes Silva;Marco Tulio Valente;Marcelo de Almeida Maia;Nicolas Anquetil	2015	2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSM.2015.7332448	maintenance engineering;real-time computing;computer science;operating system;pattern matching;database;cluster analysis;programming language;java	SE	-56.44992847969391	34.50677648933264	184812
8d0fafa466ca7738ef2652f57720fe787bc2048a	weighted execution profiles for software testing	minimization;software testing;measurement;test suite minimization execution profiles software testing;software engineering;test suite minimization;minimization software testing measurement schedules java software engineering conferences;schedules;execution profiles;program testing minimisation;test suite minimization technique weighted execution profiles software testing;conferences;java	Existing execution profiles comprise information about the execution of program elements such as statements, branches, and def-use pairs. They lack any information about what elements are potentially more relevant to failure than others, such information could be leveraged to compute dissimilarity metrics that discriminate better between failing and passing profiles, which is beneficial to software testing. In this work, we explore three heuristics from which we will derive weights to be associated with the covered program elements such that higher weights indicate more potential relevance to failure. We evaluate our heuristics by measuring their impact on an established test suite minimization technique. Our experiments showed somewhat promising results.	branch (computer science);experiment;failure;fuzzy logic;heuristic (computer science);relevance;software testing;statement (computer science);test case;test suite	Joan Farjo;Wes Masri	2014	2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops	10.1109/ICSTW.2014.33	non-regression testing;keyword-driven testing;verification and validation;regression testing;real-time computing;model-based testing;software performance testing;white-box testing;manual testing;system integration testing;schedule;computer science;software reliability testing;operating system;software engineering;software construction;test suite;software testing;programming language;java;test management approach;measurement;test harness	SE	-60.824475724067774	35.61488226315464	185361
6cb29c0151bb4d6fffe15fbebe512327f4eba169	sensa: sensitivity analysis for quantitative change-impact prediction	change impact prediction;instruments;dependence analysis;history;semantics;history semantics runtime instruments sensitivity analysis syntactics;runtime;static forward slicing sensitivity analysis sensa quantitative change impact prediction stimuli variations software engineering tasks dynamic analysis technique code relationships dynamic forward slicing;software engineering program slicing;syntactics;sensitivity analysis;execution differencing change impact prediction dependence analysis sensitivity analysis;execution differencing	Sensitivity analysis determines how a system responds to stimuli variations, which can benefit important software-engineering tasks such as change-impact analysis. We present SENSA, a novel dynamic-analysis technique and tool that combines sensitivity analysis and execution differencing to estimate the dependencies among statements that occur in practice. In addition to identifying dependencies, SENSA quantifies them to estimate how much or how likely a statement depends on another. Quantifying dependencies helps developers prioritize and focus their inspection of code relationships. To assess the benefits of quantifying dependencies with SENSA, we applied it to various statements across Java subjects to find and prioritize the potential impacts of changing those statements. We found that SENSA predicts the actual impacts of changes to those statements more accurately than static and dynamic forward slicing. Our SENSA prototype tool is freely available for download.	autoregressive integrated moving average;download;java;prototype;software engineering	Haipeng Cai;Siyuan Jiang;Raúl A. Santelices;Ying-Jie Zhang;Yiji Zhang	2014	2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation	10.1109/SCAM.2014.25	real-time computing;computer science;data mining;semantics;sensitivity analysis;dependence analysis	SE	-62.349300572379335	35.68141703596818	185569
23cf6138412430605e057ba601b1c207199b016e	fluid source code views	navigation space technology application software programming profession scattering space exploration hypertext systems mice joining processes software systems;information sources;fluid source code view;system documentation;information space;visual cues;fluid document technology;source code;user interaction;code examination;user interfaces;code comprehension fluid source code view user interaction fluid document technology code examination;code comprehension;user interfaces reverse engineering system documentation;reverse engineering	Fluid documents enable the interactive incorporation of related material into a primary document. Visual cues within primary material indicate that related content is available and user interaction reveals related content in context. Fluid documents reduce the need for explicit navigation, maintain context when considering related material and support synthesization of disjoint yet related information. Source code documents are an excellent medium for the application of fluid document technology. The definition of software in the form of text based documents manifests a highly linked and fragmented information space requiring considerable navigation between elements during code examination and comprehension. In this paper we present fluid source code views, the application of fluid document technology to source code documents	fltk;primary source;text-based (computing)	Michael Desmond;Margaret-Anne D. Storey;Christopher Exton	2006	14th IEEE International Conference on Program Comprehension (ICPC'06)	10.1109/ICPC.2006.24	kpi-driven code analysis;code review;sensory cue;human–computer interaction;computer science;theoretical computer science;operating system;multimedia;internal documentation;programming language;user interface;reverse engineering;static program analysis;source code	HCI	-54.680607852605014	35.2531341580303	185593
7c89ca5ff4b700706f432225f782b4b11bfae7c7	design of the codeboost transformation system for domain-specific optimisation of c++ programs	optimising compilers;rewrite rule;design optimization runtime optimizing compilers informatics productivity software performance performance analysis bridges software engineering software libraries;numerical software;perforation;c language;software tools;sophus code abstract coding style domain specific optimization program compiler source to source transformation tool c program optimisation parsing semantic analysis stratego program transformation language user defined rewrite rules codeboost transformation framework totem annotation;domain specificity;semantic analysis;software tools optimising compilers c language	The use of a high-level, abstract coding style can greatly increase developer productivity. For numerical software, this can result in drastically reduced run-time performance. High-level, domain-specific optimisations can eliminate much of the overhead caused by an abstract coding style, but current compilers have poor support for domain-specific optimisation. In this paper we present CodeBoost, a source-to-source transformation tool for domain-specific optimisation of C++ programs. CodeBoost performs parsing, semantic analysis and pretty-printing, and transformations can be implemented either in the Stratego program transformation language, or as user-defined rewrite rules embedded within the C++ program. CodeBoost has been used with great success to optimise numerical applications written in the Sophus high-level coding style. We discuss the overall design of the CodeBoost transformation framework, and take a closer look at two important features of CodeBoost: user-defined rules and totem annotations. We also show briefly how CodeBoost is used to optimise Sophus code, resulting in applications that run twice as fast, or more.	c++;cascading style sheets;code refactoring;compiler;embedded system;gnu;high- and low-level;list of numerical analysis software;mathematical optimization;non-functional requirement;overhead (computing);parse tree;parsing;prettyprint;printing;program transformation;programmer;programming style;rewrite (programming);rewriting;source lines of code;source transformation;standards-compliant;supercomputer;technical standard;transformation language;web page	Otto Skrove Bagge;Karl Trygve Kalleberg;Magne Haveraaen;Eelco Visser	2003		10.1109/SCAM.2003.1238032	numerical analysis;computer science;theoretical computer science;software engineering;program optimization;database;programming language	PL	-52.54151824428811	36.014437865461545	185700
8b664f508aa021f50795e4f916ee617e16677ebb	how significant is the effect of fault interactions on coverage-based fault localizations?	software;debugging;instruments;measurement;software performance evaluation;object oriented programming;interference;fault interleaving phenomenon coverage based fault localization effectiveness software testing research community fault localization techniques coverage statistics fault interactions fault interference phenomenon programming languages object oriented features debugging performance coverage based fault localization;measurement debugging java interference software instruments;program testing;fault localizations;software reliability object oriented languages object oriented programming program debugging program testing software performance evaluation;program debugging;empirical studies;empirical studies fault localizations debugging;software reliability;object oriented languages;java	The effectiveness of coverage-based fault localizations in the presence of multiple faults has been a major concern for the software testing research community. A commonly held belief is that the fault localization techniques based on coverage statistics are less effective in the presence of multiple faults and their performance deteriorates. The fault interference phenomenon refers to cases where the software under test contains multiple faults whose interactions hinder effective debugging. The immediate research question that arises is to what extent fault interactions are influential. This paper focuses on verifying the existence of fault interference phenomenon in programs developed in programming languages with object-oriented features. The paper then statistically measures the influence and significance of fault interactions on the performance of debugging based on coverage-based fault localizations. The result verifies that the fault interleaving phenomenon occurs. However, its impact on the performance of fault localizations is negligible.	debugging;forward error correction;interaction;interference (communication);programming language;software testing;verification and validation	Xiaozhen Xue;Akbar Siami Namin	2013	2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement	10.1109/ESEM.2013.22	reliability engineering;real-time computing;fault coverage;computer science;stuck-at fault;distributed computing;programming language;object-oriented programming	SE	-62.296855392511006	36.02884890428627	186479
723647e55f226a02ec5610fe9ffb8615f2431ef3	predicting fault-prone software modules with rank sum classification	software metrics;software;measurement;support vector machines;software metrics program testing software fault tolerance;niobium;metrics;software fault tolerance;testing;support vector machines measurement niobium software testing inspection nasa;inspection;program testing;machine learning;machine learning metrics fault proneness;fault proneness;nasa;nb classifiers fault prone software modules prediction rank sum classification defect detection defect correction software development automated testing code inspections code fragments automated fault identification fault prone modules detection rates software metrics data predictive modelling fault proneness rank sum representation testing environments nasa metrics data program mdp data sets support vector machine svm naïve bayes classifier	The detection and correction of defects remains among the most time consuming and expensive aspects of software development. Extensive automated testing and code inspections may mitigate their effect, but some code fragments are necessarily more likely to be faulty than others, and automated identification of fault prone modules helps to focus testing and inspections, thus limiting wasted effort and potentially improving detection rates. However, software metrics data is often extremely noisy, with enormous imbalances in the size of the positive and negative classes. In this work, we present a new approach to predictive modelling of fault proneness in software modules, introducing a new feature representation to overcome some of these issues. This rank sum representation offers improved or at worst comparable performance to earlier approaches for standard data sets, and readily allows the user to choose an appropriate trade-off between precision and recall to optimise inspection effort to suit different testing environments. The method is evaluated using the NASA Metrics Data Program (MDP) data sets, and performance is compared with existing studies based on the Support Vector Machine (SVM) and Naïve Bayes (NB) Classifiers, and with our own comprehensive evaluation of these methods.	focus group;naive bayes classifier;precision and recall;predictive modelling;software development;software inspection;software metric;support vector machine;test automation	Jaspar Cahill;James M. Hogan;Richard N. Thomas	2013	2013 22nd Australian Software Engineering Conference	10.1109/ASWEC.2013.33	reliability engineering;support vector machine;niobium;inspection;computer science;software engineering;machine learning;data mining;software testing;metrics;software fault tolerance;software metric;measurement	SE	-62.72344990676436	37.552723930274595	186564
cf9584e1515b024bf01a2579f56da7841096500b	towards a reversible bpel debugger	debugging;web services bpel reversible debugging debugging model;bpel process testing;reversible debugging;history;web service reversible bpel debugger bpel process testing bpel process verification bpel process analysis abstract debugging api reversible concurrent debugging model;debugging model;web service;time factors;specification languages;business data processing;application program interfaces;business;abstract debugging api;web services;web services application program interfaces business data processing program debugging specification languages;reversible concurrent debugging model;bpel;bpel process analysis;bpel process verification;program debugging;reversible bpel debugger;service oriented architecture;debugging web services history service oriented architecture switches software engineering software testing data analysis kernel control systems;conferences	Despite the great momentum gained about the testing, analysis and verification of BPEL process, little attention has paid to the debugging issues, especially about the building of ad hoc debuggers. In this paper, we propose and implement RBDB (reversible BPEL debugger), a specially made reversible debugger for BPEL process. RBDB is built on the abstract debugging APIs to fulfill its functionality. A reversible concurrent debugging model and three strategies to handle different type of external Web services are introduced later. Finally, a comprehensive analysis of experiment data are presented. Evaluation results demonstrate that RBDB can improve users' efficiency significantly and decrease the invoking times of external services substantially.	business process execution language;debugger;debugging;hoc (programming language);web service	Liang Bao;Sheng Chen;Xiyang Liu;Shengming Hu;Ping Chen	2008	2008 IEEE International Conference on Web Services	10.1109/ICWS.2008.48	web service;real-time computing;computer science;database;programming language;law	Robotics	-54.93873239710188	38.32361421456633	186806
3a4aef593baad6797d70e7d1eee1a9bc49c994d1	dpjf - design pattern detection with high accuracy	libraries;observers java maintenance engineering terminology bridges libraries receivers;automated design pattern detection;automated design;behavioural analysis technique;systems re engineering object oriented programming programming environments reverse engineering software quality;programming environments;structural analysis technique;detection quality;program comprehension;bridges;maintenance engineering;development practice;object oriented programming;observers;receivers;reengineering task;pattern detection;software quality improvement automated design pattern detection reengineering task detection quality ide development practice dpjf structural analysis technique behavioural analysis technique program comprehension software quality assesment;ide;design pattern;terminology;software quality assesment;software quality improvement;software quality;dpjf;reverse engineering;java;systems re engineering	Automated design pattern detection (DPD) is a challenging reengineering task. The detection quality (precision and recall) of DPD tools has so far been insufficient to make DPD integral part of current IDEs and development practices. In this paper, we present a novel approach and a related tool, DPJF. For all implemented pattern detectors and all projects used for evaluation, DPJF achieves 100% precision and the best recall of all evaluated tools. Still, its analysis speed competes with the fastest existing tools. The high detection quality is achieved by a well-balanced combination of structural and behavioural analysis techniques whereas the good performance is achieved by emprically validated simplifications of the individual techniques. Our results lay the basis for routine application of DPD in program comprehension and let DPJF pioneer novel uses of DPD for software quality assesment and improvement.	code refactoring;fastest;floor and ceiling functions;pattern recognition;precision and recall;program analysis;program comprehension;sensor;software design pattern;software quality	Alexander Binun;Günter Kniesel	2012	2012 16th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2012.82	maintenance engineering;reliability engineering;computer science;systems engineering;engineering;operating system;software engineering;design pattern;programming language;object-oriented programming;terminology;java;software quality;reverse engineering	SE	-57.59761718596233	35.78092305169052	187039
ec406ca2663f7000a436c8066cb14abebb8842c3	measuring multi-language software evolution: a case study	similarity metric;information content;kolmogorov complexity;measurement of evolution;software evolution;software development;complearn;multi language software;compression;information theory;scripting language	Characterising and measuring software developed in multiple languages is a problem for practitioners. Rather than a language-based approach, we avoid difficulties related to syntax, semantics and language paradigms by looking directly at relative shared information content to perform these tasks. Measuring, for each language, the relative number of bits of shared binary information between artefacts representative of consecutive releases of the project using a common tool permits the direct comparison of evolution results for the multiple languages. This paper presents a case study of the program suite called git, written in C, perl and Bourne shell. The study uses this method to show that, for git, code in scripting languages does not prototype later C, Bourne shell and C code are written together and that the languages' code contributions occur concurrently.	bourne shell;perl;prototype;scripting language;self-information;software evolution	Tom Arbuckle	2011		10.1145/2024445.2024461	natural language processing;self-information;information theory;computer science;domain-specific language;engineering;software evolution;theoretical computer science;software development;software engineering;scripting language;programming language;compression	PL	-53.788154224709345	35.622659961070674	187407
b93770c685b15dbfff05ed37ff7e816a31485180	recommending auto-completions for software modeling activities		Abstract. Auto-completion of textual inputs benefits software developers using IDEs and editors. However, graphical modeling tools used to design software do not provide this functionality. The challenges of recommending auto-completions for graphical modeling activities are largely unexplored. Recommending auto-completions during modeling requires detecting meaningful partly completed activities, tolerating variance in user actions, and determining the most relevant activity that a user wants to perform. This paper proposes an approach that works in the background while a developer is creating or evolving a model and handles all these challenges. Editing operations are analyzed and matched to a predefined but extensible catalog of common modeling activities for structural UML models. In this paper we solely focus on determining recommendations rather than automatically completing an activity. We demonstrated the quality of recommendations generated by our approach in a controlled experiment with 16 students evolving models. We recommended 88% of the activities that a user wanted to perform within a short list of ten recommendations.	graphical user interface;integrated development environment;sensor;software developer;unified modeling language;user (computing)	Tobias Kuschke;Patrick Mäder;Patrick Rempel	2013		10.1007/978-3-642-41533-3_11	computer science;systems engineering;data mining;database;world wide web	SE	-56.143246578721964	33.69394185174483	187634
72e6158a46779cc683afc28d62b8d9b22da575a2	the device software engineer's best friend	developpement logiciel;debugging;puesta a punto programa;debugging embedded systems hardware trace;systeme embarque;embedded systems program debugging;hardware software tools software testing system testing digital audio players embedded computing software debugging embedded software computer errors computer bugs;software engineering;embedded system;debogage;embedded systems;desarrollo logicial;software development;genie logiciel;program debugging;program bugs embedded devices hardware trace;ingenieria informatica;embedded device;hardware trace	As embedded devices become increasingly complex, time and quality pressures have become one of the greatest challenges facing device software organizations. Although no solution will banish all of a software engineer's problems, hard ware trace offers a valuable tool that can help solve many of them. Whether the challenges consist of intermittent bugs or serious performance problems, hardware trace offers unique visibility into the workings of software that lets developers deliver better products in less time and with less risk.	embedded system;software bug;software engineer;warez	Michael Lindahl	2006	Computer	10.1109/MC.2006.179	embedded system;real-time computing;computer science;software development;operating system;software engineering;programming language;debugging	SE	-49.29929611034813	32.53530932016789	187636
90de04a09b6ea2fd26e1e92da31242f967197668	automated refactoring of atl model transformations: a search-based approach	refactoring;search based software engineering;model transformations	Model transformation programs evolve through a process of continuous change. However, this process may weaken the design of the transformation programs and make it unnecessarily complex, leading to increased fault-proneness. Refactoring improves the software design while preserving overall functionality and behavior. However, very few studies addressed the problem of refactoring model transformation programs. These existing studies provided an entirely manual or semi-automated refactoring support to transformation languages such as ATL. In this paper, we propose a fully-automated search-based approach to refactor model transformations based on a multi-objective algorithm that recommends the best refactoring sequence (e.g. extract rule, merge rules, etc.) optimizing a set of ATL-based quality metrics (e.g. number of rules, coupling, etc.). To validate our approach, we apply it to a comprehensive dataset of model transformations. The statistical analysis of our experiments over 30 runs shows that our automated approach recommended useful refactorings based on benchmark of ATL programs and compared to random search, mono-objective search formulation and a semi-automated refactoring approach not based heuristic search.	atlas transformation language;algorithm;benchmark (computing);code refactoring;experiment;heuristic;model transformation;random search;semiconductor industry;software design;software developer;software quality assurance	Bader Alkhazi;Terry Ruas;Marouane Kessentini;Manuel Wimmer;William I. Grosky	2016		10.1145/2976767.2976782	reliability engineering;search-based software engineering;computer science;systems engineering;software engineering;data mining;programming language;code refactoring	SE	-58.61428313504634	34.42033257643606	188035
1f7432f3b8cf8196dbbf46086deb65aa1bb43997	automatic identification of bug-introducing changes	graph theory;software bugs;false positives;annotation graphs;automatic bug identification;nonsemantic source code changes;false negative;program debugging configuration management graph theory;computer bugs open source software software debugging history project management inspection risk analysis fixtures computer industry electrical equipment industry;outlier fix automatic bug identification bug fixing software bugs false positives false negatives annotation graphs nonsemantic source code changes;bug fixing;outlier fix;source code;program debugging;false positive;configuration management;false negatives	Bug-fixes are widely used for predicting bugs or finding risky parts of software. However, a bug-fix does not contain information about the change that initially introduced a bug. Such bug-introducing changes can help identify important properties of software bugs such as correlated factors or causalities. For example, they reveal which developers or what kinds of source code changes introduce more bugs. In contrast to bug-fixes that are relatively easy to obtain, the extraction of bug-introducing changes is challenging. In this paper, we present algorithms to automatically and accurately identify bug-introducing changes. We remove false positives and false negatives by using annotation graphs, by ignoring non-semantic source code changes, and outlier fixes. Additionally, we validated that the fixes we used are true fixes by a manual inspection. Altogether, our algorithms can remove about 38%~51% of false positives and 14%~15% of false negatives compared to the previous algorithm. Finally, we show applications of bug-introducing changes that demonstrate their value for research	algorithm;automatic identification and data capture;software bug	Sunghun Kim;Thomas Zimmermann;Kai Pan;E. James Whitehead	2006	21st IEEE/ACM International Conference on Automated Software Engineering (ASE'06)	10.1109/ASE.2006.23	software bug;type i and type ii errors;computer science;graph theory;theoretical computer science;false positive paradox;data mining;configuration management;world wide web;source code	SE	-61.49437621163753	38.51406424731494	188274
3a0b5cae1c70f4eea85d6b45ecaf91e1147c421a	search-based fault localization	fault localization;optimal solution;selected works;software systems;software fault tolerance;spectrum;simulated annealing;software engineering;optimization problem;software fault tolerance genetic algorithms program debugging search problems simulated annealing;bepress;genetic algorithm;genetic algorithms;search problems;program debugging;computer bugs training biological cells genetic algorithms accuracy search problems simulated annealing;simulated annealing search based fault localization spectrum based fault localization measures program bugs optimization problem search based approach genetic algorithm	Many spectrum-based fault localization measures have been proposed in the literature. However, no single fault localization measure completely outperforms others: a measure which is more accurate in localizing some bugs in some programs is less accurate in localizing other bugs in other programs. This paper proposes to compose existing spectrum-based fault localization measures into an improved measure. We model the composition of various measures as an optimization problem and present a search-based approach to explore the space of many possible compositions and output a heuristically near optimal composite measure. We employ two search-based strategies including genetic algorithm and simulated annealing to look for optimal solutions and compare the effectiveness of the resulting composite measures on benchmark software systems. Compared to individual spectrum-based fault localization techniques, our composite measures perform statistically significantly better.	benchmark (computing);genetic algorithm;heuristic;mathematical optimization;optimization problem;simulated annealing;software bug;software system	Shaowei Wang;David Lo;Lingxiao Jiang;Lucia;Hoong Chuin Lau	2011	2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)	10.1109/ASE.2011.6100124	mathematical optimization;genetic algorithm;computer science;theoretical computer science;software engineering;machine learning	SE	-59.755082149429846	35.09374206970879	188355
6b319c1b3d12e123a83ae124cd49e29b36ada1f8	the crss metric for package design quality	software systems;design quality;reachable set	Package designis concerned with the determining the best way to partition the classes in a system into subsystems. A poor packag e design can adversely affect the quality of a software system. In thi s paper we present a new metric, Class Reachability Set Size (CRSS) , the distribution of which can be used to determine if the relationships be twe n the classes in a system preclude it from a good package design. We compute CRSS distributions for programs in a software corpus in orde r to show that some real programs are precluded in this way. Also we sho w how the CRSS metric can be used to identify candidates for refact oring so that the potential package structure of a system can be impro ved.	code refactoring;list of system quality attributes;open-source software;reachability;software system;software testability	Hayden Melton;Ewan D. Tempero	2007			reliability engineering;real-time computing;computer science;theoretical computer science	SE	-61.32366079179688	34.95211161036305	188424
9d76d1ab0d39ceef98eccd8d57f154a6f139986b	spectrum-based fault localization in regression testing	fault localization;software;program under test;software testing;regression testing;program under test modified software revalidation conventional regression testing approach modified program version pre existing test suite spectrum based fault localization strategy;test oracle regression testing fault localization program spectra;modified software revalidation;program spectra;software fault tolerance;maintenance engineering;spectrum;spectrum based fault localization strategy;program testing;conventional regression testing approach;writing;regression analysis;software software reliability maintenance engineering writing bioinformatics software testing;software fault tolerance program testing regression analysis;pre existing test suite;software reliability;modified program version;program development;test oracle;bioinformatics	In maintenance, regression testing is used to revalidate modified software which is also an expensive and frequently executed process. In conventional regression testing approach, developers generally re-run all test suite or selectively run a sub-set of existing test cases on the modified version of program. After executing the test cases over program, developer may reveal the regression faults due to changes in code and use these non obsolete test cases from pre-existing test suite to explore and eradicate regression faults. This paper addresses the fundamental limitations of conventional regression testing approach. To overcome these limitations, this paper presents a spectrum-based fault localization strategy by which the stated limitations are resolved in effective manner. Spectrum-based fault localization strategy utilizes various program spectra to identify the behavioral differences between old and new version of the program under test. This comparison is also useful in pinpointing the cause of failures or errors and presence of difference in program spectra may indicate those test cases for which the construction of expected output or oracle or specification is not needed. It is very expensive to compute and verify expected output and some times it is impossible to compute expected outputs for non-trivial programs. This approach is used in regression testing to identify behavioral differences between old and new version of a program.	regression testing;test case;test suite	Shailesh Tiwari;K. K. Mishra;Anoj Kumar;Arun Kumar Misra	2011	2011 Eighth International Conference on Information Technology: New Generations	10.1109/ITNG.2011.40	maintenance engineering;oracle;spectrum;regression testing;real-time computing;model-based testing;computer science;operating system;software engineering;test suite;software testing;writing;test management approach;algorithm;software quality;software fault tolerance;regression analysis	SE	-60.970353285675955	35.82736399401163	188555
d5bda9161deac69e4fed8da63971f773c60f3caf	codex: source code plagiarism detection based on abstract syntax tree		CodEX is a source code search engine that allows users to search a repository of source code snippets using source code snippets as the query also. A potential use for such a search engine is to help educators identify cases of plagiarism in students’ programming assignments. This paper evaluates CodEX in this context. Abstract Syntax Trees (ASTs) are used to represent source code files on an abstract level. This, combined with node hashing and similarity calculations, allows users to search for source code snippets that match suspected plagiarism cases. A number of commonly-employed techniques to avoid plagiarism detection are identified, and the CodEX system is evaluated for its ability to detect plagiarism cases even when these techniques are employed. Evaluation results are promising, with 95% of test cases being identified	abstract syntax tree;plagiarism;trees (plant)	M. Zheng;Xingyu Pan;D. Lillis	2018				SE	-59.75125480587671	38.95924588174916	188975
37d11b005125bacba9939a34d35b22ab9c987aa0	fuzzing the out-of-memory killer on embedded linux: an adaptive random approach	swinburne;fuzzing;embedded linux;adaptive random testing;out of memory killer;black box testing	Fuzzing is an automated black-box testing technique conducted with a destructive aim to crash (that is, to reveal failures in) the software under test. In this paper, we propose an adaptive random approach to fuzz the Out-Of-Memory (OOM) Killer on an embedded Linux distribution. The fuzzing process has revealed OOM Killer failures that cause the Linux kernel to remain in the OOM condition and become non-responsive. We have also found that the OOM Killer failures are more likely to occur when the Linux kernel has a higher over-commitment of memory requests. Finally, we have shown that the proposed adaptive random approach for fuzzing can reveal an OOM Killer failure with significantly fewer test inputs compared to the pure random approach.	black box;black-box testing;embedded system;killer application;linux on embedded systems;out of memory	Kwan Yong Sim;Fei-Ching Kuo;Robert G. Merkel	2011		10.1145/1982185.1982268	embedded system;fuzz testing;white-box testing;computer science;operating system;programming language;world wide web;computer security	Security	-61.643297737869275	37.55629513038217	189214
26e3b202f56346d939705eaf49b4aff4f9a1d804	revisiting the performance of automated approaches for the retrieval of duplicate reports in issue tracking systems that perform just-in-time duplicate retrieval	duplicate issue report retrieval;duplicate bug reports;just-in-time duplicate issue report retrieval	Issue tracking systems (ITSs) allow software end-users and developers to file issue reports and change requests. Reports are frequently duplicately filed for the same software issue. The retrieval of these duplicate issue reports is a tedious manual task. Prior research proposed several automated approaches for the retrieval of duplicate issue reports. Recent versions of ITSs added a feature that does basic retrieval of duplicate issue reports at the filing time of an issue report in an effort to avoid the filing of duplicates as early as possible. This paper investigates the impact of this just-in-time duplicate retrieval on the duplicate reports that end up in the ITS of an open source project. In particular, we study the differences between duplicate reports for open source projects before and after the activation of this new feature. We show how the experimental results of prior research would vary given the new data after the activation of the just-in-time duplicate retrieval feature. We study duplicate issue reports from the Mozilla-Firefox, Mozilla-Core and Eclipse-Platform projects. In addition, we compare the performance of the state of the art of the automated retrieval of duplicate reports using two popular approaches (i.e., BM25F and REP). We find that duplicate issue reports after the activation of the just-in-time duplicate retrieval feature are less textually similar, have a greater identification delay and require more discussion to be retrieved as duplicate reports than duplicates before the activation of the feature. Prior work showed that REP outperforms BM25F in terms of Recall rate and Mean average precision. We observe that the performance gap between BM25F and REP becomes even larger after the activation of the just-in-time duplicate retrieval feature. We recommend that future studies focus on duplicates that were reported after the activation of the just-in-time duplicate retrieval feature as these duplicates are more representative of future incoming issue reports and therefore, give a better representation of the future performance of proposed approaches.	eclipse;firefox;futures studies;information retrieval;issue tracking system;just-in-time compilation;open-source software;precision and recall;sensitivity and specificity	Mohamed Sami Rakha;Cor-Paul Bezemer;Ahmed E. Hassan	2017	Empirical Software Engineering	10.1007/s10664-017-9590-5	data mining;tracking system;software;computer science;performance gap	Web+IR	-62.37530393523861	38.435390035121266	189265
34e4ae708510bffd34ce368d4444a79f8a1a5776	"""corrections to """"the effectiveness of control structure diagrams in source code comprehension activities"""""""	senior members;software engineering;systems engineering and theory;visualization;control structure;back;source code;computer science;java	ÐRecently, the first two in a series of planned comprehension experiments were performed to measure the effect of the control structure diagram (CSD) on program comprehensibility. Upper-and lower-division computer science and software engineering students were asked to respond to questions regarding the structure and execution of one source code module of a public domain graphics library. The time taken for each response and the correctness of each response was recorded. Statistical analysis of the data collected from these two experiments revealed that the CSD was highly significant in enhancing the subjects' performance in this program comprehension task. The results of these initial experiments promise to shed light on fundamental questions regarding the effect of software visualizations on program comprehensibility.	cambridge structural database;computer science;control flow;correctness (computer science);diagram;experiment;graphics library;list comprehension;program comprehension;software engineering;unified modeling language	T. Dean Hendrix;James H. Cross;Saeed Maghsoodloo	2002	IEEE Trans. Software Eng.	10.1109/TSE.2002.1010064	kpi-driven code analysis;visualization;computer science;theoretical computer science;software engineering;programming language;control flow;java;static program analysis;source code	SE	-54.610278796840355	35.973690890490644	189404
68e42898d72e3408b6d2b5765e31152e15fde9fd	fixing recurring crash bugs via analyzing q&a sites (t)	software;web pages;search engines;source code software program debugging query processing;会议论文;computer bugs web pages search engines registers context software;registers;computer bugs;bug fixing approaches recurring crash bugs q a sites software systems human written templates query extraction crash traces edit scripts source code real world crash bugs;context	Recurring bugs are common in software systems, especially in client programs that depend on the same framework. Existing research uses human-written templates, and is limited to certain types of bugs. In this paper, we propose a fully automatic approach to fixing recurring crash bugs via analyzing Q&A sites. By extracting queries from crash traces and retrieving a list of Q&A pages, we analyze the pages and generate edit scripts. Then we apply these scripts to target source code and filter out the incorrect patches. The empirical results show that our approach is accurate in fixing real-world crash bugs, and can complement existing bug-fixing approaches.	client (computing);complement (complexity);experiment;scalability;software bug;software system;tracing (software);web search engine	Qing Gao;Hansheng Zhang;Jie Wang;Yingfei Xiong;Lu Zhang;Hong Mei	2015	2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1109/ASE.2015.81	software bug;bebugging;computer science;operating system;web page;database;processor register;programming language;debugging;world wide web;search engine	SE	-59.31239794170925	39.446418021283854	189465
f742a2888d96f9e2d2d53be3855ad1ff380f9c73	on the relationships of faults for boolean specification based testing	verification;formal specification;specification based testing;swinburne;program verification;boolean expression;boolean algebra;fault detection software testing system testing humans hardware information technology australia programming manufacturing computer science;program testing;fault class analysis;fault detection;program verification program testing boolean algebra formal specification;verification boolean specification based testing fault relationships test case generation fault based methods fault detection boolean expression fault detecting test cases fault class analysis	Various methods of generating test cases based on Boolean specijications have previously been proposed. These methods are fault-based in the sense that test cases are aimed at detecting particular types of faults. Empirical results suggest that these methods are good at detecting particular types of faults. However, there is no information on the ability of these test cases in detecting other types of faults. This paper summarizes the relationships of faults in a Boolean expression in the form of a hieramhy. A test case that detects the faults at the lower level of the hierarchy will always detect the faults at the upper level of the hierarchy. The hierarchy helps us to better understand the relationships of faults ina Boolean expression, and hence to select fault-detecting test cases in a more systematic and eflcient mannel:	boolean expression;boolean satisfiability problem;sensor;test case	Man Fai Lau;Yuen-Tak Yu	2001		10.1109/ASWEC.2001.948494	reliability engineering;boolean algebra;boolean circuit;and-inverter graph;verification;fault coverage;boolean expression;computer science;stuck-at fault;theoretical computer science;formal specification;programming language;fault detection and isolation;algorithm	SE	-59.74634070263869	33.86985600858545	189750
07e353ff6670b35814e0bdb073c0ab9ca002fb9a	integrating information sources for visualizing java programs	information sources;data integrity;java programming;software maintenance;data;system documentation;visualization java documentation software maintenance programming profession software tools navigation wheels identity based encryption data mining;presentation;integration;public domain;visualization;software exploration environment information source integration java program visualization source code java program documentation public domain tools data integration presentation integration;software maintenance program visualisation java system documentation;control;source code;tools;program visualisation;java	This paper describes the integration of information sources to support the exploration of source code and documentation of Java programs. There are many public domain tools that are available for extracting information and documentation from Java programs. We describe how data integration and presentation integration were used to enable the visualization of this information within a software exploration environment.	code refactoring;component-based software engineering;documentation;interoperability;java platform, enterprise edition;parsing;prototype;reasonable server faces;reverse engineering;serialization;software visualization;victoria (3d figure);xml metadata interchange	Jeff Michaud;Margaret-Anne D. Storey;Hausi A. Müller	2001		10.1109/ICSM.2001.972738	public domain;visualization;computer science;information integration;software engineering;data integrity;database;real time java;programming language;software maintenance;java;world wide web;scientific control;data;java annotation;source code	SE	-54.02154594817975	34.62167798770512	190096
e46d45ed4902715811c301b3b3c233a2be59e9d4	using mutation to enhance gui testing coverage	social network services;mutation testing;software testing;gui modeling;formal specification;resource allocation;automatic test pattern generation;test case execution and verification;resource allocation automatic test pattern generation formal specification formal verification graphical user interfaces program debugging program testing;formal verification;graphical user interfaces;test case generation;program testing;valid event sequence gui testing coverage enhancement mutation testing system bug detection capability code areas software code software testing automatic technique mutant test cases parameter specification gui model robust model invalid event sequence automatic execution verification technique automatic software test coverage improvement;gui testing;xml;random test case generation graphical user interfaces software testing automation social network services xml software algorithms gui modeling mutation testing test case generation test case execution and verification;software algorithms;program debugging;graphical user interfaces software testing automation social network services xml software algorithms;random test case generation;automation	Mutation testing improves a system's bug-detection capability. It also helps improve coverage by exposing software or code areas that other types of testing might not expose. However, the value of mutation testing is often bypassed because it consumes extra resources from already limited ones. The author presents an automatic technique to generate valid and mutant test cases. In traditional mutation testing, one or more parameters in the specification or the code are changed, and the technique finds the test cases that can detect those mutations. In the author's approach, the test cases generated by a GUI model are mutated and the mutants are then applied the model to test its capability to kill the mutant test cases by rejecting them. A robust model is expected to differentiate between a valid and invalid event sequence. The author also describes an automatic execution and verification technique to evaluate the test cases rejected by the GUI model and to calculate coverage based on their number relative to the total number of test cases. Experimental results indicate the value of this mutation process and its potential for improving software test coverage automatically.	fault coverage;graphical user interface testing;mutation (genetic algorithm);mutation testing;software testing;test case	Izzat Alsmadi	2013	IEEE Software	10.1109/MS.2012.22	reliability engineering;test data generation;xml;white-box testing;formal verification;resource allocation;computer science;automatic test pattern generation;automation;formal specification;graphical user interface;database;mutation testing;software testing;programming language;test case;graphical user interface testing	SE	-59.15960194425611	35.942764619729445	190114
2015e4860980ef783763084a5f71792a916bd555	detecting double faults on term and literal in boolean expressions	double fault class;detection condition;previous study;boolean expressions;certain prescribed fault;boolean expression;double fault;single fault detection;fault detection condition;test case selection strategy;single fault;fault detection;software fault tolerance;software quality	Fault-based testing aims at selecting test cases to guarantee the detection of certain prescribed faults in programs. The detection conditions of single faults have been studied and used in areas like developing test case selection strategies, establishing relationships between faults and investigating the fault coupling effect. It is common, however, for programmers to commit more than one fault. Our previous studies on the detection conditions of faults in Boolean expressions show that (1) some test case selection strategies developed for the detection of single faults can also detect all double faults related to terms, but (2) these strategies cannot guarantee to detect all double faults related to literals. This paper supplements our previous studies and completes our series of analysis of the detection condition of all double fault classes in Boolean expressions. Here we consider the fault detection conditions of combinations of two single faults, in which one is related to term and the other is related to literal. We find that all such faulty expressions, except two, can be detected by some test case selection strategies for single fault detection. Moreover, the two exception faulty expressions can be detected by existing strategies when used together with a supplementary strategy which we earlier developed to detect double literal faults.	boolean expression;double fault;fault detection and isolation;literal (mathematical logic);programmer;test case	Man Fai Lau;Ying Liu;Yuen-Tak Yu	2007	Seventh International Conference on Quality Software (QSIC 2007)	10.1109/QSIC.2007.23	reliability engineering;real-time computing;fault coverage;boolean expression;fault indicator;computer science;engineering;stuck-at fault;fault model;programming language;general protection fault;fault detection and isolation;algorithm;software quality;software fault tolerance	SE	-60.57840451689888	35.115463577743114	190714
b76d5b4bc378ae475b8a4d89c1b0bcfe8dde8d67	investigation on performance testing and evaluation of prewebn: a java technique for implementing web application	java;web sites;software metrics;software performance evaluation;statistical testing;transaction processing;java technique;mercury loadrunner;microsoft .net technology;prewebn;web application;performance testing;statistical testing;stress testing;transaction summary	Performance testing of web application is essential from the perspective of users as well as developers, since it directly reflects the behaviour of the application. As such the authors have developed a prototype research web application based on Java technique to study the performance and to evaluate the technique used for developing the web application. The application is called as PReWebN (prototype research web application in netbeans platform). Load and stress testing have been carried out on PReWebN using Mercury LoadRunner to study the performance, stability, scalability, reliability, efficiency and cost effectiveness of the technique. The performance depends on metrics such as hits/s, response time, throughput, errors/s and transaction summary. These metrics for the application are tested with different stress levels. The statistical testing on the recorded data has been carried out to study the stability and quality of the application. The present study reveals that the web application developed with Java technique is more stable, reliable, scalable and cost effective than its other counterpart such as Microsoft .NET technology. The authors present the architecture, testing procedure, results of performance testing as well as the results of statistical testing on recorded data of PReWebN.	java;software performance testing;web application	Mitashree Kalita;Sanjoy Khanikar;Tulshi Bezboruah	2011	IET Software	10.1049/iet-sen.2011.0030		SE	-54.90132660776809	38.58495438116007	190864
fb69ca4ce65a5c796f6247559205322e9796f2fc	credal: towards locating a memory corruption vulnerability with your core dump	vulnerability analysis;memory corruption;core dump	After a program has crashed and terminated abnormally, it typically leaves behind a snapshot of its crashing state in the form of a core dump. While a core dump carries a large amount of information, which has long been used for software debugging, it barely serves as informative debugging aids in locating software faults, particularly memory corruption vulnerabilities. A memory corruption vulnerability is a special type of software faults that an attacker can exploit to manipulate the content at a certain memory. As such, a core dump may contain a certain amount of corrupted data, which increases the difficulty in identifying useful debugging information (e.g. , a crash point and stack traces). Without a proper mechanism to deal with this problem, a core dump can be practically useless for software failure diagnosis. In this work, we develop CREDAL, an automatic tool that employs the source code of a crashing program to enhance core dump analysis and turns a core dump to an informative aid in tracking down memory corruption vulnerabilities. Specifically, CREDAL systematically analyzes a core dump potentially corrupted and identifies the crash point and stack frames. For a core dump carrying corrupted data, it goes beyond the crash point and stack trace. In particular, CREDAL further pinpoints the variables holding corrupted data using the source code of the crashing program along with the stack frames. To assist software developers (or security analysts) in tracking down a memory corruption vulnerability, CREDAL also performs analysis and highlights the code fragments corresponding to data corruption.  To demonstrate the utility of CREDAL, we use it to analyze 80 crashes corresponding to 73 memory corruption vulnerabilities archived in Offensive Security Exploit Database. We show that, CREDAL can accurately pinpoint the crash point and (fully or partially) restore a stack trace even though a crashing program stack carries corrupted data. In addition, we demonstrate CREDAL can potentially reduce the manual effort of finding the code fragment that is likely to contain memory corruption vulnerabilities.	archive;core dump;debugger;debugging;exploit (computer security);information;memory corruption;snapshot (computer storage);software bug;software developer;stack trace;tracing (software)	Jun Xu;Dongliang Mu;Ping Chen;Xinyu Xing;Pei Wang;Peng Liu	2016		10.1145/2976749.2978340	real-time computing;core dump;computer science;vulnerability assessment;distributed computing;computer security;memory corruption	Security	-60.727862518783105	42.309479088067995	191356
2a4f6812ed2828f8aeb1dd2b4b1bc6c2a3e9b640	workshop preview of the 2nd international workshop on software for parallel systems (seps 2015)	manycore;parallel programming;software engineering;multicore;parallel systems	The second international workshop on Software Engineering for Parallel Systems (SEPS) will be held in Pittsburgh, PA, USA on October 27, 2015 and co-located with the ACM SIGPLAN conference on Systems, Programming, Languages and Applications: Software for Humanity (SPLASH 2015). The purpose of this workshop is to provide a stable forum for researchers and practi-tioners dealing with compelling challenges of the software development life cycle on modern parallel platforms. The increased complexity of parallel applications on modern parallel platforms (e.g. multicore, manycore, distributed or hybrid) requires more insight into development processes, and necessitates the use of advanced methods and techniques supporting developers in creating parallel applications or parallelizing and reengineering sequential legacy applications. We aim to advance the state of the art in different phases of parallel software development, covering software engineering aspects such as requirements engineering and software specification; design and implementation; program analysis, profiling and tuning; testing and debugging.	code refactoring;debugging;formal specification;manycore processor;multi-core processor;parallel computing;performance tuning;profiling (computer programming);program analysis;requirements engineering;software development process;software engineering;software testing	Ali Jannesari;Siegfried Benkner;Xinghui Zhao;Ehsan Atoofian;Yukionri Sato	2015		10.1145/2814189.2833200	multi-core processor;computer architecture;parallel computing;search-based software engineering;computer science;software design;social software engineering;software framework;component-based software engineering;software development;operating system;software engineering;software construction;systems development life cycle;programming language;software analytics;resource-oriented architecture;software deployment;software development process;software requirements;software system	SE	-51.49770233294657	32.386159406116505	191793
f325a71f130635d23686edcd7a1c9ec8b1bc1a10	effective test generation for combinatorial decision coverage	symbolic execution;test case generation;combinatorial decision coverage;fault detection	Complex software systems present significant challenges to existing software testing techniques. Simply applying exhaustive testing will lead to the execution of a prohibitively large number of test cases. Furthermore, many testing techniques today provide neither promising coverage achievement nor reliable fault detection strength. In this paper, we propose a technique, which represents an innovative synthesis of combinatorial testing and symbolic execution, to generate test cases based on a novel coverage criterion, namely combinatorial decision coverage (CDC). Strength t (or t-way) CDC requires each t-tuple of decision outcomes to be executed by at least one test case. Given a program, our CDC-based technique first uses a revised version of a symbolic executor, S2E, to collect all program decisions with symbolic variables as well as their corresponding constraints and then applies a combinatorial test generation tool, ACTS, to generate t-way combinations for the outcomes of these decisions. A test case can be generated with respect to each combination that represents a single path-condition of the program. Case studies were conducted on three versions of photo editing applications. Our results indicate that a test set generated using the proposed technique has higher statement, decision, and all use coverage as well as better fault detection strength than a test set of the same size generated by random testing and genetic algorithm-based test generation.	fault detection and isolation;genetic algorithm;image editing;random testing;software system;software testing;symbolic execution;test case;test set	Ruizhi Gao;Linghuan Hu;W. Eric Wong;Han-Lin Lu;Shih-Kun Huang	2016	2016 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)	10.1109/QRS-C.2016.11	modified condition/decision coverage;test data generation;real-time computing;all-pairs testing;computer science;automatic test pattern generation;test suite;data mining;code coverage;test case;test management approach;algorithm	SE	-60.394664049676955	34.88733443653594	191804
17f749f9a992e1918b2f7ab025e98ae6365b7cc9	stateful testing: finding more errors in code and contracts	automated testing;random processes program testing;random testing;dynamic analysis stateful testing automated random testing;software engineering;lines of code;program testing;automation random testing dynamic analysis;random processes;testing contracts databases search problems arrays data mining;data structure;dynamic analysis;automation	Automated random testing has shown to be an effective approach to finding faults but still faces a major unsolved issue: how to generate test inputs diverse enough to find many faults and find them quickly. Stateful testing, the automated testing technique introduced in this article, generates new test cases that improve an existing test suite. The generated test cases are designed to violate the dynamically inferred contracts (invariants) characterizing the existing test suite. As a consequence, they are in a good position to detect new faults, and also to improve the accuracy of the inferred contracts by discovering those that are unsound. Experiments on 13 data structure classes totalling over 28,000 lines of code demonstrate the effectiveness of stateful testing in improving over the results of long sessions of random testing: stateful testing found 68.4% new faults and improved the accuracy of automatically inferred contracts to over 99%, with just a 7% time overhead.	automated testing framework;data structure;experiment;overhead (computing);random testing;source lines of code;state (computer science);stateful firewall;test automation;test case;test suite	Yi Wei;Hannes Roth;Carlo A. Furia;Yu Pei;Alexander Horton;Michael Steindorfer;Martín Nordio;Bertrand Meyer	2011	2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)	10.1109/ASE.2011.6100094	non-regression testing;random testing;keyword-driven testing;reliability engineering;black-box testing;real-time computing;model-based testing;fuzz testing;orthogonal array testing;data structure;software performance testing;white-box testing;manual testing;integration testing;computer science;theoretical computer science;automation;functional testing;dynamic testing;dynamic program analysis;risk-based testing;smoke testing;software testing;programming language;source lines of code;system testing	SE	-59.98769144510054	36.35771249145396	191809
e53a100a95136b7621398a8efbd3b800649c1009	the effect of granularity level on software defect prediction	software;hierarchical structure;functional method level;software defect prediction;granularity level;static code attributes;component;training;biological system modeling;cost benefit analysis component defect prediciton naive bayes classifier static code attributes;program verification;source file level;data mining;naive bayes classifier;eclipse dataset;program testing;software development;pattern classification;program verification pattern classification program testing;cost effectiveness;predictive models;source code;defect prediciton;softlab dataset;bayes classifier;cost benefit analysis;static code attributes software defect prediction granularity level software development naive bayes classifier functional method level source file level softlab dataset eclipse dataset cost benefit analysis;data models;software testing software performance cost benefit analysis laboratories programming resource management data mining research and development management software development management engineering management	Application of defect predictors in software development helps the managers to allocate their resources such as time and effort more efficiently and cost effectively to test certain sections of the code. In this research, we have used Naïve Bayes Classifier (NBC) to construct our defect prediction framework. Our proposed framework uses the hierarchical structure information about the source code of the software product, to perform defect prediction at a functional method level and source file level. We have applied our model on SoftLAB and Eclipse datasets. We have measured the performance of our proposed model and applied cost benefit analysis. Our results reveal that source file level defect prediction improves the verification effort, while decreasing the defect prediction performance in all datasets.	eclipse;naive bayes classifier;software bug;software development;software sizing	Gül Çalikli;Ayse Tosun Misirli;Ayse Basar Bener;Melih Celik	2009	2009 24th International Symposium on Computer and Information Sciences	10.1109/ISCIS.2009.5291866	data modeling;bayes classifier;naive bayes classifier;cost-effectiveness analysis;computer science;cost–benefit analysis;software development;operating system;machine learning;data mining;component;database;predictive modelling;source code	SE	-62.5505446463565	35.377074134242946	192543
27ed2d865025d3aba146bcf83d9c88ecc3d9dc0e	capturing expert knowledge for automated configuration fault diagnosis	internet expert systems fault diagnosis fault tolerant computing;apache web server configuration expert knowledge automated configuration fault diagnosis software misconfiguration problem three phase framework machine learning configuration troubleshooting;machine learning web servers training accuracy labeling machine learning algorithms software;expert systems;fault tolerant computing;internet;machine learning;expert knowledge;capturing expert knowledge;configuration troubleshooting;capturing expert knowledge configuration troubleshooting fault diagnosis machine learning;fault diagnosis	The process of manually diagnosing a software misconfiguration problem is time consuming. Manually writing and updating rules to detect future problems is still the state of the practice. Consequently, there is a need for increased automation. In this paper, we propose a three-phase framework using machine learning techniques for automated configuration faults diagnosis. This system can also help in capturing expert knowledge of configuration troubleshooting. Our experiments on Apache web server configurations are generally encouraging and non-experts can use this system to diagnose misconfigurations effectively.	experiment;machine learning;server (computing);web server	Mengliao Wang;Xiaoyu Shi;Kenny Wong	2011	2011 IEEE 19th International Conference on Program Comprehension	10.1109/ICPC.2011.24	reliability engineering;legal expert system;the internet;computer science;systems engineering;data mining;expert system	SE	-62.212102512448965	40.696895749943515	192581
855f99bcbb891278c051877a76721f8696fe3dbe	toward a better understanding of tool usage: nier track	software;graph theory;history;paints;software systems;software engineering;browsers;switches browsers software software engineering paints programming history;tool usage;empirical validation;software tools;software tools graph theory software development management;aisema systems nier track software tools software system development automated in process software engineering measurement and analysis;switches;programming;software development management	"""Developers use tools to develop software systems and always alleged better tools are being produced and purchased. Still there have been only limited studies on how people really use tools; these studies have used limited data, and the interactions between tools have not been properly elaborated. The advent of the AISEMA (Automated In-Process Software Engineering Measurement and Analysis) systems [3] has enabled a more detailed collection of tools data. Our """"new idea"""" is to take advantage of such data to build a simple model based on an oriented graph that enables a good understanding on how tools are used individually and collectively. We have empirically validated the model analyzing an industrial team of 19 developers for a period of 10 months."""	interaction;orientation (graph theory);software engineering;software system	Alberto Sillitti;Giancarlo Succi;Jelena Vlasenko	2011	2011 33rd International Conference on Software Engineering (ICSE)	10.1145/1985793.1985917	programming;network switch;computer science;systems engineering;engineering;graph theory;software engineering;software system;computer engineering	SE	-55.75629358238757	34.24596513415495	192615
88eb395a5c8622d14ff0862d61b00f2e21e7b741	sicsoft (paper session)	system designer;information analyst;business information systems;systems approach	Development of methodologies and techniques for systematic testing of computer software is an active area of investigation. In this session, four speakers will discuss four approaches to systematic program testing. Leon Osterweil will discuss the role of allegations in static testing, Bob Hoffman will describe an interactive program testing system, and Lori Clarke will discuss symbolic execution techniques. Hanan Samet will describe an approach to validating the translations performed by a compiler. These four papers represent current trends in program testing, and illustrate the diversity of testing strategies being investigated.	compiler;edmund m. clarke;interactive computing;leon;software testing;static program analysis;symbolic execution	Leon J. Osterweil;Robert H. Hoffman;Lori A. Clarke;Hanan Samet;Richard E. Fairley	1976		10.1145/800191.805644	test strategy;simulation;computer science;software engineering;session-based testing	SE	-54.80084548742571	32.50884728467334	192740
1ebd6053885eadb95c64cf4d3af4702091cff81e	automatic test data generation for data flow testing using a genetic algorithm	software testing;automatic testing;test data generation;random testing;general techniques;satisfiability;automatic generation;genetic algorithm;data flow	One of the major difficulties in software testing is the automatic generation of test data that satisfy a given adequacy criterion. This paper presents an automatic test data generation technique that uses a genetic algorithm (GA), which is guided by the data flow dependencies in the program, to search for test data to cover its def-use associations. The GA conducts its search by constructing new test data from previously generated test data that are evaluated as effective test data. The approach can be used in test data generation for programs with/without loops and procedures. The proposed GA accepts as input an instrumented version of the program to be tested, the list of def-use associations to be covered, the number of input variables, and the domain and precision of each input variable. The algorithm produces a set of test cases, the set of def-use associations covered by each test case, and a list of uncovered defuse associations, if any. In the parent selection process, the GA uses one of two methods: the roulette wheel method or a proposed method, called the random selection method, according to the user choice. Finally, the paper presents the results of the experiments that have been carried out to evaluate the effectiveness of the proposed GA compared to the random testing technique, and to compare the proposed random selection method to the roulette wheel method.	crossover (genetic algorithm);dataflow architecture;experiment;genetic algorithm;mutation (genetic algorithm);random testing;selection (genetic algorithm);software testing;test case;test data generation	Moheb R. Girgis	2005	J. UCS	10.3217/jucs-011-06-0898	random testing;keyword-driven testing;data flow diagram;test data generation;genetic algorithm;test data;computer science;theoretical computer science;automatic test pattern generation;software engineering;test suite;data mining;software testing;test script;algorithm;satisfiability	SE	-59.99145901768345	35.23374760586478	192918
097798b1f239a4fed4e9bd9267702abbcae39461	bql: capturing and reusing debugging knowledge	databases;debugging;query language;search engine;search engines;computer bugs debugging databases semantics database languages search engines;semantics;debugging knowledge bug query language bql infrastructure open source project;bql;query languages;public domain software;reusing debugging knowledge bql;program debugging;reusing debugging knowledge;query languages program debugging public domain software;computer bugs;database languages;open source	When fixing a bug, a programmer tends to search for similar bugs that have been resolved in the past. A fix for a similar bug may help him fix his bug or at least understand his bug. We designed and implemented the Bug Query Language (BQL) and its accompanying tools to help users search for similar bugs to aid debugging. This paper demonstrates the main features of the BQL infrastructure. We populated BQL with bugs collected from open-source projects and show that BQL could have helped users to fix real-world bugs.	debugging;hypertext transfer protocol;open-source software;population;programmer;query language;software bug;tracing (software);upload	Zhongxian Gu;Earl T. Barr;Zhendong Su	2011	2011 33rd International Conference on Software Engineering (ICSE)	10.1145/1985793.1985975	computer science;database;semantics;programming language;world wide web;query language	SE	-59.52111205232618	39.61188369888801	193456
e3930b43b7c742a25e8b46774c8042b347109b63	metropoljs: visualizing and debugging large-scale javascript program structure with treemaps		As a result of the large scale and diverse composition of modern compiled JavaScript applications, comprehending overall program structure for debugging is challenging. In this paper we present our solution: MetropolJS. By using a Treemap-based visualization it is possible to get a high level view within limited screen real estate. Previous approaches to Treemaps lacked the fine detail and interactive features to be useful as a debugging tool. This paper introduces an optimized approach for visualizing complex program structure that enables new debugging techniques where the execution of programs can be displayed in real time from a bird's-eye view. The approach facilitates highlighting and visualizing method calls and distinctive code patterns on top of code segments without a high overhead for navigation. Using this approach enables fast analysis of previously difficult-to-comprehend code bases.		Joshua D. Scarsbrook;Ryan K. L. Ko;Bill Rogers;David Bainbridge	2018		10.1145/3196321.3196368	debugging;visualization;database;data mining;javascript;computer science	SE	-54.309465186231314	35.99041597365599	193571
a1acc10d602ae82b7f1aa80e7a9eb0bf2fad3c6b	interactive exploration of compacted visualizations for understanding behavior in complex software	diagram compression;reverse engineering;complex software systems;interaction;software visualization;software behavior;exploration	In this poster we present Chrono, a tool that creates sequence diagram based visualizations. Since the diagrams produced by traditional sequence diagramming tools become large and unmanageable when dealing with complex code bases, Chrono focuses on removing less relevant information, condensing diagram components, and allowing for interactive exploration.	list of concept- and mind-mapping software;sequence diagram	Elizabeth L. Murnane;Vineet Sinha	2008		10.1145/1449814.1449850	block diagram;sequence diagram;software visualization;interaction;simulation;system context diagram;exploration;human–computer interaction;computer science;software development;class diagram;software construction;component diagram;reverse engineering;software system	HCI	-54.19293458272076	35.16465693638328	193765
b7517fdd30de38f15a07b4848f52bad03e164312	mining stackoverflow for program repair		In recent years, automatic program repair has been a hot research topic in the software engineering community, and many approaches have been proposed. Although these approaches produce promising results, some researchers criticize that existing approaches are still limited in their repair capability, due to their limited repair templates. Indeed, it is quite difficult to design effective repair templates. An award-wining paper analyzes thousands of manual bug fixes, but summarizes only ten repair templates. Although more bugs are thus repaired, recent studies show such repair templates are still insufficient. We notice that programmers often refer to Stack Overflow, when they repair bugs. With years of accumulation, Stack Overflow has millions of posts that are potentially useful to repair many bugs. The observation motives our work towards mining repair templates from Stack Overflow. In this paper, we propose a novel approach, called SOFix, that extracts code samples from Stack Overflow, and mines repair patterns from extracted code samples. Based on our mined repair patterns, we derived 13 repair templates. We implemented these repair templates in SOFix, and conducted evaluations on the widely used benchmark, Defects4J. Our results show that SOFix repaired 23 bugs, which are more than existing approaches. After comparing repaired bugs and templates, we find that SOFix repaired more bugs, since it has more repair templates. In addition, our results also reveal the urgent need for better fault localization techniques.	benchmark (computing);mined;programmer;software bug;software engineering;stack overflow;tree accumulation	Xuliang Liu;Hao Zhong	2018	2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)	10.1109/SANER.2018.8330202	software bug;stack overflow;data mining;benchmark (computing);computer science	SE	-61.41461312025126	38.20131976708164	193889
26672c7be57c83126636158d68db00b60d8e2db8	dodom: leveraging dom invariants for web 2.0 application robustness testing	mice;web pages;application client side behavior;error detection web 2 0 dynamic invariants robustness testing;dodom;software fault tolerance;browsers web pages servers robustness testing mice;client server systems;error detection dodom dom invariant leveraging web 2 0 application robustness testing document object models user actions application client side behavior dom structure fault injection experiment;testing;object oriented programming;user actions;browsers;document object model;servers;software fault tolerance client server systems distributed object management internet object oriented programming program testing;internet;program testing;dom invariant leveraging;distributed object management;web 2 0;robustness;robustness testing;document object models;dom structure;fault injection experiment;error detection;false positive;fault injection;web 2 0 application robustness testing;dynamic invariants	Web 2.0 applications are increasing in popularity. However, they are also prone to errors because of their dynamic nature. This paper presents DoDOM, an automated system for testing the robustness of Web 2.0 applications based on their Document Object Models (DOMs). DoDOM repeatedly executes the application under a trace of recorded user actions and observes the client-side behavior of the application in terms of its DOM structure. Based on the observations, DoDOM extracts a set of invariants on the web application’s DOM structure. We show that invariants exist for real applications and can be learned within a reasonable number of executions. We further use fault-injection experiments to demonstrate the uses of the invariants in detecting errors in web applications. The invariants are found to provide high coverage in detecting errors that impact the DOM, with a low rate of false positives.	blender (software);client-side;document object model;experiment;fault injection;fault model;feedback;invariant (computer science);max august zorn;out-of-order execution;robustness testing;sensor;web 2.0;web application	Karthik Pattabiraman;Benjamin G. Zorn	2010	2010 IEEE 21st International Symposium on Software Reliability Engineering	10.1109/ISSRE.2010.17	robustness testing;document object model;the internet;error detection and correction;type i and type ii errors;computer science;operating system;web page;database;distributed computing;software testing;programming language;object-oriented programming;web 2.0;world wide web;software fault tolerance;server;robustness	SE	-58.9732039699877	39.80148425256965	193975
042744b2868b8d156b34c3b15f448cdf292bb5fe	how should we measure functional sameness from program source code? an exploratory study on java methods	functionally similar code;clone detection;method name similarity;structural similarity;vocabulary similarity	Program source code is one of the main targets of software engineering research. A wide variety of research has been conducted on source code, and many studies have leveraged structural, vocabulary, and method signature similarities to measure the functional sameness of source code. In this research, we conducted an empirical study to ascertain how we should use three similarities to measure functional sameness. We used two large datasets and measured the three similarities between all the method pairs in the datasets, each of which included approximately 15 million Java method pairs. The relationships between the three similarities were analyzed to determine how we should use each to detect functionally similar code. The results of our study revealed the following. (1) Method names are not always useful for detecting functionally similar code. Only if there are a small number of methods having a given name, the methods are likely to include functionally similar code. (2) Existing file-level, method-level, and block-level clone detection techniques often miss functionally similar code generated by copy-and-paste operations between different projects. (3) In the cases we use structural similarity for detecting functionally similar code, we obtained many false positives. However, we can avoid detecting most false positives by using a vocabulary similarity in addition to a structural one. (4) Using a vocabulary similarity to detect functionally similar code is not suitable for method pairs in the same file because such method pairs use many of the same program elements such as private methods or private fields.	cut, copy, and paste;duplicate code;java;sensor;software engineering;structural similarity;type signature;vocabulary	Yoshiki Higo;Shinji Kusumoto	2014		10.1145/2635868.2635886	computer science;bioinformatics;structural similarity;data mining;world wide web	SE	-59.98902986577499	39.752562533602486	194194
02e69740ad9eebdcacc15a9aba9f47d8981f246f	identifying code smells with multiple concern views	software;code smell identification;object oriented methods;complexity theory;god class;visualization color spirals couplings complexity theory software tutorials;color;program visualisation object oriented methods;anomaly detection;program comprehension;modular structure representation;concern package class method structure;code visualization tool;multiple views;divergent change smell code smell identification multiple concern view code visualization tool anomaly detection modular structure representation concern inheritance wise structure concern package class method structure concern dependency weight open source system concern driven view god class;program comprehension software visualization code smells concerns;multiple concern view;visualization;concern dependency weight;tutorials;concern inheritance wise structure;open source system;spirals;divergent change smell;concern driven view;source code;concerns;couplings;exploratory study;program visualisation;open source;software visualization;code smells	Code smells are anomalies often caused by the way concerns are realized in the source code. Their identification might depend on properties governing the structure of individual concerns and their inter-dependencies in the system implementation. Although code visualization tools are increasingly applied to support anomaly detection, they are mostly limited to represent modular structures, such as methods, classes and packages. This paper presents a multiple views approach that enriches four categories of code views with concern properties, namely: (i) concern’s package-class method structure, (ii) concern’s inheritance-wise structure, (iii)concern dependency, and (iv) concern dependency weight. An exploratory study was conducted to assess the extent to which visual views support code smell detection. Developers identified a set of well-known code smells on five versions of an open source system. Two important results came out of this study. First, the concern-driven views provided useful support to identify God Class and Divergent Change smells. Second, strategies for smell detection supported by the multiple concern views were uncovered.	anomaly detection;code smell;god object;method (computer programming);open-source software	Glauco de Figueiredo Carneiro;Marcos Silva;Leandra Mara;Eduardo Figueiredo;Cláudio Sant'Anna;Alessandro F. Garcia;Manoel G. Mendonça	2010	2010 Brazilian Symposium on Software Engineering	10.1109/SBES.2010.21	simulation;computer science;data mining;computer security;code smell	SE	-56.46711043550285	34.3059610897076	194391
8b1326cd9b6411cbf1ffc325e0c48afbd0d29f24	special feature an implementation guide to a proposed standard for floating-point arithmetic	document handling;software performance;floating point arithmetic software standards hardware software performance h infinity control microcomputers programming profession software algorithms document handling;programming profession;software algorithms;software standards;floating point arithmetic;h infinity control;microcomputers;hardware	This guide to an IEEE draft standard provides practical algorithms for floating-point arithmetic operations and suggests the hardware/software mix for handling exceptions.	algorithm	Jerome T. Coonen	1980	Computer	10.1109/MC.1980.1653344	verification and validation;arbitrary-precision arithmetic;software sizing;software performance testing;computer science;floating point;package development process;backporting;software design;theoretical computer science;software framework;component-based software engineering;software development;software design description;operating system;software engineering;software construction;microcomputer;programming language;resource-oriented architecture;software requirements;software system	Crypto	-49.584696491390375	33.255869462654395	194752
3805b7faead85a9b60663744cb8b509d49077767	automated testing framework for mobile applications based in user-interaction features and historical bug information	historical bug information and interest point detector and descriptor;program testing human computer interaction mobile computing program debugging;software androids humanoid robots mobile applications detectors computer bugs graphical user interfaces;automated software testing;mobile applications;historical bug information and interest point detector and descriptor automated software testing mobile applications user interaction features;user interaction features;captured images mobile applications user interaction features application logic content presentation navigation features automated testing framework historical bug information interest point detector interest point descriptor bug analyzer	Mobile applications support a set of user-interaction features that are independent of the application logic. These features include content presentation or navigation features. Rotating the device, gestures such as scroll or zoom into screens are some examples. In this paper an automated testing framework for mobile applications is proposed. Our framework integrates user-interaction features, historical bug information, and an interest point detector and descriptor to identify new bugs. A model of the application is automatically created and explored. While the exploration the model is performed we introduce user-interaction features and we capturing images. These images are passed to a bug analyzer to search bugs. The bug analyzer uses an interest point detector and descriptor to search for areas prone to bugs in the captured images. The use of historical bug information is proposed to determine sequences of events to better search bugs in applications. Preliminary results show that using the proposed technique is feasible to identify bugs in mobile applications.	android;automated testing framework;bibliothèque de l'école des chartes;business logic;digital zoom;fire emblem: path of radiance;graphical user interface;han unification;linear algebra;mobile app;naruto shippuden: clash of ninja revolution 3;power-on reset;scrolling;software bug;speeded up robust features;unique name assumption	Abel Méndez-Porras;Jorge Alfaro-Velásco;Marcelo Jenkins;Alexandra Martínez Porras	2015	2015 Latin American Computing Conference (CLEI)	10.1109/CLEI.2015.7359996	computer vision;simulation;computer science;world wide web	SE	-58.667318624720764	40.29666854268783	194774
4faf6f217c2bd07332234777e8a6409954a0147b	bug report enrichment with application of automated fixer recommendation		For large open source projects (e.g., Eclipse, Mozilla), developers usually utilize bug reports to facilitate software maintenance tasks such as fixer assignment. However, there are a large portion of short reports in bug repositories. We find that 78.1% of bug reports only include less than 100 words in Eclipse and require bug fixers to spend more time on resolving them due to limited informative contents. To address this problem, in this paper, we propose a novel approach to enrich bug reports. Concretely, we design a sentence ranking algorithm based on a new textual similarity metric to select the proper contents for bug report enrichment. For the enriched bug reports, we conduct a user study to assess whether the additional sentences can provide further help to fixer assignment. Moreover, we assess whether the enriched versions can improve the performance of automated fixer recommendation. In particular, we perform three popular automated fixer recommendation approaches on the enriched bug reports of Eclipse, Mozilla, and GNU Compiler Collection (GCC). The experimental results show that enriched bug reports improve the average F-measure scores of the automated fixer recommendation approaches by up to 10% for DREX, 13.37% for DRETOM, and 8% for DevRec when top-10 bug fixers are recommended.	algorithm;bug tracking system;eclipse;gnu compiler collection;gene ontology term enrichment;information;open-source software;software bug;software maintenance;usability testing	Tao Zhang;Jiachi Chen;He Jiang;Xiapu Luo;Xin Xia	2017	2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)		compiler;data mining;information retrieval;computer science;software maintenance;sentence	SE	-59.918893127258926	38.96969557860169	194820
fe4ceb9408265dc354b93f240b596016b496b774	behavioral anomaly detection approach based on log monitoring	security of data pattern clustering;log sequences behavioral anomaly detection approach log monitoring large scale software systems log semantics analysis frequency features analysis log messages log normalization hierarchical clustering operation behavior pattern set generation anomaly score	Log monitoring has been an effective measure to detect anomalies in large-scale software systems. Many researches for anomaly detection are based on the analysis of log semantics or frequency features in a single time interval. In this paper, we present a new detection method which predicts the system state by detecting anomalous behaviors extracted from log messages. Our detection method consists of 2 major steps: First, preprocess log messages by log normalization and an efficient hierarchical clustering operation. Second, generate behavior pattern sets from clustered messages and assign an anomaly score to new log sequences according to the relation between the log sequences and corresponding behavior patterns. Experiments on real world log data show that our method can predict system anomalies with a high accuracy.	anomaly detection;cluster analysis;data logger;database normalization;experiment;hierarchical clustering;participatory monitoring;preprocessor;sensor;software system	Sizhong Du;Jian Cao	2015	2015 International Conference on Behavioral, Economic and Socio-cultural Computing (BESC)	10.1109/BESC.2015.7365981	pattern recognition;data mining;mathematics;statistics	Robotics	-61.650106516595685	41.40678260532606	195072
75b02de9f3c19cbcdca80bee8b4a6ee37adf0ac4	extending coverage criteria by evaluating their robustness to code structure changes	ing inf 05 sistemi di elaborazione delle informazioni	Code coverage is usually used as a measurement of testing quality and as adequacy criterion. Unfortunately, code coverage is very sensitive to modi cations of the code structure, and, therefore, the same test suite can achieve di erent degrees of coverage on the same program written in two syntactically di erent ways. For this reason, code coverage can provide the tester with misleading information. In order to understand how a testing criterion is a ected by code structure modi cations, we introduce a way to measure the sensitivity of coverage to code changes. We formalize the modi cations of the code structure using semantic preserving code-to-code transformations and we propose a framework to evaluate coverage robustness to these transformations, extending actual existing coverage criteria. This allows us to de ne which programs and which test suites can be considered robust with respect to a certain set of transformations. We can identify when the obtained coverage is fragile and we extend the concept of coverage criterion by introducing an index that measures the fragility of the coverage of a given test suite. We show how to compute the fragility index and we evidence that also well-written industrial code and realistic test suites can be fragile. Moreover, we suggest how to deal with this kind of testing fragility.	code coverage;extended precision;fault detection and isolation;java;software brittleness;test suite	Angelo Gargantini;Marco Guarnieri;Eros Magri	2012		10.1007/978-3-642-34691-0_13	computer science;data mining;mathematics;algorithm	SE	-60.257961061191686	33.642344524411925	195541
e44b2f376915b577a6f2e08329196d1f811ca766	discovering model transformation pre-conditions using automatically generated test models	incomplete domain specification model transformation testing model generation pre condition;model generation;model transformation;testing;unified modeling language formal specification graph theory program testing relational databases;incomplete domain specification;pre condition;unified modeling language computational modeling transforms metals electronic mail benchmark testing;rdbms models model transformation precondition discovery automatic test model generation model transformation specification infinite modeling domain complex object graphs pramana input domain partitioning benchmark transformation uml class diagram models	Specifying a model transformation is challenging as it must be able to give a meaningful output for any input model in a possibly infinite modeling domain. Transformation pre-conditions constrain the input domain by rejecting input models that are not meant to be transformed by a model transformation. This paper presents a systematic approach to discover such pre-conditions when it is hard for a human developer to foresee complex graphs of objects that are not meant to be transformed. The approach is based on systematically generating a finite number of test models using our tool, PRAMANA to first cover the input domain based on input domain partitioning. Tracing a transformation's execution reveals why some pre-conditions are missing. Using a benchmark transformation from simplified UML class diagram models to RDBMS models we discover new pre-conditions that were not initially specified.	benchmark (computing);class diagram;model transformation;test case;unified modeling language	Jean-Marie Mottu;Sagar Sen;Juan José Cadavid;Benoit Baudry	2015	2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)	10.1109/ISSRE.2015.7381802	computer science;engineering;theoretical computer science;software engineering;domain model;data mining;software testing;algorithm	SE	-54.11646499255205	32.47842127132238	195561
43faccfff448d78e108f9bb7643bd024fb61ebbb	data generation for testing and grading sql queries	mutation testing;test data generation;sql query grading	Correctness of SQL queries is usually tested by executing the queries on one or more datasets. Erroneous queries are often the results of small changes or mutations of the correct query. A mutation Q $$'$$ ′ of a query Q is killed by a dataset D if Q(D) $$\ne $$ ≠ Q $$'$$ ′ (D). Earlier work on the XData system showed how to generate datasets that kill all mutations in a class of mutations that included join type and comparison operation mutations. In this paper, we extend the XData data generation techniques to handle a wider variety of SQL queries and a much larger class of mutations. We have also built a system for grading SQL queries using the datasets generated by XData. We present a study of the effectiveness of the datasets generated by the extended XData approach, using a variety of queries including queries submitted by students as part of a database course. We show that the XData datasets outperform predefined datasets as well as manual grading done earlier by teaching assistants, while also avoiding the drudgery of manual correction. Thus, we believe that our techniques will be of great value to database course instructors and TAs, particularly to those of MOOCs. It will also be valuable to database application developers and testers for testing SQL queries.	comparison sort;correctness (computer science);massive open online course;media queries;mutation (genetic algorithm);sql	Bikash Chandra;Amol Bhangadia;Bhupesh Chawda;Biplab Kar;K. V. Maheshwara Reddy;Shetal Shah;S. Sudarshan	2015	The VLDB Journal	10.1007/s00778-015-0395-0	test data generation;computer science;query by example;data mining;database;mutation testing;world wide web;spatial query	DB	-59.220662237621816	38.47003906751661	195783
47087feb2790968b1f7a93879c824cd8596d043c	model-based regression test case prioritization	regression testing;source code;object oriented programming;empirical study;software maintenance	We propose a model-based regression test case prioritization technique for object-oriented programs. Our technique involves constructing a graph model of the source code to represent control and data dependences as well as static object relations such as inheritance, aggregation and association. We construct a forward slice of the model to identify all the model elements that may be affected by a change. Subsequently, the model elements are marked with the specific test case(s) testing it. We construct backward slice with respect to each model element executed by a test case to determine all elements indirectly tested by the test case. Then all the affected model elements and the elements being tested are used to prioritize test cases. Empirical studies carried out by us show that our technique increases the detection of number of faults on an average of 30 percent as compared to traditional approaches in detecting regression errors	regression testing;test case	Chhabi Rani Panigrahi;Rajib Mall	2010		10.1007/978-3-642-12035-0_39	reliability engineering;regression testing;computer science;software engineering;machine learning;data mining;programming language;object-oriented programming;empirical research;software maintenance;source code	Robotics	-62.218066075463966	35.25646395477041	195837
887d76cb0c6e0e65c4763433bed84ba5e8a6f209	intel mpx explained: a cross-layer analysis of the intel mpx system stack		Memory-safety violations are the primary cause of security and reliability issues in software systems written in unsafe languages. Given the limited adoption of decades-long research in software-based memory safety approaches, as an alternative, Intel released Memory Protection Extensions (MPX)---a hardware-assisted technique to achieve memory safety. In this work, we perform an exhaustive study of Intel MPX architecture along three dimensions: (a) performance overheads, (b) security guarantees, and (c) usability issues.  We present the first detailed root cause analysis of problems in the Intel MPX architecture through a cross-layer dissection of the entire system stack, involving the hardware, operating system, compilers, and applications. To put our findings into perspective, we also present an in-depth comparison of Intel MPX with three prominent types of software-based memory safety approaches. Lastly, based on our investigation, we propose directions for potential changes to the Intel MPX architecture to aid the design space exploration of future hardware extensions for memory safety.  A complete version of this work appears in the 2018 proceedings of the ACM on Measurement and Analysis of Computing Systems.	algorithm;compiler;design space exploration;emoticon;experiment;gnu compiler collection;intel mpx;memory protection;memory safety;operating system;software bug;software system;usability	Oleksii Oleksenko;Dmitrii Kuvaiskii;Pramod Bhatotia;Pascal Felber;Christof Fetzer	2018		10.1145/3219617.3219662	compiler;intel mpx;software system;operating system;software;design space exploration;distributed computing;memory safety;usability;computer science;memory protection	Metrics	-56.26299511143174	39.440993106082416	195863
ed278bbbd14d98542e4fecaa252fcef78cf6ea2b	mutation analysis of relational database schemas		The schema is the key artefact used to describe the structure of a relational database, specifying how data will be stored and the integrity constraints used to ensure it is valid. It is therefore surprising that to date little work has addressed the problem of schema testing, which aims to identify mistakes in the schema early in software development. Failure to do so may lead to critical faults, which may cause data loss or degradation of data quality, remaining undetected until later when they will prove much more costly to fix. This thesis explores how mutation analysis – a technique commonly used in software testing to evaluate test suite quality – can be applied to evaluate data generated to exercise the integrity constraints of a relational database schema. By injecting faults into the constraints, modelling both faults of omission and commission, this enables the faultfinding capability of test suites generated by different techniques to be compared. This is essential to empirically evaluate further schema testing research, providing a means of assessing the effectiveness of proposed techniques. To mutate the integrity constraints of a schema, a collection of novel mutation operators are proposed and implementation described. These allow an empirical evaluation of an existing data generation approach, demonstrating the effectiveness of the mutation analysis technique and identifying a configuration that killed 94% of mutants on average. Cost-effective algorithms for automatically removing equivalent mutants and other ineffective mutants are then proposed and evaluated, revealing a third of mutation scores to be mutation adequate and reducing time taken by an average of 7%. Finally, the execution cost problem is confronted, with a range of optimisation strategies being applied that consistently improve efficiency, reducing the time taken by several hours in the best case and as high as 99% on average for one DBMS.	algorithm;best, worst and average case;data integrity;data quality;database schema;elegant degradation;mathematical optimization;mutation testing;relational database;software development;software testing;test suite	Christopher V E Wright	2015			schema migration;reliability engineering;computer science;conceptual schema;data mining;database	SE	-60.867621995917304	37.873401963242046	196267
26c253dbdff010ea2386b4ddd4e7f6e89d0edb2a	on evaluating and constraining assertions using conflicts in absent scenarios		Mining from simulation data has been introduced as an effective solution to assertion generation for the design under verification (DUV) in prior work. As the simulation data is inherently incomplete, the quality of mined assertions is to be measured. In this paper, we propose a Belief-failRate framework to evaluate the success possibility of mined assertions, by taking both occurrences of free variable assignments and conflicts of absent scenarios into consideration. Meanwhile, a success possibility guided assertion constraining method is given to improve the quality of generated assertions. Experimental results show that assertions with high ranking using the proposed Belief-failRate framework are likely to have high possibility to be true assertions. Furthermore, guided by the success possibility, the proposed assertion constraining procedure can find assertions that cover new design functionality.	assertion (software development);free variables and bound variables;mined;simulation	Huina Chao;Huawei Li;Xiaoyu Song;Tiancheng Wang;Xiaowei Li	2017	2017 IEEE 26th Asian Test Symposium (ATS)	10.1109/ATS.2017.45	data mining;real-time computing;assertion;decision tree;ranking;algorithm design;benchmark (computing);data modeling;computer science	Embedded	-60.25844345065023	34.20261449216538	196491
465e8e8b54608d9c25fe00f892e6f10a617a78ff	concern localization using information retrieval: an empirical study on linux kernel	empirical study;kernel;linux kernel;software maintenance;bepress selected works;information retrieval;software systems;kernel linux information retrieval vectors large scale integration software systems;linux kernel dataset concern localization information retrieval software maintenance activity natural languages;concern localization information retrieval linux kernel mean average precision;software engineering;large scale integration;vectors;software maintenance information retrieval linux;linux;mean average precision;concern localization	Many software maintenance activities need to find code units (functions, files, etc.) that implement a certain concern (features, bugs, etc.). To facilitate such activities, many approaches have been proposed to automatically link code units with concerns described in natural languages, which are termed as concern localization and often employ Information Retrieval (IR) techniques. There has not been a study that evaluates and compares the effectiveness of latest IR techniques on a large dataset. This study fills this gap by investigating ten IR techniques, some of which are new and have not been used for concern localization, on a Linux kernel dataset. The Linux kernel dataset contains more than 1,500 concerns that are linked to over 85,000 C functions. We have evaluated the effectiveness of the ten techniques on recovering the links between the concerns and the implementing functions and ranked the IR techniques based on their precisions on concern localization. Keywords-concern localization; information retrieval; Linux kernel; mean average precision;	c standard library;information retrieval;language model;linear discriminant analysis;linux;natural language;non-negative matrix factorization;probabilistic latent semantic analysis;programming language;software bug;software maintenance;software system;source lines of code;text corpus;topic model;viable system model	Shaowei Wang;David Lo;Zhenchang Xing;Lingxiao Jiang	2011	2011 18th Working Conference on Reverse Engineering	10.1109/WCRE.2011.72	computer science;operating system;software engineering;data mining;database;linux kernel	SE	-57.474037157062874	33.533843879937386	197074
63a6f206627180fd075efefafb6440934f3eb4e0	a generic scheme and properties of bidirectional transformations		The recent rise of interest in bidirectional transformations (BXs) has led to the development of many BX frameworks, originating in diverse computer science disciplines. From a user perspective, these frameworks vary significantly in both interface and predictability of the underlying bidirectionalization technique. In this paper we start by presenting a generic BX scheme that can be instantiated to different concrete interfaces, by plugging-in the desired notion of update and traceability. Based on that scheme, we then present several desirable generic properties that may characterize a BX framework, and show how they can be instantiated to concrete interfaces. This generic presentation is useful when exploring the BX design space: it might help developers when designing new frameworks and end-users when comparing existing ones. We support the latter claim, by applying it in a comparative survey of popular existing BX frameworks.	bidirectional transformation;bidirectionalization;computer science;traceability;x86	Hugo Pacheco;Nuno Macedo;Alcino Cunha;Janis Voigtländer	2013	CoRR		simulation;theoretical computer science;algorithm	Security	-51.634183601261334	40.55903708252328	197188
7f01fef2d8b1c307ad514c892240832139d8ac9b	self-configuring object-to-relational mapping queries	generic algorithm;persistence;object to relational mapping;object oriented programming;abstract syntax tree;software maintainability	Object-to-relational maps are nowadays routinely utilized in providing a persistency mechanism for object-oriented programs. We present how an object-to-relational mapper, such as Hibernate, provides transparent persistency to object-oriented programs. We then show how the existing approaches of eager and lazy fetching of associations are problematic under the face of program evolution. As an improvement, we present self-configuring components, which reflectively configure the persistency layer usage sites, thus leading to improved maintainability of software.  A self-configuring component analyses the actual persistency layer usage pattern. Based on this information, the actual queries are configured. A general algorithm for retrieving the actual usage pattern is given. As an implementation mechanism, we discuss different approaches for the self-configuring components. Practical choices for implementation can vary between analysis of abstract syntax trees and Java byte-code analysis. Suitability of two byte-code analysis frameworks, namely BCEL and Soot are evaluated.	abstract syntax tree;algorithm;byte;java bytecode;lazy evaluation;map;object-relational mapping;static program analysis	Pietu Pohjalainen;Juha Taina	2008		10.1145/1411732.1411740	persistence;genetic algorithm;computer science;data mining;database;programming language;object-oriented programming;maintainability;abstract syntax tree	PL	-56.002732023224844	37.77518922045733	197425
08bda531600baaa910e84bcbdbd5cff89f91aa35	flavers: a finite state verification technique for software systems	selected works;software systems;theorem proving;formal verification;bepress	Software systems are increasing in size and complexity and, subsequently, are becoming ever more difficult to validate. Finite state verification (FSV) has been gaining credibility and attention as an alternative to testing and to formal verification approaches based on theorem proving. There has recently been a great deal of excitement about the potential for FSV approaches to prove properties about hardware descriptions but, for the most part, these approaches do not scale adequately to handle the complexity usually found in software. In this paper, we describe an FSV approach that creates a compact and conservative, but imprecise, model of the system being analyzed, and then assists the analyst in adding additional details as guided by previous analysis results. This paper describes this approach and a prototype implementation called FLAVERS, presents a detailed example, and then provides some experimental results demonstrating scalability.	automated theorem proving;complexity;formal verification;hardware description language;prototype;scalability;software system	Jamieson M. Cobleigh;Lori A. Clarke;Leon J. Osterweil	2002	IBM Systems Journal	10.1147/sj.411.0140	formal verification;software verification;computer science;theoretical computer science;database;automated theorem proving;programming language;algorithm;software system	SE	-48.780858888273954	33.846429944405095	197515
f940d0224fac7d7baf6aeb79694a11b9fbe89276	ontology-based test modeling and partition testing of web services	completeness and consistency checking web services ontology based test modeling partition testing service oriented architecture test ontology model web ontology language;owl;owl s;generators;test model;service provider;service orientation;specification based testing;collaboration;test ontology model;testing;web service;ontologies artificial intelligence;data partitioning;web services ontologies artificial intelligence program testing software architecture;software architecture;program testing;web ontology language;completeness and consistency checking;owl s test model partition testing ontology web services;unified modeling language;web services;partition testing;ontologies;service oriented architecture;ontology based test modeling;ontology;ontologies web services automatic testing collaboration contracts owl service oriented architecture software testing collaborative work computer science	Testing is useful to establish trust between service providers and clients. To test the service-oriented applications, automated and specification-based test generation and test collaboration are necessary. The paper proposes an ontology-based approach for Web services (WS) testing. A test ontology model (TOM) is defined to specify the test concepts, relationships, and semantics from two aspects: test design (such as test data, test behavior, and test cases) and test execution (such as test plan, schedule and configuration). The TOM specification using OWL (Web ontology language) can serve as test contracts among test components. Based on the WS semantic specification in OWL-S, the paper discusses the techniques to generate the sub-domains for input partition testing. Data pools are established for each parameter of the specified service. Data partitions are derived by class property and relationship analysis. Completeness and consistency (C&C) checking can be performed on the data partitions and data values, both within the TOM and against the OWL-S, by ontology class computation and reasoning. A prototype tool is implemented to support OWL-S analysis, test ontology generation and C&C checking.	artificial intelligence;computation;owl-s;ontology (information science);ontology learning;prototype;random testing;service-oriented software engineering;test case;test data;test design;test plan;tom;web ontology language;web service	Xiaoying Bai;Shufang Lee;Wei-Tek Tsai;Yinong Chen	2008	2008 IEEE International Conference on Web Services	10.1109/ICWS.2008.111	web service;computer science;ontology;test suite;data mining;database;web ontology language;law;world wide web;test case;test management approach;test harness	SE	-48.94943987548469	35.203336676934015	198017
6081ceb60d07fa0a2f0037ece6e540228e4edf73	automatic documentation generation via source code summarization of method context	source code summarization	A documentation generator is a programming tool that creates documentation for software by analyzing the statements and comments in the software's source code. While many of these tools are manual, in that they require specially-formatted metadata written by programmers, new research has made inroads towards automatic generation of documentation. These approaches work by stitching together keywords from the source code into readable natural language sentences. These approaches have been shown to be effective, but carry a key limitation: the generated documents do not explain the source code's context. They can describe the behavior of a Java method, but not why the method exists or what role it plays in the software. In this paper, we propose a technique that includes this context by analyzing how the Java methods are invoked. In a user study, we found that programmers benefit from our generated documentation because it includes context information.	documentation generator;image stitching;java;natural language;programmer;programming tool;usability testing	Paul W. McBurney;Collin McMillan	2014		10.1145/2597008.2597149	kpi-driven code analysis;computer science;database;internal documentation;programming language;world wide web;docstring;code generation;source code	SE	-52.906785159819414	36.11090004678881	198148
0bbebfcf7970d978282773272650c7b9ff16d531	model-based programming environments for spreadsheets	programming environment;haslab haslab uminho;article letter to editor;article in monograph or in proceedings	Although spreadsheets can be seen as a flexible programming environment, they lack some of the concepts of declarative programming languages, such as structured data types. This can lead the user to edit the spreadsheet in a wrong way and perhaps cause corrupt or redundant data. We devised a method for extraction of a relational model from a spreadsheet and the subsequent embedding of the model back into the spreadsheet to create a model-based spreadsheet programming environment. The extraction algorithm is specific for spreadsheets since it considers particularities such as layout and column arrangement. The extracted model is used to generate formulas and visual elements that are embedded in the spreadsheet helping users to edit correct data. We present preliminary experimental results from applying our approach to a sample of spreadsheets from the EUSES Spreadsheet Corpus.	algorithm;database schema;embedded system;experiment;functional dependency;integrated development environment;plug-in (computing);programming language;relational database;relational model;spreadsheet;structured programming	Jácome Cunha;João Saraiva;Joost Visser	2012		10.1007/978-3-642-33182-4_10	reactive programming;computer science;data mining;database	SE	-51.8763062934993	34.18373336949279	198364
8d1eb10c5f1cce3d2e641b0e2849a3a9db1ad8d9	the impact of fault type on the relationship between code coverage and fault detection		Structural coverage criteria are commonly used to determine the adequacy of a test suite. However, studies investigating structural coverage and fault-finding capabilities have mixed results. Some studies have shown generating test suites to satisfy structural coverage criteria has a positive effect on finding faults, while other studies show the opposite. These mixed results indicate there are factors not yet known that affect the ability of test suites satisfying structural coverage criteria to find faults. In order to improve the fault-finding capabilities of test suites, it is essential to understand what factors are causing this variance. Unfortunately very little work has been done to investigate the variance observed in the relationship between structural coverage criteria and fault-finding capabilities. In this paper, we investigate one possible source of variation in the results observed: fault type. We provide an empirical study which narrows down the focus of the relationship between structural coverage and fault-finding capabilities by focusing on object-oriented bugs. Specifically, we investigated 26 different types of object-oriented faults and evaluated how effectively test suites with high coverage percentages were able to detect each type of fault. We found that a test suiteu0027s ability to find faults varied significantly according to the type of fault (ranging from a rate of 0% to 87.5% mutants detected per fault type). We also found there are particular types of faults that were consistently found less frequently across all object programs.	code coverage;fault detection and isolation	Amanda Schwartz;Michael Hetzel	2016		10.1109/AST.2016.013	reliability engineering;real-time computing;fault coverage;computer science;engineering;software engineering;software testing;code coverage;forensic engineering	SE	-62.705292600955666	34.935905178995846	198544
656f05213348bb3a5455ed66d7faf020510c5b97	sdiff: a comparison tool based in syntactical document structure	syntactical document structure;text based mechanism sdiff comparison tool syntactical document structure version control systems;document structure;software engineering software configuration management version control system software development tool;software;sdiff;control systems;document handling;text analysis document handling software tools;electronic mail;programming language;comparison tool;text analysis;software configuration management;software engineering;cloning;version control system;visualization;syntactics;text based mechanism;version control systems;xml;software development tool;software development tools;software tools;xml software syntactics cloning control systems visualization electronic mail	Version control systems tend to compare textual information from documents in which the indivisible element is the line or word. However, the versioned content is usually highly structured (for example, programming languages) and using a text based mechanism can disrespect syntactical limits and other properties of the document, making it difficult to interpret what has really changed. In this work we describe a tool that compares documents using their syntactic structure, identifying more precisely the relevant difference to the reader, as well as reducing the effort to understand the semantics of the changes.	control system;diff utility;indivisible;programming language;software versioning;text-based (computing);version control	Thiago Pinheiro de Araújo;Arndt von Staa	2010	2010 Brazilian Symposium on Software Engineering	10.1109/SBES.2010.23	computer science;database;programming language;world wide web	SE	-52.503091394778814	36.039471762489065	198738
b0d2efad4d469872e651fe4d5a5f0d03881e5ee2	bidirectional symbolic analysis for effective branch testing	analytical models;coverage measurement bidirectional symbolic analysis branch testing structural coverage metrics branch coverage test suite thoroughness measure test case generation symbolic execution symbolic reachability analysis test objectives;settore inf 01 informatica;measurement;branch coverage;symbolic reachability analysis;testing;structural testing;computational modeling;symbolic execution;settore ing inf 05 sistemi di elaborazione delle informazioni;program analysis;valves;analytical models testing reachability analysis valves computational modeling measurement concrete;program testing program diagnostics;reachability analysis;concrete	Structural coverage metrics, and in particular branch coverage, are popular approaches to measure the thoroughness of test suites. Unfortunately, the presence of elements that are not executable in the program under test and the difficulty of generating test cases for rare conditions impact on the effectiveness of the coverage obtained with current approaches. In this paper, we propose a new approach that combines symbolic execution and symbolic reachability analysis to improve the effectiveness of branch testing. Our approach embraces the ideal definition of branch coverage as the percentage of executable branches traversed with the test suite, and proposes a new bidirectional symbolic analysis for both testing rare execution conditions and eliminating infeasible branches from the set of test objectives. The approach is centered on a model of the analyzed execution space. The model identifies the frontier between symbolic execution and symbolic reachability analysis, to guide the alternation and the progress of bidirectional analysis towards the coverage targets. The experimental results presented in the paper indicate that the proposed approach can both find test inputs that exercise rare execution conditions that are not identified with state-of-the-art approaches and eliminate many infeasible branches from the coverage measurement. It can thus produce a modified branch coverage metric that indicates the amount of feasible branches covered during testing, and helps team leaders and developers in estimating the amount of not-yet-covered feasible branches. The approach proposed in this paper suffers less than the other approaches from particular cases that may trap the analysis in unbounded loops.	code coverage;executable;reachability;symbolic execution;test case;test suite	Mauro Baluda;Giovanni Denaro;Mauro Pezzè	2016	IEEE Transactions on Software Engineering	10.1109/TSE.2015.2490067	program analysis;real-time computing;concrete;computer science;theoretical computer science;operating system;software engineering;software testing;code coverage;programming language;computational model;concolic testing;algorithm;measurement	SE	-59.760863561549485	36.284419781333646	198803
6b1a367d1828dfe08666d5cd309d65ac2a64db42	identification and elimination of platform-specific code smells in high performance computing applications	hpc	A code smell is a code pattern that might indicate a code or design problem, which makes the application code hard to evolve and maintain. Automatic detection of code smells has been studied to help users find which parts of their application codes should be refactored. However, code smells have not been defined in a formal manner. Moreover, existing detection tools are designed mainly for object-oriented applications, but rarely provided for high performance computing (HPC) applications. HPC applications are usually optimized for a particular platform to achieve a high performance, and hence have special code smells called platform-specific code smells (PSCSs). The purpose of this work is to develop a code smell alert system to help users find PSCSs of HPC applications to improve the performance portability across different platforms. This paper presents a PSCS alert system that is based on an abstract syntax tree (AST) and XML. Code patterns of PSCSs are defined in a formal way using the AST information represented in XML. XML Path Language (XPath) is used to describe those patterns. A database is built to store the transformation recipes written in XSLT files for eliminating detected PSCSs. The recall and precision evaluation results obtained by using real applications show that the proposed system can detect potential PSCSs accurately. The evaluation on performance portability of real applications demonstrates that eliminating PSCSs leads to significant performance	abstract syntax tree;code refactoring;code smell;domain-specific language;loop invariant;openacc;parse tree;performance evaluation;platform-specific model;precision and recall;sensor;software portability;supercomputer;xml;xpath;xslt	Chunyan Wang;Shoichi Hirasawa;Hiroyuki Takizawa;Hiroaki Kobayashi	2015	IJNC		parallel computing;computer science;operating system;programming language;code generation;source code	HPC	-56.67713477035739	36.756450820961	198852
236a932b95398602c2693f67ca28250c07140564	the effect of program and model structure on mc/dc test adequacy coverage	software metrics;avionics;mc dc test adequacy coverage;probability density function;testing;satisfiability;data mining;modified condition and decision coverage criterion mc dc test adequacy coverage civil avionics source code model based development program structure expression folding;computational modeling;source coding avionics program testing software metrics;structural coverage metrics;critical system;civil avionics;program testing;aerospace electronics;model based development;source code;atmospheric modeling;modified condition and decision coverage criterion;expression folding;software testing aerospace electronics dc generators nasa permission system testing current measurement computer industry certification logic testing;program structure;wheels;source coding	In avionics and other critical systems domains, adequacy of test suites is currently measured using the MC/DC metric on source code (or on a model in model-based development). We believe that the rigor of the MC/DC metric is highly sensitive to the structure of the implementation and can therefore be misleading as a test adequacy criterion. We investigate this hypothesis by empirically studying the effect of program structure on MC/DC coverage.  To perform this investigation, we use six realistic systems from the civil avionics domain and two toy examples. For each of these systems, we use two versions of their implementation-with and without expression folding (i.e., inlining). To assess the sensitivity of MC/DC to program structure, we first generate test suites that satisfy MC/DC over a non-inlined implementation. We then run the generated test suites over the inlined implementation and measure MC/DC achieved. For our realistic examples, the test suites yield an average reduction of 29.5% in MC/DC achieved over the inlined implementations at 5% statistical significance level.	avionics;inline expansion;model-driven engineering;structured programming;test suite	Ajitha Rajan;Michael W. Whalen;Mats Per Erik Heimdahl	2008	2008 ACM/IEEE 30th International Conference on Software Engineering	10.1145/1368088.1368111	avionics;reliability engineering;embedded system;real-time computing;computer science;engineering;operating system;software engineering;programming language;source code	SE	-60.86488551424932	32.96120708569337	198916
a02975e131216ed9e14046a3b1f1cbd54d68372e	antipattern-based detection of deficiencies in java multithreaded software	java;multi threading	We investigate an antipattern-based approach to analyze Java multithreaded (MT) programs. We present a library of 38 antipatterns, which describe predefined recognized sources of multithreading related errors in the code. The antipatterns are archived in practical, easy to use templates, and are classified according to their potential effects on the program behavior. We also report on our experience in using these antipatterns in the analysis of real multithreaded applications.	anti-pattern;archive;backward compatibility;categorization;concurrency (computer science);correctness (computer science);deadlock;eclipse;experiment;high-level programming language;java memory model;multithreading (computer architecture);programmer;prototype;race condition;refactoring software, architectures, and projects in crisis;relevance;software documentation;software release life cycle;synchronization (computer science);thread (computing);usability	Hesham Hallal;El Hachemi Alikacem;W. P. Tunney;Sergiy Boroday;Alexandre Petrenko	2004	Fourth International Conference onQuality Software, 2004. QSIC 2004. Proceedings.	10.1109/QSIC.2004.1357968	parallel computing;multithreading;java concurrency;computer science;operating system;real time java;programming language;java;java annotation	SE	-56.76296326773274	36.43509143297391	199234
58294450521faab6ee24e4ba8d623f495df70096	bounded abstract interpretation	qa mathematics inc computing science	In practice, software engineers are only able to spend a limited amount of resources on statically analyzing their code. Such resources may refer to their available time or their tolerance for imprecision, and usually depend on when in their workflow a static analysis is run. To serve these different needs, we propose a technique that enables engineers to interactively bound a static analysis based on the available resources. When all resources are exhausted, our technique soundly records the achieved verification results with a program instrumentation. Consequently, as more resources become available, any static analysis may continue from where the previous analysis left off. Our technique is applicable to any abstract interpreter, and we have implemented it for the .NET static analyzer Clousot. Our experiments show that bounded abstract interpretation can significantly increase the performance of the analysis (by up to 8x) while also increasing the quality of the reported warnings (more definite warnings that detect genuine bugs).	abstract interpretation;experiment;instrumentation (computer programming);interactivity;interpreter (computing);profiling (computer programming);software bug;software engineer;static program analysis	Maria Christakis;Valentin Wüstholz	2016		10.1007/978-3-662-53413-7_6	computer science;theoretical computer science	SE	-57.10518846531332	38.9792792004488	199431
