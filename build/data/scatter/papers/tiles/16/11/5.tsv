id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
51e923759bf2f479bb70322f30865a9971aa244a	spatial discrimination in task-driven attention	velocity control;task driven gaze orienting spatial discrimination task driven attention vision based attentional framework robot vision;computer vision;autonomous agent;robot vision;selective attention;velocity control robot vision;visual attention;layout robot vision systems orbital robotics object recognition focusing computational modeling human robot interaction computer vision autonomous agents biological system modeling	Visual attention is becoming an increasingly imperative capability to endow computer vision systems and autonomous agents with. Starting from a biological inspired model of attention, we present an experiment aimed to study selective attention in 3D space. Depth has been proved to be an important feature affecting the way attention is deployed when observing a scene. We studied preferential scanning paths and fixation zones in a task-driven wandering of the tutor gaze over a scene where multiple targets had been disposed on different depth planes. We supposed that selective attention would aggregate targets in cliques that maximize utility, minimizing meanwhile visual effort produced when passing from closer planes to further planes or between different cluttered locations. By means of a purposely designed machine, we stored visual and motor data of the tutor's head; we clustered different scanning paths of the gaze shifts according to velocity and space criteria to determine a preference model of attentional shifts and fixations. We propose subsequently a utility model that can formalize acquired information and establish a vision-based attentional framework for robots. We show that an interpretation of task-driven gaze orienting based on the presented preference criteria correctly accounts for the studied behaviours, as further reported in the literature	aggregate data;autonomous agent;autonomous robot;computation;computational model;computer vision;experiment;imperative programming;software deployment;velocity (software development)	Anna Belardinelli;Fiora Pirri;Andrea Carbone	2006	ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2006.314437	computer vision;simulation;attention;computer science;artificial intelligence;autonomous agent	Robotics	48.972215290921746	-31.862852590001694	48283
2bf90c0c0274df77d52f96429d655b2f494f8f61	workload analysis and efficient opencl-based implementation of sift algorithm on a smartphone	paper;sift workload analysis optimized cpu only implementation energy consumption descriptor generation keypoint detection buffer transferring time opencl kernels serial c code mobile gpu tablets mobile devices heterogeneous computing mobile processors computational complexity distinctive invariant feature detection distinctive invariant feature extraction scale invariant feature transform algorithm computer vision applications smartphones sift algorithm opencl based implementation;software performance evaluation;smart phones;computer vision;power aware computing;transforms c language computational complexity computer vision feature extraction graphics processing units mobile computing open systems power aware computing smart phones software performance evaluation;c language;sift;opencl sift gpu mobile soc cpu gpu algorithm partitioning;computational complexity;feature extraction;mobile communication graphics processing units kernel feature extraction partitioning algorithms algorithm design and analysis;graphics processing units;transforms;arm;computer science;mobile computing;opencl;open systems	Feature detection and extraction are essential in computer vision applications such as image matching and object recognition. The Scale-Invariant Feature Transform (SIFT) algorithm is one of the most robust approaches to detect and extract distinctive invariant features from images. However, high computational complexity makes it difficult to apply the SIFT algorithm to mobile applications. Recent developments in mobile processors have enabled heterogeneous computing on mobile devices, such as smartphones and tablets. In this paper, we present an OpenCL-based implementation of the SIFT algorithm on a smartphone, taking advantage of the mobile GPU. We carefully analyze the SIFT workloads and identify the parallelism. We implemented major steps of the SIFT algorithm using both serial C++ code and OpenCL kernels targeting mobile processors, to compare the performance of different workflows. Based on the profiling results, we partition the SIFT algorithm between the CPU and GPU in a way that best exploits the parallelism and minimizes the buffer transferring time to achieve better performance. The experimental results show that we are able to achieve 8.5 FPS for keypoints detection and 19 FPS for descriptor generation without reducing the number and the quality of the keypoints. Moreover, the heterogeneous implementation can reduce energy consumption by 41% compared to an optimized CPU-only implementation.	algorithm;c++;central processing unit;computational complexity theory;computer vision;feature detection (computer vision);floating point systems;gaussian blur;graphics processing unit;heterogeneous computing;image registration;mathematical optimization;memory bandwidth;mobile app;mobile computing;mobile device;mobile processor;opencl api;outline of object recognition;parallel computing;reference (c++);reference design;scale-invariant feature transform;smartphone;speedup;tablet computer	Guohui Wang;Blaine Rister;Joseph R. Cavallaro	2013	2013 IEEE Global Conference on Signal and Information Processing	10.1109/GlobalSIP.2013.6737002	embedded system;parallel computing;computer science;theoretical computer science	EDA	42.806026657839475	-36.3102901374511	48549
f6966fac0b0f506eadf36757f8bb0675518a47d1	bayesian tactile exploration for compliant docking with uncertain shapes		This paper presents a Bayesian approach for active tactile exploration of a planar shape in the presence of both localization and shape uncertainty. The goal is to dock the robot’s end-effector against the shape – reaching a point of contact that resists a desired load – with as few probing actions as possible. The proposed method repeatedly performs inference, planning, and execution steps. Given a prior probability distribution over object shape and sensor readings from previously executed motions, the posterior distribution is inferred using a novel and efficient Hamiltonian Monte Carlo method. The optimal docking site is chosen to maximize docking probability, using a closedform probabilistic simulation that accepts rigid and compliant motion models under Coulomb friction. Numerical experiments demonstrate that this method requires fewer exploration actions to dock than heuristics and information-gain strategies.	3d printing;automated planning and scheduling;docking (molecular);experiment;geodesic grid;heuristic (computer science);hybrid monte carlo;kullback–leibler divergence;monte carlo method;numerical method;point cloud;robot end effector;sampling (signal processing);simulation	Kris Hauser	2018		10.15607/RSS.2018.XIV.051	artificial intelligence;machine learning;computer vision;computer science;docking (dog);bayesian probability	Robotics	51.782477990067854	-25.865566830635053	48787
5da473aba79dbb1bca84a5129690c2f11dfc805c	a volumetric representation for obstacle detection in vegetated terrain	lasers;robot sensing systems;measurement by laser beam;terrestrial four wheel robot volumetric representation vegetated terrain 3d based obstacle detection autonomous vehicle navigation vegetated environments volumetric resolutions geometric relationships scene vegetation water surface robot;laser beams;robot kinematics measurement by laser beam robot sensing systems laser beams lasers;terrain mapping collision avoidance mobile robots;robot kinematics	This paper presents a method for 3-D based obstacle detection on autonomous vehicles navigating in vegetated environments. At its core three different methods processing the surrounding occupancy, taken at separate stages and volumetric resolutions, are combined to a reliable and broad solution. Geometric relationships are evaluated at a coarse, yet robust, volumetric representation to form an initial assessment on obstacles. Then, a more careful evaluation takes place, at finer resolutions, to determine which obstacles are part of the scene's vegetation, thus not real obstacles. Field experiments are shown to validate the method's applicability on two different autonomous vehicles: a water surface robot and a terrestrial four-wheeled one.	autonomous robot;experiment;rectifier (neural networks);robotic arm;solidity;terrestrial television;volumetric display	André Lourenço;Francisco Marques;Pedro F. Santana;José Barata	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090344	computer vision;simulation;laser;computer science;artificial intelligence;robot kinematics;remote sensing	Robotics	52.66924220528251	-33.263917798605675	49148
d57da76befc7f80b167f48634393924e4af0189a	optimal positioning of multiple cameras for object recognition using cramer-rao lower bound	recursive estimation;object recognition;hidden feature removal;uncertainty;monte carlo experiments;bayes methods;recursive estimation bayes methods hidden feature removal image sensors monte carlo methods object recognition position control principal component analysis;cramer rao lower bound;bayesian methods;cameras object recognition state estimation image analysis noise robustness uncertainty bayesian methods recursive estimation monte carlo methods sensor fusion;image sensors;state estimation;noise robustness;probabilistic model;position control;principal component analysis;monte carlo experiment;recursive bayesian state estimation;active objects;image analysis;occlusion modelling;principle component analysis;information fusion;optimal positioning;object classification;sensor fusion;multiple cameras;monte carlo methods;cameras;object model;occlusion modelling optimal positioning multiple cameras object recognition cramer rao lower bound pose estimation principle component analysis recursive bayesian state estimation monte carlo experiments;pose estimation	In this paper the problem of active object recognition/pose estimation is investigated. The principle component analysis is used to produce an observation vector from images captured simultaneously by multiple cameras from different view angles of an object belonging to a set of a priori known objects. Models of occlusion and sensor noise have been incorporated into a probabilistic model of sensor/object to increase the robustness of the recognition process with respect to such uncertainties. A recursive Bayesian state estimation problem is formulated to identify the object and estimate its pose by fusing the information obtained from the cameras at multiple steps. In order to enhance the quality of the estimates and to reduce the number of images taken, the positions of the cameras are controlled based on a statistical performance criterion, the Cramer-Rao lower bound (CRLB). Comparative Monte Carlo experiments conducted with a two-camera system demonstrate that the features of the proposed method, i.e. information fusion from multiple sources, active optimal sensor planing, and occlusion modelling are all highly effective for object classification/pose estimation in the presence of structured noise	3d pose estimation;active object;algorithm;bayesian network;estimation theory;experiment;image noise;monte carlo method;outline of object recognition;planning;pose (computer vision);principal component analysis;recursion;sensor web;statistical model	F. Farshidi;Shahin Sirouspour;Thia Kirubarajan	2006	Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.	10.1109/ROBOT.2006.1641829	computer vision;image analysis;pose;3d pose estimation;computer science;pattern recognition;mathematics;3d single-object recognition;statistics;principal component analysis	Robotics	51.72872861446984	-34.869413324149924	49153
a78604c5b521fec1a8799febc257cfddea6e25db	cooperative relative robot localization with audible acoustic sensing	robot localization;mobile robot;path planning;kalman filters;kalman filter;mobile robots;robot localization robot sensing systems particle filters mobile robots microphones time of arrival estimation signal resolution uncertainty acoustic measurements yield estimation;rao blackwellised particle filter;distance measurement;cooperative systems;position control;particle filter;multi robot systems;acoustic signal detection;time of arrival;acoustic signal detection mobile robots path planning cooperative systems multi robot systems position control kalman filters distance measurement;pose estimation cooperative relative robot localization audible acoustic sensing mobile robots robot identification time of arrival estimation odometry robot positions robot heading angles rao blackwellised particle filter gaussian processes particle filters kalman filter;pose estimation	We describe a method for estimating the relative poses of a team of mobile robots using only acoustic sensing. The relative distances and bearing angles of the robots are estimated using the time of arrival of audible sound signals on stereo microphones. The robots emit specially designed sound waveforms that simultaneously enable robot identification and time of arrival estimation. These acoustic observations are then combined with odometry to update a belief state describing the positions and heading angles of all the robots. To efficiently resolve the ambiguity in the heading angle of the observing robot as well as the back-front ambiguity of the observed robot, we employ a Rao-Blackwellised particle filter (RBPF) where the distribution over heading angles is represented by a discrete set of particles, and the uncertainty in the translational positions conditioned on each of these particles is described by a Gaussian. This approach combines the representational accuracy of conventional particle filters with the efficiency of Kalman filter updates in modeling the pose distribution over a number of robots. We demonstrate how the RBPF can quickly resolve uncertainties in the binaural acoustic measurements and yield a globally consistent pose estimate. Simulations as well as an experimental implementation on robots with generic sound hardware illustrate the accuracy and the convergence of the resulting pose estimates.	acoustic cryptanalysis;binaural beats;computer simulation;course (navigation);kalman filter;microphone;mobile robot;odometry;particle filter;robotic mapping;time of arrival	Yuanqing Lin;Paul Vernaza;Jihun Ham;Daniel D. Lee	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545056	kalman filter;mobile robot;monte carlo localization;computer vision;simulation;computer science;engineering;artificial intelligence;control theory	Robotics	52.594677662110804	-34.50461795437841	49399
17e855fd30e252329447b9b27b4fa397d5c12545	modeling and prediction of human behavior	simulation ordinateur;modelizacion;prediccion;industrie automobile;chaine markov;cadena markov;comportement;pedestrian safety;modelo markov;occupational training;poison control;etude experimentale;injury prevention;dynamic model;safety literature;traffic safety;injury control;human behavior;dynamical system;home safety;modelisation;systeme dynamique;injury research;control system;safety abstracts;markov model;human factors;conducta;formacion profesional;industria automovil;occupational safety;safety;safety research;accident prevention;violence prevention;bicycle safety;simulacion computadora;modele markov;sistema dinamico;short period;behavior;poisoning prevention;modeling;falls;computer simulation;estudio experimental;prediction;ergonomics;suicide prevention;automobile industry;formation professionnelle;markov chain	We propose that many human behaviors can be accurately described as a set of dynamic models (e.g., Kalman filters) sequenced together by a Markov chain. We then use these dynamic Markov models to recognize human behaviors from sensory data and to predict human behaviors over a few seconds time. To test the power of this modeling approach, we report an experiment in which we were able to achieve 95 accuracy at predicting automobile drivers' subsequent actions from their initial preparatory movements.	behavior;kalman filter;markov chain;markov model;movement	Alex Pentland;Andrew Liu	1999	Neural Computation	10.1162/089976699300016890	markov chain;simulation;prediction;computer science;suicide prevention;human factors and ergonomics;injury prevention;dynamical system;mathematics;markov model;statistics;behavior	AI	43.153634467602295	-28.116090689606033	49628
5e6088a4e19e8c94076997b13e4e9570d2bd9177	navigation of an unmanned surface vessel under bridges	nonlinear filters;kalman filters;simultaneous localization and mapping navigation unmanned surface vessels extended kalman filter;mobile robots;remotely operated vehicles;bridges structures;robot vision;marine vehicles;global positioning system;bridges vehicles laser radar robots global positioning system shape;collision avoidance;slam robots bridges structures collision avoidance global positioning system kalman filters marine vehicles mobile robots nonlinear filters object detection remotely operated vehicles robot vision;slam robots;extended kalman filter unmanned surface vessel navigation bridge pier structures obstacle detection obstacle localization safe vehicle navigation unmanned surface vessels usv gps signals vehicle position estimation parameterized map simultaneous localization and mapping slam;object detection	Obstacle detection and localization relative to surrounding objects or structures are important for safe vehicle navigation. This capability is particularly useful for unmanned surface vessels (USVs) operating near large structures that will block GPS signals. This study proposes a relative navigation approach, which allows estimating the vehicles position under bridges where GPS signals are not available by building a parameterized map of bridge pier structures in the framework of simultaneous localization and mapping (SLAM). The feasibility of the algorithm is demonstrated in outdoor experiments.	algorithm;autonomous robot;binary prefix;complex systems;experiment;gps signals;global positioning system;point cloud;simultaneous localization and mapping;unmanned aerial vehicle	Jungwook Han;Jinwhan Kim	2013	2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2013.6677343	remotely operated underwater vehicle;kalman filter;mobile robot;computer vision;simulation;global positioning system;computer science;artificial intelligence;mobile robot navigation;simultaneous localization and mapping	Robotics	52.82257822781404	-36.003955114842746	49677
c0cde3526697e6932f5d18b7aebaad7fb196676d	distributed deep reinforcement learning based indoor visual navigation		Recently, as the rise of deep reinforcement learning, it not only can help the robot to convert the complicated environment scene to motor control command directly but also can accomplish the navigation task properly. In this paper, we propose a novel structure, where the objective is to achieve navigation in large-scale indoor complex environment without pre-constructed map. Generally, it requires good understanding of such indoor environment to make complex spatial perception possible, especially when the indoor space consists of many walls and doors which might block the view of robot leading to complex navigation path. By the proposed distributed deep reinforcement learning in different local regions, our method can achieve indoor visual navigation in the aforementioned large-scale environment without extra map information and human instruction. In the experiments, we validate our proposed method by conducting highly promising navigation tasks both in simulation and real environments.		Shih-Hsi Hsu;Shao-Hung Chan;Ping-Tsang Wu;Kun Xiao;Li-Chen Fu	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8594352	computer vision;doors;artificial intelligence;task analysis;motor control;computer science;reinforcement learning;robot;visualization;perception	Robotics	49.966312818036194	-30.70131584352477	49806
0b7d4d5cd922c36350acd7bdf599d44bfdf6c875	time-bounded lattice for efficient planning in dynamic environments	moving object;state space methods;lattices;uncertainty;path planning;real time;remotely operated vehicles;data mining;dynamic environments;dynamic environment;navigation;cluttered environments;trajectory;state space methods collision avoidance navigation remotely operated vehicles road vehicles;state space;data structure time bounded lattice dynamic environments vehicle navigation cluttered environments state of the art planning algorithms tracking trajectory prediction moving objects state space moving obstacles path planning algorithm;robots;path planning algorithm;moving obstacles;trajectory prediction;moving objects;planning;time bounded lattice;collision avoidance;vehicle navigation;state of the art planning algorithms;data structure;vehicle dynamics;tracking;road vehicles;lattices vehicle dynamics path planning trajectory vehicles navigation robustness humans process planning predictive models	For vehicles navigating initially unknown cluttered environments, current state-of-the-art planning algorithms are able to plan and re-plan dynamically-feasible paths efficiently and robustly. It is still a challenge, however, to deal well with the surroundings that are both cluttered and highly dynamic. Planning under these conditions is more difficult for two reasons. First, tracking and predicting the trajectories of moving objects (i.e., cars, humans) is very noisy. Second, the planning process is computationally more expensive because of the increased dimensionality of the state-space, with time as an additional variable. Moreover, re-planning needs to be invoked more often since the trajectories of moving obstacles need to be constantly re-estimated. In this paper, we develop a path planning algorithm that addresses these challenges. First, we choose a representation of dynamic obstacles that efficiently models their predicted trajectories and the uncertainty associated with the predictions. Second, to provide real-time guarantees on the performance of planning with dynamic obstacles, we propose to utilize a novel data structure for planning - a time-bounded lattice - that merges together short-term planning in time with longterm planning without time. We demonstrate the effectiveness of the approach in both simulations with up to 30 dynamic obstacles and on real robots.	algorithm;automated planning and scheduling;data structure;humans;motion planning;real-time clock;robot;simulation;state space;vii	Aleksandr Kushleyev;Maxim Likhachev	2009	2009 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2009.5152860	remotely operated underwater vehicle;planning;robot;control engineering;computer vision;navigation;vehicle dynamics;simulation;uncertainty;data structure;computer science;state space;engineering;artificial intelligence;trajectory;lattice;motion planning;tracking	Robotics	52.87880825070097	-24.15407818946074	49877
8dda45e651282901badfe95c7662d11de61f2d7e	spatial interpolation for robotic sampling: uncertainty with two models of variance		Several important forms of robotic environmental monitoring involve estimating a spatial field from comparatively few measurements. A number of researchers use linear least squares estimation techniques, frequently either the geostatistical Kriging framework or a Gaussian Process regression formulation, that provide estimates of quantities of interest at unmeasured locations. These methods enable selection of sample locations (e.g., for adaptive sampling) by quantifying uncertainty across the scalar field. This paper assesses the role of pose uncertainty and measurement error on variance of the estimated spatial field. We do this through a systematic empirical comparison of scalar fields reconstructed from measurements taken with our robot using multiple imperfect sensors and actively estimating its pose. We implement and compare two models of variance: Kriging Variance (KV) and Interpolation Variance (IV), illustrating that the latter —which has not been used in a robotics context before— has several advantages when used for online planning of sampling tasks. Using two separate experimental scenarios, we assess the estimated variance in scalar fields constructed from measurements taken by robots. Physical robots sampling within our office building suggest that using IV to select sampling sites gathers more data for a given time window (45% more than KV), travels a shorter distance to collect the same number of samples (25% less than KV), and has a promising speed-up with multiple robots. Water quality data from an Autonomous Underwater Vehicle survey of Lake Pleasant, AZ. also show that IV produces better qualities for given a distance and time.	adaptive sampling;areal density (computer storage);assignment zero;autonomous robot;greedy algorithm;kinetic void;kriging;linear least squares (mathematics);multivariate interpolation;robotics;sampling (signal processing);sensor;whittaker–shannon interpolation formula	Young-Ho Kim;Dylan A. Shell;Colin Ho;Srikanth Saripalli	2012		10.1007/978-3-319-00065-7_51	mathematical optimization	Robotics	51.7274284418009	-26.241977835196746	49976
0adf55f5289a47dd919cf970cbc8ed69206832ab	a task-oriented vision system	surf;saliency;top down;embedded system;visual attention	Recently, biologically inspired vision systems have been the focus of intense research effort to emulate the high energy-efficiency, performance and robustness of mammalian vision systems. However, previous vision accelerators have only focused on speeding up computationally intense portions of the system without exploiting effects seen in the human brain that demonstrate the task influence in the vision mechanism. In this paper, we propose a task-oriented two-level vision system which is composed of Saliency and SURF. To the best of our knowledge, our design is the first embedded system that utilizes task influence in the computation of visual attention and recognition. As a result, we show that the new system can achieve at most 12.75% accuracy improvement while saving 25% computation work.	computation;embedded system;speeded up robust features	Yang Xiao;Kevin M. Irick;Jack Sampson;Narayanan Vijaykrishnan;Chuanjun Zhang	2014		10.1145/2591513.2591602	embedded system;computer vision;simulation;surf;computer science;salience;top-down and bottom-up design;computer graphics (images)	Robotics	45.66570134910665	-34.271120403776365	50006
9ab87209ee227fabac2cd7a793257a80519094ea	decentralized target search in topology maps based on weighted least square method		A distributed target search and predictive approach that uses wireless sensor network topology map is presented in this paper. The decision-making and information gathering is distributed, and the target prediction is based on the weighted least square method. The knowledge of target location and moving direction is obtained by time sets recorded by each sensor node. Time set is a collection of time values that describes when the target was detected by a particular node. All the locations are calculated in a topology map, which is an economical alternative to physical map. The results show that the proposed method has been able to search the target, even though the target changes the moving pattern from time to time. In addition, the robot avoids the obstacles and searches for the target in an effective way.	map;network topology;sensor node	Ashanie Gunathillake;Andrey V. Savkin	2017	2017 IEEE 85th Vehicular Technology Conference (VTC Spring)	10.1109/VTCSpring.2017.8108197	wireless sensor network;network topology;computer science;robot kinematics;sensor node;topology;trajectory;least squares	Robotics	53.241913537791824	-27.058004326419244	50040
911130bfc8f2b9f377d8ae716c278e16a9caf058	3-d perception and modeling	modelizacion;telerobotics aerospace robotics intelligent robots laser ranging mobile robots path planning robot vision;vision ordenador;motion level teleoperation 3d perception autonomous robot intelligent autonomous function lunar robotics challenge 3d laser range finder planar surface fast extraction situational awareness improvement path planning mobile robot operation planetary exploration;high resolution;3d point cloud;intelligent robots;surface representation;mobile robot;path planning;motion level teleoperation;plane fitting;space exploration;mobile robots;autonomy;remotely operated vehicles;intelligent robots orbital robotics mobile robots intelligent sensors robot sensing systems moon context modeling stereo vision project management remotely operated vehicles;3 d mapping;laser ranging;three dimensional;computer vision;modelisation;situational awareness improvement;planetary exploration;3d laser range finder;robot vision;laser range finder;planar surface fast extraction;plane fitting space robotics planetary exploration telerobotics autonomy 3 d mapping surface representation;global positioning system;three dimensional displays;intelligent autonomous function;space robotics;aerospace robotics;field of view;situation awareness;telerobotics;3d perception;lunar robotics challenge;vision ordinateur;long range;point cloud;mobile robot operation;modeling;autonomous robot;robot team	In the context of the 2008 Lunar Robotics Challenge (LRC) of the European Space Agency (ESA), the Jacobs Robotics team investigated three-dimensional (3-D) perception and modeling as an important basis of autonomy in unstructured domains. Concretely, the efficient modeling of the terrain via a 3D laser range finder (LRF) is addressed. The underlying fast extraction of planar surface patches can be used to improve situational awareness of an operator or for path planning. 3D perception and modeling is an important basis for mobile robot operations in planetary exploration scenarios as it supports good situation awareness for motion level teleoperation as well as higher level intelligent autonomous functions. It is hence desirable to get long-range 3D data with high resolution, large field of view, and very fast update rates. 3D LRF have a high potential in this respect. In addition, 3D LRF can operate under conditions where standard vision based methods fail, e.g., under extreme light conditions. However, it is nontrivial to transmit the huge amount of data delivered by a 3D LRF to an operator station or to use this point cloud data as basis for higher level intelligent functions. Based on our participation in the LRC of the ESA, it is shown how the huge amount of 3D point cloud data from 3D LRF can be tremendously reduced. Concretely, large sets of points are replaced by planar surface patches that are fitted into the data in an optimal way. The underlying computations are very efficient and hence suited for online computations onboard of the robot.	autonomous robot;computation;darpa grand challenge;esa;image resolution;mobile robot;moon;motion planning;planetary scanner;point cloud;robotics	Andreas Birk;Narunas Vaskevicius;Kaustubh Pathak;Sören Schwertfeger;Jann Poppinga;Heiko Bülow	2009	IEEE Robotics & Automation Magazine	10.1109/MRA.2009.934822	control engineering;mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence	Robotics	52.903906459747226	-37.9024377680044	50048
40c96e1a236089b929bb0d83069b54bdf5fb2eb9	mission planning in unstructured environments: a reinforcement learning approach	robot sensing systems;learning algorithm;learning;learning resource;reinforcement learning;resource management;optimal policy;mission planning;information gathering;decision problem;autonomous agent;parameter tuning;state space;mathematical model;planning;value function;humans;exponential growth;humans planning mathematical model learning resource management robot sensing systems	Mission planning scenarios in robotics typically involve one or more semi-autonomous agents and a human operator. The high-level goal is to find an optimal allocation of agents to tasks. Each individual task or collection of tasks may have subgoals, such as search, localization, or information gathering. Each of these lower-level goals are their own topics, and the high-level goal of task accomplishment can be categorized as a decision problem. The focus of this work is on solving decision problems through reinforcement learning. Tasks are completely parameterized by a minimal set of basis constraints—spatial, temporal, and (agent-task) coupling—which produces a single cost for each agent-task pairing. The cost completely captures the ability of a specific agent to perform a specific task. A novel state representation is presented that mitigates exponential growth of the state space by scaling independently of the spatial dimension. The decision problem is cast as an MDP and an optimal policy is found using Q-learning. Simulation results are presented for a two-agent two-task example, showing convergence of the value function and improved learning over time. To highlight the scalability of the learning algorithm, additional simulations compare the learned policy to a hand-coded greedy policy for varying number of tasks. The learned policy is shown to reliably allocate agents to tasks with minimal parameter tuning and is robust to low-level changes to agent dynamics as well as random agent motion.	2.5d;autonomous robot;bellman equation;categorization;computer multitasking;decision problem;greedy algorithm;high- and low-level;human-based computation;internationalization and localization;mathematical optimization;multi-agent system;q-learning;reinforcement learning;robotics;scalability;semiconductor industry;simulation;state space;time complexity	Brandon Basso;Hugh F. Durrant-Whyte;J. Karl Hedrick	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6160834	planning;multi-task learning;exponential growth;simulation;computer science;state space;artificial intelligence;autonomous agent;resource management;machine learning;decision problem;mathematical model;mathematics;bellman equation;reinforcement learning;q-learning	Robotics	50.01591473072075	-25.60778645656728	50239
7c3ad614589c0ad541869c3ce6dd72905804eaf1	an experimental and theoretical investigation into simultaneous localisation and map building	navegacion;robot movil;estimacion;autonomous vehicle;localization;localizacion;navigation;localisation;robot mobile;estimation;theoretical foundation;lower bound;moving robot;simultaneous localisation and map building	The simultaneous localisation and map building (SLAM) problem asks if it is possible for an autonomous vehicle to start in an unknown location in an unknown environment and then to incrementally build a map of this environment while simultaneously using this map to compute absolute vehicle location. Starting from the estimation-theoretic foundations of this problem developed in [5, 4, 2], this paper proves that a solution to the SLAM problem is indeed possible. The underlying structure of the SLAM problem is first elucidated. A proof that the estimated map converges monotonically to a relative map with zero uncertainty is then developed. It is then shown that the absolute accuracy of the map and the vehicle location reach a lower bound defined only by the initial vehicle uncertainty. Together, these results show that it is possible for an autonomous vehicle to start in an unknown location in an unknown environment and, using relative observations only, incrementally build a perfect map of the world and simultaneously to compute a bounded estimate of vehicle location.	simultaneous localization and mapping	Gamini Dissanayake;Paul Newman;Hugh F. Durrant-Whyte;Steve Clark;M. Csorba	1999		10.1007/BFb0119405	estimation;navigation;simulation;internationalization and localization;control theory;mathematics;upper and lower bounds;statistics	Vision	52.00373280028734	-29.782451713358125	50347
e5284384c072df636eb859f3cf7d3439d308d8c2	continuous-discrete von mises-fisher filtering on s2 for reference vector tracking		This paper is concerned with tracking of reference vectors in the continuous-discrete-time setting. For this end, an Itô stochastic differential equation, using the gyroscope as input, is formulated that explicitly accounts for the geometry of the problem. The filtering problem is solved by restricting the prediction and filtering distributions to the von Mises-Fisher class, resulting in ordinary differential equations for the parameters. A strategy for approximating Bayesian updates and marginal likelihoods is developed for the class of conditionally spherical measurement distributions' which is realistic for sensors such as accelerometers and magnetometers, and includes robust likelihoods. Furthermore, computationally efficient and numerically robust implementations are presented. The method is compared to other state-of-the-art filters in simulation experiments involving tracking of the local gravity vector. Additionally, the methodology is demonstrated in the calibration of a smartphone's accelerometer and magnetometer. Lastly, the method is compared to state-of-the-art in gravity vector tracking for smartphones in two use cases, where it is shown to be more robust to unmodeled accelerations.	algorithmic efficiency;experiment;gyroscope;marginal model;numerical analysis;sensor;simulation;smartphone	Filip Tronarp;Roland Hostettler;Simo Särkkä	2018	2018 21st International Conference on Information Fusion (FUSION)	10.23919/ICIF.2018.8455299	computer science;directional statistics;machine learning;von mises–fisher distribution;artificial intelligence;stochastic differential equation;filter (signal processing);mathematical optimization;accelerometer;ordinary differential equation;gyroscope;filtering problem	Vision	40.53652150632814	-26.941195252976154	50501
a1753fe372f673a4ffb07e9ef8e7a787a5354991	a concurrent spectral-screening pct algorithm for remote sensing applications	imaging spectrometer;image fusion;concurrent algorithm;remote sensing;image quality;principal component transform;color mapping scheme;principal component	The paper presents a concurrent algorithm for remot sensing applications that provides significant performance and image quality enhancements over con ventional uniprocessor PCT techniques. The algorithm combines spectral angle classification, p rincipal component transform, and human centered co lor mapping. It is evaluated from an image quality per s ctive using images collected with the Hyper-spec tral Digital Imagery Collection Experiment (HYDICE) sens or, an airborne imaging spectrometer. These images correspond to foliated scenes taken from an altitud e of 2000 to 7500 meters at wavelengths between 400 m and 2.5 micron. The scenes contain mechanized vehic les sitting in open fields as well as under camoufl age. The algorithm operates with close to linear speedup on shared memory multiprocessors and can be readil y extended to operate on multiple, low-cost PC-style servers connected with high-performance networking. A simple analytical model is outlined that allows the impact on performance of practical, application-sp ecific properties to be assessed. These properties include image resolution, number of spectral bands, increa ses in the number of processors, changes in processor tech nology, networking speeds, and system clock rates.	active sitting;airborne ranger;central processing unit;clock rate;concurrent algorithm;geforce 7 series;image quality;image resolution;naruto shippuden: clash of ninja revolution 3;processor technology;shared memory;spec#;speedup;uniprocessor system	Tiranee Achalakul;Stephen Taylor	2000	Information Fusion	10.1016/S1566-2535(00)00014-2	image quality;computer vision;computer science;image fusion;imaging spectrometer;remote sensing;principal component analysis;computer graphics (images)	Networks	42.1374356430662	-32.882557245905716	50706
64296e23d2d2daf4742c17e475d9c29e6fdf89d9	online data-driven fault detection for robotic systems	hidden markov model;computer model;h670 robotics and cybernetics;online learning;data model;computational modeling;hidden markov models;monitoring;fault detection;robots;service robot;pure data;fault detection robots hidden markov models data models monitoring delay computational modeling;fault model;fault detection and diagnosis;data models	In this paper we demonstrate the online applicability of the fault detection and diagnosis approach which we previously developed and published in [1]. In our former work we showed that a purely data driven fault detection approach can be successfully built based on monitored inter-component communication data of a robotic system and used for a-posteriori fault detection. Here we propose an extension to this approach which is capable of online learning of the fault model as well as for online fault detection. We evaluate the application of our approach in the context of a RoboCup task executed by our service robot BIRON in corporation with an expert user.	algorithm;experiment;fault detection and isolation;fault model;inter-process communication;microsoft outlook for mac;online and offline;sensor;service robot;with high probability	Raphael Golombek;Sebastian Wrede;Marc Hanheide;Martin Heckmann	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6095034	robot;data modeling;simulation;data model;computer science;machine learning;fault model;distributed computing;computational model;fault detection and isolation;hidden markov model	Robotics	48.84832525858267	-25.273029106070283	51564
c0c314c5d653e636b85e457f2797315d46e4d987	image processing units on ultra-low-cost embedded hardware: algorithmic optimizations for real-time performance	embedded vision;image processing units;human tracking	The design and development of image processing units (IPUs) has traditionally involved trade-offs between cost, real-time properties, portability, and ease of programming. A standard PC can be turned into an IPU relatively easily with the help of readily available computer vision libraries, but the end result will not be portable, and may be costly. Similarly, one can use field programmable gate arrays (FPGAs) as the base for an IPU, but they are expensive and require hardware-level programming. Finally, general purpose embedded hardware tends to be under-powered and difficult to develop for due to poor support for running advanced software. In recent years a new option has surfaced: single-board computers (SBCs). These generally inexpensive embedded devices would be attractive as a platform on which to develop IPUs due to their inherent portability and good compatibility with existing computer vision (CV) software. However, whether their performance is sufficient for real-time image processing has thus far remained an open question. Most SBCs (especially the ultra-low-cost ones which we target) do not offer CUDA/OpenCL support which makes it difficult to port GPU-based CV applications. In order to utilize the full power of the SBCs, their GPUs need to be used. In our attempts at doing this, we have observed that the CV algorithms which an IPU uses have to be re-designed according to the OpenGL support available on these devices. This work presents a framework where a selection of CV algorithms have been designed in a way that they optimize performance on SBCs while still maintaining portability across devices which offer OpenGL ES 2.0 support. Furthermore, this paper demonstrates an IPU based on a representative SBC (namely the Raspberry Pi) along with two CV applications backed by it. The robustness of the applications as well as the performance of the IPU are evaluated to show that SPCs can be used to build IPUs capable of producing accurate data in real time. This opens the possibilities of large scale economically deployment of vision system especially in remote and barren lands. Finally, the software developed as a part of this work has been released open source.	embedded system;image processing;real-time transcription	Suraj Nair;Nikhil Somani;Artur Grunau;Emmanuel C. Dean-Leon;Alois Knoll	2018	Signal Processing Systems	10.1007/s11265-017-1267-1	artificial intelligence;robustness (computer science);parallel computing;image processing;real-time computing;software deployment;machine learning;computer hardware;software;machine vision;embedded system;pi;computer science;software portability;cuda	Embedded	41.841763048438	-34.189460001395496	52209
361793989faa1447c195f94a10ad48db0046968c	live demonstration of portable systems based on silicon sensors for the monitoring of physiological parameters of driver drowsiness and pulse wave velocity		In this paper we present a PHYSIO-Sensor based on STM proprietary technology able to accurately reconstruct in a portable format both the PPG (PhotoPlethysmoGraphy) signal and Pulse Wave Velocity (PWV).		Sabrina Conoci;Francesco Rundo;Giorgio Fallica;Davide Lena;Irene Buraioli;D. Demarchi	2018	2018 IEEE Biomedical Circuits and Systems Conference (BioCAS)	10.1109/BIOCAS.2018.8584709	feature extraction;optical filter;computer vision;pulse wave velocity;computer science;electronic engineering;photoplethysmogram;artificial intelligence	EDA	44.46953491199101	-30.334778644964295	52287
307ede7905939cf414a738218dbfc61bfe6ff5ee	user preferred behaviors for robot navigation exploiting previous experiences	navigation;path planning;collision avoidance;trajectory prediction	Industry demands flexible robots that are able to accomplish different tasks at different locations such as navigation and mobile manipulation. Operators often require mobile robots operating on factory floors to follow definite and predictable behaviors. This becomes particularly important when a robot shares the workspace with other moving entities. In this paper, we present a system for robot navigation that exploits previous experiences to generate predictable behaviors that meet user’s preferences. Preferences are not explicitly formulated but implicitly extracted from robot experiences and automatically considered to plan paths for the successive tasks without requiring experts to hard-code rules or strategies. Our system aims at accomplishing navigation behaviors that follow user’s preferences also to avoid dynamic obstacles. We achieve this by considering a probabilistic approach for modeling uncertain trajectories of the moving entities that share the workspace with the robot. We implemented and thoroughly tested our system both in simulation and on a real mobile robot. The extensive experiments presented in this paper demonstrate that our approach allows a robot for successfully navigating while performing predictable behaviors and meeting user’s preferences.	encode;entity;experience;experiment;exploit (computer security);feedback;hard coding;mobile manipulator;mobile robot;robotic mapping;simulation;software factory;workspace	Lorenzo Nardi;Cyrill Stachniss	2017	Robotics and Autonomous Systems	10.1016/j.robot.2017.08.014	operator (computer programming);computer vision;simulation;workspace;social robot;artificial intelligence;mobile robot navigation;mobile robot;motion planning;probabilistic logic;computer science;robot	Robotics	51.97546978721869	-29.219894411866015	52576
0043be96362eefbfaa000f0e6575ef320c972c84	multistatic tracking using bistatic range - range rate measurements	digital video broadcasting;multistatic radar;clutter;multidimensional assignment multistatic tracking passive radar multistatic radar;bioreactors;radar tracking;multitarget tracking;filters;range rate measurement;receivers;passive radar multistatic tracking bistatic radar range rate measurement multidimensional assignment 3d multitarget tracking scenario;radar tracking target tracking passive radar bistatic radar multidimensional systems bioreactors transmitters filters clutter digital video broadcasting;transmitters;target tracking passive radar radar tracking;mathematical model;3d multitarget tracking scenario;passive radar;multistatic tracking;target tracking;bistatic radar;algorithm design and analysis;multidimensional assignment;multidimensional systems	In this paper, we implement a multidimensional assignment based multistatic tracker and test it on a 3D multitarget tracking scenario that includes crossing targets as well as targets moving in formation. We find that the assignment based multistatic tracker can successfully keep tracking initiated tracks accurately. However, efficient track initiation needs further study.	algorithm;bittorrent tracker;cluster analysis;lookup table;map;rough set;signal-to-noise ratio;simulation	Ali Onder Bozdogan;Gokhan Soysal;Murat Efe	2009	2009 12th International Conference on Information Fusion		computer vision;electronic engineering;remote sensing	Robotics	50.01985598531973	-33.39587091668657	52832
ff170795cc2400158a3dd3e261ec642524433d7b	real time simultaneous localization and mapping: towards low-cost multiprocessor embedded systems	signal image and speech processing;circuits and systems;control structures and microprogramming;electronic circuits and devices	Simultaneous localization and mapping (SLAM) is widely used by autonomous robots operating in unknown environments. Research community has developed numerous SLAM algorithms in the last 10 years. Several works have presented many algorithms’ optimizations. However, they have not explored a system optimization from the system hardware architecture to the algorithmic development level. New computing technologies (SIMD coprocessors, DSP, multi-cores) can greatly accelerate the system processing but require rethinking the algorithm implementation. This article presents an efficient implementation of the EKF-SLAM algorithm on a multi-processor architecture. The algorithm-architecture adequacy aims to optimize the implementation of the SLAM algorithm on a low-cost and heterogeneous architecture (implementing an ARM processor with SIMD coprocessor and a DSP core). Experiments were conducted with an instrumented platform. Results aim to demonstrate that an optimized implementation of the algorithm, resulting from an optimization methodology, can help to design embedded systems implementing low-cost multiprocessor architecture operating under real-time constraints. Introduction Autonomous robots must be able to localize themselves. Simultaneous localization and mapping (SLAM) algorithms aim to build an environment map while estimating the robot pose. Many researches were conducted to develop SLAM algorithms like extended Kalman filter for SLAM (EKF-SLAM) [1,2], FAST SLAM [3], GRAPH SLAM [4], DP-SLAM [5] which aim to improve consistency, accuracy or robustness. Other algorithms derivate from the EKF-SLAM, such as algorithms using unscented Kalman filter (UKF) [6] which increases the localization accuracy against the classical EKF algorithm based on a linearizedmodel. Only fewworks deal with the implementation of low-cost SLAM embedded systems. Most of SLAM implementations rely on the use of accurate and dense measurements provided by expensive sensors like laser rangefinder sensors [7] or time of flight cameras [8]. High-priced smart sensors are not suitable to *Correspondence: bastien.vincke@u-psud.fr 1 Univ Paris-Sud, CNRS, Institut d’Electronique Fondamentale, F-91405 Orsay, France Full list of author information is available at the end of the article be integrated inmost of embedded systems in commercial objectives or industrial applications. Simultaneous localization and mapping systems using low-cost sensors have been recently designed. Abrate et al. [9] provide an implementation of the EKF-SLAM algorithm on a Khepera robot. The robot hosts limited range, sparse and noisy IR sensors. Experimental results have shown the importance of the sensor characteristics, the primitives (lines) extraction and data association. Yap and Shelton [10] use cheap, noisy and sparse sonar sensors embedded in a P3-DX robot. To cope with these low-cost sensors, the implemented SLAM algorithm uses a multiscan approach and an orthogonality assumption to map indoor environments. Classical SLAM algorithms are too computationally intensive to run on an embedded computing unit. They require at least laptop-level performances. Gifford et al. [11] present a low-cost approach to autonomous multirobot mapping and exploration for unstructured environments. The robot hosts a Gumstix computing unit (600Mhz), 6 IR scanning range arrays, a 3-axis gyroscope and odometers. Running DP-SLAM alone on the Gumstix with 15 particles takes on average 3 s per update. While © 2012 Vincke et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Vincke et al. EURASIP Journal on Embedded Systems 2012, 2012:5 Page 2 of 14 http://jes.eurasipjournals.com/content/2012/1/5 using 25 particles, it takes more than 10 s per update. Authors have underlined the difficulty to find the right SLAM parameters to fit within the available computing power and the real-time processing. Magnenat et al. [12] present a system based on the co-design of a low-cost sensor (a slim rotating scanner), a SLAM algorithm, a computing unit, and an optimization methodology. The computing unit is based on an ARM processor (533 Mhz) running a FASTSLAM 2.0 algorithm [13]. Magnenat et al. [12] use an evolution strategy to find the best configuration of the algorithm and setting of the parameters. As pointed out by [11,12], the first improvement of a SLAM algorithm is an efficient setting of the various parameters of the algorithm. Other modifications were investigated to reach real-time constraints. These modifications are necessary due to the low computing power and limited memory resources available on embedded systems. Features restriction for EKF-SLAM algorithm has been implemented to decrease the processing time [14]. Schroter et al. [15] focused on reducing the memory footprint of particle-based gridmap SLAM by sharing the map between several particles. Robust laser-based SLAMnavigation has long existed in robot applications, but systems implement sensors that, in some cases, are more expensive than the final product. Neato Robotics has developed a vacuum cleaner that implements a navigation system using a SLAM algorithm. The approach is based on a low-cost system implementing a designed laser rangefinder [16]. This article presents an efficient implementation of the EKF-SLAM algorithm on a multi-processor architecture. The approach is based on an algorithm implementation adequate to a defined architecture. The aim is to optimize the implementation of the SLAM algorithm on a low-cost and heterogeneous architecture implementing an SIMD coprocessor (NEON) and a DSP core. The hardware includes several low-cost sensors. As [17], we chose to use a low-cost camera (exteroceptive sensor) and odometers (proprioceptive sensors). Following [12], we efficiently tune the parameters of the SLAM algorithm. We improve on previous works by proposing an adequate implementation of the EKF-SLAM algorithm on a multiprocessing architecture (ARM processor, SIMD NEON coprocessor, DSP core). The specifications related to the NEON coprocessor and the DSP core improve the processing time and the system performance. Results aim to demonstrate that an optimized implementation of the algorithm, resulting from an evaluation methodology, can help to design embedded systems implementing low-cost multiprocessor architecture operating under real-time constraints. Section “EKF-SLAM algorithm” introduces the EKFSLAM algorithm. Section “Multiprocessor architecture and system configuration” presents the embedded multiprocessor architecture and the system configuration. Section “Evaluation methodology and algorithm implementation” details the evaluation methodology, provides a first algorithm implementation and analyzes this implementation in terms of processing time. A Hardware– software optimization is proposed and analyzed in Section “Hardware–software optimization and improvements”. It presents SIMD optimizations and DSP parallelization. A performance comparison is then performed between the optimized and non-optimized instances. Finally, Section “Conclusion” concludes this article.	arm architecture;autonomous robot;coprocessor;correspondence problem;davis–putnam algorithm;digital signal processor;ekf slam;embedded system;evolution strategy;extended kalman filter;genetic algorithm;laptop;mathematical optimization;memory footprint;multiprocessing;parallel computing;performance;program optimization;real-time clock;real-time transcription;reflection mapping;robotics;simd;sonar (symantec);sensor;simultaneous localization and mapping;sparse matrix;springer (tank);system configuration;vacuum cleaner	Bastien Vincke;Abdelhafid Elouardi;Alain Lambert	2012	EURASIP J. Emb. Sys.	10.1186/1687-3963-2012-5	embedded system;parallel computing;real-time computing;computer science;operating system	Robotics	43.87577740609626	-37.214727936764326	53711
00de4cf0c8ff69484c87313557b5e2e02937a647	firm: feedback controller-based information-state roadmap - a framework for motion planning under uncertainty	history;partially observed markov decision process;uncertainty;bismuth;adaptive control;bismuth planning aerospace electronics markov processes history adaptive control uncertainty;aerospace electronics;markov process;motion planning;planning;markov processes	Direct transformation of sampling-based motion planning methods to the Information-state (belief) space is a challenge. The main bottleneck for roadmap-based techniques in belief space is that the incurred costs on different edges of the graph are not independent of each other. In this paper, we generalize the Probabilistic RoadMap (PRM) framework to obtain a Feedback controller-based Information-state RoadMap (FIRM) that takes into account motion and sensing uncertainty in planning. The FIRM nodes and edges lie in belief space and the crucial feature of FIRM is that the costs associated with different edges of FIRM are independent of each other. Therefore, this construct essentially breaks the “curse of history” in the original Partially Observable Markov Decision Process (POMDP), which models the planning problem. Further, we show how obstacles can be rigorously incorporated into planning on FIRM. All these properties stem from utilizing feedback controllers in the construction of FIRM.	cobham's thesis;curse of dimensionality;feedback;graph (discrete mathematics);markov chain;motion planning;partially observable markov decision process;probabilistic roadmap;sampling (signal processing);statistical relational learning	Ali-akbar Agha-mohammadi;Suman Chakravorty;Nancy M. Amato	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6095010	mathematical optimization;simulation;partially observable markov decision process;adaptive control;engineering;artificial intelligence;control theory;markov process;statistics	Robotics	51.53622581620754	-24.03769914788876	54031
908efd80899743e5b037625b18bf5d4ffa377c3f	exploring unknown environment and map construction using ultrasonic sensing of normal direction of walls	planning algorithm;robot sensing systems;normal wall direction;sensor systems;sensor phenomena and characterization;acoustic wave reflection mobile robots path planning ultrasonic transducers navigation;mobile robot;vector map;path planning;reflection characteristics;robot sensing systems ultrasonic variables measurement sensor phenomena and characterization navigation sensor systems mobile robots intelligent sensors sensor fusion infrared sensors orbital robotics;ultrasonic variables measurement;acoustic wave reflection;map construction;mobile robots;ultrasonic transducers;orbital robotics;navigation;navigation map construction ultrasonic sensing mobile robots reflection characteristics vector map normal wall direction planning algorithm;sensor fusion;ultrasonic sensing;infrared sensors;intelligent sensors;ultrasonic sensor	Ultrasonic sensors are very useful for mobile robots because they have the advantage that the systems are simple and are easy to handle. When a mobile robot is navigated using an ultrasonic sensor, the map of the environment should include information about the reflection characteristics of objects. For this purpose, the map using a vector expression is better than a grid map. In this paper, an environment reconstruction method using a vector map is proposed. In this method, the environment is reconstructed from the data obtained by using our novel ultrasonic sensor which can measure the normal direction of walls, and the next sensing point is determined automatically. The experimental results of a map construction using this algorithm are shown. As a result of the experiments, it is found that the measurement of walls is possible using this method and the effectiveness of the planning algorithm for the sensing point is verified. >		Akihisa Ohya;Yoshiaki Nagashima;Shin'ichi Yuta	1994		10.1109/ROBOT.1994.351251	control engineering;mobile robot;computer vision;electronic engineering;computer science;engineering;artificial intelligence	Robotics	53.14917387504246	-33.878525864927106	54670
cb2ca6c81ecadf711596772a92b3cfd8741adbd1	towards fusing uncertain location data from heterogeneous sources	beads;roadside sensors;uncertainty fusion	Properly incorporating location-uncertainties – which is, fully considering their impact when processing queries of interest – is a paramount in any application dealing with spatio-temporal data. Typically, the location-uncertainty is a consequence of the fact that objects cannot be tracked continuously and the inherent imprecision of localization devices. Although there is a large body of works tackling various aspects of efficient management of uncertainty in spatio-temporal data – the settings consider homogeneous localization devices, e.g., either a Global Positioning System (GPS), or different sensors (roadside, indoor, etc.).In this work, we take a first step towards combining the uncertain location data – i.e., fusing the uncertainty of moving objects location – obtained from both GPS devices and roadside sensors. We develop a formal model for capturing the whereabouts in time in this setting and propose the Fused Bead (FB) model, extending the bead model based solely on GPS locations. We also present algorithms for answering traditional spatio-temporal range queries, as well as a special variant pertaining to objects locations with respect to lanes on road segments – augmenting the conventional graph based road network with the width attribute. In addition, pruning techniques are proposed in order to expedite the query processing. We evaluated the benefits of the proposed approach on both real (Beijing taxi) and synthetic (generated from a customized trajectory generator) data. Our experiments demonstrate that the proposed method of fusing the uncertainties may eliminate up to 26 % of the Research Supported by the NSF grant III 1213038. Research Supported by the NSF grants CNS 0910952 and III 1213038, and ONR grant N00014-14-10215. Bing Zhang bing@u.northwestern.edu Goce Trajcevski goce@eecs.northwestern.edu Liu Liu leoliu@u.northwestern.edu 1 Northwestern University, 2145 Sheridan Road, Evanston, IL 60201, USA November 2015 Accepted: 180 Geoinformatica (2016) 20:179–212 false positives in the Beijing taxi data, and up to 40 % of the false positives in the larger synthetic dataset, when compared to using the traditional bead uncertainty models.	algorithm;cns;database;experiment;fused filament fabrication;global positioning system;ibm notes;internationalization and localization;location-based service;range query (data structures);sensor;synthetic data;synthetic intelligence	Bing Zhang;Goce Trajcevski;Liu Liu	2016	GeoInformatica	10.1007/s10707-015-0238-6	simulation;data mining;cartography	DB	52.7971051699737	-32.477433670273726	54683
d6e7733748c07152f7aa732a94921856c740da3b	vision based navigation system by variable template matching for autonomous mobile robots	fuzzy set theory mobile robots path planning robot vision ccd image sensors;path planning;optimal method;mobile robots;autonomous mobile robot;fuzzy set theory;ccd image sensors;robot vision;evolution strategy;navigation system;binary density projection vision based navigation system variable template matching autonomous mobile robot landmarks evolution strategy;machine vision navigation mobile robots robotics and automation cameras robot sensing systems robot vision systems inspection histograms systems engineering and theory;template matching	We have previously developed a navigation system for autonomous mobile robots. It can find landmarks on the ceiling to move and work. However, it is impossible to recognize landmarks when the size of landmark is different from the size of template. In such cases, the robot cannot continue to work. Therefore we proposed variable template matching (VTM) that makes it possible to recognize landmarks of different sizes. We realize VTM by using the optimization method called the evolution strategy (ES). The robot can recognize the landmark faster, inferring landmark parameters by binary density projection.	autonomous robot;mobile robot;template matching	Yasunori Abe;Masaru Shikano;Toshio Fukuda;Fumihito Arai;Yoshio Tanaka	1998		10.1109/ROBOT.1998.677209	mobile robot;computer vision;simulation;template matching;computer science;engineering;artificial intelligence;social robot;motion planning;robot control;fuzzy set;evolution strategy;mobile robot navigation;personal robot	Robotics	50.97037629142317	-37.47101609975181	54699
777c9ed7b99292da84582a0997f2a55d34e280b5	computation principles for the development of visual skills in robotics	computer vision computation principles visual skills robotics visual behaviors physical interaction navigation vector field visual learning guidance topological navigation sub goal placement obstacle avoidance navigation enhancement;path planning;mobile robots;computer vision;robot vision;obstacle avoidance;visual learning;navigation robustness computer vision mathematics testing robot vision systems neural networks mathematical analysis particle measurements gratings;learning artificial intelligence;vector field;learning artificial intelligence robot vision mobile robots path planning;physical interaction	Different working principles are often considered when different visual behaviors are implemented in an agent. This occurs basically because the physical interaction between the behavior and the environment is not studied in depth. This paper shows how apparently different visual behavior share common theoretical principles for their working mechanism. In particular properties related to the navigation vector field they compute in the environment, provide a base to explain visual learning, guidance, topological navigation, sub goal placement, obstacle avoidance and navigation enhancement. To handle the mathematics of a vector field robust tools are needed. Techniques borrowed from Computer Vision literature provide the necessary mathematical tools. All behaviors described here have been tested in real robots. On going research is still in progress for topological navigation and subgoal placement.	computation;computer vision;fundamental interaction;obstacle avoidance;robot;robotics;visual learning	Giovanni M. Bianco;Paolo Fiorini	2001		10.1109/IROS.2001.977213	mobile robot;computer vision;vector field;simulation;computer science;artificial intelligence;visual odometry;motion planning;obstacle avoidance;mobile robot navigation	Robotics	48.83391383775435	-30.118736908051837	54868
11c040c4dc1a54613841d799335a270449cd09c3	information coding with neural ensembles for a mobile robot	biological neural networks neurons robot kinematics mobile robots associative memory encoding;neural nets;mobile robot;robot navigation;mobile robots;neurocontrollers channel capacity collision avoidance content addressable storage mobile robots neural nets;associative memory information coding neural ensembles mobile robot robot navigation obstacle avoidance neural network information capacity noncorrelated data correlated data robot task;obstacle avoidance;channel capacity;information capacity;associative memory;collision avoidance;neurocontrollers;content addressable storage;correlated data;neural network	For robot navigation (obstacle avoidance) we propose to use special neural network, because of its large information capacity for non correlated data. We prove this feature in contrast for correlated data in the robot task. This information is generated by a simulator and coded into neural ensembles. The coding method allows different parameters with their numeric values to be stored; it also provides similarity for close values and eliminates it in other case. The developed system combines the quality of the neural network as associative memory and the coding method to permit learning from some specific situations. So we prove the system introducing only the situation information and retrieving the appropriate maneuver for it.	artificial neural network;channel capacity;content-addressable memory;hebbian theory;maximal set;mobile robot;neural coding;neural ensemble;numerical analysis;obstacle avoidance;robotic mapping;simulation	Daniel Calderon Reyes;Tatiana Baidyk;Ernst M. Kussul	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033307	mobile robot;computer vision;computer science;artificial intelligence;machine learning;artificial neural network	Robotics	50.47010393290505	-29.24788384111848	54986
6b0b39525fe536c0625163ddb8fb49445cf8af9b	a programmable array of silicon neurons for the control of legged locomotion	cmos integrated circuits;vlsi;legged locomotion;motion control;neural chips;programmable circuits;programmable logic arrays;silicon;if neurons;vlsi cpg emulator chip;artificial neural networks;central pattern generator;chip architecture;legged locomotion;muscles;natural locomotory systems;neural circuits;oscillators synchronization;oscillatory periodic waveforms;programmable silicon neuron array;robot limbs;silicon neurons coupling;spinal cord	The biological foundation of most natural locomotory systems is the central pattern generator (CPG). The CPG is a set of neural circuits found in the spinal cord, arranged to produce oscillatory periodic waveforms that activate muscles in a coordinated manner. A 2nd generation VLSI CPG emulator chip - with more and improved neurons, enhanced flexibility, and a higher degree of programmability - has been developed to synchronize oscillators with different frequencies and phases, also produced by the chip, through the coupling of integrate-and-fire (IF) silicon neurons. These oscillators are then used to control the movement of robot's limbs by using the IF neurons to set a specific phase difference between the oscillators. The chip's architecture is examined in detail, and the construction and implementation of the artificial neural networks that produce the waveforms required for locomotion is described.	artificial neural network;biological neuron model;central pattern generator;emulator;neural oscillation;very-large-scale integration	Francesco Tenore;Ralph Etienne-Cummings;M. Anthony Lewis	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)		control engineering;embedded system;electronic engineering;computer science;engineering;electrical engineering;machine learning;very-large-scale integration;oscillation;artificial neural network	Arch	42.59006004663118	-27.218061731870954	55093
532837040f8c3178a373418a48f7a1efd9876249	image processing architectures for binary morphology and labeling	elektroteknik och elektronik;real time;binary image processing;morphology;hardware architectures;labeling	Conventional surveillance systems are omnipresent and most are still based on analog techniques. Migrating to the digital domain grants access to the world of digital image processing enabling automation of such systems, which means extracting information from the image stream without human interaction. The resolution, frame rates, and functionality in these systems are continuously increasing alongside the number of video streams to be processed. The sum of all these parameters imposes high data rates and memory bandwidths which are impossible to handle in pure software solutions. Therefore, accelerating key operations and complex repetitive calculations in dedicated hardware architectures is crucial to sustain real-time performance in future advanced high resolution and frame rate systems. To achieve this goal, this thesis presents four architectures of hardware accelerators to be used in real-time embedded image processing systems, implemented as an FPGA or ASIC. Two morphological architectures performing binary erosion or dilation, with low complexity and low memory requirement, have been developed. One supports static, and the other locally adaptive flat rectangular structuring elements of arbitrary size. Furthermore, a high-throughput architecture calculating the distance transform has also been developed. This architecture supports either the city-block or chessboard distance metric and is based on adding the result of parallel erosions. The fourth architecture performs connected component labeling based on contour tracing and supports feature extraction. A modified version of the morphological architecture supporting static structuring elements, as well as the labeling architecture, has been successfully integrated into a prototype of an automated digital surveillance system for which implementation aspects are presented. The system has been implemented and is running on an FPGA based development board using a CMOS sensor for image acquisition. The prototype currently has segmentation, filtering, and labeling accelerated in hardware, and additional image processing performed in software running on an embedded processor.	application-specific integrated circuit;cmos;computer and network surveillance;connected component (graph theory);connected-component labeling;digital image processing;dilation (morphology);distance transform;embedded system;feature extraction;field-programmable gate array;hardware acceleration;high-throughput computing;image resolution;image segmentation;mathematical morphology;microprocessor development board;prototype;real-time clock;real-time transcription;streaming media;throughput	Hugo Hedberg	2008			embedded system;real-time computing;computer science;theoretical computer science;digital image processing	EDA	44.27233968994715	-35.071664774176874	55101
89bc38dfff0dc73798f54ba9f9507ba041f806db	planning and executing optimal non-entangling paths for tethered underwater vehicles		In this paper, we present a method to improve the navigation of tethered underwater vehicles by computing optimal paths that prevent their tethers from becoming entangled in obstacles. To accomplish this, we define the Non-Entangling Travelling Salesperson Problem (NE-TSP) as an extension of the Travelling Salesperson Problem with a non-entangling constraint. We compute the optimal solution to the NE-TSP by constructing a Mixed Integer Programming model, leveraging homotopy augmented graphs to plan an optimal trajectory through a set of inspection points, while maintaining a non-entangling guarantee. To avoid the computational expense of computing an optimal solution to the NE-TSP, we also introduce several methods to compute near-optimal solutions. In a set of simulated trials, our method was able to plan optimal non-entangling paths through a variety of environments. These results were then validated in a set of pool and field trials using a Seabotix vLBV300 underwater vehicle. The paths generated by our method were then compared to human-generated paths.	analysis of algorithms;anytime algorithm;approximation;computation;greedy algorithm;heuristic;integer programming;linear programming;planning;programming model;shortest path problem;simulated annealing;simulation;solver;time complexity;travelling salesman problem	Seth McCammon;Geoffrey A. Hollinger	2017	2017 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2017.7989349	control engineering;engineering;homotopy;mathematical optimization;trajectory;integer programming;graph	Robotics	53.65728200425879	-24.504473463197293	55164
365f61956def1bb4efd536a87f41813e9e0fb691	where is your dive buddy: tracking humans underwater using spatio-temporal features	underwater robots;underwater tracking humans target tracking motion detection mobile robots propulsion video sequences orbital robotics fourier transforms frequency;video sequence;undulating flipper motion dive buddy tracking humans underwater spatio temporal features underwater robots mobile targets tracking periodic motion propulsion underwater human swimmers kicking local amplitude spectra video sequence fourier transform low frequency oscillations;fourier transform;undulating flipper motion;low frequency;dive buddy;low frequency oscillation;mobile robots;tracking humans underwater;low frequency oscillations;target tracking fourier transforms mobile robots robot vision;robot vision;local amplitude spectra;mobile targets tracking;fourier transforms;spatio temporal features;field of view;propulsion underwater;periodic motion;human swimmers kicking;target tracking	We present an algorithm for underwater robots to track mobile targets, and specifically human divers, by detecting periodic motion. Periodic motion is typically associated with propulsion underwater and specifically with the kicking of human swimmers. By computing local amplitude spectra in a video sequence, we find the location of a diver in the robot's field of view. We use the Fourier transform to extract the responses of varying intensities in the image space over time to detect characteristic low frequency oscillations to identify an undulating flipper motion associated with typical gaits. In case of detecting multiple locations that exhibit large low-frequency energy responses, we combine the gait detector with other methods to eliminate false detections. We present results of our algorithm on open-ocean video footage of swimming divers, and also discuss possible extensions and enhancements of the proposed approach for tracking other objects that exhibit low- frequency oscillatory motion.	algorithm;kadir–brady saliency detector;robot combat;sensor	Junaed Sattar;Gregory Dudek	2007	2007 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2007.4399527	fourier transform;computer vision;simulation;engineering	Robotics	48.23010788664064	-37.46846931646225	55423
5d2d58c63fd2d6dbdb643245c45a3e8a91a9f8dd	algorithms and data structures for computer topology	topology;image numerique;topological space;image processing;topologie;homeomorphisme;procesamiento imagen;traitement image;computer graphic;topologia;homeomorfismo;estructura datos;imagen numerica;homeomorphism;structure donnee;digital image;data structure;algorithms and data structure;computational topology	"""The paper presents an introduction to computer topology with applications to image processing and computer graphics. Basic topological notions such as connectivity, frontier, manifolds, surfaces, combinatorial homeomorphism etc. are recalled and adapted for locally finite topological spaces. The paper describes data structures for explicitly representing classical topological spaces in computers and presents some algorithms for computing topological features of sets. Among them are: boundary tracing (n=2,3), filling of interiors (n=2,3,4), labeling of components, computing of skeletons and others. Introduction: Topology and Computers Topology plays an important role in computer graphics and image analysis. Connectedness, boundaries and inclusion of regions are topological features which are important for both rendering images and analyzing their contents. Computing these features is one of the tasks of the computer topology. We use this term rather than """"computational topology"""" since our approach is analogous to that of digital geometry rather than to that of computational geometry: we are using models of topological spaces explicitly representing each element of a finite topological space as an element of the computer memory, defined by its integer coordinates. The other possible approach would be to think about the Euclidean space, to define objects by equations and inequalities in real coordinates and to approximate real coordinates on a computer by floating point variables. Computer topology may be of interest both for computer scientists who attempt to apply topological knowledge for analyzing digitized images, and for mathematicians who may use computers to solve complicated topological problems. Thus, for example, essential progress in investigating three-dimensional manifolds has been reached by means of computers (see e.g. [14]). Topological ideas are becoming increasingly important in modern theoretical physics where attempts to develop a unique theory of gravitation and quantum mechanics have led to the Topological Quantum Field Theory (see e.g. [2,13]), in which topology of multi-dimensional spaces plays a crucial role. This is one more possible application field for computer topology. Thus, computer topology is important both for applications in computer imagery and in basic research in mathematics and physics."""	approximation algorithm;boundary tracing;computational geometry;computational topology;computer graphics;computer memory;computer scientist;data structure;digital geometry;image analysis;image processing;quantum field theory;quantum mechanics	Vladimir Kovalevsky	2000		10.1007/3-540-45576-0_3	topological dynamics;topological vector space;category of topological spaces;combinatorics;discrete mathematics;computational topology;topology;data structure;image processing;computer science;compact-open topology;mathematics;topological space;general topology;digital image;finite topological space;homeomorphism;digital topology	Theory	44.765847732837486	-27.138828172514444	55501
2e2579dfaeb8906e4219271be8c7d54102eb7228	biologically inspired visual odometry based on the computational model of grid cells for mobile robots	biological system modeling;navigation;visualization;computational modeling;simultaneous localization and mapping;neurons	Visual odometry is a core component of many visual navigation systems like visual simultaneous localization and mapping (SLAM). Grid cells have been found as part of the path integration system in the rat's entorhinal cortex, and they provide inputs for place cells in the rat's hippocampus. Together with other cells, they constitute a positioning system in the brain. Some computational models of grid cells based on continuous attractor networks have also been proposed in the computational biology community, and using these models, self-motion information can be integrated to realize dead-reckoning. However, so far few researchers have tried to use these computational models of grid cells directly in robot visual navigation in the robotics community. In this paper, we propose to apply continuous attractor network model of grid cells to integrate the robot's motion information estimated from the vision system, so a biologically inspired visual odometry can be realized. The experimental results show that good dead-reckoning can be achieved for different mobile robots with very different motion velocities using our algorithm. We also implement a full visual SLAM system by simply combining the proposed visual odometry with a quite direct loop closure detection derived from the well-known RatSLAM, and comparable results can be achieved in comparison with RatSLAM.	algorithm;computation;computational biology;computational model;dead reckoning;experiment;matlab;machine vision;mobile robot;network model;positioning system;real-time clock;robotics;simulation;simultaneous localization and mapping;visual odometry;whole earth 'lectronic link	Huimin Lu;Junhao Xiao;Lilian Zhang;Shaowu Yang;Andreas Zell	2016	2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2016.7866387	computer vision;navigation;simulation;visualization;computer science;artificial intelligence;visual odometry;computational model;simultaneous localization and mapping	Robotics	49.905129049955484	-31.09473972722678	55757
dbf2eed77e507039497d63d45608a8329c7d1ee3	reaction-diffusion based computational model for autonomous mobile robot exploration of unknown environments			autonomous robot;computation;computational model;mobile robot	Alejandro Vázquez-Otero;Jan Faigl;Natividad Duro;Raquel Dormido	2014	IJUC		reaction–diffusion system;computer vision;mobile robot;artificial intelligence;computer science	Robotics	53.28609835857327	-28.38099620112233	55865
cadbb6f353e2233fb233053f9877f420951ac4f9	a genetic algorithm for mobile robot localization using ultrasonic sensors	range data;non linear filtering;mobile robot;optimal method;localization;mobile robots;autonomous mobile robot;local system;real world application;ultrasonic sensors;position estimation;mobile robot localization;genetic algorithm;genetic algorithms;navigation system;ultrasonic sensor	A mobile robot requires the perception of its local environment for position estimation. Ultrasonic range data provide a robust description of the local environment for navigation. This article presents an ultrasonic sensor localization system for autonomous mobile robot navigation in an indoor semi-structured environment. The proposed algorithm is based upon an iterative nonlinear filter, which utilizes matches between observed geometric beacons and an a-priori map of beacon locations, to correct the position and orientation of the vehicle. A non-linear filter based on a genetic algorithm as an emerging optimization method to search for optimal positions is described. The resulting self-localization module has been integrated successfully in a more complex navigation system. Experiments demonstrate the effectiveness of the proposed method in real world applications.	autonomous robot;experiment;genetic algorithm;iterative method;mathematical optimization;mobile robot;nonlinear system;robotic mapping;semiconductor industry;sensor	Luis Moreno;Jose M. Armingol;Santiago Garrido;Arturo de la Escalera;Miguel Angel Salichs	2002	Journal of Intelligent and Robotic Systems	10.1023/A:1015664517164	control engineering;mobile robot;monte carlo localization;computer vision;simulation;genetic algorithm;computer science;engineering;artificial intelligence;ultrasonic sensor;mobile robot navigation	Robotics	53.456197224220254	-33.50108949564377	56052
6c6f3cd37f36af373a89852510aec16ac48994c4	wifi localization for mobile robots based on random forests and gplvm	wifi slam;gplvm;mobile robots;khepera wifi slam mobile robots random forests gplvm;random forests;khepera iii mobile robot wifi localization random forests gplvm wifi networks wifi signals mobile device location indoor environments mobile robot location random forests algorithm regression techniques classification techniques cooperative supervised localization models training data gaussian process latent variable model subjective localization models;wireless lan control engineering computing gaussian processes mobile robots regression analysis;khepera	The proliferation of WiFi networks has attracted many research communities to employ WiFi signals in estimating the location of mobile devices in indoor environments. In this paper, we propose a localization framework that is capable of determining the location of mobile robots in indoor limited areas. The proposed framework exploits the random forests algorithm in both classification and regression techniques, which are used to build cooperative supervised localization models. The localization models are trained offline based on training data that contains measurements of WiFi signal strengths and the location of these measurements. We also propose an extension of our framework using the gaussian process latent variable model (GPLVM), which gives our framework the ability to build subjective localization models which do not require any prior knowledge about ground truth on the localization place. Our experimental evaluation of the proposed framework using the Khepera III mobile robot in one test bed show that it gives high accuracy, where the calculated mean localization error is ±36 cm.	algorithm;gaussian process;ground truth;khepera mobile robot;latent variable model;mobile device;online and offline;random forest;testbed	Reda Elbasiony;Walid Gomaa	2014	2014 13th International Conference on Machine Learning and Applications	10.1109/ICMLA.2014.42	mobile robot;random forest;embedded system;simulation;computer science;machine learning	Robotics	49.956090603671875	-34.68609496437685	56240
6f7378ff84f15ac8314e676f422ddc5019f7ca29	bio-inspired embedded vision system for autonomous micro-robots: the lgmd case	g400 computer science;collision avoidance cameras machine vision visualization neurons robot vision systems;h671 robotics;low cost bio inspired lgmd collision avoidance embedded system autonomous robot	In this paper, we present a new bio-inspired vision system embedded for micro-robots. The vision system takes inspiration from locusts in detecting fast approaching objects. Neurophysiological research suggested that locusts use a wide-field visual neuron called lobula giant movement detector (LGMD) to respond to imminent collisions. In this paper, we present the implementation of the selected neuron model by a low-cost ARM processor as part of a composite vision module. As the first embedded LGMD vision module fits to a micro-robot, the developed system performs all image acquisition and processing independently. The vision module is placed on top of a micro-robot to initiate obstacle avoidance behavior autonomously. Both simulation and real-world experiments were carried out to test the reliability and robustness of the vision system. The results of the experiments with different scenarios demonstrated the potential of the bio-inspired vision system as a low-cost embedded module for autonomous robots.	arm architecture;autonomous robot;biological neuron model;british informatics olympiad;collision detection;embedded system;experiment;fits;microbotics;motion detector;obstacle avoidance;robotics;sensor;simulation	Cheng Hu;Farshad Arvin;Caihua Xiong;Shigang Yue	2017	IEEE Transactions on Cognitive and Developmental Systems	10.1109/TCDS.2016.2574624	computer science;robot;robustness (computer science);autonomous robot;machine vision;computer vision;obstacle avoidance;artificial intelligence;arm architecture	Robotics	46.39995854785245	-33.92896614474469	56837
08e3d7d06965bc16433d81e30865034b8f31124d	mapping and pursuit-evasion strategies for a simple wall-following robot	modelizacion;bucle cerrado;robot movil;graph theory;simultaneous localization and mapping slam;sensing;comptage;windings;winding number;text;motion control;relation ordre partiel;sensors;clocks;simultaneous localization and mapping slam exploration information spaces minimal sensing pursuit evasion;tactile sensor;mobile robot;efecto pared;informacion incompleta;path planning;localization;slam;path winding number;wall effect;information space;mobile robots;localizacion;robotics;cartographie;contaje;pursuit evasion;wall following robot;juego persecucion evasion;modelisation;captador medida;slam robots graph theory mobile robots motion control path planning sensors;information spaces;wall following motion execution;incomplete information;cartografia;measurement sensor;capteur mesure;localisation;robot mobile;sensor;partial ordering;windings clocks mobile robots tactile sensors;simultaneous localization and mapping;information incomplete;closed loop;minimal sensing;counting;cut ordering;exploration;robotica;tactile sensors;cartography;jeu poursuite evasion;mapping;effet paroi;robotique;relacion orden parcial;boucle fermee;modeling;slam robots;pursuit evasion strategy;moving robot;slam pursuit evasion strategy wall following robot sensor wall following motion execution path winding number combinatorial map cut ordering simultaneous localization and mapping;pursuit evasion game;combinatorial map	This paper defines and analyzes a simple robot with local sensors that moves in an unknown polygonal environment. The robot can execute wall-following motions and can traverse the interior of the environment only when following parallel to an edge. The robot has no global sensors that would allow precise mapping or localization. Special information spaces are introduced for this particular model. Using these, strategies are presented to solve several tasks: 1) counting vertices, 2) computing the path winding number, 3) learning a combinatorial map, which is called the cut ordering, that encodes partial geometric information, and 4) solving pursuit-evasion problems.	combinatorial map;evasion (network security);internationalization and localization;pursuit-evasion;robot;sensor;traverse	Max Katsev;Anna Yershova;Benjamín Tovar;Robert Ghrist;Steven M. LaValle	2011	IEEE Transactions on Robotics	10.1109/TRO.2010.2095570	mobile robot;computer vision;simulation;computer science;sensor;artificial intelligence;graph theory;mathematics;robotics;tactile sensor	Robotics	52.22453679214776	-29.412397482108442	57178
7bd4825d6b6a812ac33d8cdfa6e6a2d61cf90aa2	metrics for feature-aided track association	map;sensor systems;sensor phenomena and characterization;probability;radar tracking;sensor model;feature aided tracking;metrics;track fusion;maximum likelihood estimation;state estimation;kinematics;track to track association hypothesis;target tracking maximum likelihood estimation probability sensor fusion state estimation;target density;feature aided tracking tracking track association metrics;track association;probability distribution;target kinematic data;taxonomy;fusion power generation;sensor models;feature aided track association;sensor fusion;target model;target tracking;association metric;sensors network;target tracking radar tracking kinematics sensor phenomena and characterization state estimation sensor systems sensor fusion fusion power generation taxonomy probability distribution;sensor models feature aided track association track fusion sensors network association metric track to track association hypothesis target kinematic data target density maximum a posteriori probability approach map target model;maximum a posteriori probability approach;tracking	Track fusion over a network of sensors requires association of the tracks before the state estimates can be combined. Track association generally involves two steps: evaluating an association metric to score each track-to-track association hypothesis, and selecting the best assignment between two sets of tracks. In many applications feature-aided track association can provide better performance than association with only kinematic data (e.g., position and velocity) when the target density is high. This paper develops a general association metric to support feature-aided track association that considers similarity in both the feature and kinematic domains. The association metric is based upon the maximum a posteriori probability (MAP) approach and can be used for general target and sensor models. Special forms of the association metric are given for some common situations. Numerical results illustrate the performance of different feature association metrics	numerical method;sensor;velocity (software development)	Chee-Yee Chong;Shozo Mori	2006	2006 9th International Conference on Information Fusion	10.1109/ICIF.2006.301700	computer vision;joint probabilistic data association filter;geography;machine learning;pattern recognition	Vision	51.32107762118806	-34.26232136395353	57382
c8143ddda0632dc14686acd6c9f504296eaa303e	real-time path planning to dispatch a mobile sensor into an operational area		Abstract This paper addresses problems of large planning time and cost uncertainty for informative path planning of a mobile sensor where the location of sensor deployment is different of that of an operational area. The first problem is that the cost has no term dependent on sensor state before arriving at the operational area and it causes large planning time. The information of the state of interest dissipates over time during the planning time and it degrades performance of sensing operation. The other problem is that the cost is dependent on the parameters to be estimated. To assess the cost, the target state in the future should be predicted by integrating the system model based on noisy initial estimate. The limitation of the informative path planning has a greater impact on performance in this specific problem. A strategy to cope with these problems is to devise a real-time path planning algorithm by using online optimization. The proposed algorithm is divided into two phases; determining the path to the boundary of the operational area and guiding the sensor by an informative potential field in the area. Detailed analysis on performance of the proposed algorithm compared to an optimal solution by nonlinear programming is given. The simulation results have demonstrated that the proposed algorithm can cope with performance degradation observed in the optimal solution.	dynamic dispatch;motion planning;real-time path planning;real-time transcription	Youngjoo Kim;Wooyoung Jung;Hyochoong Bang	2019	Information Fusion	10.1016/j.inffus.2018.01.010	machine learning;real-time path planning;nonlinear programming;software deployment;real-time computing;artificial intelligence;mathematics;system model;online optimization;motion planning	Robotics	52.638173165451015	-26.58415534575674	57575
3ab933d2b2fae937eb77376e31ef55252b7d4609	an improved parallel mems processing-level simulation implementation using graphic processing unit	mems;gpu;cuda;micro electro mechanical system;3d model;processing level simulation;parallel;graphic processing unit	Micro-Electro–Mechanical System (MEMS) is the integration of mechanical elements, sensors, actuators, and electronics on a common silicon substrate through micro fabrication technology. With MEMS technologies, micron-scale sensors and other smart products can be manufactured. Because of its micron-scale, MEMS products’ structure is nearly invisible, even the designer is hard to know whether the device is well-designed and well-produced. So a visual 3D MEMS simulation implement, named ZProcess[1], was proposed in our previous work to help designers realizing and improving their designs. ZProcess shows the MEMS device’s 3D model using voxel method. It’s accurate, but its speed is unacceptable when the scale of voxel-data is large. In this paper, an improved parallel MEMS simulation implementation is presented to accelerate ZProcess by using GPU (Graphic Processing Unit). The experimental results show the parallel implement gets maximum 160 times speed up comparing with the sequential program.	algorithm;big data;computational complexity theory;computer-aided design;graphics processing unit;gyroscope;microelectromechanical systems;run time (program lifecycle phase);sensor;simulation;smart products;speedup;voxel	Yupeng Guo;Xiaoguang Liu;Gang Wang;Fan Zhang;Xin Zhao	2010		10.1007/978-3-642-13136-3_30	embedded system;parallel computing;computer hardware;computer science;operating system;parallel;microelectromechanical systems;computer graphics (images)	EDA	43.44226278817854	-32.60866291606176	57736
7f49535e716124ed92c4ce3a55843bd12753ecb8	degree of familiarity art2 in fuzzy fusion landmine detection	fuzzy rule based system;real time fuzzy rule based system art2 familiarity degree fuzzy fusion landmine detection self organising network fuzzy output value multilayer perceptron fuzzy rule based fusion technique processed polarisation resolved image multispectral bands textural bands updated alternative architecture real time landmine detection;image processing;landmine detection;landmine detection fuzzy systems australia polarization image resolution real time systems intelligent systems multilayer perceptrons fuzzy neural networks weapons;real time;long term memory;multilayer perceptron;buried object detection;fuzzy logic;civil engineering computing;fuzzy rule base;self organising feature maps;real time systems weapons civil engineering computing buried object detection art neural nets self organising feature maps fuzzy logic image processing sensor fusion;art neural nets;sensor fusion;weapons;real time systems	alarms [l]. Modern landmines [2]are encased in moulded plastic and contain practically no metal. Electro-optics, which cover the spectral range from ultraviolet to the mid-infrared has been identified as having good potential for investigations. The self organising network A R E has been modijied to provide a fuzzy output value, which indicated the degree of familiarity of a new analogue input pattern to previous patterns stored in the long term memory weights of the network. The outputs of the multilayer perceptron and this modification output of ART;’ allows it to now provide an analogue value to a fuzzy rule-based fusion technique which also uses a processed polarisation resolved image as its third input. In real-time these two classifier outputs indicate the likelihood of a surjace landmine target when presented with a number of multispectral and textural bands. Due to the modifications in ART2 this updated alternative architecture (to that of a previous network in reference [3] ) has improved real-time landmine detection capabilities although the registration of all bands is more critical to the accuracy of results in this case. The real-time fuzzy rule based system has detected two of the 3 landmines and the landmine surrogate with 2 false alarms.	fuzzy rule;logic programming;multilayer perceptron;multispectral image;parameter (computer programming);real-time clock;rule-based system;self-organization	Arthur Filippidis;Peter Lozo;Lakhmi C. Jain	1998		10.1109/KES.1998.725934	computer vision;engineering;artificial intelligence;machine learning	ML	51.25641756004201	-33.27026290901442	58205
2a992664c72925e36e7244b3e752accdb800d458	loop closing in topological maps	robot sensing systems;robot sensing systems pollution measurement sensor arrays mobile robots computer science economic indicators navigation hazards length measurement;mobile robot;dempster shafer theory of evidence;hazard assessment;pollution measurement;hazards;false alarm rate;mobile robots;length measurement;indexing terms;navigation;computer science;sensor arrays;economic indicators	In order to create consistent maps of unknown environments, a robot must be able to recognize when it has returned to a previously visited place. In this paper, we introduce an evidential approach to the loop-closing problem for topological maps, based on the Dempster-Shafer theory of evidence. In our approach, the robot makes a hypothesis whenever it may have revisited a place. It then attempts to verify hypotheses by continuing to traverse the environment, gathering evidence that supports (or refutes) the hypotheses. We describe methods for managing belief about multiple loop-closing hypotheses, and for determining a belief assignment given a piece of evidence. We also discuss methods for reducing the false alarm rate of our loop-closing algorithm, and provide simulated and real-world experimental results that verify the effectiveness of our approach.	algorithm;closing (morphology);map;robot;traverse	Kristopher R. Beevers;Wesley H. Huang	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570792	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;mobile robot navigation	Robotics	52.67520134072165	-34.2523207709213	58437
6f060254c0b56912bd590a80ce686a796ea1a999	synchrony in metapopulations with sporadic dispersal	stochastic dispersal;tritrophic models;synchronization	We study synchronization in ecological networks under the realistic assumption that the coupling among the patches is sporadic/stochastic and due to rare and short-term meteorological conditions. Each patch is described by a tritrophic food chain model, representing the producer, consumer, and predator. If all three species can migrate, we rigorously prove that the network can synchronize as long as the migration occurs frequently, i.e. fast compared to the period of the ecological cycle, even though the network is disconnected most of the time. In the case where only the top trophic level (i.e. the predator) can migrate, we reveal an unexpected range of intermediate switching frequencies where synchronization becomes stable in a network which switches between two nonsynchronous dynamics. As spatial synchrony increases the danger of extinction, this counterintuitive effect of synchrony emerging from slower switching dispersal can be destructive for overall metapopulation persistence, presumably expected from switching between two dynamics which are unfavorable to extinction.	food chain;network switch;persistence (computer science);trophic function	Russell Jeter;Igor Belykh	2015	I. J. Bifurcation and Chaos	10.1142/S0218127415400027	synchronization;simulation	Metrics	41.92246225575147	-25.20932675940773	58722
0d08a4302837b96a60c087f9dd17c6597a19cbf9	multi-sensor fusion using evidential slam for navigating a probe through deep ice	slam;evidence theory;navigation;mapping;mulit sensor fusion	We present an evidential multi-sensor fusion approach for navigating a maneuverable ice probe designed for extraterrestrial sample analysis missions. The probe is equipped with a variety of sensors and has to estimate its own position within the ice as well as a map of its surroundings. The sensor fusion is based on an evidential SLAM approach which produces evidential occupancy grid maps that contain more information about the environment compared to probabilistic grid maps. We describe the different sensor models underlying the algorithm and we present empirical results obtained under controlled conditions in order to analyze the effectiveness of the proposed multi-sensor fusion approach. In particular, we show that the localization error is significantly reduced by combining multiple sensors.		Joachim Clemens;Thomas Reineking	2014		10.1007/978-3-319-11191-9_37	computer vision;geography;cartography;remote sensing	Robotics	53.39727689956841	-35.03580145805996	58771
a3d9e0f56c00d30758ac0d0bedc3086007115077	perceptual system for intelligent service robot by using a three-dimensional range camera	manipulators;human vision;information extraction;neural nets;intelligent robots;three dimensional range camera;information extraction method;spiking neural network perceptual system intelligent service robot three dimensional range camera human visual perception autonomous robot human vision ecological psychology human retinal structure information extraction method;intelligent systems intelligent robots service robots robot vision systems smart cameras humans intelligent sensors robot sensing systems retina neural networks;service robots;service robots intelligent robots mobile robots neural nets robot vision;mobile robots;ecological psychology;data mining;perceptual system;three dimensional;distance measurement;spiking neural network;robot vision;retina;robots;service robot;human retinal structure;humans;neurons;perception and action;intelligent service robot;autonomous robot;human visual perception	This paper discusses a perceptual system for a intelligent service robot from the viewpoint of human visual perception. Recently, various sensors were equipped with an autonomous robot enable to get too much information on the environment. However, the robot must perceive the necessary information from too much information to take a flexible action like a human. In this study, we emphasize the importance of human vision for the robot to realize perception and action flexibility. Especially, we focus on the perceptual system based on perceiving-acting cycle discussed in ecological psychology. First, we propose a retinal model for a 3D-range camera based on human retinal structure, and the information extraction method using a spiking neural network based on perceiving-acting cycle. Next, we apply the proposed method for a task of clearing the table. As an experimental result, we show the proposed method can directly perceive the necessary information by the attention mechanism for the flexible perception according to spatiotemporal context based on the spiking neural network.	artificial neural network;autonomous robot;ecological psychology;experiment;information extraction;invariant (computer science);parabolic antenna;plasma cleaning;range imaging;robotic arm;sensor;service robot;spiking neural network	Hiroyuki Masuta;Naoyuki Kubota	2009	2009 IEEE Workshop on Robotic Intelligence in Informationally Structured Space	10.1109/RIISS.2009.4937916	computer vision;simulation;computer science;artificial intelligence;social robot	Robotics	47.974280892642255	-31.758431813420597	58794
71b887a49754fa04eccbfc48020a419980f9e08d	invariant filtering for simultaneous localization and mapping	recursive estimation;recursive estimation mobile robots filtering theory spatial variables measurement;map building;mobile robot;spatial variables measurement;filtering simultaneous localization and mapping stochastic processes mobile robots recursive estimation robot kinematics uncertainty nonlinear filters position measurement extraterrestrial measurements;mobile robots;simultaneous localization and mapping;spatial relationships invariant filtering simultaneous localization simultaneous mapping map building mobile robot recursive feature estimation;filtering theory	This paper presents an algorithm for simultaneous localization and map building for a mobile robot moving in an unknown environment. The robot can measure only the bearings to identi able targets and its own relative motion. The approach is to recursively estimate features of the environment which are invariant to the robot pose in order to decouple the pose error from the map error. The highly nonlinear nature of this problem requires more explicit reasoning about the spatial relationships between landmarks and between the robot and landmarks than those used in previous methods.	algorithm;computation;coupling (computer programming);greedy algorithm;map;mobile robot;nonlinear system;recursion;requirement;scalability;simultaneous localization and mapping;triangulation (geometry)	Matthew Deans;Martial Hebert	2000		10.1109/ROBOT.2000.844737	mobile robot;monte carlo localization;computer vision;computer science;artificial intelligence;machine learning;control theory;mathematics	Robotics	52.55322618051955	-35.27218809985856	58833
bce24b2e15c02aa2c18518f749b99f3e56e315d4	exploiting environmental computation in a multi-agent model of slime mould		Very simple organisms, such as the single-celled amoeboid slime mould Physarum polycephalum possess no neural tissue yet, despite this, are known to exhibit complex biological and computational behaviour. Given such limited resources, can environmental stimuli play a role in generating the complexity of slime mould’s behaviour? We use a multi-agent collective model of slime mould to explore a two-way mechanism where the collective’s behaviour is influenced by simulated chemical concentration gradient fields and, in turn, this behaviour alters the spatial pattern of the concentration gradients. This simple mechanism yields complex behaviour amid the dynamically changing gradient profiles and suggests how the apparently `intelligent’ response of the slime mould could possibly be due to outsourcing of computation to the environment. The true slime mould Physarum polycephalum is a single-celled organism possessing no nervous system yet (in its vegetative plasmodium stage) exhibits a complex range of biological and computational behaviours (for an overview of its abilities see [1]). Plasmodium of P. polycephalum is comprised of an adaptive gel/sol transport network. The ectoplasmic gel phase is composed of a sponge-like matrix of actin and myosin fibres through which the endoplasmic sol flows, transported by spontaneous and selforganised oscillatory contractions. The organism behaves as a distributed computing material, capable of responding to a wide range of spatially represented stimuli, including chemoattractants, chemorepellents, temperature changes and light irradiation. Slime mould performs these complex feats using only very simple components and its behaviour has even been described as intelligent [2]. How can such complex behaviour emerge in such a simple organism? In this abstract we describe a two-way mechanism by which environmental information in the form of spatial concentration gradients can be exploited to generate apparently intelligent spatial behaviours seen in slime mould. We use a multi-agent particle based model of slime mould which replicates the self-organised network formation and adaptation of slime mould (see [3] for a detailed description). Coupled mobile agents move on a 2D lattice, sensing and depositing a simulated chemoattractant which diffuses over time. The virtual slime mould is comprised of a large population of these simple agents which collectively exhibits cohesion, network formation, network minimisation and shape adaptation. Simple rules (based on local space availability and crowding) govern the growth and shrinkage of the collective. Environmental stimuli in the form of attractants (positive displacement of the lattice) and repellents (negative displacement) are projected onto the same lattice and the diffusing stimuli cause the collective to deform at its border and move towards attractants and away from repellents. Concentration Dependent Morphology As observed in P. polycephalum [4], attractant concentration affects the growth patterns of the collective – low-concentration stimuli results in pseudopod-like growth fronts and dendritic networks (Fig. 1), whereas high-concentration stimuli generates florid radial growth patterns (Fig. 2). Fig. 1: Dendritic growth of virtual plasmodium in response to low-concentration stimuli. Inoculation site is circled, attractant stimuli indicated by dots, emergent transport network connects the attractant sources. Fig. 2: Radial expansive growth of virtual plasmodium in response to high-concentration stimuli. Inoculation site is at the centre of the lattice on simulated nutrient-rich substrate (light grey) with individual ‘oat flakes’ (white). Network growth is radial and network minimisation occurs when the background substrate is depleted. The different morphologies emerge because weaker concentration gradients present a smaller, isolated and directed stimulus region to the collective, whereas higher concentration s (for example within a nutrient-rich oatmeal agar substrate) present an all -encompassing stimulus, encouraging growth in all directions. The spatial pattern of concentration gradients presents a 2D stimulus map influencing the growth pattern and direction of the collective. These results suggest that the apparent complexity (or ‘intelligence’) of slime mould’s behaviour may be partially due to the configuration of its environment. Two-way Interaction with Gradient Fields The virtual plasmodium does not just simply follow the concentration profile of this 2D stimulus map but also modifies it, by suppressing the fields via engulfment and consumption of nutrients. The result is a dynamical interaction between the collective and the environment which may be summarised (using only attractants, for simplicity) in the following scheme: 1. A chemoattractant gradient field is generated by the diffusion of nutrients. 2. The collective is attracted towards, and moves towards, the higher concentration gradient profiles. 3. On encountering the source of the attractant, the collective suppresses and/or consumes the nutrient, reducing its chemoattractant concentration gradient. 4. The spatial pattern of concentration gradients is changed, altering the stimulus presented to the virtual plasmodium. 5. Resume part 2. The effect of this two-way interaction between environment (presentation of gradient) an d virtual plasmodium (modification of gradient) can be seen when the concentration gradient is visualised, as shown in in Fig. 3 which demonstrates the construction of a simple spanning tree network by the virtual plasmodium. Fig. 3: Construction of a spanning tree by virtual plasmodium. (a) Small population (particle positions shown) inoculated on lowest node (bottom) growing towards First node and engulfing it, reducing chemoattractant projection, (b-d) Model population grows to nearest sources of chemoattractant completing construction of the spanning tree, (e-h) Visualisation of the changing chemoattractant gradient as the population senses, engulfs and suppresses nutrient diffusion (darker regions indicate stronger chemoattractant concentration). The low-concentration attractants used for the experiment in Fig. 3 presents a directed stimulus towards the collective (initialised at the bottom nutrient source). When the growing collective reaches the nearest source, its projection o f attractant is damped by engulfment of the collective, and the collective splits to connect the left and right nutrients (whose distance from the current node is very similar). The suppression of each node as it is encountered combined with the dynamically changing gradient field presents a new stimulus pattern to the collective, until the spanning tree connecting all nutrients is formed. We have also demonstrated using the model that it is possible to respond to differences in nutrient distance, nutrient size, and nutrient concentration by moving towards the closest, largest and highest concentration, respectively. The model also responds to the addition of new nutrients and the removal of nutrients (subject to a short delay whilst the diffusion stimulus profile is reconfigured) [5]. Combining Attractant and Repellent Stimuli Attractant stimuli generate networks with proximity graph connectivity, for example Minimum Spanning Trees as in Fig. 3. In contrast, repellent stimuli generate plane division graphs as in the Voronoi diagram, where the model occupies the regions furthest away from the repellent sources (Fig. 4, left). By using individual repellent sources of varying concentrations the weighted Voronoi diagram can be approximated (Fig. 4, right). The combination of attractants and repellents allows us to guide the model using combinations of attractant and repellent stimuli (to ‘pull’ the collective towards certain regions or to ‘push’ the collective away from other regions). When similar concentrations of attractant and repellent stimuli are used there is no ambiguous behaviour. Instead hybrid graphs are formed (Fig. 5) which contain features of both proximity graphs and plane division graphs [6]. Fig. 4: Variation in repellent stimulus size allows approximation of the Weighted Voronoi diagram. Left: Repellent stimuli cause the model plasmodium to move away from stimuli sites forming Voronoi diagram (classically computed Voronoi diagram is overlaid). Right: Changing the size of the repellent stimuli causes greater distortions in the diffusion field, causing the model plasmodium to adopt patterns which approximate the multiplicatively weighted Voronoi diagram. Fig. 5: Variation in repellent concentration cause a transition between classical Voronoi diagram and hybrid graphs which exhibit both plane division and minimisation features. Left: vir tual plasmodium exposed to highconcentration repellent diffusion (from edge of planar shapes) adopts the Voronoi diagram. Middle: reducing the repellent concentration allows the contractile behaviour of the model to exert its effect in competition with the repellent field. Right: further reduction in repellent concentration forms a graph structure which exhibits both plane division and internal path minimisation features.	approximation algorithm;cohesion (computer science);computation;connectivity (graph theory);crowding;dependent ml;displacement mapping;distortion;distributed computing;emergence;emergentism;file spanning;gradient;mathematical morphology;mobile agent;multi-agent system;network formation;outsourcing;persistent vegetative state;radial (radio);slime;self-organization;simulation;spanning tree;spatiotemporal pattern;spontaneous order;tree network;weighted voronoi diagram;while;xfig;zero suppression	Jeff Jones	2015	CoRR		theoretical computer science;computation;control engineering;computer science;slime mold	ML	46.719745705117916	-25.762428864082736	59206
2b59397e10737166e0428182fc1423a02286aedf	body wall force sensor for simulated minimally invasive surgery: application to fetal surgery		Surgical interventions are increasingly executed minimal invasively. Surgeons insert instruments through tiny incisions in the body and pivot slender instruments to treat organs or tissue below the surface. While a blessing for patients, surgeons need to pay extra attention to overcome the fulcrum effect, reduced haptic feedback and deal with lost hand-eye coordination. The mental load makes it difficult to pay sufficient attention to the forces that are exerted on the body wall. In delicate procedures such as fetal surgery, this might be problematic as irreparable damage could cause premature delivery. As a first attempt to quantify the interaction forces applied on the patient's body wall, a novel 6 degrees of freedom force sensor was developed for an ex-vivo set up. The performance of the sensor was characterised. User experiments were conducted by 3 clinicians on a set up simulating a fetal surgical intervention. During these simulated interventions, the interaction forces were recorded and analysed when a normal instrument was employed. These results were compared with a session where a flexible instrument under haptic guidance was used. The conducted experiments resulted in interesting insights in the interaction forces and stresses that develop during such difficult surgical intervention. The results also implicated that haptic guidance schemes and the use of flexible instruments rather than rigid ones could have a significant impact on the stresses that occur at the body wall.	experiment;haptic technology;sensor;simulation;video-in video-out	Allan Javaux;Laure Esteveny;David Bouget;Caspar Gruijthuijsen;Danail Stoyanov;Tom Vercauteren;Sébastien Ourselin;Dominiek Reynaerts;Kathleen Denis;Jan Deprest;Emmanuel B. Vander Poorten	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8202150	fetal surgery;computer science;biomedical engineering;degrees of freedom (mechanics);psychological intervention;haptic technology	Robotics	40.04546051252046	-37.26224485275252	59496
9499365eb1ccaa73f57bafcee4e2db61a137fb52	a digital implementation of neuron-astrocyte interaction for neuromorphic applications	neuron astrocyte interactions;linear approximation;fpga;neuromorphic	Recent neurophysiologic findings have shown that astrocytes play important roles in information processing and modulation of neuronal activity. Motivated by these findings, in the present research, a digital neuromorphic circuit to study neuron-astrocyte interaction is proposed. In this digital circuit, the firing dynamics of the neuron is described by Izhikevich model and the calcium dynamics of a single astrocyte is explained by a functional model introduced by Postnov and colleagues. For digital implementation of the neuron-astrocyte signaling, Single Constant Multiply (SCM) technique and several linear approximations are used for efficient low-cost hardware implementation on digital platforms. Using the proposed neuron-astrocyte circuit and based on the results of MATLAB simulations, hardware synthesis and FPGA implementation, it is demonstrated that the proposed digital astrocyte is able to change the firing patterns of the neuron through bidirectional communication. Utilizing the proposed digital circuit, it will be illustrated that information processing in synaptic clefts is strongly regulated by astrocyte. Moreover, our results suggest that the digital circuit of neuron-astrocyte crosstalk produces diverse neural responses and therefore enhances the information processing capabilities of the neuromorphic circuits. This is suitable for applications in reconfigurable neuromorphic devices which implement biologically brain circuits.		Soheila Nazari;Karim Faez;Mahmood Amiri;Ehsan Karami	2015	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2015.01.005	computer science;theoretical computer science;neuromorphic engineering;field-programmable gate array;linear approximation	EDA	42.46151104496065	-27.164942075897745	59529
850d6278d2ac4f30dc8ac50a48449802d681a8bc	matching evaluation of 2d laser scan points using observed probability in unstable measurement environment	matching evaluation mobile robot localization laser scanner measurement urban areas sidewalk unstable measurement environment observed probability 2d laser scan point;probability;optical scanners;path planning;mobile robots;probability mobile robots optical scanners path planning;current measurement feature extraction area measurement probability robot kinematics measurement by laser beam	In the real environment such as urban areas sidewalk, the laser scanner measurement is unstable due to various noise such as moving object. The unstable measurement makes difficult for mobile robot localization. While, the stable measured object is effective as landmark. And, it is expect that around the stable measured object, the scan points is frequently obtained greater than unstable measurement with multiple scans. Hence, the observed probability of scan point allows to extract the features of stable measured object and decrease the influence of unstable measurement. This paper presents the calculation of the observed probability of laser scan point for mobile robot localization. The observed probability is statistically obtained from multiple scans when obtained in a priori. Using this localization method, our robot moves completely on the pedestrian environment of about 1.2km at Tsukuba Challenge 2011.	3d scanner;ct scan;control theory;image scanner;mobile robot;robotic mapping	Taichi Yamada;Akihisa Ohya	2013	Proceedings of the 2013 IEEE/SICE International Symposium on System Integration	10.1109/SII.2013.6776688	computer vision;simulation;geography;optics	Robotics	52.95416390445863	-36.308156895384826	59806
4ceaee46c8d6db0ca498ea769de25e916b0296d4	an architecture for hierarchical collision detection	complex objects;real time;virtual reality;hardware architecture;chip;force feedback;collision detection;graphics hardware;object hierarchies;physically based simulation;computer animation;high speed	We present novel algorithms for efficient hierarchical collision detection and propose a hardware architecture for a single-chip accelerator. We use a hierarchy of bounding volumes defined by k-DOPs for maximum performance. A new hierarchy traversal algorithm and an optimized triangle-triangle intersection test reduce bandwidth and computation costs. The resulting hardware architecture can process two object hierarchies and identify intersecting triangles autonomously at high speed. Real-time collision detection of complex objects at rates required by force-feedback and physically-based simulations can be achieved.	algorithm;benchmark (computing);best, worst and average case;bounding volume;bounding volume hierarchy;collision detection;computation;glossary of computer graphics;graphics hardware;haptic technology;interactivity;polygon soup;simulation;speedup;tree traversal;vhdl	Gabriel Zachmann;Günter Knittel	2003			chip;parallel computing;real-time computing;computer science;artificial intelligence;operating system;hardware architecture;virtual reality;computer animation;haptic technology;graphics hardware;collision detection;computer graphics (images)	EDA	43.68468817364701	-31.9776489766658	60396
8f7f7ba8f1d4beba26ae5c5346f23328fb738fe0	probabilistic and count methods in map building for autonomous mobile robots	bayes rule;map building;statistical significance;autonomous mobile robot;ultrasonic sensor	In this paper two computationally efficient methods for building a map of the occupancy of a space based on measurements from a ring of ultrasonic sensors are presented. The first is a method based on building a histogram of the occurrence of free and occupied space. The second is based on the calculation of the rate between occupied space measurements with respect to the total. The resulting occupancy maps have been compared with those obtained with other well-known methods, bothcoun t as well as Bayes-rule-based ones, in static environments. Free space, occupied space and unknown labels were also compared subsequent to the application of a simple segmentation algorithm. The results obtained gave rise to statistically significant differences between all the different types on comparing the resulting maps. In the case of comparing occupancy labels, no differences were found between the following pairs of methods: RATE and SUM (p - value = 0.157), ELFES and RATE (p - value = 0.600) and ELFES and SUM (p - value = 0.593).	autonomous robot	Miguel A. Rodríguez;José Correa;Roberto Iglesias;Carlos V. Regueiro;Senén Barro	1999		10.1007/3-540-40044-3_8	computer vision;simulation;machine learning;mathematics	Robotics	52.76623870367927	-35.04238537420214	60779
f8778dac357e7daacd3e62709a9acbbd96042bb3	in-process grinding monitoring by acoustic emission	grinding machines;high sampling rate data acquisition system;digital signal processing;workpiece holder fixed sensor;acoustic emission signal processing;surface grinding machine;aluminum oxide;acoustic signal processing;burn detection;correlation methods;data acquisition system;ratio of power;grinding process thermal damage;kurtosis statistics;monitoring acoustic emission acoustic signal detection root mean square digital signal processing signal processing grinding machines aluminum oxide wheels acoustic sensors;process monitoring;mvd;statistical analysis;monitoring;acoustic emission correlation;cfar;signal processing;skew statistics;mean value deviance;acoustic signal detection;trabalho apresentado em evento;acoustic emission;root mean square;acoustic sensors;al sub 2 o sub 3 skew statistics kurtosis statistics in process grinding monitoring acoustic emission signal processing grinding process thermal damage surface grinding machine aluminum oxide grinding wheel abnt 1045 workpiece holder fixed sensor high sampling rate data acquisition system burn detection rms statistics acoustic emission correlation constant false alarm ratio of power mean value deviance cfar mvd rop 2 5 mhz;constant false alarm;rop;statistical analysis grinding acoustic emission process monitoring acoustic signal processing data acquisition correlation methods;al 2 o 3;rms statistics;data acquisition;2 5 mhz;wheels;grinding;aluminum oxide grinding wheel;abnt 1045;in process grinding monitoring	This work aims to investigate the efficiency of digital signal processing tools of acoustic emission (AE) signals in order to detect thermal damage in grinding processes. To accomplish such a goal, an experimental work was carried out for 15 runs in a surface grinding machine, operating with an aluminum oxide grinding wheel and ABNT 1045. The acoustic emission signals were acquired from a fixed sensor placed on the workpiece holder. A high sampling rate data acquisition system at 2.5 MHz was used to collect the raw acoustic emission instead of root mean square value usually employed. Many statistics have shown effective to detect burn, such as the root mean square (RMS), correlation of the AE, constant false alarm (CFAR), ratio of power (ROP) and mean-value deviance (MVD). However, the CFAR, ROP, kurtosis and correlation of the AE are presented as being more sensitive than the RMS.	acoustic cryptanalysis;acoustic model;constant false alarm rate;data acquisition;digital signal processing;mean squared error;methods of computing square roots;sampling (signal processing)	Paulo Roberto de Aguiar;Paulo José Amaral Serni;Eduardo Carlos Bianchi;Fábio R. L. Dotto	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1327133	computer vision;computer science;signal processing;data acquisition;statistics	Robotics	39.235677028918126	-30.814726623573083	60940
0ddd24d05d6a1b19539ed0c11e7a8c985d30223f	support vector machine based robotic traversability prediction with vision features	field robot;support vector machine;intelligent decision;traversability prediction	This paper presents a novel method on building relationship between the vision features of the terrain images and the terrain traversability which manifests the difficulty of field robot traveling across one terrain. Vision features of the image are extracted based on color and texture. The travesability is labeled with the relative vibration. The support vector machine regression method is adopted to build up the inner relationship between them. In order to avoid the over-learning during training, k-fold method is used and average mean square error is defined as the target minimized to get the optimal parameters based on parameter space grid method. For the traveling smoothness of field robot, the original traversability prediction is transformed to computed traversability prediction based on different initial sub-regions. The optimal path is given by minimizing the sum of computed traversability prediction of all sub-regions in each path. Three experiments are discussed to demonstrate the effectiveness and efficiency of the method mentioned in this paper.	experiment;feature vector;mean squared error;numerical weather prediction;personalization;robot;support vector machine	Jianwei Cui;Baoming Sun;Huatao Zhang;Kui Qian;Jiatong Bao;Aiguo Song	2013	Int. J. Comput. Intell. Syst.	10.1080/18756891.2013.802107	support vector machine;computer vision;computer science;artificial intelligence;machine learning	Robotics	53.72532378627889	-31.688929728012884	60961
b7281ef77e0561479256d175751c14062a34d5ed	probabilistic self-localization and mapping - an asynchronous multirate approach	loop closing problem;least square fitting;multirate system;least squares approximations;probabilistic self localization and mapping;probability;map building;asynchronous multirate technique;slam robots image fusion kalman filters least squares approximations linear quadratic control mobile robots pose estimation probability robot vision;image fusion;kalman filters;kalman filter;mobile robots;cartographie;probabilistic approach;linear system;systeme multicadence;fitting;distance measurement;cartografia;robot vision;estimation;linear quadratic regulator control;enfoque probabilista;approche probabiliste;feature extraction;linear quadratic control;robots;least square;lqg control;probabilistic self localization and mapping robot pose estimation algorithm asynchronous multirate technique multirate fusion loop closing problem least square fitting linear quadratic regulator control kalman filter;cartography;robot pose estimation algorithm;linear quadratic regulator;image fusion kalman filters least squares approximations linear quadratic control mobile robots pose estimation probability robot vision slam robots;sistema cadencia multiple;slam robots;filtering sampling methods simultaneous localization and mapping state estimation filters robot sensing systems least squares approximation robustness fuses sensor fusion;multirate fusion;kalman filter probabilistic self localization and mapping robot pose estimation algorithm asynchronous multirate technique multirate fusion loop closing problem least square fitting linear quadratic regulator control;pose estimation;automation	One of the main contributions of this article is related to the multirate asynchronous filtering approach for the SLAM problem based on PFs. Previous multirate filter contributions are mainly for linear systems. A Kalman filter is applied for linear quadratic regulator (LQG) control, while in a Kalman filter is developed using lifting techniques. In this article, significant improvements for robot pose estimation are obtained when introducing multirate techniques to FastSLAM. In particular, it is shown that multirate fusion aims to provide more accurate results in loop-closing problems in SLAM (localization and map building problems with closed paths). Additionally, in this article a pose estimation algorithm based on least squares (LS) fitting of line features is proposed. Since the complexity of LS fitting is linear to the number of features, this implies a low computational cost than other techniques. Therefore, methods based on PFs such as MCL and FastSLAM that require a large number of particles may benefit from this fact. In particular, this provides an accurate approximation of the posterior PDF for FastSLAM 2.0.	3d pose estimation;algorithm;algorithmic efficiency;approximation;closing (morphology);computation;kalman filter;least squares;lifting scheme;linear system;monte carlo localization;portable document format;simultaneous localization and mapping	Leopoldo Armesto;Gianluca Ippoliti;Sauro Longhi;Josep Tornero	2008	IEEE Robotics & Automation Magazine	10.1109/M-RA.2007.907355	kalman filter;control engineering;mathematical optimization;computer science;artificial intelligence;control theory	Robotics	51.88103426213205	-34.972128665232795	61111
8aa36bcb4f12bb5c15e92fc4303f47d02dc28ac1	augmenting vehicle localization accuracy with cameras and 3d road infrastructure database		Accurate and continuous vehicle localization in urban environments has been an important research problem in recent years. In this paper, we propose a landmark based localization method using road signs and road markings. The principle is to associate the online detections from onboard cameras with the landmarks in a pre-generated road infrastructure database, then to adjust the raw vehicle pose predicted by the inertial sensors. This method was evaluated with data sequences acquired on urban streets. The results prove the contribution of road signs and road markings for reducing the trajectory drift as absolute control points.	internationalization and localization;sensor	Lijun Wei;Bahman Soheilian;Valérie Gouet-Brunet	2014		10.1007/978-3-319-16178-5_13	computer vision;computer security	Robotics	50.93684933719739	-36.970735610930426	61522
bbcfb31554a0d64bf6bd182a43192e0674a736ba	a cloned linguistic decision tree controller for real-time path planning in hostile environments	uav;learning;fuzzy control;behavioural cloning;interpretability;optimization;linguistic modelling	The idea of a Cloned Controller to approximate optimised control algorithms in a real-time environment is introduced. A Cloned Controller is demonstrated using Linguistic Decision Trees (LDTs) to clone a Model Predictive Controller (MPC) based on Mixed Integer Linear Programming (MILP) for Unmanned Aerial Vehicle (UAV) path planning through a hostile environment. Modifications to the LDT algorithm are proposed to account for attributes with circular domains, such as bearings, and discontinuous output functions. The cloned controller is shown to produce near optimal paths whilst significantly reducing the decision period. Further investigation shows that the cloned controller generalises to the multi-obstacle case although this can lead to situations far outside of the training dataset and consequently result in decisions with a high level of uncertainty. A modification to the algorithm to improve the performance in regions of high uncertainty is proposed and shown to further enhance generalisation. The resulting controller combines the high performance of MPC–MILP with the rapid response of an LDT while providing a degree of transparency/interpretability of the decision making. © 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	approximation algorithm;autopilot;carry-lookahead adder;data point;decision tree;game controller;global descriptor table;high-level programming language;image noise;lejos rcx;linear programming;lookahead carry unit;motion planning;obstacle avoidance;operational system;real-time clock;real-time path planning;real-time transcription;simulation;unmanned aerial vehicle;while	Oliver Turnbull;Jonathan Lawry;Mark Lowenberg;Arthur Richards	2016	Fuzzy Sets and Systems	10.1016/j.fss.2015.08.017	simulation;computer science;artificial intelligence;machine learning;fuzzy control system	AI	52.74309079599029	-27.978573749812128	61721
80de368f386abd00451be811f6e517a8dc490370	automated spatial-semantic modeling with applications to place labeling and informed search	online annotated database;object recognition;robot vision learning artificial intelligence object recognition;cognitive robotics;spatial reasoning;automated spatial semantic modeling system;intelligent robots;object maps place labeling;mobile robots;layout;spatial reasoning automated spatial semantic modeling system object maps place labeling automated learning online annotated database robot platform object recognition;information search;data mining;computer vision;semantic model;semantic information;computational modeling;robot vision;robots;labelme;mathematical model;labeling robot kinematics robotics and automation layout object recognition robot vision systems mobile robots cognitive robotics intelligent robots computer vision;search problems;learning artificial intelligence;informed search;labelme place labeling informed search;robot vision systems;robotics and automation;place labeling;robot platform;labeling;automated learning;robot kinematics	This paper presents a spatial-semantic modeling system featuringautomated learning of object-place relations from an online annotateddatabase, and the application of these relations to a variety ofreal-world tasks. The system is able to label novel scenes with placeinformation, as we demonstrate on test scenes drawn from the same sourceas our training set. We have designed our system for future enhancementof a robot platform that performs state-of-the-art object recognitionand creates object maps of realistic environments. In this context, wedemonstrate the use of spatial-semantic information to performclustering and place labeling of object maps obtained from real homes.This place information is fed back into the robot system to inform anobject search planner about likely locations of a query object. As awhole, this system represents a new level in spatial reasoning andsemantic understanding for a physical platform.	assistive technology;cluster analysis;curious george;map;outline of object recognition;robot;robotics;search algorithm;software deployment;spatial–temporal reasoning;test set	Pooja Viswanathan;David Meger;Tristram Southey;James J. Little;Alan K. Mackworth	2009	2009 Canadian Conference on Computer and Robot Vision	10.1109/CRV.2009.49	semantic data model;robot;layout;mobile robot;computer vision;labeling theory;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;mathematical model;spatial intelligence;computational model;robot kinematics;cognitive robotics	Robotics	49.93883280422665	-37.66214968551903	61825
f5802b63b077b0dd9b5b3184afd112fa53ca60bd	a ground-based optical system for autonomous landing of a fixed wing uav	satellite navigation aerospace components autonomous aerial vehicles cameras closed loop systems learning artificial intelligence object detection object tracking;cameras global positioning system adaptive optics optical imaging aircraft atmospheric modeling visualization;gnss ground based visual approach unmanned aerial vehicle global navigation satellite systems pan tilt unit ptu visible light camera adaboost method target tracking target detection 3d coordinates closed loop control autonomous landing strategy	This paper presents a new ground-based visual approach for guidance and safe landing of an unmanned aerial vehicle (UAV) in Global Navigation Satellite System(GNSS)-denied environments. In our previous work, the old system consists of one pan-tilt unit(PTU) with two cameras, whose detection range is limited by the baseline. To achieve long-range detection and cover wide field of regard, we mounted two separate sets of PTU integrated with visible light camera on both sides of the runway instead of our previous assembled stereo vision system. Then, the well-known AdaBoost method was evaluated with regard to detecting and tracking the target. To achieve the relative position between the UAV and landing area, we used triangulation to calculate the 3D coordinates of the UAV. By combining the estimated position in the closed loop control, we obtain the autonomous landing strategy. Finally, we present several real flights in outdoor environments, and compare its accuracy with ground truth provided by GNSS. The results support the validity and accuracy of the presented system.	adaboost;aerial photography;autonomous robot;baseline (configuration management);experiment;field of regard;galileo (satellite navigation);ground truth;machine vision;pan–tilt–zoom camera;robustness (computer science);satellite navigation;sensor;stereopsis;unmanned aerial vehicle;whole earth 'lectronic link	Weiwei Kong;Dianle Zhou;Yu Zhang;Daibing Zhang;Xun Wang;Boxin Zhao;Chengping Yan;Lincheng Shen;Jianwei Zhang	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6943244	computer vision;simulation;remote sensing	Robotics	53.31131987526751	-37.18338166166303	61886
41f540c279d6b2a904c5d167c229834d7ee1bb5e	a decentralised particle filtering algorithm for multi-target tracking across multiple flight vehicles	decentralised particle filtering algorithm;correlated estimation errors;gaussian processes;airborne tracking decentralised particle filtering algorithm multi target tracking multiple flight vehicles 3d feature tracking limited communication bandwidth decentralised data fusion correlated estimation errors discrete particle sets gaussian mixture models airborne data;multiple flight vehicles;limited communication bandwidth;mobile robots;remotely operated vehicles;data fusion;gaussian mixture model;decentralised control;particle filter;airborne data;telerobotics aerospace robotics aircraft decentralised control gaussian processes mobile robots multi robot systems particle filtering numerical methods remotely operated vehicles sensor fusion;gaussian mixture models;3d feature tracking;aerospace robotics;multi target tracking;multi robot systems;filtering algorithms particle tracking unmanned aerial vehicles particle filters bandwidth mobile robots information filtering information filters remotely operated vehicles estimation error;telerobotics;discrete particle sets;estimation error;sensor fusion;decentralised data fusion;airborne tracking;aircraft;particle filtering numerical methods	This paper presents a decentralised particle filtering algorithm that enables multiple vehicles to jointly track 3D features under limited communication bandwidth. This algorithm, applied within a decentralised data fusion (DDF) framework, deals with correlated estimation errors due to common past information when fusing two discrete particle sets. Our solution is to transform the particles into Gaussian mixture models (GMMs) for communication and fusion. Not only can decentralised fusion be approximated by GMMs, but this representation also provides summaries of the particle set. Less bandwidth per communication step is required to communicate a GMM than the particle set itself hence conversion to GMMs for communication is an advantage. Real airborne data is used to demonstrate the accuracy of our decentralised particle filtering algorithm for airborne tracking and mapping	airborne ranger;approximation algorithm;distribution frame;mixture model;oracle fusion architecture;particle filter	Lee-Ling S. Ong;Ben Upcroft;Tim Bailey;Matthew Ridley;Salah Sukkarieh;Hugh F. Durrant-Whyte	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.282155	control engineering;computer vision;simulation;computer science;engineering;artificial intelligence;mixture model;sensor fusion;statistics	Robotics	51.813538736521885	-33.85952401576412	61929
61525527124a29b9a33aa5fc6b13d84213336863	autonomous learning of vision-based layered object models on mobile robots	temporal assessment;second order;color distribution statistics;environmental factors;wheeled robots;object recognition;vision based layered object model;mobile robot;learning model;image matching;computer model;robot vision environmental factors feature extraction gradient methods image matching learning artificial intelligence mobile robots object detection object recognition object tracking;wheeled robots visual learning recognition;wheeled robot platform autonomous learning vision based layered object model mobile robot real world application environmental change temporal assessment image gradient feature object detection color distribution statistics spatial representation object recognition object tracking spatial similarity measure;mobile robots;recognition;computational modeling;real world application;robot vision;image color analysis;wheeled robot platform;computational modeling image color analysis mobile robots mathematical model feature extraction robustness;feature extraction;visual cues;visual learning;indoor environment;image gradient feature;object tracking;mathematical model;spatial representation;gradient methods;robustness;spatial similarity measure;learning artificial intelligence;environmental change;similarity measure;autonomous learning;object detection;environmental modeling;object model;base layer	Although mobile robots are increasingly being used in real-world applications, the ability to robustly sense and interact with the environment is still missing. A key requirement for the widespread deployment of mobile robots is the ability to operate autonomously by learning desired environmental models and revising the learned models in response to environmental changes. This paper presents an approach that enables a mobile robot to autonomously learn layered models for environmental objects using temporal, local and global visual cues. A temporal assessment of image gradient features is used to detect candidate objects, which are then modeled using color distribution statistics and a spatial representation of gradient features. The robot incrementally revises the learned models and uses them for object recognition and tracking based on a matching scheme comprising a spatial similarity measure and second order distribution statistics. All algorithms are implemented and tested on a wheeled robot platform in dynamic indoor environments.	algorithm;coherence (physics);experiment;image gradient;mobile robot;outline of object recognition;similarity measure;software deployment	Xiang Li;Mohan Sridharan;Shiqi Zhang	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5980435	computer simulation;mobile robot;computer vision;simulation;computer science;artificial intelligence;machine learning	Robotics	49.515683756532624	-37.8750451565644	62214
532eeb6cd8874de67890ae1229c9067ecaae7d12	environment and solar map construction for solar-powered mobile systems	solar powered vehicles gaussian processes mobile robots;mapping field robots;geometry;yttrium;solar panels;robots;mobile communication;position measurement;sun;robots field robots geometry solar panels position measurement mobile communication;solar current solar map construction solar powered mobile systems solar power estimation magnitude estimation solar classification position classification data driven gaussian process method	Energy harvesting using solar panels can significantly increase the operational life of mobile robots. If a map of expected solar power is available, energy efficient paths can be computed. However, estimating this map is a challenging task, especially in complex environments. In this paper, we show how the problem of estimating solar power can be decomposed into the steps of magnitude estimation and solar classification. Then, we provide two methods to classify a position as sunny or shaded: a simple data-driven Gaussian Process method and a method that estimates the geometry of the environment as a latent variable. Both of these methods are practical when the training measurements are sparse, such as with a simple robot that can only measure solar power at its own position. We demonstrate our methods on simulated randomly generated environments. We also justify our methods with measured solar data by comparing the constructed height maps with satellite images of the test environments, and in a cross-validation step where we examine the accuracy of predicted shadows and solar current.	approximation algorithm;bump mapping;competitive analysis (online algorithm);computer vision;cross-validation (statistics);exploration problem;external ray;gaussian process;heightmap;information;integrated development environment;kriging;latent variable;map;mobile robot;online and offline;procedural generation;ray casting;ray tracing (graphics);robot framework;sampling (signal processing);sensor;shading;simulation;sparse matrix;thermal copper pillar bump	Patrick A. Plonski;Joshua Vander Hook;Volkan Isler	2016	IEEE Transactions on Robotics	10.1109/TRO.2015.2501924	robot;computer vision;simulation;mobile telephony;computer science;artificial intelligence;yttrium;remote sensing	Robotics	49.79317932192091	-26.256245028086784	62410
1b3d51c458a2ea667de0b6e58082ff26e524a4a8	multiple target tracking with uncertain sensor state applied to autonomous vehicle data		In a conventional multitarget tracking (MTT) scenario, the sensor position is assumed known. When the MTT sensor, e.g., an automotive radar, is mounted to a moving vehicle with uncertain state, it becomes necessary to relax this assumption and model the unknown sensor position explicitly. In this paper, we compare a recently proposed filter that models the unknown sensor state [1], to two versions of the track-oriented marginal MeMBer/Poisson (TOMB/P) filter: the first does not model the sensor state uncertainty; the second models it approximately by artificially increasing the measurement variance. The results, using real measurement data, show that in terms of tracking performance, the proposed filter can outperform TOMB/P without sensor state uncertainty, and is comparable to TOMB/P with increased variance.	marginal model;radar;sensor;tomb raider: anniversary;usb hub	Markus Fröhle;Karl Granström;Henk Wymeersch	2018	2018 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2018.8450842	radar;kalman filter;control theory;mathematics;poisson distribution	Robotics	40.35697685173916	-26.99419115806493	62823
308b5dc3da5c72b1605bec02a5a27f5756045a29	parameter estimation for multistatic active sonar using extended fixed points	sonar equipment;clutter;kalman filter measurement update parameter estimation approach multistatic active sonar extended fixed points object tracking;estimation receivers clutter scattering sonar equipment sonar navigation;scattering;sonar kalman filters;receivers;estimation;sonar navigation	The main task of multistatic active sonar is the localisation and tracking of objects of interest (targets). Therefore, a precise knowledge of the parameters of the multistatic scenario is mandatory. These are the positions of the acoustic sources, the times of transmission, as well as the position and heading of the own sonar sensor. Reflections from so called “fixed points” can be used to improve knowledge about these parameters. A fixed point can be a wreck or geographical feature (e.g. a small island) with known position. In general these fixed points consist of multiple scattering points, thus, the assumption of a point-like target is not fulfilled. In this paper we discuss the impact that an extension of the fixed point has on the estimation process and derive a method within the Kalman filter measurement update to incorporate knowledge about the extension in the estimation approach. Results will be discussed for simulated data.	acoustic cryptanalysis;course (navigation);estimation theory;fixed point (mathematics);kalman filter;sonar (symantec);the times	Martina Broetje;Kolja Pikora	2015	2015 18th International Conference on Information Fusion (Fusion)		computer vision;synthetic aperture sonar;electronic engineering;geography;remote sensing	Robotics	50.37852947549347	-33.58931377114729	62836
d24df73c4fee6470c4810d2a233330ac48fdaba9	multimedia communication techniques for remote cable-based video-surveillance systems	cable television;cable tv;remote control;sensor systems;catv network;video surveillance;image processing;high speed commercial cable modem systems multimedia communication techniques remote cable based video surveillance systems wired cable tv networks video sensors catv network remote control centre pc based image processing architecture image processing tasks real time multimedia data transmission generic cable based avs applications upstream noisy channels received images;remote cable based video surveillance systems;surveillance;upstream noisy channels;generic cable based avs applications;real time;multimedia communication techniques;image sensors;data communication;multimedia systems;remote control centre;closed circuit television;high speed commercial cable modem systems;wired cable tv networks;multimedia data;multimedia communication;video sensors;cable modem;pc based image processing architecture;telecommunication channels multimedia communication cable television surveillance closed circuit television image processing;received images;real time multimedia data transmission;multimedia communication communication cables cable tv multimedia systems image processing data communication costs context image sensors sensor systems;telecommunication channels;communication cables;image processing tasks;high speed;context	This paper aims at presenting a study of costs and performances concerning different multimedia communication techniques that can be employed in the context of the implementation of 2 generation remote video-surveillance systems using wired Cable TV networks as communication means. The multimedia information is acquired by video sensors and transmitted through the CATV network to a remote control centre, where a PC-based image processing architecture performs the image processing tasks needed by the implementation of the foreseen system functionaliti es. The paper will show some possible analogue and digital solutions to the problems concerning the real-time multimedia data transmission in generic cable-based AVS applications. Laboratory simulations of multimedia data transmission over upstream noisy channels have been performed in order to test the performances in terms of quality of the received images provided by the most advanced high-speed commercial cable modem systems	cable modem;image processing;performance;real-time computing;real-time transcription;remote control;sensor;simulation	Claudio Sacchi;Carlo S. Regazzoni	1999		10.1109/ICIAP.1999.797747	embedded system;cable internet access;telecommunications;image processing;cable modem;computer science;digital cable;television;computer network;remote control	Embedded	45.67369094844508	-35.60877381389234	63275
dd77d2fb521aa2de47236ecf9f9ce0e34c72f036	simultaneous localization and mapping based on (μ+1)-evolution strategy for mobile robots	slam;evolution strategy;intelligent robotics;occupancy grid map	Simultaneous Localization and Mapping SLAM is one of the most important capabilities for autonomous mobile robots, and many researches have been proposed demonstrating the effective SLAM methods. However, these SLAM methods sometimes require assumptions such as the sensor model, which is difficult to implement and use the SLAM methods. In our previous work, a SLAM method based on Evolution Strategy ES was proposed and the on-line SLAM in indoor environments was realized. However, the definition of the map building method was not clear. Therefore, we propose a SLAM method based on a simple map building and search method. In this paper, we explain our autonomous mobile robot system and propose our SLAM method based on µ+1-ES. The experimental results show the effectiveness of the proposed method.	mobile robot;simultaneous localization and mapping	Yuichiro Toda;Naoyuki Kubota	2015		10.1007/978-3-319-22873-0_6	computer vision;simulation;computer science;artificial intelligence;evolution strategy	Robotics	53.32180242420507	-32.99023154369219	63284
cbee520b1f997dd4f4332cd415c3479dfd3e90d1	towards information-theoretic decision making in a conservative information space	aerial visual simultaneous localization and mapping information theoretic decision making conservative information fusion technique probability distribution function pdf conservative approximation gaussian probability distribution entropy computation informative image observation;slam robots approximation theory decision making entropy gaussian distribution image fusion robot vision;probability density function;decision making entropy probability density function random variables simultaneous localization and mapping jacobian matrices aerospace electronics;random variables;simultaneous localization and mapping;aerospace electronics;entropy;jacobian matrices	We propose the conceptual idea of resorting to conservative information fusion techniques for information-theoretic decision making, aiming to address challenges involved with decision making over a high-dimensional, possibly highly-correlated, information space. Our key observation is that in certain cases, the impact of any two actions (or controls) on an appropriate utility measure, such as entropy, has the same trend regardless if using the original probability distribution function (pdf) or a conservative approximation of thereof. This observation suggests that in these cases, decision making can be performed over a conservative pdf, instead of the original pdf, without sacrificing performance. We introduce and prove this concept for the basic one-dimensional case assuming Gaussian probability distributions, and then consider its extension to a high-dimensional state space. In particular, we consider a specific conservative pdf that decouples the random variables in the joint pdf, admitting extremely efficient entropy computation. We then present our progress in identifying classes of problems in which information-theoretic decision making over this conservative and original pdfs produce identical results. The concept is illustrated in the context of choosing informative image observations in an aerial visual simultaneous localization and mapping scenario.	aerial photography;approximation;computation;entropy (information theory);information theory;portable document format;simultaneous localization and mapping;state space	Vadim Indelman	2015	2015 American Control Conference (ACC)	10.1109/ACC.2015.7171095	random variable;entropy;mathematical optimization;probability density function;artificial intelligence;principle of maximum entropy;machine learning;control theory;mathematics;statistics;simultaneous localization and mapping	AI	39.77508196172753	-25.725552106225102	63300
e4d3fd0970ee772c654493a34ea3ad8c38a5d387	gesture recognition using a marionette model and dynamic bayesian networks (dbns)	modelo dinamico;analisis imagen;modelizacion;distributed system;image recognition;reconocimiento imagen;anticipacion;systeme reparti;mise a jour;anticipation;reconnaissance geste;perception sociale;image processing;sociologia;gesture;dynamic model;procesamiento imagen;inertial navigation;robotics;traitement image;dynamical system;feasibility;actualizacion;modelisation;systeme dynamique;reseau bayes;sistema repartido;percepcion social;red bayes;dynamic bayesian network;modele dynamique;reconnaissance image;navegacion por inercia;bayes network;navigation inertie;robotica;image analysis;robotique;social perception;sociologie;sistema dinamico;point of view;modeling;analyse image;geste;gesture recognition;inertial sensor;sociology;practicabilidad;faisabilite;updating;social robot;gesto	This paper presents a framework for gesture recognition by modeling a system based on Dynamic Bayesian Networks (DBNs) from a Marionette point of view. To incorporate human qualities like anticipation and empathy inside the perception system of a social robot remains, so far an open issue. It is our goal to search for ways of implementation and test the feasibility. Towards this end we started the development of the guide robot ’Nicole’ equipped with a monocular camera and an inertial sensor to observe its environment. The context of interaction is a person performing gestures and ’Nicole’ reacting by means of audio output and motion. In this paper we present a solution to the gesture recognition task based on Dynamic Bayesian Network (DBN). We show that using a DBN is a human-like concept of recognizing gestures that encompass the quality of anticipation through the concept of prediction and update. A novel approach is used by incorporating a marionette model in the DBN as a trade-off between simple constant acceleration models and complex articulated models.	dynamic bayesian network;entity;gesture recognition;international symposium on fundamentals of computation theory;kalman filter;kinesiology;social robot	Jörg Rett;Jorge Dias	2006		10.1007/11867661_7	computer vision;image analysis;systems modeling;image processing;computer science;artificial intelligence;social robot;dynamical system;bayesian network;gesture recognition;anticipation;robotics;inertial navigation system;gesture;dynamic bayesian network;social perception	Robotics	52.25992786965391	-30.240199769523542	63499
aa666e2fc262c74f6994af4c34eab147c0112e8f	detection of tire types using tire noise from passing vehicles	tires vehicles roads noise decision support systems accidents correlation;tyres feature extraction road safety road traffic road vehicles signal detection traffic engineering computing;accidents;roads;decision support systems;tires;vehicles;correlation;time domain tire type detection tire noise passing vehicle traffic accident prevention road surface microphone acoustic sensor signal feature extraction frequency domain;noise	Winter tire is important and helpful for road users or automobile drivers to obviate serious traffic accidents. They also help road administrators to prevent many slip traffic accidents by such vehicles especially from the expressways, particularly in a snowy area. This paper is concerned with the reliable detection of tire types using only tire noise from passing vehicles. In practice, the tire noise emitted from moving vehicles varies momentarily depending on several mechanisms, such as road surface properties, tire tread patterns, and so on. As a result, it may be possible to passively and easily detect the tire type. For example, the least signal differences between winter and summer tires. To detect tire noise from running vehicles at 30, 40, 50 and 60 km/h on average, only when road surfaces were dry or wet state, we used a commercially available microphone as an acoustic sensor, which enabled us to easily reduce cost and size in a practical system for detecting tire types. We propose simple detection methods based on the cumulative distribution function of the power spectrum and the autocorrelation function of the tire noise signals to extract the signal features in the frequency domain and the time domain, respectively. Experimental results obtained from recorded signals in the snowy area demonstrated that the proposed method achieves high classification accuracy.	acoustic cryptanalysis;autocorrelation;microphone;sensor;spectral density	Wuttiwat Kongrattanaprasert	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041753	roadway noise;engineering;automotive engineering;transport engineering;forensic engineering	EDA	39.46692137705291	-34.10365950822505	63628
f0ce0f1a31b5c3d1f4a5564ec09baa14e37ca841	fault diagnosis of auv based on bayesian networks	computers;belief networks;autonomous underwater vehicle;bayesian network;optical control;lighting control;probability;underwater vehicles;uncertainty reasoning autonomous underwater vehicle fault diagnosis bayesian networks;optimal method;auv system;underwater vehicles belief networks fault diagnosis inference mechanisms remotely operated vehicles uncertainty handling;bayesian methods;fault;optical computing;inference mechanisms;optimization method;uncertainty handling;remotely operated vehicles;joints;computer networks;computer vision;underwater communication;probability distribution;cognition;fault diagnosis system;inference mechanism;uncertainty reasoning;communication system control;fault diagnosis bayesian methods computer networks computer vision optical computing underwater communication optical control probability distribution lighting control communication system control;diagnosis;fault diagnosis;uncertainty reasoning fault diagnosis system auv system autonomous underwater vehicle bayesian network inference mechanism optimization method;bayesian networks	As for the uncertainty and complex association in the fault diagnosis of AUV system, a method based on Bayesian networks is proposed, which is applied into fault diagnosis of AUV. By the inference mechanism based on Bayesian networks, an example of the modeling process of fault diagnosis system of AUV and the optimizing method of fault diagnosis strategy are given. Results of testing prove that this method highly improves the ability and accuracy of fault diagnosis of AUV	bayesian network	Changting Shi;Rubo Zhang;Ge Yang	2006	First International Multi-Symposiums on Computer and Computational Sciences (IMSCCS'06)	10.1109/IMSCCS.2006.224	control engineering;simulation;engineering;machine learning	AI	51.31521482995526	-30.356167243447814	63697
5d21250b7330fe5de861fe369dbfae3069086697	fpga-based remote pulse rate detection using photoplethysmographic imaging	cameras standards biomedical measurement field programmable gate arrays band pass filters pulse measurements;standards;band pass filters;a low cost camera photoplethysmography ppg pulse rate pr;field programmable gate arrays;biomedical measurement;cameras;pulse measurements	This paper presents first several steps towards an FPGA-based electronic system for remote pulse rate (PR) measurement. The system uses a low-cost digital camera as an image sensor, which operates at up to 30 frames per second (fps) in WXGA (1200×800 pixels) resolution. A novel algorithm for PR measurement was implemented using an FPGA development board. A commercially-available photoplethysmography module (TP-TSD200A from BIOPAC) was used as a golden standard to verify the performance of the suggested system. Ten male subjects were simultaneously examined using both the suggested system and the golden standard, and the results were compared. The proposed system leads to a potential means for providing mobile healthcare using smart phones and other mobile consumer products.	algorithm;digital camera;field-programmable gate array;graphics display resolution;image sensor;microprocessor development board;pixel;pulse;smartphone	He Liu;Yadong Wang;Lei Wang	2013	2013 IEEE International Conference on Body Sensor Networks	10.1109/BSN.2013.6575508	embedded system;computer science;electrical engineering;band-pass filter;field-programmable gate array	Robotics	44.03469707703519	-30.11448468355238	63744
7b1778d7560c7fe0b9050a94da5447442bf60926	an automotive perception system using distance cameras and polar occupancy grids	occupancy grid	One of the basic tasks of automotive collision avoidance and collision mitigation systems is the robust and reliable detection of objects as well as the prediction of future trajectories. This paper presents a system which uses a distance camera as range sensor to solve this task. The underlying algorithms uses an innovative Polar occupancy grid, which supports the segmentation of the distance image of the range sensor. An efficient sampling approach for the ego-motion compensation of the Polar occupancy grid is shown. Furthermore, we present a tracking system based on Unscented Kalman filters, which uses the segmented measurements of the distance image. Results of practical tests of the system are presented for two use cases: A front view application and a blind spot observation application.	algorithm;collision detection;grayscale;kalman filter;motion compensation;object detection;sampling (signal processing);tracking system	Norman Mattern;Robin Schubert;Christian Adam;Gerd Wanielik	2009			occupancy grid mapping;collision;blind spot;computer vision;occupancy;kalman filter;simulation;sampling (statistics);tracking system;artificial intelligence;computer science;automotive industry	Robotics	53.67748286848956	-37.88310787808142	63822
ea641cc0dda2c27822c5157829d9a87dccd18b77	motion planning using hierarchical aggregation of workspace obstacles	complexity theory;geometry;shape;robots;planning;collision avoidance;probabilistic logic	Sampling-based motion planning is the state-of-the-art technique for solving challenging motion planning problems in a wide variety of domains. While generally successful, their performance suffers from increasing problem complexity. In many cases, the full problem complexity is not needed for the entire solution. We present a hierarchical aggregation framework that groups and models sets of obstacles based on the currently needed level of detail. The hierarchy enables sampling to be performed using the simplest and most conservative representation of the environment possible in that region. Our results show that this scheme improves planner performance irrespective of the underlying sampling method and input problem. In many cases, improvement is significant, with running times often less than 60% of the original planning time.	level of detail;motion planning;sampling (signal processing);workspace	Mukulika Ghosh;Shawna L. Thomas;Marco Morales;Samuel Rodríguez;Nancy M. Amato	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759841	planning;robot;mathematical optimization;simulation;shape;artificial intelligence;mathematics;probabilistic logic	Robotics	51.866467189334166	-24.295206591705252	64544
7423381228ee7fc43373c7769ed118c66e355a78	global path planning for unmanned ground vehicle based on road map images	roads vehicles image color analysis navigation satellites trajectory;utm wg84 coordinate system a algorithm autonomous navigation dijkstra s algorithm path planning the shortest path estimation;a algorithm global path planning automatic navigation unmanned ground vehicle autonomous vehicle systems vehicle auto navigation online road map images directed road network;remotely operated vehicles cartography geophysical image processing path planning	In automatic navigation of mobile systems, first, they require providing a path network for robot/vehicle motion. Therefore, path planning is an important task of autonomous vehicle systems. To deal with the problem, this paper presents a method for constructing the shortest path, which support for vehicle auto-navigation in outdoor environments. The method using online road map images to estimate not only the shape of road network but also the directed road network, which could not be estimated by the use of only aerial/satellite images. The proposed method to solve this problem includes three stages. First, a raw network of path for motion is detected using the road map images. Second, the path network is converted to the Global coordinates, which provides a convenience for online auto-navigation task. Third, the shortest path for motion is estimated based on the A* algorithm. The experimental results demonstrate robustness and effectiveness of the method for path networks estimation under the large scene of outdoor environments.	a* search algorithm;aerial photography;autonomous car;autonomous robot;cognitive map;geographic coordinate system;global positioning system;motion planning;shortest path problem;unmanned aerial vehicle	Van-Dung Hoang;Danilo Cáceres Hernández;Joko Hariyono;Kang-Hyun Jo	2014	2014 7th International Conference on Human System Interactions (HSI)	10.1109/HSI.2014.6860453	computer vision;simulation;area navigation;geography;mobile robot navigation;remote sensing	Robotics	52.9100167746584	-36.5188081688048	64664
4a2a53f82ac4f150f48166a7c5215bcf7981b285	learning nonparametric policies by imitation	graph theory;robot sensing systems;humanoid robot;history;high dimensionality;sensors;gaussian processes;gaussian process regression;degree of freedom;degree of freedom robot;human teacher;sensor based feedback control nonparametric policies artificial intelligence human teacher high dimensional spaces probabilistic approach degree of freedom robot fujitsu hoap 2 humanoid robot graphical model probabilistic inference gaussian process regression;probabilistic approach;probabilistic inference;action plan;artificial intelligent;feedback;humanoid robots;sensors feedback gaussian processes graph theory humanoid robots regression analysis;robots robot sensing systems probabilistic logic humans history humanoid robots planning;number of factors;robots;fujitsu hoap 2 humanoid robot;sensor based feedback control;graphical model;nonparametric policies;artificial intelligence;planning;regression analysis;humans;probabilistic logic;high dimensional spaces;feedback control	A long cherished goal in artificial intelligence has been the ability to endow a robot with the capacity to learn and generalize skills from watching a human teacher. Such an ability to learn by imitation has remained hard to achieve due to a number of factors, including the problem of learning in high-dimensional spaces and the problem of uncertainty. In this paper, we propose a new probabilistic approach to the problem of teaching a high degree-of-freedom robot (in particular, a humanoid robot) flexible and generalizable skills via imitation of a human teacher. The robot uses inference in a graphical model to learn sensor-based dynamics and infer a stable plan from a teacherpsilas demonstration of an action. The novel contribution of this work is a method for learning a nonparametric policy which generalizes a fixed action plan to operate over a continuous space of task variation. A notable feature of the approach is that it does not require any knowledge of the physics of the robot or the environment. By leveraging advances in probabilistic inference and Gaussian process regression, the method produces a nonparametric policy for sensor-based feedback control in continuous state and action spaces. We present experimental and simulation results using a Fujitsu HOAP-2 humanoid robot demonstrating imitation-based learning of a task involving lifting objects of different weights from a single human demonstration.	artificial intelligence;dimensionality reduction;feedback;gaussian process;graphical model;hoap;humanoid robot;kriging;lambda lifting;randomized algorithm;robot locomotion;simulation	David B. Grimes;Rajesh P. N. Rao	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4650778	robot learning;computer vision;simulation;computer science;humanoid robot;artificial intelligence;graph theory;machine learning;feedback	Robotics	49.916045697330546	-26.93821273446307	64885
608f39f17cbb38cb6184d393f808cba94a6604f9	visibility-based probabilistic roadmaps for motion planning	software platform;global planning;path planning;configuration space;motion planning;probabilistic roadmaps;collision avoidance;probabilistic roadmap method	This paper presents a variant of probabilistic roadmap methods (PRM) that recently appeared as a promising approach to motion planning. We exploit a free-space structuring of the con guration space into visibility domains in order to produce small roadmaps, called visibility roadmaps. Our algorithm integrates an original termination condition related to the volume of the free space covered by the roadmap. The planner has been implemented within a software platform allowing us to address a large class of mechanical systems. Experiments show the ef ciency of the approach, in particular for capturing narrow passages of collision-free con guration spaces.	algorithm;computation;estimation of signal parameters via rotational invariance techniques;experiment;motion planning;movable type;plan;probabilistic roadmap;statistical relational learning	Thierry Siméon;Jean-Paul Laumond;Carole Nissoux	2000	Advanced Robotics	10.1163/156855300741960	computer vision;simulation;any-angle path planning;computer science;artificial intelligence;motion planning	Robotics	52.245725481726204	-24.40672806327667	64890
dabff7ca8461f82971e5ffc7a7d1ecc047e9e3f8	driver state monitoring with hierarchical classification		While vehicle automation increases in the near future human drivers will still be responsible for monitoring of the driving environment or as a fallback for critical situations. Thus, systems for autonomy levels 2 to 3 require proper knowledge about the driver's state including, for example, the seating position, activity, or hands-on-steering-wheel status. We introduce an interior sensing system to classify the state of the driver inside a vehicle based on images of a single time-of-flight camera. We present an efficient and robust approach based on a hierarchical label structure and a cost function which focuses only on relevant labels inside this hierarchical label structure. Our approach reduces the number of output neurons while creating a fallback option inside the classifier simultaneously. This leads to improved classification results compared to common approaches. In our experiments we demonstrate a solution for combined assessment of seat occupancy, driving position, hands on steering wheel, and driver-object interaction.		Patrick Weyers;Alexander Barth;Anton Kummert	2018	2018 21st International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2018.8569467		Robotics	48.47378327994733	-32.47779544000511	65006
d678f287e9c5165e087e40a92a23ed1ce7661d52	recognition and classification of path features with self-organizing maps during reactive navigation	sonar patterns;self organizing maps;cognitive robotics;sonar applications;neural model;turning;mobile robot;heuristic programming;hospitals;mobile robots computerised navigation neurocontrollers self organising feature maps sonar signal processing feature extraction pattern classification heuristic programming;mobile robots;data mining;reactive navigation;heuristic procedures;feature categorization;incremental learning;self organizing neural model;self organising feature maps;free paths;self organizing feature maps;feature extraction;pattern classification;turning point;path feature classification;mobile robot path feature recognition path feature classification self organizing maps reactive navigation self organizing neural model occupancy bitmaps sonar patterns obstacle boundaries free paths heuristic procedures topological correctness feature distinction feature categorization;self organizing feature maps sonar navigation turning mobile robots feature extraction self organizing networks sonar applications hospitals cognitive robotics data mining;self organization;topological correctness;self organized map;sonar navigation;neurocontrollers;path feature recognition;sonar signal processing;obstacle boundaries;feature distinction;occupancy bitmaps;computerised navigation;self organizing networks	The paper focuses on recognition and classification of path features during navigation of a mobile robot. The extracted features play the role of relevant navigation situations as (in a corridor), (at a turning point), (in a narrow passage). The method is an incremental learning and classification technique, based on a self-organizing neural model. Two different self-organizing networks are used to encode occupancy bitmaps generated from sonar patterns in terms of obstacles boundaries and free paths, and heuristic procedures are applied to these growing networks to add and prune units, to determine topological correctness between units, to distinguish and categorize features.	organizing (structure);self-organization;self-organizing map	Gianni Vercelli;Pietro G. Morasso	1998		10.1109/IROS.1998.724792	mobile robot;computer vision;computer science;artificial intelligence;machine learning	Robotics	50.05420673749204	-29.46819221862745	65430
026c5eeb946f404c1777b3acd091b86a64ad1e96	large scale semi-global matching on the cpu	embedded processors semiglobal matching cpu sgm real time stereo vision automotive context reconfigurable hardware fpga graphics hardware gpu intelligent vehicles fine grained parallelization kitti benchmark lidar vga image pairs;image coding training data field programmable gate arrays graphics processing units hardware degradation sociology;stereo image processing field programmable gate arrays graphics processing units image matching optical radar real time systems	Semi-Global Matching (SGM) is widely used for real-time stereo vision in the automotive context. Despite its popularity, only implementations using reconfigurable hardware (FPGA) or graphics hardware (GPU) achieve high enough frame rates for intelligent vehicles. Existing real-time implementations for general purpose PCs use image and disparity sub-sampling at the expense of matching quality. We study methods to improve the efficiency of SGM on general purpose PCs, through fine grained parallelization and usage of multiple cores. The different approaches are evaluated on the KITTI benchmark, which provides real imagery with LIDAR ground truth. The system is able to compute disparity maps of VGA image pairs with a disparity range of 128 values at more than 16 Hz. The approach is scalable to the number of available cores and portable to embedded processors.	advanced vector extensions;algorithm;autonomous car;benchmark (computing);binocular disparity;central processing unit;embedded system;field-programmable gate array;graphics hardware;graphics processing unit;ground truth;high- and low-level;map;online and offline;parallel computing;real-time clock;real-time transcription;reconfigurable computing;simd;sampling (signal processing);scalability;second generation multiplex;semiconductor industry;stereopsis;video graphics array	Robert Spangenberg;Tobias Langner;Sven Adfeldt;Raúl Rojas	2014	2014 IEEE Intelligent Vehicles Symposium Proceedings	10.1109/IVS.2014.6856419	embedded system;real-time computing;computer science;real-time computer graphics;computer graphics (images)	Vision	43.32955347141757	-35.79358305575286	65594
44b599d84398747c87027245a846c0efb8252e70	computing the sensory uncertainty field of a vision-based localization sensor	maximum likelihood estimation image sensors mobile robots path planning robot vision covariance matrices;uncertain systems;performance evaluation;computer vision uncertainty sensor phenomena and characterization computational modeling covariance matrix robustness motion planning sonar navigation dispersion image sensors;perforation;path planning;mobile robot sensory uncertainty field vision based localization sensor robust motion planners configuration space robust navigation algorithms;path planning robot vision mobile robots image sensors position measurement uncertain systems;mobile robots;indexing terms;image sensors;maximum likelihood estimation;configuration space;robot vision;computer vision uncertainty sensor phenomena and characterization covariance matrix space technology robustness sonar navigation dispersion testing mathematics;covariance matrices;position measurement;motion planning;robust navigation algorithms sensory uncertainty field vision based localization sensor robust motion planners configuration space performance map vision based sensor real images synthetic images closed form formulas sensing strategies	It has been recognized that robust motion planners should take into account the varying performance of localization sensors across the configuration space. Although a number of works have shown the benefits of using such a performance map, the work on actual computation of such a performance map has been limited and has addressed mostly range sensors. Since vision is an important sensor for localization, it is important to have performance maps of vision sensors. We present a method for computing the performance map of a vision-based sensor. We compute the map and show that it accurately describes the actual performance of the sensor, both on synthetic and real images. The method we use involves evaluating closed form formulas and hence is very fast. Using the performance map computed by this method for motion planning and for devising sensing strategies will contribute to more robust navigation algorithms.		Amit Adam;Ehud Rivlin;Ilan Shimshoni	2001	IEEE Trans. Robotics and Automation	10.1109/70.938383	control engineering;computer vision;simulation;computer science;engineering;artificial intelligence;motion planning	Robotics	52.74328326777059	-35.63697214884005	65668
89bd525373ee07868d63799d226eab5ecd0b1885	modelling threshold exceedence levels for spatial stochastic processes observed by sensor networks	standards;polynomials;stochastic processes monitoring wireless sensor networks mathematical model standards polynomials estimation;generalized pareto models sensor network quantile regression stochastic processes extreme value theory;estimation;monitoring;stochastic processes;mathematical model;wireless sensor networks	We develop a new framework for explicitly modelling the threshold exceedence levels of the spatial stochastic process being monitored by a sensor network. Our framework also allows incorporating additional observed features as explanatory factors for the behaviour of the spatial stochastic process, and in particular the probability of exceedence of a user defined threshold level in any given region of space. Such a model has many practical applications for accurate decision making under uncertainty when the monitored process exceeds user specified critical thresholds.	decision theory;image noise;stochastic process	Gareth W. Peters;Ido Nevat;Shaowei Lin;Tomoko Matsui	2014	2014 IEEE Ninth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)	10.1109/ISSNIP.2014.6827635	stochastic geometry models of wireless networks;econometrics;continuous-time stochastic process;computer science;machine learning;statistics	Robotics	39.54623241124979	-25.49009469560898	66178
536e4c6ac178e4ad3a0bc5a78372e4a9447f4c9a	a general purpose, expandable processor for real-time computer graphics	real time computing	"""A class of critical computer requirements for real-time scan T.V. computer graphics is examined in relation to commercially available CPU architectures. Finding general purpose processors not suited, a new processor is proposed which is designed around the concept of """"instruction set partitioning."""" In this design, special hardware-implemented algorithms may be included in the machine instruction set, and these processors allowed to operate asynchronously from each other. The design is projected to generate a complete new frame of a color T.V. picture every 0.1-0.8 seconds depending on image complexity. Due to its inherent generality, the CPU may be similarly expanded to encompass a wide variety of other specialized, or real-time tasks with minimal additional hardware. The 32-bit parallel processor has a design cycle time of 100 ns and is in the price class of a minicomputer."""	real-time computer graphics;real-time transcription	Jeffrey F. Eastman;David R. Wooten	1975	Computers & Graphics	10.1016/0097-8493(75)90036-9	computer architecture;graphics pipeline;computer hardware;computer science;real-time computer graphics;computer graphics (images)	Graphics	43.607051258318876	-31.695099284943737	66179
69636e67ff0b90903b0413348618e96d06d5a3aa	human influence of robotic swarms with bandwidth and localization issues	robot sensing systems;standards;bandwidth humans robot sensing systems spatial resolution standards color;color;path planning;bandwidth limitation human swarm interaction;radio limitations human influence robotic swarms bandwidth issues localization issues local rules complex emergent behaviors human swarm interaction hsi indoor search rescue task underwater exploration bandwidth restriction swarm foraging task;mobile robots;human robot interaction;path planning human robot interaction mobile robots multi robot systems;bandwidth limitation;multi robot systems;bandwidth;humans;human swarm interaction;spatial resolution	Swarm robots use simple local rules to create complex emergent behaviors. The simplicity of the local rules allows for large numbers of low-cost robots in deployment, but the same simplicity creates difficulties when deploying in many applicable environments. These complex missions sometimes require human operators to influence the swarms towards achieving the mission goals. Human swarm interaction (HSI) is a young field with few user studies exploring operator behavior. These studies all assume perfect information between the operator and the swarm, which is unrealistic in many applicable scenarios. Indoor search and rescue or underwater exploration may present environments where radio limitations restrict the bandwidth of the robots. This study explores this bandwidth restriction in a user study. Three levels of bandwidth are explored to determine what amount of information is necessary to accomplish a swarm foraging task. The lowest bandwidth condition performs poorly, but the medium and high bandwidth condition both perform well. The medium bandwidth condition does so by aggregating useful swarm information to compress the state information. Further, the study shows operators preferences that should have hindered task performance, but operator adaptation allowed for error correction.	emergence;error detection and correction;geographic information system;horizontal situation indicator;interaction;robot;software deployment;swarm intelligence;swarm robotics;text simplification;usability testing	Steven Nunnally;Phillip M. Walker;Andreas Kolling;Nilanjan Chakraborty;Michael Lewis;Katia P. Sycara;Michael A. Goodrich	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6377723	human–robot interaction;mobile robot;swarm robotics;simulation;image resolution;computer science;artificial intelligence;motion planning;bandwidth	Robotics	53.62347903497281	-30.950675999114875	66223
3d1b6c80c021ef555ef78e1513d8d54da5793366	autonomous robotic exploration using a utility function based on rényi's general theory of entropy	autonomous exploration;graph slam;entropy	In this paper we present a novel information-theoretic utility function for selecting actions in a robot-based autonomous exploration task. The robot’s goal in an autonomous exploration task is to create a complete, high-quality map of an unknown environment as quickly as possible. This implicitly requires the robot to maintain an accurate estimate of its pose as it explores both unknown and previously observed terrain in order to correctly incorporate new information into the map. Our utility function simultaneously considers uncertainty in both the robot pose and the map in a novel way and is computed as the difference between the Shannon and the Renyi entropy of the current distribution over maps. Renyi’s entropy is a family of functions parameterized by a scalar, with Shannon’s entropy being the limit as this scalar approaches unity. We link the value of this scalar parameter to the predicted future uncertainty in the robot’s pose after taking an exploratory action. This effectively decreases the expected information gain of the action, with higher uncertainty in the robot’s pose leading to a smaller expected information gain. Our objective function allows the robot to automatically trade off between exploration and exploitation in a way that does not require manually tuning parameter values, a significant advantage over many competing methods that only use Shannon’s definition of entropy. We use simulated experiments to compare the performance of our proposed utility function to these state-of-the-art utility functions. We show that robots that use our proposed utility function generate maps with less uncertainty and fewer visible artifacts and that the robots have less uncertainty in their pose during exploration. Finally, we demonstrate that a real-world robot using our proposed utility function is able to successfully create a high-quality map of an indoor office environment.	robot;robotic spacecraft;utility	Henry Carrillo;Philip M. Dames;Vijay Kumar;José A. Castellanos	2018	Auton. Robots	10.1007/s10514-017-9662-9	artificial intelligence;computer science;scalar (physics);simulation;computer vision;parameterized complexity;rényi entropy;machine learning;robot;trade-off	Robotics	51.12759237113939	-25.920531182281895	66246
415a3b57ce2bcb5861fd682431835e2420b7cd06	sensor node localisation using a stereo camera rig	detectors;light emitting diode;low power;localisation;sensor;stereo vision;sensor nodes;stereo;environmental pollution;rig;node;camera	In this paper, we use stereo vision processing techniques to detect and localise sensors used for monitoring simulated environmental events within an experimental sensor network testbed. Our sensor nodes communicate to the camera through patterns emitted by light emitting diodes (LEDs). Ultimately, we envisage the use of very low-cost, low-power, compact microcontroller-based sensing nodes that employ LED communication rather than power hungry RF to transmit data that is gathered via existing CCTV infrastructure. To facilitate our research, we have constructed a controlled environment where nodes and cameras can be deployed and potentially hazardous chemical or physical plumes can be introduced to simulate environmental pollution events in a controlled manner. In this paper we show how 3D spatial localisation of sensors becomes a straightforward task when a stereo camera rig is used rather than a more usual 2D CCTV camera.	closed-circuit television;diode;low-power broadcasting;microcontroller;radio frequency;sensor node;simulation;stereo camera;stereopsis;testbed	Dermot Diamond;Noel E. O'Connor;Alan F. Smeaton;Stephen Beirne;Brian Corcoran;Philip Kelly;King Tong Lau;Roderick Shepherd	2007		10.1145/1278972.1278983	embedded system;stereo cameras;computer vision;engineering;remote sensing	Mobile	51.15047621038272	-32.22032157968767	66251
3d8a0a2a2b32da0f4a0946d82c93b9b43dc1eff7	application of rbf neural network to temperature compensation of gas sensor	visual c;temperature compensation rbf neural network gas sensor;temperature sensors;temperature compensation method;gas sensor;environmental temperature;radial basis function networks;artificial neural networks;compensation;rbf neural network;visual c rbf neural network temperature compensation gas sensor temperature compensation method;neurons;temperature measurement;gas detectors;temperature;radial basis function networks compensation gas sensors;gas sensors;temperature compensation;neural network;neural networks temperature sensors gas detectors radial basis function networks neurons transfer functions stability layout hardware circuits	Gas sensor is vulnerable to the impact of environmental temperature, thereby limiting its accuracy. In order to overcome this shortcoming, the paper proposes a new temperature compensation method based on RBF neural network, which is realized with Visual C++ 6.0 program software. The result of experiment indicates that the biggest error of the sensor outputs may be up to 20.0 percent before temperature compensation. After we adopted the temperature compensation method based on BP neural network, the biggest error reduced to 1.44 percent, even down to 0.12 percent through the method based on RBF neural network. Therefore this way has better effect on the temperature compensation so that the gas sensor may have higher accuracy and temperature stability after compensation.	artificial neural network;c++;radial basis function;sensor web	Weimin Hao;Xiaohui Li;Minglu Zhang	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.735	temperature;temperature measurement;computer science;machine learning;control theory;artificial neural network	Robotics	40.85111637868384	-24.195603394457585	66340
d3d12ae714668ad9fc600b029f0e7b3b2a95149a	sensor space planning with applications to construction environments	computational complexity sensors planning construction industry civil engineering computing;lidar information goal sensor planning sensor space planning construction environments construction industry software planning tool construction information goals computational complexity sensor placements;sensors;construction industry;civil engineering computing;computational complexity;sensor placement;sensor phenomena and characterization laser radar sensor systems application software inspection cost function service robots robot sensing systems orbital robotics robotics and automation;planning;set cover	Outlined is a new approach to sensor space planning and its application to the construction industry. The software planning tool described here generates sensor placements automatically for use in assessing deviations in construction environments. The first step is to separate construction information goals into clusters, simplifying the planning space in order to reduce computational complexity. For each cluster, the planner generates the space of potential sensor placements for a set of information goals and selects a minimal set of subspaces to take advantage of views that can achieve multiple goals simultaneously. Sensing locations are chosen that maximize the probability of achieving each goal and a path is generated to minimize the transit cost between the various sensing locations within each cluster. Finally, paths are generated that minimize the transit cost between clusters. This method is demonstrated on a desktop computer and shown to support LIDAR information goal sensor planning within a construction site.	computational complexity theory;computer cluster;desktop computer;sensor	Edward Latimer;IV Latimer DeWittLatimer;Rajiv Saxena;Catherine Lyons;L. Michaux-Smith;Scott Thayer	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1302419	planning;simulation;computer science;engineering;sensor;transport engineering;set cover problem;computational complexity theory	Robotics	53.39643151475142	-27.050082153042005	66349
077996cd68ffea2132070a58b26b32c696e43599	aerial robotic tracking of a generalized mobile target employing visual and spatio-temporal dynamic subject perception	visualization;estimation;heuristic algorithms;robots;mobile communication;robot vision autonomous aerial vehicles closed loop systems collision avoidance;vehicle dynamics;cameras;aerial robotic tracking temporal invariance mobile tracking collision free trajectory generation algorithms environment structure autonomous aerial robotic tracking structural invariance closed loop process feature tracking visual cues mesh connection algorithm 3d visible scenery representation unknown map dynamic generalized subject visual tracking spatio temporal dynamic subject perception visual dynamic subject perception generalized mobile target;visualization robots cameras heuristic algorithms mobile communication vehicle dynamics estimation	This paper proposes a methodology for visual tracking of a dynamic generalized subject within an unknown map, by relying on its perception as a separate entity which can be distinguished spatially and visually from its environment. To this purpose, a 3D-representation of the visible scenery is examined, and the subject is spatially identified by its externally viewed hull via a mesh-connection algorithm aided by visual cues, and visually identified by distinct feature tracking based on an incrementally built list of key-aspects. These two processes operate in closed-loop, and employing a set of assumptions regarding the subject's structural/temporal invariance the tracking health state can be determined. This work additionally presents the framework for the deployment of this scheme for autonomous aerial robotic subject tracking, employing the dynamic subject/environment distinction to obtain knowledge of the environment structure, and collision-free trajectory generation algorithms to achieve mobile tracking.	aerial photography;algorithm;autonomous robot;motion estimation;software deployment;video tracking	Christos Papachristos;Dimos Tzoumanikas;Anthony Tzes	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353989	robot;control engineering;computer vision;estimation;vehicle dynamics;simulation;visualization;mobile telephony;tracking system;computer science;artificial intelligence;statistics	Robotics	50.53986226672086	-35.83945077041143	66444
b1d730ad00d877b5ac6a1abb720483945a1904fd	experience replay for real-time reinforcement learning control	erbium;erbium trajectory approximation algorithms approximation methods real time systems learning complexity theory;manipulators;approximate algorithm;complexity theory;learning;approximation algorithms;approximation method;real time control;nonlinear control systems;real time;reinforcement learning;q learning;robotics;reinforcement learning rl;robotic manipulator control experience replay real time reinforcement learning control optimal control nonlinear system stochastic system reinforcement learning algorithm pendulum swing up problem vision based control goalkeeper robot incremental rl technique approximate q learning sarsa algorithm state action reward state action algorithm;stochastic system;optimal control;learning systems;trajectory;sarsa experience replay er q learning real time control reinforcement learning rl robotics;simulation study;approximation methods;experience replay er;learning artificial intelligence;pendulums;sarsa;real time systems learning artificial intelligence learning systems manipulators nonlinear control systems optimal control pendulums;real time systems	Reinforcement-learning (RL) algorithms can automatically learn optimal control strategies for nonlinear, possibly stochastic systems. A promising approach for RL control is experience replay (ER), which learns quickly from a limited amount of data, by repeatedly presenting these data to an underlying RL algorithm. Despite its benefits, ER RL has been studied only sporadically in the literature, and its applications have largely been confined to simulated systems. Therefore, in this paper, we evaluate ER RL on real-time control experiments that involve a pendulum swing-up problem and the vision-based control of a goalkeeper robot. These real-time experiments are complemented by simulation studies and comparisons with traditional RL. As a preliminary, we develop a general ER framework that can be combined with essentially any incremental RL technique, and instantiate this framework for the approximate Q-learning and SARSA algorithms. The successful real-time learning results that are presented here are highly encouraging for the applicability of ER RL in practice.	approximation algorithm;erdős–rényi model;experiment;inverted pendulum;iteration;nonlinear system;optimal control;q-learning;real-time clock;real-time locating system;real-time transcription;reinforcement learning;robot;robotic arm;simulation;state-action-reward-state-action;stochastic process	Sander Adam;Lucian Busoniu;Robert Babuska	2012	IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)	10.1109/TSMCC.2011.2106494	pendulum;erbium;real-time control system;optimal control;computer science;artificial intelligence;trajectory;machine learning;sarsa;control theory;robotics;reinforcement learning;q-learning	Robotics	50.367025219922795	-26.909618252223297	66726
36d87a35b7e19f5a110991ad9a8d2fa51ec21100	a memory-efficient architecture of full hd around view monitor systems	field programmable gate arrays memory efficient architecture view monitor systems smart vision systems advanced driver assistance systems full high definition level resolution high memory performance full hd image processing multiple cameras full hd avm system high memory bandwidth input pixel data single write performance dram subsystem dram aware data mapping ddr2 200 sdrams;memory architecture dram chips driver information systems image resolution;random access memory cameras bandwidth dram chips memory management high definition video;memory bandwidth advanced driver assistance system adas around view monitor avm full high definition hd	The around view monitor (AVM) is one of the representative features of smart vision systems adopted in various application areas, e.g., advanced driver assistance systems. The design of AVM systems with full high-definition (HD)-level resolution presents significant technical challenges. In particular, a high-memory performance is required to process full HD images obtained from multiple cameras. Specifically, a full HD AVM system requires a high-memory bandwidth (six times higher than D1 image-based systems) and is characterized by a significant amount of single writes, which degrades the effective performance of modern DRAM. To address these problems, two methods are proposed in this paper. The first method reduces the required memory bandwidth by storing in the memory only the input pixel data that will be used in the final processing step, and the second improves the single-write performance of a DRAM subsystem by DRAM-aware data mapping. The effectiveness of the proposed methods is proved by designing an AVM system incorporating field-programmable gate arrays and DDR2-200 SDRAMs. The proposed methods reduce the memory bandwidth requirement by 51%, allowing a full HD AVM system to run at over 24 frames per second.	astronomy visualization metadata;dynamic random-access memory;field-programmability;field-programmable gate array;hdmi;memory bandwidth;pixel	Byeongchan Jeon;Gyuro Park;Junseok Lee;Sungjoo Yoo;Hong Jeong	2014	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2014.2325215	embedded system;interleaved memory;real-time computing;cas latency;computer hardware;computer science;computer memory;memory controller;universal memory;registered memory	Visualization	43.65932018967627	-35.490021956895085	66741
a748ae30a6386b6a10f2e6268f80cc8fa6dcdc33	modified vector field histogram with a neural network learning model for mobile robot path planning and obstacle avoidance		In this work, a Modified Vector Field Histogram (MVFH) has been developed to improve path planning and obstacle avoidance for a wheeled driven mobile robot. It permits the detection of unknown obstacle to avoid collisions by simultaneously a steering the mobile robot toward the target; a regular grid map representation for a work space environment is caurried out. A Neural Network (NN) model is used to learn many critical situations of environment during robot navigation among obstacles using MVFH. Also, digital filter has been utilized for improving the robustness of obstacle avoidance trajectory of mobile robot. The proposed MVFH-NN has been implemented and tested by using MobotSim program simulation and MATLAB . The developed algorithm showed good navigation properties and can be used in complex real world (maze-like environment), it also shows good ability to overcome limitations of the traditional VFH algorithm (like wide candidate valley, narrow hallway and target distance limitation).	algorithm;artificial neural network;digital filter;matlab;mobile robot;motion planning;obstacle avoidance;regular grid;robotic mapping;simulation;vector field histogram	Bahaa I. Kazem;Ali H. Hamad;Mustafa M. Mozael	2010	Int. J. Adv. Comp. Techn.		mobile robot;computer vision;vector field histogram;computer science;artificial intelligence;machine learning;motion planning;obstacle avoidance;artificial neural network	Robotics	53.249237143498924	-32.63587782992303	67021
8e58f95f57b1ce3914681614575f5ed7910f9f03	autonomous exploration by expected information gain from probabilistic occupancy grid mapping		Occupancy grid maps are spatial representations of environments, where the space of interest is decomposed into a number of cells that are considered either occupied or free. This paper focuses on exploring occupancy grid maps by predicting the uncertainty of the map. Based on recent improvements in computing occupancy probability, this paper presents a novel approach for selecting robot poses designed to maximize expected map information gain represented by the change in entropy. This result is simplified with several approximations to develop an algorithm suitable for real-time implementation. The predicted information gain proposed in this paper governs an effective autonomous exploration strategy when applied in conjunction with an existing motion planner to avoid obstacles, which is illustrated by numerical examples.	algorithm;approximation;approximation algorithm;autonomous robot;computation;entropy (information theory);information gain in decision trees;kullback–leibler divergence;map;numerical analysis;real-time clock;real-time computing	Evan Kaufman;Taeyoung Lee;Zhuming Ai	2016	2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)	10.1109/SIMPAR.2016.7862403	computer vision;simulation;artificial intelligence;occupancy grid mapping	Robotics	51.7241852812816	-25.759163226091513	67905
bc5fbe1897f0f3373fca4e76504d9d70fa0e78f2	design considerations of real-time adaptive beamformer for medical ultrasound research using fpga and gpu	medical ultrasound;performance evaluation;conference_paper;image resolution;reconfigurable architectures array signal processing biomedical ultrasonics field programmable gate arrays graphics processing units image resolution medical image processing performance evaluation real time systems;reconfigurable architectures;medical ultrasound systems;array signal processing;design considerations;adaptive beamformer;medical image processing;graphics processing units;field programmable gate arrays;array signal processing graphics processing units field programmable gate arrays ultrasonic imaging imaging covariance matrix real time systems;development time;adaptive beamforming;real time reconfigurable image formation system real time adaptive beamformer design considerations medical ultrasound research medical imaging quality improvement medical ultrasound systems lateral resolution image contrast imaging penetration algorithm exploration framework high performance platform solution reprogrammable platform real time adaptive beamforming performance evaluation gpu based solutions fpga based solutions real time throughput i o bandwidth;biomedical ultrasonics;real time systems	Adaptive beamforming has been well considered as a potential solution for improving the imaging quality of medical ultrasound systems. Despite the promised improvement in lateral resolution, image contrast and imaging penetration, the use of adaptive beamforming is substantially more computationally demanding than conventional delay-and-sum beamformers. While a dedicated hardware solution may be able to address the computational demand of one particular design, the need for an efficient algorithm exploration framework demands a platform solution that is high-performance and easily reprogrammable. To that end, the use of FPGA and GPU for implementing real-time adaptive beamforming on such platform has been explored. The results are evaluated quantitatively in terms of performance and image quality, and qualitatively with respect to ease of system integration and ease of use. In our test cases, both FPGA- and GPU-based solutions achieved real-time throughput exceeding 80 frames-per-second, and over 38x improvement when compared to our baseline CPU implementation. While the development time on GPU platform remains much lower than its FPGA counterpart, the FPGA solution is effective in providing the necessary I/O bandwidth to enable an end-to-end real-time reconfigurable image formation system.		Junying Chen;Alfred C. H. Yu;Hayden Kwok-Hay So	2012	2012 International Conference on Field-Programmable Technology	10.1109/FPT.2012.6412134	embedded system;real-time computing;computer science;adaptive beamformer	EDA	41.24405500133263	-33.36120058156285	68048
e25bb05f9fde0fa05026eb385ef1a577b23bd7a4	map building and object tracking inside intelligent spaces using static and mobile sensors	environment maps;experimental tests;mobile sensor;estimation theory;covariance analysis;tracking system;mobile sensors;map building;estimation method;mobile robot;path planning;kalman filters;kalman filter;mobile robots;intelligent space;laser ranging;image sensors;distributed sensors;environment mapping;laser range finder;covariance intersection map building object tracking intelligent space static sensor mobile sensor environment mapping distributed sensors mobile robot laser range finder estimation method kalman filter;space use;object tracking;intelligent sensors intelligent structures sensor phenomena and characterization mobile robots orbital robotics sensor systems intelligent robots robot sensing systems space technology intelligent networks;target tracking covariance analysis distributed sensors estimation theory image sensors kalman filters laser ranging mobile robots object detection path planning;target tracking;static sensor;object detection;covariance intersection	This paper deals with the problem of object tracking and environment mapping inside a space with distributed sensors - intelligent space. In a conventional approach the distributed sensors are used for these tasks, however since the sensors are static this has several disadvantages. In this paper in addition to static sensors we introduce the use of a mobile robot as mobile sensor to gather additional information and improve the estimation performance. We discuss the characteristics of such a tracking system, mainly concentrating on a system that uses laser range finders as both mobile and static sensors. Estimation methods based on Kalman filter and covariance intersection are presented and analyzed. Finally, the presented methods are experimentally tested.	algorithm;computation;covariance intersection;experiment;extended kalman filter;kalman filter;mobile phone;mobile robot;reflection mapping;robot control;sensor;spaces;tracking system	Drazen Brscic;Hideki Hashimoto	2007	2007 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2007.4399582	kalman filter;control engineering;mobile robot;computer vision;computer science;engineering;artificial intelligence;control theory	Robotics	53.407783169808646	-34.049832786462275	68051
1b8443ffe68778d6d9022dc5cc315342659278eb	dense 3-d mapping with spatial correlation via gaussian filtering		Constructing an occupancy representation of the environment is a fundamental problem for robot autonomy. Many accurate and efficient methods exist that address this problem but most assume that the occupancy states of different elements in the map representation are statistically independent. The focus of this paper is to provide a model that captures correlation of the occupancy of map elements. Correlation is important not only for improved accuracy but also for quantifying uncertainty in the map and for planning autonomous mapping trajectories based on the correlation among known and unknown areas. Recent work proposes Gaussian Process (GP) regression to capture covariance information and enable resolution-free occupancy estimation. The drawback of techniques based on GP regression (or classification) is that the computation complexity scales cubically with the length of the measurement history. Our main contribution is a new approach for occupancy mapping that models the binary nature of occupancy measurements precisely, via a Bernoulli distribution, and provides an efficient approximation of GP classification with complexity that does not scale with time. We prove that the error between the estimates provided by our method and those provided by GP classification is negligible. The proposed method is evaluated using both simulated data and real data collected using a Velodyne Puck 3-D range sensor.	algorithm;approximation;autonomous robot;autonomy;bernoulli polynomials;binary prefix;computation;emoticon;expectation propagation;gaussian process;kalman filter;multidimensional digital pre-distortion;scalability;simulation;whole earth 'lectronic link	Ke Sun;Kelsey Saulnier;Nikolay Atanasov;George J. Pappas;Vijay Kumar	2018	2018 Annual American Control Conference (ACC)	10.23919/ACC.2018.8431777	occupancy;independence (probability theory);kernel (linear algebra);control theory;computer science;spatial correlation;filter (signal processing);computational complexity theory;gaussian process;pattern recognition;artificial intelligence;covariance	Robotics	40.00596562278807	-26.203907627500122	68269
d54f53da5b2c91ad25651f3d6486f20eb38657c6	robot motion planning: multi-sensory uncertainty fields enhanced with obstacle avoidance	uncertain systems;mobile robot;path planning;mobile robots;navigation;obstacle avoidance;robot motion planning;indoor environment;robot motion motion planning uncertainty robot sensing systems navigation mobile robots computer science technology planning motion estimation indoor environments;sensor fusion;indoor environments robot motion planning msuf multisensory uncertainty field suf obstacle avoidance robot sensors navigation algorithm navigation resuming technique;computerised navigation mobile robots path planning uncertain systems sensor fusion navigation;computerised navigation	Robot motion planning is being approached in this paper b y estimating the uncertainty of its configuration that is computed b y the robot sensors. Since mobile robotic platforms are usually equipped with a variety of range sensors, measurements returned from all sensors are employed for the above estimation. The notion of sensory uncertainty fields (SUF’s), recently proposed, is being extended to incorporate all the available sources of external sensory data . The multi-sensory uncertainty f ie ld (MSUF) is introduced which results in more accurate configuration estimation. Moreover, in order t o cope with unexpected objects (obstacles) encountered a t execution time, the navigation algorithm is augmented with an obstacle avoidance and navigation resuming technique. The introduction of multip l e sensors and obstacle avoidance facilitates accurate navigation in indoor environments and in the presence of unexpected objects. This is demonstrated b y navigation results obtained from an implementation of this method.	algorithm;motion planning;obstacle avoidance;robot;run time (program lifecycle phase);sensor	Panos E. Trahanias;Yiannis Komninos	1996		10.1109/IROS.1996.568963	control engineering;mobile robot;computer vision;simulation;computer science;artificial intelligence;robot control;obstacle avoidance;mobile robot navigation	Robotics	53.46565546978171	-33.92701750380803	68280
3bd07ff3bdbe067cbf78355ec9b57c0576111487	map building by sequential estimation of inter-feature distances	map building;mobile robot;multi dimensional scaling;slam;prior information;bayesian filtering;mapping;sequential estimation	This paper proposes an alternative solution to a mapping problem in two different cases; when bearing measurement to features (landmarks) and odometry are measured and when local position of features are measured. Our approach named M-SEIFD (Mapping by Sequential Estimation of Inter-Feature Distances) first estimates inter-feature distances, then finds global position of all features by enhanced multi-dimensional scaling (MDS). M-SEIFD is different from the conventional SLAM methods based on Bayesian filtering in that robot self-localization is not compulsory and that M-SEIFD is able to utilize prior information about relative distances among features directly. We show that M-SEIFD is able to achieve a decent map of features both in simulation and in real-world environment with a mobile robot.		Atsushi Ueta;Takehisa Yairi;Hirofumi Kanazaki;Kazuo Machida	2008		10.1007/978-3-540-89197-0_41	sequential estimation;mobile robot;computer vision;simulation;multidimensional scaling;computer science;artificial intelligence;machine learning;statistics	EDA	52.526238202161885	-34.671416748516606	68433
e96814a114a7337aefc46c3b2cccf2bc8dbd3ef4	watermarking on cnn-um for image and video authentication	filigranage numerique;protection information;analisis imagen;digital watermarking;watermarking;microprocessor;detecteur image;image processing;video signal processing;generateur forme;real time;authentication;generador forma;pattern generation;cnn um;procesamiento imagen;multimedia application;nonlinear;cellular neural nets;traitement image;authentification;chip;reseau neuronal cellulaire;prototipo;autenticacion;proteccion informacion;information protection;pattern generator;automate cellulaire;system;filigrana digital;traitement signal video;universal machine;image analysis;microprocesseur;detector imagen;sista;cellular automata;analyse image;prototype;microprocesador;cellular automaton;image sensor;automata celular	In this paper a new approach to fragile watermarking technique is introduced. This problem is particularly interesting in the field of modern multimedia applications, when image and video authentication are required. The approach exploits the Cellular Automata suitability to work as Pseudorandom Pattern Generators and extends the related algorithms under the framework of the Cellular Nonlinear Networks (CNNs). The result is a novel way to perform watermarking generation in real time, using the presently available CNN-Universal chip prototypes. In this paper, both the CNN algorithms for fragile watermarking as well as on-chip experimental results are reported, confirming the suitability of CNNs to successfully act as real-time watermarking generators. The availability of CNN-based visual microprocessors allows to have powerful algorithms to watermark in real time images or videos for efficient smart camera applications.	algorithm;authentication;cellular automaton;digital watermarking;microprocessor;pseudorandom number generator;pseudorandomness;real-time clock;smart camera	Müstak E. Yalçin;Joos Vandewalle;Paolo Arena;Adriano Basile;Luigi Fortuna	2004	I. J. Circuit Theory and Applications	10.1002/cta.297	cellular automaton;telecommunications;digital watermarking;computer science;electrical engineering;theoretical computer science;authentication;algorithm;computer graphics (images)	EDA	42.38706144593428	-30.243726304657805	69195
1d3507f56c29fd51acc5f5d4fcef2d2c6649022b	a document page classification algorithm in copy pipeline	document page classification algorithm;classification algorithm;image classification document image processing;real time;copy quality;image classification;page classification document classification;low complexity;user interaction simplification;copy mode selector;page classification;document image processing;copy pipeline;document classification;user interaction;classification algorithms pipelines strips image segmentation printers art graphics brightness rendering computer graphics stray light;user interaction simplification document page classification algorithm copy pipeline copy mode selector copy quality	This paper describes a real-time, strip-based, low-complexity document page classification algorithm, which can be used as a copy mode selector in the copy pipeline. The benefits of such a copy mode selector include improving copy quality, simplifying user interaction, and increasing copy rate.	algorithm;interaction;real-time clock	Xiaogang Dong;Peter Majewicz;Gordon McNutt;Charles A. Bouman;Jan P. Allebach;Ilya Pollak	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379290	contextual image classification;computer science;data mining;world wide web;information retrieval	Robotics	39.50900540807811	-35.44029558377667	69311
330320a0938113bb12a5aec7ef3cf4b815ea185c	towards hardware stereoscopic 3d reconstruction a real-time fpga computation of the disparity map	edge detection;embedded systems;field programmable gate arrays;image reconstruction;logic design;stereo image processing;sobel edge detector;virtex2 pro fpga;algorithmic parameter;computer vision;correlation window size;design exploration;disparity map;disparity range;embedded hardware;embedded system;hardware stereoscopic 3d reconstruction;input image size;real-time fpga computation;real-time system;stereoscopic image;system architecture;disparity computation;fpga signal processing;stereo vision	Stereoscopic 3D reconstruction is an important algorithm in the field of Computer Vision, with a variety of applications in embedded and real-time systems. Existing software-based implementations cannot satisfy the performance requirements for such constrained systems; hence an embedded hardware mechanism might be more suitable. In this paper, we present an architecture of a 3D reconstruction system for stereoscopic images, which we implement on Virtex2 Pro FPGA. The architecture uses a Sobel edge detector to achieve real-time (75 fps) performance, and is configurable in terms of various application parameters, making it suitable for a number of application environments. The paper also presents a design exploration on algorithmic parameters such as disparity range, correlation window size, and input image size, illustrating the impact on the performance for each parameter.	3d reconstruction;algorithm;binocular disparity;computation;computer vision;edge detection;embedded system;field-programmable gate array;image resolution;real-time clock;real-time computing;requirement;sobel operator;stereoscopy	Stavros Hadjitheophanous;Christos Ttofis;Athinodoros S. Georghiades;Theocharis Theocharides	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)			EDA	43.83464586889768	-34.913176545268804	69463
49acb0d13671ae10cf5d0253411bd090bdc590dd	image feature extraction for mobile processors	image feature extraction;image features;histograms;kernel;mobile device;low power cpu;interest points;gma x3100 image feature extraction mobile processors high quality cameras sparse localized interest point detection scale invariant feature transform code generation framework laptop cpu core 2 duo low power cpu intel atom low power gpu;intel atom;high quality cameras;code generation;code generation framework;computer vision;transforms feature extraction microprocessor chips;laptop cpu;low power gpu;low power;sparse localized interest point detection;mobile processors;scale invariant feature transform;feature extraction;mobile communication;transforms;feature extraction mobile computing application software image analysis portable computers cameras augmented reality computer vision computer architecture runtime;optimization;gma x3100;core 2 duo;high performance;program processors;mobile application;parallel processing;mobile augmented reality;microprocessor chips	High-quality cameras are a standard feature of mobile platforms, but the computational capabilities of mobile processors limit the applications capable of exploiting them. Emerging mobile application domains, for example Mobile Augmented Reality (MAR), rely heavily on techniques from computer vision, requiring sophisticated analyses of images followed by higher-level processing. An important class of image analyses is the detection of sparse localized interest points. The Scale Invariant Feature Transform (SIFT), the most popular such analysis, is computationally representative of many other feature extractors. Using a novel code-generation framework, we demonstrate that a small set of optimizations produce high-performance SIFT implementations for three very different architectures: a laptop CPU (Core 2 Duo), a low-power CPU (Intel Atom), and a low-power GPU (GMA X3100). We improve the runtime of SIFT by more than 5X on our low-power architectures, enabling a low-power mobile device to extract SIFT features up to 63% as fast as the laptop CPU.	algorithm;atom;augmented reality;central processing unit;code;computation;computer vision;feature extraction;gradient;graphics processing unit;high- and low-level;horst rittel;image analysis;intel gma;interest point detection;internationalization and localization;kilobyte;laptop;low-power broadcasting;mathematical optimization;maxima and minima;megabyte;mobile app;mobile device;multi-core processor;simd;scale space;scale-invariant feature transform;server (computing);software deployment;sparse matrix;tyler oakley;visual odometry	Mark Murphy;Kurt Keutzer;Hong Wang	2009	2009 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2009.5306789	embedded system;parallel processing;computer architecture;parallel computing;kernel;mobile telephony;computer hardware;feature extraction;mobile processor;computer science;operating system;scale-invariant feature transform;mobile device;histogram;feature;code generation	Arch	42.68368586383849	-36.21757894905822	69528
f0dbc12948fca0ebbaf3a2d3851531dfd2af3cac	registration of range data from unmanned aerial and ground vehicles	range data;unmanned aerial vehicle;land vehicles unmanned aerial vehicles global positioning system laser radar nist iterative closest point algorithm iterative algorithms iterative methods vehicle detection navigation;remotely operated vehicles;laser ranging;iterative methods;optical radar;global positioning system;feature extraction;image registration;position measurement;position information range data registration unmanned ground vehicle unmanned aerial vehicle global positioning system scanning ladar rangefinder feature based registration method iterative closest point algorithm vehicle position estimation gps information;feature extraction image registration global positioning system laser ranging optical radar remotely operated vehicles position measurement iterative methods	In the research reported in this paper, we propose to overcome the unavailability of Global Positioning System (GPS) using combined information obtained froma scanning LADAR rangefinder on an Unmanned Ground Vehicle (UGV) and a LADAR mounted on an Unmanned Aerial Vehicle (UAV) that flies over the terrain being traversed. The approach to estimate and update the position of the UGV involves registering range data from the two LADARs using a combination of a feature-based registration method and a modified version of the wellknown IterativeClosest Point (ICP) algorithm.Registration of range data thus guarantees an estimate of the vehicle’s position even when only one of the vehicles has GPS information. Additionally, such registration over time (i.e., from sample to sample), enables position information to be maintained even when both vehicles can no longer maintain GPS contact. The approach has been validated by conducting systematic experiments on complex real-world data.	aerial photography;aerial survey;algorithm;corner detection;experiment;global positioning system;harris affine region detector;iterative method;piaget's theory of cognitive development;real-time computing;real-time transcription;sensor;unavailability;unmanned aerial vehicle	Anthony Downs;Raj Madhavan;Tsai Hong	2003		10.1109/AIPR.2003.1284247	computer vision;simulation;geography;remote sensing	Robotics	53.74540823243376	-36.92398190891666	69634
817c2a8ca118ee26580df14ff12ccda18b2f10f3	anticipatory robot navigation by simultaneously localizing and building a cognitive map	cognitive map;behavior based robotics;cognitive systems;mobile robot;bayes methods;behavior based robotic system robot navigation cognitive map mobile robot bayes filter;navigation mobile robots bayes methods cognitive systems adaptive filters;robot navigation;mobile robots;navigation;adaptive filters;cognitive robotics navigation robot kinematics mobile robots robot sensing systems elevators buildings laboratories educational institutions filters	This paper presents a method for a mobile robot to construct and localize relative to a “cognitive map”, where the cognitive map is assumed to be a representational structure that encodes both spatial and behavioral information. The localization is performed by applying a generic Bayes filter. The cognitive map was implemented within a behavior-based robotic system, providing a new behavior that allows the robot to anticipate future events using the cognitive map. One of the prominent advantages of this approach is elimination of the pose sensor usage (e.g., shaft encoder, compass, GPS, etc.), which is known for its limitations and proneness to various errors. A preliminary experiment was conducted in simulation and its promising results are discussed.	cluster analysis;cognitive map;computational complexity theory;encoder;global positioning system;mobile robot;robotic mapping;simulation	Yoichiro Endo;Ronald C. Arkin	2003		10.1109/IROS.2003.1250672	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;social robot;robot control;mobile robot navigation	Robotics	51.245616879774744	-30.7358632504738	70387
5b742d5884e887a7b163a511c5885c8d4a0c8184	real-time simplified edge detector architecture for 3d-hevc depth maps coding	complexity theory;standards;image edge detection;registers;encoding;algorithm design and analysis;hardware	This paper introduces the Simplified Edge Detector (SED) architecture for 3D-HEVC depth maps coding. The SED algorithm classifies the encoding block as homogeneous or edge. When SED classifies the encoding block as homogeneous, the encoding is simplified skipping the bipartition modes evaluation. This approach is capable of reducing 96.7% the bipartition modes evaluation with a drawback of only 0.94% in BD-rate. The SED hardware is capable of providing the decision for all available blocks inside a 32×32 block in only 34 cycles, with a power dissipation of only 25 μW per frame, when synthesized for 65 nm ST standard cells technology.	algorithm;blu-ray;edge detection;html;high efficiency video coding;power architecture;real-time transcription	Gustavo Sanchez;Mário Saldanha;Marcelo Schiavon Porto;Bruno Zatt;Luciano Volcan Agostini;César A. M. Marcon	2016	2016 IEEE International Conference on Electronics, Circuits and Systems (ICECS)	10.1109/ICECS.2016.7841205	computer science;theoretical computer science;algorithm;computer graphics (images)	EDA	43.34787526390226	-34.365045227508986	70458
4e72392716a373401e58790a36e0f4c2f1b9396a	yolo-lite: a real-time object detection algorithm optimized for non-gpu computers		This paper focuses on YOLO-LITE, a real-time object detection model developed to run on portable devices such as a laptop or cellphone lacking a Graphics Processing Unit (GPU). The model was first trained on the PASCAL VOC dataset then on the COCO dataset, achieving a mAP of 33.81% and 12.26% respectively. YOLO-LITE runs at about 21 FPS on a non-GPU computer and 10 FPS after implemented onto a website with only 7 layers and 482 million FLOPS. This speed is 3.8× faster than the fastest state of art model, SSD MobilenetvI. Based on the original object detection algorithm YOLOV2, YOLOLITE was designed to create a smaller, faster, and more efficient model increasing the accessibility of real-time object detection to a variety of devices.	accessibility;algorithm;flops;fastest;floating point systems;graphics processing unit;laptop;mobile phone;object detection;personal digital assistant;real-time clock;real-time transcription;solid-state drive	Jonathan Pedoeem;Rachel Huang	2018	CoRR		laptop;graphics processing unit;deep learning;flops;object detection;artificial neural network;computer science;algorithm;artificial intelligence	Vision	42.626660612170696	-35.98249160961551	70974
c686c1d2828dea0172cf00285d56b81a52d7a8a4	natural landmark-based autonomous navigation using curvature scale space	nonlinear filters;filtering;laser scan;unmodified outdoor environments;curvature scale space;working environment noise;cascading style sheets;kalman filters;mobile robots;remotely operated vehicles;laser ranging;data mining;noise robustness;laser range scanning;laser ranging kalman filters nonlinear filters filtering theory sensor fusion mobile robots robot kinematics;navigation;navigation data mining noise robustness cascading style sheets australia laser theory working environment noise remotely operated vehicles mobile robots filtering;laser theory;scale space;extended kalman filter natural landmark based autonomous navigation curvature scale space terrain aided navigation system laser scan scale space method points of maximum curvature unmodified outdoor environments odometric information;points of maximum curvature;terrain aided navigation system;autonomous navigation;laser scanning;navigation system;odometric information;sensor fusion;scale space method;extended kalman filter;natural landmark based autonomous navigation;filtering theory;conference proceeding;australia;robot kinematics	This paper describes a terrain-aided navigation system that employs points of maximum curvature extracted from laser scan data as primary landmarks. A scale space method is used to extract points of maximum curvature from laser range scans of unmodified outdoor environments. This information is then fused with odometric information to provide localization information for an outdoor vehicle. The method described is invariant to the size and orientation of the range images under consideration (with respect to rotation and translation), is robust to noise, and can reliably detect and localize naturally occurring landmarks in the operating environment. The algorithm is demonstrated in the application of a road vehicle in an unmodified operating domain. Keywords— landmark extraction, extended Kalman filter, scale space, outdoor localization.	algorithm;extended kalman filter;operating environment;scale space;simultaneous localization and mapping	Raj Madhavan;Hugh F. Durrant-Whyte;Gamini Dissanayake	2002		10.1109/ROBOT.2002.1014342	laser scanning;filter;remotely operated underwater vehicle;kalman filter;mobile robot;computer vision;navigation;scale space;simulation;computer science;artificial intelligence;sensor fusion;extended kalman filter;cascading style sheets;robot kinematics;remote sensing	Robotics	52.541209005010415	-36.457407485418145	70978
07f68078718c018abcf7fb56083ff7383cc10e34	indoor positioning data obstacle avoidance and visualization framework based on a* algorithm	indoor positioning data;vector map;a algorithm;visualization;obstacle	Currently the research on the map visualization and GIS visualization is relatively mature. But in the indoor environment, Indoor positioning system requires monitoring the location which is on vector map of positioning data and display positioning data trajectory. At the same time, Indoor positioning environment is complex, and has many obstacles such as walls. The connection between two positioning data point which are in the trajectory of positioning data always pass through the obstacles. To solve this problem, in this paper, we combine A* map routing algorithm and vector map visualization technology to propose a Obstacle Avoidance And Visualization Method of indoor positioning data trajectory which is on the vector map. And we introduce an indoor positioning data Obstacle Avoidance And Visualization Framework based on the method. The implementation of the framework is also discussed.	a* search algorithm;data point;geographic information system;indoor positioning system;obstacle avoidance;real-time transcription;routing;vector map	Ming Yang;Ruonan Rao	2011		10.1007/978-3-642-23887-1_4	computer vision;vector map;simulation;visualization;a* search algorithm;computer science;hybrid positioning system	Visualization	52.99658892770058	-36.26548390768009	71068
b2fdbad1b6d69e2aaf835e87ea31083064376b76	the maverick planner: an efficient hierarchical planner for autonomous vehicles in unstructured environments		Planning kinodynamically feasible trajectories for autonomous vehicles is computationally expensive, especially when planning over long distances in unstructured environments. This paper presents a hierarchical planner, called the Maverick planner, which can find such trajectories efficiently. It comprises two parts: a waypoint planner that uses a simplified vehicle model and an RRT∗ planner that respects full kinodynamic constraints. The waypoint planner quickly finds a directed graph of waypoints from start to goal, which is then used to bias sampling and speed up computation in RRT∗. The Maverick planner is capable of anytime planning and continuous replanning. It has been tested extensively in simulation and on real vehicles. When planning on a sensor-generated map of the SwRI test track it can find a feasible path over 0.5 km in under 16 ms, and refine that path to within 1% of the local optimum in 0.5 seconds.	analysis of algorithms;anytime algorithm;autonomous robot;computation;confidentiality;directed graph;experiment;local optimum;map;maverick.net;motion planning;open-source software;rapidly-exploring random tree;sampling (signal processing);simulation;voronoi diagram;waypoint	Neal Seegmiller;Jason Gassaway;Elliot Johnson;Jerry Towler	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206021	local optimum;simulation;computer science;control engineering;planner;directed graph;computation;speedup;waypoint	Robotics	53.359167224815	-24.097286873479305	71096
38aeb9e48692c066e1aacaba7f619ed2fbde55fa	robust map optimization using dynamic covariance scaling	graph theory;optimisation;slam robots graph theory mobile robots optimisation;mobile robots;optimization switches robustness simultaneous localization and mapping convergence standards;mobile robots robust map optimization dynamic covariance scaling slam front end graph perceptual aliasing optimization back ends constraint networks classical gating slam back end;slam robots	Developing the perfect SLAM front-end that produces graphs which are free of outliers is generally impossible due to perceptual aliasing. Therefore, optimization back-ends need to be able to deal with outliers resulting from an imperfect front-end. In this paper, we introduce dynamic covariance scaling, a novel approach for effective optimization of constraint networks under the presence of outliers. The key idea is to use a robust function that generalizes classical gating and dynamically rejects outliers without compromising convergence speed. We implemented and thoroughly evaluated our method on publicly available datasets. Compared to recently published state-of-the-art methods, we obtain a substantial speed up without increasing the number of variables in the optimization process. Our method can be easily integrated in almost any SLAM back-end.	aliasing;computation;experiment;image scaling;mathematical optimization;open-source software;overhead (computing);rate of convergence;simultaneous localization and mapping;victoria (3d figure);vii	Pratik Agarwal;Gian Diego Tipaldi;Luciano Spinello;Cyrill Stachniss;Wolfram Burgard	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630557	mobile robot;computer vision;mathematical optimization;computer science;artificial intelligence;graph theory;machine learning;control theory;mathematics	Robotics	50.96824451629021	-24.185160022216678	71187
33f83d0081b49ee365fe945e601f162260d7f350	sampling-based coverage path planning for inspection of complex structures	marine robotics;path planning;sensor based planning	We present several new contributions in sampling-based coverage path planning, the task of finding feasible paths that give 100% sensor coverage of complex structures in obstaclefilled and visually occluded environments. First, we establish a framework for analyzing the probabilistic completeness of a sampling-based coverage algorithm, and derive results on the completeness and convergence of existing algorithms. Second, we introduce a new algorithm for the iterative improvement of a feasible coverage path; this relies on a samplingbased subroutine that makes asymptotically optimal local improvements to a feasible coverage path based on a strong generalization of the RRT∗ algorithm. We then apply the algorithm to the real-world task of autonomous in-water ship hull inspection. We use our improvement algorithm in conjunction with redundant roadmap coverage planning algorithm to produce paths that cover complex 3D environments with unprecedented efficiency.	asymptotically optimal algorithm;automated planning and scheduling;autonomous robot;combinatorial optimization;iterative method;mathematical optimization;motion planning;np-completeness;np-hardness;obstacle avoidance;optimization problem;sampling (signal processing);subroutine	Brendan Englot;Franz S. Hover	2012			mathematical optimization;simulation;any-angle path planning;computer science;artificial intelligence;machine learning;motion planning	Robotics	53.29089676199566	-24.7811074101157	71358
2784e209718f21dac6971c8bcb85d2231bdec657	classifying and passing 3d obstacles for autonomous driving	dynamic obstacles 3d obstacles autonomous driving automated driving human locomotion bicycles pedestrians vehicles perception planning control system high quality perception object classification object tracking path planning algorithm probabilistic classification classification algorithm real time decision making anytime algorithm lidar data safe driving behavior;laser radar;classification;road vehicles bicycles image classification intelligent transportation systems object tracking path planning pedestrians road safety;bicycles;algorithms;perception;detection and identification systems;trajectory control;radar tracking bicycles laser radar robot sensing systems three dimensional displays	Highly automated driving in real world situations requires perception and planning to account for other forms of human locomotion, principally bicycles, and pedestrians, in addition to vehicles. Here we present a complete perception-planning-control system with high quality perception including object classification and tracking, and a path planning algorithm that makes use of probabilistic classification results. Our classification algorithm is fast enough to provide results for the vast majority of objects in on-line operation, but to ensure output for real-time decision-making, the computation and integration of classification results is done using an anytime algorithm. This work builds on recent research in object classification using LIDAR data (Teichman et. al., 2011). Our focus is on safe driving behavior in the presence of dynamic obstacles on the side of the road, which are typically bicycles, using LIDAR data. Our resulting system is extensively tested in real-world situations, and shows good performance on a large test set of interactions with bicycles.	anytime algorithm;automated planning and scheduling;autonomous car;computation;control system;display resolution;graphics processing unit;interaction;motion planning;online and offline;parallel computing;point cloud;priority queue;real-time clock;real-time computing;refinement (computing);statistical classification;test set;tracking system	Michael Delp;Naoki Nagasaka;Nobuhide Kamata;Michael R. James	2015	2015 IEEE 18th International Conference on Intelligent Transportation Systems	10.1109/ITSC.2015.204	computer vision;simulation;engineering;transport engineering	Robotics	50.39377002829379	-36.46441789629377	71448
feec5e111406744fbcb8bde713b3c39e48bea4f4	real-time parallel implementation of ssd stereo vision algorithm on csx simd architecture	processing element;real time;mobile computer;computer vision;machine vision;stereo vision;parallel implementation;sum of squared difference;low power consumption	We present a faster than real-time parallel implementation of standard sum of squared differences (SSD) stereo vision algorithm, on an SIMD architecture, the CSX700. To our knowledge, this is the first highly parallel implementation of this algorithm using 192 processing elements. For disparity range of 16 pixels, we have achieved the rate of 160 and 59 stereo pairs per second on 640x480 and 1280x720 images, respectively. Since this implementation is much faster than real time, it leaves enough time for performing other machine vision applications in real time. Our results demonstrate that CSX architecture is a powerful processor for (low level) computer vision applications. Due to the low-power consumption of CSX architecture, it can be a good candidate for mobile computer vision applications.	algorithm;real-time transcription;simd;solid-state drive;stereopsis	Fouzhan Hosseini;Amir Fijany;Saeed Safari;Ryad Chellali;Jean-Guy Fontaine	2009		10.1007/978-3-642-10331-5_75	embedded system;computer vision;machine vision;computer hardware;computer science;stereopsis;computer graphics (images)	Vision	43.99031569178345	-35.74949671574002	71656
8eb2aaec2d24cca146e286266be1b5d1295eda1f	visual measurement in simulation environment for vision-based uav autonomous aerial refueling	simulation environment hardware in loop simulation platform docking maneuver wind turbulence boom controller stationkeeping controller virtual reality pose estimation marker detection tanker uav receiver aircraft pose estimation visual measurement strategy unmanned aerial vehicle aar vision based uav autonomous aerial refueling;visualization atmospheric modeling receivers estimation aircraft feature extraction cameras;virtual reality aircraft autonomous aerial vehicles computerised instrumentation fuel measurement systems pose estimation stability;receivers;visualization;estimation;feature extraction;期刊论文;unmanned aerial vehicles uavs autonomous aerial refueling aar computer vision pose estimation;atmospheric modeling;cameras;aircraft	Autonomous aerial refueling (AAR) is an important capability of unmanned aerial vehicles (UAVs) for further applications in both military and civilian domains. This paper focuses on the description and comparison of visual measurement strategies in the simulation environment of AAR of UAVs. The visual measurement methods are employed for pose estimation between tanker and UAV receiver aircrafts. Within this effort, methods and techniques for marker detection, pose estimation, and visualization of aerial refueling in virtual reality have been designed and implemented. Series of comparative experiments indicate the preference of various visual measurement approaches in our work. Boom and stationkeeping controllers are designed to ensure system stability when wind turbulence is induced. Furthermore, the hardware-in-loop simulation platform is built to demonstrate the feasibility and effectiveness of the proposed scenarios in actual conditions. The visual attention mechanism is adopted in the preprocessing for the images of virtual scene to remove potential noises. The achieved results make excellent performances of our proposed scheme during the docking maneuver in AAR.	3d pose estimation;aerial photography;algorithm;association for automated reasoning;docking (molecular);experiment;image quality;motion estimation;performance;preprocessor;simulation;television antenna;turbulence;unmanned aerial vehicle;virtual reality	Haibin Duan;Qifu Zhang	2015	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2014.2343392	computer vision;atmospheric model;estimation;simulation;visualization;feature extraction;computer science;engineering;statistics;remote sensing	Robotics	53.322895601570465	-37.80871951934507	71913
a6adaa13d3ace533b3029be7e9db04faadc8162b	in situ measurement of cadaveric soft tissue mechanical properties and fulcrum forces for use in physics-based surgical simulation	biological tissues;surgical simulation;biomechanical properties;surgical simulation soft tissue biomechanical properties human cadaver indentation experiment;deformable models;in situ measurement;cadaver;indentation experiment;mechanical factors;deformable objects;surgery;present day;force measurement;haptic feedback;abdomen;mechanical variables measurement force measurement biological tissues mechanical factors surgery haptic interfaces cadaver humans abdomen deformable models;surgery simulation;humans;soft tissue;soft tissue mechanics;haptic interfaces;mechanical variables measurement;human cadaver;dynamic properties;haptic interface	Development of a realistic surgery simulator that delivers high fidelity visual and haptic feedback, based on the physics of deformable objects, requires the use of empirical data on the behavior of soft tissues when subjected to external loads. Measurements on live human patients present significant risks, thus making the use of cadavers a logical alternative. Cadavers are widely used in present day surgical training, are relatively easy to procure through excellent donor programs and have the right anatomy, which makes them better candidates for training than the porcine model. To investigate the static and dynamic properties of soft tissue, we have developed a high precision tactile stimulator by modifying an exisitng haptic interface device, the Phantom, and used it to record the force-displacement behavior of intra-abdominal organs of fresh human cadavers at the US Surgical in Connecticut and Albany Medical College. Another paraemeter of great practical significace, but mostly overlooked in the literature, is the fulcrum force at the point of entry of the trocar into the abdomen. The length of the surgical tools, coupled with a thick abdominal wall of overweight or obese patients, tend to produce torques at the wrist of the surgeon which are often large enough to mask any haptic sensation from the organs being operated on. To ascertain the true role of haptics in endoscopic surgery one needs to measure these fulcrum forces. In this paper we discuss techniques and some preliminary results in this direction.	displacement mapping;haptic technology;imaging phantom;procurement;simulation;surgery simulator	Yi-Je Lim;Suvranu De;Daniel O B Jones;Tejinder Paul Singh	2006	2006 14th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems	10.1109/VR.2006.85	simulation;computer science;artificial intelligence;haptic technology	Robotics	40.048035341169786	-37.67202722020927	71973
86a6e23e9514433ca538c1c0efaff706a172bdfa	surface classification for sensor deployment from uav landings	robot sensing systems;support vector machines;support vector machines robot sensing systems accelerometers accuracy acceleration decision trees cameras;acceleration;accuracy;wireless sensor networks accelerometers autonomous aerial vehicles decision trees moisture pattern classification soil support vector machines;binary decision trees surface classification sensor deployment unmanned aerial vehicles sensor networks soil moisture sensors soil softness onboard accelerometer uav landings accelerometer classification algorithms lda qda svm;decision trees;accelerometers;cameras	Using Unmanned Aerial Vehicles (UAVs) to deploy sensor networks promises an autonomous and useful method of installation in remote or hard to access locations. Some sensors, such as soil moisture sensors, must be physically installed in soft soil, yet UAVs cannot easily determine soil softness with remote sensors. In this paper, we use data from an onboard accelerometer measured during UAV landings to determine the softness of the ground. We collect and analyze over 200 data sets gathered from 8 different materials: foam, carpet, wood, tile, grass, dirt, concrete, and woodchips. Based on this analysis, we examine a number of features from the accelerometer and four classification algorithms: LDA, QDA, SVM, and binary decision trees. The decision tree performs well and is simple to implement onboard the UAV. We implement this in our UAV control system and perform experiments to verify that the UAV can accurately classify the softness of the surface with 90% accuracy. This lays the groundwork for our future work on developing a UAV capable of installing sensors in soft soil.	algorithm;autonomous robot;bandwidth (signal processing);control system;decision tree;experiment;frequency response;gyroscope;sensor;sierpinski carpet;software deployment;statistical classification;support vector machine;unmanned aerial vehicle	David J. Anthony;Elizabeth Basha;Jared Ostdiek;John-Paul Ore;Carrick Detweiler	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139678	acceleration;support vector machine;computer vision;simulation;computer science;engineering;decision tree;accuracy and precision;accelerometer;remote sensing	Robotics	50.91887169842616	-32.17723695831941	72173
40ecaccc55a5f11898717a9f910200e40b186c1c	extended object tracking using mixture kalman filtering	system modeling;qa75 electronic computers computer science;object tracking;mixture kalman filter;computer simulation	This paper addresses the problem of tracking extended objects. Examples of extended objects are ships and a convoy of vehicles. Such kind of objects have particularities which pose challenges in front of methods considering the extended object as a single point. Measurements of the object extent can be used for estimating size parameters of the object, whose shape is modeled by an ellipse. This paper proposes a solution to the extended object tracking problem by mixture Kalman filtering. The system model is formulated in a conditional dynamic linear (CDL) form. Based on the specifics of the task, two latent indicator variables are proposed, characterising the mode of maneuvering and size type, respectively. The developed Mixture Kalman filter is validated and evaluated by computer simulation.	compiler description language;computer simulation;kalman filter	Donka S. Angelova;Lyudmila Mihaylova	2006		10.1007/978-3-540-70942-8_14	computer vision;invariant extended kalman filter;simulation;computer science;machine learning;extended kalman filter	Robotics	49.74397393581272	-34.38512671603561	72470
fa5404713a72910d80a8f8bfbdc70ffebf08797e	decision making for obstacle avoidance in autonomous mobile robots by time to contact and optical flow	mobile robots;obstacle avoidance;time to contact;optical flow	Vision is a vital cue for human navigation, and it has become in a powerful tool for robot navigation. Computer vision is useful for recognition of positional relationship and relative motion between themselves and objects in the environment. So, we address the problem of navigation for mobile robot in indoor environment. To this task, we tackle the problem using monocular vision, i.e., without other kind of sensors. Insects and some animals use tools as optical flow estimations and time to contact, to their navigation [1]. So, in this paper, we propose a method for obstacle avoidance, without constantly calculating the optical flow field, only it is calculated when the robot is close to colliding with an obstacle, and so, it uses the flow field divergence to decide which direction will should be taken. Physical experiments using a real robot have been conducted in unknown environments.	autonomous robot;british informatics olympiad;computer vision;experiment;mobile robot;obstacle avoidance;optical flow;passive optical network;robotic mapping;sensor;texture mapping	Ángel Juan Sánchez-García;Homero V. Ríos-Figueroa;Antonio Marín-Hernández;Gerardo Contreras Vega	2015	2015 International Conference on Electronics, Communications and Computers (CONIELECOMP)	10.1109/CONIELECOMP.2015.7086939	mobile robot;computer vision;simulation;computer science;artificial intelligence;optical flow;robot control;obstacle avoidance;mobile robot navigation	Robotics	53.21065050720252	-32.933028097886215	73150
2118213f82811bee2c2f500fb959cd729fee12a9	sensor-based exploration for general robotic systems	configuration space data structure;robot sensing systems;sensor based exploration;sensor based exploration tree;sensors;general robotic systems;mobile robots;tree data structures;configuration space;distance measurement;computational modeling;sampling technique;data structures;tree data structures robots sensors;robots;robot sensing systems robots sensors computational modeling distance measurement mobile robots data structures;sampling techniques sensor based exploration general robotic systems rangefinders configuration space data structure sensor based exploration tree;sampling techniques;data structure;rangefinders	We present a method for sensor-based exploration of unknown environments by a robotic system equipped with rangefinders. The method is based on the incremental generation of a configuration-space data structure called sensor-based exploration tree (SET). The expansion of the SET is driven by information at the world level, where the perception process takes place. In particular, the frontiers of the explored region are used to guide the search for informative view configurations. Various exploration strategies may be obtained by instantiating the general SET method with different sampling techniques. Two of these are compared by simulations in 2D and 3D worlds.	algorithm;data structure;gadu-gadu;information;robot;sampling (signal processing);sensor;simulation	Luigi Freda;Giuseppe Oriolo;Francesco Vecchioli	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4651143	control engineering;sampling;computer vision;simulation;data structure;computer science	Robotics	52.01529772321162	-24.91477378004408	73230
08868d09f530fe5f5b0954aca92c6ca8bd50748d	wind-energy based path planning for unmanned aerial vehicles using markov decision processes	path planning;wind power;mobile robots;energy consumption;wind power autonomous aerial vehicles energy consumption gaussian distribution markov processes mobile robots path planning telerobotics;telerobotics;travel time wind energy path planning unmanned aerial vehicles markov decision processes flight duration extension uncertain wind fields gaussian distribution time varying wind fields flight direct line energy consumption;wind speed gaussian distribution planning wind energy path planning vectors uncertainty;markov processes;autonomous aerial vehicles;gaussian distribution	Exploiting wind-energy is one possible way to extend the flight duration of an Unmanned Aerial Vehicle. Wind-energy can also be used to minimise energy consumption for a planned path. In this paper, we consider uncertain, time-varying wind fields and plan a path through them that exploits the energy the field provides. A Gaussian distribution is used to determine uncertainty in the time-varying wind fields. We use a Markov Decision Process to plan a path based upon the uncertainty of the Gaussian distribution. Simulation results are presented to compare the direct line of flight between a start and target point with our planned path for energy consumption and time of travel. The result of our method is a robust path using the most visited cell while sampling the Gaussian distribution of the wind field in each cell.	aerial photography;algorithm;course (navigation);markov chain;markov decision process;motion planning;sampling (signal processing);simulation;usb attached scsi;unmanned aerial vehicle;velocity (software development);vii	Wesam H. Al-Sabban;Luis Felipe Gonzalez;Ryan N. Smith	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630662	normal distribution;wind power;telerobotics;control engineering;mobile robot;simulation;computer science;engineering;artificial intelligence;motion planning;markov process	Robotics	53.106085869072345	-25.856404883683517	73511
54fdbb7490874741ef75a9fcfd7e332bb70eba87	substructural analysis techniques for empirical structure-property correlation. application to stereochemically related molecular properties	structural properties			George W. Adamson;David Bawden	1980	Journal of Chemical Information and Computer Sciences	10.1021/ci60022a009	chemistry;computer science	Metrics	42.201323461683685	-25.620891277603437	73684
7646ebebb3d669f63c70a8464ce22469956520a3	a feht algorithm for detecting and tracking vlo targets	time measurement;radar tracking;time measurement target tracking logic gates trajectory radar tracking computational complexity transforms;trajectory;logic gates;computational complexity;transforms;random sampling target detection very low observable targets tracking vlo targets fast ergodic hough transform algorithm feht algorithm failure detection;target tracking hough transforms object detection signal sampling;target tracking	In order to address the problem of detecting and tracking very low observable (VLO) targets in heavy clutter, a fast ergodic Hough transform (FEHT) algorithm is proposed. The contributions consist of three aspects. First, the FEHT does not invoke a random combination but rather an ergodic combination of all possible measurement pairs to generate the line parameters, which avoids the failure detection due to random sampling. Second, a joint gate is proposed to reject the measurement pairs with negligible probability, and hence reduce the computational and storage burden. Third, the computation complexity of the proposed method is analyzed. Simulation results shows the effectiveness of the proposed method.	algorithm;clutter;computation;computational complexity theory;dataspaces;enumerated type;ergodicity;hough transform;observable;php development tools;sampling (signal processing);sensor;simulation	Yunfei Guo;Guoquan Chen;Xiaofeng Zheng	2016	2016 19th International Conference on Information Fusion (FUSION)		computer vision;mathematical optimization;machine learning;mathematics	Robotics	49.05716635503057	-32.9387439406732	73723
3b73d50c0745cf513776281805dd2185dad940cf	characterization of sky-region morphological-temporal airborne collision detection	unmanned aerial vehicle;system performance;collision detection;system design;safety critical system;temporal processing;experimental evaluation	Automated airborne collision-detection systems are a key enabling technology for facilitating the integration of unmanned aerial vehicles (UAVs) into the national airspace. These safety-critical systems must be sensitive enough to provide timely warnings of genuine airborne collision threats, but not so sensitive as to cause excessive false alarms. Hence, an accurate characterization of detection and false-alarm sensitivity is essential for understanding performance tradeoffs, and system designers can exploit this characterization to help achieve a desired balance in system performance. In this paper, we experimentally evaluate a sky-region, image-based, aircraft collision-detection system that is based on morphological and temporal processing techniques. (Note that the examined detection approaches are not suitable for the detection of potential collision threats against a ground clutter background.) A novel collection methodology for collecting realistic airborne collision-course target footage in both head-on and tail-chase engagement geometries is described. Under (hazy) blue sky conditions, our proposed system achieved detection ranges greater than 1540 m in three flight test cases with no false-alarm events in 14.14 h of nontarget data (under cloudy conditions, the system achieved detection ranges greater than 1170 m in four flight test cases with no false-alarm events in 6.63 h of nontarget data). Importantly, this paper is the first documented presentation of detection range versus false-alarm curves generated from airborne target and nontarget image data. C © 2012 Wiley Periodicals, Inc.		John Lai;Jason J. Ford;Luis Mejías Alvarez;Peter O'Shea	2013	J. Field Robotics	10.1002/rob.21443	simulation;computer science;engineering;computer performance;computer security;collision detection;remote sensing;systems design	Mobile	48.0804828964276	-35.609021340814	73904
aa549976c8dfe4667bee8c8e3d3bed2ef57a5c80	appearance-based mapping and localization for mobile robots using a feature stability histogram	human memory;image features;omnidirectional vision;mobile robot;omnidirectional image;localization and mapping;time of day;dynamic environment;local features;seasonality;topological maps;bag of words;appearance based	The strength of appearance-based mapping models for mobile robots lies in their ability to represent the environment through high-level image features and to provide human-readable information. However, developing a mapping and a localization method using these kinds of models is very challenging, especially if robots must deal with long-term mapping, localization, navigation, occlusions, and dynamic environments. In otherwords, themobile robot has to dealwith environmental appearance change,which modifies its representation of the environment. This paper proposes an indoor appearance-basedmapping and a localizationmethod formobile robots based on the humanmemorymodel, whichwas used to build a Feature Stability Histogram (FSH) at each node in the robot topological map. This FSH registers local feature stability over time through a voting scheme, and the most stable features were considered for mapping, for Bayesian localization and for incrementally updating the current appearance reference view in the topological map. The experimental results are presented using an omnidirectional images dataset acquired over the long-term and considering: illumination changes (time of day, different seasons), occlusions, random removal of features, and perceptual aliasing. The results include a comparison with the approach proposed by Dayoub and Duckett (2008) [19] and the popular Bag-of-Words (Bazeille and Filliat, 2010) [35] approach. The obtained results confirm the viability of our method and indicate that it can adapt the internal map representation over time to localize the robot both globally and locally. © 2011 Elsevier B.V. All rights reserved.	aliasing;bag-of-words model;bayesian network;experiment;fasttrack scripting host;finite-state machine;high- and low-level;human-readable medium;internationalization and localization;map;mobile robot;sensor web;similarity measure;software transactional memory;speeded up robust features;statistical classification;time complexity	Bladimir Bacca;Joaquim Salvi;Xavier Cufí	2011	Robotics and Autonomous Systems	10.1016/j.robot.2011.06.008	mobile robot;computer vision;simulation;computer science;artificial intelligence;bag-of-words model;memory;feature;seasonality	Robotics	49.39166547419877	-37.90883879115528	74130
759683e2ee7b311c2739cf9e5fa75eee71338470	efficient multi-sensor data fusion for space surveillance	single sensor tracking method multisensor data fusion network space surveillance single probabilistic state description logarithmic opinion pool weight selection scheme quadrature integration method multiple ground based optical sensor position tracking;cost function observers data integration probabilistic logic space vehicles extraterrestrial measurements bayes methods;cost function;bepress selected works;bayes methods;data fusion dynamical systems sensor networks sensor nodes space surveillance tracking position integration method multi sensor fusion multi sensor networks multiple grounds multisensor data fusion orders of magnitude selection scheme tracking method sensor data fusion;observers;probabilistic logic;extraterrestrial measurements;weighing integration object tracking optical sensors probability sensor fusion space debris surveillance;data integration;space vehicles	Multi-sensor networks can extend the sensing region of a single sensor in order to provide a more geometrically diverse and comprehensive view of the state of a dynamical system. The use of a multi-sensor network gives rise to the need for a fusion step that combines the outputs of all sensor nodes into a single probabilistic state description. This paper examines a fusion method based on logarithmic opinion pools and develops algorithms for multi-sensor data fusion as well as investigates weight selection schemes for the opinion pool using efficient quadrature integration methods. The proposed fusion rules are applied to the tracking of a space object using multiple ground-based optical sensors. It is shown that the multi-sensor fusion rule leads to an increase of nearly two orders of magnitude in the position tracking accuracy as compared to the traditional single-sensor tracking method.	algorithm;dynamical system;sensor;velocity (software development)	Kyle J. DeMars;James S. McCabe;Jacob E. Darling	2015	2015 American Control Conference (ACC)	10.1109/ACC.2015.7172153	computer vision;soft sensor;computer science;engineering;data integration;data mining;control theory;sensor fusion;probabilistic logic	Robotics	52.42050567819064	-34.92463590578719	74189
ec0775a1569fed4ad8626c8a0c019a6aa5684d6a	a contour preserving fpga architecture for pal to xga format conversion	video streaming field programmable gate arrays image sequences;interpolation image edge detection field programmable gate arrays complexity theory computer architecture streaming media power demand;fpga implementation contour preserving fpga architecture xga format conversion pal frame format conversion video display systems video stream resolution transmission screen resolution digital display contour preserving approach video up scaling format conversions contour direction scaling method video sequences	Frame format conversion represents a key part of video display systems, as usually the transmission video stream resolution is different from the screen resolution of the digital display. In this paper a contour-preserving approach for video up-scaling is proposed, along with a low-cost, efficient FPGA architecture for one of the most used format conversions, namely PAL to XGA. The algorithm first checks for edges existence within the region around the pixel to be interpolated and adapts the scaling method following the contour direction. This adaptive system keeps the edges sharp and provides good quality scaled video sequences when compared with other scaling methods. Moreover, we propose an FPGA implementation for handling the algorithm reload, very efficient in terms of resources and complexity.	adaptive system;algorithm;bilinear filtering;computation;display device;field-programmable gate array;graphics display resolution;image scaling;interpolation;pal;parallel computing;pixel;real-time transcription;streaming media;video coding format	Frédéric Amiel;Boubacar Barry;Maria Trocan	2014	2014 IEEE 12th International New Circuits and Systems Conference (NEWCAS)	10.1109/NEWCAS.2014.6934019	computer vision;computer hardware;computer science;video capture;video processing;smacker video;computer graphics (images)	Visualization	43.61284982682971	-34.24712943499105	75046
5d24d9c4d67c4db2cc5c46c6ae95db92ed59e76d	vision based lane change detection using true flow features		Detection of lane change maneuvers of other traffic participants is one key element looking at highly automated driving and future driver assistance systems. A wide number of publications have shown approaches exploiting solely high level features from processed sensor data. Looking at low level features, vision based systems offer high potential for the tasks of lane change detection in the image domain, like the positioning of objects relative to the lane markings or the evaluation of their movement towards adjacent lanes. Among conventional image processing algorithms deployed, optical flow is an appropriate approach towards the latter task, that is affected by translation and rotation of the ego vehicle. In this paper we introduce true flow as an addition to conventional optical flow. Translational and rotational influence of ego motion is eliminated, allowing to estimate solely the observed object behavior. Features for lane change detection are presented and their invariance is shown under variable conditions, using simulation data. Experimental evaluation shows feature performance and predictability based on a real world data test set.	algorithm;autonomous car;high-level programming language;image processing;maximum flow problem;microsoft outlook for mac;motion estimation;optical flow;simulation;test set	Joran Zeisler;Fabian Schonert;Marcel Johne;Vladimir Haltakov	2017	2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2017.8317954	computer vision;advanced driver assistance systems;artificial intelligence;digital image processing;flow (psychology);predictability;engineering;change detection;test set;optical flow	Robotics	49.087054172190136	-37.162816379864154	75077
49bd33c32db9e05d7f23b5d1f2aa074c12ffd019	real-time feature point tracking at 1000 fps	moving object;real time;high speed processing feature point tracking algorithm high speed vision platform;robot vision;robot vision pointing systems;feature point tracking;pointing systems;high speed;pixel hardware real time systems tracking field programmable gate arrays image processing karhunen loeve transforms feature extraction signal processing robot vision systems	Real-time feature point tracking at 1000 fps was performed by implementing a feature point tracking algorithm on a high-speed vision platform, which is improved for hardware integration and high-speed processing at real time. The high-speed vision platform on which the improved algorithm is hardware-implemented can be used to track feature points of 1024×1024 pixel images at 1000 fps. By considering fast-moving objects in the real world, we verified the performance of our developed real-time feature point tracking system.	algorithm;pixel;real-time clock;real-time computing;real-time transcription;tracking system	Idaku Ishii;Ryo Sukenobe;Yuta Moriue;Kenichi Yamamoto	2009	2009 IEEE International Symposium on Computational Intelligence in Robotics and Automation - (CIRA)	10.1109/CIRA.2009.5423150	computer vision;feature detection;simulation;tracking system;computer science;feature;computer graphics (images)	Robotics	44.83507645386691	-35.93289747830319	76225
5166f015fd4115b1a11bf9268effc99a1e3f7490	online navigation summaries	robot sensing systems;cluster algorithm;sensor phenomena and characterization;summary image;navigation robotics and automation intelligent robots mobile robots robot sensing systems sensor phenomena and characterization image sensors usa councils machine intelligence bayesian methods;video signal processing;intelligent robots;approximation algorithms;path planning;image fitness measurement;bayesian methods;mobile robots;usa councils;image sensors;noise measurement;video signal processing mobile robots robot vision;secretary problem;navigation noise trajectory robots clustering algorithms noise measurement image color analysis;navigation;visualization;max surprise score mobile robot navigation summary sampling problem;robot vision;trajectory;position control;streaming media;image color analysis;machine intelligence;robots;sampling methods mobile robots path planning position control;summary image online navigation summaries robot visual experience classical secretaries problem bayesian surprise image fitness measurement;clustering algorithms;bayesian surprise;classical secretaries problem;sampling methods;on line algorithm;robotics and automation;robot visual experience;online navigation summaries;noise	Our objective is to find a small set of images that summarize a robot's visual experience along a path. We present a novel on-line algorithm for this task. This algorithm is based on a new extension to the classical Secretaries Problem. We also present an extension to the idea of Bayesian Surprise, which we then use to measure the fitness of an image as a summary image.	bayesian network;face (geometry);google street view;luc steels;lucas sequence;maximally stable extremal regions;mobile robot;online algorithm;online and offline;scale-invariant feature transform;scoring functions for docking;speeded up robust features;vii	Yogesh A. Girdhar;Gregory Dudek	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509464	robot;mobile robot;sampling;computer vision;navigation;simulation;visualization;bayesian probability;computer science;noise measurement;noise;artificial intelligence;trajectory;machine learning;image sensor;motion planning;secretary problem;cluster analysis;approximation algorithm	Robotics	52.14473899239871	-37.46314977671774	76525
17f5712b77b25b3f35b88fb65101404ce6da6fae	predicting pedestrian trajectories using velocity-space reasoning		We introduce a novel method to predict pedestrian trajectories using agent-based velocity-space reasoning for improved human-robot interaction. This formulation models the trajectory of each moving pedestrian in a robot’s environment using velocity obstacles and learns the simulation parameters based on tracked data. The resulting motion model for each agent is computed using statistical inferencing techniques from noisy data. This includes the combination of Ensemble Kalman filters and maximum likelihood estimation algorithm to learn individual motion parameters at interactive rates. We highlight the performance of our motion model in real-world crowded scenarios. We also compare its performance with prior techniques and demonstrate improved accuracy in the predicted trajectories.	agent-based model;algorithm;human–robot interaction;ibm notes;image noise;kalman filter;mobile robot;online and offline;signal-to-noise ratio;simulation;statistical model;velocity (software development);velocity obstacle;wilkie investment model	Sunyoung Kim;Stephen J. Guy;Wenxi Liu;Rynson W. H. Lau;Ming C. Lin;Dinesh Manocha	2012		10.1007/978-3-642-36279-8_37		AI	49.655207509527855	-27.92882317537405	76626
2d05eb3ab3f7912beae120301830b2653f9cb7bf	tracking sponge size and behaviour with fixed underwater observatories		More and more fixed underwater observatories (FUO) are being used for high temporal coverage and resolution monitoring of specific areas of interest, such as coral reefs. FUOs equipped with fixed HD cameras and other sensors make it possible to relate visual characteristics of a species to characteristics of its environment.		Torben Möller;Ingunn Nilssen;Tim W. Nattkemper	2018		10.1007/978-3-030-05792-3_5	computer vision;coral reef;segmentation;artificial intelligence;underwater;computer science	Logic	49.47098168366216	-34.452039762977535	76668
7d8f72f75f8fe06ac4e7048155bf7db40d680612	a comparative study of probabilistic roadmap planners	moving object;wiskunde en informatica;configuration space;sampling technique;ordered by external client;motion planning	The probabilistic roadmap approach is one of the leading motion planning techniques. Over the past eight years the technique has been studied by many different researchers. This has led to a large number of variants of the approach, each with its own merits. It is difficult to compare the different techniques because they were tested on different types of scenes, using different underlying libraries, implemented by different people on different machines. In this paper we provide a comparative study of a number of these techniques, all implemented in a single system and run on the same test scenes and on the same computer. In particular we compare collision checking techniques, basic sampling techniques, and node adding techniques. The results should help future users of the probabilistic roadmap planning approach to choose the correct techniques.	collision detection;experiment;library (computing);motion planning;probabilistic roadmap;randomness;sampling (signal processing);statistical relational learning	Roland Geraerts;Mark H. Overmars	2002		10.1007/978-3-540-45058-0_4	simulation;computer science;theoretical computer science;data mining	SE	51.377156926256866	-24.384251802257328	77367
579bda83b8577a6a684d7747752f751e350735d3	collective localization: a distributed kalman filter approach to localization of groups of mobile robots	robot sensing systems robot kinematics mobile robots sensor systems pollution measurement sensor phenomena and characterization intelligent robots intelligent sensors filters motion measurement;orientation distributed kalman filter multiple mobile robots collective localization sensor fusion position control;mobile robot;distributed processing;kalman filters;kalman filter;mobile robots;cooperative systems;position control;multi robot systems;kalman lter;sensor fusion;sensor fusion mobile robots multi robot systems cooperative systems kalman filters position control	This paper presents a new approach to the cooperative localization problem, namely collective localization. A group of M robots is viewed as a single system composed of robots that carry, in general, di erent sensors and have di erent positioning capabilities. A single Kalman lter is formulated to estimate the position and orientation of all the members of the group. This centralized schema is capable of fusing information provided by the sensors distributed on the individual robots while accommodating independencies and interdependencies among the collected data. In order to allow for distributed processing, the equations of the centralized Kalman lter are treated so that this lter can be decomposed in M modi ed Kalman lters each running on a separate robot. The collective localization algorithm is applied to a group of 3 robots and the improvement in localization accuracy is presented.	algorithm;centralized computing;cross-correlation;distributed computing;galaxy morphological classification;interdependence;kalman filter;mobile robot;propagation of uncertainty;robotic mapping;sensor;software propagation	Stergios I. Roumeliotis;George A. Bekey	2000		10.1109/ROBOT.2000.846477	kalman filter;control engineering;mobile robot;simulation;computer science;engineering;artificial intelligence;control theory;robot control;simultaneous localization and mapping	Robotics	53.050800743211646	-34.065633149588805	77378
b19e39f112985fba96615336aa004ff361d4987e	samba: a smartphone-based robot system for energy-efficient aquatic environment monitoring	computer vision;robotic sensor;smartphone;object detection	Monitoring aquatic environment is of great interest to the ecosystem, marine life, and human health. This paper presents the design and implementation of Samba -- an aquatic surveillance robot that integrates an off-the-shelf Android smartphone and a robotic fish to monitor harmful aquatic processes such as oil spill and harmful algal blooms. Using the built-in camera of on-board smartphone, Samba can detect spatially dispersed aquatic processes in dynamic and complex environments. To reduce the excessive false alarms caused by the non-water area (e.g., trees on the shore), Samba segments the captured images and performs target detection in the identified water area only. However, a major challenge in the design of Samba is the high energy consumption resulted from the continuous image segmentation. We propose a novel approach that leverages the power-efficient inertial sensors on smartphone to assist the image processing. In particular, based on the learned mapping models between inertial and visual features, Samba uses real-time inertial sensor readings to estimate the visual features that guide the image segmentation, significantly reducing energy consumption and computation overhead. Samba also features a set of lightweight and robust computer vision algorithms, which detect harmful aquatic processes based on their distinctive color features. Lastly, Samba employs a feedback-based rotation control algorithm to adapt to spatiotemporal evolution of the target aquatic process. We have implemented a Samba prototype and evaluated it through extensive field experiments, lab experiments, and trace-driven simulations. The results show that Samba can achieve 94% detection rate, 5% false alarm rate, and a lifetime up to nearly two months.	algorithm;android;aquatic ecosystem;built-in self-test;computation;computer vision;earthbound;experiment;image processing;image scaling;image segmentation;multistage interconnection networks;on-board data handling;overhead (computing);pixel;prototype;real-time clock;robot;samba tng;sensor;simulation;smartphone;thresholding (image processing)	Yu Wang;Rui Tan;Guoliang Xing;Jianxun Wang;Xiaobo Tan;Xiaoming Liu	2015		10.1145/2737095.2737100	embedded system;computer vision;simulation;computer science	Mobile	51.461527390632114	-31.848913408314147	77463
3de9ff12e10a1ab5b506a21ecf0dbf1fa30411f9	efficient parallel implementation of morphological operation on gpu and fpga	video signal processing computer vision feature extraction field programmable gate arrays graphics processing units image morphing object detection object recognition;morphological operation delay line architecture fpga platform shared memory texture memory global memory gpu platform field programming gate array graphics processing unit computer vision moving object detection feature extraction object recognition video applications versatile image;fpga morphological operation computer vision gpu;graphics processing units morphological operations field programmable gate arrays throughput optimization instruction sets random access memory	Morphological operation constitutes one of a powerful and versatile image and video applications applied to a wide range of domains, from object recognition, to feature extraction and to moving objects detection in computer vision where real-time and high-performance are required. However, the throughput of morphological operation is constrained by the convolutional characteristic. In this paper, we analysis the parallelism of morphological operation and parallel implementations on the graphics processing unit (GPU), and field programming gate array (FPGA) are presented. For GPU platform, we propose the optimized schemes based on global memory, texture memory and shared memory, achieving the throughput of 942.63 Mbps with 3×3 structuring element. For FPGA platform, we present an optimized method based on the traditional delay-line architecture. For 3×3 structuring element, it achieves a throughput of 462.64 Mbps.	analog delay line;computer graphics;computer vision;data rate units;feature extraction;field-programmable gate array;graphics processing unit;mathematical optimization;outline of object recognition;parallel computing;program optimization;real-time locating system;shared memory;structuring element;texture memory;throughput	Teng Li;Yong Dou;Jingfei Jiang;Jing Gao	2014	Proceedings 2014 IEEE International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)	10.1109/SPAC.2014.6982728	computer vision;computer hardware;computer science;real-time computer graphics;general-purpose computing on graphics processing units;computer graphics (images)	EDA	43.37607007374143	-35.51319040623634	77533
e83714ec034783a43d32478451514c0852485933	a density-based recursive ransac algorithm for unmanned aerial vehicle multi-target tracking in dense clutter		Target tracking is a hot topic for unmanned aerial vehicle surveillance. Recently, the novel random sample consensus (RANSAC) algorithm shows a good tracking performance in dense clutter environment. However, the heavy computational burden limits the usage for unmanned aerial vehicle (UAV). In this paper, a density-based recursive random sample consensus (DBR-RANSAC) algorithm is proposed, which utilizes the density property of measurements within several steps to direct sampling. In the DBR-RANSAC, the randomness of sampling can be avoided and the computation complexity can be reduced particularly in dense clutter. The simulation results show the validity of the proposed algorithm.	aerial photography;algorithm;clutter;computation;distributed bragg reflector;random sample consensus;randomness;recursion;sampling (signal processing);sampling in order;simulation;unmanned aerial vehicle	Feng Yang;Weikang Tang;Hua Lan	2017	2017 13th IEEE International Conference on Control & Automation (ICCA)	10.1109/ICCA.2017.8003029	noise measurement;ransac;randomness;engineering;clutter;computation;algorithm design;sampling (statistics);recursion;computer vision;artificial intelligence	Robotics	49.437436836219526	-33.158535810953126	77803
039a69fc95f539205fe789ac823e4a6184915369	hardware accelerated collision detection - an architecture and simulation results	complex objects;queue management;single chip acceleration;physically based simulations;computer graphics hardware description languages pipeline processing parallel architectures microprocessor chips real time systems force feedback simulation;memory management;medical simulation;detection algorithms;hardware acceleration computer architecture computational modeling medical simulation computer simulation object detection computer science computer graphics detection algorithms;computer graphics;real time;bounding volume hierarchy;simulation;speed up;hardware description languages;network processor;parallelized architecture;hardware accelerator;bounding volume hierarchies;vhdl simulation;acceleration;computer graphic;hardware architecture;chip;computer architecture;force feedback;computational modeling;collision detection;parallel architectures;physically based simulation;hardware accelerated collision detection;real time collision detection;hierarchical collision detection algorithm;discretely oriented polytopes;computer science;collision queries;computer simulation;hardware implementation;physically based simulations hardware accelerated collision detection hardware architecture single chip acceleration hierarchical collision detection algorithm collision queries discretely oriented polytopes intersecting triangles pipelined architecture parallelized architecture traversal schemes bounding volume hierarchies vhdl simulation speed up real time collision detection complex objects force feedback;object detection;pipeline processing;software implementation;microprocessor chips;traversal schemes;pipelined architecture;hardware;real time systems;intersecting triangles	We present a hardware architecture for a single-chip acceleration of an efficient hierarchical collision detection algorithm as well as simulation results for collision queries using this architecture. The architecture consists of two main stages, one for traversing simultaneously a hierarchy of discretely oriented polytopes, and one for intersecting triangles. Within each stage, the architecture is deeply pipelined and parallelized. For the first stage, we compare and evaluate different traversal schemes for bounding volume hierarchies. A simulation in VHDL shows that a hardware implementation can offer a speed-up over a software implementation by orders of magnitude. Thus, real-time collision detection of complex objects at rates required by force-feedback and physically-based simulations can be achieved.	algorithm;bounding volume hierarchy;collision detection;haptic technology;parallel computing;pipeline (computing);real-time clock;simulation;vhdl	Andreas Raabe;Blazej Bartyzel;Joachim K. Anlauf;Gabriel Zachmann	2005	Design, Automation and Test in Europe	10.1109/DATE.2005.167	computer simulation;chip;acceleration;medical simulation;embedded system;parallel computing;real-time computing;hardware acceleration;speedup;computer science;theoretical computer science;operating system;hardware architecture;queue management system;hardware description language;haptic technology;computer graphics;computational model;bounding volume hierarchy;collision detection;network processor;memory management	EDA	43.68620536396098	-31.97924674290585	77992
8f04f19fe1ec7a16df29fb78f66aaf9e6592289e	an information theoretic view of the scheduling problem in whole-body cad	whole body;problem;computer aided diagnosis;corps entier;probleme;informacion;scaling up;objective function;medical diagnostics;scheduling;conditional entropy;computer aided detection;scheduling problem;body imaging;head;problema;task scheduling;information gain;information theoretic;conditional probability;information;ordonnancement;reglamento;cuerpo entero	Emerging whole-body imaging technologies push computer aided detection/diagnosis (CAD) to scale up to a whole-body level, which involves multiple organs or anatomical structure. To be exploited in this paper is the fact that the various tasks in whole-body CAD are often highly dependent (e.g., the localization of the femur heads strongly predicts the position of the iliac bifurcation of the aorta). One way to effectively employ task dependency is to schedule the tasks such that outputs of some tasks are used to guide the others. In this sense, optimal task scheduling is key to improve overall performance of a whole-body CAD system. In this paper, we propose a method for task scheduling that is optimal in an information-theoretic sense. The central idea is to schedule tasks in such an order that each operation achieves maximum expected information gain over all the tasks. The formulation embeds two intuitive principles: (1) a task with higher confidence tends to be scheduled earlier; (2) a task with higher predictive power for other tasks tends to be scheduled earlier. More specifically, task dependency is modeled by conditional probability; the outcome of each task is assumed to be probabilistic as well; and the objective function is based on the reduction of the summed conditional entropy over all tasks. The validation is carried out on a challenging CAD problem, multi-organ localization in wholebody CT. Compared to unscheduled and ad hoc scheduled organ detection/localization, our scheduled execution achieves higher accuracy with much less computation time.	bifurcation theory;computation;computer-aided design;conditional entropy;hoc (programming language);imaging technology;information gain in decision trees;information theory;internationalization and localization;kullback–leibler divergence;loss function;optimization problem;scheduling (computing);time complexity	Yiqiang Zhan;Xiang Sean Zhou;Arun Krishnan	2008		10.1117/12.770922	job shop scheduling;real-time computing;simulation;information;conditional probability;telecommunications;computer science;distributed computing;kullback–leibler divergence;head;scheduling;conditional entropy	Robotics	45.02477700553031	-27.94375782427923	78059
dd68da7794b45e5cca988860582cac0e8ecf7976	localization of a team of heterogeneous robots for a distributed sensing task	traveling salesman problem;robot localization;robot sensing systems;group 1;multidepot traveling salesman problem heterogeneous robot distributed sensing task mobile robot;mobile robot;travelling salesman problems mobile robots;service robots;mobile robots;systems engineering and theory;collective robotics;optimal path;laser range finder;multidepot traveling salesman problem;electronics industry;travelling salesman problems;field of view;heterogeneous robot;optimization;positional information;traveling salesman problems;collision avoidance;robot sensing systems robot kinematics mobile robots systems engineering and theory service robots electronics industry sonar collision avoidance traveling salesman problems robot localization;infrared;optimization collective robotics robot localization task allocation;distributed sensing;distributed sensing task;task allocation;robot kinematics;sonar	This paper presents a methodology for localizing the members of a heterogeneous team of mobile robots in an unstructured and unknown environment. In this approach robots are being divided into two groups: (1) localizers, and (2) missioners. The localizers are equipped with precise non-contact sensors such as laser range finders and/or vision. Therefore, they can find the distance, bearing and orientation of the other robots with high accuracy, as long as they are in localizer's field of view. The missioners would be equipped with simple odometry and/or local range sensors (i.e., SONAR, Infrared etc.) for collision avoidance. All robots are capable of cross communication by passing messages via a network. When a missioner is being detected by a localizer, it receives a message from the localizer which contains missioner's precise position information. Hence it is able to update its belief about its position on fly. The effect of this methodology in a distributed sensing task in the context of solving a multi-depot traveling salesman problem (MDTSP) has been briefly addressed where missioners go through sub-optimal paths to their target positions being assigned through an auction process	algorithm;ccir system a;cooperative multitasking;internationalization and localization;mobile robot;motion planning;odometry;sensor;simulation;sonar;travelling salesman problem	Shaahin Mehdinezhad Rushan;Mehran Mehrandezh;Raman B. Paranjape	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277605	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence	Robotics	53.267102176370294	-33.99906282731308	78114
8b4c7c5bd341e967a4a0390fb7736db2ae949912	pattern recognition in classification of intelligent composites during smart manufacturing	systeme temps reel;characteristic;sistema experto;systeme intelligent;fibra optica;decision tree;curing;learning;electromechanical properties;optical measurement;real time;sistema inteligente;capteur optique;base connaissance;intelligence artificielle;systeme integre;sistema integrado;caracteristica;arbol decision;computer modelling;captador optico;propiedad electromecanica;aprendizaje;captador medida;apprentissage;induccion;fichier log;measurement sensor;fichero actividad;capteur mesure;induction;mesure optique;classification system;intelligent system;medida optica;process control;optical fibre;pattern classification;saladura;pattern recognition;caracteristique;artificial intelligence;base conocimiento;real time system;sistema tiempo real;inteligencia artificial;reconnaissance forme;systeme expert;salage;reconocimiento patron;optical fiber;optical sensor;integrated system;manufacturing system;data acquisition;arbre decision;piezoelectric sensor;propriete electromecanique;log file;fibre optique;knowledge base;expert system;classification forme	The development of an on line computer based classification system for the real time classification of different composites is addressed in this study. Different parameters were collected simultaneously when embedded sensors (dielectric, optical fibre, and piezoelectric sensors) were used within two different composite matrices during the curing process. The measurements were used by an algorithm-based software as a logged data file, resulting in to induction of a decision tree. Later, a systematic software is designed based on the rules derived from this decision tree, to recognise the type of composites used in the experiment together with recognition of their physical and mechanical characteristics. This is a new approach to data acquisition in intelligent materials produced by smart manufacturing system.	pattern recognition	Nasser Esmaili;Afshad Talaie	2003		10.1007/978-3-540-45080-1_135	knowledge base;telecommunications;computer science;artificial intelligence;optical fiber;process control;expert system	Robotics	39.4490732498145	-31.324409594526262	78192
ccc498c52f531344f140512a41ab202397073813	tracking extended targets - a switching algorithm versus the sjpdaf	robot sensing systems;probability;sample based joint probabilistic data association filter;radar tracking;mobile robot;mobile robotics;kalman filters;kalman filter;mobile robots;tracking filters;switching algorithm;tracking extended targets;tracking filters kalman filters mobile robots particle filtering numerical methods probability sensor fusion target tracking;particle filtering tracking extended targets switching algorithm sample based joint probabilistic data association filter sjpdaf mobile robotics kalman filter viterbi algorithm geometrical characteristics;particle filter;viterbi algorithm;sjpdaf;particle filtering;clustering algorithms;robustness;computer science;particle tracking;sensor fusion;particle filters;target tracking;extended targets;particle filter tracking mobile robotics extended targets viterbi algorithm;tracking;geometrical characteristics;particle filtering numerical methods;target tracking radar tracking mobile robots viterbi algorithm robot sensing systems particle filters particle tracking robustness computer science clustering algorithms	Tracking extended targets is of central interest in mobile robotics as it is a prerequisite for interaction with the environment. There are significant differences between tracking punctiform targets and tracking extended targets. Most of the existing algorithms assume the targets to be punctiform, which is not always suitable. In recent years, two advanced algorithms for tracking extended targets have been developed by the authors-a switching algorithm and the SJPDAF. The switching algorithm uses the Kalman filter and an adapted version of the Viterbi algorithm, which includes certain geometrical characteristics of the problem. The SJPDAF combines the idea of particle filtering with the JPDAF. In this paper we present these two algorithms and compare them with respect to accuracy, speed and robustness in case of crossing targets. We show, that the more recently developed switching algorithm outperforms the SJPDAF regarding these criterions	joint probabilistic data association filter;kalman filter;mobile robot;particle filter;robotics;viterbi algorithm	Andreas Kräußling;Dirk Schulz	2006	2006 9th International Conference on Information Fusion	10.1109/ICIF.2006.301628	computer vision;simulation;computer science;control theory	Robotics	51.9363041260307	-34.585612294042484	79087
cc5c82da4c46587a5124f6b000395098def0ced7	pe-tld: parallel extended tracking-learning-detection for multi-target tracking	vibe;detection;multiple target tracking;gpgpu;tld	Multi-target tracking in video has been a research focus, with the combination of many fields, such as computer vision, artificial intelligence, pattern matching. In this paper, we present an efficient multi-target recognition and tracking algorithm based on TLD Tracking-Learning-Detection, named PE-TLD. A new foreground extraction filter using ViBe is introduced to improve the speed and accuracy of detection. A new target recognition component is added, and core detector is improved. Based on that, we further implemented a parallel version, taking advantage of the state-of-the-art parallel computing techniques such as OpenMP and OpenCL, which runs efficiently on a system with both multi-core CPU and GPU. Experiments showed that PE-TLD is up to 5 times faster than the serial version. PE-TLD is an automatic multi-target recognition and tracking system, which is efficient enough to be deployed for real-time usage.		Chenggang Zhou;Qiankun Dong;Wenjing Ma;Guoping Long;Tao Li	2015		10.1007/978-3-319-27122-4_46	computer vision;parallel computing;computer hardware;computer science;operating system;video tracking;thermoluminescent dosimeter;general-purpose computing on graphics processing units;computer graphics (images)	Vision	43.65105716049476	-36.3199095706303	79440
31a0baff4852a75d8fae04f68e888f5d7640a797	sensor registration and calibration using moving targets	unsupervised learning;target tracking mutual information sensor registration sensor calibration imm;sensor calibration;nonparametric statistics;signal streams;distributed sensor fusion;imm;laser sensors;calibration sensor fusion mutual information sensor phenomena and characterization multimodal sensors probability distribution unsupervised learning laser fusion laser modes target tracking;target occlusion;statistical distributions;nonparametric sensing model;moving targets;unsupervised learning calibration nonparametric statistics sensor fusion statistical distributions target tracking;probability distribution;multiple model;mutual information;sensor registration;target tracking sensor registration sensor calibration moving targets distributed sensor fusion probability distribution sensory signals mutual information unsupervised learning nonparametric sensing model signal streams laser sensors target occlusion;sensor fusion;target tracking;calibration;conference proceeding;sensory signals	Multimodal sensor registration and calibration are crucially important aspects in distributed sensor fusion. Unknown relationships of sensors and joint probability distribution between sensory signals make the sensor fusion nontrivial. In this paper, we adopt a Mutual Information (MI) based approach for sensor registration and calibration. It is based on unsupervised learning of a nonparametric sensing model by maximizing mutual information between signal streams. Experiments were carried out in an office like environment with two laser sensors capturing arbitrarily moving people. Attributes of the moving targets are used. Problems due to target occlusions are alleviated by the multiple model tracker. The registration and calibration methodology does not require any artificially generated patterns or motions unlike other calibration methodologies	multimodal interaction;mutual information;sensor;unsupervised learning	Sarath Kodagoda;Alen Alempijevic;James Patrick Underwood;Suresh Kumar;Gamini Dissanayake	2006	2006 9th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2006.345361	unsupervised learning;probability distribution;computer vision;computer science;machine learning;pattern recognition;mathematics;statistics	Robotics	50.95223081801555	-35.032278607374145	79562
f0db98a09ff47e7c42de654cd3c6a69731de150f	a fuzzy-logic architecture for autonomous multisensor data fusion	automatic control;tratamiento datos;control systems;object recognition;architecture systeme;fuzzy membership function;commande logique;calculus mobile robots path planning fuzzy control sensor fusion resource allocation inference mechanisms object recognition fuzzy logic;autonomous vehicle;resource allocation;path planning;fuzzy control;department of defense;multisensor data fusion;logique floue;resource management;data processing;automatic object recognition;logica difusa;inference mechanisms;traitement donnee;mobile robots;remotely operated vehicles;fuzzy membership functions;data fusion;control logico;data association;feature based object identification;fuzzy logic;sensor resource allocation;control proceso;fuzzy logic architecture;obstacle avoidance;calculus;fuzzy inference;process control;pattern recognition;logic control;arquitectura sistema;fuzzy logic automatic control control systems pattern recognition resource management laboratories object recognition remotely operated vehicles mobile robots process control;process model;mission completion;robust fuzzy calculus;sensor fusion;self direction;system architecture;applied research;situation assessment;robust fuzzy calculus autonomous vehicle self direction obstacle avoidance mission completion multisensor data fusion fuzzy logic architecture fuzzy membership functions data association multiple hypothesis trackers pattern recognition feature based object identification fuzzy inference sensor resource allocation automatic object recognition process control;object identification;commande processus;multiple hypothesis trackers	Fuzzy logic techniques have become popular to address various processes for multisensor data fusion. Examples include the following: (1) fuzzy membership functions for data association; (2) evaluation of alternative hypotheses in multiple hypothesis trackers; (3) fuzzy-logic-based pattern recognition (e.g., for feature-based object identification); and (4) fuzzy inference schemes for sensor resource allocation. These approaches have been individually successful but are limited to only a single subprocess within a data fusion system. At The Pennsylvania State University, Applied Research Laboratory, a general-purpose fuzzy-logic architecture has been developed that provides for control of sensing resources, fusion of data for tracking, automatic object recognition, control of system resources and elements, and automated situation assessment. This general architecture has been applied to implement an autonomous vehicle capable of self-direction, obstacle avoidance, and mission completion. The fuzzy logic architecture provides interpretation and fusion of multisensor data (i.e., perception) as well as logic for process control (action). This paper provides an overview of the fuzzy-logic architecture and a discussion of its application to data fusion in the context of the Department of Defense (DoD) Joint Directors of Laboratories (JDL) Data Fusion Process Model. A new, robust, fuzzy calculus is introduced. An application example is provided.	fuzzy logic	James A. Stover;David L. Hall;Ronald E. Gibson	1996	IEEE Trans. Industrial Electronics	10.1109/41.499813	computer vision;data processing;computer science;engineering;control system;artificial intelligence;resource management;machine learning;automatic control;process control;sensor fusion	Robotics	50.671151131784086	-30.964058952282308	79612
65ebea6e57800e0bf06e0ed2f546716e0812259a	playing in traffic: an investigation of low-cost, non-invasive traffic sensors for street light luminaire deployments		Real-time traffic monitoring is essential to the development of smart cities as well as its potential for energy savings. However, real-time traffic monitoring is a task that requires sophisticated and expensive hardware. Due to the prohibitive cost of specialized sensors, accurate traffic counts are typically limited to intersections where traffic information is used for signalling purposes. The sparse arrangement of traffic detection points does not provide adequate information for intelligent lighting applications, such as adaptive dimming. This paper investigates the low-cost and off-the-shelf sensors to be installed inside street lighting luminaires for traffic sensing. A luminaire-mounted sensor test-bed installed on a moderately-busy road trialled three non-invasive presence-detection sensors: Passive Infrared (PIR), Sonar (UVD) and lidar. The proof-of-concept study revealed that a HC-SR501 PIR motion detector could count traffic with 73% accuracy at a low cost and may be suitable for intelligent lighting applications if accuracy can be further improved.	sensor	Karl Mohring;Trina S. Myers;Ian M. Atkinson	2018	IJGUC	10.1504/IJGUC.2018.10016123	computer science;wireless sensor network;real-time computing;computer network;street light;intelligent lighting;sonar;lidar;motion detector;internet of things	Networks	51.09254709808853	-32.02157188648731	79823
81995a043be9c159b1c00c5bb2ce96697120b3e0	global localization of mobile robots by reverse projection of sensor readings	reverse projection;robot sensing systems;local algorithm;mobile robot;discretized pose space;mobile robots;correlation based approaches;data mining;state estimation;comparative advantage;distance measurement;current measurement;mobile robots robot sensing systems orbital robotics state estimation biomimetics biosensors computational modeling costs probability distribution noise measurement;sensor readings;markov based approaches;robots;global localization;correlation based approaches global localization mobile robots reverse projection sensor readings discretized pose space state estimation markov based approaches;markov processes;correlation;state estimation markov processes mobile robots;noise	Global localization algorithms involve a search over all possible poses of the robot that can be typically over a large space in huge maps. Essentially it involves computing a posterior by seeing how probable are the obtained sensor readings at each of the discretized states in a map. Instead in this paper by reverse projecting the sensor readings from the obstacle boundaries onto the surroundings, a solution is obtained by searching over the space of obstacle boundaries than by a search in the discretized pose space. That this search over obstacle boundaries is considerably less if the ratio of free space to boundary space in a map is high is straightforward. However we also show theoretically that even when the boundary space exceeds the free space the computations due to the current method does not exceed those due to the popular Markov and Correlation based approaches to global localization. The comparative advantages are well documented in simulation section of the paper. The approach is able to consistently localize a laser equipped robot in our lab.	algorithm;computation;discretization;map;markov chain;mobile robot;sensor;simulation	Hemanth Korrapati;K. Madhava Krishna;Aditya Teja	2008	2008 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2009.4913262	mobile robot;computer vision;mathematical optimization;simulation;computer science;artificial intelligence;mathematics	Robotics	52.62042364312849	-35.380706478756174	80302
77631ccd9d2e2d1c85ec34d3b27016775aa4ca7b	multi-sensor integration in intelligent robotic workstation	robot sensing systems;engineering workstations;parallel distributed processing;convergence;intelligent robots;global behavior;distributed processing;sensor integration;intelligent robots workstations labeling intelligent sensors robot sensing systems distributed processing intelligent systems convergence robot vision systems smart cameras;local convergence;constraint satisfaction;computer vision;industrial robots;object identification task;workstations;smart cameras;intelligent systems;local convergence properties;relaxation labeling;sensor fusion;object identification task sensor fusion engineering workstations computer vision multi sensor integration intelligent robotic workstation relaxation labeling parallel distributed processing constraint satisfaction local convergence properties global behavior;intelligent robotic workstation;relaxation theory;sensor fusion computer vision convergence distributed processing engineering workstations industrial robots knowledge based systems parallel processing relaxation theory;robot vision systems;object identification;multi sensor integration;knowledge based systems;parallel processing;intelligent sensors;labeling	Automated manyfaclwing systems often include intelligent robotic workrtations which comist of multiple sensors gathering information about an environment in various aspects. The efficiency and accuracy of integrating all the measured as well as given information contribute towards the success of tasks performed at the workrtations. This problem has been investigated by many researchers using different approaches. In this paper, the technique of relaxation labeling U presented. Relaxation labeling is a parallel, distributed processing mechanism. The basic idea is the multiple constraint satidaction. AN the constraints are formukated based on some known information. The application of some iterative relaxation labeling algorithms to image processing, object identification and scene analysis has produced satis actory and partially satisfactory results in the past. However, the mathfenatics underlying the relaxation labeling models has not been thoroughly understood. The convergence properties of some of these algorithms have been investigated by a few researchers; but, issues such as how the convergence is progressed is seIdom addressed. For these reasons, a new relaxation labeling algorithm is developed and presented in this paper. The local convergence properties of a labeling rocess are established. Its global behavior is examined numerically. fo illustrate ils potential applications in robotic workstatiom, the new relaxation labeling algorithm is applied to perform an object identi&ation fask in an intelligent robotic workration with fwo cameras.	algorithm;connectionism;distributed computing;image processing;iterative method;lagrangian relaxation;linear programming relaxation;local convergence;national fund for scientific research;numerical analysis;robot;sensor;successive over-relaxation;workstation	Qing Chen;J. Y. S. Luh	1992		10.1109/ROBOT.1992.220169	local convergence;smart camera;parallel processing;computer vision;labeling theory;simulation;workstation;convergence;constraint satisfaction;computer science;artificial intelligence;distributed computing;sensor fusion;intelligent sensor	Robotics	50.668130315082735	-30.94312796583807	80832
0c04ba95810111b6e9f1c5456eb0ebc3c94f8420	reduced memory implementation of modified serial watershed algorithm based on ordered queue	code standards field programmable gate arrays application specific integrated circuits video coding image segmentation mathematical morphology feature extraction;video object;mathematical morphology;image segmentation;hardware gray scale field programmable gate arrays application specific integrated circuits mpeg 4 standard layout image segmentation floods guidelines video coding;code standards;chip;fpga asic implementation reduced memory implementation modified serial watershed algorithm mpeg 4 standard video coding video object extraction image segmentation mathematical morphology ordered queue;video coding;watershed transform;application specific integrated circuits;feature extraction;field programmable gate arrays;hardware implementation	The MPEG-4 standard offers guidelines to compose video objects to get complete visual scenes. In order to extract video objects from the video content of a scene, image segmentation becomes a necessary first step in the MPEG-4 video coding. The watershed transformation is one of the basic tools in mathematical morphology, which is employed in image segmentation. Direct implementation of a serial watershed algorithm based on ordered queue is very slow and consumes a lot of resources. Faster implementation of the algorithm on a chip is difficult owing to a large amount of memory required. In this paper, we propose a modified algorithm which requires fifty times less memory compared to the original watershed algorithm by ordered queue, and is also suitable for hardware implementation. Simulation results have been furnished to validate the proposed algorithm. An architecture suitable for FPGA/ASIC implementation is also proposed.	algorithm;watershed (image processing)	Kumud Prakash Gupta;S. Srinivasan	2003		10.1109/ITCC.2003.1197582	computer vision;computer science;theoretical computer science;computer graphics (images)	EDA	43.67766029579703	-34.42410114275934	81036
07d3d5500dd47f500e908ee1f65e7e0b3208ebf7	robust registration of multi-modal images: towards real-time clinical applications	high resolution;clinical application;real time;real time processing;robust estimation;parallelism;mr imaging;image guided therapy;matching;medical imaging;registration;high performance computer;medical application;parallel architecture	High performance computing has become a key step to introduce computer tools, like real-time registration, in the medical field. To achieve real-time processing, one usually simplifies and adapts algorithms so that they become application and data specific. This involves designing and programming work for each application, and reduces the generality and robustness of the method. Our goal in this paper is to show that a general registration algorithm can be parallelized on an inexpensive and standard parallel architecture with a mall amount of additional programming work, thus keeping intact the algorithm performance. For medical applications, we show that a cheap cluster of dual-processor PCs connected by an Ethernet network is a good trade-off between the power and the cost of the parallel platform. Portability, scalability and safety requirements led us to choose OpenMP to program multi-processor machines and MPI to coordinate the different nodes of the cluster. The resulting computation times are very good on small and medium resolution images, and they are still acceptable on high resolution MR images (resp. 19, 45 and 95 seconds on 5 dual-processors Pentium III 933 MHz).	algorithm;central processing unit;computation;image resolution;message passing interface;modal logic;multi-core processor;multiprocessing;openmp;parallel computing;real-time locating system;real-time transcription;requirement;robustness (computer science);scalability;software portability	Sébastien Ourselin;Radu Stefanescu;Xavier Pennec	2002		10.1007/3-540-45787-9_18	matching;medical imaging;computer vision;real-time computing;simulation;radiology;image resolution;medicine;computer science;computer graphics (images)	HPC	41.45585105136132	-33.804684980354885	81386
208c0ac6ba974cee145e805df4a8f421f51672e8	autonomous indoor robot navigation using a sketch interface for drawing maps and routes	mobile robots;real world scenarios autonomous indoor robot navigation sketch interface drawing routes drawing maps hand drawn sketches indoor environment local deformations metric manifold;navigation robot kinematics trajectory robot sensing systems measurement manifolds	Hand-Drawn sketches are natural means by which abstract descriptions of environments can be provided. They represent weak prior information about the scene, thereby enabling a robot to perform autonomous navigation and exploration when a full metrical description of the environment is not available beforehand. In this paper, we present an extensive evaluation of our navigation system that uses a sketch interface to allow the operator of a robot to draw a rough map of an indoor environment as well as a desired trajectory for the robot to follow. We employ a theoretical framework for sketch interpretation, in which associations between the sketch and the real world are modeled as local deformations of a suitable metric manifold. We investigate the effectiveness of our system and present empirical results from a set of experiments in real-world scenarios, focusing both on the navigation capabilities and the usability of the interface.	autonomous robot;experiment;map;robotic mapping;tablet computer;the times;usability;usability testing	Federico Boniardi;Abhinav Valada;Wolfram Burgard;Gian Diego Tipaldi	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487453	mobile robot;computer vision;simulation;computer science;artificial intelligence;mobile robot navigation;computer graphics (images)	Robotics	51.650069059783014	-36.180688073672165	81403
3aa1d664c89932f44fd9056ff0f06f151876a44b	stereo-camera-based urban environment perception using occupancy grid and object tracking	multiple model estimator stereo camera based urban environment perception occupancy grid automobile applications onboard sensors object based methods grid based method stereo sensors inverse sensor model dempster shafer theory bayes theorem grid cell clusters object tracking framework;moving object;urban environment;reliability;image processing;sensors;radar tracking;sensor model;bayes methods;computer model;occupancy grid;stereoscopic cameras;grids coordinates;inference mechanisms;uncertainty handling;motion;sensors cameras vehicles computational modeling radar tracking tracking urban areas;computational modeling;urban areas;mathematical models;signal processing;environment perception;dempster shafer theory;object tracking;stereo image processing;statistics;urban area;stereo image processing environment perception object tracking occupancy grid;vehicles;detection and identification systems;road safety;uncertainty handling bayes methods cameras driver information systems inference mechanisms object tracking road safety stereo image processing;driver information systems;cameras;tracking;interacting multiple model	This paper deals with environment perception for automobile applications. Environment perception comprises measuring the surrounding field with onboard sensors such as cameras, radar, lidars, etc., and signal processing to extract relevant information for the planned safety or assistance function. Relevant information is primarily supplied using two well-known methods, namely, object based and grid based. In the introduction, we discuss the advantages and disadvantages of the two methods and subsequently present an approach that combines the two methods to achieve better results. The first part outlines how measurements from stereo sensors can be mapped onto an occupancy grid using an appropriate inverse sensor model. We employ the Dempster-Shafer theory to describe the occupancy grid, which has certain advantages over Bayes' theorem. Furthermore, we generate clusters of grid cells that potentially belong to separate obstacles in the field. These clusters serve as input for an object-tracking framework implemented with an interacting multiple-model estimator. Thereby, moving objects in the field can be identified, and this, in turn, helps update the occupancy grid more effectively. The first experimental results are illustrated, and the next possible research intentions are also discussed.	grid computing;interaction;object-based language;radar tracker;sensor;signal processing;stereo camera	Thien-Nghia Nguyen;Bernd Michaelis;Ayoub Al-Hamadi;Michael Tornow;Marc-Michael Meinecke	2012	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2011.2165705	computer vision;radar tracker;simulation;dempster–shafer theory;image processing;computer science;sensor;motion;occupancy grid mapping;signal processing;video tracking;mathematical model;reliability;tracking;computational model;statistics	Robotics	51.04631492908275	-35.638402101426884	81720
3ea7120d92e18b41e4b74038806198f924169de1	a 240 × 180 130 db 3 µs latency global shutter spatiotemporal vision sensor	dynamic vision sensor dvs;address event representation aer;spike based;cmos image sensor;spike based active pixel sensor aps address event representation aer cmos image sensor dynamic and active pixel vision sensor davis dynamic vision sensor dvs event based neuromorphic engineering;dynamic and active pixel vision sensor davis;voltage control robot sensing systems photoreceptors photodiodes photoconductivity cameras universal serial bus;active pixel sensor aps;neuromorphic engineering;event based	Event-based dynamic vision sensors (DVSs) asynchronously report log intensity changes. Their high dynamic range, sub-ms latency and sparse output make them useful in applications such as robotics and real-time tracking. However they discard absolute intensity information which is useful for object recognition and classification. This paper presents a dynamic and active pixel vision sensor (DAVIS) which addresses this deficiency by outputting asynchronous DVS events and synchronous global shutter frames concurrently. The active pixel sensor (APS) circuits and the DVS circuits within a pixel share a single photodiode. Measurements from a 240×180 sensor array of 18.5 μm 2 pixels fabricated in a 0.18 μm 6M1P CMOS image sensor (CIS) technology show a dynamic range of 130 dB with 11% contrast detection threshold, minimum 3 μs latency, and 3.5% contrast matching for the DVS pathway; and a 51 dB dynamic range with 0.5% FPN for the APS readout.	active pixel sensor;angular defect;automatic transmitter identification system (television);autonomous robot;cmos;data compression;data rate units;decibel;dynamic voltage scaling;fixed-pattern noise;gene regulatory network;high dynamic range;image sensor;machine vision;mobile device;movie projector;outline of object recognition;real-time transcription;region of interest;robotics;sparse matrix;statistical classification;transistor	Christian Brandli;Raphael Berner;Minhao Yang;Shih-Chii Liu;Tobi Delbruck	2014	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2014.2342715	embedded system;computer vision;electronic engineering;computer science;image sensor;cmos sensor;neuromorphic engineering	Robotics	45.78963221407809	-33.71679976130437	81731
2be87360b3d4c9f0de1c476cf1e3200b588024d0	co-evolution of active sensing and locomotion gaits of simulated snake-like robot	genetic program;snakebot;locomotion;genetic programming;navigation;active sensing;evolutionary optimization	We propose an approach of automated co-evolution of the optimal values of attributes of active sensing (orientation, range and timing of activation of sensors) and the control of locomotion gaits of simulated snake-like robot (Snakebot) that result in a fast speed of locomotion in a confined environment. The experimental results illustrate the emergence of a contactless wall-following navigation of fast sidewinding Snakebots. The wall-following is accomplished by means of differential steering, facilitated by the evolutionary defined control sequences incorporating the readings of evolutionary optimized sensors.	active galactic nucleus;contactless smart card;emergence;robot;sensor;the wall street journal	Ivan Tanev;Katsunori Shimohara	2008		10.1145/1389095.1389135	genetic programming;computer vision;navigation;simulation;computer science;artificial intelligence;snakebot	Robotics	53.293266806026374	-29.44251722657405	81761
e48dea5a5c4d74ec12b6817997bbef7975bde526	planetary monocular simultaneous localization and mapping		Planetary monocular simultaneous localization and mapping (PM-SLAM), a modular, monocular SLAM system for use in planetary exploration, is presented. The approach incorporates a biologically inspired visual saliency model (i.e., semantic feature detection) for visual perception in order to improve robustness in the challenging operating environment of planetary exploration. A novel method of generating hybrid-salient features, using point-based descriptors to track the products of the visual saliency models, is introduced. The tracked features are used for rover and map state-estimation using a SLAM filter, resulting in a system suitable for use in long-distance autonomous (micro)rover navigation, and the inherent hardware constraints of planetary rovers. Monocular images are used as an input to the system, as a major motivation is to reduce system complexity and optimize for microrover platforms. This paper sets out the various components of the modular SLAM system and then assesses their comparative performance using simulated data from the Planetary and Asteroid Natural Scene Generation Utility (PANGU), as well as real-world datasets from the West Wittering field trials (performed by the STAR Lab) and the SEEKER field trials in Chile (performed by the European Space Agency). The system as a whole was shown to perform reliably, with the best performance observed using a combination of Hou-saliency and speeded-up robust features (SURF) descriptors with an extended Kalman filter, which performed with higher accuracy than a state-of-the-art, independently optimized visual odometry localization system on a challenging real-world dataset. C © 2015 Wiley Periodicals, Inc.	autonomous robot;computation;computational complexity theory;dead reckoning;depth perception;extended kalman filter;feature detection (computer vision);feature detection (web development);for loop;john d. wiley;kadir–brady saliency detector;limited availability;machine vision;modular design;operating environment;particle filter;planetary scanner;qualitative comparative analysis;rover (the prisoner);simulation;simultaneous localization and mapping;speeded up robust features;time complexity;visual odometry	Abhinav Bajpai;Guy Burroughes;Affan Shaukat;Yang Gao	2016	J. Field Robotics	10.1002/rob.21608	computer vision;simulation;remote sensing	Robotics	53.1056054105696	-37.960073009045274	81833
c9f1e95c9122613d33410f76f262bb70e19474ba	understanding human interaction for probabilistic autonomous navigation using risk-rrt approach	disturbance risk human interaction probabilistic autonomous navigation risk rrt approach personal assistance mobile service robotics robot navigation systems social conventions risk based navigation method collision risk;human interaction;probability;gaussian processes;proxemics;mobile robot;path planning;human aware navigation;risk management;robot navigation;mobile robots;adaptive behavior;human robot interaction;humans navigation collision avoidance trajectory mobile robots gaussian processes;navigation;mobile service;trajectory;risk assessment proxemics human aware navigation;emergent behavior;risk management human robot interaction mobile robots path planning probability;risk assessment;autonomous navigation;humans;collision avoidance;gaussian process	With the growing demand of personal assistance to mobility and mobile service robotics, robot navigation systems must be “aware” of the social conventions followed by people. They must respect proximity constraints but also respect people interacting. For example, they may not break interaction between people talking, unless the occupants want to take part in the conversation. In this case, they must be able to join the group using a socially adapted behavior. This paper proposes a risk-based navigation method including both the traditional notion of risk of collision and the notion of risk of disturbance. Results exhibit new emerging behavior showing how a robot takes into account social conventions in its navigation strategy.	algorithm;autonomous robot;autonomous system (internet);experiment;interaction;robotic mapping;robotics;sensor;simulation	Jorge Rios-Martinez;Anne Spalanzani;Christian Laugier	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6094496	mobile robot;computer vision;simulation;risk management;computer science;artificial intelligence;gaussian process;mobile robot navigation	Robotics	51.132109234169626	-27.429002625826072	81977
53448131a579644475db6b55d5e376dc68ce2645	statistical modeling and design issues of a crossbeam sensor	robot sensing systems;object recognition;optimisation;industrial manufacturing tasks;reduced intricacy control;reduced instruction set computing;service robots;manufacturing industries;optimal design technique;statistical model;density functional theory;robot control;statistical analysis;optical arrays;industrial robots;reduced intricacy sensing;risc robotics;optimal design;multiple hypotheses testing;optical sensors;reduced instruction set computing robot sensing systems service robots sensor arrays robot control manufacturing industries hardware object recognition density functional theory optical arrays;statistical modeling;crossbeam sensor design issues;optimisation industrial robots optical sensors statistical analysis;optimal design technique statistical modeling crossbeam sensor design issues risc robotics reduced intricacy sensing reduced intricacy control industrial manufacturing tasks multiple hypotheses testing method;multiple hypotheses testing method;sensor arrays;hardware	RISC (Reduced Intricacy Sensing and Control) Robotics has been launched and well investigated in the past few years [ 1-81. The basic idea of RlSC Robotics is an attempt to per$orm challenging industrial manufacturing tasks by using a combination of simple hardware and sophisticated algorithms. Many efective strategies and algorithms have been explored. Howevel; the issue of optimal design of RISC Sensor has not been solved. The main reason is the shortage of good models. In this papel; we propose a statistical model for one of the @pica1 RlSC sensors, i.e. the crossbeam sensor Based on this statistical model we employ the Multiple Hypotheses-Testing method [9, 101 as optimal design technique and present its applied strategies. It is believed that this work will lead to development of new RlSC sensors because a new principle and a pertinent model are introduced into this area.	algorithm;optimal design;relevance;robotics;sensor;statistical model	Xiao-Gang Wang;Helen C. Shen;Mehrdad Moallem	2001		10.1109/IROS.2001.977185	control engineering;statistical model;electronic engineering;simulation;computer science;engineering;artificial intelligence;statistics	Robotics	51.66824093436231	-32.64570289891658	82071
d65b589f7d3e94e82e53d9b00c27c6b7c3b0fa6d	parallelized background substitution system on a multi-core embedded platform	dsp cores parallelized human background substitution system automatic human background substitution system random walk algorithm multicore processing architecture fast algorithm large linear system gauss seidel method type index face detection human shape prior model approximated human body background area seed points automatic segmentation rw algorithm heterogeneous multicore embedded platform arm processor;image segmentation;image resolution;multi core embedded system random walk algorithm image segmentation background substitution parallelization;humans face indexes arrays face detection embedded systems mathematical model;multiprocessing systems approximation theory embedded systems face recognition image resolution image segmentation iterative methods;approximation theory;arrays;iterative methods;embedded systems;indexes;face recognition;background substitution;parallelization;mathematical model;multi core embedded system;random walk algorithm;face;humans;multiprocessing systems;face detection	We present an automatic human background substitution system based on a Random Walk (RW) algorithm on a multi-core processing architecture. Firstly, a fast algorithm is proposed to solve the large linear system in RW based on adapting the Gauss-Seidel method. Two tables, TYPE and INDEX, are introduced to fast locate the required data for the close-form solution. Then, face detection along with a human shape prior model are exploited to decide the approximated human body and background area. Pixels inside these areas are used as seed points in RW algorithm for automatic segmentation. The proposed method is designed to be highly parallelizable and suitable for running on a multi-core architecture. We demonstrate the parallelization strategies for the proposed fast RW algorithm and face detection on heterogeneous multi-core embedded platform to make the most use of the system architecture. Compared to the single processor implementation, the experimental results show significant speedup ratio of the parallelized human background substitution system on a multi-core embedded platform, which consists of an ARM processor and two DSP cores.	arm architecture;approximation algorithm;baseline (configuration management);embedded system;experiment;face detection;gauss–seidel method;intel core (microarchitecture);linear system;merge sort;multi-core processor;parallel computing;pixel;read-write memory;rewriting;speedup;systems architecture;table (database);upsampling	Yutzu Lee;Chen-Kuo Chiang;Te-Feng Su;Yu-Wei Sun;Chi-Bang Kuan;Shang-Hong Lai	2012	2012 41st International Conference on Parallel Processing Workshops	10.1109/ICPPW.2012.72	facial recognition system;face;database index;computer vision;face detection;parallel computing;image resolution;computer science;theoretical computer science;operating system;mathematical model;iterative method;image segmentation;random walk;approximation theory	Robotics	41.632935968037295	-35.32536909004978	82202
1f2fb9f4ea20538e7e4c2df0c44ee325b0ea0569	satellite super resolution image reconstruction based on parallel support vector regression		Super Resolution (SR) refers to the reconstruction of a high resolution image from one or more low resolution images for the same scene. The reconstruction process is considered an inverse problem to the observation model. In this paper the SR problem is formulated by using Support Vector Regression (SVR). SVR is a very expensive computationally algorithm, thus it could be accelerated by using the computational power of a Graphics Processing Unit (GPU). The proposed parallel SVR has been implemented using NVidia’s compute device unified architecture (CUDA). An experiment has been done for a real satellite image. The experimental result demonstrates the speedup of the presented GPU implementation and compared with the serial CPU implementation and state-of-the-art techniques. The speedup of the presented SVR GPU-based implementation is up to approximately 50 times faster than the corresponding optimized CPU.	iterative reconstruction;super-resolution imaging;support vector machine	Marwa Moustafa;Hala Mousher Ebied;Ashraf K. Helmy;Taymoor M. Nazamy;Mohamed F. Tolba	2014		10.1007/978-3-319-13461-1_22	iterative reconstruction;support vector machine;architecture;graphics processing unit;computer vision;speedup;cuda;superresolution;artificial intelligence;inverse problem;computer science	ML	41.32444567107835	-32.90557392550237	82296
172ed4ae323cb3ff8de8b6440fff9d4492f8a700	robotic manipulation in object composition space	planning grasping markov processes three dimensional displays uncertainty robot sensing systems;rgb d sensor robotic manipulation object composition space cluttered environment decision making object composition information manipulation planning problem partially observable markov decision process pomdp model expected task specific utility physical robot arm;path planning decision making manipulators markov processes	Manipulating unknown objects in a cluttered environment is difficult because object composition is uncertain. Because of this uncertainty, earlier work has concentrated on finding the “best” object composition and based on this composition decided on manipulation actions. Contrary to earlier work, we 1) utilize different possible object compositions in decision making, 2) take advantage of object composition information provided by robot actions, 3) take into account the effect of different competing object hypothesis on the actual task to be performed. We cast the manipulation planning problem as a partially observable Markov decision process (POMDP) which plans over possible hypotheses of object compositions. The POMDP model chooses the action that maximizes the long-term expected task specific utility, and while doing so, considers the value of informative actions and the effect of different object hypotheses on the completion of the task. In experiments with a physical robot arm and an RGB-D sensor, our approach outperforms an approach that only considers the most likely object composition.	experiment;information;markov chain;object composition;partially observable markov decision process;partially observable system;robot;robotic arm;state space	Joni Pajarinen;Ville Kyrki	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6942372	computer vision;simulation;computer science;machine learning	Robotics	48.86873065985736	-31.7207660370555	82470
d943fd212d6615b59fc59c853570731fb35f9df0	a smart vision system-on-a-chip design based on programmable neural processor integrated with active pixel sensor	programmable neural processor;vision system;sensor systems and applications;smart pixels;focal plane;sensor systems;cmos technology;mcm chip;smart image window handler;smart vision;1 kpixel smart vision system on a chip design programmable neural processor active pixel sensor low power system aps sensor smart image window handler mcm chip standard cmos technology ultra high speed smart sensory information processing focal plane robotic vision machine vision navigation automotive applications consumer electronics spacecraft;systems engineering;active pixel sensor;1 kpixel;consumer electronics;low power system;image sensors;focal planes;system on a chip;windows;computer vision;focal planes computer vision neural chips cmos image sensors low power electronics;chip;cmos image sensors;standard cmos technology;navigation;neural chips;imaging system;ships;low power;automotive applications;system design;machine vision system on a chip intelligent sensors sensor systems and applications image sensors cmos technology smart pixels buildings sensor systems windows;machine vision;robotic vision;information processing;low power electronics;pixels;ultra high speed smart sensory information processing;system on a chip design;aps sensor;high speed;spacecraft;intelligent sensors;buildings;neural processor	A low power smart vision system based on a large format (currently 1KxlK) active pixel sensor (APS) integrated with a programmable neural processor for fast vision applications is presented. The concept of building a low power smart vision system is demonstrated by a system design, which is composed with an APS sensor, a smart image window handler, and a neural processor. The paper also shows that it is feasible to put the whole smart vision system into a single MCM chip in a standard CMOS technology. This smart vision system on-a-chip can take the combined advantages of the optics and electronics to achieve ultra-high-speed smart sensory information processing and analysis at the focal plane . The proposed system will enable many applications including robotics and machine vision, guidance and navigation, automotive applications, and consumer electronics. Future applications will also include scientific sensors such as those suitable for highly integrated imaging systems used in NASA deep space and planetary spacecraft. 1. Low Power Smart Machine Vision System Figure 1 shows a system diagram of the proposed smart vision system. The functional blocks include: (a) an active pixel sensor, (b) a smart image window handler, (c) a programmable neural processor, and (d) a host interface and timing control card. The APS is used as the optical sensing array in the system. The smart window handler manipulates the APS image data and provides the windowed image for the neural processor. The neural processor is programmed to perform various vision tasks in high speed due to its massively parallel computing structures and learning capabilities. The host computer through host interface and timing control card controls the APS sensor, the smart image window handler, and the programmable neural processor. The output image or vision science data will be displayed by the host computer. It is feasible to build the proposed smart vision system in a single CMOS chip. This smart vision system on-a-chip can take the combined advantages of the optics and electronics to achieve low-power high-speed smart sensory hformation processing and analysis at the focal plane. Tbe proposed system will enable many applications including robotics and machine vision, guidance and navigation, automotive applications, and consumer electronics. Future applications will also include scientific sensors such as those suitable for highly integrated imaging systems used in NASA deep space and planetary spacecraft. The following sections describe technical details of each building block of the proposed smart vision system and also show the feasibility to put the whole system into a single chip in a standard low power CMOS technology. 2. CMOS Active Pixel Sensor A low power CMOS APS camera-on-a-chip has been developed for producing imaging systems that can be manufactured with low cost, low power, and with excellent imaging quality [ 11. competing technology for image sensors. However, CCDs cannot be easily integrated with CMOS without additional fabrication complexity. In addition, CCDs require two-order-of-magnitude higher power dissipation than that of APS. The CCD does not have the windowing capability to provide the input data to the neural processor. On the other hand, an APS imager does not have the above limitations and it is the suitable candidate for the proposed smart vision system. The lKxlK APS is used as the optical sensing array and integrated with the neural processor to build the smart vision system for high definition vision applications. A low power lKxlK CMOS APS (operate from a +3.3 V supply) using 0.55 Om n-well process was designed and characterized at JPL. Testing results show that the large format APS with small feature size (10 micron pixel pitch) is capable of excellent imaging performance. Charge-coupled devices (CCDs) are currently the	active pixel sensor;cmos;charge-coupled device;diagram;dot pitch;emoticon;focal (programming language);host (network);image sensor;information processing;low-power broadcasting;machine vision;multi-chip module;parallel computing;planetary scanner;robotics;system on a chip;systems design;window function	Wai-Chi Fang	2000		10.1109/ISCAS.2000.856275	aps-c;chip;system on a chip;smart camera;embedded system;computer vision;navigation;electronic engineering;cardinal point;machine vision;computer hardware;computer science;engineering;image sensor;spacecraft;cmos sensor;cmos;pixel;low-power electronics;intelligent sensor;systems design	Robotics	45.23658855660109	-34.1732229299549	82505
b755c700126607a38413390bf9d43b34c02e85c5	surveillance and activity recognition with depth information	surveillance image sensors cmos image sensors computer vision optoelectronic and photonic sensors smart pixels image resolution intelligent sensors cmos technology charge coupled devices;ccd camera;time of flight;image segmentation;image resolution;surveillance;segmentation;computer vision algorithm;indexing terms;image sensors;computer vision;photonic mixer device;object detection activity recognition depth information surveillance image sensor computer vision algorithm photonic mixer device time of flight principle image resolution image segmentation;image acquisition;3d surveillance;depth information;segmentation computer vision 3d surveillance photonic mixer device;time of flight principle;image sensor;surveillance computer vision image resolution image segmentation image sensors object detection;object detection;activity recognition	In the present treatise an image sensor acquiring additional depth information is applied to extend regular computer vision algorithms. The so called photonic mixer device (PMD) basing on the time-of-flight principle can measure the distance between a smart pixel on the image sensor and the object being recorded. Since the resolution of such a sensor is rather low, they are not intended to replace existing image acquisition technologies, i.e. CMOS or CCD cameras, but can rather be employed to assist and speed up several tasks, especially image segmentation or object detection tasks.	activity recognition;algorithm;cmos;charge-coupled device;computer vision;image segmentation;image sensor;object detection;pmd;pixel;time-of-flight camera	Frank Wallhoff;Martin Rub;Gerhard Rigoll;Johann Göbel;Hermann Diehl	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284847	computer vision;computer science;image sensor;computer graphics (images);activity recognition	Vision	45.4465942628277	-35.89973882271935	82544
2fdbbafcfe81757cd95919dc135bcfdff06b14d9	on exploiting haptic cues for self-supervised learning of depth-based robot navigation affordances	tecnologia industrial tecnologia mecanica;tecnologia electronica telecomunicaciones;affordances;autonomous robots;tactile sensing;depth sensing;terrain assessment;self supervised learning;tecnologias;grupo a	This article presents a method for online learning of robot navigation a↵ordances from spatiotemporally correlated haptic and depth cues. The method allows the robot to incrementally learn which objects present in the environment are actually traversable. This is a critical requirement for any wheeled robot performing in natural environments, in which the inability to discern vegetation from non-traversable obstacles frequently hampers terrain progression. A wheeled robot prototype was developed in order to experimentally validate the proposed method. The robot prototype obtains haptic and depth sensory feedback from a pan-tilt telescopic antenna and from a structured light sensor, respectively. With the presented method, the robot learns a mapping between objects’ descriptors, given the range data provided by the sensor, and objects’ sti↵ness, as estimated from the interaction between the antenna and the object. Learning confidence estimation is considered in order to progressively reduce the number of required physical interactions with acquainted objects. To raise the number of meaningful interactions per object under time pressure, the several segments of the object under analysis are prioritised according to a set of morphological criteria. Field trials show the ability of the robot to progressively learn which elements of the environment are traversable. keywords: autonomous robots, self-supervised learning, a↵ordances, terrain assessment, depth sensing, tactile sensing	autonomous robot;color gradient;depth perception;earthbound;experiment;feedback;fundamental interaction;haptic technology;motion planning;online machine learning;prototype;robotic arm;robotic mapping;stereopsis;structured light;structured-light 3d scanner;supervised learning;synthetic intelligence	José Baleia;Pedro F. Santana;José Barata	2015	Journal of Intelligent and Robotic Systems	10.1007/s10846-015-0184-4	robot learning;computer vision;simulation;computer science;engineering;social robot;affordance;mobile robot navigation	Robotics	48.74124802540323	-35.485504510940174	82557
65b8f40dfd3192068f7cbe315423f9e294152a99	comparing two generations of embedded gpus running a feature detection algorithm		Graphics processing units (GPUs) in embedded mobile platforms are reaching performance levels where they may be useful for computer vision applications. We compare two generations of embedded GPUs for mobile devices when running a state-of-the-art feature detection algorithm, i.e., HarrisHessian/FREAK. We compare architectural differences, execution time, temperature, and frequency on Sony Xperia Z3 and Sony Xperia XZ mobile devices. Our results indicate that the performance soon is sufficient for real-time feature detection, the GPUs have no temperature problems, and support for large work-groups is important.	algorithm;central processing unit;clock rate;computer vision;decade (log scale);embedded system;freak;feature detection (computer vision);feature detection (web development);graphics processing unit;mobile device;real-time clock;run time (program lifecycle phase)	Max Danielsson;Håkan Grahn;Thomas Sievert;Jim Rasmusson	2018	CoRR		mobile device;algorithm;computer science;graphics;feature detection;freak	EDA	42.80133980810133	-35.7860545299461	82645
1484139143c4396ae16e9878f6d24f36fccf8a24	attitude estimation from polarimetric cameras		In the robotic field, navigation and path planning applications benefit from a wide range of visual systems (e.g, perspective cameras, depth cameras, catadioptric cameras, etc.). In outdoor conditions, these systems capture information in which sky regions cover a major segment of the images acquired. However, sky regions are discarded and are not considered as visual cue in vision applications. In this paper, we propose to estimate attitude of Unmanned Aerial Vehicle (UAV) from sky information using a polarimetric camera. Theoretically, we provide a framework estimating the attitude from the skylight polarized patterns. We showcase this formulation on both simulated and real-word data sets which proved the benefit of using polarimetric sensors along with other visual sensors in robotic applications.		Mojdeh Rastgoo;Cédric Demonceaux;Ralph Seulin;Olivier Morel	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593575	computer vision;artificial intelligence;computer science;polarimetry;motion planning;atmospheric model;catadioptric system;skylight;data set;sky	Robotics	53.31895845080331	-37.27342409635712	82894
f651979874cb73ce53c5cfc6b8a2b5403df253b3	comparison of credal assignment algorithms in kinematic data tracking context		This paper compares several assignment algorithms in a multitarget tracking context, namely: the optimal Global Nearest Neighbor algorithm (GNN) and a few based on belief functions. The robustness of the algorithms are tested in different situations, such as: nearby targets tracking, targets appearances management, etc. It is shown that the algorithms performances are sensitive to some design parameters. It as well shown that, for kinematic data based assignment problem, the credal assignment algorithms do not outperform the standard GNN algorithm.	assignment problem;k-nearest neighbors algorithm;performance;sensor	Samir Hachour;François Delmotte;David Mercier	2014		10.1007/978-3-319-08852-5_21	computer science;artificial intelligence;machine learning;algorithm	EDA	48.658958715391556	-34.08265211573815	82899
79948a34eb43d3f3f8f4800af5bbc77fdd105f49	lidar and stereo imagery integration for safe navigation in outdoor settings	stereo image processing geometry learning artificial intelligence mobile robots navigation optical radar robot vision statistical analysis;geometry;mobile robots;autonomous safe navigation stereo imagery integration lidar advanced sensing systems mobile robot multisensory approach automatic traversable ground detection 3d range sensors stereovision self learning scheme adaptive training stage classification stage geometric appearance single sensor classifiers;laser radar three dimensional displays training vehicles feature extraction robot sensing systems;navigation;robot vision;statistical analysis;optical radar;stereo image processing;learning artificial intelligence	Environment awareness through advanced sensing systems is a major requirement for a mobile robot to operate safely, particularly when the environment is unstructured, as in an outdoor setting. In this paper, a multi-sensory approach is proposed for automatic traversable ground detection using 3D range sensors. Specifically, two classifiers are presented, one based on laser data and one based on stereovision. Both classifiers rely on a self-learning scheme to detect the general class of ground and feature two main stages: an adaptive training stage and a classification stage. In the training stage, the classifier learns to associate geometric appearance of 3D data with class labels. Then, it makes predictions based on past observations. The output obtained from the single-sensor classifiers is statistically combined exploiting their individual advantages in order to reach an overall better performance than could be achieved by using each of them separately. Experimental results, obtained with a test bed platform operating in a rural environment, are presented to validate this approach, showing its effectiveness for autonomous safe navigation.	autonomous robot;complementarity theory;emoticon;experiment;mobile robot;sensor;stereopsis;testbed	Giulio Reina;Annalisa Milella;Werner Halft;Rainer Worst	2013	2013 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)	10.1109/SSRR.2013.6719333	computer vision;simulation;geography;mobile robot navigation;remote sensing	Robotics	48.85174095489127	-36.350255967306765	82950
cb0e2797c4e8beac82032cf31f041464b21a4729	fast reconstruction technique for medical images using graphics processing unit	medical image;graphic processing unit	In many medical imaging modalities, the Fast Fourier Transform (FFT) is being used for the reconstruction of images from acquired raw data. The objective of the paper is to develop FFT and Inverse FT algorithms to run under GPU for performing in much faster way. The GPU based FFT implementation provides much faster reconstruction of Medical images than CPU based implementation. The GPU based algorithm is developed in MATLAB environment. GPUMat is used to running CUFFT library code in MATLAB. This work exercises the acceleration of MRI reconstruction algorithm on NVIDIA’s GPU and Intel’s Core2 Duo based CPU. The reconstruction technique shows that GPU based MRI reconstruction achieved significant speedup compared to the CPUs for medical applications at a cheaper cost.	algorithm;central processing unit;data acquisition;fast fourier transform;graphics processing unit;interrupt;iterative reconstruction;matlab;medical imaging;operating system;speedup;window of opportunity	Mohammad Nazmul Haque;Mohammad Shorif Uddin;Mohammad Abdullah-Al-Wadud;Yoojin Chung	2011		10.1007/978-3-642-27183-0_32	parallel computing;computer hardware;computer science;computer graphics (images)	Graphics	41.30614523392314	-33.65345332189659	83161
899da587f33ca009cbfbdeac7c8b54e267a1bef3	vision-based deep reinforcement learning to control a manipulator		In this paper, we apply Reinforcement Learning (RL) to control a manipulator using camera images. Basically, RL algorithm helps the agent to choose actions under some specific policies, which maximize rewards accumulated from a sequential result of the actions. In order to control a manipulator, the agent generally considers joint angles or torques of the manipulator as the states. Deep learning has enabled to deal with a very high dimensional state space so that raw images can be considered as states. However, using raw images as state spaces can require extensive computational resources because of their high dimension, so there has been much research using additory algorithms such as Convolutional Neural Network (CNN) or autoencoder to extract features from images. While those algorithms reduce the computational load, it is still a quite complicated process. In this paper we consider a task for the end-effector to reach a random target and propose a new approach using a vision-based direction vector, which has low dimension and can be simply implemented. We calculate the direction vectors via the camera and exploit them as the states of the policy. Then, based on the direction vectors, the manipulator can learn how to control each joint. For the simulation, we demonstrate a multiple degree of freedom(dof) manipulator of a commercial robot, and the simulation results show that the manipulator successfully tracks the target.	algorithm;autoencoder;computation;computational resource;converge;convolutional neural network;deep learning;need to know;pixel;reinforcement learning;robot end effector;simulation;state space	Wonchul Kim;Taewan Kim;Jonggu Lee;Hyoun Jin Kim	2017	2017 11th Asian Control Conference (ASCC)	10.1109/ASCC.2017.8287315	convolutional neural network;direction vector;autoencoder;machine learning;deep learning;state space;torque;exploit;reinforcement learning;computer science;artificial intelligence	Robotics	48.9055439581373	-27.865857594717205	83246
8d85fd8ca7f3d7c8b8f77026dac4f4e42b167318	a 256×256 14k range maps/s 3-d range-finding image sensor using row-parallel embedded binary	mask circuit;row parallel embedded binary search tree;clocks;binary search trees;trees mathematics;image sensors;distance 400 mm 3d range finding image sensor row parallel embedded binary search tree address encoder current mode subtraction mask circuit;arrays;solid state circuits;distance measurement;pixel;trees mathematics distance measurement encoding image sensors;address encoder;distance 400 mm;encoding;current mode subtraction;image sensors circuits clocks encoding binary search trees calculators photodiodes optical reflection shift registers photoconductivity;3d range finding image sensor	These days, 3-D information technology is being developed rapidly and has been applied to various fields. Moreover, ultra-fast 3-D range-finding makes way for the possibilities of additional applications such as drop tests, observation of a high-speed moving target, and automated controls in robot vision. While 3-D range-finding image sensors using time-of-flight (TOF) don't achieve high-speed 3-D range finding [1,2], several high-speed and high-accuracy 3-D range-finding image sensors employing the light-section method have been reported [3–6]. Column-parallel search of a pixel activated by the reflection of a projected sheet beam is repeated the same number of times as the number of rows [3,4]. Row-parallel search of an activated pixel in each row at the same time is suitable for the light-section method when the light-section method uses a sheet beam as a horizontal scanner. Previous work [5,6] employs row-parallel search and realizes high-speed detection of the pixel; however, the methods require long address encoding paths and additional clock cycles (O(log N), N: the # of pixels in a row) whenever they acquire column addresses of detected pixels.	clock signal;embedded system;image sensor;pixel	Shingo Mandai;Makoto Ikeda;Kunihiro Asada	2010	2010 IEEE International Solid-State Circuits Conference - (ISSCC)	10.1109/ISSCC.2010.5433975	embedded system;computer vision;electronic engineering;binary search tree;computer science;theoretical computer science;image sensor;pixel;encoding	Robotics	46.21379139735236	-34.82386624529598	83635
2bcd7362be7066bbff7cdf08c1b557fcc676accd	a neural gait synthesizer for autonomous biped robots	neural nets;biped robot;real time;mobile robots;synthesizers legged locomotion switches mobile robots humans robotics and automation network synthesis adaptive systems foot motion control;switch mechanism motion control mobile robots neural gait synthesizer autonomous biped robots pattern generator adaptive neural network knowledge base learning unit;learning systems;adaptive systems;neural nets adaptive systems knowledge based systems learning systems mobile robots;adaptive neural network;central pattern generator;functional unit;knowledge based systems;knowledge base	An autonomous gait synthesis mechanism based on neuro-computing is presented. The mechanism is for generating motion trajectories of biped robots in negotiating difficult terrains. It is centered on a neural gait synthesizer. The latter consists of a number of functional unit including a central pattern generator an adaptive neural network, a knowledge base, a learning unit and a switch mechanism. The central pattern generator is responsible for generating motion patterns of voluntary and involuntary motions; the adaptive network is used to modify the reflexive motion patterns in accordance with terrain conditions; the responsibility of the switching unit is to make decisions in real time to switch between voluntary and involuntary motions; the knowledge base is used to store feature parameters of motion patterns, and the learning unit extracts feature parameters from an involuntary motion. Based on the functional units, an architecture for the automated gait synthesizer is presented. >	autonomous robot	Yuan F. Zheng	1990		10.1109/IROS.1990.262457	control engineering;mobile robot;central pattern generator;knowledge base;simulation;computer science;engineering;artificial intelligence	Robotics	50.397384837454226	-29.3750197828586	83664
1f8e414990b77fbb10f825d47cd46bba6072c710	vlsi design of 3d display processing chip for head-mounted display	microdisplays;three dimensional displays very large scale integration flat panel displays liquid crystal displays microdisplays signal processing virtual reality large screen displays sdram pipelines;interpolation;scale effect;soc chip;binocular stereo displays;virtual reality;pipeline architecture;hmd100b;fpga;vlsi design;system on a chip;high precision video scaling;chip;integrated circuit design;engines;system on chip;three dimensional displays;vlsi field programmable gate arrays integrated circuit design mixed analogue digital integrated circuits stereo image processing system on chip three dimensional displays;stereo image processing;mixed digital analog circuit;bicubic interpolation;vlsi;mixed analogue digital integrated circuits;soc head mounted display 3d display field rate up conversion video scaling;soc;glasses tv;stereo display image;analog stereo video signal;video scaling;field programmable gate arrays;hmd100b vlsi design 3d display processing chip head mounted display binocular stereo displays glasses tv pipeline architecture stereo display processing high precision video scaling bicubic interpolation field rate up conversion analog stereo video signal fpga stereo display image soc chip mixed digital analog circuit;3d display;hardware implementation;field rate up conversion;stereo display processing;hardware;3d display processing chip;head mounted display	In order to develop the core chip supporting binocular stereo displays for head-mounted display (HMD) and glasses-TV, a VLSI design scheme is proposed by using pipeline architecture for 3D display processing chip (HMD100B). Some key techniques including stereo display processing and high precision video scaling based on bicubic interpolation, and their hardware implementations are presented. A new method of field rate up-conversion for analog stereo video signal (CVBS) is proposed, and it can eliminate the large area of flicker. The proposed HMD100B chip is verified by FPGA, and the stereo display image is clear. As one of innovative and high integration SoC chip, HMD100B is designed by digital and analog mixed circuit. It can support binocular stereo display, has better scaling effect and integration. Hence it is applicable in virtual reality (VR), 3D games and other microdisplay domains.	bicubic interpolation;binocular vision;composite video;field-programmable gate array;flicker (screen);head-mounted display;image scaling;pipeline (computing);refresh rate;stereo display;system on a chip;very-large-scale integration;virtual reality	Chenyang Ge;Nanning Zheng;Kuizhi Mei;Jizhong Zhao	2009	2009 International IEEE Consumer Electronics Society's Games Innovations Conference	10.1109/ICEGIC.2009.5293596	computer vision;computer hardware;computer science;computer graphics (images)	Visualization	43.456682354174404	-33.73486113890375	84219
3af360b4c9b71fe394a5b8a77eb8b69c6dd14934	kinodynamic motion planning amidst moving obstacles	automatic control;directed graphs;kinodynamic motion planning;probability;aerodynamics;mobile robots kinodynamic motion planning obstacle avoidance kinematics dynamics probabilistic roadmap directed graph dynamic constraints moving obstacles;path planning;dynamic constraints;mobile robots;testing;orbital robotics;kinematics;obstacle avoidance;dynamics;directed graphs mobile robots robot kinematics robot dynamics path planning aerospace robotics probability;orbital robotics motion planning vehicle dynamics robotics and automation aerodynamics robots kinematics testing automatic control robotic assembly;directed graph;aerospace robotics;robots;motion planning;moving obstacles;on the fly;probabilistic roadmap;robotic assembly;robot dynamics;robotics and automation;vehicle dynamics;simulation environment;robot kinematics	This paper presents a randomized motion planner for kinodynamic asteroid avoidanceproblems, in which a robot must avoid collision with moving obstacles under kinematic, dynamic constraints and reach a specified goal state. Inspired by probabilistic-roadmap (PRM) techniques, the planner samples the state time space of a robot by picking control inputs at random in order to compute a roadmap that captures the connectivity of the space. However, the planner does not precompute a roadmap as most PRM planners do. Instead, for each planning query, it generates, on the fly, a small roadmap that connects the given initial and goal state. In contrast to PRM planners, the roadmap computed by our algorithm is a directed graph oriented along the time axis of the space. To verify the planner’s effectiveness in practice, we tested it both in simulated environments containing many moving obstacles and on a real robot under strict dynamic constraints. The efficiency of the planner makes it possible for a robot to respond to a changing environment without knowing the motion of moving obstacleswell in advance.	apache axis;collision detection;control system;directed graph;experiment;motion planning;on the fly;probabilistic roadmap;randomized algorithm;robot;robotics;sampling (signal processing);statistical relational learning;testbed	Robert Kindel;David Hsu;Jean-Claude Latombe;Stephen M. Rock	2000		10.1109/ROBOT.2000.844109	control engineering;computer vision;simulation;directed graph;aerodynamics;computer science;engineering;artificial intelligence;automatic control;motion planning;kinodynamic planning	Robotics	52.87975759696466	-24.064523431392548	84270
f35d195884fc06e65d02d6dd23fcb512e72444c0	hog feature extractor hardware accelerator for real-time pedestrian detection	histograms;hog real time pedestrian detection fpga hardware accelerator;feature extraction hardware histograms computer architecture real time systems support vector machines throughput;support vector machines;real time pedestrian detection;hog;computer architecture;support vector machines feature extraction object detection parallel architectures pedestrians;feature extraction;fpga hardware accelerator;object detection hog feature extractor hardware accelerator histogram of oriented gradients real time pedestrian detection parallel architecture memory access pattern support vector machine svm classifier;throughput;hardware;real time systems	Histogram of oriented gradients (HOG) is considered as the most promising algorithm in human detection, however its complexity and intensive computational load is an issue for real-time detection in embedded systems. This paper presents a hardware accelerator for HOG feature extractor to fulfill the requirements of real-time pedestrian detection in driver assistance systems. Parallel and deep pipelined hardware architecture with special defined memory access pattern is employed to improve the throughput while maintaining the accuracy of the original algorithm reasonably high. Adoption of efficient memory access pattern, which provides simultaneous access to the required memory area for different functional blocks, avoids repetitive calculation at different stages of computation, resulting in both higher throughput and lower power. It does not impose any further resource requirements with regard to memory utilization. Our presented hardware accelerator is capable of extracting HOG features for 60 fps (frame per second) of HDTV (1080x1920) frame and could be employed with several instances of support vector machine (SVM) classifier in order to provide multiple object detection.	algorithm;computation;embedded system;gradient;hardware acceleration;histogram of oriented gradients;memory access pattern;object detection;parallel computing;pedestrian detection;pipeline (computing);randomness extractor;real-time clock;real-time transcription;requirement;support vector machine;throughput	Maryam Hemmati;Morteza Biglari-Abhari;Stevan Berber;Smaïl Niar	2014	2014 17th Euromicro Conference on Digital System Design	10.1109/DSD.2014.60	embedded system;support vector machine;throughput;real-time computing;computer hardware;feature extraction;computer science;operating system;histogram	EDA	43.45558191240592	-36.65489579051696	84324
1b6e253e8a95a5f3bdffbe560283c4b95e34bdd1	position uncertainty reduction of mobile robot based on dinds in intelligent space	mobile robot;uncertainty;intelligent space;distributed sensors;tracking	This paper proposes a localization of mobile robot using the images by distributed intelligent networked devices (DINDs) in intelligent space (ISpace). This scheme combines data from the observed position using dead-reckoning sensors and the estimated position using images of moving object, such as those of a walking human, used to determine the moving location of a mobile robot. The moving object is assumed to be a point-object and projected onto an image plane to form a geometrical constraint equation that provides position data of the object based on the kinematics of the intelligent space. Using the a-priori known path of a moving object and a perspective camera model, the geometric constraint equations that represent the relation between image frame coordinates of a moving object and the estimated position of the robot are derived. The proposed approach is applied for a mobile robot in ISpace to show the reduction of uncertainty in the determining of the location of the mobile robot.	dead reckoning;image plane;mobile robot;object-based language;sensor	Tae-Seok Jin;Hideki Hashimoto	2006	2006 IEEE International Symposium on Industrial Electronics	10.20965/jaciii.2008.p0488	mobile robot;computer vision;uncertainty;computer science;tracking;statistics	Robotics	52.02704851313205	-35.881407672458124	84417
98773f086ff15e13c6046d91d1ffaf067454f367	deep neural network for real-time autonomous indoor navigation		Autonomous indoor navigation of Micro Aerial Vehicles (MAVs) possesses many challenges. One main reason is because GPS has limited precision in indoor environments. The additional fact that MAVs are not able to carry heavy weight or power consuming sensors, such as range finders, makes indoor autonomous navigation a challenging task. In this paper, we propose a practical system in which a quadcopter autonomously navigates indoors and finds a specific target, i.e. a book bag, by using a single camera. A deep learning model, Convolutional Neural Network (ConvNet), is used to learn a controller strategy that mimics an expert pilot’s choice of action. We show our system’s performance through real-time experiments in diverse indoor locations. To understand more about our trained network, we use several visualization techniques.	aerial photography;algorithmic efficiency;autonomous robot;convolutional neural network;deep learning;experiment;global positioning system;real-time clock;real-time web;scientific visualization;sensor	Dong Ki Kim;Tsuhan Chen	2015	CoRR		embedded system;computer vision;simulation	Robotics	52.66515299522434	-31.17290353233979	84515
5640bf5f10aec05d3a260f1320136b069fed0d15	efficient motion planning for manipulation robots in environments with deformable objects	high dimensionality;surgical simulation;cost function;mobile robot;path planning;gaussian process regression;computer model;training;trajectory robots planning deformable models computational modeling collision avoidance training;individual object;deformable models;deformable objects;configuration space;collision avoidance systems;computational modeling;real world application;trajectory;robots;motion planning;planning;collision avoidance;mobile manipulator;efficient estimation;deformable model;autonomous robot;physical simulation	The ability to plan their own motions and to reliably execute them is an important precondition for autonomous robots. In this paper, we consider the problem of planning the motion of a mobile manipulation robot in the presence of deformable objects. Our approach combines probabilistic roadmap planning with a physical deformation simulation system. Since the physical deformation simulation is computationally demanding, we use efficient Gaussian process regression to estimate the deformation cost for individual objects based on training examples. We generate the training data by employing a simulation system in a preprocessing step. Consequently, no simulations are needed during runtime. We implemented and tested our approach on a mobile manipulation robot. Our experiments show that the robot is able to accurately predict and thus consider the deformation cost its manipulator introduces to the environment during motion planning. Simultaneously, the computation time is substantially reduced compared to a system that employs physical simulations online.	autonomous robot;computation;dynamical simulation;experiment;gaussian process;kriging;mobile manipulator;motion planning;online and offline;precondition;preprocessor;probabilistic roadmap;run time (program lifecycle phase);simulation;time complexity	Barbara Frank;Cyrill Stachniss;Nichola Abdo;Wolfram Burgard	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6094946	computer simulation;control engineering;computer vision;simulation;computer science;artificial intelligence;motion planning	Robotics	50.7371079214847	-26.099066982064475	84560
45778adeaf29455ca4903e3bdfa2fc0edc0156cb	gpu-accelerated real-time stixel computation	dynamic programming;paper;image segmentation;computer vision;cuda;computational modeling;estimation;roads;package;nvidia tegra tx1;nvidia;optimization;computer science;nvidia geforce gtx titan x;real time systems	The Stixel World is a medium-level, compact representation of road scenes that abstracts millions of disparity pixels into hundreds or thousands of stixels. The goal of this work is to implement and evaluate a complete multistixel estimation pipeline on an embedded, energy-efficient, GPU-accelerated device. This work presents a full GPUaccelerated implementation of stixel estimation that produces reliable results at 26 frames per second (real-time) on the Tegra X1 for disparity images of 1024×440 pixels and stixel widths of 5 pixels, and achieves more than 400 frames per second on a high-end Titan X GPU card.	algorithm;binocular disparity;central processing unit;cluster analysis;computation;dynamic programming;embedded system;graphics processing unit;hybrid system;low-power broadcasting;mathematical optimization;parallel computing;pixel;real-time clock;real-time transcription;recurrent neural network;resources, events, agents (accounting model);tegra;titan rain;video processing	Daniel Hernández Juárez;Antonio Espinosa;David Vázquez;Antonio Manuel López Peña;Juan C. Moure	2017	2017 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2017.122	computer vision;estimation;parallel computing;computer science;theoretical computer science;dynamic programming;image segmentation;package;computational model;computer graphics (images)	Vision	43.12291692572483	-35.606834258377695	84641
d8446bfe25c0619c816c660c7c7f095082cc2f8d	the most efficient algorithm of high accuracy and speed method for surface modelling			algorithm	Tian-Xiang Yue;Chuan-Fa Chen;Bai-Lian Li	2010			mathematical optimization;fsa-red algorithm;computer science	Vision	45.791958341902564	-29.425491328257355	84977
7bef092bd51aaeef376821919bbebfa2680a313a	when wordhoard met pliny: breaking down of interaction silos between applications				John Bradley;Timothy Hill	2011			information silo;structural engineering;engineering	HCI	45.789341208077886	-28.388456646384608	85139
1451c8dc641e633bd17a4dbef07a0c4fc8c0c571	a biologically inspired neural net for trajectory formation and obstacle avoidance	optimal solution;shortest path;activity pattern;robot manipulator;autonomic system;article letter to editor;neural net;optimal path;obstacle avoidance;part of book or chapter of book;computer simulation;neural network	In this paper we present a biologically inspired two-layered neural network for trajectory formation and obstacle avoidance. The two topographically ordered neural maps consist of analog neurons having continuous dynamics. The first layer, the sensory map, receives sensory information and builds up an activity pattern which contains the optimal solution (i.e. shortest path without collisions) for any given set of current position, target positions and obstacle positions. Targets and obstacles are allowed to move, in which case the activity pattern in the sensory map will change accordingly. The time evolution of the neural activity in the second layer, the motor map, results in a moving cluster of activity, which can be interpreted as a population vector. Through the feedforward connections between the two layers, input of the sensory map directs the movement of the cluster along the optimal path from the current position of the cluster to the target position. The smooth trajectory is the result of the intrinsic dynamics of the network only. No supervisor is required. The output of the motor map can be used for direct control of an autonomous system in a cluttered environment or for control of the actuators of a biological limb or robot manipulator. The system is able to reach a target even in the presence of an external perturbation. Computer simulations of a point robot and a multi-joint manipulator illustrate the theory.	analog signal;artificial neural network;autonomous system (internet);biological neural networks;computer simulation;ephrin type-b receptor 1, human;feedforward neural network;inspiration function;map;numerous;obstacle avoidance;order (action);perturbation theory;population vector;short;shortest path problem;anatomical layer;collision	Roy Glasius;Andrzej Komoda;Stan C. A. M. Gielen	1996	Biological Cybernetics	10.1007/BF00209422	simulation;computer science;artificial intelligence;machine learning;control theory;obstacle avoidance;shortest path problem;artificial neural network	Robotics	50.75108225944407	-29.43636044568984	85222
babc2ae8da4432de46712423de5e37a6c5f35d2f	a region merging approach for image segmentation on fpga	image segmentation;parallel processing field programmable gate arrays image segmentation;merging image color analysis image segmentation hardware indexes real time systems field programmable gate arrays;pixel images region merging approach image segmentation fpga image processing mean shift k means algorithm hardware systems parallel processing pipelined processing xilinx xc4vlx160;field programmable gate arrays;parallel processing	Image segmentation is one of the most important tasks in the image processing, and many algorithms for the segmentation have been proposed. In the segmentation based on the mean-shift, k-means and so on, the image is once over-segmented, and then the small regions are merged. In this paper, we propose a region merging algorithm for hardware systems, in which the data are not managed strictly, and the redundant computation caused by this loose management is hidden by the pipelined and parallel processing. We have implemented the algorithm on Xilinx XC4VLX160, and its performance is about 80 fps for 768 × 512 pixel images.	algorithm;computation;field-programmable gate array;image processing;image segmentation;k-means clustering;mean shift;parallel computing;pixel	Dang Ba Khac Trieu;Tsutomu Maruyama	2012	22nd International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2012.6339166	embedded system;parallel processing;computer vision;feature detection;binary image;image processing;computer science;theoretical computer science;segmentation-based object categorization;digital image processing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;field-programmable gate array;computer graphics (images)	Robotics	42.36178195085229	-34.62971887321465	85344
e6a7c35c44315cdf5b53bf7ae1429e4a5a0aae26	mobile robot localization using multiple observations based on place recognition and gps	robust mobile robot localization system image matching park trees lawn grass gyrodometry model dead reckoning natural environments global position information scene database robot global position recognition appearance based place recognition gps observation;cameras global positioning system mobile robots detectors wheels image recognition;image matching;mobile robots;global positioning system;natural scenes;natural scenes global positioning system image matching mobile robots	In this paper, we propose a mobile robot localization system using multiple observations, which show the robot's global position. One of observations is GPS observation, the other is utilized an appearance based place recognition. Using GPS observations has still some challenging problems such as multipath and signal lost under the environments there is tall buildings nearby. It affects a significant error on localization. On the other hand, appearance based place recognition methods are efficient to recognize the robot's global position. It becomes possible to use a scene database with global position information. However, it could fail to function properly in natural environments like a lawn grass or trees in a park. We solve these demerits of each observations by using these multiple observations. Our system uses not only multiple observations but also dead reckoning with Gyrodometry model. As a result, the proposed localization system have achieved robust localization. To verify the validity of proposed method, our experiments using 1600m outdoor course in different seasons were conducted.	dead reckoning;experiment;global positioning system;mobile robot;multipath propagation;robotic mapping	Takato Saito;Yoji Kuroda	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630776	mobile robot;computer vision;simulation;global positioning system;computer science;artificial intelligence	Robotics	50.87652094114078	-37.39324626765109	85611
1d47fc410678fccc0b76d28901ede7fed9199eb6	learning to automatically detect features for mobile robots using second-order hidden markov models	hmm;mobile robots;hidden markov models;robot mobile;interpretation de donnees;sensor data interpretation	In this paper, we propose a new method based on Hidden Markov Models to interpret temporal sequences of sensor data from mobile robots to automatically detect features. Hidden Markov Models have been used for a long time in pattern recognition, especially in speech recognition. Their main advantages over other methods (such as neural networks) are their ability to model noisy temporal signals of variable length. We show in this paper that this approach is well suited for interpretation of temporal sequences of mobile-robot sensor data. We present two distinct experiments and results: the first one in an indoor environment where a mobile robot learns to detect features like open doors or T-intersections, the second one in an outdoor environment where a different mobile robot has to identify situations like climbing a hill or crossing a rock.	artificial neural network;experiment;hidden markov model;markov chain;mobile robot;pattern recognition;speech recognition	Olivier Aycard;Jean-François Mari;Richard Washington	2003	CoRR		mobile robot;computer vision;simulation;computer science;machine learning;hidden markov model	ML	48.02629256413857	-36.25288188040308	86008
7883dc54f1d6b519a544cafa934ec5e53498857f	multi-target detection by multi-sensor systems: a comparison of systems	optimal target detection;sensor system;path planning;target tracking image sensors mobile robots path planning robot vision sensor fusion;object detection target tracking statistics robot sensing systems robot kinematics motion detection sensor phenomena and characterization navigation surveillance biomimetics;mobile robots;image sensors;multitarget detection;target detection multi sensor navigation;optimal target detection multitarget detection multisensor systems navigation scenario;robot vision;multi sensor navigation;multisensor systems;navigation scenario;sensor fusion;target tracking;target detection	Different methodologies exist to direct the motion of sensors to detect targets moving across an environment in various scenarios. However, some of these do not model that navigation scenario in an environment in which obstacles are present. We extend our earlier algorithm for optimal target detection, as well as other algorithms reported in literature, to this case, and make a detailed comparison of their performance. This makes clear that the current algorithm is competitive in applications where target statistics are known in advance; otherwise, a heuristic technique by Sukhatme and Jung performs best.	algorithm;heuristic;jung;sensor	P. K. Ganesh;K. Madhava Krishna;Paulo Menezes	2006	2006 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2006.340243	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;image sensor;motion planning;sensor fusion;remote sensing	Robotics	52.4540743779056	-34.28897879748847	86127
83c77c32877172961bab43aa5d15b0b1ec6bf8f0	building 3d visual maps of interior space with a new hierarchical sensor fusion architecture	environment maps;hierarchical map building;range data;map building;3d visualization;mobile robot;top down;data collection;slam;data fusion;robotics building 3d visual maps of interior space with a new hierarchical sensor fusion architecture purdue university avinash c kak kwon;hyukseong;data abstraction;dense visual map;sensor fusion	It is now generally recognized that sensor-fusion is the best approach to the accurate construction of environmentmaps by a sensor-equippedmobile robot. Typically, range data collectedwith a range sensor is combined with the reflectance data obtained from one or more cameras mounted on the robot. In much of the past work on sensor fusion in hierarchical approaches to map construction, the fusion was carried out only at the lowest level of the hierarchy. As a result, in those approaches, only the fused datawasmade available to the higher levels in the hierarchy. This implied that any errors caused by sensor fusion would propagate upwards into the higher level representations of an interior map. Our work, on the other hand, checks for consistency between the data elements produced by the different sensors at all levels of the hierarchy. This consistency checking is carried out with the help of an interval-based representation of uncertainties in the sensor data. In addition to demonstrating that our approach to the fusion of range and image data results in dense 3D maps of the interior space, we also provide validation of our overall framework by presenting a set of loop closure results. These results demonstrate that our overall errors in the maps remain small (within 0.91% of the distance traveled for map construction) even when the robot has to traverse over large loops inside a building. © 2013 Elsevier B.V. All rights reserved.	approximation algorithm;cycle detection;data element;for loop;map;planar graph;principle of abstraction;robot;robotic mapping;strips;sensor web;traverse	Hyukseong Kwon;Khalil Mustafa Ahmad Yousef;Avinash C. Kak	2013	Robotics and Autonomous Systems	10.1016/j.robot.2013.04.016	computer vision;simulation;computer science;machine learning;sensor fusion	Robotics	52.849743723032105	-32.44793545686088	86235
2cee2b8407e8b74ee86c2623316bfd515f08e96a	value driven strategic knowledge management: a research agenda for an alternative perspective of km	knowledge management	The present invention extends the available spatial frequency content of an image through the use of a method and apparatus for combining nonlinear functions of intensity to form three dimensional patterns with spatial frequencies that are not present in either of the individual exposures and that are beyond 2/ lambda in all three spatial directions. The resulting pattern has spatial frequency content beyond the limits set by optical propagation of spatial frequencies limited to 2/ lambda (e.g. pitch reduction from DIFFERENCE lambda /2 to at least DIFFERENCE lambda /4). The extension of spatial frequencies preferably extends the use of currently existing photolithography capabilities, thereby resulting in a significant economic impact. Multiplying the spatial frequency of lithographically defined structures suitably allows for substantial improvements in, inter alia, crystal growth, quantum structure growth and fabrication, flux pinning sites for high-Tc superconductors, form birefringent materials, reflective optical coatings, photonic bandgap, electronics, optical/magnetic storage media, arrays of field emitters, DRAM (Dynamic Random Access Memory) capacitors and in other applications requiring large areas of nm-scale features.	knowledge management	Hassan Makhmali;Babak Akhgar;Majid Banaeioskoei	2009			photolithography;capacitor;photonics;dynamic random-access memory;magnetic storage;lambda;spatial frequency;dram;knowledge management;political science	AI	44.09676377081125	-25.210787516520043	86249
155456e657d13ce210109a5f19d5d0611d8b4d0e	pursuit-evasion in 2.5d based on team-visibility	graph search;robot sensing systems;appropriate classification;multirobot system;robotics;team visibility;graph searching;pursuit evasion;large scale;sampling strategic location;image edge detection;graph representation;multi robot systems;robotteknik och automation;mathematical model;robot sensing systems robot kinematics mathematical model image edge detection equations;multirobot system pursuit evasion team visibility graph representation sampling strategic location graph searching robot team appropriate classification;robot kinematics;robot team;target height	In this paper we present an approach for a pursuit-evasion problem that considers a 2.5d environment represented by a height map. Such a representation is particularly suitable for large-scale outdoor pursuit-evasion, captures some aspects of 3d visibility and can include target heights. In our approach we construct a graph representation of the environment by sampling strategic locations and computing their detection sets, an extended notion of visibility. From the graph we compute strategies using previous work on graph-searching. These strategies are used to coordinate the robot team and to generate paths for all robots using an appropriate classification of the terrain. In experiments we investigate the performance of our approach and provide examples including a sample map with multiple loops and elevation plateaus and two realistic maps, a village and a mountain range. To the best of our knowledge the presented approach is the first viable solution to 2.5d pursuit-evasion with height maps.	evasion (network security);experiment;graph (abstract data type);heightmap;map;pursuit-evasion;robot;sampling (signal processing)	Andreas Kolling;Alexander Kleiner;Michael Lewis;Katia P. Sycara	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5649270	computer vision;simulation;computer science;artificial intelligence;machine learning;mathematical model;graph;robotics;robot kinematics	Robotics	53.477983075197415	-30.697576606153905	86252
138c6b978dc5f4310b124a2ad4b1a057667640c0	place recognition and self-localization in interior hallways by indoor mobile robots: a signature-based cascaded filtering framework		We present a robot self-localization approach that is based on using a cascade of filters that increasingly refine a robot's guess regarding where it is in a hallway system. The location refinement carried out by each stage of the cascade compares a signature extracted from a stereo pair of camera images taken at the current location of the robot with a database of such signatures collected previously during a training phase. A central question in this approach to robot localization is what signatures to use for each stage of the cascade. An answer to this question must recognize the special importance of the first stage of the cascade - we refer to this as the prefiltering stage. The signature used for prefiltering must be significantly viewpoint invariant, while possessing sufficient locale uniqueness to yield a set of possible locations for the robot that includes the true location with a high probability. On the other hand, the signature(s) used for downstream filtering in the cascade must then prune away the inapplicable locales from the list yielded by the prefilter. What that implies is that the downstream filters must be increasingly viewpoint variant and locale specific. Although the framework we propose allows for an arbitrary number of filters to follow the prefiltering stage, the results we present in this paper are for a two-stage cascade consisting of a prefilter followed by one additional filter. The signatures we use in our experiments are based on 3D-JUDOCA features that can be extracted from stereo pairs of images. The proposed framework for choosing the best signatures for the prefiltering stage and the filtering stage that follows was tested in a large indoor hallway system with a total linear length of 1539 m. The validation results we show are based on a dataset of 6209 stereo images collected by a robot from the hallways during its training phase. The performance evaluation presented in this paper demonstrates that our framework can lead to high localization accuracy with good time performance by a robot.	3d computer graphics;3d film;antivirus software;downstream (software development);electronic signature;experiment;fo (complexity);hamming distance;mobile robot;multiple encryption;performance evaluation;random sample consensus;randomness;refinement (computing);robotic mapping;speeded up robust features;type signature;window function	Khalil Mustafa Ahmad Yousef;Johnny Park;Avinash C. Kak	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6943271	computer vision;simulation	Robotics	49.08158673602464	-38.010211384125746	86357
3527cac9373f1717939eef260986c4be319c4448	detection of unknown moving objects by reciprocation of observed information between mobile robot	image sensors mobile robots object detection computerised navigation navigation;moving object;mobile robot;real time;azimuth changes unknown moving object detection observed information reciprocation mobile robot multiple mobile robot navigation location estimation motion estimation conic projection image sensor copis omnidirectional view conic mirror reciprocating robot cooperative observation;mobile robots;image sensors;navigation;multiple mobile robots;object detection mobile robots motion estimation robot sensing systems azimuth mirrors navigation cameras image sensors yagi uda antennas;image sensor;object detection;computerised navigation	Described here is a method for navigating multiple mobile robots which estimate locations and motions of unknown moving objects. Each robot with conic projection image sensor COPIS can observe an omnidirectional view around the robot in real-time with use of a conic mirror. First, each robot communicates and exchanges information of sensory data. Then, unknown moving objects and a reciprocating robot are discriminated, and motions and locations of unknown moving objects are estimated by cooperative observation of azimuth changes between the two reciprocating robots. >	mobile robot;observed information	Yasushi Yagi;Ya Lin;Masahiko Yachida	1994		10.1109/IROS.1994.407481	mobile robot;computer vision;simulation;computer science;artificial intelligence;image sensor;mobile robot navigation;remote sensing	Robotics	51.82868884042843	-35.48731331345826	86447
7db3dd74e5611e7b8c66d5b7d9094f769a58c5c5	hw/sw co-design of an embedded omni-imaging system	software;digital signal processing;image coding;hardware software codesign;hardware software co design;image resolution;hardware software field programmable gate arrays digital signal processing image coding image color analysis imaging;omni directional imaging;omnidirectional image unwarping hw sw codesign embedded omniimaging system hardware software codesign method real time high definition embedded system design hardware software partitioning functional modules field programmable gate array fpga plus dsp system architecture digital signal processor imaging resolution;会议论文;omni directional image unwarping;embedded system;omni directional image unwarping omni directional imaging hardware software co design embedded system;embedded systems;image color analysis;imaging;digital signal processing chips;field programmable gate arrays;image resolution digital signal processing chips embedded systems field programmable gate arrays hardware software codesign;hardware	Omni-imaging can be used in many practical applications that need a wide field of view, therefore a real-time and high-definition embedded system design and implementation of omni-imaging is desired. In this study, we propose a hardware/software co-design method for the design and implementation of embedded omni-imaging systems. In order to achieve real-time and high-definition goals, we perform hardware/software partitioning based on the analysis of functional modules in a basic embedded omni-imaging system. In the experiments, the proposed hardware/software co-design omni-imaging system is implemented in a FPGA (Field Programmable Gate Array) plus DSP (Digital Signal Processor) system architecture. Results indicate that the omni-imaging speed achieved is 39fps with the imaging resolution set at 1024×768 for the original omni-image and 1280×288 for the unwarped image.	digital signal processor;display resolution;embedded system;experiment;field-programmable gate array;hdmi;real-time clock;shattered world;systems architecture;systems design	Zhihui Xiong;L. Irene Cheng;Maojun Zhang;Anup Basu	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6378314	medical imaging;embedded system;real-time computing;image resolution;embedded software;computer science;digital signal processing;hardware architecture;field-programmable gate array	EDA	43.82008050869657	-34.39048446710623	86531
98592b6012c7b916e08be549229995ae069550f6	a submodularity-based approach for multi-agent optimal coverage problems		We consider the optimal coverage problem where a multi-agent network is deployed in an environment with obstacles to maximize a joint event detection probability. We first show that the objective function is monotone submodular, a class of functions for which a simple greedy algorithm is known to be within 1-1/e of the optimal solution. We then derive two tighter lower bounds by exploiting the curvature information of the objective function. We further show that the tightness of these lower bounds is complementary with respect to the sensing capabilities of the agents. Simulation results show that this approach leads to significantly better performance relative to previously used algorithms.	distributed shared memory;elemental;feasible region;global optimization;gradient descent;greedy algorithm;initial condition;loss function;maximal set;multi-agent system;optimization problem;simulation;submodular set function;time complexity;monotone	Xinmiao Sun;Christos G. Cassandras;Xiangyu Meng	2017	2017 IEEE 56th Annual Conference on Decision and Control (CDC)	10.1109/CDC.2017.8264258	monotone polygon;computer science;mathematical optimization;submodular set function;curvature;multi-agent system;linear programming;space exploration;greedy algorithm	Robotics	51.2618541654964	-25.575350053684403	86747
e9fb523c52610bb530fbae7fac956c7ce6ca7509	improved disparity map computation on stereoscopic streaming video with multi-core parallel implementation	shared memory;stereoscopic 3d;disparity map;sub wordparallelism;parallel processing	Stereo vision has become an important technical issue in the field of 3D imaging, machine vision, robotics, image analysis, and so on. The depth map extraction from stereo video is a key technology of stereoscopic 3D video requiring stereo correspondence algorithms. This is the matching process of the similarity measure for each disparity value, followed by an aggregation and optimization step. Since it requires a lot of computational power, there are significant speed-performance advantages when exploiting parallel processing available on processors. In this situation, multi-core CPU may allow many parallel programming technologies to be realized in users computing devices. This paper proposes parallel implementations for calculating disparity map using a shared memory programming and exploiting the streaming SIMD extension technology. By doing so, we can take advantage both of the hardware and software features of multi-core processor. For the performance evaluation, we implemented a parallel SAD algorithm with OpenMP and SSE2. Their processing speeds are compared with non parallel version on stereoscopic streaming video. The experimental results show that both technologies have a significant effect on the performance and achieve great improvements on processing speed.	binocular disparity;computation;multi-core processor;stereoscopy;streaming media	Cheong-Ghil Kim;Yong Soo Choi	2015	TIIS	10.3837/tiis.2015.02.014	shared memory;stereoscopy;parallel processing;computer vision;computer science;theoretical computer science;operating system;computer graphics (images)	HPC	43.43103826512869	-35.90737332182049	87248
0640bc2468a32535220cff602c2728f081fdd2ef	surgical skill evaluation by force data for endoscopic sinus surgery training system	endoscopic sinus surgery;quantitative evaluation;force sensor	In most surgical training systems, task completion time and error ratio are common metrics of surgical skill. To avoid applying unnecessary and injurious force to the tissue, surgeons must know for themselves how much force they are exerting as they handle surgical tools. Our goal is to develop an endoscopic sinus surgery (ESS) training system that quantitatively evaluates the trainee's surgical skills. In this paper, we present an ESS training system with force sensors for surgical skill evaluation. Our experiment revealed that the integral of the force data can also be one of the useful metrics of surgical skill.	bit error rate;dummy variable (statistics);experiment;sensor	Yasushi Yamauchi;Juli Yamashita;Osamu Morikawa;Ryoichi Hashimoto;Masaaki Mochimaru;Yukio Fukui;Hiroshi Uno;Kazunori Yokoyama	2002		10.1007/3-540-45786-0_6	medicine;pathology;surgery;force-sensing resistor	Robotics	39.936137933353685	-37.25221030766653	87919
797230c284a24ae7b3c9781f8188a59345b90838	improving robot reactivity to passers-by with a faster people detector		Perceptual systems deployed in embedded intelligent systems are handicapped with limited computation resources. As a result, they require explicit computation time consideration during design time. In this paper, we investigate the impact a visual person detector explicitly optimized to minimize computational time has on an exemplar robotic application - navigation in human shared environments. The experimental results attest the improved perceptual frame rate leads to significant improvements in vision system performance and robotic mission success rate.	computation;embedded system;robot;time complexity	Alhayat Ali Mekonnen;Frédéric Lerasle;Ariane Herbulot	2016	2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)		robot;mobile robot;computer vision;detector;navigation;simulation;visualization;computer science;artificial intelligence;robot control;mobile robot navigation	Robotics	46.89985409819616	-33.88332526326928	88062
426d8b42c4eb2c91671677da2db98fbd0ba54c3d	real-time topometric localization	databases;kidnapped robot problem real time topometric localization autonomous vehicles gps denied situations vehicle localization visual imagery range information metric method geometric accuracy route navigation gps equipped vehicle visual features 3d features bayesian filter;measurement;seasonal variation;autonomous vehicle;path planning;bayes methods;probability density function;real time;vehicles feature extraction global positioning system databases visualization measurement probability density function;geometry;global position system;mobile robots;visualization;robot vision;robot vision bayes methods feature extraction geometry global positioning system mobile robots path planning;global positioning system;feature extraction;position estimation;bayesian filtering;vehicles;environmental change	Autonomous vehicles must be capable of localizing even in GPS denied situations. In this paper, we propose a real-time method to localize a vehicle along a route using visual imagery or range information. Our approach is an implementation of topometric localization, which combines the robustness of topological localization with the geometric accuracy of metric methods. We construct a map by navigating the route using a GPS-equipped vehicle and building a compact database of simple visual and 3D features. We then localize using a Bayesian filter to match sequences of visual or range measurements to the database. The algorithm is reliable across wide environmental changes, including lighting differences, seasonal variations, and occlusions, achieving an average localization accuracy of 1 m over an 8 km route. The method converges correctly even with wrong initial position estimates solving the kidnapped robot problem.	algorithm;bayesian network;global positioning system;image resolution;information theory;internationalization and localization;kidnapped robot problem;real-time clock;real-time computing;real-time transcription;vii	Hernán Badino;Daniel F. Huber;Takeo Kanade	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224716	mobile robot;computer vision;probability density function;simulation;visualization;environmental change;global positioning system;feature extraction;computer science;machine learning;motion planning;seasonality;measurement;statistics	Robotics	52.64049038084123	-36.18401971743179	88349
0292fd59e206b8f6dc58b7a183601f283e313683	characterization of a 2-d laser scanner for mobile robot obstacle negotiation	obstacle avoidance sick lms 200 2d laser scanner data transfer rate target surface incidence angle range measurement model mobile robot 3d map building;mobile robots laser radar laser modes laser noise least squares approximation laser theory lasers and electrooptics millimeter wave radar image sensors stereo vision;mobile robot;laser scanner;mobile robots;laser ranging;navigation;collision avoidance;collision avoidance mobile robots laser ranging navigation;data transfer	This paper presents a characterization study of the Sick LMS 200 laser scanner. A number of parameters, such as operation time, data transfer rate, target surface properties, as well as the incidence angle, which may potentially affect the sensing performance, are investigated. A probabilistic range measurement model is built based on the experimental results. The paper also analyzes the mixed pixels problem of the scanner.	incidence matrix;mobile robot;operation time;pixel	Cang Ye;Johann Borenstein	2002		10.1109/ROBOT.2002.1013609	mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;optics	Robotics	53.43734542245487	-35.13276788168493	88355
2117afee7c5b4d84374c2680e24b7faacb142597	multi level modeling and loop closing with gridtiles	solid modelling driver information systems;data storage multilevel modeling loop closing gridtile algorithm occupancy grids human driven car neighbourhood relation local mapping global mapping;tiles merging memory management vehicles trajectory robot sensing systems;loop closing driver assistance systems multi level modeling occupancy grid efficient memory management;driver information systems;solid modelling	The objective of this paper is to use GridTiles to model a complex multi level scenario and additionally use the resulting model to help solving the loop closing problem. The GridTile algorithm is a memory and computational efficent method to map an unbounded outdoor environment of a human driven car. GridTiles are a set of small occupancy grids with a neighbourhood relation in the four directions. This paper discusses four of the major problems in modelling: the local and the global mapping, multi level modelling and loop closing. For each of these problems possible solutions will be shown. Local and global maps can be directly modelled with the standard GridTile algorithm. Because the neighbourhood of the tiles is only known locally, multi level scenarios can be modelled implicitly. Additionally a loop closing solution is proposed to correct the built model afterwards in a memory efficient way. Instead of the classic approach that has to store all measurements over an unpredictable duration, the GridTiles are used as a data storage.	algorithm;closing (morphology);computation;computer data storage;map;memory management;multi-storey car park;reflection mapping;xfig	Christian Heigele;Holger Mielenz;Joerg Heckel;Dieter Schramm	2013	2013 18th International Conference on Methods & Models in Automation & Robotics (MMAR)	10.1109/MMAR.2013.6670002	control engineering;real-time computing;simulation;engineering	Robotics	52.564727680568254	-31.310755890357406	88511
78720e0d9028f8f47049aced6e6d859900cba3d8	a multiple-predictor approach to human motion prediction		The ability to accurately predict human motion is imperative for any human-robot interaction application in which the human and robot interact in close proximity to one another. Although a variety of human motion prediction approaches have already been developed, they are often designed for specific types of tasks or motions, and thus do not generalize well. Furthermore, it is not always obvious which of these methods is appropriate for a given task, making human motion prediction difficult to implement in practice. We address this problem by introducing a multiple-predictor system (MPS) for human motion prediction. In our approach, the system learns directly from task data in order to determine the most favorable parameters for each implemented prediction method and which combination of these predictors to use. Our implementation consists of three complementary methods: velocity-based position projection, time series classification, and sequence prediction. We describe the process of forming the MPS and our evaluation of its performance against the individual methods in terms of accuracy of predictions of human position over a range of look-ahead time values. We report that our method leads to a reduction in mean error of 18.5%, 28.9%, and 37.3% when compared with the three individual methods, respectively.	algorithm;biasing;human–robot interaction;imperative programming;kerrison predictor;kinesiology;model selection;time series;velocity (software development)	Przemyslaw A. Lasota;Julie A. Shah	2017	2017 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2017.7989265	time series;hidden markov model;machine learning;trajectory;mean squared error;computer science;artificial intelligence	Robotics	48.918207439964284	-28.371866792142093	88572
33b715e3b19f863cdbc72104aef83aa623cbce23	algorithm for sensor network attitude problem		Sensor network attitude problem consists in retrieving the attitude of each sensor of a network knowing some relative orientations between pairs of sensors. The attitude of a sensor is its orientation in an absolute axis system. We present in this paper a method for solving the sensor network attitude problem using quaternion formalism which allows to apply linear algebra tools. The proposed algorithm solves the problem when all of the relative attitudes are known. A complete characterisation of the algorithm is established: spatial complexity, time complexity and robustness. Our algorithm is validated in simulations and with real experiments.	algorithm;apache axis;experiment;linear algebra;semantics (computer science);sensor;simulation;time complexity	Mikael Carmona;Olivier J. J. Michel;Jean-Louis Lacoume;Nathalie Sprynski;Barbara Nicolas	2011	CoRR		mathematical optimization;simulation;brooks–iyengar algorithm;control theory;mathematics	Robotics	51.9172518084119	-35.0273862655982	88639
b4ef832c7a03d534e3b36426f853362e58934021	the edge flag algorithm — a fill method for raster scan displays	frame store;computer graphics;raster scan animation filled colored shapes frame store graphics polygon fill;animation;filled colored shapes;graphics;raster scan;polygon fill;filling shape parity check codes image color analysis memory management computers hardware;polygon fill edge flag algorithm fill method raster scan displays bit map memory frame store display scratch pad working space contour filling algorithms read write properties microcomputer controlled frame store display system execution speed processor memory requirements cpu memory computer graphics	Contour (polygon) filling is a primitive required in many application areas of raster scan graphics. The bit-map memory in a frame-store display is computationally well suited to this task, as it provides a large scratch pad working space. In this paper, a number of contour filling algorithms based on the read/write properties of the frame-store memory are compared with the classical `ordered-edge-list' approach. Performance is evaluated on a microcomputer controlled frame-store display system in terms of ability to fill correctly, execution speed and processor memory requirements. A new algorithm, based on a more exact definition of an object edge, is presented. This algorithm, denoted edge flag algorithm, is implemented within the frame-store memory. It features high speed, in conjunction with minimal CPU memory requirements, making it ideally suited to hardware or microcode (firmware) implementation.	algorithm;bitmap;central processing unit;contour line;firmware;frame language;graphics;microcode;microcomputer;raster scan;requirement;scratch (programming language);workspace	Bryan D. Ackland;Neil Weste	1981	IEEE Transactions on Computers	10.1109/TC.1981.6312155	raster scan;anime;embedded system;parallel computing;raster graphics;scan line;computer hardware;computer science;graphics;operating system;computer graphics;computer graphics (images)	Visualization	44.24532730128194	-32.15871479447625	88897
f3f4db39d92996662fbb6659edbd6189fbbd9b5f	using general-purpose graphic processing units for bci systems	microelectrodes;high density;microelectrode system general purpose graphic processing units bci system biomems electrode array fabrication technique online bci experiment computational resource restraints gpu bci processing eeg ecog;real time;array signal processing;microfabrication;arrays;action potentials brain humans man machine systems signal processing computer assisted;graphics processing units;graphic processing unit;graphics processing unit real time systems electroencephalography central processing unit instruction sets arrays signal processing algorithms;biomedical electrodes;brain computer interfaces;electroencephalography;signal processing algorithms;graphics processing unit;microfabrication biomedical electrodes biomems brain computer interfaces electroencephalography graphics processing units medical signal processing microelectrodes;medical signal processing;central processing unit;biomems;instruction sets;real time systems	BioMEMS electrode array fabrication techniques are used to develop high-density arrays with hundreds of channels. However, it was previously impossible to process more than a fraction of these channels real-time for online BCI experiments due to computational resource restraints. It is now possible to use graphics processing units (GPUs), which can have several hundred processing cores each, to processes large amounts of data quickly. This paper summarizes advances in using GPUs for BCI processing for EEG, ECoG, and micro-electrode systems, with speedups of more than 30 times that of current state-of-the-art CPU-based BCI implementations.	biomems;brain–computer interface;cpu (central processing unit of computer system);cuda;central processing unit;computation (action);computational resource;computer graphics;electrocorticogram;electrocorticography;electroencephalography;experiment;field-programmable gate array;general-purpose computing on graphics processing units;general-purpose modeling;graphics processing unit;greater than;interface device component;one thousand;opencl api;physical restraint equipment (device);programmer;real-time clock;requirement;scalability	J. Adam Wilson	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091145	embedded system;brain–computer interface;microfabrication;neuroscience;electroencephalography;computer hardware;microelectrode;computer science;central processing unit;instruction set;bio-mems;computer graphics (images)	EDA	41.6283533901919	-26.335641760414788	89974
cf2d31e4ba02f8bbaecdfc62b674f17d7d5c1a43	an implementation of the neuro-fuzzy inference circuit	real time learning application;fuzzy neural nets;neuro fuzzy inference circuit;fuzzy reasoning;high speed tuning;real time;fpga;membership functions neuro fuzzy inference circuit real time learning application fpga high speed tuning inference rules parallel processing;neural chips circuit tuning fuzzy neural nets fuzzy reasoning learning artificial intelligence;inference rule;inference algorithms fuzzy systems fuzzy reasoning field programmable gate arrays circuit optimization parallel processing fuzzy logic industrial control financial management neural networks;neural chips;membership functions;fuzzy inference;membership function;circuit tuning;learning artificial intelligence;high speed;parallel processing;inference rules	In this paper, we propose a neuro-fuzzy inference circuit suitable for real-time learning application. We could confirm through our experimental use of the FPGA that the circuit can realize a high speed tuning of inference rules first by doing the parallel processing of the operations and then by fixing the membership functions of the antecedent part	field-programmable gate array;neuro-fuzzy;parallel computing;real-time clock	Kuniaki Fujimoto;Hirofumi Sasaki;Yoichiro Masaoka;Ren-Qi Yang;Masaharu Mizumoto	2006	First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)	10.1109/ICICIC.2006.232	parallel processing;adaptive neuro fuzzy inference system;computer science;artificial intelligence;machine learning;data mining;rule of inference	EDA	40.88905802328802	-24.714836647650625	90116
5d91f2eadd9fc0d2e416125b538744e4246e5161	feature detection and identification using a sonar-array	object recognition;feature detection;path planning mobile robots sonar arrays navigation sensor fusion feature extraction object recognition;path planning;sonar arrays;mobile robots;navigation;feature extraction;indoor environment;multiple hypothesis testing;computer vision sonar detection sonar navigation sensor fusion robot sensing systems testing data mining uncertainty surface acoustic waves indoor environments;sensor fusion;object recognition feature detection sonar array sensor fusion navigation indoor environments mobile robot feature localization feature extraction	This work explores techniques for sonar sensor fusion in the context of environmental feature detection and identification for navigation tasks. By detecting common features in indoor environments and using them as landmarks, a robot can navigate reliably, recovering its pose when necessary. Preliminary results on a multiple hypothesis testing procedure for feature localization and identification show that accurate feature information can be acquired with adequate sonar models and configurations. In addition, a method that associates sonar configuration with the precision of feature extraction is discussed, as well as its utility for guiding an active sonar sensor.	feature detection (computer vision);feature detection (web development);feature extraction;landmark point;mobile robot;robot;sonar (symantec);sensor web;transducer	Elizeth Araujo;Roderic A. Grupen	1998		10.1109/ROBOT.1998.677370	mobile robot;computer vision;navigation;feature extraction;computer science;engineering;cognitive neuroscience of visual object recognition;feature detection;motion planning;sensor fusion;multiple comparisons problem;mobile robot navigation;remote sensing	Robotics	52.82903644457008	-34.595093704466684	90188
f7fb6da0a37ebf5043f0a17e8a7fe7f46e86ca0a	learning navigation behaviors end to end		A longstanding goal of behavior-based robotics is to solve high-level navigation tasks using end to end navigation behaviors that directly map sensors to actions. Navigation behaviors, such as reaching a goal or following a path without collisions, can be learned from exploration and interaction with the environment, but are constrained by the type and quality of a robot’s sensors, dynamics, and actuators. Traditional motion planning handles varied robot geometry and dynamics, but typically assumes high-quality observations. Modern vision-based navigation typically considers imperfect or partial observations, but simplifies the robot action space. With both approaches, the transition from simulation to reality can be difficult. Here, we learn two end to end navigation behaviors that avoid moving obstacles: point to point and path following. These policies receive noisy lidar observations and output robot linear and angular velocities. We train these policies in small, static environments with Shaped-DDPG, an adaptation of the Deep Deterministic Policy Gradient (DDPG) reinforcement learning method which optimizes reward and network architecture. Over 500 meters of on-robot experiments show , these policies generalize to new environments and moving obstacles, are robust to sensor, actuator, and localization noise, and can serve as robust building blocks for larger navigation tasks. The path following and point and point policies are 83% and 56% more successful than the baseline, respectively.	algorithm;analysis of algorithms;angularjs;australian privacy foundation;baseline (configuration management);behavior-based robotics;experiment;gradient;high- and low-level;motion planning;network architecture;pf (firewall);peer-to-peer;reinforcement learning;robot;sensor;simulation	Hao-Tien Lewis Chiang;Aleksandra Faust;Marek Fiser;Anthony Francis	2018	CoRR		engineering;simulation;point-to-point;network architecture;motion planning;reinforcement learning;robot;actuator;end-to-end principle;artificial intelligence;robotics	Robotics	51.142999197804976	-28.05381228354458	90975
1945ad13defb9de724ff4572cd3c45c11c9b9816	topological modeling and classification in home environment using sonar gridmap	graph theory;robot sensing systems;sonar applications;data mining robot sensing systems graphical models sonar navigation mobile robots simultaneous localization and mapping mesh generation robotics and automation sonar applications human robot interaction;sensors;rotational invariant matching home environment sonar gridmap topological representation topological classification sonar sensors approximate cell decomposition normalized graph cut graphical model template matching method local gridmap;approximate cell decomposition;path planning;sonar sensors;rotational invariant matching;mobile robots;human robot interaction;normalized graph cut;data mining;sonar gridmap;sonar graph theory home automation mobile robots path planning robot vision sensors;distance measurement;rotation invariance;graphical models;robot vision;home environment;spatial relation;image edge detection;graph cut;local gridmap;simultaneous localization and mapping;template matching method;graphical model;sonar navigation;mesh generation;template matching;topological classification;robotics and automation;environmental modeling;home automation;topological representation;sonar	This paper presents a method of topological representation and classification in home environment using only low-cost sonar sensors. Approximate cell decomposition and normalized graph cut are applied to sonar gridmap to extract graphical model of the environment. The extracted model represents spatial relation of the environment appropriately by segmenting several subregions. Moreover, node classification is achieved by applying template matching method to a local gridmap. Rotational invariant matching is used to obtain candidate location for each node and the true node can be classified by considering detail distance information. The proposed method extracts well-structured topological model of the environment and classification also results in reliable matching even under the uncertain and sparse sonar data. Experimental results verify the performance of proposed environmental modeling and classification in real home environment.	approximation algorithm;cluster analysis;computation;cut (graph theory);distance-vector routing protocol;graph cuts in computer vision;graphical model;map;norm (social);sonar (symantec);sensor;signal-to-noise ratio;sparse matrix;template matching;towed array sonar	Jinwoo Choi;Minyong Choi;Kyoungmin Lee;Wan Kyun Chung	2009	2009 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2009.5152881	computer vision;simulation;computer science;artificial intelligence;graph theory;machine learning;graphical model	Robotics	50.99381262209797	-37.99195410720961	92016
011f7aafc0f7944b3ce7312be97a4cce634dc52c	exploring in 3d with a climbing robot: selecting the next best base position on arbitrarily-oriented surfaces	robot sensing systems;climbing robots;two dimensional displays;trajectory;three dimensional displays	This paper presents an approach for selecting the next best base position for a climbing robot so as to observe the highest information gain about the environment. The robot is capable of adhering to and moving along and transitioning to surfaces with arbitrary orientations. This approach samples known surfaces, and takes into account the robot kinematics, to generate a graph of valid attachment points from which the robot can either move to other positions or make observations of the environment. The information value of nodes in this graph are estimated and a variant of A* is used to traverse the graph and discover the most worthwhile node that is reachable by the robot. This approach is demonstrated in simulation and shown to allow a 7 degree-of-freedom inchworm-inspired climbing robot to move to positions in the environment from which new information can be gathered about the environment.	a* search algorithm;attachments;information gain in decision trees;kullback–leibler divergence;robot;simulation;traverse;while	Phillip Quin;Gavin Paul;Alen Alempijevic;Dikai Liu	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759849	monte carlo localization;computer vision;bang-bang robot;cartesian coordinate robot;simulation;engineering;artificial intelligence;trajectory;social robot	Robotics	53.715118553439154	-24.94135691329569	92124
0a7f6e585995850fd64173bc0d3915d4d0c471b6	real-time temporal frequency detection in fpga using event-based vision sensor		A dynamic vision sensor (DVS) is a new type of vision sensor in which each pixel acts as a motion sensor and generates highly time-accurate events when it detects movement in the scene. The high temporal precision of these types of vision sensors allows the extraction of different low-level temporal features, which is not possible when using a frame-based camera. Hierarchical vision-processing systems use low-level features to recognize a higher level of abstraction. One of the lowlevel features that can be extracted with DVS is the temporal frequency. This feature can be used along with other visual features for more accurate object recognition when the object has rotating parts, such as a quadcopter. This work is an extension of our previous work, wherein we proposed an algorithm to extract this temporal low-level feature by using a DVS. In this work, we proposed a digital circuit with a small footprint to extract the frequency of rotating objects in real time with very low latency. We have synthesized the digital circuit in Spartan-6 field-programmable gate array (FPGA) and also in UMC 180-nm technology to measure the performance, power consumption, and occupied area. MATLAB and Verilog codes for this work are available for academic purposes upon request.	algorithm;application-specific integrated circuit;best, worst and average case;clock rate;code;digital electronics;dynamic voltage scaling;field-programmability;field-programmable gate array;high- and low-level;image sensor;matlab;motion detector;multi-core processor;multiplexing;outline of object recognition;parallel computing;pixel;real-time clock;spinnaker;verilog	Sahar Hoseini;Bernab&#x00E9; Linares-Barranco	2018	2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing (ICCP)	10.1109/ICCP.2018.8516629	verilog;computer vision;field-programmable gate array;pixel;artificial intelligence;digital electronics;latency (engineering);gate array;feature extraction;quadcopter;computer science	Robotics	44.74973113805399	-35.047887883822504	92442
2bcc4b68bcbf2670c3c1e688ba58a151be1fef5c	enabling smart camera networks with smartphone processors	smartphone processors;streaming media smart cameras cameras program processors bandwidth servers real time systems;video processing;smart camera networks;computer vision;hardware smartphone processors computer vision smart camera networks video processing object tracking distributed sensing embedded systems;embedded systems;servers;streaming media;object tracking;smart cameras;bandwidth;distributed sensing;program processors;cameras;hardware;real time systems	Distributed smart cameras exploit smartphone processor performance in their node communication and video metadata exchange, allowing the network to collectively reason in interpreting the scene, generating alerts, and making decisions.	smart camera;smartphone	Sek M. Chai	2015	Computer	10.1109/MC.2015.105	smart camera;embedded system;real-time computing;computer hardware;computer science;video tracking;video processing;bandwidth;server	Arch	44.864034680854694	-36.88215081395806	92842
2ad38334d443aecfa47a80fe2c1b4a6651f4ebd5	an efficient algorithm (fapric) for finding the principl contancts possibly established due to uncertainties	uncertain systems;efficient algorithm;uncertain systems robots computational complexity;s tope model efficient algorithm fapric principal contacts surface features task uncertainties position uncertainties orientation uncertainties location uncertainties;real time processing;uncertainty robotic assembly robotics and automation computer science solid modeling data mining robot sensing systems manipulators algorithm design and analysis personal communication networks;computational complexity;robots;geometric model	An implemented algorithm is presented for finding all principal contacts possibly established between two surface features (i.e., faces, edges, or vertices) of two polyhedral objects due to location (i.e., position and orientation) uncertainties. The algorithm FAPRIC (for Finding All PRIncipal Contacts) requires as inputs: the estimated locations of the two objects, the geometric models of the objects, and the bounds on position and orientation uncertainties. The algorithm is based on checking intersections between the grown regions of the objects b y location uncertainties [12] with the S-tope model [5]. It is very eficient and a p t for real-time processing. The information. obtained can serve as a guade for more accurate extraction and further reasoning of the contact data, possibly with additional sensors. A s the surface of an arbitrary object can be approximated b y planar patches, the algorithm can also be applied to general objects. The work is motivated b y the need for automatically recognizing contacts in spite of uncertainties in robotic tasks.	approximation algorithm;patch (computing);polyhedron;real-time clock;robot;sensor	Jing Xiao;Lixin Zhang	1995		10.1109/ROBOT.1995.525321	robot;computer vision;simulation;computer science;artificial intelligence;geometric modeling;machine learning;computational complexity theory	Robotics	51.17817126926243	-37.64036559306749	93064
5712e4f215a9d4f197c4650ebd312edd5e86cb3e	fast and simple topological map construction based on cooccurrence frequency of landmark observation	robot vision topological map heuristics multivariate data analysis multidimensional scaling landmark observation object recognition mobile robot path planning;topology;object recognition;optimisation;path planning;mobile robots;robot vision;multivariate data analysis;multidimensional scaling;optimisation mobile robots path planning robot vision object recognition topology;frequency robot sensing systems intelligent sensors mobile robots data analysis intelligent robots degradation robustness uncertainty robot vision systems	This paper proposes a new map construction method based on rough information of “how often two objects are observed simultaneously”. This method is founded on the combination of a simple and general heuristics “closely located objects are likely to be seen simultaneously more often than distant objects” and a well-known technique in the multivariate data analysis Multi-Dimensional Scaling (MDS). A significant feature of this method is that it requires little quantitative models nor precise sensor information, unlike conventional map construction methods. Simulation and experiment results suggest that this method is sufficiently practical for grasping a topological configuration of identifiable landmarks quickly.	approximation algorithm;backup;heuristic (computer science);map;multi-core processor;multidimensional scaling;robot;rough set;scalability;simulation	Takehisa Yairi;Kosuke Hirama;Koichi Hori	2001		10.1109/IROS.2001.977156	mobile robot;computer vision;multidimensional scaling;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;motion planning;multivariate analysis;mobile robot navigation	Robotics	52.2427394170512	-37.41107386473462	93096
a985cc4c9414251010a7d1b742ba65416638acf8	towards a meta motion planner b: algorithm and applications	path planning;nau;optimal policy;decision theory;robots;decision process;search problems;markov processes;markov decision process;learning artificial intelligence;reinforcement learning meta motion planner navigation sensor customization markov decision process search problem robots;learning artificial intelligence path planning computerised navigation robots markov processes decision theory search problems;navigation packaging robot sensing systems application software mathematics computer science industrial engineering paper technology motion measurement path planning;computerised navigation	In ci componioiz paper [l] we have developed a frame,work fo,r rating 01‘ comparing navigation packages. For a giuen enuir.onment a navigation package consists of (I motion planner and a sensor to be used during nauigation. The cibility to rate or measure a navigation package is aniportant in order to address issues like sensor custom%zution for an environment and choice of a m.otion plarumr in an environment. In this poper ‘we present th.e nlgorathm. irhich, we use i n or.der to rate a given navigation package. Under the franiezuo,rk ,which was presented in [l], a partially observable Markov decision process (POMDP) is defind. The algorrtlim searches for an optimal policy to be employed in this decision process. We briefly reriiew the problem and the framework, develop the crlgor~ith~m, an,d present experimental results.	algorithm;markov chain;partially observable markov decision process;partially observable system	Amit Adam;Ehud Rivlin;Ilan Shimshoni	2001		10.1109/ROBOT.2001.932568	markov decision process;robot;simulation;decision theory;computer science;artificial intelligence;machine learning;motion planning;markov process;mobile robot navigation	Robotics	51.41371348620975	-27.90593797091662	93160
05524c615132d48289182e8381d5dff56c6b50e8	new likelihood updating for the imm approach application to outdoor vehicles localization	constant turning;sensors;imm approach application;real time;dynamic model;vehicles navigation sensor fusion traffic engineering computing;asynchronous sensors likelihood updating imm approach application outdoor vehicles localization interacting multiple model vehicle evolution space constant velocity constant turning multi sensors applications;navigation;computational modeling;global positioning system;mathematical model;robustness vehicle dynamics global positioning system sensor systems vehicle driving road vehicles modeling frequency equations intelligent robots;traffic engineering computing;asynchronous sensors;vehicles;vehicle evolution space;sensor fusion;likelihood updating;multi sensors applications;outdoor vehicles localization;interacting multiple model;covariance matrix;data models;constant velocity	This paper presents the problematic of outdoor vehicle localization under the IMM (interacting multiple model) approach. The IMM is now a well known modular approach, which is based on the discretization of the vehicle evolution space into simple maneuvers, represented each by a simple dynamic model such as constant velocity or constant turning etc. This allows the method to be optimized for highly dynamic vehicles. Unfortunately classical IMM shows some drawbacks concerning some real time multi sensors applications. In this work, we focus on outdoor vehicle localization with asynchronous sensors in order to report these drawbacks and then propose a new solution. Many tests carried out with simulated and real data confirm the interest for using such a solution in our applications.	algorithm;autonomous robot;behavior model;blocking (computing);computation;discretization;downtime;gps signals;global positioning system;interaction;internationalization and localization;mathematical model;nonlinear system;radar tracker;receiver autonomous integrity monitoring;sensor;sun outage;systems modeling;velocity (software development)	Alexandre Ndjeng Ndjeng;Dominique Gruyer;Sebastien Glaser	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5353896	control engineering;data modeling;covariance matrix;navigation;simulation;global positioning system;computer science;engineering;sensor;mathematical model;control theory;sensor fusion;computational model;statistics	Robotics	49.97131696595804	-32.20482188283773	93363
b555030158a424fcf86696eceaca8d5e719830cd	an adaptive sampling approach for evaluating robot self-righting capabilities		To properly evaluate the ability of robots to operate autonomously in the real world, it is necessary to develop methods for quantifying their self-righting capabilities. Here, we improve upon a sampling-based framework for evaluating self-righting capabilities that was previously validated in two dimensions. To apply this framework to realistic robots in three dimensions, we require algorithms capable of scaling to high-dimensional configuration spaces. Therefore, we introduce a novel adaptive sampling approach that biases queries toward the transitional states of the system, thus, identifying the critical transitions of the system using substantially fewer samples. To demonstrate this improvement, we compare our approach to results that were generated via the previous framework and were validated on hardware platforms. Finally, we apply our technique to a high-fidelity three-dimensional model of a US Navy bomb-defusing robot, which was too complex for the previous framework to analyze.	3d modeling;adaptive sampling;algorithm;dimensions;image scaling;robot (device);sampling (signal processing);sampling - surgical action	Galen E. Mullins;Chad Kessens;Satyandra K. Gupta	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2018.2864350	control engineering;robot;sampling (statistics);scaling;solid modeling;engineering;adaptive sampling	Robotics	51.11095538669981	-26.044299058442448	93498
239d5a6be483026fab0faf7fe5d42da97e1c62c7	a reinforcement learning based robotic navigation system	mobile robot navigation reinforcement learning robotic navigation system machine learning q learning;navigation q learning mobile robots;navigation robot sensing systems mobile robots collision avoidance sonar;path planning control engineering computing intelligent robots learning artificial intelligence mobile robots	It is a challenging task for an autonomous robot to navigate in an unknown environment. Machine learning could be useful to support the robot to adapt to the environment and learn the correct navigation skills quickly. In this paper, a reinforcement learning (Q-learning) based approach is proposed to help a robot to move out of an unknown maze. The definitions of the world states, actions and rewards of the algorithm are presented and some experiments are completed to validate the approach. The experimental results show that the proposed approach does have a good performance on mobile robot navigation.	algorithm;autonomous robot;experiment;machine learning;mobile robot;q-learning;reinforcement learning;robotic mapping	Bashan Zuo;Jiaxin Chen;Larry Wang;Ying Wang	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974463	mobile robot;robot learning;computer vision;simulation;machine learning;robot control;mobile robot navigation	Robotics	50.654717472878644	-28.27367520205367	93540
2112e3e8ea93137e10fbf841a340cce92b78f35e	a bacterial colony growth framework for collaborative multi-robot localization	robot localization;robot sensing systems;statistical analysis multi robot systems;statistical analysis bacterial colony growth framework collaborative multi robot localization biology inspired approach species reproduction convergence aptitude;microorganisms collaboration robot sensing systems robotics and automation robustness multirobot systems computer architecture hardware mobile robots uncertainty;uncertainty;species reproduction;collaboration;mobile robots;multirobot systems;bacterial colony growth framework;computer architecture;collaborative multi robot localization;statistical analysis;biology inspired approach;multi robot systems;robustness;convergence aptitude;microorganisms;robotics and automation;performance assessment;hardware	In this paper the multi-robot localization problem is addressed. A new biology-inspired approach is proposed and implemented: the bacterial colony growth framework (BCGF). It takes advantage of the models of species reproduction to provide a suitable framework for carrying on the multi-hypothesis, along with proper policies for both autonomous and collaborative contexts. Collaboration among robots is obtained by exchanging sensory data and their relative distance and orientation. This information is integrated into the framework in such a way that the convergence aptitude is enhanced. Several simulations in different environments have been performed, comparing autonomous and collaborative localization, along with proper statistical analysis for performance assessment.	autonomous robot;internationalization and localization;robotic mapping;simulation;aptitude	Andrea Gasparri;Mattia C. F. Prosperi	2008	2008 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2008.4543635	mobile robot;simulation;uncertainty;computer science;engineering;artificial intelligence;microorganism;statistics;robustness;collaboration	Robotics	52.437521840039494	-33.7147299008478	93689
46e87fd32617868205317aebd37fd1ca75073f0b	multiple land mines localization using a wireless sensor network	explosive vapours;chemicals;markov chain monte carlo sampling scheme;wireless transmission system;least squares approximations;landmine detection;markov chain monte carlo sampling scheme multiple land mines localization wireless sensor network explosive vapours wireless transmission system fusion centre explosive chemicals inverse problem least squares optimization probabilistic bayesian techniques;sensors;bayes methods;explosive detection;least squares optimization;landmine detection sensors inverse problems wireless sensor networks chemicals vectors explosives;wireless sensor network;multiple land mines localization;explosive chemicals;inverse problem;vectors;wireless sensor networks bayes methods explosive detection inverse problems landmine detection least squares approximations markov processes monte carlo methods;fusion centre;explosives;markov processes;wireless sensor networks;monte carlo methods;probabilistic bayesian techniques;inverse problems	This paper considers the problem of localization of multiple land mines using information collected by a network of wireless sensors deployed in the area of interest. These sensors detect the concentration of explosive vapours, emanating from buried land mines, in the air and have a wireless transmission system for exchanging information with a treatment or fusion centre. One of the key contribution of this paper is to locate and estimate the emission rate of multiple land mines. Using a model for the transport of the explosive chemicals in the air, we formulate the inverse problem consisting in determining sequentially the positions and emission rates of the land mines knowing concentration measurements provided by the sensors. To solve the inverse problem, we present a first solution based on a Least Squares optimization approach and a second solution based on probabilistic Bayesian techniques using a Markov Chain Monte Carlo sampling scheme. These two approaches are tested and compared on simulated data.		Hiba Haj Chhadé;Fahed Abdallah;Imad Mougharbel;Amadou Gning;Lyudmila Mihaylova;Simon J. Julier	2014			mathematical optimization;electronic engineering;wireless sensor network;engineering;machine learning;key distribution in wireless sensor networks;mobile wireless sensor network	Mobile	45.99424520154577	-27.15449387797414	93768
b91c55e9c10b0f0cab9597be6799d9335af2433a	mirus-lab: a project on management of imaging and robots utilization in surgery	surgery computerised picture processing robots;surgery computerised picture processing mirus lab imaging robots;project management robots surgery laboratories biomedical imaging medical diagnostic imaging plastics medical robotics image reconstruction technology management;robots;surgery;computerised picture processing	A project on management of imaging and robots utilization in surgery, MIRUS-LAB is under development in Italy. Objectives of the study, aims of the laboratory, the current situation, the scheme of the feasibility phase and the study group are reported in the paper. >	robot	Alberto Rovetta	1990		10.1109/IROS.1990.262531	robot;simulation;computer science;engineering;artificial intelligence;biological engineering	Robotics	39.38491101501406	-37.748259872229546	94009
b0680b8dee5a39eb5e70d4cd1bd1ff43fe5fc29b	image-based homing	navigation mobile robots contracts indoor environments cultural differences marine vehicles solid modeling military computing information science data mining;mobile robots;computerised pattern recognition;three dimensional;computer vision;imaging system;indoor environment;mobile robots computer vision computerised navigation computerised pattern recognition;image based navigation 3d structure inferring mobile robots 3d models target locations image based local homing algorithm;three dimensional structure;computerised navigation	A system that allows a robot to use a model of its environment to navigate is reported. The system maps the environment as a set of snapshots of the world taken at target locations. The robot uses an image-based local homing algorithm to navigate between neighboring and target locations. The approach has an imaging system that acquires a compact, 360 degrees representation of the environment as an intensity waveform, and an image-based, qualitative homing algorithm that allows the robot to navigate without explicitly inferring three-dimensional structure from the image. The result of an experiment in a typical indoor environment are reported. They show that image-based navigation is a feasible alternative to approaches using 3-D models and more complex model-based vision algorithms.<<ETX>>	algorithm;map;missile guidance;multiple homing;robot;waveform	Jiawei Hong;Xiaonan Tan;Brian Pinette;Richard Weiss;Edward M. Riseman	1991	IEEE Control Systems	10.1109/ROBOT.1991.131651	mobile robot;three-dimensional space;computer vision;simulation;computer science;mobile robot navigation	Robotics	51.52690604789183	-36.80360092684548	94237
21f5568a9290fe0af67a4f2b3a8f6cadc481f7e4	refining grasp affordance models by experience	kernel;grasping;cognitive robotics;visual stimuli;object gripper configuration;automatic testing;importance sampling object grasp affordance model object gripper configuration 6d gripper poses grasp density visual stimuli visual model batch oriented experience based learning paradigm;grasp density;usa councils;orbital robotics;kinematics;object grasp affordance model;learning systems;density functional theory;visualization;robots cognitive robotics robotics and automation density functional theory grippers orbital robotics usa councils automatic testing process control kinematics;visual model;learning systems grippers importance sampling;visual modeling;estimation;6d gripper poses;three dimensional displays;visual cues;robots;grippers;learning object;success rate;process control;batch oriented experience based learning paradigm;importance sampling;experience base;density functional;robotics and automation	We present a method for learning object grasp affordance models in 3D from experience, and demonstrate its applicability through extensive testing and evaluation on a realistic and largely autonomous platform. Grasp affordance refers here to relative object-gripper configurations that yield stable grasps. These affordances are represented probabilistically with grasp densities, which correspond to continuous density functions defined on the space of 6D gripper poses. A grasp density characterizes an object's grasp affordance; densities are linked to visual stimuli through registration with a visual model of the object they characterize. We explore a batch-oriented, experience-based learning paradigm where grasps sampled randomly from a density are performed, and an importance-sampling algorithm learns a refined density from the outcomes of these experiences. The first such learning cycle is bootstrapped with a grasp density formed from visual cues. We show that the robot effectively applies its experience by downweighting poor grasp solutions, which results in increased success rates at subsequent learning cycles. We also present success rates in a practical scenario where a robot needs to repeatedly grasp an object lying in an arbitrary pose, where each pose imposes a specific reaching constraint, and thus forces the robot to make use of the entire grasp density to select the most promising achievable grasp.	algorithm;autonomous robot;batch processing;bootstrapping (compilers);image noise;importance sampling;programming paradigm;randomness;robot end effector;sampling (signal processing);social affordance;visual modeling	Renaud Detry;Dirk Kraft;Anders Glent Buch;Norbert Krüger;Justus H. Piater	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509126	robot;computer vision;kinematics;estimation;kernel;simulation;visualization;sensory cue;visual perception;importance sampling;computer science;artificial intelligence;process control;density functional theory;statistics;cognitive robotics	Robotics	50.20951212936438	-27.08593678008987	94951
7d3a5ee836ae076a9d4302038306360ec0865476	image stitching techniques for an intelligent portable aerial surveillance system	feature based image stitching method;low cost unmanned aerial vehicle;image processing;surveillance;intelligent portable aerial surveillance system;image stitching techniques;light weight unmanned aerial vehicle;cameras feature extraction surveillance calibration robot vision systems artificial intelligence aircraft;feature extraction;ipass;aerial images;artificial intelligence;infini tehomography based method;aerial surveillance information image stitching techniques intelligent portable aerial surveillance system ipass low cost unmanned aerial vehicle light weight unmanned aerial vehicle aerial images feature based image stitching method infini tehomography based method;robot vision systems;calibration;autonomous aerial vehicles;cameras;aerial surveillance information;aircraft;surveillance autonomous aerial vehicles image processing	Intelligent Portable Aerial Surveillance System (IPASS) is aimed at developing a low-cost, light-weight unmanned aerial vehicle that can provide sufficient battlefield intelligence for individual troops. IPASS consists of a ground station and an aircraft, which can take images high above the ground. To get a wider view of the ground, three images are taken simutaneously and then get stitched into a panorama. This paper first briefly introduces the system design of the project. Then it focuses on the processing of aerial images acquired from the three onboard cameras. The paper describes both the feature-based image stitching method and the infini-tehomography-based method for this task. Compared to the popular feature-based method, the second one can greatly reduce computation time with very little compromise in the quality of the panorama, which makes real-time video display of the surroundings on the ground station possible. This paper provides quantitative comparison of these two image stitching techniques to evaluate the performance in acquiring aerial surveillance information from multiple vision.	aerial photography;computation;display device;high above;image stitching;real-time transcription;systems design;time complexity;unmanned aerial vehicle	Ruixiang Du;Taskin Padir	2014	2014 IEEE International Conference on Technologies for Practical Robot Applications (TePRA)	10.1109/TePRA.2014.6869146	computer vision;simulation;engineering;remote sensing	Robotics	52.637532823252194	-37.90688379607984	95510
6529bfb13484e61a89a8b197b45776de2a54a746	improving occupancy grid fastslam by integrating navigation sensors	search and rescue;lidar unit occupancy grid fastslam autonomous vehicle simultaneous localization and mapping vehicle exploration multiple integrated navigation sensor;map building;autonomous vehicle;sensors;vehicles simultaneous localization and mapping laser radar cameras navigation current measurement;occupancy grid;laser radar;prior knowledge;navigation;current measurement;vehicles sensors slam robots;simultaneous localization and mapping;vehicles;slam robots;cameras	When an autonomous vehicle operates in an unknown environment, it must remember the locations of environmental objects and use those object to maintain an accurate location of itself. This vehicle is faced with Simultaneous Localization and Mapping (SLAM), a circularly defined robotics problem of map building with no prior knowledge. The SLAM problem is a difficult but critical component of autonomous vehicle exploration with applications to search and rescue missions. This paper presents the first SLAM solution combining stereo cameras, inertial measurements, and vehicle odometry into a Multiple Integrated Navigation Sensor (MINS) path. The FastSLAM algorithm, modified to make use of the MINS path, observes and maps the environment with a LIDAR unit. The MINS FastSLAM algorithm closes a 140 meter loop with a path error that remains within 1 meter of surveyed truth. This path reduces the error 79% from an odometry FastSLAM output and uses 30% of the particles.	algorithm;autonomous robot;kalman filter;map;real-time clock;robotics;sensor;simultaneous localization and mapping;stereo camera;stereo cameras;visual odometry	Christopher Weyers;Gilbert Peterson	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6094514	lidar;computer vision;navigation;simulation;computer science;sensor;artificial intelligence;remote sensing;simultaneous localization and mapping	Robotics	53.72268690654829	-35.41251591388141	95598
07638fe080ec3ef4eb4d5f65602a310bdaa639ce	a parallel path planning algorithm for mobile robots		This paper presents a path planning algorithm for mobile robots. We introduce a parallel search approach which is based on a regular grid representation of the map. The search is formulated as a cellular automaton by which local inter-cell communication rules are defined. The algorithm is made adaptive by utilising a multiresolution representation of the map. It is implemented on AMT DAP 510, which is a SIMD machine.	algorithm;automated planning and scheduling;cell signaling;cellular automaton;icl distributed array processor;inter-process communication;mobile robot;motion planning;multiresolution analysis;regular grid;simd	Chang Shu;Hilary Buxton	1990		10.5244/C.4.68	computer science;theoretical computer science;machine learning;distributed computing	Robotics	52.92900557854668	-28.271865544360754	95756
b5e54cce4a47c5b7f3e524affd7bca05c267517e	robot obstacle avoidance learning based on mixture models		We briefly surveyed the existing obstacle avoidance algorithms; then a new obstacle avoidance learning framework based on learning from demonstration (LfD) is proposed. The main idea is to imitate the obstacle avoidance mechanism of human beings, in which humans learn to make a decision based on the sensor information obtained by interacting with environment. Firstly, we endow robots with obstacle avoidance experience by teaching them to avoid obstacles in different situations. In this process, a lot of data are collected as a training set; then, to encode the training set data, which is equivalent to extracting the constraints of the task, Gaussian mixture model (GMM) is used. Secondly, a smooth obstacle-free path is generated by Gaussian mixture regression (GMR). Thirdly, a metric of imitation performance is constructed to derive a proper control policy. The proposed framework shows excellent generalization performance, whichmeans that the robots can fulfill obstacle avoidance task efficiently in a dynamic environment.More importantly, the framework allows learning a wide variety of skills, such as grasp andmanipulationwork, which makes it possible to build a robot with versatile functions. Finally, simulation experiments are conducted on a Turtlebot robot to verify the validity of our algorithms.	algorithm;control theory;encode;experiment;google map maker;interaction;machine learning;mathematical optimization;mixture model;multi-objective optimization;obstacle avoidance;optimization problem;reinforcement learning;robot;simulation;test set;turtle (robot);velocity (software development)	Huiwen Zhang;Xiaoning Han;Mingliang Fu;Weijia Zhou	2016	J. Robotics	10.1155/2016/7840580	simulation;artificial intelligence;machine learning;obstacle avoidance	Robotics	49.5641556223679	-27.454731197415903	96051
80f009be8fa2eb079906103004f97870901b995f	effect of grip force and training in unstable dynamics on micromanipulation accuracy	grip force;biological patents;micromanipulation;biomedical journals;learning;text mining;unstable dynamics;europe pubmed central;point to point;standard deviation;training;citation search;manipulator dynamics;grip force micromanipulation accuracy unstable dynamics learning visual magnification;visual magnification;statistical analysis force control grippers haptic interfaces manipulator dynamics micromanipulators;indexing terms;citation networks;force;three dimensional;force accuracy visualization haptic interfaces three dimensional displays trajectory training;accuracy;visualization;trajectory;statistical analysis;research articles;standard deviation grip force unstable dynamics micromanipulation accuracy haptic error amplification visual magnification micromanipulation precision point to point movement 3d space;three dimensional displays;abstracts;open access;grippers;success rate;life sciences;clinical guidelines;micromanipulators;control subjects;full text;haptic interfaces;rest apis;orcids;europe pmc;biomedical research;tk electrical engineering electronics nuclear engineering;bioinformatics;literature search;haptic interface;force control	This paper investigates whether haptic error amplification using unstable dynamics can be used to train accuracy in micromanipulation. A preliminary experiment first examines the possible confounds of visual magnification and grip force. Results show that micromanipulation precision is not affected by grip force in both naive and experienced subjects. On the other hand, precision is increased by visual magnification of up to 10×, but not further for larger magnifications. The main experiment required subjects to perform small-range point-to-point movements in 3D space in an unstable environment which amplified position errors to the straight line between start and end point. After having trained in this environment, subjects performing in the free conditions show an increase in success rate and a decrease in error and its standard deviation relative to the control subjects. This suggests that this technique can improve accuracy and reliability of movements during micromanipulation.	control theory;haptic device component;haptic technology;influenza;large;micromanipulation;movement;point-to-point protocol;standard deviation;unstable medical device problem	Eileen Lee Ming Su;Ganesh Gowrishankar;Che Fai Yeong;Chee Leong Teo;Wei Tech Ang;Etienne Burdet	2011	IEEE Transactions on Haptics	10.1109/TOH.2011.33	control engineering;three-dimensional space;computer vision;text mining;simulation;visualization;index term;point-to-point;computer science;engineering;trajectory;accuracy and precision;haptic technology;standard deviation;force;statistics	Robotics	40.31816828872696	-37.22824621546749	96377
9884ab64f1873ce1d62c709ed247bba7d94aba6c	combining evolution and training in a robotic controller for autonomous vehicle navigation		This work investigates a robotic control system designed to autono-mously navigate a vehicle in urban environments. Our approach is based on the use of two Artificial Neural Networks (ANNs), one is trained for image processing with road recognition and template matching and the second is evolved for navigation control. This paper focuses on experiments and evaluations using a Genetic Algorithm (GA) to evolve the ANN responsible to provide steering and velocity control to the autonomous vehicle.		Jefferson Rodrigo de Souza;Gustavo Pessin;Fernando Santos Osório;Denis Fernando Wolf;Patrícia Amâncio Vargas	2012		10.1007/978-3-642-32527-4_43	control engineering;simulation;control theory	Robotics	53.10599468612775	-29.572290883920246	97284
71274d2ac87de38c374983519a637b3b8912a1a8	map based road boundary estimation	automotive engineering;radar sensor;lane markings;map;mono video camera road shape driver assistance systems lane markings road boundary estimation map radar sensor;edge detection;construction industry;usa councils;road vehicle radar;monos devices;cameras automotive engineering shape control road vehicles simultaneous localization and mapping intelligent vehicles usa councils monos devices radar detection system testing;shape;estimation;roads;video cameras;three dimensional displays;intelligent vehicles;driver assistance systems;video cameras driver information systems edge detection road vehicle radar roads;simultaneous localization and mapping;road boundary estimation;system testing;radar detection;shape control;mono video camera;road shape;driver information systems;cameras;radar;driver assistance system;road vehicles	Knowledge about the road shape is a key element for driver assistance systems which support the driver in complex scenarios like construction sites. Systems only using information derived from lane markings reach a limit here. The paper presents an approach to estimate road boundaries based on static objects bounding the road. A map based environment description and an interpretation algorithm identifying the road boundaries in the map are used. Two approaches are presented for estimating the map, one based on a radar sensor, one on a mono video camera. Besides that two fusion approaches are described. The estimated boundaries are independent of road markings and as such can be used as orthogonal information with respect to detected markings. Results of practical tests using the estimated road boundaries for a lane keeping system are presented.	algorithm;radar	Michael Darms;Matthias Komar;Stefan Lüke	2010	2010 IEEE Intelligent Vehicles Symposium	10.1109/IVS.2010.5548011	computer vision;simulation;geography;transport engineering	Robotics	52.20950076374585	-36.86651145694412	97803
390daede6dce096ad3c578a2d99cfe8c6b1107b1	softvns 2 real time video processing and tracking software	real time;video processing			James Harley	2005	Computer Music Journal	10.1162/comj.2005.29.4.97	real-time computing;video tracking;video processing	Vision	45.112259243838714	-36.91178743351056	97816
0237c6f489cff11def54f3c42388cb491502e015	a survey on real-time motion estimation techniques for underwater robots	brief;real time;motion estimation;binary features;rovs	Over the last few years, we have assisted to an impressive evolution in the state-of-the-art of feature extraction, description and matching. Feature matching-based methods are among the most popular approaches to the problem of motion estimation. Thus, the need of studying the evolution of the feature matching field arises naturally. The application chosen is the motion estimation of a Remotely Operated Vehicle (ROV). A challenging environment such as an underwater environment is an excellent test bed to evaluate the performance of the several recent developed feature extractors and descriptors. The algorithms were tested using the same open source framework to give a fair assessment of their performance especially in terms of computational time. The various possible combinations of algorithms were compared to an approach developed by the authors that showed good performance in the past. A data set collected by the ROV Romeo in typical operations is used to test the methods. Quantitative results in terms of robustness to noise and computational time are presented and demonstrate that the recent trend of binary features is very promising.	algorithm;byte;computation;computer memory;data descriptor;feature extraction;feature vector;graphics processing unit;matching (graph theory);motion estimation;open-source software;real-time locating system;remotely operated vehicle;robot;scale-invariant feature transform;speeded up robust features;testbed;time complexity	Fausto Ferreira;Gianmarco Veruggio;Massimo Caccia;Gabriele Bruzzone	2014	Journal of Real-Time Image Processing	10.1007/s11554-014-0416-z	remotely operated underwater vehicle;embedded system;computer vision;real-time computing;simulation;computer science;artificial intelligence;motion estimation	Robotics	43.76077951142916	-37.99172755720107	97953
d81364dbf658a2bc13c1912e23329b154b2d9d49	a robot vision system for collision avoidance using a bio-inspired algorithm	field programmable gate array;nervous system;real time;analog computation;robot vision;collision avoidance;visual system;mixed analog digital integrated circuits	Locusts have a remarkable ability of visual guidance that includes collision avoidance exploiting the limited nervous networks in their small cephalon. We have designed and tested a real-time intelligent visual system for collision avoidance inspired by the visual nervous system of a locust. The system was implemented with mixed analog-digital integrated circuits consisting of an analog resistive network and field-programmable gate array (FPGA) circuits so as to take advantage of the real-time analog computation and programmable digital processing. The response properties of the system were examined by using simulated movie images, and the system was tested also in real-world situations by loading it on a motorized miniature car. The system was confirmed to respond selectively to colliding objects even in complex real-world situations.	algorithm;robot	Hirotsugu Okuno;Tetsuya Yagi	2007		10.1007/978-3-540-69162-4_12	embedded system;computer vision;simulation;visual system;computer science;nervous system;field-programmable gate array	Robotics	46.079412169827606	-33.35414750635289	98154
a7fb548498f5d7ba4476bd33f7c5c80106984bf4	a neuro-fuzzy real-time image processing system	processing element;image recognition;fuzzy neural nets;handwriting recognition;image processing;software libraries;clocks;real time systems image processing pattern recognition software libraries linux hardware image matching clocks handwriting recognition image recognition;image matching;real time;linear array;fuzzy neural nets image processing real time systems;object oriented software;paprica 3;object oriented software library;array processor;linear array processor;chip;development environment;pattern recognition real time image processing neuro fuzzy paprica 3 linear array processor object oriented software library development environment array processor;neuro fuzzy;pattern recognition;linux;real time image processing;high speed;hardware;real time systems	This paper presents a real-time image processing system based on the PAPRICA-3 linear array processor. The system comprises a PCI board, an object oriented software library and a development environment running under Windows 95/NT or Linux on a PC. The PCI board is equipped with up to 4 PAPRICA-3 chips (128 processing elements). The board features local image and program memory, the ability to grab images from several video sources through an interchangeable daughter-board and high speed hardware processing of fuzzy image matching instructions. The array processor runs from a 100 MHz-clock and is able of nearly one operation per clock cycle affecting a complete image line. The application that drove the design was the real-time recognition of the amount handwritten on banking checks, but the system can perform efficiently any kind of low-level image processing and pattern recognition.	image processing;neuro-fuzzy;real-time transcription	Claudio Sansoè;Francesco Gregoretti;Leonardo Maria Reyneri	1999		10.1109/EURMIC.1999.794445	embedded system;real-time computing;computer hardware;image processing;computer science;digital image processing	Robotics	44.386093828008065	-35.27256902409366	99024
62ca6c62bf0a0402022cb0342e3bc912be2bb73c	automatic generation of a highly accurate map for driver assistance systems in road construction sites	sensor systems;measurement by laser beam;road accidents;video signal processing cartography driver information systems optical scanners road traffic traffic engineering computing video cameras;video signal processing;optical scanners;road traffic;construction industry;laser scanner;laser radar;vehicle driving;automatic generation;accuracy;road construction;current measurement;roads;road work map;video cameras;driver assistance systems;laser scanners;position measurement;environmental economics;driver circuits;cartography;traffic engineering computing;map generation;vehicles;road construction sites;laser scanners map generation driver assistance systems road construction sites digital map road work map video camera;digital map;road accidents road vehicles cameras environmental economics costs capacitive sensors vehicle driving sensor systems laser radar current measurement;video camera;driver information systems;cameras;capacitive sensors;digital mapping;driver assistance system;road vehicles	Road construction sites often are the reason for traffic jams and accidents due to the reduced road width. Driver assistance systems for these demanding environments highly benefit from a digital representation of the road layout. This digital map includes all important infrastructure elements, such as barriers, temporary road markings and guiding reflector posts. The paper describes the automatic generation of a detailed and highly accurate “Road Work Map” using video camera and laser scanners.	differential gps;online and offline;sensor;vii	Andreas Wimmer;Tobias Jungel;Manuel Glueck;Klaus C. J. Dietmayer	2010	2010 IEEE Intelligent Vehicles Symposium	10.1109/IVS.2010.5548047	computer vision;simulation;advanced driver assistance systems;engineering;transport engineering	Robotics	50.331806650609415	-36.25039846966237	99071
d9824362e89ddc9e3259add166bbeb3838d6ae22	an image signal processor for ultra small hd video sensor with 3a in camera phones	video signal processing;mobile handsets;video quality;camera phones;ultra small hd video sensor;image sensors;fpga demonstration board;field programmable gate arrays;image signal processor;gain;cmos integrated circuits	In this paper, we propose the use of an image signal processor (ISP) for an ultra small HD grade video sensor with 3A (AWB, AE, and AF) in camera phones that can capture 720P/30fps video. In order to enhance the video quality of the systems, it is necessary to achieve the high performance of 3A. The proposed ISP is also implemented on a FPGA demonstration board.	anisotropic filtering;camera phone;field-programmable gate array;image processor;signal processing;spherical basis;video sensor technology	Won-Woo Jang;Joohyun Kim;Hagyong Han;Hoongee Yang;Bongsoon Kang	2009	2009 Digest of Technical Papers International Conference on Consumer Electronics		embedded system;computer vision;electronic engineering;computer science;video capture;video tracking;video processing;camera phone	EDA	45.3030962012179	-36.17545770451857	99719
c5d73eafeb576955f241f05d68e8d9f27acc2696	path planning in graph slam using expected uncertainty	reliability;uncertainty;trajectory;simultaneous localization and mapping	In this work we address the problem of trajectory planning in Graph SLAM. We propose the use of Expected Value of the Final Uncertainty, which summarizes all the possible uncertainties that should be considered. In fully explored environments, this is used to determine the most reliable path to the final position. In partially explored environments, this criteria quantifies the reliability of the path planned in the free space. Tests demonstrate its ability to avoid unreliable paths in fully explored environments as compared to other uncertainty based criteria. In the exploration scenario, potential paths not present in the original graph are proposed using Voronoi Diagram of the space ahead observed by the sensors. A cost function is proposed considering the length of the path as well as the expected final uncertainty, thus including potentially shorter, but still reliable paths. Tests demonstrate the shortest path is preferred as long as it contains loop closures with low uncertainty.	algorithm;approximation error;automated planning and scheduling;experiment;loss function;motion planning;robustness (computer science);sensor;shortest path problem;simultaneous localization and mapping;voronoi diagram	Leonardo Fermín-Leon;José Neira;José A. Castellanos	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759676	mathematical optimization;simulation;uncertainty;any-angle path planning;computer science;trajectory;machine learning;reliability;mathematics;simultaneous localization and mapping	Robotics	52.162716980422324	-24.450388608121287	99790
7499d05578a68fcc0388deb233855c1cf3479a0c	a vlsi model of the bat lateral superior olive for azimuthal echolocation	cmos integrated circuits;vlsi;bioacoustics;biomedical electronics;biomimetics;integrated circuit design;integrated circuit testing;neural nets;neurophysiology;cmos vlsi circuit;lateral superior olive cells;vlsi model;azimuthal echolocation;bat lateral superior olive;complex 3d environments;interaural level differences processing;neurobiological basis;neuromorphic system;spiking neural model;computational modeling;very large scale integration;neuroscience;circuits;frequency	Bats have long been the envy of engineers due to their ability to use echolocation to fly with speed and agility through complex 3D environments. By understanding the neurobiological basis for echolocation, we hope to emulate the efficient implementation demonstrated by nature. Bats use interaural level differences (ILD) as their primary cue for azimuthal echolocation. The Lateral Superior Olive (LSO) is the bats' first ILD processing center and plays an important role. We have designed a CMOS VLSI circuit based neuromorphic system that mimics ILD processing in the bat LSO. In this paper, we propose a simple spiking neural model of LSO cells, and a VLSI implementation of an array of cells representing the LSO population.	cmos;internet listing display;lateral computing;lateral thinking;local shared object;neuromorphic engineering;very-large-scale integration	Rock Z. Shi;Timothy K. Horiuchi	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)		biomimetics;electronic engineering;computer science;engineering;electrical engineering;very-large-scale integration;cmos;neurophysiology;artificial neural network;bioacoustics;integrated circuit design	Arch	42.51071978658622	-27.19774877026048	99794
c821ee4263e187708d2794a1f9fd3d8d404fc8f4	a semantic-based gas source localization with a mobile robot combining vision and chemical sensing	artificial intelligence;bayes;mdp;machine learning;markov decision process;mobile robot olfaction;chemical sensors;e-nose;electronic nose;gas classification;gas sensor;gas source localization;mobile robot;object recognition;semantics;sensor fusion;uncertainty;uncertainty propagation	This paper addresses the localization of a gas emission source within a real-world human environment with a mobile robot. Our approach is based on an efficient and coherent system that fuses different sensor modalities (i.e., vision and chemical sensing) to exploit, for the first time, the semantic relationships among the detected gases and the objects visually recognized in the environment. This novel approach allows the robot to focus the search on a finite set of potential gas source candidates (dynamically updated as the robot operates), while accounting for the non-negligible uncertainties in the object recognition and gas classification tasks involved in the process. This approach is particularly interesting for structured indoor environments containing multiple obstacles and objects, enabling the inference of the relations between objects and between objects and gases. A probabilistic Bayesian framework is proposed to handle all these uncertainties and semantic relations, providing an ordered list of candidates to be the source. This candidate list is updated dynamically upon new sensor measurements to account for objects not previously considered in the search process. The exploitation of such probabilities together with information such as the locations of the objects, or the time needed to validate whether a given candidate is truly releasing gases, is delegated to a path planning algorithm based on Markov decision processes to minimize the search time. The system was tested in an office-like scenario, both with simulated and real experiments, to enable the comparison of different path planning strategies and to validate its efficiency under real-world conditions.		Javier Gonzalez Monroy;José-Raúl Ruiz-Sarmiento;F. Moreno;Francisco Melendez-Fernandez;Cipriano Galindo;Javier Gonzalez-Jimenez	2018		10.3390/s18124174		AI	51.92081902062621	-33.100735786072185	100371
b9a62682584760b2de0bfbd66457a6cbfe9a856b	vision-based obstacle avoidance for micro air vehicles using an egocylindrical depth map		Obstacle avoidance is an essential capability for micro air vehicles. Prior approaches have mainly been either purely reactive, mapping low-level visual features directly to headings, or deliberative methods that use onboard 3-D sensors to create a 3-D, voxel-based world model, then generate 3-D trajectories and check them for potential collisions with the world model. Onboard 3-D sensor suites have had limited fields of view. We use forward-looking stereo vision and lateral structure from motion to give a very wide horizontal and vertical field of regard. We fuse depth maps from these sources in a novel robot-centered, cylindrical, inverse range map we call an egocylinder. Configuration space expansion directly on the egocylinder gives a very compact representation of visible freespace. This supports very efficient motion planning and collision-checking with better performance guarantees than standard reactive methods. We show the feasibility of this approach experimentally in a challenging outdoor environment.	depth map;obstacle avoidance	Roland Brockers;Anthony T. Fragoso;Brandon Rothrock;Connor Lee;Larry H. Matthies	2016		10.1007/978-3-319-50115-4_44	control engineering;engineering;configuration space;voxel;motion planning;structure from motion;optical flow;obstacle avoidance;depth map;field of view	Robotics	51.86426495913249	-35.868274267767504	100531
b3f692387a01741f7b576eb4967d9c7237cc0a4f	monitoring of manipulation activities for a service robot using supervised learning	success state estimation;robot sensing systems;manipulators;impedance;service robot abilities;grasping;supervised learning;support vector machines;path planning;training;failed action recovery;adaptive control;service robots;manipulation action monitoring;manipulation action adaptation;failure state;learning systems;supervised training process;monitoring;action success classification manipulation activity monitoring supervised learning helper grasping service robot abilities manipulation action adaptation action planning failed action correction failed action recovery manipulation action execution robotic hand position force exertion manipulation action monitoring success state estimation failure state high dimensional data collection ongoing activity classification support vector machine svm supervised training process bimanual manipulation demonstrator sensor reading monitoring;manipulation action execution;bimanual manipulation demonstrator;manipulation activity monitoring;grippers;signal classification;robotic hand position;service robot;support vector machines adaptive control grippers learning artificial intelligence learning systems manipulators path planning service robots signal classification;svm;monitoring support vector machines robot sensing systems impedance training;force exertion;action planning;sensor reading monitoring;support vector machine;learning artificial intelligence;ongoing activity classification;helper;action success classification;high dimensional data collection;failed action correction	To be a good helper, grasping and manipulation are the most important abilities of a service robot. It should be able to adapt its manipulation actions to new tasks and environments. During the execution, it is important to rate the success of actions, so that the robot can plan and execute further actions to correct and recover from the failed actions. The successful execution of manipulation actions depends on various factors during the whole execution, such as the position of the robotic hand and forces exerted by the robot. The goal of the manipulation action monitoring is to estimate the success state from the huge amount of data collected during the execution. The main challenge to solve this problem is to identify the success or failure state from the the high dimensional data collection. We propose a method to classify ongoing activities using a set of support vector machines (SVM). After a supervised training process with manually labeled successful or failure results, our system can correctly estimate the resulting state of a manipulation activity. We present experiments on our bimanual manipulation demonstrator and evaluate the results.	experiment;microsoft outlook for mac;service robot;supervised learning;support vector machine;time-invariant system	Steffen W. Ruehl;Zhixing Xue;Rüdiger Dillmann	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224803	support vector machine;computer vision;simulation;adaptive control;computer science;engineering;machine learning;supervised learning	Robotics	40.18840627497207	-35.63550813591315	100561
8ec317a56fca499a35095c2a730e97cf36322d96	gpu based parallel optimization for real time panoramic video stitching		Panoramic video is a sort of video recorded at the same point of view to record the full scene. With the development of video surveillance and the requirement for 3D converged video surveillance in smart cities, CPU and GPU are required to possess strong processing abilities to make panoramic video. The traditional panoramic products depend on post processing, which results in high power consumption, low stability and unsatisfying performance in real time. In order to solve these problems,we propose a real-time panoramic video stitching framework.The framework we propose mainly consists of three algorithms, LORB image feature extraction algorithm, feature point matching algorithm based on LSH and GPU parallel video stitching algorithm based on CUDA.The experiment results show that the algorithm mentioned can improve the performance in the stages of feature extraction of images stitching and matching, the running speed of which is 11 times than that of the traditional ORB algorithm and 639 times than that of the traditional SIFT algorithm. Based on analyzing the GPU resources occupancy rate of each resolution image stitching, we further propose a stream parallel strategy to maximize the utilization of GPU resources. Compared with the L-ORB algorithm, the efficiency of this strategy is improved by 1.6-2.5 times, and it can make full use of GPU resources. The performance of the system accomplished in the paper is 29.2 times than that of the former embedded one, while the power dissipation is reduced to 10W. Keywords—panoramic video; image stitching; embedded GPU; Local ORB; stream parallel optimization	algorithm;central processing unit;closed-circuit television;digital video;embedded system;feature (computer vision);feature extraction;graphics processing unit;image processing;image stitching;mathematical optimization;paradiseo;point of view (computer hardware company);real-time locating system;real-time transcription;smart city;time complexity;zero suppression;lsh	Sabiha Hussain;Jingling Yuan;Julie Blaskewicz Boron;Lin Li;Mincheng Chen;Tao Li	2018	CoRR		orb (optics);point set registration;computer vision;machine learning;feature extraction;artificial intelligence;scale-invariant feature transform;sort;computer science;image stitching	Vision	43.92857444218234	-35.93646316318154	100720
12961645b445b4c77705904796502b70de714f29	3-d object modeling and recognition for telerobotic manipulation	robot sensing systems;virtual world building;object recognition;manipulators;telerobotic manipulation;tractable problem 3 d object modeling 3 d object recognition telerobotic manipulation virtual world building remote operations planar surface representations quadric surface representations laser rangefinder data surface representations motion planning robot safeguarding algorithms graphical programming limited operator interaction;human interaction;sensor systems;robot safeguarding algorithms;surface representations;surface representation;3 d object recognition;quadric surface representations;laser rangefinder;work environment;laser ranging;laser rangefinder data;limited operator interaction;computer vision;graphical programming;signal processing;motion planning;telerobotics;laser ranging telerobotics manipulators object recognition signal processing;humans;3 d object modeling;us department of energy;planar surface representations;laser modes;robotics and automation;tractable problem;cameras;remote operations;telerobotics robotics and automation cameras robot sensing systems sensor systems humans laser modes manipulators computer vision us department of energy;object model;virtual worlds	This paper describes a system that semi-automatically builds a virtual world for remote operations by constructing 3-D models of a robot’s work environment. With a minimum of human interaction, planar and quadric surface representations of objects typically found in manmade facilities are generated from laser rangefinder data. The surface representations are used to recognize complex models of objects in the scene. These object models are incorporated into a larger world model that can be viewed and analyzed by the operator, accessed by motion planning and robot safeguarding algorithms, and ultimately used by the operator to command the robot through graphical programming and other high level constructs. Limited operator interaction, combined with assumptions about the robots task environment, make the problem of modeling and recognizing objects tractable and yields a solution that can be readily incorporated into many telerobotic control schemes.	algorithm;cobham's thesis;graphical user interface;high-level programming language;motion planning;overhead (computing);planar (computer graphics);range imaging;region of interest;robot;semiconductor industry;telerobotics;virtual world;visual programming language;jquery	Andrew Edie Johnson;Patrick Leger;Regis Hoffman;Martial Hebert;James Osborn	1995		10.1109/IROS.1995.525782	telerobotics;computer vision;interpersonal relationship;simulation;object model;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;signal processing;motion planning;visual programming language	Robotics	51.32903701580958	-37.22192769262077	100783
f455e7b139edc01ad840a666bc90f307c4d8738d	needle detection by electro-localization for a needle emg exam robotic simulator	phantoms electromyography medical disorders medical robotics medical signal detection needles neurophysiology;phantoms;accuracy;electrodes;curved phantom needle detection electrolocalization needle emg exam robotic simulator electromyogram neurological exam multiple pathological conditions pathological disorders nee skill training needle tip localization position dependent emg signal source skilled neurologists pathological situation prostate brachytherapy needle operation emg signals needle tip position tissue like conductive phantom needle localization nee robotic simulator electrode pair near linear voltage distribution depth direction detection accuracy computer simulation model;robots;needle tip localization needle emg exam tissue like phantom voltage distribution bio electric simulation model;electromyography;voltage measurement;needles;phantoms needles voltage measurement electrodes accuracy robots electromyography	Needle EMG (Electromyogram) Exam (NEE) is an important neurological exam, and neurology interns and novice medical need repetitive training to gain the necessary skill to perform the exam. However, until now it has been difficult to reproduce multiple pathological conditions for their training, since in most cases, trainees serve as human subjects for each other. A robotic simulator that could reproduce behavior with various pathological disorders can be of great help for NEE skill training. Needle tip localization is a key component of the robotic simulator, since position-dependent-EMG is the signal source for skilled neurologists to determine the pathological situation. The needle tip localization has been investigated for many medical tests and applications, such as prostate brachytherapy, etc. However, only few studies have been reported on the process of needle operation in muscle based on EMG signals dependent on needle tip position. Our idea is to apply a tissue-like conductive phantom so as to realize both physical sense of insertion, and needle localization for the NEE robotic simulator. A pair of electrodes was designed to generate a near-linear voltage distribution along the depth direction of the tissue-like phantom, by which the inserted needle could be localized. The influence of the shape of phantom and electrodes on detection accuracy were investigated by a set of measurement experiment and a computer simulation. The results showed that, the estimated depth values agreed with that of the computer simulation model, and the curved phantom had a much steeper distribution in the deeper region (better accuracy for needle tip detection).	computer simulation;electromyography;experiment;glossary of computer graphics;imaging phantom;phantom reference;robot;voltage source	Siyu He;José Gómez-Tames;Wenwei Yu	2015	2015 IEEE International Symposium on Medical Measurements and Applications (MeMeA) Proceedings	10.1109/MeMeA.2015.7145247	simulation;engineering;biological engineering;surgery	Robotics	39.996000081836314	-36.87737362695803	101423
20f4209c7b4a95a27831e4d61a3978858f1e6170	a light-weight yet accurate localization system for autonomous cars in large-scale and complex environments	robot sensing systems;two dimensional displays;laser radar;global positioning system;three dimensional displays;autonomous automobiles	We propose a light-weight yet accurate localization system for autonomous cars that operate in large-scale and complex urban environments. It provides appropriate localization accuracy and processing time at high frequency suitable for fast control actions, besides low power consumption desirable for limited energy availability in commercial cars. The localization system is based on the Particle Filter (PF) localization, which corrects particles' poses by applying map-matching between 2D global occupancy grid-maps computed offline and 2D local occupancy grid-maps constructed online. The localization system converts the dense 3D point clouds of a 3D LiDAR into sparse local maps, which allowed the reduction of the localization processing time without deteriorating its accuracy. We propose two map-matching distance functions: one is an improvement on the traditional Likelihood Field distance between two grid-maps and the other is an adaptation of the standard Cosine distance between two high-dimensional vectors, commonly used in the Information Retrieval research area. To collect data for evaluating the localization system, we used an in-house autonomous car. Experimental results showed that the localization system was able to operate at about 300Hz and 100Hz for the map-matching Likelihood Field distance and the Cosine distance functions, respectively, using a population of 500 particles.	autonomous car;cosine similarity;information retrieval;internationalization and localization;map matching;online and offline;particle filter;point cloud;population;sparse matrix	Lucas de Paula Veronese;José E. Guivant;Fernando Alfredo Auat Cheeín;Thiago Oliveira-Santos;Filipe Wall Mutz;Edilson de Aguiar;Claudine Badue;Alberto Ferreira de Souza	2016	2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2016.7795604	embedded system;computer vision;simulation;engineering	Robotics	53.70067472917498	-36.107396341174834	101484
ef97a93df2ac9438944186cd8848e42cb7432a74	visual place recognition for autonomous robots	radiology;estimation theory;object recognition;probability;computed tomography;visual place recognition;image matching;path planning;statistical analysis mobile robots path planning navigation robot vision object recognition probability image matching estimation theory;mobile robots;navigation;robot vision;shape;statistical analysis;stochastic processes;autonomous mobile robots;pixel;robots;robot vision visual place recognition autonomous mobile robots image matching stochastic model probability statistical analysis estimation theory;predictive models;computer science;stochastic model;robots cameras computer science computed tomography pixel shape radiology stochastic processes predictive models probability;autonomous robot;cameras	Visual Place Recognition for Autonomous Robots Hemant D. Tagare Department of Diagnostic Radiology and Department of Electrical Engineering Drew McDermott Hong Xiao Department of Computer Science Yale University ftagare, mcdermott,xiaog@cs.yale.edu Abstract| The problem of place recognition is central to robot map learning. A robot needs to be able to recognize when it has returned to a previously visited place, or at least to be able to estimate the likelihood that it has been at a place before. Our approach is to compare images taken at two places, using a stochastic model of changes due to shift, zoom, and occlusion to predict the probability that one of them could be a perturbation of the other. We have performed experiments to gather the value of a 2 statistic applied to image matches from a variety of indoor locations. Image pairs gathered from nearby locations generate low 2 values, and images gathered from di erent locations generate high values. The rate of false positive and false negative matches is low.	autonomous robot;computer science;drew mcdermott;electrical engineering;experiment;image processing;optic axis of a crystal;pixel;radiology	Hemant D. Tagare;Drew McDermott;Hong Xiao	1998		10.1109/ROBOT.1998.680722	robot;mobile robot;stochastic process;computer vision;navigation;simulation;shape;computer science;stochastic modelling;cognitive neuroscience of visual object recognition;machine learning;probability;motion planning;predictive modelling;computed tomography;estimation theory;pixel;statistics	Robotics	52.31160834765358	-36.488916655565845	101921
84ecfa5ff1e373684e19675f6fabb22897bdd39b	road boundary detection for run-off road prevention based on the fusion of video and radar	edge detection;road traffic;kalman filters;kalman filter framework road boundary detection run off road prevention video and radar fusion road markings detection video camera radar sensor road barrier road edge detection;image sensors;traffic engineering computing edge detection image sensors kalman filters radar imaging road traffic;radar imaging;roads image edge detection radar detection radar measurements cameras radar tracking;traffic engineering computing	An approach for detecting the road boundary on different types of roads without any preliminary knowledge is presented. We fuse information obtained from an algorithm which detects road markings and road edges in images acquired by a video camera as well as data from a radar sensor. Each road marking, each road edge and each road barrier is tracked individually. Hence we can even capture exits or laybys. We use an edge image for road marking detection and texture information for road edge detection. Additional data provided by a radar sensor is used to measure targets referring to static barriers along the road side such as guardrails. The output of each processing unit is fused into a Kalman filter framework, where the confidence of each subsystem influences the innovation of the overall system. The underlying geometric road model comprises parameters for multiple lanes, the flanking road edge as well as the vehicle's relative pose. The work is part of the project Interactive.	algorithm;edge detection;interactivity;item unique identification;kalman filter;radar;sensor;super off road	Florian Janda;Sebastian Pangerl;Eva Lang;Erich Fuchs	2013	2013 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2013.6629625	man-portable radar;computer vision;simulation;geography;remote sensing	Robotics	50.2941331924684	-36.22678921564505	101945
ec05bb70390a69ec4a6fc925cf9b244f2e66377e	convolutional neural network for pixel-wise skyline detection		Outdoor augmented reality applications are an emerging class of software systems that demand the fast identification of natural objects, such as plant species or mountain peaks, in low power mobile devices. Convolutional Neural Networks (CNN) have exhibited superior performance in a variety of computer vision tasks, but their training is a labor intensive task and their execution requires non negligible memory and CPU resources. This paper presents the results of training a CNN for the fast extraction of mountain skylines, which exhibits a good balance between accuracy (94,45% in best conditions and 86,87% in worst conditions), memory consumption (9,36 MB on average) and runtime execution overhead (273 ms on a Nexus 6 mobile phone), and thus has been exploited for implementing a real-world augmented reality applications for mountain peak recognition running on low to mid-end mobile phones.	augmented reality;central processing unit;computer vision;convolutional neural network;gigabyte;mathematical optimization;mobile device;mobile phone;overhead (computing);pixel;random-access memory;software system;summit;uncontrolled format string	Darian Frajberg;Piero Fraternali;Rocio Nahime Torres	2017		10.1007/978-3-319-68612-7_2	pattern recognition;convolutional neural network;software system;pixel;machine learning;artificial intelligence;skyline;mobile phone;mobile device;computer science;augmented reality;nexus (standard)	Mobile	42.455465718145724	-36.048756438866995	102040
96c2b2b0d59dc118edbe94f5e132de6115484a1e	multilayered image processing for multiscale harris corner detection in digital realization	detectors;field programmable gate array;corner detection;image processing hardware software algorithms delay pipelines computer architecture field programmable gate arrays detectors shape real time systems;image processing;pattern recognition field programmable gate array fpga multilayered image processing multiscale harris corner detection;edge detection;real time;pipeline architecture;resource wasting;field programmable gate array fpga;image data;multiscale harris corner detection;computer architecture;shape;digital realization;pipelines;pattern recognition;software algorithms;pipeline processing edge detection field programmable gate arrays pattern recognition;field programmable gate arrays;data flow;pc based software programming;thin plate spline;luxuriant image processing algorithms;multilayered image processing;pipeline processing;pattern recognition multilayered image processing multiscale harris corner detection digital realization pc based software programming luxuriant image processing algorithms resource wasting image data pipeline architecture field programmable gate array;hardware;real time systems	The PC-based software programming used in complex or luxuriant image processing algorithms is time consuming and resource wasting. As appropriate processing for the image data indeed speedups complicated algorithms, we focus on a crucial case - multilayered processes. In this paper, we gauge deeply into the data flow of multilayered image processing to avoid waiting for the result from every previous steps to access the memory which occurs in many applicable algorithms. Based on combining the parallel and pipelined properties to eliminate unnecessary delays, we propose new visual pipeline architecture and use field programmable gate array to implement our hardware scheme. For verification, the multiscale Harris corner detector in cooperating with shape context and thin-plate splines were combined to complete our real-time experiment of the integrated hardware and software (H/S) system for pattern recognition.	algorithm;computer programming;corner detection;dataflow;field-programmable gate array;harris affine region detector;image processing;pattern recognition;pipeline (computing);real-time clock;shape context;thin plate spline	Pei-Yung Hsiao;Chieh-Lun Lu;Li-Chen Fu	2010	IEEE Transactions on Industrial Electronics	10.1109/TIE.2010.2040556	embedded system;electronic engineering;image processing;computer science;theoretical computer science;digital image processing;field-programmable gate array	Visualization	43.81940478303407	-34.554271664000765	102161
9afb2faf25022b38d25f9c09257c9747e81b934d	assessment of laparoscopic skills based on force and motion parameters	tissue handling;biological tissues;instruments;motion control;training methods;force data;laparoscopy;training;force parameters;training methods box trainers force and motion surgical trainer formost force feedback laparoscopy objective assessment;biomechanics;students medical;mechanical processes;motion;force;0;trainee performance;signal processing computer assisted;motion parameters laparoscopic skill training programs tissue handling force data motion data force control motion control dynamic bimanual positioning tasks force parameters training tasks trainee performance box trainers;force feedback;motion parameters;feedback;clinical competence;surgery;objective assessment;motion data;training tasks;humans;training biological tissues biomechanics biomedical equipment force control surgery;correlation;force and motion surgical trainer formost;laparoscopic skill training programs;force instruments training laparoscopes correlation biomedical measurement surgery;box trainers;biomedical measurement;biomedical equipment;dynamic bimanual positioning tasks;laparoscopes;force control	Box trainers equipped with sensors may help in acquiring objective information about a trainee's performance while performing training tasks with real instruments. The main aim of this study is to investigate the added value of force parameters with respect to commonly used motion and time parameters such as path length, motion volume, and task time. Two new dynamic bimanual positioning tasks were developed that not only requiring adequate motion control but also appropriate force control successful completion. Force and motion data for these tasks were studied for three groups of participants with different experience levels in laparoscopy (i.e., 11 novices, 19 intermediates, and 12 experts). In total, 10 of the 13 parameters showed a significant difference between groups. When the data from the significant motion, time, and force parameters are used for classification, it is possible to identify the skills level of the participants with 100% accuracy. Furthermore, the force parameters of many individuals in the intermediate group exceeded the maximum values in the novice and expert group. The relatively high forces used by the intermediates argue for the inclusion of training and assessment of force application during tissue handling in future laparoscopic skills training programs.	handling (psychology);instrument - device;training programs;sensor (device)	Tim Horeman;Jenny Dankelman;Frank-Willem Jansen;John van den Dobbelsteen	2014	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2013.2290052	control engineering;motion control;computer vision;simulation;engineering;biomechanics;motion;feedback;haptic technology;correlation;force;physics;quantum mechanics	Visualization	39.918727054801636	-37.496795523950766	102207
dd8b7ee9056e0dd51dee757366ef95e912b74750	image-based satellite-roadway-vehicle integration for informatic vicinity generation	vision system;birds eye imagery;image processing;roadway texture extraction;informatics layout humans robot sensing systems robot vision systems road accidents object detection machine vision mobile robots road safety;road traffic;real time;mobile robots;satellite avionics sensors;gps;global positioning system;feature extraction;road traffic feature extraction global positioning system image processing mobile robots;decision process;image based satellite roadway vehicle integration;birds eye imagery image based satellite roadway vehicle integration informatic vicinity generation satellite avionics sensors onboard vision gps roadway texture extraction;onboard vision;informatic vicinity generation;invariant feature	A new framework is presented for integrating satellite/avionics sensors with onboard vision for expanding the horizon of perception. Real time bindings of the bird's eye image and the driver's view via GPS provides informatic vicinity for supporting in situ decision processes. As the invariant feature for multi-viewpoint association, chromatic complexity of roadway texture is extracted from frontal scene and adapted to the bird's eye imagery. The feasibility of the framework with requirements for vision system is discussed through simulation and experimental studies	avionics;global positioning system;informatics;requirement;sensor;simulation	Kohji Kamejima	2006	ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2006.314439	computer vision;simulation;global positioning system;image processing;computer science	Robotics	50.70584357465007	-35.95203053320147	102368
ef9fd0ab44aed0ec2884ca90469c3a7cfba7437a	supervised deep actor network for imitation learning in a ground-air uav-ugvs coordination task		Ground-Air coordination is a very complex environment for a machine learning algorithm. We focus on the case where an Unmanned Aerial Vehicle (UAV) needs to support a group of Unmanned Ground Vehicles (UGVs). The UAV is required to broadcast an image that contains all UGVs, thus, offering a bird-eye-view on the group as a whole. The source of complexity in this task is twofold. First, coordination needs to occur without communication between the UAV and UGVs. Second, the ability of the UAV to sense the UGVs is coupled with the ability of the UAV to learn how to track laterally the UGVs and adapt its vertical position so that the images of the UGVs are appropriately spaced within the camera field of view. In this paper, we propose using the Deep Actor Network component of an Actor-Critic Deep Reinforcement Learning architecture as a supervised learner. The advantage of this approach is that it offers a step towards autonomous learning whereby the full Actor-Critic model can be utilized in the future. Human demonstrations are collected for the deep Actor network to learn from. The system is built using the Gazebo Simulator, Robot Operating System, and the OpenAI Gym. We show that the proposed setup is able to train the UAV to follow the UGVs while maintaining all UGVs within camera range in situations where UGVs are performing complex maneuvers.	aerial photography;algorithm;autonomous robot;complexity;machine learning;norm (social);reinforcement learning;robot operating system;unmanned aerial vehicle	Hung The Nguyen;Matthew Garratt;Lam Thu Bui;Hussein A. Abbass	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8285387	architecture;simulation;imitation;robot;vertical direction;reinforcement learning;field of view;computer science	Robotics	50.062079574395916	-29.484583848914436	102538
890e303cbfd0dd4931e43f789104b335624b6859	a 1.5nj/pixel super-resolution enhanced fast corner detection processor for high accuracy ar	image resolution energy resolution feature extraction accuracy augmented reality real time systems signal resolution;shift registers augmented reality computer vision feature extraction image resolution microprocessor chips object recognition;augmented reality fast corner detection processor high accuracy ar vision application high resolution image feature detection system power consumption wearable device features from accelerated segment test high throughput super resolution 4 core cluster adaptive multiissue multiply accumulate ammac unit shift register angle integrator energy efficiency object recognition	Most vision applications such as object recognition and augmented reality require a high resolution image because their performance is heavily dependent on a local feature point like an edge and a corner. Unfortunately, the vulnerability of correct feature detection always exists in vision applications. Moreover, it is hard to increase image resolution because there is the trade-off between the image resolution and the system power consumption in a wearable device. To resolve this, we present an energy-efficient Features from Accelerated Segment Test (FAST) corner detection processor with a high-throughput super-resolution 4-core cluster for low-power and high accuracy AR applications. To perform high throughput super-resolution, the hardware is proposed with an adaptive multi-issue multiply-accumulate (AMMAC) unit and a shift register (SHR) based angle integrator. Finally, a proposed super-resolution enhanced FAST corner detection processor performs 13.51% detection accuracy enhanced FAST corner detection on up to a 16× super-resolution image with only 1.5nJ/pixel energy efficiency.	augmented reality;corner detection;feature detection (computer vision);feature detection (web development);features from accelerated segment test;head-mounted display;high-throughput computing;image resolution;low-power broadcasting;multiply–accumulate operation;outline of object recognition;pixel;real-time clock;shr;shift register;super-resolution imaging;throughput;wearable technology	Seongwook Park;Gyeonghoon Kim;Junyoung Park;Hoi-Jun Yoo	2014	ESSCIRC 2014 - 40th European Solid State Circuits Conference (ESSCIRC)	10.1109/ESSCIRC.2014.6942054	embedded system;computer vision;feature detection;computer hardware;computer science;computer graphics (images)	Vision	44.23909321752823	-35.522415924639944	102608
8f93765ee1da478af65217f102f567d7828b1a97	vibrotactile force feedback system for minimally invasive surgical procedures	adverse event;minimal invasive surgery;minimally invasive;telerobotics force feedback medical robotics surgery tactile sensors;medical robotics;force feedback;telerobotic surgery;surgery;telerobotics;tactile sensors;vibrotactile force feedback system;surgical procedure;minimally invasive surgery force feedback surges force sensors hospitals force control tactile sensors humans biological materials force measurement;vibrotactile stimulation;telerobotic surgery vibrotactile force feedback system minimally invasive surgical procedure vibrotactile stimulation;minimally invasive surgical procedure	Lack of adequate force feedback for the surgeon in minimally invasive surgery (MIS) can lead to unnecessary trauma to tissue and adverse events during surgery. The successful use of vibrotactile stimulation to augment overloaded or deficient sensory modes in the human operator in other application areas warrants an investigation into its application in MIS. A vibrotactile force feedback system was designed, and its ability to provide useful force information to subjects performing a simulated MIS task was evaluated. Results showed that the system responds as predicted against the bottom surface of the foot, and that subjects were able to perceive a linear increase in force as linear increase in vibration intensity. Furthermore, vibrotacile force information increased one's sensitivity to tissue contact (1.3 N maximum force -no vibration, 1.0 N maximum force-fine step vibration feedback; p<0.001) and improved one's ability to consistently and accurately differentiate tissue softness in a simulated MIS task. Vibrotactile force feedback in MIS appears to have benefits which can lead to a decrease in trauma to tissue and adverse events.	haptic technology;maximum force;minimally invasive education;sacral nerve stimulation;simulation;telerobotics;tooltip	Ryan Everett Schoonmaker;Caroline G. L. Cao	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.385233	telerobotics;adverse effect;computer science;artificial intelligence;haptic technology;tactile sensor	Robotics	40.160070896799496	-37.27556433823473	102668
21fe165c615bad5f292ac7a3a3fdcba672db81b0	total precision inspection of machine tools with virtual metrology	machining precision;machining metrology machine tools accuracy manufacturing feature extraction standards;total inspection;virtual reality acoustic noise feature extraction inspection machine tools machining precision engineering production engineering computing vibrations;automatic virtual metrology avm machining precision total inspection;automatic virtual metrology avm;gavm system total precision inspection machine tool automatic virtual metrology machining precision measurement avm server process data sensor data machining parameters machining accuracy prediction vibrations loud noises signal to noise ratio workpiece measurements high tech industries machining industry embedding essential sensors nc file data cleaning process sensor data s n ratio feature extraction segmented raw process data ged plus avm system	The purpose of this study is trying to apply VM [1][2] for measuring machining precision of machine tools. As shown in the left portion of Fig. 1, the AVM server [3] needs process data, which contain sensor data and machining parameters, as inputs for predicting machining accuracies. However, most machine tools do not possess sensors for providing raw process data such as vibration, current, etc. Furthermore, machining operations usually possess severe vibrations and loud noises. It makes the raw data collected from sensors attached to machine tools with low signal-to-noise (S/N) ratios, thereby affecting the prediction accuracy of VM. Even though the AVM has been an effective way to conduct workpiece measurements in high-tech industries [1], there exist challenges when applying the AVM to machining industry. Besides embedding essential sensors on the machine tool effectively, the challenges are 1) Segmentation: to accurately segment essential part of the raw process data from the original NC file, 2) Data Cleaning: to effectively handle raw process/sensor data with low S/N ratios, and 3) Feature Extraction: to properly extract significant features from the segmented raw process data. This work proposes a novel GED-plus-AVM (GAVM) system as depicted in Fig.1 to resolve all the challenges mentioned above. These challenges are judiciously addressed and successfully resolved in this paper.	2d-plus-depth;astronomy visualization metadata;existential quantification;feature extraction;sensor;server (computing);signal-to-noise ratio;virtual metrology;z/vm	Hao Tieng;Haw Ching Yang;Fan-Tien Cheng	2015	2015 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/CoASE.2015.7294301	engineering;engineering drawing;manufacturing engineering;mechanical engineering	DB	39.38882655082076	-30.810400316472037	102826
a2815d51a6bb72c5a014c8ebeeebe9ca8e14c1cb	a spiking implementation of the lamprey's central pattern generator in neuromorphic vlsi	hardware and architecture;neurons biological system modeling neuromorphics hardware biological neural networks robot kinematics;vlsi actuators biocontrol bio inspired materials biomechanics biomimetics brain models low power electronics motion control neural nets neurophysiology power consumption robot kinematics;biomedical engineering	The lamprey has been often used as a model for understanding the role of Central Pattern Generators (CPGs) in locomotion. Many artificial neural network models have been proposed in the past to explain the neuro-physiology and behavioral data measured from the lamprey, and several robotic implementations have been built to test the software models in a real physical bio-mimetic artifact, and to reproduce the characteristic locomotion patterns observed in the real lamprey. However, in these systems there has typically been a clear separation between the mechanical component of the system (the body), and its control part (the CPG), typically implemented with conventional digital platforms, such as micro-controllers or Field Programmable Gate Arrays (FPGAs). Here we propose to implement a CPG network using neuromorphic electronic circuits, that can be directly interfaced to the robotic actuators of a bio-mimetic robotic lamprey, eliminating the distinction between software and hardware. These circuits comprise low-power analog silicon neurons and synapses, that are affected by device mismatch and noise. The challenge is therefore to determine the CPG model that can best implement robust locomotion control of the bio-mimetic artifact, in face of the constraints imposed by the neuromorphic implementation. As these constraints are similar to the ones faced by the neurons and synapses in the real lamprey (e.g., finite and small power consumption, finite and small resolution or signal to noise ratio, large variability, etc.), the final system implementation will shed light onto the neural processing principles used by real CPG networks to produce robust and distributed control of locomotion in a physical bio-mimetic artifact.	action potential;artificial neural network;autonomous robot;british informatics olympiad;central pattern generator;distributed control system;electronic circuit;feedback;field-programmable gate array;full custom;integrated circuit;low-power broadcasting;neuromorphic engineering;printed circuit board;signal-to-noise ratio;spatial variability;synapse;very-large-scale integration	Elisa Donati;Federico Corradi;Cesare Stefanini;Giacomo Indiveri	2014	2014 IEEE Biomedical Circuits and Systems Conference (BioCAS) Proceedings	10.1109/BioCAS.2014.6981775	control engineering;electronic engineering;simulation;engineering	EDA	42.6244483445889	-27.269624866790775	103028
8e2b2e373fd590ce2ad65598f18326ecbc0b179d	incremental and robust construction of generalized voronoi graph (gvg) for mobile guide robot	range data;sensors;generalized voronoi graph;computational geometry;mobile robots;sensor scan incremental vornoi graph construction robust voronoi graph construction generalized voronoi graph mobile guide robot sensor based navigation tool navigation algorithm semi unstructured map sensor data matching technique;navigation;mobile guide;pattern matching;sensor fusion;robustness mobile robots navigation robot sensing systems biosensors mechanical sensors covariance matrix sensor phenomena and characterization biomechatronics mechanical engineering;pattern matching computational geometry mobile robots sensors navigation sensor fusion	"""Abstracl-GVG has been effectively used as a sensor based navigation tool using 360' sensor data. For mobile guide mhot applications, however, we can only use 180' s e m r data and the rohusiness of the navigation algorithm is critical for successful applications. For that purpose, the robot should he equipped with three capabilities. Those are 1) incremental GVG construction, 2) ruhnst GVG navigation and 3) navigation strategy that just uses half of the sensor wan, i.e. 180"""". In this paper, we propose a GVG navigation algorithm that has above 3 capabilities. We firstly propose a method that can estimate the invisible 180"""" range from previous range data. Moreover, we suggest a way of robust GVG navigation algorithm by using a sensor data matching technique. The simulation result validates that the proposed algorithm can incrementally and robustly navigate the semiunstmtured map by using 180"""" sensor xan."""	algorithm;baldur's gate (series);robot;simulation	SungHwan Ahn;Nakju Lett Doh;Kyoungmin Lee;Wan Kyun Chung	2003		10.1109/IROS.2003.1249739	mobile robot;computer vision;navigation;simulation;computational geometry;computer science;engineering;sensor;machine learning;pattern matching;sensor fusion;mobile robot navigation	Robotics	53.70185656930372	-33.58858964066601	103196
e39e63dd9880d173e9354478ed95ecf1eb968a29	real-time vehicle localization by using a top-down process	bayesian network real time vehicle localization mobile robot localization system top down multi sensorial approach maps development;vehicles detectors robots accuracy uncertainty bayes methods;sensor fusion belief networks kalman filters mobile robots path planning	In this paper, a localization system for a mobile robot is proposed, using a top-down multi-sensorial approach and exploiting a map of the environment. Nowadays the wide development of maps make relevant localization approachesable to use such maps. A crucial point of all localization systems is the way of the data provided by different sensors are fused. The proposed approach is based on a Bayesian network able to select the best feature to detect in the map with the best sensor in order to reach both precision and integrity of the robot localization. This process is working in real time and was validated in simulated and real environments.	bayesian network;map;mobile robot;real-time computing;real-time transcription;relevance;robotic mapping;sensor;simulation;top-down and bottom-up design;triplet state	Claude Aynaud;Coralie Bernay-Angeletti;Roland Chapuis;Romuald Aufrère;Christophe Debain;Nadir Karam	2014	17th International Conference on Information Fusion (FUSION)		computer vision;simulation;engineering;machine learning	Robotics	52.858457496031605	-34.60412483186736	103364
6234278e63dddcfc3b764c68538b8b57db0d63c3	let the light guide us: vlc-based localization	photonics;light emitting diodes;fingerprint recognition;signal resolution;correlation;electron tubes;light sources	We propose to use visible-light beacons for low-cost indoor localization. Modulated light-emitting diode (LED) lights are adapted for localization as well as illumination. The proposed solution consists of two components: light-signal decomposition and Bayesian localization.	bayesian network;diode;illumination (image);internationalization and localization;modulation;oled;vlc media player	Kejie Qiu;Fangyi Zhang;Ming Liu	2016	IEEE Robotics & Automation Magazine	10.1109/MRA.2016.2591833	electronic engineering;photonics;computer science;correlation;fingerprint recognition;light-emitting diode	Robotics	47.41108917123903	-34.74228490668546	103415
76ac0e9ca3392a2bd63d642ba2ee45174d722b35	curating long-term vector maps	uncertainty;heuristic algorithms;feature extraction;robots;robustness;monte carlo methods;laser noise	Autonomous service mobile robots need to consistently, accurately, and robustly localize in human environments despite changes to such environments over time. Episodic non-Markov Localization addresses the challenge of localization in such changing environments by classifying observations as arising from Long-Term, Short-Term, or Dynamic Features. However, in order to do so, EnML relies on an estimate of the Long-Term Vector Map (LTVM) that does not change over time. In this paper, we introduce a recursive algorithm to build and update the LTVM over time by reasoning about visibility constraints of objects observed over multiple robot deployments. We use a signed distance function (SDF) to filter out observations of short-term and dynamic features from multiple deployments of the robot. The remaining long-term observations are used to build a vector map by robust local linear regression. The uncertainty in the resulting LTVM is computed via Monte Carlo resampling the observations arising from long-term features. By combining occupancy-grid based SDF filtering of observations with continuous space regression of the filtered observations, our proposed approach builds, updates, and amends LTVMs over time, reasoning about all observations from all robot deployments in an environment. We present experimental results demonstrating the accuracy, robustness, and compact nature of the extracted LTVMs from several long-term robot datasets.	algorithm;autonomous robot;experiment;markov chain;mobile robot;monte carlo method;recursion (computer science);software deployment;vector map	Samer B. Nashed;Joydeep Biswas	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759683	robot;computer vision;simulation;uncertainty;feature extraction;computer science;artificial intelligence;machine learning;statistics;robustness;monte carlo method	Robotics	49.81491222015928	-37.31569777439039	103725
8a7e771fd7317318db13042b5ed80697eae41445	map-based adaptive foothold planning for unstructured terrain walking	robot learning;robot sensing systems;information sources;map based adaptive foothold planning algorithm;legged locomotion;approximation method;path planning;decision surface map based adaptive foothold planning algorithm unstructured terrain walking hexapod walking robot local terrain map inexpensive structured light sensor polynomial based approximation method;structured light;foot;local terrain map;mobile robots;inexpensive structured light sensor;polynomials;polynomial based approximation method;a priori knowledge;hexapod walking robot;unstructured terrain walking;walking robot;terrain mapping mobile robots path planning polynomial approximation;legged locomotion robot sensing systems terrain mapping foot orbital robotics robotics and automation stability robot vision systems humans usa councils;decision surface;approximation methods;terrain mapping;leg;polynomial approximation	This paper presents an adaptive foothold planning method for a hexapod walking robot. A local terrain map acquired with an inexpensive structured light sensor is exploited as the information source for the planning algorithm, which uses a polynomial-based approximation method to create a decision surface. The robot learns from simulations, therefore no a priori knowledge is required. The results show that the method is general enough to work on various types of terrain. The planned footholds enable the robot to walk more stable, avoiding slippages and fall-downs.	algorithm;approximation;automated planning and scheduling;coefficient;decision boundary;information source;mobile robot;polynomial;simulation;structured light;structured-light 3d scanner	Dominik Belter;Przemyslaw Labecki;Piotr Skrzypczynski	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509670	control engineering;mobile robot;robot learning;computer vision;a priori and a posteriori;simulation;structured light;computer science;artificial intelligence;motion planning;foot;polynomial	Robotics	53.701227626361764	-32.182945196404326	104334
acd9ab65bec32a28b7e435659f97b0b1d2048db3	an extremely fast pattern based line detector		This article describes a completely new, fully automatic line detector algorithm that takes advantage of look-up tables to recognize and fit straight line patterns. The algorithm first recognizes any possible 4×4 pixel line patterns among the binary edge pixels and then uses several small look up tables to decide whether the connected patterns form a line or not. It is designed for real time processing of high resolution images such as the ones used in computer vision applications. The algorithm's system on chip implementation is even cheaper and faster making it especially valuable for battery operated mobile robot applications. Based on the benchmarks, at the time of the writing, this algorithm is the fastest line detector in the literature.	algorithm;benchmark (computing);computer vision;fastest;image resolution;lookup table;mobile robot;pixel;real-time computing;system on a chip	I. Cem Baykal;Ibrahim Yilmaz	2017	2017 13th IEEE International Conference on Intelligent Computer Communication and Processing (ICCP)	10.1109/ICCP.2017.8117032	artificial intelligence;system on a chip;computer vision;pixel;line (geometry);algorithm design;mobile robot;computer science;detector;binary number;lookup table	Robotics	44.451564644686194	-35.19565981271707	104402
5367985db6719898a6c28c44f3bf8baa3521f268	robot soccer using optical analog vlsi sensors	analog vlsi;robot soccer	In this paper we show how a combination of low dimensional vision sensors can be used to aid the higher level visual processing task of colour blob tracking, carried out by a conventional vision system. The processing elements are neuromorphic analog VLSI (aVLSI) vision sensors capable of computing motion and estimating the spatial position of high-contrast moving targets. In these devices both sensing and processing is done on the chip’s focal plane. The neuromorphic sensors can calculate optical flow, position of sharpest edge, and motion of sharpest edge, in real-time. The processing capability of the system is investigated in a mobile robotics application. Firstly, for visualization and evaluation purposes, a correlation analysis is performed between the data collected from the neuromorphic vision sensors and the standard vision system of the autonomous robot, then, we process the multiple neuromorphic sensory signals with a standard auto-regression method in order to achieve a higher level vision processing task at a much higher update rate. At the end we argue why this result is of great relevance for the application domain of reactive and lightweight mobile robotics, at the hands of a soccer robot, where the fastest sensory-motor feedback loop is imperative for a successful participation in a RoboCup soccer competition.	application domain;autonomous robot;autoregressive model;blob detection;focal (programming language);fastest;feedback;image sensor;imperative programming;mobile robot;neuromorphic engineering;optical flow;real-time clock;relevance;robotics;very-large-scale integration	Vlatko Becanovic;Ramin Hosseiny;Giacomo Indiveri	2004	I. J. Robotics and Automation	10.2316/Journal.206.2004.4.206-2718	embedded system;electronic engineering;simulation;computer science	Robotics	47.36393712351204	-32.638636229907625	104545
0a65271de058a16841216f8a4ea75aa1484a8a05	a 3d simultaneous localization and mapping exploration system	distributed architecture 3d simultaneous localization and mapping exploration system robot system indoor environment slam algorithm 3d map robot navigation plan modular architecture;path planning;extremities;mobile robots;three dimensional;skeleton;robot vision;three dimensional displays;indoor environment;simultaneous localization and mapping;skeleton simultaneous localization and mapping three dimensional displays iterative closest point algorithm extremities;iterative closest point algorithm;slam robots;slam robots mobile robots path planning robot vision;distributed architecture	In this work we develop a robot system capable of autonomously exploring and mapping an indoor environment. For this we apply a Simultaneous Localization and Mapping (SLAM) algorithm to build up a 3D map and use it to plan the navigation of the robot to further explore the environment. The system is realized in a modular and distributed architecture. Finally, we critically evaluate the system as a whole.	algorithm;autonomous robot;component-based software engineering;distributed computing;robot;simultaneous localization and mapping;software architecture;software system	Gregor Michalicek;Denis Klimentjew;Jianwei Zhang	2011	2011 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2011.6181428	mobile robot;embedded system;three-dimensional space;computer vision;simulation;computer science;artificial intelligence;motion planning;skeleton;mobile robot navigation;simultaneous localization and mapping	Robotics	51.84334041493794	-37.620020146233294	104810
1d765f5761151053c1d02f468c839e052d5ad298	topology-based visualization of transformation pathways in complex chemical systems	physical sciences and engineering chemistry i 3 8 computing methodologies;j 2 4 computer applications physical sciences and engineering;j 2 4 computer applications;computer graphics applications;applications;chemistry i 3 8 computing methodologies computer graphics;categories and subject descriptors according to acm ccs	Studying transformation in a chemical system by considering its energy as a function of coordinates of the system’s components provides insight and changes our understanding of this process. Currently, a lack of effective visualization techniques for high-dimensional energy functions limits chemists to plot energy with respect to one or two coordinates at a time. In some complex systems, developing a comprehensive understanding requires new visualization techniques that show relationships between all coordinates at the same time. We propose a new visualization technique that combines concepts from topological analysis, multi-dimensional scaling, and graph layout to enable the analysis of energy functions for a wide range of molecular structures. We demonstrate our technique by studying the energy function of a dimer of formic and acetic acids and a LTA zeolite structure, in which we consider diffusion of methane.	ab initio quantum chemistry methods;algorithm;blackwell (series);cluster analysis;color;compiler;complex systems;contour line;discretization;domino tiling;energy citations database;eurographics;force field (chemistry);graph (discrete mathematics);graph drawing;ibm websphere extreme scale;image scaling;interactivity;level of detail;map;mathematical optimization;maxima and minima;molecular modelling;multidimensional scaling;persistence (computer science);quantum mechanics;scientific visualization;semiconductor industry;simulation;text simplification;user interface	Kenes Beketayev;Gunther H. Weber;Maciej Haranczyk;Peer-Timo Bremer;Mario Hlawitschka;Bernd Hamann	2011	Comput. Graph. Forum	10.1111/j.1467-8659.2011.01915.x	computational science;computer science;artificial intelligence;theoretical computer science;mathematics;information technology;algorithm;computer graphics (images)	Visualization	43.98816914771738	-26.84849046461156	105126
28cadf312a42ab566491b1e6fe05d6d5848bba28	image segmentation using linked mean-shift vectors for simd architecture	image segmentation;berkeley segmentation dataset image segmentation linked mean shift vectors simd architecture single instruction multiple data architecture iteration process;image segmentation vectors computer architecture clustering algorithms standards bandwidth acceleration;parallel processing image segmentation;parallel processing	This paper presents a new mean-shift based segmentation algorithm for single instruction multiple data (SIMD) architecture. A standard mean-shift algorithm has different number of computations for each pixel because the endpoints of each pixel's iteration process are different, thus a standard mean-shift algorithm is hard to be accelerated using SIMD architecture. The proposed algorithm, however, equalizes the number of computations for each pixel by constructing links between pixels using their first mean-shift vectors without iteration process. It makes the proposed algorithm more suitable for the SIMD architecture without a complicated scheduling module. Experimental results using the Berkeley segmentation dataset show the proposed algorithm successfully equalizes the number of computations with reasonable segmentation quality.	algorithm;computation;image segmentation;iteration;mean shift;pixel;simd;scheduling (computing)	Hanjoo Cho;Young Hwan Kim	2014	2014 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2014.6776096	parallel processing;computer vision;parallel computing;computer science;theoretical computer science;segmentation-based object categorization;image segmentation;scale-space segmentation	Vision	42.179510748262736	-34.62204795811732	105195
eb6200fa8cf5be15d3bfe9d8e236a9c99331ef9f	induced path transit function, betweenness and monotonicity		The system and method for detecting an intervehicle distance to a preceding vehicle which is moving on the same lane as the vehicle, means for radiating and sweeping an electromagnetic wave such as a laser beam toward the moving direction of the vehicle is provided, with the confirmation of a lane on which the vehicle moves on the basis of a comparison of their sweep angles to reflectors located on both ends of a road on which the vehicle moves from a center axis of the vehicleu0027s movement direction, the traffic lane on which a vehicle moving in front of the vehicle moves is determined on the basis of their sweep angles of the electromagnetic wave with respect to the vehicle from the other vehicle moving in front of the vehicle to the reflectors located on both ends of the road and having the same distance to the vehicle as the other vehicle, and the intervehicle distance data is outputted upon determination that both vehicles move on the same traffic lane, a correct intervehicle distance from the vehicle to the preceding vehicle can thus be measured. In addition, the center axis is corrected according to a steering angle of the vehicleu0027s steering wheel.	betweenness;induced path	Manoj Changat;Joseph Mathews;Henry Martyn Mulder	2003	Electronic Notes in Discrete Mathematics	10.1016/S1571-0653(04)00531-1	induced path;beam (structure);discrete mathematics;electromagnetic radiation;steering wheel;acoustics;monotonic function;betweenness centrality;mathematics	Theory	53.27752491612044	-29.934059231625685	105300
3be9677d197f6d09f8f0459c0cafa8a05574de5d	a bayesian approach to multiple target detection and tracking	bayesian methods object detection target tracking filtering particle filters density measurement particle measurements sampling methods size measurement trajectory;bayesian framework;bayes estimation;filtre particule;filtering;densite probabilite;sample size;deteccion blanco;probability;probability density;density measurement;bayesian approach;particle measurements;target tracking bayes methods object detection particle filtering numerical methods probability;bayes methods;methode bayes;echantillonnage;simulation;cible multiple;particle filters bayes procedures object detection and tracking;object detection and tracking;bayesian methods;particle filtering scheme;size measurement;simulacion;bayes procedures;blanco multiple;filtro particulas;densidad probabilidad;sampling;detection cible;detection objet;estimacion bayes;trajectory;joint multitarget probability density;particle filter;exact computation;poursuite cible;particle filtering scheme bayesian approach multiple target detection target tracking joint multitarget probability density;multiple target detection;multiple target;particle filters;target tracking;sampling methods;muestreo;target detection;object detection;particle filtering numerical methods;estimation bayes	This paper considers the problem of simultaneously detecting and tracking multiple targets. The problem can be formulated in a Bayesian framework and solved, in principle, by computation of the joint multitarget probability density (JMPD). In practice, exact computation of the JMPD is impossible, and the predominant challenge is to arrive at a computationally tractable approximation. A particle filtering scheme is developed for this purpose in which each particle is a hypothesis on the number of targets present and the states of those targets. The importance density for the particle filter is designed in such a way that the measurements can guide sampling of both the target number and the target states. Simulation results, with measurements generated from real target trajectories, demonstrate the ability of the proposed procedure to simultaneously detect and track ten targets with a reasonable sample size	algorithm;analysis of algorithms;approximation;cluster analysis;cobham's thesis;collision detection;computation;computer cluster;filter design;nominal level;particle filter;sampling (signal processing);sensor;simulation;thresholding (image processing)	Mark R. Morelande;Christopher M. Kreucher;Keith Kastella	2007	IEEE Transactions on Signal Processing	10.1109/TSP.2006.889470	sampling;econometrics;particle filter;bayesian probability;pattern recognition;mathematics;statistics	Robotics	40.581610465408225	-27.608158064760104	105307
521bb4bc3a0e3e64922ab70eff3bf98d0f4090b5	a two-stage optimized next-view planning framework for 3-d unknown environment exploration, and structural reconstruction	space exploration;navigation;hidden markov models;heuristic algorithms;simultaneous localization and mapping;planning	In this paper, we present a solution for autonomous exploration and reconstruction in 3-D unknown environments without a priori knowledge of the environments. In our framework, a two-stage heuristic information gain-based next-view planning algorithm is performed to dynamically select and update candidate viewpoints, followed by immediate next-best-view extraction and corresponding motion planning. The two-stage planner consists of a frontier-based boundary coverage planner and a fixed start open travelling salesman problem solver, such that the planner returns an exploration path with the consideration of global optimality in the context of accumulated space information. The effectiveness of our work is evaluated with simulation and experimental tests. The results and comparisons prove our system is able to autonomously explore the 3-D unknown environments and reconstruct the structural model with improved exploration efficiency in terms of path quality and total exploration time.	algorithm;automated planning and scheduling;autonomous robot;heuristic;information gain in decision trees;kullback–leibler divergence;motion planning;simulation;solver;travelling salesman problem	Zehui Meng;Hailong Qin;Ziyue Chen;Xudong Chen;Hao Sun;Feng Lin;Marcelo H. Ang	2017	IEEE Robotics and Automation Letters	10.1109/LRA.2017.2655144	planning;mathematical optimization;navigation;simulation;computer science;engineering;space exploration;machine learning;hidden markov model;simultaneous localization and mapping	Robotics	52.77213622133497	-24.988425349975547	105671
15c60378885b7c5b3168c48250474a4882ba4a76	time-of-flight 3d imaging for mixed-critical systems	automotive engineering;traffic engineering computing computer vision image sensors road safety safety systems software architecture;sensors;three dimensional displays safety cameras automotive engineering sensors hardware;iso 26262 time of flight 3d imaging mixed critical systems computer vision consumer electronics cyber physical systems automotive technology security certified processor systems automotive processing platform multicore concepts hardware software architecture raw time of flight sensor data 3d data hardware accelerators safety critical automotive applications;three dimensional displays;safety;functional safety time of flight 3d sensing automotive applications mixed critical multi core;cameras;hardware	Computer vision is becoming more and more important in the fields of consumer electronics, cyber-physical systems, and automotive technology. Recognizing and classifying one's environment reliably is imperative for safety-critical applications, as they are omnipresent, e.g., in the automotive or aviation domain. For this purpose, the Time-of-Flight imaging technology is suitable, which enables robust and cost-efficient three-dimensional sensing of the environment. However, the resource limitations of safety- and security-certified processor systems as well as complying to safety standards, poses a challenge for the development and integration of complex Time-of-Flight-based applications. Here we present a Time-of-Flight system approach that focuses in particular on the automotive domain. This Time-of-Flight imaging approach is based on an automotive processing platform that complies to safety and security standards. By employing state-of-the-art hardware/software and multi-core concepts, a robust Time-of-Flight system solution is introduced that can be used in a mixed-critical application context. In this work we demonstrate the feasible implementation of the proposed hardware/software architecture by means of a prototype for the automotive domain. Raw Time-of-Flight sensor data is taken and 3D data is calculated with up to 80 FPS without the usage of dedicated hardware accelerators. In a next step, safety-critical automotive applications (e.g., parking assistance) can exploit this 3D data in a mixed-critical environment respecting the needs of the ISO 26262.	3d reconstruction;application domain;automatic parking;computation;computer vision;context (computing);cost efficiency;cyber-physical system;fits;floating point systems;hardware acceleration;image processing;imaging technology;imperative programming;infineon aurix;mathematical optimization;multi-core processor;prototype;requirement;software architecture;stereoscopy	Norbert Druml;Gerwin Fleischmann;Christoph Heidenreich;Andrea Leitner;Helmut Martin;Thomas Herndl;Gerald Holweg	2015	2015 IEEE 13th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2015.7281943	embedded system;real-time computing;simulation;engineering	EDA	44.38694371265315	-37.866463625695964	105872
8cfdf93f363f5366dd2feede0bc207a9d0fdd75e	visual saliency-aware receding horizon autonomous exploration with application to aerial robotics		This paper presents a novel strategy for autonomous visual saliency-aware receding horizon exploration of unknown environments using aerial robots. Through a model of visual attention, incrementally built maps are annotated regarding the visual importance and saliency of different objects and entities in the environment. Provided this information, a path planner that simultaneously optimizes for exploration of unknown space, and also directs the robot's attention to focus on the most salient objects, is developed. Following a two-step optimization paradigm, the algorithm first samples a random tree and identifies the branch maximizing for new volume to be explored. The first viewpoint of this path is then provided as a reference to the second planning step. Within that, a new tree is spanned, admissible branches arriving at the reference viewpoint while respecting a time budget dependent on the robot endurance and its environment exploration rate are found and evaluated in terms of reobserving salient regions at sufficient resolution. The best branch is then selected and executed by the robot, and the whole process is iteratively repeated. The proposed method is evaluated regarding its ability to provide increased attention toward salient objects, is verified to run onboard a small aerial robot, and is demonstrated in a set of challenging experimental studies.	aerial photography;aerobot;algorithm;autonomous robot;entity;iterative method;map;mathematical optimization;motion planning;programming paradigm;random tree;robotics;simulation	Tung Dang;Christos Papachristos;Kostas Alexis	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8460992	salient;control engineering;random tree;visualization;motion planning;horizon;robot;salience (neuroscience);machine learning;engineering;robotics;artificial intelligence	Robotics	50.9503324655135	-26.95163411729742	106096
28fd153e7c966ef92534f3189439dad5afae9a76	a massively parallel keypoint detection and description (mp-kdd) algorithm for high-speed vision chip	keypoint;surf;vision chip;sift;massively parallel;vision chip massively parallel keypoint sift surf;半导体物理	This paper proposes a massively parallel keypoint detection and description (MP-KDD) algorithm for the vision chip with parallel array processors. The MP-KDD algorithm largely reduces the computational overhead by removing all floating-point and multiplication operations while preserving the currently popular SIFT and SURF algorithm essence. The MP-KDD algorithm can be directly and effectively mapped onto the pixel-parallel and row-parallel array processors of the vision chip. The vision chip architecture is also enhanced to realize direct memory access (DMA) and random access to array processors so that the MP-KDD algorithm can be executed more effectively. An FPGA-based vision chip prototype is implemented to test and evaluate our MP-KDD algorithm. Its image processing speed reaches 600–760 fps with high accuracy for complex vision applications, such as scene recognition. 本文提出了一种面向视觉芯片并行图像处理阵列的高效图像特征点提取和描述算法。 该算法基于SIFT特征点检测及SURF特征点描述, 但简化避免了浮点运算以及乘除法操作, 极大地节约了硬件开销和处理时间。 该算法可以直接、 高效地映射到视觉芯片的像素级并行和行并行阵列处理器。 本文实现了基于FPGA的视觉原型, 在其上成功测试了所提出的算法, 达到了600–700帧/秒的较高速度。	central processing unit;data mining;digital image processing;direct memory access;field-programmable gate array;overhead (computing);parallel algorithm;parallel array;pixel;prototype;random access;scale-invariant feature transform;speeded up robust features	Cong Shi;Jie Yang;Liyuan Liu;Nanjian Wu;Zhihua Wang	2014	Science China Information Sciences	10.1007/s11432-014-5174-9	computer vision;parallel computing;surf;computer science;massively parallel;scale-invariant feature transform;computer graphics (images)	HPC	44.33515403042128	-35.32973994427514	106108
b0a086fdc233713fd7c524c26025b7d40084e49f	fast probabilistic collision checking for sampling-based motion planning using locality-sensitive hashing		We present a novel approach to perform fast probabilistic collision checking in high-dimensional configuration spaces to accelerate the performance of sampling-based motion planning. Our formulation stores the results of prior collision queries, and then uses such information to predict the collision probability for a new configuration sample. In particular, we perform an approximate k-NN (k-nearest neighbor) search to find prior query samples that are closest to the new query configuration. The new query sample’s collision status is then estimated according to the collision checking results of these prior query samples, based on the fact that nearby configurations are likely to have the same collision status. We use localitysensitive hashing techniques with sub-linear time complexity for approximate k-NN queries. We evaluate the benefit of our probabilistic collision checking approach by integrating it with a wide variety of sampling-based motion planners, including PRM, lazyPRM, RRT, and RRT∗. Our method can improve these planners in various manners, such as accelerating the local path validation, or computing an efficient order for the graph search on the roadmap. Experiments on a set of benchmarks demonstrate the performance of our method, and we observe up to 2x speedup in the performance of planners on rigid and articulated robots.	approximation algorithm;benchmark (computing);collision detection;graph traversal;hash function;k-nearest neighbors algorithm;locality of reference;locality-sensitive hashing;motion planning;nearest neighbor search;probabilistic roadmap;rendezvous hashing;robot;sampling (signal processing);speedup;time complexity	Jia Pan;Dinesh Manocha	2016	I. J. Robotics Res.	10.1177/0278364916640908	theoretical computer science;machine learning;pattern recognition	Robotics	51.59860583437262	-24.238658392877976	106228
da70af9cb31a2fbaa2bb6a343281161156266091	extending occupancy grid mapping for dynamic environments		In this paper, the commonly used filtering technique occupancy grid mapping for static environments is extended for dynamic environments. The proposed method is able to estimate velocities indirectly. We apply a distribution model of the respective state variable to estimate the cell dynamics by means of prediction and update cycle, as known by standard tracking filters. Therefore, we present a straight forward derivation of the prediction and update rule. Furthermore, we validate our approach by simple one dimensional simulations, and show how it can be extended into a two dimensional world, including the resulting consequences, e.g. in terms of memory requirements.	complexity;graphics processing unit;radar;requirement;sensor;simulation;velocity (software development)	Xiunan Yu;Wolfgang Utschick	2018	2018 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2018.8500362	occupancy grid mapping;vehicle dynamics;state variable;mathematical optimization;filter (signal processing);derivation;sensor fusion;computer science	Robotics	52.33437885493675	-28.145686247655078	106811
0f392d30b3931882a6bf4c781dfeec08cbe8d183	binary image segmentation based on optimized parallel k-means	image segmentation;machine learning;graphics processing units;algorithms	K-means is a classic unsupervised learning clustering algorithm. In theory, it can work well in the field of image segmentation. But compared with other segmentation algorithms, this algorithm needs much more computation, and segmentation speed is slow. This limits its application. With the emergence of general-purpose computing on the GPU and the release of CUDA, some scholars try to implement K-means algorithm in parallel on the GPU, and applied to image segmentation at the same time. They have achieved some results, but the approach they use is not completely parallel, not take full advantage of GPU’s super computing power. K-means algorithm has two core steps: label and update, in current parallel realization of K-means, only labeling is parallel, update operation is still serial. In this paper, both of the two steps in K-means will be parallel to improve the degree of parallelism and accelerate this algorithm. Experimental results show that this improvement has reached a much quicker speed than the previous research.	algorithm;binary image;cuda;cluster analysis;computation;degree of parallelism;emergence;general-purpose markup language;graphics processing unit;image segmentation;k-means clustering;parallel computing;unsupervised learning	Xiaobing Qiu;Yong Zhou;Li Lin	2015		10.1117/12.2197023	computer science;theoretical computer science;machine learning;segmentation-based object categorization;parallel algorithm;image segmentation;computer graphics (images)	AI	42.02125409632874	-34.513756219156946	106987
e3f93402bcbcaaa078d6632b8c4eeb39466ef7a2	non-linear filtering for land vehicle navigation with gps outage	nonlinear filters;non linear filtering;estimation method;non linear model;data fusion;distance measurement;filtering land vehicles navigation global positioning system noise measurement wheels equations satellites displacement measurement filters;global positioning system;particle filter;extended kalman filter nonlinear filtering land vehicle navigation gps outage fusion method partial gps measurement odometric information global estimation method pseudo range measures estimation problem particle filter;extended kalman filter;distance measurement nonlinear filters road vehicles global positioning system;road vehicles	The aim of this paper is to detail a fusion method for land vehicle navigation using (partial) GPS measurement and odometric information. We focus here on a global estimation method which uses the available GPS signal with partial GPS outage. The method deals with odometer and pseudo-range measures that contain the location parameters of the vehicle. This estimation problem is solved by a particle filter, which has proved its benefits compared to an extended Kalman filter, since it deals with non-linear models and statistics.	extended kalman filter;gps signals;global positioning system;linear model;nonlinear system;particle filter;receiver autonomous integrity monitoring;sun outage	Abdelkabir Lahrech;Christophe Boucher;Jean-Charles Noyer	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1399808	gps/ins;global positioning system;particle filter;computer science;machine learning;control theory;sensor fusion;extended kalman filter;moving horizon estimation	Robotics	51.05314888927628	-33.63074399650495	107197
46de2caa6512215d45dc5da3425eabcf63c11b1a	automated robotic parking systems: real-time, concurrent and multi-robot path planning in dynamic environments	dynamic concurrent real time path planning;heuristic search;optimal path;computational complexity;automated robotic parking garage	This paper presents an integrated framework for a suite of dynamic path planning algorithms for real time and concurrent movement of multiple robotic carts across a parking garage floor layout without driving lanes. Planning and search algorithms were formulated to address major aspects of automation for storage and retrieval of cars loaded onto robotic mobile carts. Path planning algorithms including A*, D* Lite and Uniform Cost Search were implemented and integrated within a unified framework to guide the robotic carts from a starting point to their destination during the storage and the retrieval processes. For the circumstances where there is no clear path for a car being stored or retrieved, a procedure was developed to unblock the obstacles in the path. A policy that minimizes obstructions was defined for assigning the parking spots on a given floor for arriving cars. Performance evaluation of the overall proposed system was done using a multithreaded software application. A variety of rectangular parking lot layouts including those with 20 × 20, 20 × 40, 30 × 40, and 40 × 40 parking spaces were considered in the simulation study. Performance metrics of path length, planning or search time and memory space requirements were monitored. Simulation results demonstrate that the proposed design facilitates near optimal paths, and is able to handle tens of concurrent requests in real time and in the presence of immobilized carts.	a* search algorithm;adobe flash lite;automated planning and scheduling;dspace;flash cartridge;heuristic (computer science);motion planning;multi-storey car park;multithreading (computer architecture);performance evaluation;real-time clock;requirement;robot;simulation;thread (computing);unified framework	Gürsel Serpen;Chao Dou	2014	Applied Intelligence	10.1007/s10489-014-0598-x	real-time computing;simulation;heuristic;any-angle path planning;computer science;parking guidance and information;computational complexity theory	Robotics	51.934776273497405	-27.846132001318104	107527
0993d6fc4e9a84dfd38fba99b11af21932e87533	a comparison of mht and 2d assignment algorithm for tracking with an airborne pulse doppler radar	test cases mht 2d assignment algorithm airborne pulse doppler radar performance data association target tracking multiple hypothesis tracker joint maximum likelihood solution sensor computational complexity mprf mode;target tracking airborne radar doppler radar radar tracking;maximum likelihood;radar tracking;data association;radar tracking doppler radar filters radar cross section equations antenna measurements target tracking australia radar measurements radar detection;doppler radar;airborne radar;target tracking	The paper compares the performance of two algorithms for data association in target tracking: the multiple hypothesis tracker (MHT) and the two dimensional assignment (2DA) method which follows from the joint maximum likelihood solution. The results are presented in the context of an airborne pulse Doppler radar as a sensor. The MHT (which is computationaly more complex) clearly outperforms the 2DA in the MPRF mode. The improvement in the MPRF mode, however, is minimal for the test cases considered.	airborne ranger;algorithm;mhtml;pulse-doppler radar	Branko Ristic	1999		10.1109/ISSPA.1999.818182	early-warning radar;man-portable radar;computer vision;continuous-wave radar;radar tracker;radar engineering details;radar lock-on;radar configurations and types;fire-control radar;passive radar;bistatic radar;low probability of intercept radar;monopulse radar;pulse-doppler radar;mathematics;3d radar;maximum likelihood;radar imaging;side looking airborne radar;statistics	Robotics	50.186765447742296	-33.4725379713576	108056
29312a0e8eefc8c2e2099e6973ad62fc4d586c1f	concurrent map building and localization with landmark validation	robot sensing systems;concurrent map building;sensor phenomena and characterization;convergence;cml;unknown environment;map building;mobile robots sensor phenomena and characterization robot sensing systems feature extraction pollution measurement communication industry mobile communication convergence navigation stochastic processes;mobile robot;localization;comunicacion de congreso;pollution measurement;slam;reference frame;mobile robots;robotics;communication industry;navigation;feature extraction mobile robot navigation concurrent map building localization landmark validation unknown environment map model reference frame landmark quality assessment map covariance matrix convergence stochastic models;quality assessment;robot vision;stochastic processes;covariance matrices;landmark validation;feature extraction;robots;mobile robot navigation;mobile communication;map model;covariance matrices mobile robots navigation robot vision feature extraction stochastic processes;automation robots;landmark quality assessment;map covariance matrix convergence;stochastic models	This communication addresses the issue of concurrent map building and localization (CML) for a mobile robot in an unknown environment. The proposed solution extends over previous contributions in that the environment must not be static, nor the landmarks be uniquely identifiable. To this aim we introduce a map model that includes not only the robot and landmark locations in a reference frame, but also a model for landmark quality assessment. Convergence of the map covariance is preserved in the new map model.	algorithm;correspondence problem;extended kalman filter;mobile robot;reference frame (video);robotic mapping	Juan Andrade-Cetto;Alberto Sanfeliu	2002		10.1109/ICPR.2002.1048396	mobile robot;computer vision;simulation;computer science;robotics	Robotics	52.23438047618209	-34.82254473031929	108247
daa6f5b737a447b6f290f7db98310c17ee51ed35	a collision avoiding six legged walking machine based on kohonen feature maps			feature model;self-organizing map	Paolo Ferrara;Alois Ferscha;Günter Haring	1992				Robotics	47.34109585274223	-30.525210652037874	108681
c4f8474c3f3e7b4dab783d4ad2f7c28230b0e85a	parallel computation for stereovision obstacle detection of autonomous vehicles using gpu	obstacle detection;image processing;autonomous vehicle;real time;disparity;parallelism;parallel computer;graphic processing unit;graphics processing unit;high speed	Due to the parallelism on general-purpose computation, the graphics processing unit (GPU) is applied in autonomous vehicles and a stereovision obstacle detection system is developed. We first perform census transform on the rectified stereo image pair; then employ the epipolar constraint to facilitate the visual correspondence matching. Based on the dense disparity map, the 3D coordinate of each pixel is calculated, according to which obstacles are identified. Furthermore, several techniques are listed for exploring the specific functionalities of GPU to boost the overall performance. A prototype system is finally implemented and integrated in the onboard PC of autonomous vehicles. Experimental results validate the real-time accuracy under various illuminations and road conditions. Since the low level image-processings are run by GPU in parallel, the proposed design not only merits high speed and efficiency, but also frees up CPU so as to focus on the decision and control.	computation;graphics processing unit;parallel computing;stereopsis	Zhi-Yu Xu;Jie Zhang	2010		10.1007/978-3-642-15621-2_21	embedded system;computer vision;image processing;computer science;computer graphics (images)	Robotics	43.855677977331794	-36.404580217294296	108806
aa264cf562372b23198e7791fa9cf7f8f8fd1a07	multi-threaded compression of earth observation time-series data	hdf5;multi threading;data compression;time series;modis;hdf	ABSTRACTEarth observation data are typically compressed using general-purpose single-threaded compression algorithms that operate at a fraction of the bandwidth of modern storage and processing systems. We present evidence that recently developed multi-threaded compression codecs offer substantial benefits over widely used single-threaded codecs in terms of compression efficiency when applied to a selection of moderate resolution imaging spectroradiometer (MODIS) datasets stored in the HDF5 format. Compression codecs from the LZ77 and Rice families are shown to vary in efficacy when applied to different MODIS data products, highlighting the need for compression strategies to be tailored to different classes of data. We also introduce LPC-Rice, a new multi-threaded codec, that performs particularly well when applied to time-series data.	time series	Derick Swanepoel;Frans van den Bergh	2017	Int. J. Digital Earth	10.1080/17538947.2017.1301580	moderate-resolution imaging spectroradiometer;time series;remote sensing;hierarchical data format;codec;multithreading;data compression;earth observation;computer science;bandwidth (signal processing)	HCI	44.70146544538562	-24.613669380835734	108844
6ddaf58a28a1d6376b488afc57ba6347f04a33f2	gps/odometry/map fusion for vehicle positioning using potential function	digital maps;gps;visual odometry;potential function;vehicle localization	In this paper, we present a fusion approach to localize urban vehicles by integrating a visual odometry, a low-cost GPS, and a two-dimensional digital road map. Distinguished from conventional sensor fusion methods, two types of potential functions (i.e. potential wells and potential trenches) are proposed to represent measurements and constraints, respectively. By choosing different potential functions according to data properties, data from various sensors can be integrated with intuitive understanding, while no extra map matching is required. The minimum of fused potential, which is regarded as position estimation, is confined such that fast minimum searching can be achieved. Experiments under realistic conditions have been conducted to validate the satisfactory positioning accuracy and robustness compared to pure visual odometry and map matching methods.	global positioning system;map (higher-order function);odometry	Rui Jiang;Shuai Yang;Shuzhi Sam Ge;Xiaomei Liu;Han Wang;Tong Heng Lee	2018	Auton. Robots	10.1007/s10514-017-9646-9	computer science;robustness (computer science);artificial intelligence;computer vision;odometry;global positioning system;road map;visual odometry;digital mapping;sensor fusion;map matching	Robotics	52.747314997295014	-35.75497258810124	109089
64d03bc06fd8ac5fe8dad82d01e595fae5fb96e0	real-time gradient vector flow on gpus using opencl	gradient vector flow;gpu;opencl	The Gradient Vector Flow (GVF) is a feature-preserving spatial diffusion of gradients. It is used extensively in several image segmentation and skeletonization algorithms. Calculating the GVF is slow as many iterations are needed to reach convergence. However, each pixel or voxel can be processed in parallel for each iteration. This makes GVF ideal for execution on Graphic Processing Units (GPUs). In this paper, we present a highly optimized parallel GPU implementation of GVF written in OpenCL. We have investigated memory access optimization for GPUs, such as using texture memory, shared memory and a compressed storage format. Our results show that this algorithm really benefits from using the texture memory and the compressed storage format on the GPU. Shared memory, on the other hand, makes the calculations slower with or without the other optimizations because of an increased kernel complexity and synchronization. With these optimizations our implementation can process 2D images of large sizes (5122) in real-time and 3D images (2563) using only a few seconds on modern GPUs.	16-bit;algorithm;central processing unit;fastest;gradient;graphics processing unit;image segmentation;iteration;map (parallel pattern);mathematical optimization;multi-core processor;opencl api;pixel;real-time clock;real-time transcription;shared memory;texture memory;topological skeleton;voxel	Erik Smistad;Anne C. Elster;Frank Lindseth	2012	Journal of Real-Time Image Processing	10.1007/s11554-012-0257-6	parallel computing;computer science;theoretical computer science;computer graphics (images)	HPC	42.177816184560214	-34.50748551009925	109140
44dd447c9f39b6723d032a05d8c9df47d42c38e4	multi-tasking slam	rao blackwellized particle filter;search and rescue;point to point navigation;sequential decision tasks;sequential decision making;partially observed markov decision process;cost function;uncertainty;slam algorithm;path planning;search method;multiprogramming;slam robots markov processes multiprogramming path planning;multitasking slam;navigation;robotics literature;trajectory;posterior distribution;rapidly exploring random tree;decision theoretic;simultaneous localization and mapping;refueling;motion planning;simultaneous localization and mapping robots robotics and automation navigation uncertainty motion planning process planning usa councils computer science observability;target following;planning;diverse navigation tasks;markov processes;online pomdp approach;slam robots;simulation environment;diverse navigation tasks multitasking slam simultaneous localization and mapping robotics literature target following search and rescue point to point navigation refueling sequential decision tasks slam algorithm online pomdp approach	The problem of simultaneous localization and mapping (SLAM) is one of the most studied in the robotics literature. Most existing approaches, however, focus on scenarios where localization and mapping are the only tasks on the robot's agenda. In many real-world scenarios, a robot may be called on to perform other tasks simultaneously, in addition to localization and mapping. These can include target-following (or avoidance), search-and-rescue, point-to-point navigation, refueling, and so on. This paper proposes a framework that balances localization, mapping, and other planning objectives, thus allowing robots to solve sequential decision tasks under map and pose uncertainty. Our approach combines a SLAM algorithm with an online POMDP approach to solve diverse navigation tasks, without prior training, in an unknown environment.	approximation algorithm;automated planning and scheduling;cobham's thesis;computation;depletion region;experiment;open research;partially observable markov decision process;point-to-point protocol;requirement;robot;robotics;sensor;simultaneous localization and mapping;theory	Arthur Guez;Joelle Pineau	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509969	computer vision;simulation;computer science;engineering;artificial intelligence;machine learning;motion planning;simultaneous localization and mapping	Robotics	52.12837015024757	-26.08159313791665	109462
1a17c0087d960e70a5ffc6840934ecc1377216dc	realtime and robust motion tracking by matched filter on cmos+fpga vision system	image sampling;vision system;cmos integrated circuits;matched filter algorithm realtime motion tracking robust motion tracking cmos fpga vision system positional signal angular signal mechanical system control visual feedback 2d fourier transform;control systems;sensor phenomena and characterization;angular signal;fourier transform;logic design;realtime motion tracking;matched filter algorithm;motion tracking;computer vision;mechanical system control;robust motion tracking;machine vision;fourier transforms;mechanical sensors;visual features;target tracking cmos integrated circuits computer vision field programmable gate arrays fourier transforms logic design;matched filters;visual feedback;robustness;robustness matched filters machine vision target tracking control systems mechanical systems mechanical sensors sensor phenomena and characterization sampling methods image sampling;2d fourier transform;positional signal;field programmable gate arrays;target tracking;matched filter;sampling methods;cmos fpga vision system;mechanical systems	This paper describes realtime and robust tracking of a planar motion target by matched filter implemented on the CMOS+FPGA vision system. It is required to obtain positional and angular signals around 1,000 Hz to control a mechanical system. A vision sensor must obtain visual features of a target object, synchronizing its sampling rate to the sampling rate of the control. The CMOS+FPGA vision system has been proposed to realize 1,000 Hz visual feedback. Matched filter can compute the position of a target robustly against occlusion, change of illumination, and background texture but requires much computation time since it includes 2D Fourier transform of images. Thus, matched filter algorithm is implemented on the system so that the matched filter can be performed in realtime. First we briefly introduce the CMOS+FPGA vision system. Second, we summarize the algorithm of matched filter to describe the design of the circuit performing matched filter on an FPGA. Finally, we show the experimental results of planar motion tracking by matched filter implemented on the system.	angularjs;computation;field-programmable gate array;matched filter;peterson's algorithm;sampling (signal processing);time complexity	Kazuhiro Shimizu;Shinichi Hirai	2007	Proceedings 2007 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2007.363082	control engineering;fourier transform;computer vision;electronic engineering;machine vision;computer science;filter design;matched filter	Robotics	45.48358092455749	-35.14238885148142	109725
10ff61e6c2a99d8aafcf1706f3e88c7e2dfec188	nonparametric belief propagation	modelizacion;distributed system;filtrage gauss;processus gauss;algoritmo aleatorizado;reseau capteur;vision ordenador;pistage;systeme reparti;estimation mouvement;methode particulaire;metodo monte carlo;enfoque credal;analisis estadistico;image processing;melange loi probabilite;proceso markov;position transducteur;pervasive computing;estimacion movimiento;cinematica;rastreo;procesamiento imagen;methode monte carlo;gaussian filtering;mixed distribution;motion estimation;credal approach;metodo particula;intelligence artificielle;algorithme randomise;time series;filtrado gaussiano;kinematics;traitement image;computer vision;informatica difusa;particle method;modelisation;reseau bayes;red sensores;sistema repartido;statistical analysis;red bayes;informatique diffuse;processus markov;monte carlo method;analyse statistique;cinematique;serie temporelle;inferencia;markov process;sensor array;bayes network;serie temporal;randomized algorithm;artificial intelligence;mezcla ley probabilidad;vision ordinateur;inteligencia artificial;gaussian process;proceso gauss;modeling;posicion transductor;approche credibiliste;transducer position;inference;tracking	Continuous quantities are ubiquitous in models of real-world phenomena, but are surprisingly difficult to reason about automatically. Probabilistic graphical models such as Bayesian networks and Markov random fields, and algorithms for approximate inference such as belief propagation (BP), have proven to be powerful tools in a wide range of applications in statistics and artificial intelligence. However, applying these methods to models with continuous variables remains a challenging task. In this work we describe an extension of BP to continuous variable models, generalizing particle filtering, and Gaussian mixture filtering techniques for time series to more complex models. We illustrate the power of the resulting nonparametric BP algorithm via two applications: kinematic tracking of visual motion and distributed localization in sensor networks.	approximation algorithm;artificial intelligence;backpropagation;bayesian network;belief propagation;graphical model;markov chain;markov random field;particle filter;software propagation;time series	Erik B. Sudderth;Alexander T. Ihler;Michael Isard;William T. Freeman;Alan S. Willsky	2010	Commun. ACM	10.1145/1831407.1831431	econometrics;kinematics;systems modeling;image processing;computer science;artificial intelligence;time series;motion estimation;bayesian network;gaussian process;mathematics;tracking;markov process;randomized algorithm;sensor array;statistics;belief propagation;monte carlo method	Vision	43.67672912653852	-28.926223460747373	110001
d84c676bbf90b2e90f9fe32e5caef41e0a663388	multi-agent search using sensors with heterogeneous capabilities	aeronautical engineering;aerospace engineering formerly	In this thesis we address the problem of multi-agent search. We formulate two deploy and search strategies based on optimal deployment of agents in search space so as to maximize the search effectiveness in a single step. We show that a variation of centroidal Voronoi configuration is the optimal deployment. When the agents have sensors with different capabilities, the problem will be heterogeneous in nature. We introduce a new concept namely, generalized Voronoi partition in order to formulate and solve the heterogeneous multi-agent search problem. We address a few theoretical issues such as optimality of deployment, convergence and spatial distributedness of the control law and the search strategies. Simulation experiments are carried out to compare performances of the proposed strategies with a few simple search strategies.	sensor	K. R. Guruprasad	2008		10.1145/1402782.1402788	mathematical optimization;simulation;computer science;operations research	Robotics	52.18305239523037	-25.55419677078148	110113
f9a92a4371fd56a7ddbbf3b19dd50a8f25c02551	parallel signal processing system for a super high-resolution digital camera	high resolution;signal processing	The technology of the digital still camera has recently achieved remarkable progress. The number of pixels in the main stream of consumer-use cameras has reached the 3 million-range, with 5 million the maximum. And for professional-use, the number of pixels has reached the 16 million. Digital still cameras convert signal data generated from these pixel CCD's to luminosity signals and chromaticity signals, which are compressed in conformity with JPEG standards by a signal processing circuit incorporated in the cameras. The increase in the number of pixels results in the increase in the volume of signal processing, as a matter of course. In order to prevent increase in the recording time required for the increased volume of signal processing, the signal processing circuit has been designed and developed to process the signal at a higher rate, with the application of cutting-edge fine-fabrication technology of semiconductors. However, fine-fabrication with a design rule smaller than 0.09 micron needs a vast amount of investment for production, which might lead to a delay in the progress of high-rate processing. Therefore, a new technique of high-rate processing is required that does not depend on smaller scale fine-fabrication. In this paper, we propose a technique for achieving the required high-rate signal processing by the use of parallel connection of the signal processing circuits necessary for handling signals of the digital cameras of high pixel-number CCD's. At the same time, we also report the excellent results obtained with a prototype camera made in accordance with our proposal.	charge-coupled device;conformity;digital camera;jpeg;pixel;prototype;semiconductor;series and parallel circuits;signal processing	Kaoru Adachi;Toru Nishimura;Kazuki Iwabe	2003			digital camera;digital image processing;camera auto-calibration;digital signal processing;signal processing;computer vision;stereo camera;camera resectioning;image resolution;artificial intelligence;computer science	EDA	43.38476109089668	-25.098776755167446	110236
c62942a1f7c07f9459e855b0ea7583b40df3c188	path planning for redundant manipulator without explicit inverse kinematics solution	manipulators;probability;high dimensionality;sampling methods collision avoidance end effectors probability random processes redundant manipulators;path planning;random sampling;joints;kinematics;path planning kinematics manipulators orbital robotics space technology robotics and automation robot sensing systems mechatronics biomimetics tree data structures;robot manipulator;configuration space;rapidly exploring random tree;random processes;inverse kinematics;planning;collision avoidance;sampling methods;bidirectional rrt connect algorithm path planning redundant robotic manipulator random sampling probabilistic roadmap method rapidly exploring random tree end effector collision free configuration reachable goal configuration robot joint angle joint limit self collision inverse kinematics tree based data structure;data structure;redundant manipulators;algorithm design and analysis;probabilistic roadmap method;end effectors	Path planning for redundant robotic manipulators received continuous interest in the past decades. Most efforts focused on random sampling-based methods, such as Probabilistic Roadmap method (PRM) and Rapidly-Exploring Random Tree (RRT), since they are suitable for planning in high-dimensional configuration space. Given the workspace goal position and orientation of the end-effector, however, explicitly calculating a collision-free and reachable goal configuration for robot joint angles in the presence of joint limits and self-collisions is not a trivial work. The difficulty forms a bottleneck for the broader applicability of the randomized path planning methods. In this paper, a novel two-stage approach is presented to implicitly solve the formation of inverse kinematics (IK) problems, which employs a variant of RRT to embed the process of IK calculation into construction and exploration of the tree-based data structure. Combined with bidirectional RRT-Connect algorithm, the two-stage approach can efficiently address the path planning problem for general redundant manipulators. The algorithm has been implemented and several 2-D and 3-D experiments demonstrate the effectiveness of the method.	adaptive algorithm;automated planning and scheduling;bidirectional search;computational complexity theory;data structure;experiment;heuristic (computer science);hot swapping;inverse kinematics;maxima and minima;monte carlo method;motion planning;nearest neighbor search;probabilistic roadmap;randomized algorithm;rapidly-exploring random tree;robot end effector;sampling (signal processing);statistical relational learning;workspace	Wei Wang;Yan Li	2009	2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2009.5420547	control engineering;sampling;mathematical optimization;simulation;data structure;any-angle path planning;computer science;artificial intelligence;mathematics;statistics	Robotics	52.23043765798195	-24.262888493258043	110549
e57be63196a0966a5639e3ef8a90331da627046f	vision-based mobile robot's slam and navigation in crowded environments	map building;mobile robots;three dimensional;stability;distance measurement;navigation;robot vision;trajectory;stereo image processing distance measurement feature extraction image sequences mobile robots navigation robot vision slam robots stability;scale invariant feature transform;three dimensional displays;local features;feature extraction;stability vision based mobile robot slam robot navigation crowded environment simultaneous localization and mapping vision sensors train station shopping mall 3d feature point extraction static objects dynamic objects feature point recognition landmark recognition map building self localization sequential vision images odometry;stereo image processing;simultaneous localization and mapping;cost effectiveness;feature extraction navigation simultaneous localization and mapping trajectory buildings three dimensional displays;slam robots;buildings;image sequences	Vision-based mobile robot's simultaneous localization and mapping (SLAM) and navigation has been the source of countless research contributions because of rich sensory output and cost effectiveness of vision sensors. However, existing methods of vision-based SLAM and navigation are not effective for robots to be used in crowded environments such as train stations and shopping malls, because when we extract feature points from an image in crowded environments, many feature points are extracted from not only static objects but also dynamic objects such as humans. By recognizing all such feature points as landmarks, the algorithm collapses and errors occur in map building and self-localization. In this paper, we propose a SLAM and navigation method that is effective even in crowded environments by extracting robust 3D feature points from sequential vision images and odometry. By using the proposed method, we can eliminate unstable feature points extracted from dynamic objects and perform SLAM and navigation stably. We present experiments showing the utility of our approach in crowded environments, including map building and navigation.	3d film;algorithm;coexist (image);control theory;experiment;image sensor;mobile robot;odometry;simultaneous localization and mapping	Hiroshi Morioka;Sangkyu Yi;Osamu Hasegawa	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6094847	mobile robot;three-dimensional space;computer vision;navigation;simulation;cost-effectiveness analysis;stability;feature extraction;computer science;artificial intelligence;trajectory;scale-invariant feature transform;mobile robot navigation;simultaneous localization and mapping	Robotics	50.50025491586292	-37.80870165015299	110596
35c60fafb3b492908c1d77341be3f6b0fadaf7b3	crf-filters: discriminative particle filters for sequential state estimation	robot motion crf filters discriminative particle filters sequential state estimation robotics laser range finder;sensor model;state estimation hidden markov models particle filtering numerical methods robots;sequential state estimation;dynamic system;robotics;state estimation;particle filters state estimation particle beams laser beams laser noise filtering training data robot sensing systems laser modes robot motion;hidden markov models;laser range finder;particle filter;robots;conditional random field;robot motion;laser scanning;discriminative particle filters;crf filters;discriminative model;particle filtering numerical methods	"""Particle filters have been applied with great success to various state estimation problems in robotics. However, particle filters often require extensive parameter tweaking in order to work well in practice. This is based on two observations. First, particle filters typically rely on independence assumptions such as """"the beams in a laser scan are independent given the robot's location in a map"""". Second, even when the noise parameters of the dynamical system are perfectly known, the sample-based approximation can result in poor filter performance. In this paper we introduce CRF-filters, a novel variant of particle filtering for sequential state estimation. CRF-filters are based on conditional random fields, which are discriminative models that can handle arbitrary dependencies between observations. We show how to learn the parameters of CRF-filters based on labeled training data. Experiments using a robot equipped with a laser range-finder demonstrate that our technique is able to learn parameters of the robot's motion and sensor models that result in good localization performance, without the need of additional parameter tweaking."""	algorithm;approximation;ct scan;conditional random field;discriminative model;dynamical system;graph (discrete mathematics);graphical model;ground truth;hidden markov model;image noise;internationalization and localization;machine learning;markov chain;mathematical optimization;particle filter;perceptron;quantum decoherence;robot;robotic mapping;robotics;sensor;tweaking	Benson Limketkai;Dieter Fox;Lin Liao	2007	Proceedings 2007 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2007.363957	laser scanning;robot;monte carlo localization;computer vision;simulation;particle filter;computer science;engineering;dynamical system;machine learning;robotics;conditional random field;discriminative model	Robotics	52.18734422962732	-35.35876505893527	110625
2494eee4e4e744a5ee0028dbf37b0ba42eeb85c4	spatial navigation with uncertain deviations	spatial navigation	We consider geometrical scenes with obstacles and landmarks that can’t necessarily be distinguished and generalize the notion of panoramas (Sch93; Her94), introduced in the qualitative Spatial Reasoning (QSR) approaches to robot navigation. We study various notions of motion strategies in the accessibility graph associated with the local panoramas under uncertain deviations, a natural model of uncertainty for motion planning and navigation. We show that randomized motion strategies can be better than deterministic ones with a finite memory and stress the usefulness of random decisions for qualitative spatial reasoning.	accessibility;graph labeling;motion planning;randomized algorithm;robotic mapping;spatial navigation;spatial–temporal reasoning	Michel de Rougemont;Christoph Schlieder	1997			spatial memory;computer vision;simulation;computer science	Robotics	50.660757681965535	-24.138249528357107	110864
bebca1fc6be4f2005f62b1b65b34903280fd4eeb	statistical environment representation for navigation in natural environments	dynamic change;random closed set;statistical model;natural environment;mobile robot navigation	In this paper we present a novel approach to mobile robot navigation based on environment representation by statistical models. Natural unstructured scenes are interpreted as realisations of Random Closed Sets (RCS), whose global characteristics are mapped. Contrary to the feature based approach, this environment representation does not require the existence of outstanding objects in the workspace, and is robust with respect to small dynamic changes. We address the relocalisation problem assuming that a statistical model, serving as a map of the environment, is available a priori. Simulation results demonstrate the feasibility of our approach.	mobile robot;robotic mapping;simulation;statistical model;workspace	Stefan Rolfes;Maria-João Rendas	2002	Robotics and Autonomous Systems	10.1016/S0921-8890(02)00260-9	statistical model;computer vision;simulation;computer science;artificial intelligence;machine learning;natural environment;mobile robot navigation	Robotics	51.52741402236612	-36.21510435231257	110865
3520012fd6facb46245aaac5e8e5298ebc5ab2f1	perception and action cycle for cognitive robotics		This paper describes the perceptual system based on invariant concept and the action system to aquire a better perception based on the perceiving-acting cycle to grasp an unknown object for a service robot. A service robot should take action in order to fulfill specific tasks in real time, even in an unknown environment. To work a service robot in an unknown environment, we focus on the perception system and the action system based on perceiving-acting cycle concept in ecological psychology. For the perception system, we focus on a direct perception concept. We have proposed the invariant detection based on the direct perception concept. To calculate the invariant information, we consider the relation with the principal moment of inertia of a detected object. For the action system, we focus on a perceiving-acting cycle concept. We have proposed the prediction method to acquire the better perception using the inertia tensor of a detected object. We have performed the experiments in an unknown environment which assumes to grasp an object. We discuss the cyclic system between perception and action for a robot.	cognitive robotics;ecological psychology;experiment;service robot	Hiroyuki Masuta;Tatsuo Motoyoshi;Kei Sawai;Ken'ichi Koyanagi;Toru Oshima	2017	2017 International Symposium on Micro-NanoMechatronics and Human Science (MHS)	10.1109/MHS.2017.8305180	control engineering;perceptual system;robot;machine learning;invariant (mathematics);cognitive robotics;service robot;ecological psychology;materials science;perception;grasp;artificial intelligence	Robotics	47.96998305612004	-31.662606732891604	110881
56e8c4e660f16c3653e51dde0762693683068814	estimating sensor performance and target population size with multiple sensors	bayes methods;inference mechanisms;sensor models sensor performance estimation target population size bayesian inference multisensor target detections closed form approximate bayesian estimator satellite detection automatic identification system global map sensor detection performance fusion algorithms;sensor fusion bayes methods inference mechanisms object detection;sensor fusion;target tracking radar tracking sociology statistics surveillance bayesian methods marine vehicles;object detection	A Bayesian inference approach to exploit multisensor target detections for the estimation of sensor performance is presented. Additionally, it is shown that a closed form approximate Bayesian estimator for the target population can be derived. This Bayesian method is applied to real data in the case of satellite detection of Automatic Identification System messages resulting in a global map of sensor performance. The resulting sensor detection performance estimates are also suitable for the tuning of tracking and fusion algorithms which make use of sensor models.	approximation algorithm;automatic identification and data capture;bayesian network;coefficient;correspondence problem;dvb-s2;like button;marginal model;positive feedback;sensor;transponder	Giuseppe Papa;Steven Horn;Paolo Braca;Karna Bryan;Gianmarco Romano	2012	2012 15th International Conference on Information Fusion		geography;pattern recognition;data mining;statistics	Robotics	51.12511709079435	-34.318614344356064	111014
2c169aac9b54dd02690b46dc57130fb9429a32ff	seamless execution of action sequences	performance measure;optimisation;evolutionary computation;motion control;motion planning action sequence execution robot motion evolutionary optimization jagged motion abstract actions subgoal refinement action sequence optimization action model;path planning;action model;robotics and automation humans motion planning robot motion animals motion analysis intelligent systems intelligent robots motion control robot control;side effect;action sequence optimization;robots;abstract actions;subgoal refinement;jagged motion;robots evolutionary computation motion control optimisation path planning;motion planning;robot motion;action sequence execution;evolutionary optimization	One of the most notable and recognizable features of robot motion is the abrupt transitions between actions in action sequences. In contrast, humans and animals perform sequences of actions efficiently, and with seamless transitions between subsequent actions. This smoothness is not a goal in itself, but a side-effect of the evolutionary optimization of other performance measures. In this paper, we argue that such jagged motion is an inevitable consequence of the way human designers and planners reason about abstract actions. We then present subgoal refinement, a procedure that optimizes action sequences. Sub-goal refinement determines action parameters that are not relevant to why the action was selected, and optimizes these parameters with respect to expected execution performance. This performance is computed using action models, which are learned from observed experience. We integrate subgoal refinement in an existing planning system, and demonstrate how requiring optimal performance causes smooth motion in three robotic domains.	action selection;bridging (networking);cobham's thesis;mathematical optimization;program optimization;refinement (computing);robot;seamless3d	Freek Stulp;Wolfram Koska;Alexis Maldonado;Michael Beetz	2007	Proceedings 2007 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2007.364043	computer vision;simulation;computer science;artificial intelligence;motion planning;evolutionary computation	Robotics	50.60354933038373	-26.634282292106686	111371
64af13c604328c522a7c24609a371958f8ebcd55	pomdp-based online target detection and recognition for autonomous uavs	intelligence artificielle	This paper presents a target detection and recognition mission by an autonomous Unmanned Aerial Vehicule (UAV) modeled as a Partially Observable Markov Decision Process (POMDP). The POMDP model deals in a single framework with both perception actions (controlling the camera’s view angle), and mission actions (moving between zones and flight levels, landing) needed to achieve the goal of the mission, i.e. landing in a zone containing a car whose model is recognized as a desired target model with sufficient belief. We explain how we automatically learned the probabilistic observation POMDP model from statistical analysis of the image processing algorithm used on-board the UAV to analyze objects in the scene. We also present our “optimize-while-execute” framework, which drives a POMDP sub-planner to optimize and execute the POMDP policy in parallel under action duration constraints, reasoning about the future possible execution states of the robotic system. Finally, we present experimental results, which demonstrate that Artificial Intelligence techniques like POMDP planning can be successfully applied in order to automatically control perception and mission actions hand-inhand for complex time-constrained UAV missions.	aerial photography;algorithm;artificial intelligence;autonomous robot;experiment;hardware-in-the-loop simulation;image processing;markov chain;on-board data handling;partially observable markov decision process;semantics (computer science);unmanned aerial vehicle	Caroline Ponzoni Carvalho Chanel;Florent Teichteil-Königsbuch;Charles Lesire	2012		10.3233/978-1-61499-098-7-955	computer vision;simulation;computer science;artificial intelligence	AI	51.32611825017589	-31.245458618138876	111462
9d4d96e2f1ada254c760260dda517561002e7a41	mobile robot navigation using the mathematical model of the sensing system	remote control;mobile robot;system dynamics;mobile robots;intelligent control;optimal control;control system;robot control;mobile robot navigation;computerised navigation mobile robots telerobotics neurocontrollers intelligent control optimal control;mathematical model;telerobotics;mobile robots navigation mathematical model communication system control neural networks control systems robot control mobile communication clocks workstations;neurocontrollers;robot neural network mobile robot navigation mathematical model sensing system remotely controlled mobile robot unstructured environment communication time inner clock control system short term autonomy external stimuli remote control station workstation system dynamics robot environment optimal control strategy free parameters remote control action;neural network;computerised navigation	The paper presents a remotely controlled mobile robot able to work in an unstructured environment. The main constraint is represented by the communication time, which was evaluated of two orders higher than the inner clock of the control system. This difficulty has been avoided by giving a short-term autonomy to the system. Namely, the control system was configured as a neural network on board the robot that reacts to external stimuli. A copy of the network is in the remote control station. Here, a workstation periodically identifies the system dynamics, the robot environment and the optimal control strategy. The strategy is translated in a proper selection of the free parameters of the neural network. The values of these parameters are transmitted as remote control action to the robot neural network.	mathematical model;mobile robot;robotic mapping	Bruno Apolloni;Adrian Moise	2000		10.1109/ICSMC.2000.884390	mobile robot;computer vision;simulation;computer science;control system;artificial intelligence;social robot;mobile robot navigation;artificial neural network;intelligent control	Robotics	50.596452628766315	-29.33188641263672	111614
63995d9ced522745ff0925aa64e8686cfa23e6db	outdoor robot navigation based on view-based global localization and local navigation		This paper describes a view-based outdoor navigation method. Navigation in outdoor can be divided into two levels; the global level deals with localization and subgoal selection, while the local level deals with safe navigation in a local area. We adopt an improved version of SeqSLAMmethod for global-level localization, which can cope with changes of robot’s heading and speed as well as view changes using very wideangle images and a Markov localization scheme. The global level provides the direction to move and the local level repeatedly sets subgoals with local mapping using 3D range sensors. We implemented these global and local level methods on a mobile robot and conducted on-line navigation experiments.	course (navigation);experiment;markov chain;mobile robot;online and offline;robotic mapping;sensor	Yohei Inoue;Jun Miura;Shuji Oishi	2016		10.1007/978-3-319-48036-7_63	robot;area navigation;mobile robot;computer vision;mobile robot navigation;avm navigator;artificial intelligence;computer science;markov chain	Robotics	53.52318047935685	-33.183621532365876	111642
2d414ae7dd7ee9340168cd20b08ce45a9b46a6a1	algorithms for biological cell sorting with a lab-on-a-chip	erbium;optimisation;sensors;sorting;combinatorial optimization problem;lab on achip algorithm optimization cell sorting;electric field;data mining;strontium;combinatorial optimization problems biological cell sorting lab on a chip microorganisms electric field;algorithm;cell sorting;optimisation lab on a chip;biological cells sorting lab on a chip microorganisms stem cells cells biology voltage niobium computer science isolation technology;lab on a chip;optimization;microorganisms;lab on achip	Automatic cell sorting and isolation for recovery of such live cells, mostly microorganisms, is a challenging task. Lab-on-a-Chip devices implemented as cell arrays are used for this purpose. For an abstract model of the problem, we can assume the cell array to be represented by a matrix where each cell can be any of the three types: empty, good (or desired) and bad (or undesired). The problem is to separate the good and the bad cells by pushing them under electric field to their respective receptors at the corners of the cell array. We address some combinatorial optimization problems related to this biological phenomenon.	apx;algorithm;approximation algorithm;cell (microprocessor);color;combinatorial optimization;hardness of approximation;heuristic;mathematical optimization;optimization problem;pokémon red;polynomial-time approximation scheme;sorting;turing completeness	Arijit Ghosh;Rushin Shah;Arijit Bishnu;Bhargab B. Bhattacharya	2009	2009 World Congress on Nature & Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2009.5393608	erbium;strontium;lab-on-a-chip;computer science;sorting;sensor;electric field;microorganism	ML	43.93244784625835	-25.853948063418926	111698
706fc85ae1c9c6a21eb95a9dae501a5bb995c144	cost-effective mapping using unmanned aerial vehicles in ecology monitoring applications		Ecology monitoring of large areas of farmland, rangelands and wilderness relies on routine map building and picture compilation, traditionally performed using high-flying surveys with manned-aircraft or through satellite remote sensing. Unmanned Aerial Vehicles (UAVs) are a promising alternative as a data collection platform due to the small-size, longer endurance and thus cost-effectiveness of these systems. Additionally UAVs can fly lower to the ground, collecting higher-resolution imagery than with manned aircraft or satellites. This paper discusses the development and experimental evaluation of systems and algorithms for airborne environment mapping, object detection and vegetation classification using low-cost sensor data including monocular vision collected from a UAV. Experimental results of the system are presented in multiple flights of our UAV system in three different environments and two different ecology monitoring applications, operating in remote locations in outback Australia.	aerial photography;ecology;unmanned aerial vehicle	Mitch Bryson;Alistair Reid;Calvin Hung;Fabio Tozeto Ramos;Salah Sukkarieh	2010		10.1007/978-3-642-28572-1_35	data collection;engineering;object detection;reflection mapping;vegetation classification;monocular vision;ecology	Robotics	51.569256136369326	-31.869468646088844	112010
2cb053c2b8d896dacc2ddad8b0b136707673bd92	feature-based localization using scannable visibility sectors	local algorithm;path planning feature based localization localising mobile robots indoor environment scannable sectors navigation visible sectors localization algorithms localization sensors sonar sensors probabilistic roadmap;sonar mobile robots navigation path planning position control;mobile robot;path planning;mobile robots;physical characteristic;navigation;position control;indoor environment;robot sensing systems uncertainty mobile robots data structures orbital robotics navigation sonar sensor phenomena and characterization sensor systems robot vision systems;navigation system;sonar	This paper presents methods for navigating and localizing mobile robots in a known indoor environment. We introduce a restricted visibility concept called a scannable sector that can aid many existing navigation and localization algorithms. The scannable sectors are based on the physical characteristics of the environment and the limitations of the localization sensors used. We describe a complete navigation system that includes a scannable sector based localizer, sonar sensors, and a probabilistic roadmap path planner. Simulation and hardware results using a real robot with sonar sensors show the potential of our approach.	algorithm;evernote;mobile robot;probabilistic roadmap;sonar (symantec);sensor;simulation	Jinsuck Kim;Roger A. Pearce;Nancy M. Amato	2003		10.1109/ROBOT.2003.1242025	mobile robot;embedded system;computer vision;simulation;computer science;engineering;artificial intelligence;mobile robot navigation	Robotics	53.020526943823064	-34.908229359635676	112075
26693862eb2b1b02cd82626c9eac4763de7337de	data-driven statistical modeling of a cube regrasp	kernel;uncertainty;computational modeling;estimation;probability distribution;robots;data models	Regrasping is the process of adjusting the position and orientation of an object in one's hand. The study of robotic regrasping has generally been limited to use of theoretical analytical models and cases with little uncertainty. Analytical models and simulations have so far proven unable to capture the complexity of the real world. Empirical statistical models are more promising, but collecting good data is difficult. In this paper, we collect data from 3300 robot regrasps, and use this data to learn two probability functions: 1) The probability that the object is still in the robot's hand after a regrasp action; and 2) The probability distribution of the object pose after the regrasp given that the object is still grasped. Both of these functions are learned using kernel density estimation with a similarity metric over object pose. We show that our data-driven models achieve comparable accuracy to a geometric model and an off-the-shelf simulator in classification and prediction tasks, while also enabling us to predict probability distributions.	geometric modeling;kernel density estimation;robot;robotic arm;simulation;statistical model	Robert Paolini;Matthew T. Mason	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759397	robot;probability distribution;data modeling;computer vision;estimation;kernel;simulation;uncertainty;computer science;machine learning;computational model;statistics	Robotics	49.606880331903604	-27.039987723131865	112208
345eb95508950ffa64d8e9143d85253dd20c611a	self-supervised learning of depth-based navigation affordances from haptic cues	haptic feedback self supervised learning depth based navigation affordances ground vehicle haptic cues depth cues pan tilt telescopic antenna kinect sensor robot depth sensory feedback;robot sensing systems antennas haptic interfaces robot kinematics three dimensional displays navigation;ciencia;robotic antenna autonomous robots self supervised learning affordances terrain assessment depth sensing;projetos;investigacao;publicacoes;telerobotics learning artificial intelligence mobile robots path planning;iscte iul	This paper presents a ground vehicle capable of exploiting haptic cues to learn navigation affordances from depth cues. A simple pan-tilt telescopic antenna and a Kinect sensor, both fitted to the robot's body frame, provide the required haptic and depth sensory feedback, respectively. With the antenna, the robot determines whether an object is traversable by the robot. Then, the interaction outcome is associated to the object's depth-based descriptor. Later on, the robot to predict if a newly observed object is traversable just by inspecting its depth-based appearance uses this acquired knowledge. A set of field trials show the ability of the to robot progressively learn which elements of the environment are traversable.	feedback;haptic technology;kinect;machine learning;motion planning;range imaging;robot;supervised learning	José Baleia;Pedro F. Santana;José Barata	2014	2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)	10.1109/ICARSC.2014.6849777	computer vision;simulation;engineering;communication	Robotics	48.76908613305351	-35.54305193348047	112989
6759cd959009c408d6427bf6f9200a709eb32e15	bayesian multiple extended target tracking using labeled random finite sets and splines		In this paper, we propose a technique for the joint tracking and labeling of multiple extended targets. To achieve multiple extended target tracking using this technique, models for the target measurement rate, kinematic component, and target extension are defined and jointly propagated in time under the generalized labeled multi-Bernoulli filter framework. In particular, we developed a Poisson mixture variational Bayesian model to simultaneously estimate the measurement rate of multiple extended targets and extended target extension was modeled using B-splines. We evaluated our proposed method with various performance metrics. Results demonstrate the effectiveness of our approach.	b-spline;bayesian network;bernoulli polynomials;calculus of variations;spline (mathematics)	Abdullahi Daniyan;Sangarapillai Lambotharan;Anastasios Deligiannis;Yu Gong;Wen-Hua Chen	2018	IEEE Transactions on Signal Processing	10.1109/TSP.2018.2873537	mathematical optimization;radar tracker;kinematics;mathematics;finite set;spline (mathematics);poisson distribution;bayesian probability;bayesian inference	Robotics	39.29925424277826	-26.89983777819905	113021
9506c5f53b06af93d1f2f705ffd3b9cd3f5a12d4	implementation and evaluation of image recognition algorithm for an intelligent vehicle using heterogeneous multi-core soc	computational costs image recognition algorithm intelligent vehicle heterogeneous multicore soc advanced driver assistance systems adas;system on chip driver information systems image recognition;system on chip image recognition multicore processing histograms vehicles cameras power demand	Image recognition algorithm is becoming one of the most important technology for intelligent vehicle application such as Advanced Driver Assistance Systems (ADAS), however its computational costs are still considerably high. To realize such applications using image recognition algorithm as hard real-time task with low power consumption, we have developed heterogeneous multi-core SoC specialized for image recognition [1]. Subsequently, several image recognition applications have been developed using this SoC. In this paper, we address two ADAS applications and image recognition algorithms for them, and evaluate them on the SoC. The results of the evaluation show that the SoC allows these applications to run with significantly low power consumption comparing with general purpose CPU.	algorithm;architecture design and assessment system;central processing unit;computer vision;multi-core processor;real-time clock;real-time computing	Nau Ozaki;Masato Uchiyama;Yasuki Tanabe;Shuichi Miyazaki;Takaaki Sawada;Takanori Tamai;Moriyasu Banno	2015	The 20th Asia and South Pacific Design Automation Conference	10.1109/ASPDAC.2015.7059040	embedded system;computer vision;simulation;computer science;operating system	EDA	43.59596625629075	-36.606309239446155	113335
f9eea022dd3e3d4201d9ac2523a1cbd398c79faf	target identification with multiple logical sonars using evidential reasoning and simple majority voting	sensor systems;time of flight;sonar applications;amplitude data;time measurement;mobile robot s environment;uncertainty;path planning;real time;dempster shafer rule of combination;multiple logical sonars;target identification;perception uncertainty;degrees of belief;mobile robots;echo amplitude;uncertainty handling;ultrasonic transducers;sonar target recognition;evidential reasoning;distance measurement;parametric uncertainty;nonparametric uncertainty;robust differentiation;voting;time of flight data;multi transducer pulse echo system;nonparametric uncertainty target identification multiple logical sonars evidential reasoning simple majority voting mobile robot s environment multi transducer pulse echo system time of flight data amplitude data robust differentiation degrees of belief dempster shafer rule of combination perception uncertainty echo amplitude;dempster shafer;sonar navigation;physical model;sensor fusion;case based reasoning;target tracking;majority voting;simple majority voting;mobile robots sonar measurements sonar applications sonar navigation target tracking sensor systems voting uncertainty pulse measurements time measurement;pulse measurements;sonar measurements;sonar target recognition case based reasoning uncertainty handling sensor fusion ultrasonic transducers distance measurement mobile robots path planning	In this study, physical models are used to model reflections from target primitives commonly encountered in a mobile robot's environment. These targets are differentiated b y employing a multi-transducer pulse/echo system which relies on both amplitude and time-ofjlight d a t a , allowing more robust differentiation. Targ e t features are generated as being evidentially t ied to degrees of belief which are subsequently fused b y employing multiple logical sonars at different geographical sites. Feature d a t a from multiple logical sensors are fased with Dempster-Shafer rule of combination t o improve the performance of classification b y reducing perception uncertainty. Dempster-Shafer fusion results are contrasted with the results of combination of sensor beliefs through simple majority vote. The method is verified b y experiments with a real sonar system. The evidential approach employed here helps t o overcome the vulnerability of the echo amplitude to noise and enables the modeling of non-parametric uncertainty in real time.	amiga reflections;echo (command);experiment;image noise;mobile robot;sonar (symantec);sensor;transducer	Birsel Ayrulu;Billur Barshan;Simukai W. Utete	1997		10.1109/ROBOT.1997.619267	mobile robot;case-based reasoning;majority rule;computer vision;time of flight;uncertainty;dempster–shafer theory;voting;physical model;computer science;engineering;artificial intelligence;machine learning;motion planning;sensor fusion;evidential reasoning approach;time	Robotics	51.46353593601093	-33.428591545883705	113655
d4ced2086ccd9259ade8fabdba14e0e4d9fc0c40	a mobile imaging system for medical diagnostics		Microscopy for medical diagnostics requires expensive equipment as well as highly trained experts to operate and interpret the observed images. We present a new, easy to use, mobile diagnostic system consisting of a direct imaging microlens array and a mobile computing platform for diagnosing parasites in clinical samples. Firstly, the captured microlens images are reconstructed using a light field rendering method. Then, OpenCL accelerated classification utilizing local binary pattern features is performed. A speedup of factor 4.6 was achieved for the mobile computing platform CPU (AMD C-50) compared with the GPU (AMD 6250). The results show that a relatively inexpensive system can be used for automatically detecting eggs of the Schistosoma parasite. Furthermore, the system can be also used to diagnose other parasites and thinlayer microarray samples containing stained tumor cells.	binary pattern (image generation);central processing unit;graphics processing unit;light field;list of amd mobile microprocessors;microarray databases;mobile computing;opencl api;phi-base;sensor;speedup	Sami Varjo;Jari Hannuksela	2013		10.1007/978-3-319-02895-8_20	computer vision;simulation;computer graphics (images)	ML	41.265007354890436	-33.76573235070478	113806
23395759fb3974c39f577282ba025799e1be3e74	creating and using probabilistic costmaps from vehicle experience	gaussian processes;path planning;mobile robots;a search probabilistic costmaps vehicle experience robot model ubiquitous assumptive costmaps self supervised learned manner robot navigation outdoor environment colour information overhead imagery locations traversability gaussian processes 2d map heteroscedastic noise training data probabilistic nature;search problems gaussian processes learning artificial intelligence mobile robots path planning road vehicles;training data;robots;training data probabilistic logic data models noise robots planning mathematical model;mathematical model;planning;search problems;probabilistic logic;learning artificial intelligence;noise;data models;road vehicles	Probabilistic costmaps provide a means of maintaining a representation of the uncertainty in the robot's model of the environment; in contrast to the ubiquitous assumptive costmaps which abstract this uncertainty away. In this work we show for the first time how probabilistic costmaps can be learned in a self-supervised manner by a robot navigating in an outdoor environment. Traversability estimates garnered from onboard sensing are used in conjunction with colour information from a-priori available overhead imagery to extrapolate the traversability of locations previously traversed by the robot to a much larger area. Gaussian processes are used to predict the traversability at unknown locations in the 2D map, and a number of techniques to deal with heteroscedastic noise and varying confidence in the training data are evaluated. A prior technique to exploit the probabilistic nature of the map in a probabilistic heuristic for A* search demonstrates that planning over these maps can also be done efficiently.	a* search algorithm;biasing;data point;extrapolation;gaussian process;heuristic;map;mean squared error;overhead (computing);probabilistic automaton;robot;simulation	Liz Murphy;Steven Martin;Peter I. Corke	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6386118	planning;robot;mobile robot;data modeling;computer vision;training set;simulation;computer science;engineering;noise;artificial intelligence;machine learning;mathematical model;gaussian process;motion planning;probabilistic logic	Robotics	50.203164214311016	-26.370239845063463	113971
de2cbf71abf1c739771aab5cce39d21b7cadc136	a hybrid intelligent system for robot ego motion estimation with a 3d camera	self organizing maps;robot navigation;motion estimation;hybrid intelligent system;surface model;evolution strategy;self organized map;evolutive computation;fitness function	A Hybrid Intelligent System (HIS) for self-localization working on the readings of innovative 3D cameras is presented in this paper. The system includes a preprocessing step for cleaning the 3D camera readings. The HIS consist of two main modules. First the Self-Organizing Map (SOM) is used to provide models of the preprocessed 3D readings of the camera. The 2D grid of the SOM units is assumed as a surface modeling the 3D data obtained from each snapshot of the 3D camera. The second module is an Evolution Strategy, which is used to perform the estimation of the motion of the robot between frames. The fitness function of the Evolution Strategy (ES) is given by the distance computed as the matching of the SOM unit grids.	hybrid intelligent system;motion estimation;robot	Ivan Villaverde;Manuel Graña	2008		10.1007/978-3-540-87656-4_81	computer vision;simulation;self-organizing map;computer science;artificial intelligence;hybrid intelligent system;machine learning;motion estimation;evolution strategy;fitness function	Robotics	52.183305384614094	-37.16985802354511	114021
1f40e96fd47cff1fdedabc631a8cba8ade7eb9a9	computation sharing in distributed robotic systems: a case study on slam	distributed computing networked robots robotic clusters simultaneous localization and mapping task sharing in multirobot systems;mobile robots computation sharing distributed robotic systems slam simultaneous localization and mapping localization precision map accuracy network bandwidth;slam robots mobile robots multi robot systems;computation sharing distributed robotic systems slam simultaneous localization and mapping localization precision map accuracy network bandwidth mobile robots;task sharing in multirobot systems distributed computing networked robots robotic clusters simultaneous localization and mapping;simultaneous localization and mapping computer architecture robot kinematics multi robot systems context;computer architecture;mobile robots multi robot systems slam robots;simultaneous localization and mapping;multi robot systems;context;robot kinematics	Aiming at increasing team efficiency, mobile robots may act as a node of a Robotic Cluster to assist their teammates in computationally demanding tasks. Having this in mind, we propose two distributed architectures for the Simultaneous Localization And Mapping (SLAM) problem, our main case study. The analysis focuses especially on the efficiency gain that can be obtained. It is shown that the proposed architectures enable us to raise the workload up to values that would not be possible in a single robot solution, thus gaining in localization precision and map accuracy. Furthermore, we assess the impact of network bandwidth. All the results are extracted from frequently used SLAM datasets available in the robotics community and a real world testbed is described to show the potential of using the proposed philosophy.	algorithm;benchmark (computing);central processing unit;computation;computational resource;data structure;experiment;internationalization and localization;kalman filter;load balancing (computing);loose coupling;mobile robot;response time (technology);scalability;simultaneous localization and mapping;speedup;stateful firewall;testbed	Bruno D. Gouveia;David Portugal;Daniel C. Silva	2015	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2014.2357216	mobile robot;computer vision;simulation;computer science;artificial intelligence;distributed computing;robot control;robot kinematics;simultaneous localization and mapping	Robotics	53.238633803411055	-28.97199132218037	114027
b0829626cce0e8c0abdb2a916a35ca36c62a0164	a neural network-based model for paper currency recognition and verification	verification;image recognition;reconocimiento imagen;banking;architecture systeme;neural networks costs multilayer perceptrons banking optoelectronic devices automatic testing manuals reliability engineering measurement standards particle measurements;closed separation surfaces neural network based model paper currency recognition paper currency verification banknote machine low cost optoelectronic devices light refraction multilayer perceptrons real time implementation standard microcontroller based platform;multilayer perceptron;classification;algorithme;algorithm;optoelectronic device;automatic recognition;reconnaissance image;arquitectura sistema;real time implementation;verificacion;reseau neuronal;system architecture;clasificacion;red neuronal;reconocimiento automatico;reconnaissance automatique;neural network;algoritmo	This paper describes the neural-based recognition and verification techniques used in a banknote machine, recently implemented for accepting paper currency of different countries. The perception mechanism is based on low-cost optoelectronic devices which produce a signal associated with the light refracted by the banknotes. The classification and verification steps are carried out by a society of multilayer perceptrons whose operation is properly scheduled by an external controlling algorithm, which guarantees real-time implementation on a standard microcontroller-based platform. The verification relies mainly on the property of autoassociators to generate closed separation surfaces in the pattern space. The experimental results are very interesting, particularly when considering that the recognition and verification steps are based on low-cost sensors.		Angelo Frosini;Marco Gori;Paolo Priami	1996	IEEE transactions on neural networks	10.1109/72.548175	embedded system;verification;simulation;biological classification;computer science;artificial intelligence;machine learning;multilayer perceptron;artificial neural network;functional verification;systems architecture	EDA	39.70599047121199	-31.43980115921485	114356
4884d458cb90e19e8e0a1b310ef50ef0e5044d26	an efficient fpga-based hardware framework for natural feature extraction and related computer vision tasks	detectors;object recognition;streaming media;object recognition system fpga based hardware framework feature extraction computer vision image processing higher level algorithms surf algorithm speeded up robust features feature detection;feature extraction;pipelines;pipelines feature extraction object recognition streaming media hardware detectors;object recognition computer vision feature extraction field programmable gate arrays;hardware	The paper presents an efficient and flexible framework for extensive image processing tasks. While most available frameworks concentrate on pixel-based modules and interfaces for image preprocessing tasks, our proposal also covers the seamless integration of higher-level algorithms. Window-oriented filter operations, such as noise filters, edge filters or natural feature detectors, are performed within an efficient 2D window pipeline. This structure is generated and optimized automatically based on a user-defined filter configuration. For complex, higher-level algorithms, an optimized array of independent, software-based processing units is generated. As an example application, we chose object recognition based on the well-known SURF algorithm (“Speeded Up Robust Features”), which performs natural feature detection and description. All involved image processing steps were successfully mapped to our architecture. Thus, exploiting the FPGAs full potential regarding parallelism, we synthesized one of the most efficient SURF detectors and a complete object recognition system in a single mid-size FPGA.	algorithm;computer vision;edge enhancement;fastest;feature detection (computer vision);feature detection (web development);feature extraction;field-programmable gate array;global serializability;image processing;library (computing);outline of object recognition;parallel computing;pixel;preprocessor;random-access memory;schmidt decomposition;seamless3d;sensor;speeded up robust features;usability;whole earth 'lectronic link;window function	Matthias Pohl;Michael Schaeferling;Gundolf Kiefer	2014	2014 24th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2014.6927463	computer vision;detector;feature detection;feature extraction;computer science;theoretical computer science;cognitive neuroscience of visual object recognition;machine learning;pipeline transport;3d single-object recognition;feature	Robotics	43.996697835232666	-35.72502109188977	114483
43dc52d56e731215aeae72bc9c7673f6fc7244cb	enabling factor analysis on thousand-subject neuroimaging datasets	brain modeling;computational modeling;neuroimaging;optimization;algorithm design and analysis;data models	The scale of functional magnetic resonance image data is rapidly increasing as large multi-subject datasets are becoming widely available and high-resolution scanners are adopted. The inherent low-dimensionality of the information in this data has led neuroscientists to consider factor analysis methods to extract and analyze the underlying brain activity. In this work, we consider two recent multi-subject factor analysis methods: the Shared Response Model and the Hierarchical Topographic Factor Analysis. We perform analytical, algorithmic, and code optimization to enable multi-node parallel implementations to scale. Single-node improvements result in 99χ and 2062x speedups on the two methods, and enables the processing of larger datasets. Our distributed implementations show strong scaling of 3.3x and 5.5χ respectively with 20 nodes on real datasets. We demonstrate weak scaling on a synthetic dataset with 1024 subjects, equivalent in size to the biggest fMRI dataset collected until now, on up to 1024 nodes and 32,768 cores.	baseline (configuration management);code;cognition;computation;converge;dhrystone;electroencephalography;expectation–maximization algorithm;factor analysis;garbage collection (computer science);image resolution;image scaling;jacobian matrix and determinant;linear least squares (mathematics);mathematical optimization;noise reduction;non-linear least squares;open-source software;parallel computing;performance engineering;population;program optimization;python;resonance;scalability;speedup;supercomputer;system reference manual;time formatting and storage bugs;topography	Michael J. Anderson;Mihai Capota;Javier Turek;Xia Zhu;Theodore L. Willke;Yida Wang;Po-Hsuan Chen;Jeremy R. Manning;Peter J. Ramadge;Kenneth A. Norman	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840719	data modeling;algorithm design;computer science;bioinformatics;data science;data mining;computational model;neuroimaging	HPC	40.497899250566164	-34.271493710285306	114546
e4e99ee67934411a4e03955960b88ff0a4f2c6cf	multi-lanes detection based on panoramic camera	hough transform autonomous vehicle lane detection equidistant curves panoramic camera;robust detection method multilanes detection panoramic camera lane detection system environmental perception autonomous vehicle lidar 2d gaussian kernel variable curvature curve equidistant curve;cameras roads feature extraction mathematical model mobile robots approximation methods adaptation models;optical radar feature extraction gaussian processes intelligent transportation systems mobile robots object detection	The lane detection system is one of the most important subsystems to achieve the environmental perception of autonomous vehicles. This paper addresses the problem of detecting multiple lanes in a relatively large range around the autonomous vehicle without making too much approximation on the shape of the lane marks. A panoramic camera and a LIDAR is used to obtain a wide range of image information and exclude part of the interference information. The vertical lane marks with specific width is extracted by a filter with 2D Gaussian kernel. In order to describe the lanes with relatively complex shapes like variable curvature curves, the model of equidistant curves is proposed and a robust detection method of equidistant curves is designed under the guidance of the lane model. Satisfactory experimental results in diverse environment and the successful application on autonomous vehicles demonstrate the effectiveness of the proposed method.	approximation;autonomous robot;experiment;filter (signal processing);hough transform;interference (communication);sensor	Mengyin Fu;Xinyu Wang;Hongbin Ma;Yi Yang;Meiling Wang	2014	11th IEEE International Conference on Control & Automation (ICCA)	10.1109/ICCA.2014.6870997	computer vision;simulation;computer graphics (images)	Robotics	49.33290529309392	-36.27414689804515	114692
10dc8eef5c2922d2727e35fa4e45b72b4f897eee	the design of a flexible digit towards wireless tactile sense feedback	tetherless sensing flexible digit wireless tactile sense feedback embedded signal processing function sensor arrays application specific digital system;flexible manipulators;array signal processing;minimal access surgery;control system synthesis flexible manipulators tactile sensors feedback medical robotics array signal processing surgery;medical robotics;tactile sensing;feedback;control system synthesis;signal processing;digital systems;surgery;sensor array;tactile sensors;wireless sensor networks feedback sensor arrays array signal processing endoscopes surgery sensor phenomena and characterization sensor systems and applications digital signal processing digital systems	This paper describes research on the embedded signal processing function for a flexible digit with the feedback of tactile sense, such as is appropriate to endoscopy and other minimal access surgery. The paper describes the nature of the flexible digit and its associated sensor arrays and concentrates on the nature of the signal processing function and the problem of embedding it an application specific digital system. The paper also focuses on the need for wireless or 'tetherless' sensing in such systems.	digital electronics;embedded system;sensor;signal processing	I. Peter;David J. Holding;Keith J. Blow;B. Tam;Xianghong Ma;Peter N. Brett	2004	ICARCV 2004 8th Control, Automation, Robotics and Vision Conference, 2004.	10.1109/ICARCV.2004.1468870	control engineering;embedded system;electronic engineering;computer science;engineering;signal processing;control theory;feedback;sensor array;tactile sensor	Robotics	43.97259864368342	-27.699808190518315	115504
81adde6e183131736a001285204090cac9af4ee8	fusion of tracks with road constraints	analytic functions;search radar;data fusion;digital maps;accuracy;moving targets;roads;computerized simulation;algorithms;optimization;surface targets;lagrangian functions;tracking	With the rapid building up of geographic information system (GIS) including digital road maps (DRM) and digital terrain elevation data (DTED), information about roads becomes more accurate, up to date, and accessible. Looking for a map in the Internet is at fingertips with a least cost (i.e., distance or time) route plotted to a destination. Road and terrain information has been used in the past for navigation via terrain contour matching. Other examples include the increasingly popular use of digital maps for automobile navigation with a Global Positioning System (GPS) receiver and terrain-aided navigation for aircraft. This paper is concerned with tracking of ground targets on roads and investigates possible ways to improve target state estimation via fusing a target’s track with information about the road along which the target is traveling. A target track is estimated using a surveillance radar whereas a digital map provides the road network of the region under surveillance. Target tracking is not unfamiliar with road maps. For example, target tracks are represented by colorful dots and lines blinking along road networks on a big screen, often on top of a topographic or satellite image, in a situation room, in an air traffic control tower, and on a radar operator screen. In these applications, however, target tracks and road networks are merely displayed together with little or no interaction in the data processing level. When the information about roads is as accurate as (or even better than) radar measurements, it is naturally desired to incorporate such information into target state estimation. When a vehicle travels off-road or on an unknown road, the state estimation problem is unconstrained. However, when the vehicle is traveling on a known road, be it straight or curved, the state estimation problem can be cast as constrained with the road network information available from digital road/terrain maps. In the past, such constraints are often ignored (or left for the users to perceive it as in the display example mentioned above). The resulting estimates, even obtained with the Kalman filter, cannot be optimal because they do not make full use of this additional information about state constraints. To use such state constraints, previous attempts can be put into several groups. The first group is to incorporate road information into the state estimation process. One technique is to reduce the system model parameterization. Another technique is to translate the state constraints onto the state process and/or observation noise covariance matrix for the estimation filter [10]. The use of variable structure IMM (VS-IMM) methods also belongs to this group [7, 18, 19, 22]. Yet another technique is to project a dynamic system onto linear state constraints and then apply the Kalman filter to the projected systems [11]. Similarly, for nonlinear state constraints, there is the one-dimensional (1D) representation of a target motion along a curvilinear road [27].	apple maps;dted;dynamical system;estimation theory;geographic information system;global positioning system;internet;kalman filter;map;nonlinear system;radar;topography;yet another	Chun Yang;Erik Blasch	2008	J. Adv. Inf. Fusion		computer vision;mathematical optimization;simulation;digital mapping;computer science;analytic function;mathematics;sensor fusion;accuracy and precision;tracking;statistics	HCI	49.55254025427741	-35.56437305135501	116025
7006c906752b7419c620fdc4495d87cb2fb11dee	a biologically plausible acoustic motion detection neural network	motion detection;neural network	In this paper we present an acoustic motion detection system to be used in a small mobile robot. While the first purpose of the system has been to be a reliable computational implementation, cheap enough to be built in hardware, effort has also been taken to construct a biologically plausible solution. The motion detector consists of a neural network composed of motion-direction sensitive neurons with a preferred direction and a preferred region of the azimuth. The system was designed to produce a higher response when stimulated by motion in the preferred direction than in the null direction and that is in fact what the system does, which means that, as desired, the system can detect motion and distinguish its direction.		Sofia Cavaco;John Hallam	1999	International journal of neural systems	10.1142/S0129065799000472	computer vision;simulation;computer science;artificial intelligence;machine learning;artificial neural network	Robotics	48.11896742893731	-37.691403911494184	116666
de39510222d2ca33b63369224438fb935e7c777e	hardware and software implementation of a new algorithm on photoacoustic medical imaging		In this paper, we review a new image reconstruction algorithm proposed by our research team and comment on the hardware and software implementation for that algorithm. We use Texas Instruments(TI) embedded analog front end(AFE) board and Xilinx FPGA combined with camera interface of freescale i.MX6 microprocessing unit(MPU) for compactness of implementation and develop application program for preview image of raw B-scan image based on software development kit(SDK) firmware integrated development environment(IDE). We use four segment of frame buffer for capturing raw B-scan image and store it to these four segments of frame buffer alternatively and move one segment of frame buffer to the LCD controller to remove and fix the flickering problem in LCD display. We comment on the GPU and CUDA programming for implementing of a new image reconstruction algorithm.	algorithm;analog front-end;ct scan;cuda;camera interface;embedded system;field-programmable gate array;firmware;framebuffer;graphics processing unit;image quality;integrated development environment;iterative reconstruction;liquid-crystal display;matlab;medical imaging;simulation;software development	Woonchul Ham;Enkhbat Batbayar;Ju Yeon Lee;Chulgyu Song	2018	2018 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2018.8326079	iterative reconstruction;computer hardware;camera interface;firmware;field-programmable gate array;software development;computer science;analog front-end;software;cuda;algorithm	Visualization	42.750831210023094	-32.02516795161076	117090
e06648d1e22cb0ca157a42c027ffa774b8d92522	a new strategy for exploring unknown environments for the slam problem		This article presents a new strategy aimed at solving the problem of exploration of unknown environments for SLAM, which is based in the efficient process of acquiring the maps of environments through the link between the motion planning tasks and the simultaneous localization and mapping. Although, in recent years this kind of algorithms have had a great progress, one can remember that it is a still open problem, because the solution is imposed by the human interaction that defines the destiny and the objective of the task to be carried out by the robot. Following this idea, this research project proposes a new method of deterministic exploration based on the construction of a graph as a data structure, and on the systematic exploitation of the knowledge acquired by the robot while navigating the environment.		P. AlfredoToriz;Abraham Sánchez López	2017		10.1109/MICAI-2017.2017.00010	machine learning;simultaneous localization and mapping;open problem;motion planning;motion control;robot;artificial intelligence;computer science;data structure;graph	Robotics	52.671905322054776	-24.737493338921745	117366
957baf580bc6ca53e6bbbed99e40fe3741720ae8	factors analysis of the sound field for audio directional systems	audio systems;matlab software factors analysis sound field audio directional systems;loudspeakers acoustic field audio systems;acoustic field;sound field audio directional system round piston transducer parametric acoustic array;loudspeakers;acoustics transducers attenuation arrays pistons humidity matlab	In order to analyze the complex influential factors on the sound field of audio directional systems, investigate the mechanism of the associated basic acoustic theories, and study how the system performance vary with the change of parameters. Then integrate these theories and analyze their combined effects on the sound field. The accurate numerical model of the sound field including multi-parameters is built, and the sound field distribution and its mechanism are simulated by MATLAB software. The results of comparative analysis presented in this work will facilitate the design of audio directional system.	acoustic cryptanalysis;matlab;mathematical model;qualitative comparative analysis;simulation;systems design;theory;visual graphics	Yonghui Zhang;Limin Tao	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023206	loudspeaker;speech recognition;acoustics;engineering;directional sound;audio signal;sound quality;sound reinforcement system;physics	HCI	46.99904688801104	-24.73564479141723	117520
34a77f05eba65ac16d186352f4479fc373e23631	applying synaptic delays for virtual sensing and actuation in mobile robots	automatic control;temporal correlation;robot sensing systems;behavior based robotics;neural nets;mobile robot;virtual sensing;actuators;mobile robots;synaptic delays;time delay;temporal information;perception sequence synaptic delays virtual sensing mobile robots actuation artificial neural networks;artificial neural networks;industrial control;virtual sensor;actuation;active sensing;intelligent networks;recurrent neural networks;actuators neural nets mobile robots;delay mobile robots robot kinematics robot sensing systems artificial neural networks robotics and automation recurrent neural networks automatic control intelligent networks industrial control;robotics and automation;artificial neural network;robot kinematics;perception sequence	In this article we describe the use of Artificial Neural Networks (ANN) with synaptic time delays between the nodes as a means to increase the capabilities of the usual control modules used in behavior based robotics. This inclusion allows the controllers to manage explicit temporal information in different levels. In the sensing level it permits the use of virtual sensors that improve the precision of the information provided by sensors through a temporal correlation of their values. In the actuation level we use the network with an infrasensorized robot in a problem that requires active sensing, where the control and actuation mechanisms are coordinated in order to obtain a better sensorial image of the environment by means of a spatio-temporal representation of a perception sequence. The decision of the appropriate delays is automated through learning and evolution.	artificial neural network;behavior-based robotics;neural networks;robot;sensor;synaptic package manager	Francisco Bellas;José Antonio Becerra;José Santos Reyes;Richard J. Duro	2000		10.1109/IJCNN.2000.859388	mobile robot;simulation;computer science;artificial intelligence;machine learning;artificial neural network	Robotics	50.57682707769106	-29.436701973327462	117627
e9e9ead2592ba559c7d9a70a9360e84313b29018	visual navigation using view-sequenced route representation	two wheeled robot visual navigation view sequenced route representation vision based mobile robotics localization steering angle determination obstacle detection nonmetrical model front view images correlation technique quick control procedure autonomous navigation;robot sensing systems;image recognition;obstacle detection;correlation technique;neural networks;quick control procedure;mobile robot;image matching;path planning;real time;localization;working environment noise;visual navigation;mobile robots;two wheeled robot;data mining;front view images;steering angle determination;vision based mobile robotics;navigation;robot vision;visual representation;solid modeling;nonmetrical model;autonomous navigation;navigation image sequences robot vision image matching object detection mobile robots path planning;navigation mobile robots robot vision systems image recognition cameras solid modeling robot sensing systems data mining working environment noise neural networks;view sequenced route representation;robot vision systems;cameras;object detection;image sequences	"""Previous work in vision-based mobile robotics have lacked models of the route which can be utilized for (1) localization, (2) steering angle determination, and (3) obstacle detection, simultaneously. In this paper, the authors propose a new visual representation of the route, the """"view-sequenced route representation (VSRR)."""" The VSRR is a non-metrical model of the route, which contains a sequence of front view images along a route memorized in the recording run. In the autonomous run, the three types of recognition described above are achieved in real-time by matching between the current view image and the memorized view sequence using a correlation technique. the authors also developed an easy procedure for acquiring VSRRs, and a quick control procedure using VSRRs. VSRRs are especially useful for representing routes in corridors. Results of autonomous navigation using a two-wheeled robot in a real corridor are also presented."""	machine vision	Yoshio Matsumoto;Masayuki Inaba;Hirochika Inoue	1996		10.1109/ROBOT.1996.503577	control engineering;mobile robot;computer vision;simulation;computer science;artificial intelligence;artificial neural network	Robotics	51.958633361892865	-37.10546800150911	118651
926a8249f608a92a06ee46ca4deedc05a8382150	improvement of robot mapping and localization using combined sensory data (abstract only)	localization;robotics;mapping	Robotic mapping and localization are two large areas in robotics research that focus on how robots know and navigate within an environment. The current methods used to complete these tasks are not extremely accurate due to errors that are compounded as the robot moves. Since robots are often used in situations where precise navigation is a necessity, there is a strong motivation for the mapping and localization processes to be as accurate as possible.  This project hopes to reduce the accumulated error which occurs during the mapping and localization processes through the integration of a Kinect sensor, which provides both visual and depth data about objects in front of a mobile robot and a sonar sensor which provides depth information about objects on the sides of the robot.  The Kinect's images and depth information will be used to form an occupancy grid map of the environment, while the sonar data will track the walls as the robot moves throughout the environment in order to assist in keeping track of both position and orientation of the robot. These two sensors will allow the robot to reduce the errors accumulated during localization and navigation within the map being created and in navigating the subsequently completed map.  When the occupancy grid map is completed, the cells in it will be combined into a topological map, which will hold information about the rooms such as doorway locations. The topological map will then be used as the robot plans a path to its goal.	internationalization and localization;kinect;mobile robot;robotic mapping;robotics;sonar (symantec);sensor	Deanna Biesan	2015		10.1145/2676723.2693623	computer vision;simulation;internationalization and localization;computer science;artificial intelligence;occupancy grid mapping;robotics;mobile robot navigation	Robotics	53.14993665806477	-34.322994890423814	118709
1aa9582a1a2be2c1ff999b6781c582ea67910ade	high performance realtime vision for mobile robots on the gpu	paper;nvidia geforce 6800;computer vision;package;nvidia;opengl;computer science	We present a real time vision system designed for and implemented on a graphics processing unit (GPU). After an introduction in GPU programming we describe the architecture of the system and software running on the GPU. We show the advantages of implementing a vision processor on the GPU rather than on a CPU as well as the shortcomings of this approach. Our performance measurements show that the GPU-based vision system including colour segmentation, pattern recognition and edge detection easily meets the requirements for high resolution (1024 ×768) colour image processing at a rate of up to 50 frames per second. A CPU-based implementation on a mobile PC would under these constraints achieve only around twelve frames per second. The source code of this system is available online [1].		Christian Folkers;Wolfgang Ertel	2007			embedded system;computer hardware;computer science;computer graphics (images)	Vision	44.02960626753674	-35.86295884312498	118825
6664db7020ef0c84205b96b78b5473b0cca5742c	on-line decision-theoretic golog for unpredictable domains	lenguaje programacion;trajectoire;programming language;search space;pervasive computing;mesure position;robotics;teoria decision;medicion posicion;informatica difusa;planificacion;trajectory;informatique diffuse;theorie decision;state space;decision theory;decision theoretic;position measurement;langage programmation;robotica;planning;trayectoria;robotique;planification;langage golog	DTGolog was proposed by Boutilier et al. as an integration of decision-theoretic (DT) planning and the programming language Golog. Advantages include the ability to handle large state spaces and to limit the search space during planning with explicit programming. Soutchanski developed a version of DTGolog, where a program is executed on-line and DT planning can be applied to parts of a program only. One of the limitations is that DT planning generally cannot be applied to programs containing sensing actions. In order to deal with robotic scenarios in unpredictable domains, where certain kinds of sensing like measuring one’s own position are ubiquitous, we propose a strategy where sensing during deliberation is replaced by suitable models like computed trajectories so that DT planning remains applicable. In the paper we discuss the necessary changes to DTGolog entailed by this strategy and an application of our approach in the ROBOCUP domain.	approximation algorithm;decision theory;hoc (programming language);online and offline;programming language;robot control	Alexander Ferrein;Christian Fritz;Gerhard Lakemeyer	2004		10.1007/978-3-540-30221-6_25	planning;simulation;decision theory;computer science;state space;artificial intelligence;trajectory;machine learning;mathematics;robotics;programming language;computer security;algorithm;statistics	AI	52.01210477381984	-29.629320075119956	118901
3ac8dbd7a5f599e51745c95ae9937a5d122634ac	holographic video computation and display using holo-chidi	interfaces;voltage controlled current source;data storage;displays;holography;field programmable gate arrays;video;holograms	Abstract. ‘‘Holo-Chidi’’ is a holographic video processing systemdesigned at the Massachusetts Institute of Technology (MIT) MediaLaboratory for real-time computation of computer-generated holo-grams and the subsequent display of the holograms at video framerates. The Holo-Chidi system is composed of two sets of cards—theset of processor cards and the set of video concentrator cards(VCCs). The processor cards are used for hologram computation,data archival/retrieval from a host system, and for higher level con-trol of the VCCs. The VCC formats computed holographic data frommultiple hologram-computing processor cards, converting the digitaldata to analog form to feed the acousto-optic modulators of theMedia Lab’s ‘‘MarkII’’ holographic display system. Both the proces-sor card and VCC are fabricated and tested, integrating both sets ofcards with a host system is currently in progress. The design andimplementation of the Holo-Chidi system is discussed. © 2003 SPIEand IS&T. [DOI: 10.1117/1.1579487]	computation;holography	Thomas A. Nwodoh;Stephen A. Benton	2003	J. Electronic Imaging	10.1117/1.1579487	embedded system;computer hardware;computer science;optics;holography;computer graphics (images)	Graphics	44.78198637310358	-34.18653466670022	119112
8bf13d26713d12f9a3d10c24a7c19295d9bbd79e	gpu-aided directional image/video interpolation for real time resolution upconversion	interpolation;high resolution;yarn;image resolution;video signal processing;computer graphics;real time;low resolution;gpu;compute unified device architecture;coprocessors;cuda;real time resolution upconversion;interpolation image resolution spatial resolution video compression image coding computer architecture graphics signal resolution image reconstruction multimedia communication;streaming media;directional image video interpolation;compute unified device architecture gpu directional image video interpolation real time resolution upconversion spatial resolution nvidia cuda;pixel;nvidia;video signal processing computer graphics coprocessors image resolution;algorithm design and analysis;spatial resolution	Image/video spatial resolution upconversion aims to obtain a high resolution output from the original low resolution input. Fast resolution upconversion is desired in many applications. In this paper, we develop a GPU-friendly two-pass directional image/video resolution upconversion algorithm and present a GPU implementation of the method, using the NVIDIA CUDA (Compute Unified Device Architecture) technology. Design considerations to speed up the execution are discussed, by taking full advantage of the properties of the CUDA framework and the upconversion scheme. Experimental results show that using a mid-range GPU card, the GPU-optimized resolution upconversion implementation can be more than five times as fast as the original method.	algorithm;cuda;display resolution;graphics processing unit;image resolution;interpolation	Jie Cao;Ming-Chao Che;Xiaolin Wu;Jie Liang	2009	2009 IEEE International Workshop on Multimedia Signal Processing	10.1109/MMSP.2009.5293313	computer vision;image resolution;computer science;theoretical computer science;algorithm;computer graphics (images)	Robotics	43.48564785748498	-33.957777215380005	119457
b56cf94cc1b986a84ab208880c2e987ac8d0e0f8	information theory filters for wavelet packet coefficient selection with application to corrosion type identification from acoustic emission signals	chemical process industry;acoustics;bayes theorem;corrosion monitoring;conservation of natural resources;signal processing computer assisted;wavelet packet transform;reproducibility of results;feature subset selection;models statistical;mutual information;algorithms;acoustic emission;corrosion;information theory;environmental monitoring	The damage caused by corrosion in chemical process installations can lead to unexpected plant shutdowns and the leakage of potentially toxic chemicals into the environment. When subjected to corrosion, structural changes in the material occur, leading to energy releases as acoustic waves. This acoustic activity can in turn be used for corrosion monitoring, and even for predicting the type of corrosion. Here we apply wavelet packet decomposition to extract features from acoustic emission signals. We then use the extracted wavelet packet coefficients for distinguishing between the most important types of corrosion processes in the chemical process industry: uniform corrosion, pitting and stress corrosion cracking. The local discriminant basis selection algorithm can be considered as a standard for the selection of the most discriminative wavelet coefficients. However, it does not take the statistical dependencies between wavelet coefficients into account. We show that, when these dependencies are ignored, a lower accuracy is obtained in predicting the corrosion type. We compare several mutual information filters to take these dependencies into account in order to arrive at a more accurate prediction.	acoustic cryptanalysis;biological science disciplines;cerebral peduncle;coefficient;confusion;corrosion of medical device material;discriminant;entity framework;experiment;extraction;extravasation;genetic selection;information theory;itakura–saito distance;largest;maximal set;mutual information;naive bayes classifier;naivety;network packet;password cracking;pitted medical device material;selection algorithm;sensor;spectral leakage;supercomputer;wavelet packet decomposition;funding grant;research grants	Gert Van Dijck;Marc M. Van Hulle	2011		10.3390/s110605695	structural engineering;corrosion;information theory;engineering;acoustic emission;machine learning;environmental monitoring;wavelet packet decomposition;mutual information;forensic engineering;bayes' theorem;statistics	Web+IR	40.51731877339576	-30.445215000078374	120123
2ffaa79ff448a29772ff5ea5730955ea2274555a	patterns of approximated localised moments for visual loop closure detection	perceptual aliasing;zernike moments;autonomous mobile robot navigation;visual loop closure detection;loop closing;integral images;zm based descriptor	In the context of autonomous mobile robot navigation, loop closing is defined as the correct identification of a previously visited location. Loop closing is essential for the accurate self-localisation of a robot; however, it is also challenging due to perceptual aliasing, which occurs when the robot traverses in environments with visually similar places (e.g. forests, parks, office corridors). In this study, the authors apply the local Zernike moments (ZMs) for loop closure detection. When computed locally, ZMs provide a high discrimination ability, which enables the distinguishing of similar-looking places. Particularly, they show that increasing the density over which the local ZMs are computed improves loop closing accuracy significantly. Furthermore, they present an approximation of ZMs that allows the usage of integral images, which enable realtime operation. Experiments on real datasets with strong perceptual aliasing show that the proposed ZM-based descriptor outperforms state-of-the-art methods in terms of loop closure accuracy. They also release the source-code of the implementation for research purposes.	aliasing;approximation algorithm;autonomous robot;closing (morphology);for loop;mobile robot;real-time computing;robotic mapping	Can Erhan;Evangelos Sariyanidi;Onur Sencan;Hakan Temeltas	2017	IET Computer Vision	10.1049/iet-cvi.2016.0237	computer vision;simulation;machine learning;mathematics;geometry	Robotics	49.72639035856467	-37.96810280096339	120337
02edd75faa351462d78b6c670a1766dfe8ef00e3	towards integrated threat assessment and sensor management: bayesian multi-target search	uncertainty;bayes methods;heuristic algorithms;mutual information;target tracking;command and control systems;context	Currently, most land intelligence, surveillance and reconnaissance (ISR) systems, especially those employed in critical infrastructure protection contexts, comprise of a suite of sensors (e.g. EO/IR, radar, etc.) loosely integrated into a central command and control (C2) system with limited autonomy. We consider a concept of a modular and autonomous architecture where a set of heterogeneous autonomous sensor modules (ASMs) connect to a high-level decision making module (HLDMM) in a plug and play manner. Working towards an integrated threat evaluation and sensor management approach which is capable of optimizing the ASM suite to search for, localise, and capture relevant imagery of multiple threats in and around the area under protection, we propose a Bayesian multi-target search algorithm. In contrast to earlier work we demonstrate how the algorithm can reduce the time to acquire threats through incorporation of target dynamics. The derivation of the algorithm from an information-theoretic perspective is given and its links with the probability hypothesis density (PHD) filter are explored. We discuss the results of a demonstration HLDMM system which embodies the search algorithm and was tested in realistic base protection scenarios with live sensors and targets.	autonomous robot;critical infrastructure protection;emoticon;high- and low-level;information systems research;information theory;plug and play;search algorithm;sensor;simulation;threat (computer)	Scott F. Page;James P. Oldfield;Paul Thomas	2016	2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2016.7849465	simulation;engineering;artificial intelligence;data mining	Robotics	51.37988942926116	-31.611174178935954	120468
d1db51d03e0d0c5b47bc39f751f9b1d2060849e1	the digital transformation of hollywood: format and resolution independent digital post-production	movies digital transformation hollywood digital post production format independence general purpose computers film video post production user interfaces customized applications print resolution independence marketplace post houses;production image resolution image storage costs transform coding control systems visualization computer industry computer interfaces application software;user interface;cinematography;production system;general purpose computers cinematography;general purpose computers	With the constant increase in general purpose computational power, many digital post-production systems are now being based on general purpose, commercially available computer systems rather than proprietary ones. The rate at which fi lm and video postproduction is digitally processed is proportional t o the use of general purpose computers in this industry. As general purpose computers displace the special purpose 'black boxes, ' digital post-production facilities can customize applications and user interfaces. A t the same time all constraints of resolution and format dependence are being removed. The distinction between video, f i lm and print is blum'ng. Resolution and format independence is changing the traditional market place by allowing post houses t o compete for business in markets previously closed t o them. 1 Enabling Technologies Advances in several technologies have enabled the use of digital computers for post-production applications. With the advent of powerful workstations and server class machines, the processing power necessary for computing the effects required to do postproduction is now available. These machines themselves are able to exist due to the ongoing advances in microprocessor technology, memory capacity, and bus bandwidth which are now increasing at exponential rates. Advances in disk storage technology also play an important role. The increases in disk densities along with the corresponding reduction in size now make it possible and affordable to have gigabytes of storage on line, enough for hours of full resolution digital component video or minutes of high resolution film imagery. RAID (Redundant Arrays of Inexpcnsive Disks) technology has provided the disk bandwidth so that full resolution video can be captured to, or played back from disk in real time. High speed network technology has provided the means for moving the large quantities of image data between machines at cost effective rates. CCD cameras and scanners now provide a repeatable, resolution independent mechanism for converting image data into a digital form. Other technologies such as CD-ROM and a variety of compression techniques, such as JPEG and MPEG, have enabled real-time delivery in yet another format and resolution. 2 Advantages of Digital PostProduct ion Digital post-production systems and more importantly, those based on general purpose computers, offer several advantages. The digital revolution is rapidly changing the way an entire industry has traditionally done business. The effects of general purpose, digital computing is not only fundamentally changing the production process, but the marketing of services as well. The nature of a digital system provides far more control over the process and the medium. An increase in quality and the ability to flawlessly repeat effects, unavailable from the composite analog video and film chemistry media, is the hallmark of the digital world. For improved dynamic range of color, calculations required to create effects may be performed at much higher resolution than the target meilia is capable of displaying (e.g. floating point vs. 10-bit video). In a digital post-production system an effect's production process, along with all of its controlling parameters, is captured and performed digitally. Because digital processes are deterministic they produce the same predictable result every time. Thus once source material has been converted to a digital form it is no longer subject to environmental factors such as film chem1063-6390/94 $3.00	black box;cd-rom;charge-coupled device;composite video;computation;computer;computer data storage;digital revolution;digital component video;digital data;digital electronics;disk storage;display resolution;division by zero;dynamic range;floppy disk;gigabyte;hollywood;hollywood;image resolution;jpeg;microprocessor;moving picture experts group;parallel computing;production system (computer science);raid;real-time transcription;resolution (logic);resolution independence;server (computing);tag system;thin-film transistor;time complexity;user interface;workstation;yet another	David A. Epstein;W. Randall Koons	1994		10.1109/CMPCON.1994.282887	simulation;computer science;multimedia;computer graphics (images)	Graphics	43.137123224525965	-24.60829615496165	120574
7e20d508596d4a1d5be9d87ae33b9ebccefd7507	selection of sources as a prerequesite for information fusion with application to slam	robot sensing systems;information resources;dsmt;self localization and mapping slam;intelligent robots;simultaneous localization and mapping information resources sensor fusion mobile robots robot sensing systems robotics and automation intelligent control intelligent robots laboratories automatic testing;sonar sensors;automatic testing;mobile robots;intelligent control;source selection;slam problem;slam robots case based reasoning mobile robots sensor fusion;self localization and mapping slam source selection information fusion evidence supporting measure of similarity esms dsmt;simultaneous localization and mapping;equireliable sensors;information fusion;autonomous mobile pioneer ii robot;sensor fusion;case based reasoning;dsmt based fusion machine;evidence supporting similarity measure;slam robots;slam problem source selection information fusion evidence supporting similarity measure dsmt based fusion machine autonomous mobile pioneer ii robot sonar sensors equireliable sensors simulation environment;robotics and automation;simulation environment;evidence supporting measure of similarity esms	We consider in this work evidential sources of information and propose a very general evidence supporting measure of similarity (ESMS) for selecting the most coherent subset of sources to combine among all sources available at each instant. The methodology proposed here coupled with a DSmT-based fusion machine is tested in robotics for the automatic estimation of an unknown simulated environment with obstacles where an autonomous mobile Pioneer II robot with sonar sensors evolves. Our simulation results are based on the fusion of similar and equireliable sensors but same approach can also be used with dissimilar sources as well by using a discounting method taking into account the reliability of each sensor. Our results show clearly the benefit of the selection of the sources as prerequisite for improvement of information fusion	autonomous robot;coherence (physics);computation;mobile robot;performance;robotics;sonar (symantec);sensor;simulation;simultaneous localization and mapping;thresholding (image processing);virtual reality	Xinde Li;Jean Dezert;Xinhan Huang	2006	2006 9th International Conference on Information Fusion	10.1109/ICIF.2006.301795	computer vision;simulation;engineering;artificial intelligence	Robotics	52.51459660448961	-33.79976362609234	120755
13b23e5e022b6fa65548023eec952f52c5d7dd69	learning control for space robotic operation using support vector machines	modelizacion;learning process;learning control;pistage;trajectoire;analisis estadistico;defecto;rastreo;robotics;small samples;modelisation;classification a vaste marge;trajectory;statistical analysis;human control strategy;space robotics;defect;analyse statistique;poursuite cible;defaut;robotica;trayectoria;robotique;support vector machine;maquina ejemplo soporte;vector support machine;reseau neuronal;target tracking;local minima;modeling;red neuronal;tracking;neural network	Automatical operation of space robots is a challenging and ultimate goal of space servicing. In this paper, we present a novel approach for tracking and catching operation of space robots based on learning and transferring human control strategies (HCS). We firstly use an efficient support vector machine (SVM) to parameterize the model of HCS, and then develop a new SVM-based leaning structure to improve HCS in tracking and capturing control. The approach is fundamentally valuable in dealing with some problems such as small sample data and local minima, which makes it efficient in modeling, understanding and transferring its learning process. The simulation results demonstrate that the proposed method is useful and feasible in tracking trajectory and catching objects autonomously.	robotic spacecraft;robotics;support vector machine	Panfeng Huang;Wenfu Xu;Yangsheng Xu;Bin Liang	2006		10.1007/11760023_176	support vector machine;computer vision;simulation;systems modeling;computer science;artificial intelligence;trajectory;machine learning;maxima and minima;tracking;robotics;artificial neural network	Robotics	52.429457713235855	-30.05559048878241	121171
f4c15917384243cf71599119c75ca7aaf24f48f3	adaptive edge-based interpolation for scanning rate conversion	tv signals adaptive edge based interpolation scanning rate conversion edge based line average algorithm vlsi architecture computer simulation;interpolation;video signal processing;edge detection;image converters;very large scale integration;interpolation tv very large scale integration hdtv image converters computer architecture computer simulation hardware contracts adaptive algorithm;edge based line average algorithm;contracts;adaptive edge based interpolation;computer architecture;adaptive algorithm;adaptive signal processing;scanning rate conversion;hdtv;vlsi;digital signal processing chips;tv;video signal processing adaptive signal processing interpolation vlsi digital signal processing chips edge detection;computer simulation;tv signals;hardware;vlsi architecture	An adaptive technique for scanning rate conversion and interpolation is proposed. This technique performs better than the edge based line average algorithm, especially for an image with more horizontal edges. Moreover, it is easy to implement and a simple VLSI architecture is proposed in this paper. Computer simulation shows that a 37.0 dB image can be obtained via our proposed technique, while edge based line average algorithm only achieve 35.2 dB.	interpolation	Chung J. Kuo;Ching Liao;Ching C. Lin	1996		10.1109/ICASSP.1996.545734	computer simulation;computer vision;computer science;theoretical computer science;very-large-scale integration	Crypto	45.90773724335614	-24.634881529160673	121623
910ce36163c98e7abfdd611af8ec1484100dafcd	localization using low-resolution optical sensors	optical sensors wireless sensor networks localization;field programmable gate array;optical sensors wireless sensor networks intelligent sensors object detection sensor arrays acoustic sensors sensor systems field programmable gate arrays cameras radio frequency;data collection;localization;low resolution;high resolution camera low resolution optical sensors wireless sensor networks passive localization visual object identification techniques field programmable gate array sensor platform fpga resource constrained environment;sensor network;wireless sensor network;wireless sensor networks cameras field programmable gate arrays optical sensors;high resolution camera;optical sensors;field programmable gate arrays;optical sensor;object identification;wireless sensor networks;cameras	In wireless sensor networks, the data collected from numerous low end sensors can be equivalent or superior to a few high fidelity sensors. In this paper, we evaluate the validity of this concept with low-resolution optical sensors. We implemented passive localization using visual object identification techniques on a field programmable gate array (FPGA) sensor platform. The complexity, memory utilization, and accuracy of our algorithms were analyzed for performance on a resource constrained environment. We show that low-resolution optical sensors can accurately localize objects on a global two dimensional plane. We also present a model that quantifies fidelity versus scale for optical sensors. Using our model and application, we demonstrate that low-fidelity optical sensors are an effective tool in sensor networks and at a certain scale can provide superior coverage of an area over a high-resolution camera. This analysis lays the groundwork for future advances in optical sensors.	acoustic cryptanalysis;algorithm;complexity;convolution;field-programmable gate array;image resolution;internationalization and localization;pixel;rf modulator;radio frequency;sensor	Tammara Massey;Rahul Kapur;Foad Dabiri;Linh Nam Vu;Majid Sarrafzadeh	2007	2007 IEEE Internatonal Conference on Mobile Adhoc and Sensor Systems	10.1109/MOBHOC.2007.4428621	embedded system;wireless sensor network;computer science	Mobile	51.1700355772139	-32.40086384626702	121626
7f9aee86f3dcf011971e7e80c38e5eb9fe8e8304	comparison of boosting based terrain classification using proprioceptive and exteroceptive data		The terrain classification is a very important subject to the all-terrain robotics community. The knowledge of the type of terrain allows a rover to deal with its environment more efficiently. The work presented in this paper shows that it is possible to differentiate terrains based on their aspects, using exteroceptive sensors, as well as based on their influence on the rover’s behavior, using proprioceptive sensors. Using a boosting method (AdaBoost), these two sets of classifiers are trained and applied independently. The resulting dual algorithm identifies offline the nature of the terrain on which the vehicle is virtually driving and classifies it according to categories previously labeled, such as sand or grass. Due to the good results obtained for the classification based solely on each type of sensor, this paper concludes that the correlation between data from proprioceptive and exteroceptive sensors could be used for further applications. This paper is a summarized version of the one presented at the ISER conference.	adaboost;boosting (machine learning);diffusing update algorithm;online and offline;robotics;rover (the prisoner);sensor	Ambroise Krebs;Cédric Pradalier;Roland Siegwart	2008		10.1007/978-3-642-00196-3_11	computer vision;simulation;artificial intelligence	Robotics	48.174341676988675	-36.47465040462853	121804
8c0b760d777e68fca377b4664ea5bb21d620b44d	1ms column parallel vision system and it s application of high speed target tracking	vision system;real time visualization;image features;cycle time;ccd camera;machine vision target tracking robot vision systems robot control feedback visual servoing charge coupled devices charge coupled image sensors pixel cameras;1 ms parallel vision system target tracking parallel processor array parallel architecture visual feedback feature extraction visual servoing active vision;robot vision;robot control;parallel architectures;feature extraction;feature extraction robot vision active vision target tracking parallel architectures;visual feedback;parallel architecture;target tracking;visual servoing;high speed;active vision	0 Robot control using a real-time visual feedback has been recently improved (visual servoing.) Conventional vision systems are too slow for these application, because the CCD cameras are restricted to the video frame rate ( N T S C 30Hz, PAL 25Hz). To solve this problem, we have developed a 1ms vision system, to provide a far faster frame rate than that of the conventional systems. Our 1ms vision system has a 128×128 PD array and an all parallel processor array connect to each other in a column parallel architecture, so that the bottleneck of an image transfer has been solved. 1ms visual feedback has been realized in this system, in which the image feature value is extracted in 1ms cycle-time for visual servoing. We have also developed a high speed Active Vision System (A VS)-II, which makes a gaze of vision system to move at high speed. In this paper, we will provide a detail discussion on our 1ms vision system and its performance through some experiments.	active vision;algorithm;charge-coupled device;dynamical system;experiment;feature (computer vision);image processing;online advertising;pal;parallel computing;processor array;real-time locating system;real-time transcription;robot control;user interface;visual servoing	Yoshihiro Nakabo;Masatoshi Ishikawa;Haruyoshi Toyoda;Seiichiro Mizuno	2000		10.1109/ROBOT.2000.844126	embedded system;computer vision;active vision;machine vision;feature extraction;cycle time variation;computer science;artificial intelligence;robot control;charge-coupled device;visual servoing;feature;computer graphics (images)	Robotics	45.2069325750723	-35.61163058983869	122139
7e71799dde4255d7c71edd8543566e2352e22c0a	incorporating range sensing in the robot navigation function	modelizacion;navegacion;robot movil;trajectoire;range data;incertidumbre;uncertainty;planning artificial intelligence;tactile algorithms range sensing mobile robot navigation point automaton unknown obstacles local sensing information stereo vision range finder active sensing motion planning;robot navigation;planning artificial intelligence computer vision distance measurement mobile robots;mobile robots;robotics;computer vision;algorithme;modelisation;algorithm;distance measurement;navigation;planificacion;trajectory;robot mobile;mobile robot navigation;stereo vision;motion planning;robotica;robot sensing systems navigation robot kinematics robotics and automation mobile robots stereo vision motion planning automata shape robot vision systems;planning;trayectoria;robotique;incertitude;planification;modeling;moving robot;algoritmo	A model of mobile robot navigation is considered whereby the robot is a point automaton operating in an environment with unknown obstacles of arbitrary shapes. The robot’s input information includes its own and the target point coordinates, as well as local sensing information such as from stereo vision or a range finder. These algorithmic issues are addressed 1) Is it possible to combine sensing and planning functions, thus producing, similar to the way it is done in nature, “active sensing” guided by the needs of planning? (The answer is “yes”). 2) Can richer sensing (e.g., stereo vision versus tactile) guarantee better performance, that is, resulting in shorter paths? (The general answer is “no.”) A paradigm for combining range data with motion planning is presented. It turns out that extensive modifications of simpler “tactile” algorithms are needed to take full advantage of additional sensing capabilities. Two algorithms that guarantee convergence and exhibit different “styles” of behavior are described, and their performance is demonstrated in simulated examples.	algorithm;automated planning and scheduling;automaton;mobile robot;motion planning;navigation function;programming paradigm;robotic mapping;simulation;stereopsis	Vladimir J. Lumelsky;Tim Skewis	1990	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.59969	planning;mobile robot;computer vision;navigation;simulation;systems modeling;uncertainty;computer science;stereopsis;artificial intelligence;trajectory;motion planning;robotics;mobile robot navigation	Robotics	52.06567946256831	-29.71294853397894	122297
85c747bfa982d99e29ac5f6f6f933176e52ccec2	x-ray vision with only wifi power measurements using rytov wave models	robot sensing systems;wireless lans;wave motion;x ray imaging antennas computerised instrumentation control engineering computing image resolution path planning position control power measurement radiotelemetry wireless lan;signal processing;imaging;ieee 802 11 standards;mathematical model;approximation methods;ieee 802 11 standard;see through imaging framework x ray vision wifi power measurement rytov wave model unmanned vehicle wireless power measurement wireless local area network wlan card wave propagation scattering phenomena sparse signal processing robotic path planning high resolution imaging line of sight los antenna alignment error;trajectory control;autonomous vehicle guidance;imaging ieee 802 11 standards robot kinematics mathematical model approximation methods robot sensing systems;line of sight;robot kinematics;x rays	In this paper, unmanned vehicles are tasked with seeing a completely unknown area behind thick walls based on only wireless power measurements using wireless local area network (WLAN) cards. We show that a proper modeling of wave propagation that considers scattering and other propagation phenomena can result in a considerable improvement in see-through imaging. More specifically, we develop a theoretical and experimental framework for this problem based on Rytov wave models and integrate it with sparse signal processing and robotic path planning. Our experimental results show high-resolution imaging of three different areas, validating the proposed framework. Moreover, they show considerable performance improvement over the state of the art that only considers the line-of-sight (LOS) path, allowing us to image more complex areas not possible before. Finally, we show the impact of robot positioning and antenna alignment errors on our see-through imaging framework.	image resolution;line-of-sight (missile);motion planning;robot;signal processing;software propagation;sparse matrix;unmanned aerial vehicle	Saandeep Depatla;Lucas Buckland;Yasamin Mostofi	2015	IEEE Transactions on Vehicular Technology	10.1109/TVT.2015.2397446	electronic engineering;simulation;telecommunications;computer science;engineering;wave;signal processing;mathematical model;robot kinematics;computer network	Robotics	47.65573729990339	-29.815172909420927	122510
ce838f7f825810d7b6a982aec8d36f219f8571da	an efficient hardware design of sift algorithm using fault tolerant reversible logic	logic gates hardware adders fault tolerance fault tolerant systems algorithm design and analysis feature extraction;video rate constraint sift algorithm scale invariant feature transform image features generation fault tolerant reversible logic field programmable gate array fpga digital hardware logic adder functions multiplier functions soc based robotic vision system system on chip low power cmos design complimentary metal oxide semiconductors dsp applications digital signal processing battery operated embedded systems image processing rft gates reversible fault tolerant gates image parameters;logic gates;fault tolerant systems;adders;feature extraction;fault tolerance;logic gates fault tolerant computing feature extraction field programmable gate arrays;system generator scale invariant feature transform sift reversible logic fault tolerant difference of gaussian dog reversible fault tolerant rft gate system on chip soc fpga field programmable gate array;algorithm design and analysis;hardware	Scale Invariant Feature Transform (SIFT) algorithm is used to generate image features which is very essential for object recognition, feature detection, image matching etc. This paper proposes an optimized hardware architecture for realizing the SIFT algorithm with reversible logic prototyped using Field Programmable Gate Array (FPGA). The digital hardware logic has been implemented with reversible and fault tolerant capabilities at significant design sections substituting the adder and multiplier functions which is one of the first of its kind of implementation of this application needed for designing energy efficient systems such as SoC (System on Chip) based robotic vision system. Reversible logic is emerging as an important research area for low power CMOS design, DSP applications and battery operated embedded systems meant for image processing. The reversible logic is implemented using our new proposed RFT (Reversible Fault Tolerant) gates (which is reversible as well as fault tolerant) that is used to design a new innovated adder circuit. The new adder circuit uses very less hardware resource which is again substituted with minimum complexity reversible gate. The proposed design shows invariancy to various image parameters such as scale, rotation, viewpoint and noise unlike other state of the art works. Moreover our design can process a frame of resolution 640*480 in 15 millisecond, at a rate of 64 frames per second which meets the real time video rate constraint, what represents a speed up of 415x compared to the software execution of the method.	adder (electronics);algorithm;cmos;digital electronics;electronic hardware;embedded system;fault tolerance;feature detection (computer vision);feature detection (web development);field-programmable gate array;image noise;image processing;image registration;outline of object recognition;request for tender;reversible computing;robot;scale-invariant feature transform;system on a chip	Chandrajit Pal;Pabitra Das;Sudhindu Bikash Mandal;Amlan Chakrabarti;Samik Basu;Ranjan Ghosh	2015	2015 IEEE 2nd International Conference on Recent Trends in Information Systems (ReTIS)	10.1109/ReTIS.2015.7232933	embedded system;electronic engineering;real-time computing;programmable logic array;computer science	EDA	44.61861719164266	-35.304256991227284	122595
9e57d64d41d34ecdce9ef257e88c81b0f25e6490	preliminary study of obstacles detecting method in farmland based on stereovision	obstacle detection;agricultural robot;stereovision;stereo correspondence	A preliminary study of obstacles detecting method in farmland based on stereovision was developed for obstacle avoidance of agricultural robot,five different obstacles and their position information in different environments were got. Camera calibration,image acquisition,stereo rectification, stereo correspondence and depth calculation were developed in this paper,by using the methods such as Bouguet algorithm,area match,triangulation,and so on,and computer vision library OpenCV was used to improve the real time property. The experiment showed that the obstacles can be detected correctly and the results are satisfactory. But, the method needed further improvement to be universal for various obstacles.	sensor;stereopsis	Fuzeng Yang;Shan Liu;Liping Chen;Yuanjie Wang;Zheng Wang;Xinxing Xu	2011		10.1007/978-3-642-31919-8_86	computer vision;simulation;geography;remote sensing	HCI	53.2027702480565	-35.07376982119416	122633
64a506cfde2f4343b0ec03c9f773c1870263f66f	sensor synchronization for ar applications	sensor synchronization;sensors;stable synchronization sensor synchronization ar application hardware affect host processor mobile phone low cost consumer grade device;computer vision;mobile phone;temporal matching;synchronisation;temporal matching sensor synchronization;mobile handsets;synchronization clocks cameras universal serial bus accuracy calibration estimation;augmented reality;cameras;direct method;synchronisation augmented reality cameras computer vision mobile handsets sensors	In this paper, we give a brief introduction to the sensor synchronization problem, highlight how the choice of hardware affects what synchronization methods can be applied, and present our ongoing research on sensor synchronization. Our work is based on estimating timestamps on the host processor, using a method suitable for mobile phones and other low-cost consumer grade devices since it does not require special hardware support. We also describe an experiment to measure sensor synchronization performance using a simple calibration rig. Our initial results show that the estimated timestamps provide a stable synchronization and a clear improvement in synchronization accuracy compared to the direct method of using sample arrival times as timestamps.	direct method in the calculus of variations;mobile phone;sensor;synchronization (computer science);trusted timestamping	Tuomas Kantonen	2010	2010 IEEE International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2010.5643589	direct method;clock synchronization;embedded system;synchronization;computer vision;augmented reality;real-time computing;simulation;computer science;sensor;operating system;data synchronization;frame synchronization	Robotics	46.15548634407416	-36.54400527044009	122782
dae1737f7e39b8842b9d741c54ef935f84352820	circular data matrix fiducial system and robust image processing for a wearable vision-inertial self-tracker	image processing;robustness image processing sensor fusion smart cameras intelligent sensors wearable sensors prototypes measurement units digital signal processing printers;nonuniform lighting circular data matrix fiducial system robust image processing wearable low power hybrid vision inertial self tracker flexible sensor fusion core architecture reconfiguration inertial measurement unit outward looking wide angle smart camera built in dsp 2d bar coded fiducials black and white printer uninterrupted tracking real world lighting conditions large building campus homomorphic image processing algorithms;sensor fusion computer vision optical tracking augmented reality;inertial measurement unit;computer vision;low power;optical tracking;sensor fusion;augmented reality	A wearable low-power hybrid vision-inertial tracker has been demonstrated based on a flexible sensor fusion core architecture, which allows easy reconfiguration by plugging-in different kinds of sensors. A particular prototype implementation consists of one inertial measurement unit and one outward-looking wide-angle Smart Camera, with a built-in DSP to run all required image-processing tasks. The Smart Camera operates on newly designed 2-D bar-coded fiducials printed on a standard black-and-white printer. The fiducial design allows having thousands of different codes, thus enabling uninterrupted tracking throughout a large building or even a campus at very reasonable cost. The system operates in various real-world lighting conditions without any user intervention due to homomorphic image processing algorithms for extracting fiducials in the presence of very non-uniform lighting .	algorithm;canonical account;code;fiducial marker;image processing;intel core (microarchitecture);low-power broadcasting;printer (computing);printing;prototype;sensor;smart camera;wearable computer	Leonid Naimark;Eric Foxlin	2002		10.1109/ISMAR.2002.1115065	embedded system;inertial measurement unit;computer vision;augmented reality;simulation;image processing;computer science;sensor fusion	Mobile	46.19981131726011	-36.10129229045348	122809
e1670305a15d928973c592df4bb5f3bfef4247e1	rrt+ : fast planning for high-dimensional configuration spaces		In this paper we propose a new family of RRT based algorithms, named RRT+, that are able to find faster solutions in high-dimensional configuration spaces compared to other existing RRT variants by finding paths in lower dimensional subspaces of the configuration space. The method can be easily applied to complex hyperredundant systems and can be adapted by other RRT based planners. We introduce RRT+ and develop some variants, called PrioritizedRRT+, PrioritizedRRT+-Connect, and PrioritizedBidirectionalT-RRT+, that use the new sampling technique and we show that our method provides faster results than the corresponding original algorithms. Experiments using the state-of-the-art planners available in OMPL show superior performance of RRT+ for high-dimensional motion planning problems.	algorithm;best, worst and average case;experiment;iteration;motion planning;ompl;sampling (signal processing);spaces	Marios Xanthidis;Ioannis M. Rekleitis;Jason M. O'Kane	2016	CoRR		configuration space;engineering;linear subspace;control engineering;motion planning;sampling (statistics);algorithm	Robotics	51.529007979717235	-24.09774774707688	122943
75fec47092ebdec118fac54ce1664fc700349357	steps toward derandomizing rrts	state space methods;motion control;path planning;randomised algorithms;robotics;state space methods path planning random processes randomised algorithms tree searching;commande mouvement;control movimiento;search trees;rapidly exploring random tree;state space;random processes;motion planning;robotica;robotique;sampling methods space exploration state space methods computer science urban planning cryptography sorting performance analysis dispersion performance evaluation;tree searching;randomization motion planning rapidly exploring random tree voronoi bias search trees state space method	We present two new motion planning algorithms, based on the rapidly exploring random tree (RRT) family of algorithms. These algorithms represent the first work in the direction of derandomizing RRTs; this is a very challenging problem due to the way randomization is used in RRTs. In RRTs, randomization is used to create Voronoi bias, which causes the search trees to rapidly explore the state space. Our algorithms take steps to increase the Voronoi bias and to retain this property without the use of randomization. Studying these and related algorithms would improve our understanding of how efficient exploration can be accomplished, and would hopefully lead to improved planners. We give experimental results that illustrate how the new algorithms explore the state space and how they compare with existing RRT algorithms.	computation;data structure;ms-dos;motion planning;randomized algorithm;rapidly-exploring random tree;sampling (signal processing);selection bias;state space	Stephen R. Lindemann;Steven M. LaValle	2004	Proceedings of the Fourth International Workshop on Robot Motion and Control (IEEE Cat. No.04EX891)	10.1109/ROMOCO.2004.240739	mathematical optimization;simulation;computer science;artificial intelligence;machine learning;mathematics;motion planning;robotics	Robotics	51.78473223245576	-23.96374781791244	122988
6dd7b2bf9cd532abcc6b3d1d54b17e4fbaac4809	fast marching adaptive sampling	sampling methods mobile robots path planning;submarine cable fast marching adaptive sampling least cost path adaptive sequential sampling algorithm fmex confidence bound sampling maximum variance sampling bathymetric data sampling;reactive and sensor based planning motion and path planning probability and statistical methods;frequency modulation planning robot sensing systems uncertainty computational modeling level set	A challenging problem for autonomous exploration is estimating the utility of future samples. In this paper, we consider the problem of placing observations over an initially unknown continuous cost field to find the least-cost path from a fixed start to a fixed goal position. We propose the adaptive sequential sampling algorithm FMEx to successively select observation locations that maximize the probability of improving the best path. FMEx evaluates a set of proposed observation locations using a novel fast marching update method and selects a location based on the probabilistic likelihood of improving the current best path. Simulated results show that FMEx finds lower-cost paths with fewer samples than random, maximum variance and confidence bound sampling. We also show results for sampling bathymetric data to find the best route for a submarine cable. In problems where sampling is expensive, FMEx selects observation locations that minimize the true path cost.	adaptive sampling;algorithm;autonomous robot;bathymetry;fast marching method;sampling (signal processing)	Nicholas R. J. Lawrance;Jen Jen Chung;Geoffrey A. Hollinger	2017	IEEE Robotics and Automation Letters	10.1109/LRA.2017.2651148	computer vision;mathematical optimization;computer science;machine learning	Robotics	52.09533569263106	-25.608450733542742	123021
132dbe131a254a248282d695530901a80c21b858	simple agents learn to find their way: an introduction on mapping polygons	mobile robot;map construction;polygon mapping;autonomous agent;visibility graph reconstruction	This paper gives an introduction to the problem of mapping simple polygons with autonomous agents. We focus on minimalistic agents that move from vertex to vertex along straight lines inside a polygon, using their sensors to gather local data at each vertex. Our attention revolves around the question whether a given configuration of sensors and movement capabilities of the agents allows them to capture enough data in order to draw conclusions regarding the global layout of the polygon. In particular, we study the problem of reconstructing the visibility graph of a simple polygon by an agent moving either inside or on the boundary of the polygon. Our aim is to provide insight about the algorithmic challenges faced by an agent trying to map a polygon. We present an overview of techniques for solving this problem with agents that are equipped with simple sensorial capabilities. We illustrate these techniques on examples with sensors that measure angles between lines of sight or identify the previous location. We also give an overview over related problems in combinatorial geometry as well as graph exploration. © 2013 Elsevier B.V. All rights reserved.	algorithm;autonomous agent;autonomous robot;sensor;visibility graph	Jérémie Chalopin;Shantanu Das;Yann Disser;Matús Mihalák;Peter Widmayer	2013	Discrete Applied Mathematics	10.1016/j.dam.2013.01.006	mobile robot;computer vision;mathematical optimization;combinatorics;simulation;point in polygon;visibility polygon;autonomous agent;mathematics;geometry	AI	52.205894864499655	-28.808574638311573	123057
3cd94fd6247fcc48570ab2355f8cb109750cde05	sparse latent space policy search	policy search;reinforcement learning;robotics;dimensionality reduction	Computational agents often need to learn policies that involve many control variables, e.g., a robot needs to control several joints simultaneously. Learning a policy with a high number of parameters, however, usually requires a large number of training samples. We introduce a reinforcement learning method for sampleefficient policy search that exploits correlations between control variables. Such correlations are particularly frequent in motor skill learning tasks. The introduced method uses Variational Inference to estimate policy parameters, while at the same time uncovering a lowdimensional latent space of controls. Prior knowledge about the task and the structure of the learning agent can be provided by specifying groups of potentially correlated parameters. This information is then used to impose sparsity constraints on the mapping between the high-dimensional space of controls and a lowerdimensional latent space. In experiments with a simulated bi-manual manipulator, the new approach effectively identifies synergies between joints, performs efficient low-dimensional policy search, and outperforms state-of-the-art policy search methods.	computation;experiment;reinforcement learning;sparse matrix;synergy;variational principle	Kevin Sebastian Luck;Joni Pajarinen;Erik Berger;Ville Kyrki;Heni Ben Amor	2016			simulation;computer science;artificial intelligence;machine learning;robotics;reinforcement learning;dimensionality reduction	AI	48.394523210200504	-26.54438665840952	123207
16b441f7495e26ad56c1904ef2f4b0162bfd538f	design of novel algorithm and architecture for gaussian based color image enhancement system for real time applications		This paper presents the development of a new algorithm for Gaussian based color image enhancement system. The algorithm has been designed into architecture suitable for FPGA/ASIC implementation. The color image enhancement is achieved by first convolving an original image with a Gaussian kernel since Gaussian distribution is a point spread function which smoothes the image. Further, logarithmdomain processing and gain/offset corrections are employed in order to enhance and translate pixels into the display range of 0 to 255. The proposed algorithm not only provides better dynamic range compression and color rendition effect but also achieves color constancy in an image. The design exploits high degrees of pipelining and parallel processing to achieve real time performance. The design has been realized by RTL compliant Verilog coding and fits into a single FPGA with a gate count utilization of 321,804. The proposed method is implemented using Xilinx Virtex-II Pro XC2VP40-7FF1148 FPGA device and is capable of processing high resolution color motion pictures of sizes of up to 1600×1200 pixels at the real time video rate of 116 frames per second. This shows that the proposed design would work for not only still images but also for high resolution video sequences.	algorithm;application-specific integrated circuit;color image;convolution;display resolution;dynamic range;fits;field-programmable gate array;gate count;holomatix rendition;image editing;image resolution;parallel computing;pipeline (computing);pixel;smoothing;verilog;virtex (fpga)	M. C. Hanumantharaju;M. Ravishankar;D. R. Ramesh Babu	2013	CoRR	10.1007/978-3-642-36321-4_56	embedded system;computer vision;electronic engineering;computer science;computer graphics (images)	Robotics	43.27667400499889	-34.004929829221346	123276
38966099e0127703eac2141d6507794f950df72c	vehicle classification and accurate speed calculation using multi-element piezoelectric sensor	axles tires classification algorithms accuracy feature extraction motorcycles;roads computerised monitoring data acquisition intelligent transportation systems piezoelectric transducers;vehicle speed calculation piezoelectric sensor vehicle classification track width estimation;intelligent transportation system activity vehicle classification accurate speed calculation vehicle monitoring passenger vehicles highway conditions classification decision axle spacing track width vehicle front axle tires data acquisition unit single traffic lane multielement piezoelectric sensor class 1 motorcycle vehicles inductive loops vehicle parameters pavement damage us dot departments of transportation its	Vehicle monitoring and classification is a necessary Intelligent Transportation System ITS activity, as nationwide departments of transportation (DOT) use the information to effectively design safe and durable roadways. Because over 70% of the weight of goods shipped in the U.S. are trucked, substantial pavement damage is becoming more and more problematic [1]. Thus, an accurate classification system for estimating vehicle parameters is sorely needed. Currently, the most widely used classification solution consists of a combination of inductive loops and piezoelectric sensors. Installing these systems causes pavement damage. Even more challenging is that current systems greatly under-classify class 1 motorcycle vehicles. In this paper we present a novel system for classifying vehicles and determining track width and speed. The system employs a multi-element piezoelectric sensor positioned diagonally across a single traffic lane; a data acquisition unit; and a processing and classification algorithm operating on a computing device. Vehicle front axle tires distinctively impact different element sensors, which aids in calculating track width, speed and axle spacing. Given these factors, a classification decision can be made using vehicle axle spacing. The developed system was tested on highway conditions. Classification accuracy was 86.9% overall and even better for class 1 motorcycles (100%) and passenger vehicles (98.9%).	algorithm;computer;data acquisition;piezoelectricity;sensor	Samer A. Rajab;Ahmad Mayeli;Hazem H. Refai	2014	2014 IEEE Intelligent Vehicles Symposium Proceedings	10.1109/IVS.2014.6856432	embedded system;engineering;automotive engineering;forensic engineering	Robotics	39.73670197429473	-34.22377702643364	123492
77bd9cf3c1acfece527032ad0b838b583cf9c694	reliability of the tiletrack capacitive user tracking system in smart home environment	transmitters reliability capacitance receivers footwear legged locomotion tiles;reliability;legged locomotion;ultrasonic positioning system tiletrack capacitive user tracking system reliability smart home environment short reliability analysis device free human positioning system passive human positioning system capacitive measurements system reliability moving human tracking passive indoor positioning;footwear;receivers;ultrasonic measurement capacitance measurement reliability;transmitters;capacitance;tiles;smart home indoor positioning capacitive measurement	This paper presents a short reliability analysis of the passive and device-free human positioning system called TileTrack. The presented tracking system is fully unobtrusive and requires the user to neither to wear any tags nor to perform any special action, such as talk, to be tracked. Thus, it promotes the concept of calm technology by pushing the sensing actions to the background. The system uses capacitive measurements to determine the placement of feet on different transmitting floor segments. The system's reliability and capability to track moving humans was tested in a real apartment, and the test results are presented as well as analyzed in this paper.	calm technology;home automation;positioning system;tracking system;transmitter;unobtrusive javascript	Tero Kivimäki;Timo Vuorela;Miika Valtonen;Jukka Vanhala	2013	ICT 2013	10.1109/ICTEL.2013.6632162	embedded system;transmitter;electronic engineering;simulation;engineering;electrical engineering;reliability;capacitance;statistics	Robotics	45.58235379659791	-26.28113069343422	123571
8799ae1260d278296140f577248bc6db2ceecd9e	building occupancy estimation with people flow modeling in anylogic	kalman filters behavioural sciences computing digital simulation estimation theory;buildings estimation kalman filters data models cameras measurement uncertainty real time systems;kalman filters;measurement uncertainty;estimation;cameras;buildings;data models;real time systems;people flow modeling occupancy estimation people flow estimation complicated people behavior simulation people simulation softwares kalman filter high rise buildings anylogic	With more and more high rise buildings in modern cities, people flow and occupancy estimation has gained lots of attention in recent years. Most previous works on this topic are based on the Kalman filter (KF) due to its high efficiency. However, the Kalman filter requires good people flow models which are usually hard to get due to the complexity of people behavior. People simulation softwares such as AnyLogic have the capability to simulate complicated people behaviors in buildings. In this work, AnyLogic is adopted to simulate people flow and identify the people flow model, which is further used in the Kalman filter to estimate people flow in both simulation and real people experiment.	anylogic;kalman filter;simulation	Keyu Li;Kai Zhang;Xiangbao Li;Jie Xi;Hui Fang;Zhen Jia	2016	2016 12th IEEE International Conference on Control and Automation (ICCA)	10.1109/ICCA.2016.7505355	kalman filter;data modeling;computer vision;econometrics;estimation;simulation;computer science;engineering;mathematics;statistics;measurement uncertainty	Robotics	49.757868160441525	-32.03556334510747	123596
55ec7808d8aa10d81e3a0250880964f750ab0e8c	an economical tonal display for interactive graphics and image analysis data	image analysis;interactive graphics	Abstract   The use of tonal displays in image analysis and interactive graphics has always dictated the use of expensive refresh memories for the display output device. This has involved the use of high speed digital drums, multiple head discs, and analog storage tubes. Recently, the introduction of very long shift registers has allowed the designer to consider their use for refresh memories. A prototype display using 1024 bit MOS static shift registers has been developed. It has been shown that a reasonable cost versus performance tradeoff can be obtained. The first efforts has resulted in a 128 × 128 × 4 bit (64k) memory; it is now in the process of being expanded to 256 × 256 × 8 bits (512k). This memory is cost competitive with digital disc memories and both cost and performance competitive with storage tube scan converters.	graphics;image analysis	T. E. McCracken;B. W. Sherman;Samuel J. Dwyer	1975	Computers & Graphics	10.1016/0097-8493(75)90037-0	computer vision;computer hardware;computer science;operating system;multimedia;computer graphics (images)	Graphics	42.99716849179416	-24.697180428652267	124278
97f9f47789875896d345231c656d151e03a86518	methods for precise false-overlap detection in tile-based rendering	false overlap;tile binning;edge walk test;rasterizer;precise false overlap detection method;counting x ratio;data mining;crossed product;strontium;tile list tile based rendering false overlap;graphics processing;tile list;image edge detection;overlap test;detection algorithm;overlap test tile based rendering precise false overlap detection method graphics processing tile binning embedded device rasterizer cross product test edge walk test counting x ratio;data access;tile based rendering;tiles;external memory;cross product test;tiles rendering computer graphics testing graphics hardware costs partitioning algorithms iterative algorithms embedded computing computer science;rendering computer graphics;information filters;embedded device	In graphics processing, overlap test is a crucial step before tile-binning in tile-based rendering for embedded devices. An object in a frame is decomposed into primitives, triangles of different sizes, for processing. In tile-binning process, these triangular primitives are typically represented by bounding boxes. However, the bounding box of a primitive usually covers a significant number of tiles which are not overlapped by the primitive. These tiles are called false-overlap tiles and approximate 70% of the tiles of a bounding box. Therefore, in tile-based rendering, identifying and eliminating those false-overlap tiles in a bounding box to reduce both storage pressures in tile-binning and data accesses of external memory for rasterizer become inviting. Existing false-overlap detection algorithms are either too tedious to reduce computation or too rough to gain high coverage. In this paper, we propose three methods to eliminate all false-overlap tiles: Cross-Product Test (CPT), Edge-Walk Test (EWT), and Counting X-Ratio (CXR). We partition the bounding box of a primitive into three rectangles at most according to the number of primitive vertices which are also the vertices of the bounding box. The edges of the primitive then become the diagonals of these rectangles, and false overlap detection becomes a well-formulated math processing. The false-overlap detection of these three rectangles may be processed in parallel to improve performance further. The proposed methods are tested using Doom3 and Quake4 for different screen sizes.	approximation algorithm;cpt (file format);computation;computer graphics;embedded system;language primitive;minimum bounding box;overlap–add method;product binning;rasterisation;tiled rendering;vertex (geometry)	Hsiu-ching Hsieh;Chih-Chieh Hsiao;Hui-Chin Yang;Chung-Ping Chung;Jean Jyh-Jiun Shann	2009	2009 International Conference on Computational Science and Engineering	10.1109/CSE.2009.466	bounding volume;data access;rasterisation;tiled rendering;bounding interval hierarchy;strontium;computer hardware;computer science;minimum bounding box;theoretical computer science;operating system;data mining;minimum bounding box algorithms;bounding volume hierarchy;computer graphics (images)	DB	44.587912652770584	-31.266181878467656	124416
2ef97099e56b8f4b35ce4c1a4d96b63de4ea8378	fast colour balance adjustment of ikonos imagery using cuda	remote sensing image;remote sensing image processing parallel processing shared memory systems;data parallel;sensor technology;image segmentation;image processing;image resolution;color;probability density function;graphics processor unit;compute unified device architecture;image sensors;data mining;processing time;cuda;computer architecture;ikonos imagery;graphical user interfaces;shared memory systems;parallel architectures;geophysical signal processing;remote sensing image processing;remote sensing;satellites;color optical sensors satellites image sensors image resolution graphics computer architecture remote sensing image processing;decision support systems;graphics processors;remote sensing image processing fast colour balance adjustment ikonos imagery cuda sensor technology image resolution optical satellites satellite imagery mosaic composition processing time color balancing graphics processor unit data parallel capability compute unified device architecture;optical satellites;satellite imagery;color balancing;optical sensors;remote sensing geophysical signal processing geophysical techniques graphical user interfaces image resolution image segmentation parallel architectures;fast colour balance adjustment;shared memory system;parallel processing;graphics;geophysical techniques;mosaic composition;data parallel capability	With the advancement of sensor technology, the image resolution of optical satellites improves, increasing image size in the process. For satellite imagery mosaic composition, the task of performing colour balancing becomes more intensive due the sheer size of data needed to be processed. In order to reduce the processing time for color balancing, the proposed procedure takes advantage of the graphics processor unit's data-parallel capabilities using compute unified device architecture (CUDA). By repartitioning the image data and modifying the conventional colour balancing algorithm to suit CUDA's requirements, we are able to cut down the processing time to a fraction of the original time taken by conventional methods, showing CUDA's capabilities in remote sensing image processing applications.	algorithm;cuda;color balance;graphics processing unit;image processing;image resolution;requirement	Yong Kiat Allan Tan;Wee Juan Tan;Leong Keong Kwoh	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779179	parallel processing;computer vision;probability density function;image resolution;image processing;computer science;sensor;graphics;image sensor;graphical user interface;image segmentation;satellite;remote sensing;computer graphics (images)	Embedded	42.77484370672295	-34.14582968114478	124519
26edcd7cc0c79fbb851a844240ed57ad670d9594	parallel approaches to integration with applications in optical coherence tomography	gpu;integration;parallel optimization;real time imaging	This paper presents a comparison between three GPU-based parallel approaches to integration in digital signal processing (DSP). In many cases in practice, the integration is applied on large signals. Its implementation in real-time systems imposes strict limitations on the computational time. In cases where sequential implementation does not meet these limitations, parallel optimization is expected to provide a solution. The discussed parallel approaches are based on the many-core architecture of the GPU. These approaches are employed in generating synthesized scanning laser ophthalmoscopy (SLO) images. These images are based on multiple en-face images, generated by optical coherence tomography (OCT) system. Optimal parallel approaches, which meet the real-time criterion, are identified.	algorithm;cuda;computation;digital signal processing;fast fourier transform;graphics processing unit;image scanner;intel core (microarchitecture);manycore processor;mathematical optimization;paradiseo;real-time clock;real-time computing;real-time locating system;time complexity;tomography;zermelo–fraenkel set theory	Konstantin Kapinchev;Fred R. M. Barnes;Sylvain Rivet;Adrian Bradu;Adrian Gh. Podoleanu	2016	2016 10th International Conference on Signal Processing and Communication Systems (ICSPCS)	10.1109/ICSPCS.2016.7843350	computer vision;computer science;theoretical computer science;computer graphics (images)	HPC	41.30680849285293	-33.053364025074536	124763
44dca51b3f0e1d4323c4c22992df513591ea681b	analysis of the possibility of using radar tracking method based on grnn for processing sonar spatial data		This paper presents the approach of applying radar tracking methods for tracking underwater objects using stationary sonar. Authors introduce exist- ing in navigation methods of target tracking with particular attention to methods based on neural filters. Their specific implementation for sonar spatial data is also described. The results of conducted experiments with the use of real sono- grams are presented.	radar;sonar	Witold Kazimierski;Grzegorz Zaniewicz	2014		10.1007/978-3-319-08729-0_32	computer vision;speech recognition;engineering;artificial intelligence	Robotics	50.21343281432885	-34.08283959955665	124827
dbff0953718d28d6c73673799636fecbdfe4a910	tidynamics: a tiny package to compute the dynamics of stochastic and molecular simulations			molecular modelling;simulation	Pierre de Buyl	2018	J. Open Source Software	10.21105/joss.00877	physics	SE	42.94418058811326	-25.912041559196638	125207
fd4a85621fb14bc5c9befc409885b24b2fc2339b	short-term visual mapping and robot localization based on learning classifier systems and self-organizing maps	robot trajectory control visual mapping robot localization learning classifier system self organizing map ground wheeled autonomous robot driverless car position estimation intelligent vehicle navigation mobile robot neural network visual memory;robots navigation neurons trajectory roads visualization vehicles;wheels control engineering computing intelligent transportation systems learning artificial intelligence mobile robots path planning road vehicles robot vision self organising feature maps slam robots trajectory control	Ground wheeled autonomous robots like driverless cars are examples of applications which would assist humans on different tasks. From an explicit or emerging need, these systems have come to replace or assist drivers. Estimating the position is a primary function for intelligent vehicle navigation. Different existing solutions use high-end sensors. This paper proposes to augment the autonomy level of a mobile robot based on learning classifier systems and self-organizing maps. From a simple monocular system, whilst the classifier system leads the robot for topological localization tasks, the neural network is applied as a short-term visual memory for internal representation of the environment. These two concepts are presented as separate approaches, wherein each method performs a specific task for the robot's trajectory control.	learning classifier system;organizing (structure);robotic mapping;self-organization;self-organizing map	Arthur Miranda Neto	2015		10.1109/IVS.2015.7225692	mobile robot;robot learning;computer vision;simulation;engineering;artificial intelligence;social robot;robot control;mobile robot navigation;personal robot	Robotics	50.35403115633683	-30.23754168114481	125283
e6de39743094cf3e853a40be2a8c917bf9ef39ab	identifying mirror symmetry density with delay in spiking neural networks		The ability to rapidly identify symmetry and anti-symmetry is an essential attribute of intelligence. Symmetry perception is a central process in human vision and may be key to human 3D visualization. While previous work in understanding neuron symmetry perception has concentrated on the neuron as an integrator, here we show how the coincidence detecting property of the spiking neuron can be used to reveal symmetry density in spatial data. We develop a method for synchronizing symmetry-identifying spiking artificial neural networks to enable layering and feedback in the network. We show a method for building a network capable of identifying symmetry density between sets of data and present a digital logic implementation demonstrating an 8x8 leaky-integrate-and-fire symmetry detector in a field programmable gate array. Our results show that the efficiencies of spiking neural networks can be harnessed to rapidly identify symmetry in spatial data with applications in image processing, 3D computer vision, and robotics.	artificial neuron;biological neuron model;boolean algebra;computer stereo vision;computer vision;feedback;field-programmable gate array;image processing;neural networks;robotics;sensor;spiking neural network;visualization (graphics)	Jonathan K. George;Cesare Soci;Volker J. Sorger	2017	CoRR		machine learning;mirror symmetry;integrator;artificial neural network;spiking neural network;field-programmable gate array;image processing;artificial intelligence;spatial analysis;mathematics;detector	AI	45.883459441601126	-32.59375353401364	125979
d66ec156971b57c0fe6e7e99743182b0221fc8ff	weighted synergy graphs for role assignment in ad hoc heterogeneous robot teams	graph theory;normal distribution;intelligent robots;approximation algorithms;training;space exploration;mobile robots;weighted graph structure weighted synergy graph ad hoc heterogeneous robot team optimal role assignment policy normal distribution learning algorithm team formation algorithm log likelihood robot simulation robocup rescue domain foraging task;normal distribution graph theory intelligent robots learning systems mobile robots multi robot systems;robots training approximation algorithms gaussian distribution computational modeling cities and towns space exploration;learning systems;computational modeling;robots;multi robot systems;cities and towns;gaussian distribution	Heterogeneous robot teams are formed to perform complex tasks that are sub-divided into different roles. In ad hoc domains, the capabilities of the robots and how well they perform as a team is initially unknown, and the goal is to find the optimal role assignment policy of the robots that will attain the highest value. In this paper, we formally define the weighted synergy graph for role assignment (WeSGRA), that models the capabilities of robots in different roles as Normal distributions, and uses a weighted graph structure to model how different role assignments affect the overall team value. We contribute a learning algorithm that learns a WeSGRA from training examples of role assignment policies and observed values, and a team formation algorithm that approximates the optimal role assignment policy. We evaluate our model and algorithms in extensive experiments, and show that the learning algorithm learns a WeSGRA model with high log-likelihood that is used to form a near-optimal team. Further, we apply the WeSGRA model to simulated robots in the RoboCup Rescue domain, and to real robots in a foraging task, and show that the role assignment policy found by WeSGRA attains a high value and outperforms other algorithms, thus demonstrating the efficacy of the WeSGRA model.	algorithm;cross-validation (statistics);experiment;heterogeneous computing;hoc (programming language);interaction;lego mindstorms;nao (robot);nxt;robot;simulation;synergy;test data	Somchaya Liemhetcharat;Manuela M. Veloso	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6386027	normal distribution;simulation;computer science;artificial intelligence;graph theory;machine learning;statistics	Robotics	49.09986423551486	-25.89556344628309	125995
bdcda88e9e7272741cdc4b0a592b40372828a3e9	scene matching based visual slam navigation for small unmanned aerial vehicle	scene matching;suav nonlinear state model;slam robots autonomous aerial vehicles image matching kalman filters mobile robots navigation nonlinear control systems position control robot vision;suav;visual slam;image matching;nonlinear control systems;weighted hausdorff distance;kalman filters;nonlinear relationship model;positioning accuracy visual slam navigation algorithm small unmanned aerial vehicle simultaneous localization and mapping ekf extended kalman filtering scene matching method weighted hausdorff distance waypoint accurate abstraction suav nonlinear state model nonlinear relationship model data association continuous prediction continuous estimation visual information estimation error navigation system;mobile robots;data association;noise measurement;simultaneous localization and mapping image edge detection visualization navigation jacobian matrices noise measurement;continuous prediction;small unmanned aerial vehicle;navigation;suav visual slam scene matching ekf;visualization;robot vision;scene matching method;position control;image edge detection;visual information;visual slam navigation algorithm;simultaneous localization and mapping;positioning accuracy;extended kalman filtering;navigation system;estimation error;ekf;jacobian matrices;slam robots;autonomous aerial vehicles;continuous estimation;waypoint accurate abstraction	A scene matching based visual SLAM (simultaneous localization and mapping) navigation algorithm is proposed for SUAV (small unmanned aerial vehicle) which described by EKF (Extended Kalman Filtering). Firstly, a scene matching method with weighted Hausdorff distance was introduced for waypoints accurate abstraction. On this foundation, the SUAV's nonlinear state model was analyzed to establish nonlinear relationship model between the measurement and the waypoints, and then the state of the model was predicted and estimated to deal with data association and extend the state for new waypoints. Last, through continuously predicting and estimating, the algorithm located the SUAV accurately by visual information. Simulation results show that the proposed algorithm could effectively reduce the estimation error of navigation system and improves the positioning accuracy for SUAV.	aerial photography;algorithm;correspondence problem;extended kalman filter;hausdorff dimension;nonlinear system;simulation;simultaneous localization and mapping;unmanned aerial vehicle	Yao-Jun Li;Quan Pan;Zhenlu Jin;Chunhui Zhao;Feng Yang	2012	2012 15th International Conference on Information Fusion		computer vision;simulation;geography;machine learning	Robotics	53.28160169237022	-37.269946452603314	126096
5049bb066fbe5297b080afa60ec1e4903a0c7c8a	an architecture for person-following using active target search		This paper addresses a novel architecture for person-following robots using active search. The proposed system can be applied in real-time to general mobile robots for learning features of a human, detecting and tracking, and finally navigating towards that person. To succeed at personfollowing, perception, planning, and robot behavior need to be integrated properly. Toward this end, an active target searching capability, including prediction and navigation toward vantage locations for finding human targets, is proposed. The proposed capability aims at improving the robustness and efficiency for tracking and following people under dynamic conditions such as crowded environments. A multi-modal sensor information approach including fusing an RGB-D sensor and a laser scanner, is pursued to robustly track and identify human targets. Bayesian filtering for keeping track of human and a regression algorithm to predict the trajectory of people are investigated. In order to make the robot autonomous, the proposed framework relies on a behavior-tree structure. Using Toyota Human Support Robot (HSR), real-time experiments demonstrate that the proposed architecture can generate fast, efficient person-following behaviors.	algorithm;autonomous robot;experiment;hierarchical state routing;interaction;loss function;mobile robot;modal logic;motion planning;naive bayes spam filtering;planning;real-time clock;real-time locating system;reinforcement learning;sensor;statistical model;support vector machine;tree structure;utility	Minkyu Kim;Miguel Arduengo;Nick Walker;Yuqian Jiang;Justin W. Hart;Peter Stone;Luis Sentis	2018	CoRR		engineering;simulation;behavior-based robotics;robot;computer vision;filter (signal processing);architecture;robustness (computer science);mobile robot;artificial intelligence	Robotics	50.12640268930516	-34.56808839395475	126582
a48ef73b4fd71c224f84dea5683922fd64abc855	cheap joint probabilistic data association filters in an interacting multiple model design	driver assistance;joint probabilistic data association filter;vision sensor;symmetry;shadow;joint probabilistic data association;virtual sensor;visual features;sensor fusion;interacting multiple model	This paper presents an approach to fuse multiple sensors in an Interacting Multiple Model design. Visual features like shadow and symmetry, treated as independent stand-alone virtual sensors, are employed for detection and tracking of vehicles for driver assistance tasks. Cheap Joint Probabilistic Data Association is utilised to account for the large amount of clutter in the measurements provided by these sensors. Special attention is devoted to the different noise characteristics of the measurements. The individual sensors are considered in a sequential manner, leading to a versatile fusion architecture that allows easy integration of further sensor modules.	radar tracker	Christian Hoffmann;Thao Dang	2009	Robotics and Autonomous Systems	10.1016/j.robot.2008.10.009	computer vision;shadow;simulation;joint probabilistic data association filter;computer science;machine learning;sensor fusion;symmetry	Robotics	46.75031597962672	-37.82114157048307	126827
ab46439b9f5bd75a356a8d30f024b7640731d87b	a strategy for efficient observation pruning in multi-objective 3d slam	search and rescue;mobile robot;three dimensional;visualization;robot vision;trajectory;estimation;simultaneous localization and mapping;simultaneous localisation and mapping;distributed search;simultaneous localization and mapping cameras estimation visualization robot vision systems trajectory;robot vision systems;cameras;conference proceeding	An efficient automatic solution to the feature-based simultaneous localisation and mapping (SLAM) of mobile robots operating in conditions where a number of competing objectives operate simultaneously is proposed. The formulation quantitatively measures the merit of incoming data with respect to multiple priorities, automatically adjusting the amount of observations to be used in the estimation process for the best possible combined outcome. The methodology enables a selection mechanism which can efficiently exploit the observations available to the robot to best fulfil the objectives of differing tasks throughout the course of a mission, e.g. localisation, mapping, exploration, feature distribution, searching for specific objects or victims, etc. The work is particularly motivated by navigation in three-dimensional terrains, and an example considering the objectives of robot localisation and map expansion in a search and rescue environment using an RGB-D camera is utilised for discussion and results.	mobile robot;simultaneous localization and mapping	Jaime Valls Miró;Weizhen Zhou;Gamini Dissanayake	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6095156	mobile robot;three-dimensional space;computer vision;estimation;simulation;visualization;computer science;artificial intelligence;trajectory;mobile robot navigation;simultaneous localization and mapping	Robotics	52.567275483360305	-33.1169579954167	127096
dbea951b76fa58e3b112462afcc1c41e66867859	first results in detecting and avoiding frontal obstacles from a monocular camera for micro unmanned aerial vehicles	microrobots;aircraft control;image matching;robot vision;collision avoidance;frontal obstacle detection frame to frame enlargement quadrotor autonomous flight tests image spacing template matching surf feature matches image patches relative size change detection monocular cameras payload constraints lightweight microaerial vehicles microunmanned aerial vehicles frontal obstacle avoidance;robot vision aircraft control autonomous aerial vehicles collision avoidance helicopters image matching microrobots;helicopters;autonomous aerial vehicles;vehicles optical sensors biomedical optical imaging collision avoidance cameras feature extraction optical imaging	Obstacle avoidance is desirable for lightweight micro aerial vehicles and is a challenging problem since the payload constraints only permit monocular cameras and obstacles cannot be directly observed. Depth can however be inferred based on various cues in the image. Prior work has examined optical flow, and perspective cues, however these methods cannot handle frontal obstacles well. In this paper we examine the problem of detecting obstacles right in front of the vehicle. We developed a method to detect relative size changes of image patches that is able to detect size changes in the absence of optical flow. The method uses SURF feature matches in combination with template matching to compare relative obstacle sizes with different image spacing. We present results from our algorithm in autonomous flight tests on a small quadrotor. We are able to detect obstacles with a frame-to-frame enlargement of 120% with a high confidence and confirmed our algorithm in 20 successful flight experiments. In future work, we will improve the control algorithms to avoid more complicated obstacle configurations.	aerial photography;algorithm;autonomous robot;central processing unit;depth perception;experiment;field-programmable gate array;image resolution;laptop;on-board data handling;optical flow;real-time computing;response time (technology);sensor;signal processing;speeded up robust features;template matching;unmanned aerial vehicle	Tomoyuki Mori;Sebastian Scherer	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630807	computer vision;simulation;remote sensing	Robotics	49.19648228491432	-37.219101734499496	127142
81fc5a085be0b2a49e94637f5306d5d5f836403f	monitoring the static vehicle load on a heavy goods vehicle	strain measurement calibration;axle loads;strain measurement;vehicles axles strain instruments strain measurement loading wheels;static loads;conference paper;weigh in motion heavy goods vehicles;spot load data display static vehicle load monitoring axle strain measurement instrumented heavy good vehicle calibration daily weigh bridge measurement vehicle tracking unit;trucks;calibration	Static vehicle load can be calculated accurately from measurements of axle strain. This paper describes, in outline form, a series of laboratory and field tests on an instrumented heavy goods vehicle. The vehicle was instrumented in the laboratory and an extensive series of calibration tests performed to establish the relationship between measurements of axle strain and vehicle load, this testing regime was also used to validate the instrumentation. A field trial was then conducted with the vehicle being returned to service and axle strain measurements being recorded continuously. Estimates of the static load were then validated against daily weigh-bridge measurements. The system has been in continuous service for several years now and has proven to be a robust and accurate method of calculating static vehicle load which can be easily installed on a vehicle at relatively low cost. When combined with a vehicle tracking unit real time load data can be made available to a fleet operator and on the spot load data displayed to the driver. Inadvertent overloading, with all its implications for safety and road damage, can thus be avoided.	function overloading;structural load;vehicle tracking system	Ivor Humphreys;Paul McDonald;Margaret O'Mahony;Dermot Geraghty	2015	2015 IEEE 18th International Conference on Intelligent Transportation Systems	10.1109/ITSC.2015.233	engineering;automotive engineering;transport engineering;forensic engineering	Robotics	39.802590746236326	-34.26071627253202	127446
066329e7c445003157a124892d84193fe9f85fd9	stereo vision system for moving object detecting and locating based on cmos image sensor and dsp chip	stereo vision;cmos;moving object detecting;dsp	Currently, most of the stereo vision systems are constructed on PC-based or multi-CPU combination structures with two CCD cameras. It is difficult to be applied in movable plants for stand-alone requirement. Due to electronic technology development, the complementary metal-oxide semiconductor (CMOS) image sensor has been widely used in a lot of electronic commercial products and the digital signal processor (DSP) operation speed and capacity are good enough for stereo vision system requirement. Here, a new stereo vision platform is designed with TMS320C6416 DSK board integrated with two CMOS color image sensors for detecting and locating moving objects. The data communication interface, system monitoring timing flow, and image pre-processing software programs are developed, too. This system can be used to detect and track any moving object without object color and shape limitations of previous study. Experimental results are used to evaluate this system’s dynamic performance. This low cost stereo vision system can be employed in movable platform for stand-alone application, i.e., mobile robot.	autoregressive integrated moving average;cmos;central processing unit;charge-coupled device;color image;computation;digital signal processor;image processing;image sensor;interaction;mobile robot;preprocessor;principle of good enough;semiconductor;signal processing;stereopsis;sum of absolute differences;system monitor;system requirements;toys	Shiuh-Jer Huang;Fu-Ren Ying	2010	Pattern Analysis and Applications	10.1007/s10044-010-0197-3	computer stereo vision;embedded system;stereo cameras;computer vision;computer science;stereopsis;digital signal processing;cmos	Robotics	44.92224021060218	-35.321164954534915	127545
dd670f753871cd2a93095df0a3ac5f25ff350f57	self-supervised terrain classification based on moving objects using monocular camera	moving object;terrain mapping image classification image motion analysis mobile robots path planning robot vision;image motion analysis;visual appearance self supervised terrain classification moving objects observation monocular camera autonomous robot safe pathway;path planning;supervised classification;image classification;mobile robots;data mining;robot vision;roads;image color analysis;feature extraction;robots;error rate;humans;vehicles;terrain mapping;roads robots humans image color analysis data mining vehicles feature extraction;autonomous robot	For autonomous robots equipped with a camera, terrain classification is essential in finding a safe pathway to a destination. Terrain classification is based on learning, but the amount of data cannot be infinite. This paper presents a self-supervised classification approach to enable a robot to learn the visual appearance of terrain classes in various outdoor environments by observing moving objects, such as humans and vehicles, and to learn about the terrain, based on their paths of movement. We verified the performance of our proposed method experimentally and compared the results with those obtained using supervised classification. The difference in error rates between self-supervised and supervised methods was about 0–11%.	autonomous robot;experiment;gene regulatory network;machine learning;supervised learning	Donghui Song;Chuho Yi;Il Hong Suh;Byung-Uk Choi	2011	2011 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2011.6181340	robot;mobile robot;computer vision;contextual image classification;simulation;feature extraction;word error rate;computer science;artificial intelligence;motion planning;remote sensing	Robotics	48.203488382094186	-36.44732561568717	127792
6ff5bfca41c7a842064526947910671446d50021	a minimalist algorithm for multirobot continuous coverage	robot sensing systems;memoryless robots;minimalist algorithm;navigation robot sensing systems rfid tags markov processes inverse problems cleaning;real time;rfid tag;real time search ant like algorithms coverage distributed robot systems;navigation;rfid tags;inverse problem;multirobot controlled frequency coverage;multirobot continuous coverage;multi robot systems;markov process;distributed robot systems;coverage;markov processes;memoryless robots minimalist algorithm multirobot continuous coverage multirobot controlled frequency coverage frequency distribution;ant like algorithms;frequency distribution;real time search;cleaning;inverse problems;distributed robotics	This paper describes an algorithm, which has been specifically designed to solve the problem of multirobot-controlled frequency coverage (MRCFC), in which a team of robots are requested to repeatedly visit a set of predefined locations of the environment according to a specified frequency distribution. The algorithm has low requirements in terms of computational power, does not require inter-robot communication, and can even be implemented on memoryless robots. Moreover, it has proven to be statistically complete as well as easily implementable on real, marketable robot swarms for real-world applications.	algorithm;algorithmic efficiency;computation;experiment;inter-process communication;radio-frequency identification;requirement;robot	Giorgio Cannata;Antonio Sgorbissa	2011	IEEE Transactions on Robotics	10.1109/TRO.2011.2104510	radio-frequency identification;real-time computing;simulation;inverse problem;engineering;mathematics;distributed computing;markov process;statistics	Robotics	53.37478389394841	-25.374649355739205	128193
8b58b55da53b79981d2a7e5fd4ad77d47855744d	scalable coverage path planning for cleaning robots using rectangular map decomposition on large environments		The goal of coverage path planning is to create a path that covers the entire free space in a given environment. Coverage path planning is the most important component of cleaning robot technology, because it determines the cleaning robot’s movement. When the environment covered by a cleaning robot is extremely large and contains many obstacles, the computation for coverage path planning can be complicated. This can result in significant degradation of the execution time for coverage path planning. Not many studies have focused on the scalability of coverage path planning methods. In this paper, we propose a scalable coverage path planning method based on rectangular map decomposition. The experimental results demonstrate that the proposed method reduces the execution time for coverage path planning up to 14 times when compared with conventional methods.	automated planning and scheduling;computation;elegant degradation;motion planning;plasma cleaning;robot;run time (program lifecycle phase);scalability;sputter cleaning	Xu Miao;Jaesung Lee;Bo-Yeong Kang	2018	IEEE Access	10.1109/ACCESS.2018.2853146	computer science;distributed computing;scalability;motion planning;robot;computation;robot kinematics	Robotics	52.95314619432418	-24.843275461624522	128306
700ab8577f882d5e612f942b4c45d3ade5219fd7	grid map building based on d-s evidence theory	two dimensional grid map building;environment maps;robot sensing systems;d s evidence theory;environment exploration effect;approximate process algorithm;unstructured unknown environment mapping;map building;sensors;mobile robot;fuses;ultrasonic sensor d s evidence theory grid map building mobile robot;acoustics;multisensor information fusion;evidence theory;mobile robots;uncertainty handling;data mining;unstructured unknown environment mapping d s evidence theory ultrasonic sensors dempster shafer evidence theory mobile robot multisensor information fusion approximate process algorithm two dimensional grid map building environment exploration effect;uncertainty handling grid computing mobile robots sensor fusion sensors ultrasonic devices;mobile robots path planning decision making grid computing telecommunication computing buildings civil engineering computer architecture sun sensor fusion;ultrasonic sensors;dempster shafer;grid map building;dempster shafer evidence theory;information fusion;sensor fusion;grid computing;ultrasonic devices;ultrasonic sensor	A method of constructing a grid map using ultrasonic sensors based on Dempster-Shafer evidence theory (D-S evidence theory) is proposed with respect to the problem of unstructured unknown environment mapping. Mobile robot explores environment with ultrasonic sensors; D-S evidence theory is used to fuse multi-sensor information; The problem that D-S evidence theory can’t be applied to information fusion under certain circumstances and the matter that D-S evidence theory have counter-intuitive behaviors in some cases are discussed; An approximate process algorithm is advanced to avoid above problems; Finally, A two-dimensional grid map is built and a way of evaluating the environment and environment exploration effect is suggested. Simulation result shows that this method is appropriate for unstructured unknown environment mapping.		Hongyu Cao;Zhi Tan;Hanxu Sun;Tao Yu	2009		10.1109/ICNC.2009.349	computer vision;engineering;artificial intelligence;machine learning	NLP	52.71229225527999	-33.99705160734152	128425
843d2a82ea757b42c12c35616fe980a9aa6c371f	measurement fusion via covariance intersection for ballistic missile tracking			ballistic missile;covariance intersection	Mack Pasqual;Daniel DeLaurentis	2012		10.2514/6.2012-2564	computer vision	Robotics	51.34480603631504	-33.80418634730844	128517
f9aaa7eb7094c3da200548f36fbff27d181b4d9c	a rrt-based motion planning of dual-arm robot for (dis)assembly tasks	manipulation path planning algorithm rrt based motion planning disassembly tasks assembly tasks dual arm robot assembly path fixed grasp regrasping path rapidly exploring random tree algorithm manipulation path linear puzzles alpha puzzles;path planning;manipulation motion planning rrt rapidly exploring random trees dual arm robot;trees mathematics;trees mathematics assembly planning grippers industrial manipulators path planning random processes;rrt rapidly exploring random trees;assembly planning;dual arm robot;robots;grippers;random processes;motion planning;manipulation;industrial manipulators	This paper presents the manipulation planning of a dual-arm robot for an assembly task. To assemble the parts, two types of paths for the robot are required: assembly and re-grasping. The assembly path assembles and holds the objects with a fixed grasp, while the re-grasping path changes the robots grasp on objects depending on how the object was picked up and how it will be used. We implement a Rapidly-exploring Random Trees (RRT) algorithm to generate the assembly path and the re-grasping path in different ways to obtain the manipulation path of the dual-arm robot for the assembly task. Finally, both simple problems like linear puzzles and very complicated problems like alpha puzzles can be solved using the proposed manipulation planning algorithm. These examples are tested through simulation.	algorithm;automated planning and scheduling;motion planning;robot;simulation	Dong-Hyung Kim;Sung-Jin Lim;Duck-Hyun Lee;Ji Yeong Lee;Chang-Soo Han	2013	IEEE ISR 2013	10.1109/ISR.2013.6695698	stochastic process;computer vision;simulation;computer science;artificial intelligence;operations management;motion planning	Robotics	52.76411613574282	-24.21018005371242	128717
1899b0c051637cd0f8adfbf0fc17e2cc8237c4f1	neural network based signal processing scheme for automatic tool wear recognition	manufacturing systems;machining;sensor phenomena and characterization;neural networks;real time;neural networks signal processing computerized monitoring condition monitoring machining sensor phenomena and characterization signal to noise ratio drilling life estimation manufacturing systems;computerized monitoring;drilling;condition monitoring;automatic detection;signal processing;tool wear;life estimation;machine tool;signal to noise ratio;neural network	"""Monitoring of components in the mufactwing plants involves the automatic detection and identification of Mure events. One of the important machine monitoring problems is the monitoring of tool wear in automatic metal drilling systems. The purpose of tool wear detection systems is to actually track down the wearing prooess of the machining tool, allowing the estimation of the quality of parts being machined by tool and prediction of the nseful life of tools. Conventional methods of detecting the tool wear from pmcessing the sensor measured signals h e led to tool wear detection systems which perform well for a given set of machining parameters, but are not capable of meeting performance requirements in real ma"""" . g operations, where the machining parameters are more varied. This paper reports a automatic tool wear mgnition scheme based on neural network technology. This technolgy provides an improved tool wear recognition alternative because of potential ofneural networks to operate in real time mode and to handle data that may be distorted and noisy."""	artificial neural network;hidden markov model;neural network software;requirement;sensor;signal processing;statistical classification	Girija Chetty	1996	Fourth International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.1996.615093	machining;drilling;computer science;machine tool;signal-to-noise ratio;artificial neural network	Embedded	39.26155699864297	-31.218391478285938	128811
9e889a407d2332b65d092d51de0b16ecbfabae83	a hierarchical approach to manipulation with diverse actions	manipulators;path planning;search problems manipulators mobile robots path planning;mobile robots;bidirectional search diverse action manipulation dama problem mobile robot movable objects nonprehensile manipulation actions goal configuration multimodal planning problem hierarchical algorithm multimodal nature forward search sampling algorithm bidirectional version complicated manipulation domain;search problems;planning collision avoidance bidirectional control grasping legged locomotion clutter;article	We define the Diverse Action Manipulation (DAMA) problem in which we are given a mobile robot, a set of movable objects, and a set of diverse, possibly non-prehensile manipulation actions, and the goal is to find a sequence of actions that moves each of the objects to a goal configuration. We show that the DAMA problem can be framed as a multi-modal planning problem and describe a hierarchical algorithm that takes advantage of this multi-modal nature. We also extend our earlier forward search sampling algorithm to a bi-directional version. We give results on a complicated manipulation domain and demonstrate that both new algorithms are significantly more efficient than the original, and that the hierarchical algorithm is usually much more efficient than the forward or bi-directional searches.	algorithm;bi-directional text;mobile manipulator;mobile robot;modal logic;sampling (signal processing)	Jennifer L. Barry;Leslie Pack Kaelbling;Tomás Lozano-Pérez	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630814	mobile robot;simulation;computer science;artificial intelligence;mathematics;motion planning	Robotics	52.325315150760204	-24.385271468718663	129278
37b1a08a62e11468fccb773d68eb006a4650dab6	multi-task learning of system dynamics with maximum information gain	training training data robots covariance matrix kernel mathematical model heuristic algorithms;kernel;high dimensional state action space;bayesian technique;maximum information gain;high dimensionality;mogp;robotic blimp multitask learning system dynamics maximum information gain robotic system multiple output gaussian process high dimensional state action space bayesian technique machine learning over fitting computational cost reduction informative training set mogp cart pole system;gaussian processes;system dynamics;bayes methods;training;robotic blimp;robotic system;cart pole system;dynamic system;multiple output gaussian process;training data;machine learning;over fitting;heuristic algorithms;multi task learning;aerospace robotics;robots;airships;mathematical model;robot dynamics aerospace robotics airships bayes methods gaussian processes learning artificial intelligence;gaussian process;learning artificial intelligence;informative training set;robot dynamics;information gain;learning strategies;computational cost reduction;multitask learning;heuristic algorithm;covariance matrix	This paper introduces a new approach to adaptively learn the dynamics of a robotic system. The methodology is based on maximizing the information gain from new observations while modeling the dynamics with a Multiple Output Gaussian Process (MOGP). High-dimensional state-action spaces with unknown dependencies between inputs and outputs can be highly computationally expensive to learn. Gaussian process modeling is a Bayesian technique that naturally overcomes one of the most difficult problems in machine learning known as over-fitting. This makes it very appealing for on-line problems where testing multiple hypothesis is difficult. The computational cost of the learning task is reduced by having a smaller dataset of informative training points. Therefore we introduce a learning strategy capable of determining the most informative training set for the MOGP. This method can be implemented for learning the behavior of dynamic systems where due to their complexity and disturbances are infeasible to be analytically defined. The benefits of our approach are verified in two experiments: learning the dynamics of a cart-pole system in simulation and the dynamics of a robotic blimp.	analysis of algorithms;approximation algorithm;computational complexity theory;computer multitasking;cross-covariance;dynamical system;experiment;gaussian process;information gain in decision trees;iteration;kullback–leibler divergence;machine learning;multi-task learning;online and offline;overfitting;process modeling;robot;simulation;system dynamics;test set	Jose F. Zubizarreta-Rodriguez;Fabio Tozeto Ramos	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5979961	multi-task learning;simulation;computer science;artificial intelligence;machine learning;control theory;gaussian process;statistics	Robotics	49.20970591599595	-25.99532397110306	129348
197770c77caa79dc87082547dd62b7fa7e7d334f	topological navigation in configuration space applied to soccer robots	configuration space;principal component analysis;navigation system;robot soccer;robot programming	This paper describes a topological navigation system, based on the description of key-places by a reduced number of parameters that represent images associated to specific locations in configuration space, and the application of the developed system to robotic soccer, through the implementation of the developed algorithms to RoboCup MiddleSize League (MSL) robots, under the scope of the SocRob project (Soccer Robots or Society of Robots). A topological map is associated with a graph, where each node corresponds to a key-place. Using this approach, navigation is reduced to a graph path search. Principal Components Analysis was used to represent key-places from pre-acquired images and to recognize them at navigation time. The method revealed a promising performance navigating between key-places and proved to adapt to different graphs. Furthermore, it leads to a robot programming language based on qualitative descriptions of the target locations in configuration space (e.g., Near Blue Goal with the Goal on its Left). Simulation results of the method application are presented, using a realistic simulator.	algorithm;principal component analysis;programming language;robot fighting league;simulation	Gonçalo Neto;Hugo Costelha;Pedro U. Lima	2003		10.1007/978-3-540-25940-4_51	configuration space;computer vision;simulation;computer science;artificial intelligence;mobile robot navigation;principal component analysis	Robotics	51.514720793120546	-37.89417681519612	129419
2cecf93ea197bca9be7d403236726c33ded8a4c8	self-localization based on visual lane marking maps: an accurate low-cost approach for autonomous driving		Autonomous driving in public roads requires precise localization within the range of few centimeters. Even the best localization systems based on GNSS cannot always reach this level of precision, especially in an urban environment, where the signal is disturbed by surrounding buildings and artifacts. Recent works have shown the advantage of using maps as a precise, robust, and reliable way of localization. Typical approaches use the set of current readings from the vehicle sensors to estimate its position on the map. The approach presented in this paper exploits a short-range visual lane marking detector and a dead reckoning system to construct a registry of the detected back lane markings corresponding to the last 240 m driven. This information is used to search in the map the most similar section, to determine the vehicle localization in the map reference. Additional filtering is used to obtain a more robust estimation for the localization. The accuracy obtained is sufficiently high to allow autonomous driving in a narrow road. The system uses a low-cost architecture of sensors and the algorithm is light enough to run on low-power embedded architecture.	aerial photography;algorithm;autonomous car;autonomous robot;bayesian network;bias–variance tradeoff;course (navigation);dead reckoning;embedded system;global serializability;google street view;grid reference;gyroscope;hidden surface determination;image resolution;internationalization and localization;item unique identification;lateral thinking;low-power broadcasting;map;map matching;pixel;satellite navigation;sensor;tree accumulation	Rafael Peixoto Derenzi Vivacqua;Massimo Bertozzi;Pietro Cerri;Felipe Nascimento Martins;Raquel Frizera Vassallo	2018	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2017.2752461	computer vision;filter (signal processing);robustness (computer science);artificial intelligence;simulation;architecture;engineering;digital mapping;cost approach;systems architecture;gnss applications;dead reckoning	Robotics	52.81482243271778	-36.928108603827184	129459
2c4e5d1e59aa1d46b65b19dc12805579958ff01b	semantic representation for navigation in large-scale environments	observability;robot sensing systems;path planning mobile robots;semantics navigation buildings observability planning robot sensing systems;semantics;navigation;mobile robots semantic navigation representation object based representation;planning;buildings	Mimicking human navigation is a challenging goal for autonomous robots. This requires to explicitly take into account not only geometric representation but also high-level interpretation of the environment. In this paper, we demonstrate the capability to infer a route in a global map by using semantics. Our approach relies on an object-based representation of the world automatically built by robots from spherical images. In addition, we propose a new approach to specify paths in terms of high-level robot actions. This path description provides robots with the ability to interact with humans in an intuitive way. We perform experiments on simulated and real-world data, demonstrating the ability of our approach to deal with complex large-scale outdoor environments whilst dealing with labelling errors.	autonomous robot;experiment;high- and low-level;human–robot interaction;motion planning;object-based language;semantic web;simulation;while	Romain Drouilly;Patrick Rives;Benoit Morisset	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139314	planning;computer vision;navigation;observability;simulation;computer science;control theory;semantics;mobile robot navigation	Robotics	51.38571157492981	-36.03194195415887	129787
9b06d38f2cce4b2883d74215deab8a5ec377a8a3	fpga-based real-time moving target detection system for unmanned aerial vehicle application		Moving target detection is the most common task for Unmanned Aerial Vehicle (UAV) to find and track object of interest from a bird’s eye view in mobile aerial surveillance for civilian applications such as search and rescue operation. The complex detection algorithm can be implemented in a real-time embedded system using Field Programmable Gate Array (FPGA).This paper presents the development of real-timemoving target detection System-on-Chip (SoC) using FPGA for deployment on a UAV.The detection algorithm utilizes area-based image registration technique which includes motion estimation and object segmentation processes. The moving target detection system has been prototyped on a low-cost Terasic DE2-115 board mounted with TRDB-D5M camera. The system consists of Nios II processor and stream-oriented dedicated hardware accelerators running at 100MHz clock rate, achieving 30-frame per second processing speed for 640 × 480 pixels’ resolution greyscale videos.		Jia Wei Tang;Nasir Shaikh-Husin;Usman Ullah Sheikh;Muhammad N. Marsono	2016	Int. J. Reconfig. Comp.	10.1155/2016/8457908	embedded system;computer vision;simulation	Robotics	44.82592146757299	-35.734578623595795	130224
1571ca5518fc2b544c60ac1e2e32da6cf2ba5b66	bayesian filtering for location estimation	kalman filters bayes methods statistical analysis sensor fusion ubiquitous computing;sensors;location estimation;bayes methods;pervasive computing;kalman filters;measurement uncertainty;statistical analysis;bayesian methods filtering filters pervasive computing infrared sensors sensor systems and applications state estimation time measurement cameras sensor systems;bayesian filtering;bayesian filter techniques;ubiquitous computing;location awareness;sensor fusion;kalman filters bayesian filtering location estimation statistical tool measurement uncertainty multisensor fusion identity estimation pervasive computing;bayes filters	L ocation awareness is important to many pervasive computing applications. Unfortunately, no location sensor takes perfect measurements or works well in all situations. Thus, the motivation behind this article is twofold. First, we believe the pervasive computing community will benefit from a concise survey of Bayesian-filter techniques. Because no sensor is perfect, representing and operating on uncertainty with a statistical tool such as Bayes filters is key in any system using many sensors. Second, estimating an object’s location is arguably the most fundamental sensing task in many pervasive computing scenarios. It is thus a natural domain in which to illustrate the application of Bayesian filter techniques. Representing locations statistically enables a unified interface for location information. This lets us write applications independent of the sensors used—even when using very different sensor types, such as GPS and infrared badges. (A comparative survey of location systems appears elsewhere.1) Here, we illustrate fusing sensor data from ultrasound and infrared tags. We also discuss how to combine high-resolution location information from anonymous laser range finders with low-resolution location sensors that provide identification. Bayes filters Bayes filters2 probabilistically estimate a dynamic system’s state from noisy observations. In location estimation for pervasive computing, the state is a person’s or object’s location, and location sensors provide observations about the state. The state could be a simple 2D position or a complex vector including 3D position, pitch, roll, yaw, and linear and rotational velocities. Bayes filters represent the state at time t by random variables xt. At each point in time, a probability distribution over xt, called belief, Bel(xt), represents the uncertainty. Bayes filters aim to sequentially estimate such beliefs over the state space conditioned on all information contained in the sensor data. To illustrate, let’s assume that the sensor data consists of a sequence of time-indexed sensor observations z1, z2, ..., zt. The belief Bel(xt) is then defined by the posterior density over the random variable xt conditioned on all sensor data available at time t:	c date and time functions;dynamical system;global positioning system;image resolution;pitch (music);sensor;state space;ubiquitous computing;yaws	Dieter Fox;Jeffrey Hightower;Lin Liao;Dirk Schulz;Gaetano Borriello	2003	IEEE Pervasive Computing	10.1109/MPRV.2003.1228524	kalman filter;human–computer interaction;computer science;sensor;pattern recognition;data mining;sensor fusion;ubiquitous computing;measurement uncertainty	HCI	51.123220938299596	-34.77388929137426	130388
3cc7f6c25b46b5fbf9d63a30838db71c5cb4582a	a data fusion architecture for the dynamic follow-up of vehicles	sensor systems;omnidirectional vision;uncertainty;sensor fusion computer vision driver information systems inference mechanisms road safety;vehicle detection;inference mechanisms;vehicle driving;data fusion;computer vision;laser fusion;laser theory;laser telemeter;vehicle dynamics vehicles uncertainty vehicle detection vehicle driving road safety machine vision sensor systems laser fusion laser theory;omnidirectional vision sensor;machine vision;dempster shafer theory;vehicles;sensor fusion;road safety;laser telemeter data fusion road safety driving assistance dempster shafer theory omnidirectional vision sensor;driver information systems;vehicle dynamics;driving assistance	This article concerns road safety and driving assistance. To solve this problem, we propose a data fusion architecture based on the Dempster-Shafer theory. This multilevel approach allows the management of complementary and redundant data which come from two perception systems: an omnidirectional vision sensor and a laser telemeter. The originality of this architecture is its ability to manage and propagate uncertainties from low level data until a high level information of danger given to the driver. The first part concerns the data sensor. The second part deals with the quantification of the uncertainties of the detected vehicles, followed by a determination of situations of danger and the evaluation of their level of dangerousness with the aim of supplying the driver with an indicator of global danger around the vehicle.	device driver;high-level programming language;propagation of uncertainty	Arnaud Clerentin;Eric Brassart;Laurent Delahoche;Bruno Marhic;Sonia Izri	2007	Proceedings 2007 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2007.363997	computer vision;simulation;machine vision;computer science;engineering;sensor fusion;computer security	Robotics	50.47748037557935	-35.61455929322131	131346
5f85b0d6ef3fe17bdddfd4a65d9d15c1cf27310b	detection of geological structure using gamma logs for autonomous mining	libraries;robotic research autonomous remotely operated mining subsurface geological structure detection in ground geological information control decision planning decision in ground geological boundary detection geophysical logging sensors supervised learning algorithm gaussian processes single length scale squared exponential covariance function iron ore mine australia automatic detection gamma logs;supervised learning algorithm;planning decision;covariance analysis;supervised learning;subsurface geological structure detection;sensors;gaussian processes;mining;geophysical logging sensors;iron;control decision;well logging;data mining;accuracy;iron libraries rocks data mining fuel processing industries accuracy;well logging covariance analysis gaussian processes geology industrial robots learning artificial intelligence mineral processing mining sensors;mineral processing;geology;automatic detection;fuel processing industries;in ground geological boundary detection;industrial robots;robotic research;single length scale squared exponential covariance function;iron ore mine;planning and control;gamma logs;length scale;gaussian process;covariance function;autonomous remotely operated mining;rocks;learning artificial intelligence;in ground geological information;australia	This work is motivated by the need to develop new perception and modeling capabilities to support a fully autonomous, remotely operated mine. The application differs from most existing robotics research in that it requires a detailed world model of the sub-surface geological structure. This in-ground geological information is then used to drive many of the planning and control decisions made on a mine site. This paper formulates a method for automatically detecting in-ground geological boundaries using geophysical logging sensors and a supervised learning algorithm. The algorithm uses Gaussian Processes (GPs) and a single length scale squared exponential covariance function. The approach is demonstrated on data from a producing iron-ore mine in Australia. Our results show that two separate distinctive geological boundaries can be automatically identified with an accuracy of over 99 percent. The alternative approach to automatic detection involves manual examination of these data.	algorithm;autonomous robot;gaussian process;robotics;sensor;supervised learning;time complexity	Katherine L. Silversides;Arman Melkumyan;Derek A. Wyman;Peter Hatherly;Eric Nettleton	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5980489	engineering;machine learning;data mining;gaussian process;supervised learning;statistics	Robotics	49.785579272683925	-26.36379478736761	131850
a077631f4f0f9c0db01942780e5279e23a4f1f28	mechanisms inducing parallel computation in a model of physarum polycephalum transport networks	multi agent;morphological adaptation;unconventional computing;slime mould;material computation	The giant amoeboid organism true slime mould Physarum polycephalum dynamically adapts its body plan in response to changing environmental conditions and its protoplasmic transport network is used to distribute nutrients within the organism. These networks are e cient in terms of network length and network resilience and are parallel approximations of a range of proximity graphs and plane division problems. The complex parallel distributed computation exhibited by this simple organism has since served as an inspiration for intensive research into distributed computing and robotics within the last decade. P. polycephalum may be considered as a spatially represented parallel unconventional computing substrate, but how can this `computer' be programmed? In this paper we examine and catalogue individual low-level mechanisms which may be used to induce network formation and adaptation in a multi-agent model of P. polycephalum. These mechanisms include those intrinsic to the model (particle sensor angle, rotation angle, and scaling parameters) and those mediated by the environment (stimulus location, distance, angle, concentration, engulfment and consumption of nutrients, and the presence of simulated light irradiation, repellents and obstacles). The mechanisms induce a concurrent integration of chemoattractant and chemorepellent gradients di using within the 2D lattice upon which the agent population resides, stimulating growth, movement, morphological adaptation and network minimisation. Chemoattractant gradients, and their modulation by the engulfment and consumption of nutrients by the model population, represent an e cient outsourcing of spatial computation. The mechanisms may prove useful in understanding the search strategies and adaptation of distributed organisms within their environment, in understanding the minimal requirements for complex adaptive behaviours, and in developing methods of spatially programming parallel unconventional computers and robotic devices.	approximation algorithm;computation;computer;distributed computing;distributed database;feedback;formal grammar;gradient;graph rewriting;high- and low-level;hoc (programming language);image scaling;iterative method;modulation;multi-agent system;network formation;outsourcing;p (complexity);parallel computing;requirement;robot;robotics;slime;software propagation;unconventional computing;while	Jeff Jones	2015	Parallel Processing Letters	10.1142/S0129626415400046	simulation;computer science;artificial intelligence;slime mold;distributed computing;algorithm;unconventional computing	ML	46.74025126783198	-25.753942739095393	132185
4f32e3cc2e87629c821df16746787342476a46cb	the belief roadmap: efficient planning in linear pomdps by factoring the covariance		In this paper we address the problem of trajectory planning with imperfect state information. In many real-world domains, the position of a mobile agent cannot be known perfectly; instead, the agent maintains a probabilistic belief about its position. Planning in these domains requires computing the best trajectory through the space of possible beliefs. We show that planning in belief space can be done efficiently for linear Gaussian systems by using a factored form of the covariance matrix. This factored form allows several prediction and measurement steps to be combined into a single linear transfer function, leading to very efficient posterior belief prediction during planning. We give a belief-space variant of the Probabilistic Roadmap algorithm called the Belief Roadmap (BRM) and show that the BRM can compute plans substantially faster than conventional belief space planning. We also show performance results for planning a path across MIT campus without perfect localization.	algorithm;algorithmic efficiency;automated planning and scheduling;extended kalman filter;integer factorization;mobile agent;probabilistic roadmap;transfer function	Sam Prentice;Nicholas Roy	2007		10.1007/978-3-642-14743-2_25	mathematical optimization;computer science;artificial intelligence;machine learning	AI	50.078682202748446	-26.05092252385559	132219
2b82c79508f4fd57769f885e44029373344c3bf9	multi-modal tracking of faces for video communications	histograms;recursive estimation;integration and control of visual processes;robust tracking;face recognition data compression video signal processing tracking image colour analysis covariance matrices image matching kalman filters;face detection video compression robustness process control communication system control covariance matrix color histograms event detection recursive estimation;data compression;150 mhz multi modal face tracking visual processes face detection video communications video compression video transmission supervisor visual process activation visual process selection visual process control confidence factor unified estimation covariance matrix blink detection normalised color histogram matching cross correlation processing states robust tracking recursive estimator kalman filter pd controller;cross correlation;video signal processing;video communications;color;image matching;pan tilt zoom;kalman filters;video compression;visual process control;kalman filter;color histogram;event detection;visual processes;face tracking;visual process activation;face recognition;supervisor;processing states;recursive estimator;150 mhz;covariance matrices;image colour analysis;process control;video transmission;multi modal face tracking;active and real time vision;robustness;pd controller;face detection;communication system control;video communication;blink detection;visual process selection;normalised color histogram matching;visual processing;tracking;unified estimation;confidence factor;covariance matrix	This paper describes a system which uses multiple visual processes to detect and track faces for video compression and transmission. The system is based on an architecture in which a supervisor selects and activates visual processes in cyclic manner. Control of visual processes is made possible by a confidence factor which accompanies each observation. Fusion of results into a unified estimation for tracking is made possible by estimating a covariance matrix with each observation. Visual processes for face tracking are described using blink detection, normalised color histogram matching, and cross correlation (SSD and NCC). Ensembles of visual processes are organised into processing states so as to provide robust tracking. Transition between states is determined by events detected by processes. The result of face detection is fed into recursive estimator (Kalman filter). The output from the estimator drives a PD controller for a pan/tilt/zoom camera. The resulting system provides robust and precise tracking which operates continuously at approximately 20 images per second on a 150 megahertz computer work-station.	cnet;color histogram;cross-correlation;data compression;expectation propagation;face detection;histogram matching;kalman filter;neural correlates of consciousness;pan–tilt–zoom camera;process (computing);recursion;robustness (computer science);solid-state drive;state (computer science);state diagram;state transition table	James L. Crowley;François Bérard	1997		10.1109/CVPR.1997.609393	data compression;facial recognition system;kalman filter;computer vision;speech recognition;computer science;process control;pattern recognition;statistics	Vision	45.8815382948304	-33.76514485809259	132810
9fabb734bc3aa02407f90ce0ded7fdf923cd64af	real-time human action recognition on an embedded, reconfigurable video processing architecture	ucl;embedded devices;real time;intelligent environment;discovery;computer vision and image processing;video processing;theses;conference proceedings;fpga;computer vision;reconfigurable architecture;digital web resources;machine learning;ucl discovery;human motion;action recognition;open access;secure system;ucl library;support vector machine;book chapters;open access repository;motion history image;article;ucl research	In recent years, automatic human action recognition has been widely researched within the computer vision and image processing communities. Here we propose a real-time, embedded vision solution for human action recognition, implemented on an FPGA-based ubiquitous device. There are three main contributions in this paper. Firstly, we have developed a fast human action recognition system with simple motion features and a linear support vector machine classifier. The method has been tested on a large, public human action dataset and achieved competitive performance for the temporal template class of approaches, which include “Motion History Image” based techniques. Secondly, we have developed a reconfigurable, FPGA based video processing architecture. One advantage of this architecture is that the system processing performance can be reconfigured for a particular application, with the addition of new or replicated processing cores. Finally, we have successfully implemented a human action recognition system on this reconfigurable architecture. With a small number of human actions (hand gestures), this stand-alone system is operating reliably at 12 frames/s, with an 80% average recognition rate using limited training data. This type of system has applications in security systems, man–machine communications and intelligent environments.	computer vision;embedded system;field-programmable gate array;generic programming;image processing;intelligent environment;real-time clock;real-time computing;real-time transcription;support vector machine;video processing	Hongying Meng;Michael Freeman;Nick Pears;Chris Bailey	2008	Journal of Real-Time Image Processing	10.1007/s11554-008-0073-1	embedded system;support vector machine;computer vision;real-time computing;simulation;computer science;multimedia;video processing;field-programmable gate array	Vision	43.649663367566504	-36.92445760316876	133129
5aeef41cd0362f1f78b2fa9b07268cd1c082ac57	reliability of inference of directed climate networks using conditional mutual information	climate;transfer entropy;stability;nonlinearity;network;causality	Across geosciences, many investigated phenomena relate to specific complex systems consisting of intricately intertwined interacting subsystems. Such dynamical complex systems can be represented by a directed graph, where each link denotes an existence of a causal relation, or information exchange between the nodes. For geophysical systems such as global climate, these relations are commonly not theoretically known but estimated from recorded data using causality analysis methods. These include bivariate nonlinear methods based on information theory and their linear counterpart. The trade-off between the valuable sensitivity of nonlinear methods to more general interactions and the potentially higher numerical reliability of linear methods may affect inference regarding structure and variability of climate networks. We investigate the reliability of directed climate networks detected by selected methods and parameter settings, using a stationarized model of dimensionality-reduced surface air temperature data from reanalysis of 60-year global climate records. Overall, all studied bivariate causality methods provided reproducible estimates of climate causality networks, with the linear approximation showing Entropy 2013, 15 2024 higher reliability than the investigated nonlinear methods. On the example dataset, optimizing the investigated nonlinear methods with respect to reliability increased the similarity of the detected networks to their linear counterparts, supporting the particular hypothesis of the near-linearity of the surface air temperature reanalysis data.	algorithm;bivariate data;causal filter;causality;climate as complex networks;complex systems;conditional mutual information;directed graph;dynamical system;heart rate variability;image resolution;information exchange;information theory;interaction;iterative method;linear approximation;meteorological reanalysis;nonlinear system;numerical analysis;spatial variability;stochastic process;synchronization (computer science);temporal logic;time series;transfer entropy;usability	Jaroslav Hlinka;David Hartman;Martin Vejmelka;Jakob Runge;Norbert Marwan;Juergen Kurths;Milan Paluš	2013	Entropy	10.3390/e15062023	climate;econometrics;causality;stability;transfer entropy;nonlinear system;machine learning;mathematics;statistics	AI	39.31584693380812	-24.511219585273256	133485
8f56b2ceed4552a96680524c2a1b20a333c12125	parallel volume rendering using pc graphics hardware	add on boards;front end;volume rendering;high speed networks;computer graphic equipment;parallel programming;graphics hardware;graphic cards parallel volume rendering pc graphics hardware commodity off the shelf graphics hardware high speed network devices distributed volume rendering graphic accelerator boards opengl interface frame buffers single pc frame rates transfer speeds;distribution volume;add on boards parallel programming rendering computer graphics microcomputer applications computer graphic equipment;commodity off the shelf;microcomputer applications;rendering computer graphics;visual system;high performance;rendering computer graphics hardware personal communication networks computer graphics data visualization costs interactive systems high speed networks nasa delay	Thispaperdescribesanarchitecturethatenablestheuse of commodityoff the shelf graphicshardware along with high speednetworkdevicesfor distributedvolumerendering. Several PCs drive a numberof graphic accelerator boardsusinganOpenGLinterface. Theframebuffersof the cardsare readback andblendedtogetherfor final presentationona singlePCworkingasfrontend.Weexplainhow theattainableframeratesare limitedbythetransferspeeds over the networkas well as the overheadimplied by having to blendseveral imagestogetherlimits . An initial implementationusingfour graphiccardsachievesframerates similar to thoseof high performancevisualizationsystems.	graphics hardware;volume rendering	Marcelo Magallón;Matthias Hopf;Thomas Ertl	2001		10.1109/PCCGA.2001.962895	embedded system;visual system;computer hardware;computer science;front and back ends;graphics hardware;volume rendering;software rendering;computer graphics (images)	Graphics	43.42846431461211	-31.619700725436388	133539
ea118e2288e975d0a0c512fee8dc762f4cf4fce8	from plants to landmarks: time-invariant plant localization that uses deep pose regression in agricultural fields		Agricultural robots are expected to increase yields in a sustainable way and automate precision tasks, such as weeding and plant monitoring. At the same time, they move in a continuously changing, semi-structured field environment, in which features can hardly be found and reproduced at a later time. Challenges for Lidar and visual detection systems stem from the fact that plants can be very small, overlapping and have a steadily changing appearance. Therefore, a popular way to localize vehicles with high accuracy is based on expensive global navigation satellite systems and not on natural landmarks. The contribution of this work is a novel imagebased plant localization technique that uses the time-invariant stem emerging point as a reference. Our approach is based on a fully convolutional neural network that learns landmark localization from RGB and NIR image input in an end-to-end manner. The network performs pose regression to generate a plant location likelihood map. Our approach allows us to cope with visual variances of plants both for different species and different growth stages. We achieve high localization accuracies as shown in detailed evaluations of a sugar beet cultivation phase. In experiments with our BoniRob we demonstrate that detections can be robustly reproduced with centimeter accuracy.	agricultural robot;artificial neural network;convolutional neural network;end-to-end principle;experiment;ground truth;precision and recall;robotic mapping;satellite navigation;semiconductor industry;sensor;symantec endpoint protection;time-invariant system	Florian Kraemer;Alexander Schaefer;Andreas Eitel;Johan Vertens;Wolfram Burgard	2017	CoRR		engineering;convolutional neural network;lti system theory;rgb color model;artificial intelligence;lidar;computer vision;pattern recognition	Robotics	50.1982612072256	-37.17049610428321	133774
558dec170c35dd438f4de91436c6d132ccf79e4f	robust visualization of navigation experiments with mobile robots over the internet	robot sensing systems;predictive simulation technique;transmission gaps robust visualization navigation experiments mobile robots internet teleoperation instructable mobile robots data connections limited bandwidth video signals predictive simulation technique odometry sensor simulation complex action prediction;data connections;mobile robot;visual communication;odometry;mobile robots;robust visualization;complex action prediction;distance measurement;navigation;internet;robot control;simulation technique;navigation experiments;instructable mobile robots;limited bandwidth;sensor simulation;data visualization;transmission gaps;telerobotics;bandwidth;robustness;predictive models;video signals;computer science;distance measurement mobile robots computerised navigation visual communication internet telerobotics digital simulation;robustness navigation mobile robots internet object oriented modeling predictive models robot sensing systems computer science data visualization bandwidth;object oriented modeling;digital simulation;teleoperation;computerised navigation	Visualization is an important precondition for successful tele-operation of instructable mobile robots. Data connections with varying and limited bandwidth such as the Internet, however, prohibit the continuous transmission of video signals. In this paper we propose a predictive simulation technique which is designed to permit the reliable visualization of the robot’s actions over the Internet. It differs from previous approaches in that it includes an odometry and sensor simulation. This simulation of the robot allows the integration of a complete robot control system to reliably predict complex actions of the robot even if large transmission gaps of several seconds occur. We describe an application of the predictive simulation technique to navigation experiments with mobile robots. We present different experiments carried out with a real robot illustrating that the predictive simulation technique provides accurate visualizations of the robot’s actions even if transmission gaps of more than ten seconds occur.	autonomous robot;client–server model;control system;experiment;internet;java applet;mobile robot;odometry;physical symbol system;precondition;robot control;server (computing);simulation;software deployment;status message (instant messaging);television	Dirk Schulz;Wolfram Burgard;Armin B. Cremers	1999		10.1109/IROS.1999.812801	control engineering;mobile robot;computer vision;simulation;computer science;artificial intelligence;mobile robot navigation;data visualization	Robotics	49.220108194159664	-29.04429658967382	133997
47dc01aa2384af46c41dd0de63e76bb99798b110	a computer-aided modeling and measurement system for environmental thermal comfort sensing	measurement uncertainty environmental thermal comfort sensing pmv predicted mean vote air temperature relative humidity air velocity average radiation temperature metabolic rate clothing thermal resistance computer aided thermal comfort measurement system virtual instrument technology labview platform server computer transmission data center web page internet measurement error matlab analysis monte carlo method;instruments;monte carlo method mcm;uncertainty;temperature sensors;predicted mean vote pmv;qa75 electronic computers computer science;indexes;uncertainty computer aided measurement system monte carlo method mcm predicted mean vote pmv thermal comfort;期刊论文;virtual instrumentation computer centres environmental testing internet measurement errors measurement uncertainty monte carlo methods temperature sensors thermal variables measurement;computer aided measurement system;temperature measurement;temperature;temperature measurement temperature sensors measurement errors indexes instruments temperature;thermal comfort;measurement errors	Predicted mean vote (PMV) is a well-known thermal comfort index with four environmental variables (air temperature, relative humidity, air velocity, and average radiation temperature) and two human factors (metabolic rate and clothing thermal resistance). This paper presents a novel computer-aided thermal comfort measurement system with PMV, which combined the advanced sensors with the virtual instrument technology. The system software is developed using the LabVIEW platform. The measured data can be transmitted to the server-computer in a data center and displayed on a web page through the Internet. The impact of the measurement error of each environmental variable on PMV is analyzed via MATLAB. The system tests were conducted under the certain environmental conditions and Monte Carlo method is deployed to analyze the PMV measurement uncertainty. The experimental results show the feasibility and effectiveness of the proposed system, and confirm that the measurement uncertainty of PMV is not a constant, and varies with the environment changes.	computer program;correctness (computer science);data acquisition;data center;environment variable;human factors and ergonomics;internet;labview;matlab;modular programming;monte carlo method;multi-chip module;scalability;sensor;server (computing);server-side;software design;system of measurement;thermal resistance;velocity (software development);virtual instrumentation;web page	Qingyuan Zhu;Jian Yi;Shiyue Sheng;Chenglu Wen;Huosheng Hu	2015	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2014.2345922	database index;thermal comfort;simulation;uncertainty;temperature;temperature measurement;engineering;electrical engineering;physics;quantum mechanics;statistics;observational error;mechanical engineering	Mobile	45.33508656486513	-25.15252059754692	134169
ba7b71cb97ffea027e34028b562c7c58ee8bdbd2	route 20, autobahn 7, and slime mold: approximating the longest roads in usa and germany with slime mold on 3-d terrains	biocomputing;transportation biocomputing cartography cellular automata geographic information systems;bioinspired amorphous robotic devices route 20 autobahn 7 usa germany 3d terrains cellular slime mould physarum polycephalum nylon terrains man made transport routes cellular automaton models protoplasmic routes sensorial fusion information processing decision making;electron tubes substrates automata laboratories computers;geographic information systems;transportation;cartography;cellular automata;usa autobahn 7 german route 20 transport routes unconventional computing	A cellular slime mould Physarum polycephalum is a monstrously large single cell visible by an unaided eye. The slime mold explores space in parallel, is guided by gradients of chemoattractants, and propagates toward sources of nutrients along nearly shortest paths. The slime mold is a living prototype of amorphous biological computers and robotic devices capable of solving a range of tasks of graph optimization and computational geometry. When presented with a distribution of nutrients, the slime mold spans the sources of nutrients with a network of protoplasmic tubes. This protoplasmic network matches a network of major transport routes of a country when configuration of major urban areas is represented by nutrients. A transport route connecting two cities should ideally be a shortest path, and this is usually the case in computer simulations and laboratory experiments with flat substrates. What searching strategies does the slime mold adopt when exploring 3-D terrains? How are optimal and transport routes approximated by protoplasmic tubes? Do the routes built by the slime mold on 3-D terrain match real-world transport routes? To answer these questions, we conducted pioneer laboratory experiments with Nylon terrains of USA and Germany. We used the slime mold to approximate route 20, the longest road in USA, and autobahn 7, the longest national motorway in Europe. We found that slime mold builds longer transport routes on 3-D terrains, compared to flat substrates yet sufficiently approximates man-made transport routes studied. We demonstrate that nutrients placed in destination sites affect performance of slime mold, and show how the mold navigates around elevations. In cellular automaton models of the slime mold, we have shown variability of the protoplasmic routes might depends on physiological states of the slime mold. Results presented will contribute toward development of novel algorithms for sensorial fusion, information processing, and decision making, and will provide inspirations in design of bioinspired amorphous robotic devices.	approximation algorithm;cellular automaton;chemotactic factors;chondromyces;computational geometry;computer simulation;computers;decision making;experiment;filamentous fungus;gradient;graph - visual representation;heart rate variability;information processing;inspiration function;mathematical optimization;nutrients;nylons;physarum polycephalum;polyhedral terrain;prototype;slime;short;shortest path problem;specimen source codes - tube;williams tube	Andrew Adamatzky	2014	IEEE Transactions on Cybernetics	10.1109/TCYB.2013.2248359	cellular automaton;transport;simulation;computer science	Visualization	46.674310242683134	-25.71404301854722	134299
b5fdef80c2c97a5d1c5cfdbeec758cc9422eb7a6	the research of fire detector based on information fusion technology	false alarm rate leakage alarm single chip microcomputer multisensor information fusion technology intelligent fire detector technology fire probability temperature sensor smoke sensor air sensor intelligent arithmetic fuzzy neural network intelligent fire alarm system arithmetic simulation smoldering fire;fuzzy neural network;fuzzy neural nets;false alarm rate;chip;fuzzy inference information fusion neural network;fuzzy inference;information fusion;computerised instrumentation;sensor fusion;high sensitivity;intelligent sensors;microprocessor chips;fires detectors algorithm design and analysis temperature sensors fuzzy sets random access memory training;sensor fusion computerised instrumentation fuzzy neural nets intelligent sensors microprocessor chips;neural network	Traditional fire detection technology is based on a single parameter, fault alarm and leakage alarm is very common. This paper studies the application of single chip computer and multi-sensor information fusion technology for design of an intelligent fire detector. This fire detector adopts single chip microcomputer to be control core, fusing three sensor data including temperature, smoke and CO air which have obvious character in fire, fire probability obtained by intelligent arithmetic of fuzzy neural network. The availability and reliability of intelligent fire alarm system arithmetic simulation results shows that the system can distinguish between smoldering fire and open flame with high precision, and system has high sensitivity, reducing errors and false alarm rate, reaching the desired purpose.	artificial neural network;design rationale;microcomputer;microcontroller;neuro-fuzzy;sensor;simulation;spectral leakage	Haiqun Wang;Yugui Zhang;Ling Meng;Zhikun Chen	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023859	chip;embedded system;electronic engineering;computer science;engineering;manual fire alarm activation;machine learning;constant false alarm rate;sensor fusion;computer security;artificial neural network;intelligent sensor	Robotics	40.909880058291854	-24.694157144381983	134333
fbdc927f01d12b358167bff6e471df03a1363f8d	an attentional model for autonomous mobile robots	top down attention attention bottom up attention multisensor range scanner robotics saliency models sonar;robot sensing systems computational modeling biological system modeling visualization feature extraction	The increase of applications that use autonomous robots has endowed them with a high number of sensors and actuators that are sometimes redundant. This new highly complex systems and the type of environment where they are expected to operate require them to deal with data overload and data fusion. In humans that face the same problem when sounds, images, and smells are presented to their sensors in a daily scene, a natural filter is applied: attention. Although there are many computational models that apply attentive systems to robotics, they usually are restricted to two classes of systems: 1) those that have complex biologically based attentional visual systems and 2) those that have simpler attentional mechanisms with a larger variety of sensors. This work proposes an attentional model inspired from biological systems and that supports a variety of robotics sensors. Furthermore, it discusses the possibility of using multiple sensors to define multiple features, with feature extraction modules that can handle exogenous and endogenous attentional processes. The experiments were performed in a simulated high-fidelity environment, and results have shown that the model proposed can account for detecting salient (bottom-up) and desired (top-down) stimuli according to the modeled features.	as-interface;algorithm;autonomous robot;biological system;bottom-up proteomics;complex systems;computation;computational model;doppler effect;experiment;feature extraction;interoperable object reference;mobile robot;modulation;sensor;state space;top-down and bottom-up design	Esther Luna Colombini;Alexandre da Silva Simões;Carlos H. C. Ribeiro	2017	IEEE Systems Journal	10.1109/JSYST.2015.2499304	complex system;robot;computer science;feature extraction;mobile robot;visualization;computational model;sensor fusion;artificial intelligence;robotics	Robotics	48.39679683858692	-33.21435304739264	135068
774899b9077b1f90d73949b86f1f927cc8ca1e39	image and video analysis to enable human-friendly systems				Rainer Stiefelhagen	2010			video capture;computer vision;video compression picture types;video tracking;image processing;video processing;artificial intelligence;video post-processing;computer science	EDA	45.0997645302113	-37.0459551541637	135122
7f61e492a1bd9882a620ce7252d05f47754f3fff	real-time robot vision for collision avoidance inspired by neuronal circuits of insects	field programmable gate array circuits;field programmable gate array;motorized car real time robot vision collision avoidance insects neuronal circuits real time vision sensor visual nervous system visual information locust nervous system mixed analog digital integrated circuits analog resistive network field programmable gate array circuits simulated movie images;robot vision collision avoidance field programmable gate arrays mixed analogue digital integrated circuits mobile robots;real time robot vision;nervous system;visual nervous system;real time;mobile robots;robot vision systems collision avoidance circuits insects nervous system field programmable gate arrays robot sensing systems sensor systems robustness hardware;robot vision;motorized car;real time vision;visual information;analog resistive network;simulated movie images;mixed analogue digital integrated circuits;collision avoidance;field programmable gate arrays;locust nervous system;real time vision sensor;mixed analog digital integrated circuits;insects neuronal circuits	A real-time vision sensor for collision avoidance was designed. To respond selectively to approaching objects on direct collision course, the sensor employs an algorithm inspired by the visual nervous system in a locust, which can avoid a collision robustly by using visual information. We implemented the architecture of the locust nervous system with a compact hardware system which contains mixed analog- digital integrated circuits consisting of an analog resistive network and field-programmable gate array (FPGA) circuits. The response properties of the system were examined by using simulated movie images, and the system was tested also in real- world situations by loading it on a motorized car. The system was confirmed to respond selectively to colliding objects even in complicated real-world situations.	algorithm;field-programmability;field-programmable gate array;integrated circuit;real-time clock;real-time transcription;resistive touchscreen	Hirotsugu Okuno;Tetsuya Yagi	2007	2007 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2007.4399089	embedded system;computer vision;simulation;computer science;engineering;field-programmable gate array	Robotics	46.05856434733312	-33.48827921263979	135262
0ad2637d9cd0f1f6307b0d0cb646beee2585b8ff	a perception-driven autonomous urban vehicle	kinodynamic motion planning algorithm;incomplete map data;vehicle motion;autonomous passenger vehicle;global positioning system;dynamic environment;vehicle implementation;vehicle architecture;high-rate sensor data;autonomous driving;perception-driven autonomous urban vehicle;rapidly exploring random tree;high resolution;message passing;motion planning	This paper describes the architecture and implementation of an autonomous passenger vehicle designed to navigate using locally perceived information in preference to potentially inaccurate or incomplete map data. The vehicle architecture was designed to handle the original DARPA Urban Challenge requirements of perceiving and navigating a road network with segments defined by sparse waypoints. The vehicle implementation includes many heterogeneous sensors with significant communications and computation bandwidth to capture and process high-resolution, high-rate sensor data. The output of the comprehensive environmental sensing subsystem is fed into a kino-dynamic motion planning algorithm to generate all vehicle Figure 1: Talos in action at the National Qualifying Event. motion. The requirements of driving in lanes, three-point turns, parking, and maneuvering through obstacle fields are all generated with a unified planner. A key aspect of the planner is its use of closed-loop simulation in a Rapidly-exploring Randomized Trees (RRT) algorithm, which can randomly explore the space while efficiently generating smooth trajectories in a dynamic and uncertain environment. The overall system was realized through the creation of a powerful new suite of software tools for message-passing, logging, and visualization. These innovations provide a strong platform for future research in autonomous driving in GPS-denied and highly dynamic environments with poor a priori information.	automated planning and scheduling;autonomous car;autonomous robot;computation;darpa grand challenge (2007);global positioning system;image resolution;message passing;motion planning;randomized algorithm;randomness;requirement;sensor;server log;simulation;sparse matrix	John J. Leonard;Jonathan P. How;Seth J. Teller;Mitch Berger;Stefan Campbell;Gaston A. Fiore;Luke Fletcher;Emilio Frazzoli;Albert S. Huang;Sertac Karaman;Olivier Koch;Yoshiaki Kuwata;David Moore;Edwin Olson;Steve Peters;Justin Teo;Robert Truax;Matthew R. Walter;David Barrett;Alexander Epstein	2008		10.1007/978-3-642-03991-1_5	rapidly exploring random tree;computer vision;message passing;simulation;image resolution;computer science;engineering;artificial intelligence;motion planning;transport engineering	Robotics	53.42367139980844	-30.672831213871437	135310
4d9484694cf80e9238b1ee17917949558691f6d4	autonomous planned color learning on a legged robot	learning process;robot vision;color learning;legged robot	Our research focuses on automating the color-learning process on-board a legged robot with limited computational and memory resources. A key defining feature of our approach is that instead of using explicitly labeled training data it trains autonomously and incrementally, thereby making it robust to re-colorings in the environment. Prior results demonstrated the ability of the robot to learn a color map when given an executable motion sequence designed to present it with good color-learning opportunities based on the known structure of its environment. This paper extends these results by demonstrating that the robot can plan its own such motion sequence and perform just as well at colorlearning. The knowledge acquired at each stage of the learning process is used as a bootstrap mechanism to aid the robot in planning its motion during subsequent stages.	algorithm;apply;autonomous robot;booting;color mapping;computation;executable;heuristic;mobile robot;on-board data handling;pixel;uncontrolled format string	Mohan Sridharan;Peter Stone	2006		10.1007/978-3-540-74024-7_23	mobile robot;robot learning;computer vision;simulation;artificial intelligence;social robot;robot control;mobile robot navigation;personal robot	Robotics	50.52780911207157	-28.6749684848303	135395
d5de5c703ecfb7bbafc83703738fcda6aaf89302	fpga-based video processing for a vision prosthesis	video streaming;stimulation array;video signal processing;user interface;video streaming biomedical electronics field programmable gate arrays handicapped aids medical image processing prosthetics video signal processing;video processing;prosthetics;vision prosthesis;fpga based video processing;arrays;electrodes;handicapped aids;streaming media;medical image processing;pixel;prototype vision processing system;biomedical electronics;electrodes streaming media prosthetics pixel field programmable gate arrays arrays cameras;visual impairment;user interface fpga based video processing vision prosthesis prototype vision processing system stimulation array low power consumption video stream;field programmable gate arrays;video stream;low power consumption;cameras	Restoring sight to the visually impaired poses significant challenges across multiple disciplines. In this demonstration, we present a prototype vision processing system to perform the external processing and to provide a technical user interface for a vision prosthesis. The system transforms an input video stream into intensity values suitable for transmission to a stimulation array implanted within the retina of an impaired eye. This FPGA prototype ensures the processing is fast, accurate, and has low power consumption. We demonstrate our progress to date and hope thereby to stimulate interest in this emerging and challenging application area.	field-programmable gate array;prototype;streaming media;user interface;video processing	Benjamin Kwek;Freddie Sunarso;Melissa Teoh;Arrian van Zal;Philip Preston;Oliver Diessel	2010	2010 International Conference on Field-Programmable Technology	10.1109/FPT.2010.5681430	embedded system;computer vision;computer hardware;computer science;electrode;video processing;user interface;pixel;field-programmable gate array	EDA	44.172355322358776	-34.30802127850079	135678
ff9d123a064a90ab1ad469ed01781706333ef7a7	real time adaptive color segmentation for mars landing site identification	limited weight quantization;real time;real time identification;cascade error projection;color segmentation;adaptive learning;safe landing;landing site identification;adaptive architecture		image segmentation	Tuan A. Duong;Tuan A. Duong	2003	JACIII	10.20965/jaciii.2003.p0289	computer vision;simulation;computer science;adaptive learning	Vision	47.63134869389638	-36.93909896429254	136288
a35fa0ad88494045ab38db830b6c4aca8846b3c3	an adaptive online co-search method with distributed samples for dynamic target tracking	search problems;target tracking;estimation;heuristic algorithms;bayes methods;adaptation models	Dynamic optimal problems (DOPs) are often encountered in target search, emergency rescue, and object tracking. Motivated by the need to perform a search and rescue task, we clarify a DOP in a complex environment if a target unpredictably travels in an environment with general non-Gaussian distributed and time-varying noises. To solve this issue, we propose a recursive Bayesian estimation with a distributed sampling (RBEDS) model. Furthermore, two kinds of communication cooperative extensions, i.e., real-time communication and communication after finding the target, are analyzed. To balance between exploitation and exploration, an adaptive online co-search (AOCS) method, which consists of an online updating algorithm and a self-adaptive controller, is designed based on RBEDS. Simulation results demonstrates that searchers with AOCS can achieve a comparable search performance with a global sampling method, e.g., Markov Chain Monto Carlo estimation, by applying real-time communication. The local samples help searchers keep flexible and adaptive to the changes of the target. The proposed method with both communication and cooperation exhibits excellent performance when tracking a target. Another attractive result is that only a few searchers and local samples are demanded. The insensibility to the scale of samples makes the proposed method obtain a better solution with less computation cost than the existing methods.	benchmark (computing);computation;markov chain;mathematical optimization;missile guidance;online algorithm;real-time clock;recursion;sampling (signal processing);simulation	Feng Li;Mengchu Zhou;Yongsheng Ding	2018	IEEE Transactions on Control Systems Technology	10.1109/TCST.2017.2669154	tracking system	Robotics	49.76775923352322	-24.475130791715756	136315
ad417e5c2cf4ec809cf551b5f3e0262fc7beac28	design of chemical sensor arrays for monitoring disposal sites on the ocean floor	probability;turbulent diffusion;signal detection;water pollution measurement;satisfiability;chemical sensor;sensor arrays chemical sensors oceans water resources computerized monitoring sea measurements pollution measurement testing detectors sediments;maximum likelihood detection;probability of false alarm;diffusion process;turbulent diffusion chemical sensors signal detection water pollution measurement maximum likelihood detection probability;environmental monitoring;chemical sensors;detection probability chemical sensor arrays disposal sites monitoring ocean floor detection methods automatic environmental monitoring dredge materials harbors shipping channels pollutants underwater transport diffusion process measurement model temporal evolution spatial evolution concentration distribution hypothesis test multiple sources generalized likelihood ratio test mean detector false alarm probability;generalized likelihood ratio;hypothesis test	We develop methods for automatic environmental monitoring of disposal sites on the deep ocean floor using chemical sensor arrays and statistical hypothesis testing. Such sites have been proposed to relocate dredge materials from harbors and shipping channels. The transport of pollutants is modeled as a diffusion process, and the measurement and statistical models are derived by exploiting the spatial and temporal evolution of the associated concentration distribution. We derive two detectors, the generalized likelihood ratio (GLR) test and the mean detector, and determine their performance in terms of the probabilities of false alarm and detection. The results are applied to the design of chemical sensor arrays satisfying criteria specified in terms of these probabilities, and to optimally select a number of sensors and time samples. Numerical examples are used to demonstrate the applicability of our results.	glr parser;numerical method;sensor;statistical model	Aleksandar Jeremic;Arye Nehorai	1998		10.1109/ICASSP.1998.681537	statistical hypothesis testing;turbulent diffusion;diffusion process;probability;mathematics;environmental monitoring;statistics;detection theory;satisfiability	Vision	45.96660820491403	-27.146315404339077	137029
8a4ae1f113e6ce160806dbedc9e0410fd70b6f7f	localization and road boundary recognition in urban environments using digital street maps	image recognition;object recognition;urban environment;robot position estimation;probability;roadways;google map;autonomous robot navigation;prior information;robot navigation;mobile robots;prior knowledge;road boundary recognition;digital street maps;data association;pedestrian walkways localization recognition road boundary recognition digital street maps autonomous robot navigation google maps data association robot observation boundary lines region annotations roads buildings probabilistic framework robot position estimation complex urban environments roadways;road traffic control;navigation;robot observation;shape;navigation robots image recognition buildings lead roads shape;probabilistic framework;roads;lead;geographic information systems;region annotations;robots;sensor fusion cartography geographic information systems mobile robots navigation object recognition probability road traffic control;cartography;autonomous navigation;localization recognition;sensor fusion;pedestrian walkways;boundary lines;buildings;complex urban environments;google maps	In this study, we aim to achieve autonomous navigation for robots in environments that they have not previously visited. Many of the existing methods for autonomous navigation require a map to be built beforehand, typically by manually navigating the robot. Navigation without maps, i.e., without any prior information about the environment, is very difficult. We propose to use existing digital street maps for autonomous navigation. Nowadays digital street maps (e.g., those provided by Google Maps) are widely available and used routinely. Reuse of existing maps for robots eliminates extra cost of building maps. One of the difficulties in using existing street maps is data association between a robot's observation and the map, because the physical entities that correspond to the boundary lines in the map are unknown. We address this issue by using region annotations such as roads and buildings and prior knowledge. We introduce a probabilistic framework that simultaneously estimates a robot's position and the road's boundaries.We evaluated our method in complex urban environments. Our method successfully localized in environments that includes both roadways and pedestrian walkways.	autonomous robot;correspondence problem;entity;map	Kiyoshi Irie;Masahiro Tomono	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6225017	robot;mobile robot;computer vision;navigation;lead;simulation;shape;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;probability;sensor fusion;geographic information system;mobile robot navigation	Robotics	51.507509739572264	-36.920341804249084	137335
b3dc7fc397a4187183347bd6cbe488438e47080f	a board system for high-speed image analysis and neural networks	analisis imagen;carte electronique;field programmable gate array;cellular neural nets neural chips convolution feature extraction field programmable gate arrays optical character recognition image processing equipment printed circuit design mixed analogue digital integrated circuits;neural network application;cellular neural network emulation board system high speed image analysis anna neural network chips 6u vme board high speed platform filtering feature extraction tasks convolutions field programmable gate arrays fpga memory bus interfaces text location character recognition noise removal;image processing equipment;convolution;etude experimentale;optical character recognition;cellular neural network;cellular neural nets;printed circuit design;image analysis neural networks field programmable gate arrays character recognition cellular neural networks computer architecture filtering feature extraction computer interfaces system testing;chip;neural chips;high speed image analysis;reconnaissance caractere;feature extraction;tarjeta electronica;mixed analogue digital integrated circuits;image analysis;printed circuit board;field programmable gate arrays;reseau neuronal;analyse image;estudio experimental;character recognition;high speed;red neuronal;noise removal;reconocimiento caracter;neural network	Two ANNA neural-network chips are integrated on a 6U VME board, to serve as a high-speed platform for a wide variety of algorithms used in neural-network applications as well as in image analysis. The system can implement neural networks of variable sizes and architectures, but can also be used for filtering and feature extraction tasks that are based on convolutions. The board contains a controller implemented with field programmable gate arrays (FPGA's), memory, and bus interfaces, all designed to support the high compute power of the ANNA chips. This new system is designed for maximum speed and is roughly 10 times faster than a previous board. The system has been tested for such tasks as text location, character recognition, and noise removal as well as for emulating cellular neural networks (CNN's). A sustained speed of up to two billion connections per second (GC/s) and a recognition speed of 1000 characters per second has been measured.	algorithm;anna;architecture as topic;artificial neural network;controllers;convolution;dual;emulator;feature extraction;field-programmable gate array;hl7publishingsubsection <operations>;image analysis;mathematical morphology;naruto shippuden: clash of ninja revolution 3;neural network simulation;one thousand;optical character recognition;personality character;printer (computing);vmebus	Eduard Säckinger;Hans Peter Graf	1996	IEEE transactions on neural networks	10.1109/72.478407	embedded system;image analysis;computer science;machine learning;artificial neural network;field-programmable gate array	Visualization	40.39533625927003	-31.53975349234176	137741
2b8303ea0290551c810b9d6a3c705c629851d3e0	reinforcement learning to adjust robot movements to new situations	abt scholkopf;mpi fur intelligente systeme	Many complex robot motor skills can be represented using elementary movements, and there exist efficient techniques for learning parametrized motor plans using demonstrations and self-improvement. However, in many cases, the robot currently needs to learn a new elementary movement even if a parametrized motor plan exists that covers a similar, related situation. Clearly, a method is needed that modulates the elementary movement through the meta-parameters of its representation. In this paper, we show how to learn such mappings from circumstances to meta-parameters using reinforcement learning. We introduce an appropriate reinforcement learning algorithm based on a kernelized version of the reward-weighted regression. We compare this algorithm to several previous methods on a toy example and show that it performs well in comparison to standard algorithms. Subsequently, we show two robot applications of the presented setup; i.e., the generalization of throwing movements in darts, and of hitting movements in table tennis. We show that both tasks can be learned successfully using simulated and real robots.	algorithm;dart (programming language);existential quantification;gradient;kernel method;modulation;motion planning;reinforcement learning;robot;robotics;simulation;supervised learning	Jens Kober;Erhan Öztop;Jan Peters	2011		10.5591/978-1-57735-516-8/IJCAI11-441	simulation;computer science;artificial intelligence	Robotics	49.86013289284192	-27.21624716281966	137800
423e05c235dadc8424480ecc68e5c9079916d38f	fpga-based circular hough transform with graph clustering for vision-based multi-robot tracking	multi robot systems reconfigurable architectures circular hough transform object detection and recognition image processing;multiplier less distance calculation unit resource efficient fpga based architecture circular hough transform graph clustering vision based multirobot tracking shape based object detection computer vision circle detection field programmable gate array hardware accelerator xilinx virtex 4 fpga;image processing;reconfigurable architectures;robot vision field programmable gate arrays graph theory hough transforms multi robot systems object detection object tracking reconfigurable architectures;multi robot systems;object detection and recognition;circular hough transform	Shape-based object detection and recognition are frequently used methods in the field of computer vision. A well-known algorithm for circle detection is the Circular Hough Transform (CHT). This Hough Transform algorithm needs a huge memory space and large computational resources. Field Programmable Gate Array (FPGA)-based hardware accelerators can be used to efficiently handle such compute-intensive applications. In this paper, we present a resource-efficient FPGA-based architecture for the CHT algorithm. Additionally, we introduce a unique approach by combining the CHT algorithm with graph clustering. The combination of these algorithms and their implementation on a Xilinx Virtex-4 FPGA is used to support real-time vision-based multi-robot tracking. Furthermore, an efficient architecture is proposed to significantly reduce the required memory in the CHT module. For the Graph Clustering module, a multiplier-less distance calculation unit is implemented, significantly reducing the required FPGA resources. The proposed CHT design can handle multi-robot localization with an accuracy of 97 %, supporting a maximum video resolution of 1024x1024 with 128 frames per second, resulting in 134 MPixel/s. Our design provides significantly higher throughput compared to other implementations on embedded processors, FPGAs, and general purpose CPUs. Compared to an OpenCV implementation on a 3.2 GHz desktop CPU, our implementation achieves a speed- up of more than 5.7.	algorithm;central processing unit;cluster analysis;computational resource;computer vision;desktop computer;display resolution;embedded system;field-programmable gate array;hardware acceleration;hough transform;object detection;opencv;real-time clock;robot;robotic mapping;throughput	Arif Irwansyah;Omar W. Ibraheem;Jens Hagemeyer;Mario Porrmann;Ulrich Rückert	2015	2015 International Conference on ReConFigurable Computing and FPGAs (ReConFig)	10.1109/ReConFig.2015.7393313	embedded system;computer vision;image processing;computer science;theoretical computer science	EDA	44.021684064286845	-36.341772358304056	137817
51984ef730f7b0cf8e8db3d61463b728be258cf3	vision-based multi-agent cooperative target search	probability;surveillance autonomous aerial vehicles cameras multi agent systems multi robot systems object detection probability robot vision search problems sensor fusion;surveillance;map updating model vision based multiagent cooperative target search mobile ground targets unmanned aerial vehicles uav sensing capabilities communication capabilities airborne camera target detection probability model physical imaging process surveillance region distributed probability map updating model measurement information fusion information sharing information transmission environmental changes target search problem cooperative coverage control problem collective coverage area detection performance;drntu engineering electrical and electronic engineering;conference paper;multi agent systems;robot vision;surveillance cameras robot sensing systems convergence optimization roads;multi robot systems;search problems;sensor fusion;autonomous aerial vehicles;cameras;object detection	This paper addresses vision-based cooperative search for multiple mobile ground targets by a group of unmanned aerial vehicles (UAVs) with limited sensing and communication capabilities. The airborne camera on each UAV has a limited field of view and its target discriminability varies as a function of altitude. First, a general target detection probability model is built based on the physical imaging process of a camera. By dividing the whole surveillance region into cells, a probability map can be formed for each UAV indicating the probability of target existence within each cell. Then, we propose a distributed probability map updating model which includes the fusion of measurement information, information sharing among neighboring agents, information decaying and transmission due to environmental changes such as the target movement. Furthermore, we formulate the target search problem by multiple agents as a cooperative coverage control problem by optimizing the collective coverage area and the detection performance. The proposed map updating model and the cooperative control scheme are distributed, i.e., assuming that each agent only communicates with its neighbors within its communication range. Finally, the effectiveness of the proposed algorithms is illustrated by simulation.	aerial photography;airborne ranger;algorithm;consensus dynamics;converge;heuristic;map;mathematical optimization;multi-agent system;optimization problem;search problem;simulation;stationary process;unmanned aerial vehicle	Jinwen Hu;Lihua Xie;Jun Xu	2012	2012 12th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2012.6485276	computer vision;simulation;computer science;artificial intelligence;multi-agent system;probability;sensor fusion;statistics	Robotics	51.98828355261865	-33.26424878095159	137826
acaae7c128e0d880f071557b589175bcca1ac4c2	mobile robot localization via classification of multisensor maps	robot sensing systems;image recognition;sensor phenomena and characterization;coarse position estimation;instruments;ultrasound;path planning;ultrasonic imaging;mobile robots;indoor domains;feature level sensor fusion;navigation;grid based maps;recognition rate;multisensor maps;position estimation;workspace regions;pattern classification;pattern recognition;mobile robot localization;sensor fusion;infrared;indoor domains mobile robot localization multisensor maps pattern classification grid based maps workspace regions feature level sensor fusion spatial descriptions recognition rate coarse position estimation;infrared sensors;spatial descriptions;image recognition sensor fusion mobile robots path planning pattern recognition;mobile robots navigation robot kinematics pattern classification robot sensing systems sensor phenomena and characterization instruments ultrasonic imaging infrared sensors sensor fusion;robot kinematics	The authors solve the task of mobile robot localization through pattern classification of grid-based maps of important or interesting workspace regions. Each region is represented by registered ultrasound, vision, and infrared sensor grid maps; and feature-level sensor fusion is accomplished by extracting spatial descriptions from these maps. The coarse position of the robot is determined by classifying the map descriptions to recognize the workspace region that a given map represents. Using datasets collected from ten different rooms and ten different doorways in a building, the authors estimate a 94% recognition rate of the rooms and a 98% recognition rate of the doorways. The authors conclude that coarse position estimation in indoor domains is possible through classification of grid-based maps. >	mobile robot;robotic mapping	Jonathan D. Courtney;Anil K. Jain	1994		10.1109/ROBOT.1994.351351	mobile robot;computer vision;navigation;simulation;infrared;computer science;artificial intelligence;ultrasound;motion planning;sensor fusion;robot kinematics;remote sensing	Robotics	52.836454710667454	-34.2619713178596	137901
6dc6c21af45920da4586459184c9bb1fd0c927b9	fast and efficient fpga-based feature detection employing the surf algorithm	detectors;frames per second;object recognition;feature detection;paper;image processing;dual core intel cpu;clocks;computer vision detectors field programmable gate arrays robustness object detection real time systems clocks humans object recognition machine vision;surf algorithm;real time;machine vision applications;fpga;computer vision;speeded up robust features;dual core intel cpu fpga based feature detection surf algorithm machine vision applications speeded up robust features;machine vision;feature extraction;pixel;fpgas feature detectors opensurf;opensurf;fpgas;mathematical model;robustness;floating point;humans;fpga based feature detection;feature detectors;field programmable gate arrays;object detection;real time systems;innovation system;field programmable gate arrays computer vision feature extraction	Feature detectors are schemes that locate and describe points or regions of `interest' in an image. Today there are numerous machine vision applications needing efficient feature detectors that can work on Real-time; moreover, since this detection is one of the most time consuming tasks in several vision devices, the speed of the feature detection schemes severally affects the effectiveness of the complete systems. As a result, feature detectors are increasingly being implemented in state-of-the-art FPGAs. This paper describes an FPGA-based implementation of the SURF (Speeded-Up Robust Features) detector introduced by Bay, Ess, Tuytelaars and Van Gool; this algorithm is considered to be the most efficient feature detector algorithm available. Moreover, this is, to the best of our knowledge, the first implementation of this scheme in an FPGA. Our innovative system can support processing of standard video (640 x 480 pixels) at up to 56 frames per second while it outperforms a state-of-the-art dual-core Intel CPU by at least 8 times. Moreover, the proposed system, which is clocked at 200 MHz and consumes less than 20W, supports constantly a frame rate only 20% lower than the peak rate of a high-end GPU executing the same basic algorithm; the specified GPU consists of 128 floating point CPUs, clocked at 1.35 GHz and consumes more than 200W.	algorithm;central processing unit;clock rate;feature detection (computer vision);feature detection (web development);field-programmable gate array;graphics processing unit;machine vision;multi-core processor;pixel;real-time transcription;sensor;speeded up robust features	Dimitris Bouris;Antonis Nikitakis;Ioannis Papaefstathiou	2010	2010 18th IEEE Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2010.11	embedded system;computer vision;parallel computing;real-time computing;machine vision;image processing;computer science;operating system;feature detection;field-programmable gate array	Arch	44.1325207885201	-35.70156525620167	137915
767445eceef8e090792fc775ddd340b0269eab0c	event-driven sensing and processing for high-speed robotic vision	address event representation aer;event driven vision;articulo;security system event driven sensing event driven processing high speed robotic vision vision paradigm application visual information representation frame sequence frame free event driven vision biological system event driven sensor chip dynamic vision sensor dvs event driven convolution module array high end fpga gabor filter 3d stereo reconstruction system real system application efficient visual perception high speed visual perception vehicle automatic driving robotic application nonstructured environment intelligent surveillance;gabor filters;event driven convolutions;event driven processing;bio inspired vision address event representation aer event driven vision event driven processing event driven convolutions gabor filters high speed vision;voltage control convolution gabor filters retina robot sensing systems field programmable gate arrays;bio inspired vision;visual perception automatic guided vehicles biomedical optical imaging biomimetics data structures field programmable gate arrays gabor filters image reconstruction image sensors medical image processing medical robotics robot vision security sensor arrays surveillance;high speed vision	We present here an overview of a new vision paradigm where sensors and processors use visual information not represented by sequences of frames. Event-driven vision is inherently frame-free, as happens in biological systems. We use an event-driven sensor chip (called Dynamic Vision Sensor or DVS) together with event-driven convolution module arrays implemented on high-end FPGAs. Experimental results demonstrate the application of this paradigm to implement Gabor filters and 3D stereo reconstruction systems. This architecture can be applied to real systems which need efficient and high-speed visual perception, like vehicle automatic driving, robotic applications in non-structured environments, or intelligent surveillance in security systems.	3d reconstruction;algorithm;binocular vision;biological system;british informatics olympiad;central processing unit;computation;convolution;convolutional neural network;correspondence problem;dynamic voltage scaling;event-driven programming;experiment;field-programmable gate array;gabor filter;layer (electronics);programming paradigm;robot;sensor;simulation;stereoscopy	Luis A. Camuñas-Mesa;Teresa Serrano-Gotarredona;Bernab&#x00E9; Linares-Barranco	2014	2014 IEEE Biomedical Circuits and Systems Conference (BioCAS) Proceedings	10.1109/BioCAS.2014.6981776	embedded system;computer vision;simulation;computer science	Robotics	45.46459908473038	-34.61622180535772	137930
fcda0b11b79c6989150ec581769b8031dc491f59	obstacle avoidance and target acquisition for robot navigation using a mixed signal analog/digital neuromorphic processing system	dynamic neural fields;dynamic vision sensor;neuromorphic controller;neurorobotics;obstacle avoidance;target acquisition	Neuromorphic hardware emulates dynamics of biological neural networks in electronic circuits offering an alternative to the von Neumann computing architecture that is low-power, inherently parallel, and event-driven. This hardware allows to implement neural-network based robotic controllers in an energy-efficient way with low latency, but requires solving the problem of device variability, characteristic for analog electronic circuits. In this work, we interfaced a mixed-signal analog-digital neuromorphic processor ROLLS to a neuromorphic dynamic vision sensor (DVS) mounted on a robotic vehicle and developed an autonomous neuromorphic agent that is able to perform neurally inspired obstacle-avoidance and target acquisition. We developed a neural network architecture that can cope with device variability and verified its robustness in different environmental situations, e.g., moving obstacles, moving target, clutter, and poor light conditions. We demonstrate how this network, combined with the properties of the DVS, allows the robot to avoid obstacles using a simple biologically-inspired dynamics. We also show how a Dynamic Neural Field for target acquisition can be implemented in spiking neuromorphic hardware. This work demonstrates an implementation of working obstacle avoidance and target acquisition using mixed signal analog/digital neuromorphic hardware.	analog;autonomous robot;biological neural networks;clutter;computation (action);computer architecture;controllers;dynamic splints;dynamic voltage scaling;electronic circuit;emulator;event-driven programming;heart rate variability;inspiration function;low-power broadcasting;mixed-signal integrated circuit;network architecture;neural network simulation;neuromorphic engineering;obstacle avoidance;robotic mapping;von neumann architecture	Moritz B. Milde;Hermann Blum;Alexander Dietmüller;Dora Sumislawska;Jörg Conradt;Giacomo Indiveri;Yulia Sandamirskaya	2017		10.3389/fnbot.2017.00028	von neumann architecture;machine learning;computer science;robustness (computer science);mixed-signal integrated circuit;artificial intelligence;artificial neural network;control engineering;target acquisition;architecture;computer vision;neuromorphic engineering;obstacle avoidance	Robotics	46.61306522166442	-31.732163021709	138141
b2a64e130e9bac5d697fdb6b4fc4a1380279fd9f	reference velocity estimation method for on-road unmanned ground vehicle	remotely operated vehicles;navigation;trajectory control	The proposed method generates a safe global trajectory with reference velocities for a on-road unmanned ground vehicle to navigate in outdoor environments such as highways or roads. It generates a trajectory from a given global path consist of sparse waypoints without abrupt heading direction changes and estimates reference velocities to follow the given global path without slippages. It was implemented to verify that it can generate a safe trajectory with reference velocities using a data set acquired in a real outdoor environment.	course (navigation);sparse matrix;unmanned aerial vehicle;velocity (software development)	Byungjae Park;Woo Yong Han	2013	2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2013.6677350	remotely operated underwater vehicle;computer vision;navigation;simulation	Robotics	53.60229592305011	-36.504888028570576	138162
0d646dde271e8d91f9f50037a14aab2baaae269a	human tracking using electric fields	ubiquitous computing capacitance measurement electric fields electrodes microprocessor chips optical tracking;receiving electrode;ubiquitous location system;low frequency electric fields;pervasive tracking;low frequency;ubiquitous location systems;electric field;floor tiles;human tracking;chip;receivers;electric fields;electrodes;optical tracking;capacitance measurement;positioning accuracy;humans electrodes tiles capacitance measurement floors sensor systems intelligent sensors legged locomotion infrared sensors transmitters;ubiquitous computing;ubiquitous location system human tracking unobtrusive demo system low frequency electric fields capacitance measurement floor tiles receiving electrode single chip solution positioning accuracy;humans;capacitance;tiles;unobtrusive demo system;single chip solution;pervasive positioning;floors;microprocessor chips;electric fields pervasive positioning pervasive tracking human tracking ubiquitous location systems	In this paper we describe a simple, cheap, and unobtrusive demo system that can track multiple persons with low-frequency electric fields. The system's operation is based on measuring the capacitance between multiple floor tiles and a receiving electrode. The presented system is invisible to the user and uses a single-chip solution to measure the capacitances. The system was provided for hands-on evaluation for conference attendees with a 3.0 × 1.8 m floor space. The position and the track of the persons walking on the floor were projected on a screen. The floor of the demo system is divided to two different sections that are built of two different sizes of floor tiles to show how the tile size affects the positioning accuracy and the update rate of the system.	game demo;hands-on computing;operability;unobtrusive javascript	Miika Valtonen;Jukka Vanhala	2009	2009 IEEE International Conference on Pervasive Computing and Communications	10.1109/PERCOM.2009.4912795	embedded system;computer science;electric field;ubiquitous computing	Robotics	45.57634469567722	-26.284759894744816	138488
3f3f9f679020977e496c63965291240d0f1e72e2	a hybrid slam representation for dynamic marine environments	moving object;spline;model selection;clutter;simultaneous localization and mapping vehicle dynamics radar boats clutter clustering methods kinematics robotics and automation usa councils trajectory;motion control;reversible model selection;dynamic object;radar scan;occupancy grid;marine environment;egomotion hybrid slam representation dynamic marine environment cubic spline dynamic object point feature landmasses sliding window framework reversible data association reversible model selection occupancy grid fusion radar scan;usa councils;kinematics;data association;splines mathematics;egomotion;landmasses;trajectory;sliding window framework;position control;state space;point feature;simultaneous localization and mapping;occupancy grid fusion;vehicles;dynamic marine environment;position velocity;reversible data association;clustering methods;slam robots;marine control;robotics and automation;vehicle dynamics;sliding window;splines mathematics marine control motion control position control slam robots;radar;cubic spline;boats;hybrid slam representation	We present a hybrid SLAM system for marine environments that combines cubic splines to represent the trajectories of dynamic objects, point features to represent stationary objects and an occupancy grid to represent land masses. This hybrid representation enables SLAM to be applied in environments with moving objects, where solutions using point features alone are computationally prohibitive or where dense objects e.g. landmasses can not be represented correctly using point features. Estimation is achieved using a sliding window framework with reversible data-association and reversible model-selection. Our main contributions are: (i) a hybrid representation of the environment; (ii) occupancy grid fusion is continually refined for the duration of the sliding window; (iii) the trajectories of dynamic objects are represented using cubic splines and (iv) radar scans are re-rendered at a sub-scan resolution to compensate for the egomotion during the scan acquisition period. We show that the continual refinement of the occupancy grid greatly improves the quality of the resultant map, leading to a better estimate of the egomotion and therefore better estimates of the trajectories of dynamic objects. We also demonstrate that the use of cubic splines to represent trajectories has two major advantages: (i) the state space is compressed i.e. many vehicle poses can be represented using a single spline section and (ii) the trajectory becomes continuous and so fusing information from asynchronous sensors running at multiple frequencies becomes trivial. The efficacy of our system is demonstrated using real marine radar data, showing that it can successfully estimate the positions/velocities of objects and landmasses observed during a typical voyage on a small boat.	clutter;cubic function;model selection;movie projector;radar;refinement (computing);resultant;sensor;simultaneous localization and mapping;spline (mathematics);state space;stationary process;ti-92 series;visual odometry	Charles Bibby;Ian D. Reid	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509262	spline;computer vision;simulation;computer science;artificial intelligence;control theory	Robotics	53.559725822695746	-36.83632077237425	138855
1466d406dcb8f1552d6041ca08366f12cf0cdedb	data association for semantic world modeling from partial views	state estimation;data association;world modeling;article	Autonomous mobile-manipulation robots need to sense and interact with objects to accomplish high-level tasks such as preparing meals and searching for objects. To achieve such tasks, robots need semantic world models, defined as object-based representations of the world involving task-level attributes. In this work, we address the problem of estimating world models from semantic perception modules that provide noisy observations of attributes. Because attribute detections are sparse, ambiguous, and are aggregated across different viewpoints, it is unclear which attribute measurements are produced by the same object, so data association issues are prevalent. We present novel clustering-based approaches to this problem, which are more efficient and require less severe approximations compared to existing tracking-based approaches. These approaches are applied to data containing object type-and-pose detections from multiple viewpoints, and demonstrate comparable quality using a fraction of the computation time.	ambiguous grammar;approximation;autonomous robot;cluster analysis;computation;correspondence problem;high- and low-level;mobile manipulator;object type (object-oriented programming);object-based language;sensor;sparse matrix;time complexity	Lawson L. S. Wong;Leslie Pack Kaelbling;Tomás Lozano-Pérez	2015	I. J. Robotics Res.	10.1177/0278364914559754	computer science;artificial intelligence;data science;machine learning;data mining	AI	47.721955890842736	-37.78928052966145	138856
0ebfd2074acc2615e9c0553a72a79ab792feb54c	detection versus false alarm characterisation of a vision-based airborne dim-target collision detection system	false alarm;sensor system;hidden markov model;data collection;false alarm rate;detection;chip;vision collision detection false alarm hidden markov model morphology;morphology;hidden markov models;robot vision;collision detection;aerospace computing;robot vision aerospace computing aerospace testing autonomous aerial vehicles collision avoidance filtering theory hidden markov models object detection;unmanned aerial vehicle airborne collision avoidance problem detection characterisation false alarm characterisation vision based airborne dim target collision detection system test based detection range morphological hidden markov model filtering approach in flight collision scenario data system operating characteristic curves false alarm rate performance design trade offs dim target detection performance description data collection machine vision;hidden markov models aircraft clouds sensors system on a chip object detection cameras;operating characteristic;collision avoidance;collision;aerospace testing;target detection;vision;autonomous aerial vehicles;filtering theory;object detection	This paper presents a preliminary flight test based detection range versus false alarm performance characterisation of a morphological-hidden Markov model filtering approach to vision-based airborne dim-target collision detection. On the basis of compelling in-flight collision scenario data, we calculate system operating characteristic (SOC) curves that concisely illustrate the detection range versus false alarm rate performance design trade-offs. These preliminary SOC curves provide a more complete dim-target detection performance description than previous studies (due to the experimental difficulties involved, previous studies have been limited to very short flight data sample sets and hence have not been able to quantify false alarm behaviour). The preliminary investigation here is based on data collected from 4 controlled collision encounters and supporting non-target flight data. This study suggests head-on detection ranges of approximately 2.22 km under blue sky background conditions (1.26 km in cluttered background conditions), whilst experiencing false alarms at a rate less than 1.7 false alarms/hour (ie. less than once every 36 minutes). Further data collection is currently in progress.	airborne ranger;airborne collision avoidance system;collision detection;hidden markov model;markov chain;while	John Lai;Jason J. Ford;Luis Mejías Alvarez;Peter O'Shea;Rodney A. Walker	2011	2011 International Conference on Digital Image Computing: Techniques and Applications	10.1109/DICTA.2011.82	chip;vision;computer vision;simulation;computer science;machine learning;constant false alarm rate;computer security;collision detection;hidden markov model;statistics;data collection;collision	Robotics	48.90850311643964	-36.04540638044174	139037
f936c863f7fa6fe25694efb780b58fddb8c6a27f	motion-vector estimation and cognitive classification on an image sensor/processor 3d stacked system featuring thruchip interfaces	clocks;estimation;system on chip;three dimensional displays;feature extraction;imaging;ports computers	1,000 fps motion vector estimation and classification engine for highspeed computational imaging in a 3D stacked imager/processor module is proposed, prototyped, assembled, and also tested. The module features ThruChip interfaces for high fps image transfer, orders of magnitude more area/power efficient motion vector estimation architecture compared to conventional ones, and a cognitive classification scheme employed on motion vector patterns, enabling the classification of moving objects not possible in conventional proposals.	comparison and contrast of classification schemes in linguistics and metadata;image sensor;interference (communication);semiconductor;stacking;three-dimensional integrated circuit	Tetsuya Asai;Masafumi Mori;Toshiyuki Itou;Yasuhiro Take;Masayuki Ikebe;Tadahiro Kuroda;Masato Motomura	2016	ESSCIRC Conference 2016: 42nd European Solid-State Circuits Conference	10.1109/ESSCIRC.2016.7598253	medical imaging;system on a chip;embedded system;computer vision;estimation;electronic engineering;feature extraction;computer science	EDA	45.18541712816329	-34.65939324473169	139408
e188f5d017ef89995ef93b876a08a5b9805219ba	real time position estimation for mobile robots by means of sonar sensors	mobile robot;path planning;real time;mobile robots;dynamic environment;pyramidal structure real time position estimation sonar sensors fast localisation algorithm autonomous mobile agents dynamic environments landmarks circular depth function bi dimensional base vectorial space;mobile robots robot sensing systems orbital robotics sonar detection sonar measurements telecommunications mobile agents navigation dead reckoning algorithm design and analysis;position measurement;position estimation;fast fourier transforms;position measurement mobile robots sonar fast fourier transforms path planning;mobile agent;sonar	This paper presents a fast localisation algorithm for autonomous mobile agents in dynamic environments based on the definition of a new very small sized landmark type. These landmarks are calculated by obtaining the coordinates of the circular depth jimction obtained from a ring of equally spaced sonar sensors projected on a bidimensional base of a vectorial space. Finally, a pyramidal structure is used to enhance and fasten the performance of the localisation algorithm.	algorithm;autonomous robot;language localisation;mobile agent;sonar (symantec);sensor	Cristina Urdiales;Antonio Bandera;R. Ron;Francisco Sandoval Hernández	1999		10.1109/ROBOT.1999.772596	control engineering;mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence	Robotics	53.158624931500995	-34.262294407447186	139507
5e29878d9078275748ea3645d263b76d841ea495	a comparison of sequential and gpu-accelerated implementations of b-spline signal processing operations for 2-d and 3-d images	image resolution;splines mathematics graphics processing units matlab laplace equations kernel convolution transforms;recursive filters;splines mathematics c language graphics processing units image resolution parallel architectures recursive filters;splines mathematics;3d magnetic resonance gpu accelerated implementation b spline signal processing 2d image 3d image indirect transformation partial derivative recursive filter matlab c cuda resolution level 2d panoramic image;c language;parallel architectures;graphics processing units;gpu b spline filtering interpolation	B-spline signal processing operations are widely used in the analysis of two and three-dimensional images. In this paper, we investigate and compare some of these basic operations (direct transformations, indirect transformations, and computation of partial derivatives) by (1) recursive filter based implementations in MATLAB and C++, and (2) GPU-accelerated implementations in CUDA. All operations are compared at a variety of resolution levels on a 2-D panoramic image as well as a 3-D magnetic resonance (MR) image. Results indicate significant improvements in efficiency for the CUDA implementations. A MATLAB toolkit implementing the various B-spline signal processing tasks as well as the C++ and CUDA implementation described here is currently publicly available.	b-spline;c++;cuda;computation;download;graphics processing unit;matlab;recursion;recursive filter;resonance;signal processing	Alex Karantza;Sonia Lopez-Alarcon;Nathan D. Cahill	2012	2012 3rd International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2012.6469565	computer vision;image resolution;computer science;theoretical computer science;digital image processing;general-purpose computing on graphics processing units;computer graphics (images)	HPC	41.53847873659884	-32.19649132749774	139752
da0ee57312eb2776378c13e6198fb9d3e54b7a58	"""""""stet model"""" : a framework for realistic surveillance setup simulation"""	surveillance;sensor modeling;data fusion.;terrain modeling;target modeling;data fusion	‘STET model’ is a framework for realistic surveillance setup in which multi-target multi-sensor scenario can be modeled for surveillance applications. The components of STET model framework are Sensor (S), Targets (T), Environment (E) and Terrain (T). The set of characters relevant to surveillance operation of the sensor is captured in Sensor modeling. In target modeling, the characteristics of the target, which can be seen by sensor and the movement of it are modeled. Environment model considers the factors that affect the sensor and surveillance. The elevation profile of the terrain is considered for modeling the terrain, which influence not only the detection characteristics of the sensor but also the movement of surface target on it. STET model framework suggests the modeling of each of its component that affects the surveillance scenario independently. It then considers the system as a whole while allowing for the simulation of whole scenario.	sensor;simulation	Smita D. Naik;S. D. Suresh	2006				Robotics	49.280585465294436	-34.82959142801162	139764
28c93371dacf3f637cd7c6f9df83b502a7a06cbc	analog vlsi circuits for attention-based, visual tracking	analog vlsi;visual tracking	A one-dimensional, visual tracking chip has been implemented using neuromorphic, analog VLSI techniques to model selective visual attention in the control of saccadic and smooth pursuit eye movements. The chip incorporates focal-plane processing to compute image saliency and to select a target feature for tracking. The target position and direction of motion are reported as the target moves across the array. The chip combines and extends previous neuromorphic analog VLSI models of covert attentional processes and eye movement generation to implement both saccadic and smooth pursuit tracking behavior using a one-dimensional mechanical eye.	analog-to-digital converter;focal (programming language);neuromorphic engineering;very-large-scale integration;video tracking	Timothy K. Horiuchi;Tonia G. Morris;Christof Koch;Stephen P. DeWeerth	1996			computer vision;eye tracking;computer science;computer graphics (images)	ML	45.90682718638863	-33.86460089843427	139785
d0ffb6f5f0e691e201fa93a865cf0144e9eee632	a real time ecg preprocessing system based on ads1298	texas instruments computers electrocardiography medical signal processing microcontrollers real time systems;microcontrollers;texas instruments computers;electrocardiography;voltage 11 muv to 9 muv real time ecg preprocessing multichannel system multichannel high resolution ads1298 integrated circuit msp430f5529 microcontroller texas instruments acquisition system international standard iec60601 2 25 2011 signal parametric characteristics recovery times common mode rejection ratio cmrr noise level time 1 33 s gain 94 3 db gain 93 71 db;abstracts interference microprogramming matlab immune system electrocardiography;medical signal processing;real time systems	The aim of this paper is to discuss the main characteristics of a real time ECG pre-processing multichannel system designed by the authors of the paper. It is based on the multichannel high-resolution ADS1298 integrated circuit and the MSP430F5529microcontroller, both from Texas Instruments. The developed acquisition system complies with the international standard IEC60601-2-25:2011 {ed2.0} and provides improvements in parametric characteristics of the signal with respect to previous solutions. The recovery times decreased to 1.33 seconds and the common mode rejection ratio (CMRR) has been increased to 94.3 dB in lead I and 93.71dB in lead II as compared to the corresponding values 91.12 and 90.86 obtained by the authors in previous solutions. Noise level decreased from 11μV to 9μV. The present solution is more compact and reliable so it can be used in different equipments for real time ECG pre-processing.	deny (action);electrocardiographic recorders;image resolution;instrument - device;integrated circuit device component;preprocessor;rejection sampling;solutions	Daniel Campillo;Ronny Guardarrama;Rene Gonzalez;Jorge Rodriguez;Daniel Jiménez	2013	Computing in Cardiology 2013		embedded system;electronic engineering;engineering;electrical engineering	Robotics	43.99567046464675	-30.01165238408866	140042
7af6862304bcd47376e197bf50993d25c0ea42a5	fpga-based robust ellipse estimation for circular road sign detection	circular road sign detection;field programmable gate array;ellipse detection;video streaming;video signal processing;binary image;image preprocessing pipeline;computer vision task;ellipse estimation;road traffic;real time;model fitting algorithm;computer vision;video streams;video signal processing driver information systems field programmable gate arrays object detection road traffic traffic engineering computing;computational modeling;design and implementation;roads;robustness roads field programmable gate arrays algorithm design and analysis curve fitting computer vision streaming media pipelines pixel image analysis;mathematical model;floating point software version ellipse estimation field programmable gate array circular road sign detection image estimation computer vision task model fitting algorithm video streams ransac algorithm image preprocessing pipeline pixel level analysis xilinx spartan 3a dsp 3400a device;pixel level analysis;traffic engineering computing;image estimation;floating point;model fitting;field programmable gate arrays;driver information systems;xilinx spartan 3a dsp 3400a device;algorithm design and analysis;floating point software version;parametric curve;object detection;ransac algorithm;data models;real time systems	Estimating parametric curves from images using robust fitting algorithms is a well-known and important computer vision task. We present a complete FPGA design and implementation of a fast and robust model fitting algorithm for real-time ellipse detection on video streams. The proposed solution relies on a the RANSAC algorithm, modified for FPGA deployment, in combination with an image-preprocessing pipeline in order to perform the intensive pixel-level analysis, reducing each frame to a simple binary image of edges. The design has been developed in a parallel fashion and with specific architectural solutions so as to allow a fast response without degrading the functional performances. Experimental results on synthetic and real data show that our implementation, synthesized onto a Xilinx Spartan-3A DSP 3400A device, succesfully runs in real-time with a low resource occupation, while maintaining a functionality comparable with the floating-point software version.	algorithm;approximation;binary image;computer vision;curve fitting;digital signal processor;error detection and correction;field-programmable gate array;fixed point (mathematics);performance;pixel;preprocessor;random sample consensus;real-time clock;real-time transcription;software deployment;software versioning;streaming media;supercomputer;synthetic data;synthetic intelligence	Samuele Martelli;Roberto Marzotto;Andrea Colombari;Vittorio Murino	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops	10.1109/CVPRW.2010.5543761	embedded system;computer vision;computer science;field-programmable gate array;computer graphics (images)	Vision	43.23089612029594	-35.978490775220244	140668
7e0152ab57077b33f95addd17674e50333638fec	beobot 2.0: autonomous mobile robot localization and navigation in outdoor pedestrian environment	roads navigation collision avoidance mobile robots conferences image segmentation;mobile robots;pedestrians;pedestrians collision avoidance mobile robots;collision avoidance;service vehicles beobot 2 0 autonomous mobile robot localization autonomous mobile robot navigation outdoor pedestrian environment unconstrained urban environments long range travel usc campus obstacle avoidance	We present Beobot 2.0 [1], an autonomous mobile robot designed to operate in unconstrained urban environments. The goal of the project is to create service robots that can be deployed for various tasks that require long range travel. Over the past two years, Beobot has successfully traversed various paths across the USC campus, demonstrating its robustness in recognizing and following different types of roads, avoiding obstacles such as pedestrians and service vehicles, and finding its way to the goal.	autonomous robot;mobile robot;robotic mapping	Chin-Kai Chang;Christian Siagian;Laurent Itti	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6696642	mobile robot;embedded system;computer vision;simulation;computer science;artificial intelligence;robot control;mobile robot navigation	Robotics	53.59482539585985	-31.451693243340806	140722
0ea92701e1079c91ecf1702c161d8d5590f04f09	optimizing multi-target detection in stochastic environments with active smart camera networks		Active Smart Camera Networks (SCNs) have a wide range of applications in areas such as banks, airports and underground terminals, where it is necessary to monitor open plan spaces reliably and robustly. However, the monitoring efficiency in such environments is affected by various factors which are stochastic in nature, such as the target movement and arrival times, as well as the capabilities of the on-board camera detection module. This paper presents a modelling framework and a subsequent optimization-based solution towards the optimal reconfiguration of the camera network in order to minimize the expected number of undetected targets under uncertainties. The proposed solution is applicable to a variety of scenarios and does not require to have a full view of the area. Simulation results indicate that the proposed solution is robust to varying conditions and is able to achieve good monitoring performance with only a few cameras.	mathematical optimization;on-board data handling;optimizing compiler;robustness (computer science);simulation;smart camera;stochastic process	Christos Kyrkou;Stelios Timotheou;Theocharis Theocharides;Christoforos Panayiotou;Marios M. Polycarpou	2017		10.1145/3131885.3131914	artificial intelligence;real-time computing;computer vision;control reconfiguration;open plan;computer science;smart camera;active vision	Vision	52.48669746074637	-26.883438847532283	141001
1f70b5a0de60c663bbc19456367c2c3f8dec7f56	gaussian mixture models for probabilistic localization	gaussian processes;sensor model;probabilistic localization;mobile robots;probabilistic approach;sensor model gaussian mixture models probabilistic localization mobile robot localization robot pose;gaussian mixture model;position control;gaussian mixture models;robot sensing systems mobile robots robustness vehicles monte carlo methods filters uncertainty particle measurements robotics and automation usa councils;mobile robot localization;robot pose;mixture of gaussians;position control gaussian processes mobile robots;likelihood function	One of the key tasks during the realization of probabilistic approaches to localization is the design of a proper sensor model, that calculates the likelihood of a measurement given the current pose of the vehicle and the map of the environment. In the past, range sensors have become popular for mobile robot localization since they directly measure distance. However, in situations in which the robot operates close to edges of obstacles or in highly cluttered environments, small changes in the pose of the robot can lead to large variations in the acquired range scans. If the sensor model used does not appropriately characterize the resulting fluctuations, the performance of probabilistic approaches may substantially degrade. A common solution is to artificially smooth the likelihood function or to only integrate a small fraction of the measurements. In this paper we present a more fundamental and robust approach which uses mixtures of Gaussians to model the likelihood function for single range measurements. In practical experiments we compare our approach to previous methods and demonstrate that it yields a substantially increase in robustness.	ct scan;computation;computational resource;expectation–maximization algorithm;experiment;matrix regularization;mixture model;mobile robot;norm (social);robotic mapping;sensor	Patrick Pfaff;Christian Plagemann;Wolfram Burgard	2008	2008 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2008.4543251	monte carlo localization;computer vision;simulation;computer science;engineering;machine learning;mixture model;statistics	Robotics	52.450911593262454	-35.650045535662706	141146
86dabbe09be5f352714484199fedc2df8bb4d27e	running across the reality gap: octopod locomotion evolved in a minimal simulation	robot movil;systeme commande;sistema control;algorithm performance;neural net architecture;algoritmo genetico;control system;robot mobile;resultado algoritmo;architecture reseau neuronal;performance algorithme;algorithme genetique;algorithme evolutionniste;genetic algorithm;evolutionary algorithm;moving robot	This paper describes experiments in which neural network control architectures were evolved in minimal simulation for an octopod robot. The robot is around 30cm long and has 4 infra red sensors that point ahead and to the side, various bumpers and whiskers, and ten ambient light sensors positioned strategically around the body. Each of the robot's eight legs is controlled by two servo motors, one for movement in the horizontal plane, and one for movement in the vertical plane, which means that the robots motors have a total of sixteen degrees of freedom. The aim of the experiments was to evolve neural network control archi-tectures that would allow the robot to wander around its environment avoiding objects using its infra-red sensors and backing away from objects that it hits with its bumpers. This is a hard behaviour to evolve when one considers that in order to achieve any sort of coherent movement the controller has to control not just one or two motors in a coordinated fashion but sixteen. Moreover it is an extremely diicult setup to simulate using traditional techniques since the physical outcome of sixteen motor movements is rarely predictable in all but the simplest cases. The evolution of this behaviour in a minimal simulation, with perfect transference to reality, therefore, provides essential evidence that complex motor behaviours can be evolved in simulations built according to the theory and methodology of minimal simulations.	archi;artificial neural network;coherence (physics);controller (computing);experiment;robot;sensor;servo;simulation	Nick Jacobi	1998		10.1007/3-540-64957-3_63	simulation;genetic algorithm;computer science;control system;artificial intelligence;machine learning;evolutionary algorithm;algorithm	Robotics	51.94828858585659	-28.96018429952047	141441
7924c8823ee9d77047271a4fa4fc5a1d1cc6c3ba	high-level tracking using bayesian context fusion		This paper presents a Bayesian tracking approach that exploits various types of context information. The filtering accuracy and precision are improved by using uncertain information about (i) the constraints on the target mobility, (ii) environmental influences on the sensor performance and (iii) typical target behaviors. The approach combines particle filters with exact Bayesian networks. The overall process is equivalent to approximate inference on elaborate dynamic Bayesian networks that systematically capture non-trivial correlations between the estimated states of the dynamic processes, the associated observations and the various factors influencing the dynamic processes. The particle filter supports reasoning about continuous dynamic processes spanning large areas, while the Bayesian networks are used for the implementation of advanced sensor models and for the fusion of uncertain data on mobility constraints. The derivation of the solution is based on the decomposability principles of Bayesian networks. The approach is illustrated with the help of a challenging wildlife protection application. A set of qualitative experiments shows the improvement in tracking performance by considering the different types of context information.	approximation algorithm;behavior model;dynamic bayesian network;estimation theory;experiment;file spanning;image noise;particle filter;quantum information;relevance;seamless3d;sensor;sparse matrix;uncertain data;uncertain inference	Patrick de Oude;Gregor Pavlin;J. P. de Villiers	2018	2018 21st International Conference on Information Fusion (FUSION)	10.23919/ICIF.2018.8455342	uncertain data;artificial intelligence;machine learning;computer science;dynamic bayesian network;accuracy and precision;approximate inference;filter (signal processing);particle filter;bayesian network;bayesian probability	Robotics	50.468080241702076	-34.113243705046294	141494
c69714e76b21e2666ca0b18547fbd2acd006ffa7	optimization of robot paths computed by randomized planners	optimal control randomized path planning;randomized;cost function;path planning;orbital robotics;optimal control;tree graphs;gradient descent;agricultural engineering;robots;motion planning;optimality criteria;sampling methods;robots path planning optimal control sampling methods cost function motion planning orbital robotics tree graphs agricultural engineering laboratories	Randomized path planners have been successfully used to compute feasible paths for difficult planning problems. Such paths are typically computed without taking into account any optimality criteria and may contain many “jagged” segments because of the randomness involved in their generation. This paper presents a two-phase path planning algorithm, which uses a randomized planner to compute low-cost paths, and gradient descent to locally optimize these paths by minimizing a Hamiltonian function. The algorithm is tested on motion planning for a non-holonomic car-like robot. The results indicate that the two-phase approach is practical; however, gradient descent seems to be inefficient for the optimization of long paths.	automated planning and scheduling;gradient descent;mathematical optimization;motion planning;randomized algorithm;randomness;robot;two-phase commit protocol;two-phase locking	Stavros G. Vougioukas	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570431	mathematical optimization;simulation;any-angle path planning;computer science;artificial intelligence;machine learning;mathematics;motion planning	Robotics	52.69240461293539	-24.185805526664783	141625
3a5bc67d09e897f53982a8e04eabe3b236483b64	perception-aware receding horizon navigation for mavs		To reach a given destination safely and accurately, a micro aerial vehicle needs to be able to avoid obstacles and minimize its state estimation uncertainty at the same time. To achieve this goal, we propose a perception-aware receding horizon approach. In our method, a single forward-looking camera is used for state estimation and mapping. Using the information from the monocular state estimation and mapping system, we generate a library of candidate trajectories and evaluate them in terms of perception quality, collision probability, and distance to the goal. The best trajectory to execute is then selected as the one that maximizes a reward function based on these three metrics. To the best of our knowledge, this is the first work that integrates active vision within a receding horizon navigation framework for a goal reaching task. We demonstrate by simulation and real-world experiments on an actual quadrotor that our active approach leads to improved state estimation accuracy in a goal-reaching task when compared to a purely-reactive navigation system, especially in difficult scenes (e.g., weak texture).	active vision;aerial photography;algorithm;experiment;information;reinforcement learning;simulation;sparse voxel octree;visual odometry	Zichao Zhang;Davide Scaramuzza	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8461133	collision;task analysis;control engineering;perception;computer vision;horizon;monocular;trajectory;engineering;navigation system;artificial intelligence;active vision	Robotics	53.31502748152612	-32.54946234959512	141657
3c66bd5669065a2167669f3beef89d0a3989aa5d	rise: an automated framework for real-time intelligent video surveillance on fpga		This paper proposes RISE, an automated Reconfigurable framework for real-time background subtraction applied to Intelligent video SurveillancE. RISE is devised with a new streaming-based methodology that adaptively learns/updates a corresponding dictionary matrix from background pixels as new video frames are captured over time. This dictionary is used to highlight the foreground information in each video frame. A key characteristic of RISE is that it adaptively adjusts its dictionary for diverse lighting conditions and varying camera distances by continuously updating the corresponding dictionary. We evaluate RISE on natural-scene vehicle images of different backgrounds and ambient illuminations. To facilitate automation, we provide an accompanying API that can be used to deploy RISE on FPGA-based system-on-chip platforms. We prototype RISE for end-to-end deployment of three widely-adopted image processing tasks used in intelligent transportation systems: License Plate Recognition (LPR), image denoising/reconstruction, and principal component analysis. Our evaluations demonstrate up to 87-fold higher throughput per energy unit compared to the prior-art software solution executed on ARM Cortex-A15 embedded platform.	arm cortex-a15;arm architecture;application programming interface;automatic number plate recognition;background subtraction;closed-circuit television;data dictionary;dictionary;embedded system;end-to-end principle;field-programmable gate array;graphic art software;greedy algorithm;image processing;noise reduction;openmp;pixel;principal component analysis;prototype;rapid prototyping;real-time clock;real-time transcription;software deployment;system on a chip;throughput;usability	Bita Darvish Rouhani;Azalia Mirhoseini;Farinaz Koushanfar	2017	ACM Trans. Embedded Comput. Syst.	10.1145/3126549	automation;image processing;real-time computing;field-programmable gate array;software;computer science;pixel;reconfigurable computing;computer vision;artificial intelligence;intelligent transportation system;background subtraction	Robotics	42.81323876891761	-36.95240359423862	141769
4f1c8fdbf380cb611db05ee2081a7b32a80a94fc	probabilistic target detection by camera-equipped uavs	bird eye view camera;sensing;uav;search and rescue;aircraft control;target detection algorithm;quadrotor unmanned aerial vehicles;bird eye view camera probabilistic target detection camera equipped unmanned aerial vehicles quadrotor unmanned aerial vehicles target detection algorithm bayesian estimator camera sampling rate;sensors;bayes methods;unmanned aerial vehicle;training;false negative;bayesian methods;mobile robots;robotics;remotely operated vehicles;conference papers meetings and proceedings;features;accuracy;robot vision aerospace robotics aircraft control bayes methods cameras mobile robots object detection remotely operated vehicles;robot vision;aerospace robotics;camera equipped unmanned aerial vehicles;bayesian estimator;false positive;target detection;vision;object detection unmanned aerial vehicles cameras target tracking sampling methods bayesian methods recursive estimation time measurement robotics and automation usa councils;unmanned aerial vehicles;probabilistic target detection;robotics and automation;cameras;sensing uav vision target detection;object detection;camera sampling rate	This paper is motivated by the real world problem of search and rescue by unmanned aerial vehicles (UAVs). We consider the problem of tracking a static target from a bird's-eye view camera mounted to the underside of a quadrotor UAV. We begin by proposing a target detection algorithm, which we then execute on a collection of video frames acquired from four different experiments. We show how the efficacy of the target detection algorithm changes as a function of altitude. We summarise this efficacy into a table which we denote the observation model. We then run the target detection algorithm on a sequence of video frames and use parameters from the observation model to update a recursive Bayesian estimator. The estimator keeps track of the probability that a target is currently in view of the camera, which we refer to more simply as target presence. Between each target detection event the UAV changes position and so the sensing region changes. Under certain assumptions regarding the movement of the UAV, the proportion of new information may be approximated to a value, which we then use to weight the prior in each iteration of the estimator. Through a series of experiments we show how the value of the prior for unseen regions, the altitude of the UAV and the camera sampling rate affect the accuracy of the estimator. Our results indicate that there is no single optimal sampling rate for all tested scenarios. We also show how the prior may be used as a mechanism for tuning the estimator according to whether a high false positive or high false negative probability is preferable.	aerial photography;approximation algorithm;bird's-eye view;computer performance;experiment;fits;ground truth;iteration;negative probability;recursion;requirement;sampling (signal processing);sensor;unmanned aerial vehicle;vii	Andrew Colquhoun Symington;Sonia Waharte;Simon J. Julier;Agathoniki Trigoni	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509355	remotely operated underwater vehicle;mobile robot;vision;computer vision;simulation;type i and type ii errors;bayesian probability;computer science;engineering;sensor;artificial intelligence;accuracy and precision;robotics;statistics;remote sensing	Robotics	51.07930905476326	-34.623287319932686	141837
2fa509b101dab06390773ee1aa455fc3e1efdcce	video tracking and embedded microcontrollers	video tracking		embedded system;microcontroller;video tracking	Bassam Shaer;David Cudney	2010			discrete mathematics;video capture;microcontroller;computer vision;computer science;tracking system;video tracking;video processing;artificial intelligence	EDA	45.13764210398243	-36.63178763850236	142229
293bc015e2a0f2167fbddef9ebdc65f079b36121	curvace - curved artificial compound eyes	vision based navigation;soft robotics;aerial robotics;compound eyes;artificial vision;bio inspired engineering	CURVACE aims at designing, developing, and assessing CURVed Artificial Compound Eyes, a radically novel family of vision systems. This innovative approach will provide more efficient visual abilities for embedded applications that require motion analysis in low-power and small packages. Compared to conventional cameras, artificial compound eyes will offer a much larger field of view with negligible distortion and exceptionally high temporal resolution in smaller size and weight that will fit the requirements of a wide range of applications. © Selection and peer-review under responsibility of FET11 conference organizers and published by Elsevier B.V.	distortion;embedded system;low-power broadcasting;requirement	Ramon Pericet-Camara;Michal Karol Dobrzynski;Géraud L'Eplattenier;Jean-Christophe Zufferey;Fabien Expert;Raphaël Juston;Franck Ruffier;Nicolas H. Franceschini;Stéphane Viollet;Mohsine Menouni;Stéphanie Godiot;Andreas Brückner;Wolfgang Buss;Robert Leitel;Fabian Recktenwald;Chunrong Yuan	2011		10.1016/j.procs.2011.09.040	computer vision;simulation;artificial intelligence	Robotics	44.49933774401401	-36.71006256302215	142275
b87da767eb403db69d64490cf592eb55843c8756	outdoor map building based on odometry and rtk-gps positioning fusion	smoothing methods global positioning system mobile robots navigation path planning laser ranging;global positioning system mobile robots navigation robot sensing systems shape position measurement intelligent robots systems engineering and theory kalman filters smoothing methods;map building;path planning;mobile robots;scanning lrf equipment outdoor map building rtk gps positioning fusion real time kinematic gps mobile robot navigation odometry laser range finder kalman smoothing technique;laser ranging;navigation;smoothing methods;laser range finder;global positioning system;mobile robot navigation	The final objective of the authors is to realize mobile robot navigation in walkway of outdoor environment. The path along the walkway which is measured correctly together with landmarks must be given in the framework in this paper. The robot identifies its position by means of odometry, DGPS and laser range finder (LRF) during the motion along the path. This work contributes to the map building of landmarks along the walkway. To make the map, RTK-GPS, odometry and LRF are used for the sensory devices and Kalman smoothing technique is also incorporated to interpolate trajectory of the scanning LRF equipment between the two point that RTK-GPS measurement took place. The scanning by LRF is performed to acquire the 3-D shape of objects along the walkway. The trajectory presumed properly and well organized shapes of the objects are reconstructed.	differential gps;global positioning system;interpolation;mobile robot;odometry;real time kinematic;robotic mapping;smoothing	Kazunori Ohno;Takashi Tsubouchi;Shin'ichi Yuta	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1307228	mobile robot;computer vision;navigation;simulation;global positioning system;computer science;artificial intelligence;motion planning;mobile robot navigation;remote sensing	Robotics	52.06738106003104	-36.56511097201571	142283
1c8eebfa09d75e936473823073583fb7fa905c0f	the effects of communication and visual range on multi-robot repeated boundary coverage	uncertain systems;probability;mobile robots;robot vision;travelling salesman problems;multi robot systems;coordination multi robot boundary coverage uninformed boundary coverage informed boundary coverage teamwork;robot communication multirobot repeated boundary coverage probability reward maximization learning algorithm heuristic algorithm travelling salesman problem robot visual range;learning artificial intelligence;uncertain systems learning artificial intelligence mobile robots multi robot systems probability robot vision travelling salesman problems	We address the problem of repeated coverage by a team of robots of the boundaries of a target area and the structures inside it. The robots have limited visual and communication range. Events may occur on any parts of the boundaries and may have different importance weights. In addition, the boundaries of the area and the structures are heterogeneous, so that events may appear with varying probabilities on different parts of the boundary, and these probabilities may change over time. The goal is to maximize the reward by detecting the maximum number of events, weighted by their importance, in minimum time. The reward a robot receives for detecting an event depends on how early the event is detected. To this end, each robot autonomously and continuously learns the pattern of event occurrence on the boundaries over time, capturing the uncertainties in the target area. Based on the policy being learned to maximize the reward, each robot then plans in a decentralized manner to select the best path at that time in the target area to visit the most promising parts of the boundary. The performance of the learning algorithm is compared with a heuristic algorithm for the Travelling Salesman Problem, on the basis of the total reward collected by the team during a finite repeated boundary coverage mission. Moreover, the effects of robots' visual range and communication among the robots on the performance of the proposed algorithms are also investigated.	algorithm;elegant degradation;heuristic (computer science);importance sampling;robot;sensor;travelling salesman problem	Pooyan Fazli;Alan K. Mackworth	2012	2012 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)	10.1109/SSRR.2012.6523872	robot learning;simulation;computer science;artificial intelligence;machine learning	Robotics	53.03931934013732	-25.495667339924417	142300
8d063cf90b1eff86df0e2000bd894b0d72f0753b	pomdp planning for robust robot control	robot control;planning and control;partial observation	POMDPs provide a rich framework for planning and control in partially observable domains. Recent new algorithms have greatly improved the scalability of POMDPs, to the point where they can be used in robot applications. In this paper, we describe how approximate POMDP solving can be further improved by the use of a new theoretically-motivated algorithm for selecting salient information states. We present the algorithm, called PEMA, demonstrate competitive performance on a range of navigation tasks, and show how this approach is robust to mismatches between the robot’s physical environment and the model used for planning.	approximation algorithm;partially observable markov decision process;partially observable system;robot control;scalability	Joelle Pineau;Geoffrey J. Gordon	2005		10.1007/978-3-540-48113-3_7	robot control	Robotics	51.746014595807445	-26.75704605993049	142345
2b90d836a03b465798507801999fab8659f02dfb	reconfigurable machine vision systems using fpgas	feature detection;multiprocessing systems computer vision feature extraction field programmable gate arrays;mobile robot;plug and play;real time processing;computer vision;general purpose processor;machine vision;feature extraction;pixel;next generation;ip networks;optical flow;multiprocessing systems;production cost;correlation;field programmable gate arrays;pixel machine vision field programmable gate arrays correlation hardware cameras ip networks;mobile application;cameras;image data reconfigurable machine vision systems fpga flexible architecture parallel portions quick turn prototyping machine vision ip cores robotic mobility applications optical flow feature detection image homography real time processing;hardware	FPGAs provide a flexible architecture for implementing many different types of machine vision algorithms. They allow heavily parallel portions of those algorithms to be accelerated and optimized for high specific performance (MIPS:Watt ratio). In comparison to ASICS, FPGAs enable low cost, quick turn prototyping and algorithm development as well as lower production costs for small quantity and one off applications. FPGAs also have the ability to be reprogrammed in flight, allowing them to be configured for different applications as mission needs evolve. JPL has developed a suite of machine vision IP cores to accelerate many common machine vision tasks used in robotic mobility applications. Modules such as stereo correlation for ranging, filtering, optical flow, area based correlation, feature detection, and image homography and rectification allow the real-time processing of image data using much smaller systems with much less power draw then an appropriately sized general purpose processor. These modules, along with a vision processing framework, are being re-cast in a generic plug and play form to allow rapid, low cost configuration, reconfiguration, evolution and adaptation of next generation machine vision systems for mobile robotics.	algorithm;application-specific integrated circuit;computation;coprocessor;feature detection (computer vision);feature detection (web development);field-programmable gate array;filter (signal processing);homography (computer vision);image rectification;machine vision;mobile robot;next-generation network;norm (social);optical flow;parallel computing;performance per watt;plug and play;real-time clock;robotics;semiconductor intellectual property core	Carlos Villalpando;Raphael R. Some	2010	2010 NASA/ESA Conference on Adaptive Hardware and Systems	10.1109/AHS.2010.5546238	mobile robot;embedded system;computer vision;real-time computing;machine vision;feature extraction;computer science;operating system;feature detection;optical flow;correlation;pixel;field-programmable gate array	Robotics	44.9590832386555	-35.50171918803729	142425
6ea2ae3adf9ec3d67d4c1b8a308aec490552b9bb	towards driving autonomously: autonomous cruise control in urban environments	road traffic;vehicles acceleration cameras image segmentation trajectory target tracking urban areas;autonomous cruise control next traffic light speed limit longitudinal velocity situation analysis environment perception traffic participants automatic driving urban environments;mobile robots;road vehicles road traffic;telerobotics;driver information systems;telerobotics driver information systems mobile robots road traffic road vehicles;road vehicles;towards driving autonomously longitudinal velocity automatic driving strategies situation analysis environment perception infrastructure information traffic participants vehicle control urban environments autonomous cruise control	For automatic driving, vehicles must be able to recognize their environment and take control of the vehicle. The vehicle must perceive relevant objects, which includes other traffic participants as well as infrastructure information, assess the situation and generate appropriate actions. This work is a first step of integrating previous works on environment perception and situation analysis toward automatic driving strategies. We present a method for automatic cruise control of vehicles in urban environments. The longitudinal velocity is influenced by the speed limit, the curvature of the lane, the state of the next traffic light and the most relevant target on the current lane. The necessary acceleration is computed in respect to the information which is estimated by an instrumented vehicle.	automatic control;autonomous robot;lateral thinking;microsoft outlook for mac;velocity (software development)	Ralf Kohlhaas;Thomas Schamm;Dominik Lenk;Johann Marius Zöllner	2013	2013 IEEE Intelligent Vehicles Symposium Workshops (IV Workshops)	10.1109/IVS.2013.6629457	simulation;floating car data;vehicle information and communication system;engineering;automotive engineering;transport engineering	Robotics	50.64340428186258	-35.93995250433207	142458
6ca830d930573ca857abaab9e63f7c89a24a8f0e	structural inspection path planning via iterative viewpoint resampling with application to aerial robotics	robot sensing systems;complexity theory;inspection;three dimensional displays;inspection three dimensional displays robot sensing systems cameras planning complexity theory;planning;alternating two step optimization structural inspection path planning iterative viewpoint resampling aerial robotics complex 3d structures triangular mesh representation rotorcraft fixed wing unmanned aerial systems;telerobotics autonomous aerial vehicles condition monitoring helicopters inspection mobile robots path planning structural engineering;cameras	Within this paper, a new fast algorithm that provides efficient solutions to the problem of inspection path planning for complex 3D structures is presented. The algorithm assumes a triangular mesh representation of the structure and employs an alternating two-step optimization paradigm to find good viewpoints that together provide full coverage and a connecting path that has low cost. In every iteration, the viewpoints are chosen such that the connection cost is reduced and, subsequently, the tour is optimized. Vehicle and sensor limitations are respected within both steps. Sample implementations are provided for rotorcraft and fixed-wing unmanned aerial systems. The resulting algorithm characteristics are evaluated using simulation studies as well as multiple real-world experimental test-cases with both vehicle types.	aerial photography;algorithm;automated planning and scheduling;iteration;mathematical optimization;motion planning;point cloud;polygon mesh;programming paradigm;robotics;scalability;simulation;triangle mesh;triangulated irregular network;unmanned aerial vehicle	Andreas Bircher;Kostas Alexis;Michael Burri;Philipp Oettershagen;Sammy Omari;Thomas Mantel;Roland Siegwart	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7140101	planning;control engineering;computer vision;simulation;inspection;engineering	Robotics	53.40924100563423	-24.77903054599141	142515
39fd9d3ae7f36aa56f6a9383d0d5a8c37c7fa519	analysis on the performance of several data association algorithms using doppler information		Concerning the multi-target tracking problems of multiple-frequency CW radar system, several improved Bayes algorithms by using target radial velocity information are proposed in this paper. In order to clearly figure out the differences in the tracking performances, including the tracking precision and the real-time quality, of these improved and conventional algorithms, the detailed analysis on the performances of these algorithms in the same conditions are presented. First, the association method of each algorithm is described briefly. Then, the tracking model is established. Finally, the simulations obtained from a typical way of multi-target tracking are used, and the tracking performance of these algorithms is analyzed comprehensively in all aspects according to the simulation results.	algorithm;correspondence problem;performance;radar;radial (radio);real-time clock;simulation;velocity (software development)	Qiang Wei;Qihong Yang;Zhong Liu	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393377	doppler effect;radar tracker;machine learning;radar;artificial intelligence;probabilistic logic;clutter;computer science;bayes' theorem;algorithm;radial velocity	Robotics	49.7430842970833	-33.42493574913292	142926
1c63279c8643c0edb2af450d59b28f8dbdf65f1d	bit*: batch informed trees for optimal sampling-based planning via dynamic programming on implicit random geometric graphs		Path planning in continuous spaces has traditionally been divided between discrete and sampling-based techniques. Discrete techniques use the principles of dynamic programming to solve a discretized approximation of the problem, while sampling-based techniques use random samples to perform a stochastic search on the continuous state space. In this paper, we use the fact that stochastic planners can be viewed as a search of an implicit random geometric graph (RGG) to propose a general class of planners named Bellman random trees (BRTs) and derive an anytime optimal samplingbased algorithm, batch informed trees (BIT*). BIT* searches for a solution to the continuous planning problem by efficiently building and searching a series of implicit RGGs in a principled manner. In doing so, it strikes a balance between the advantages of discrete methods and sampling-based planners. By using the implicit RGG representation, defined as set of random samples and successor function, BIT* is able to scale more effectively to high dimensions than other optimal sampling-based planners. By using heuristics and intelligently reusing existing connections, like discrete lifelong planning algorithms, BIT* is able to focus its search in a principled and efficient manner. In simulations, we show that BIT* consistently outperforms optimal RRT (RRT*), informed RRT* and fast marching trees (FMT*) on random-world problems in R and R. We also present preliminary theoretical analysis demonstrating that BIT* is probabilistically complete and asymptotically optimal and conjecture that it may be optimally efficient under some probabilistic conditions.	anytime algorithm;approximation;asymptotically optimal algorithm;automated planning and scheduling;automation;converge;discretization;dynamic programming;embedded system;exploit (computer security);fast marching method;geometric graph theory;heuristic (computer science);icra;motion planning;ompl;random geometric graph;rapidly-exploring random tree;robotics;sampling (signal processing);shortest path problem;simulation;state space;stochastic optimization;successor function	Jonathan D. Gammell;Siddhartha S. Srinivasa;Tim D. Barfoot	2014	CoRR		mathematical optimization;discrete mathematics;artificial intelligence;theoretical computer science;mathematics;statistics	AI	51.64582171732076	-23.984262816710554	143264
99c40028ba02594924073043c6ec9d49c5851233	energy-efficient high-dimensional motion planning for humanoids using stochastic optimization	energy efficient path planning energy efficient high dimensional motion planning stochastic optimization humanoid robot high dimensional space energy consumption sampling based path planning algorithms rapidly exploring random tree rrt based algorithms complex path planning problems minimum cost path rrt cross entropy ce rrt tree asymptotic optimality;trees mathematics energy consumption humanoid robots optimal control optimisation path planning;trajectory humanoid robots energy consumption planning cost function;cost function;trajectory;humanoid robots;energy consumption;planning	This paper presents a method for planning a motion for a humanoid robot performing manipulation tasks in a high dimensional space such that the energy consumption of the robot is minimized. While sampling-based path planning algorithms, such as rapidly-exploring random tree (RRT) and its variants, have been highly effective for complex path planning problems, it is still difficult to find the minimum cost path in a high dimensional space since RRT-based algorithms extend a search tree locally, requiring a large number of samples to find a good solution. This paper presents an efficient nonmyopic motion planning algorithm for finding a minimum cost path by combining RRT* and cross entropy (CE). The proposed method constructs two RRT trees: the first tree is a standard RRT tree which is used to determine the nearest node in the tree to a randomly chosen point and the second tree contains the first tree with additional long extensions. By maintaining two separate trees, we can grow the search tree non-myopically to improve efficiency while ensuring the asymptotic optimality of RRT*. We first identify and demonstrate the limitation of RRT* when it is applied to energy-efficient path planning in a high dimensional space. Results from experiments show that the proposed method consistently achieves the lowest energy consumption against other algorithms.	asymptotically optimal algorithm;automated planning and scheduling;cross entropy;experiment;humanoid robot;mathematical optimization;motion planning;randomness;rapidly-exploring random tree;sampling (signal processing);search tree;simulation;stochastic optimization	Junghun Suh;Joonsig Gong;Songhwai Oh	2015	2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)	10.1109/HUMANOIDS.2015.7363418	planning;mathematical optimization;simulation;any-angle path planning;computer science;humanoid robot;artificial intelligence;trajectory;machine learning;mathematics	Robotics	52.08723703215427	-24.045854587699406	143700
f7d991e3ff5e31167329c6281a49241c0848f3c3	uncertainty based multi-robot cooperative triangulation		The paper presents a multi-robot cooperative framework to estimate the 3D position of dynamic targets, based on bearing-only vision measurements. The uncertainty of the observation provided by each robot equipped with a bearing-only vision system is effectively addressed for cooperative triangulation purposes by weighing the contribution of each monocular bearing ray in a probabilistic manner. The envisioned framework is evaluated in an outdoor scenario with a team of heterogeneous robots composed of an Unmanned Ground and Aerial Vehicle.	aerial photography;epipolar geometry;pixel;robot;sensor;unmanned aerial vehicle	André Dias;José Miguel Almeida;Pedro U. Lima;Eduardo P. da Silva	2014		10.1007/978-3-319-18615-3_41	artificial intelligence;simulation;computer vision;computer science;probabilistic logic;robot;machine vision;unmanned ground vehicle;monocular;bearing (mechanical);triangulation (social science)	Robotics	53.22166972020857	-35.742086120768136	143811
a68cb5d7cc2c3fc105aa5cc45c9c653d4eeeaf5c	real-time implementation of harris corner detection system based on fpga		With the development of the manufacturing technique of the microchip, the integral performance of the Field-Programmable Gate Array (FPGA) is becoming more and more adaptive for real-time embedded systems. Interest point detection is a prime step in many advanced computer vision applications such as 3-D reconstruction, 3-D object tracking and so on. Many image processing algorithms need to extract interest point in advance. We can easily implement these algorithms in software, but it can't meet with the request of a real-time embedded systems because of its low speed. In this paper, we achieved a hardware implementation of the Harris corner feature detector based on Zedboard platform which is a heterogeneous SoC composed of two ARM9 chips and a Xilinx xc7z020-clg484-1 chip. The hardware architecture is implemented with Verilog HDL and consists of 6 modules: CMOS register initialization, CMOS data fetcher, CMOS data graying, block RAM frame buffer, Harris Corner Detector(HCD), HDMI displayer. The implementation can process RGB565 video in 640×480 resolution. According to the simulation results, the hardware implementation can extract Harris corner at the speed of 154 frames per second. It can meet most requirements for real-time computer vision applications.	algorithm;apache nutch;cmos;computer vision;corner detection;display resolution;embedded system;field-programmable gate array;framebuffer;hdmi;hardware description language;harris affine region detector;high color;image processing;integrated circuit;interest point detection;random-access memory;real-time clock;real-time computing;real-time transcription;requirement;simulation;system on a chip;verilog	Shaohui Liu;Congyi Lyu;Yunhui Liu;Weihua Zhou;Xin Jiang;Peng Li;Haoyao Chen;Yuanyuan Li	2017	2017 IEEE International Conference on Real-time Computing and Robotics (RCAR)	10.1109/RCAR.2017.8311884	field-programmable gate array;digital image processing;parallel computing;verilog;interest point detection;corner detection;hardware architecture;video tracking;gate array;computer science	Robotics	44.20812489689776	-35.10060303738586	143820
0e19a62d03849ae9eb385416e6fbde63b90098c6	efficient scan-window based object detection using gpgpu	detectors;histograms;paper;cpu gpu data transfers efficient scan window based object detection gpgpu general purpose graphics hardware computing pedestrian detector oriented gradient features histogram support vector machine classifiers feature extraction;support vector machines;efficient scan window based object detection;traffic engineering computing feature extraction object detection support vector machines;object detection detectors graphics hardware support vector machines support vector machine classification rendering computer graphics histograms feature extraction intelligent robots;histograms of oriented gradients;computer vision;arrays;gpgpu;graphics hardware;feature extraction;pixel;detection rate;nvidia;nvidia quadro fx 3500;cpu gpu data transfers;traffic engineering computing;general purpose graphics hardware computing;computer science;support vector machine;support vector machine classifiers;pedestrian detector;oriented gradient features histogram;graphics;data transfer;object detection	We describe an efficient design for scan-window based object detectors using a general purpose graphics hardware computing (GPGPU) framework. While the design is particularly applied to built a pedestrian detector that uses histogram of oriented gradient (HOG) features and the support vector machine (SVM) classifiers, the methodology we use is generic and can be applied to other objects, using different features and classifiers. The GPGPU paradigm is utilized for feature extraction and classification, so that the scan windows can be processed in parallel. We further propose to precompute and cache all the histograms in advance, instead of using integral images, which greatly lowers the computation cost. A multi-scale reduce strategy is employed to save expensive CPU-GPU data transfers. Experimental results show that our implementation achieves a more-than-ten-times speed up with no loss on detection rates.	adaboost;central processing unit;computation;computer vision;face detection;feature extraction;general-purpose computing on graphics processing units;gradient;graphics hardware;graphics processing unit;microsoft windows;object detection;programming paradigm;sensor;speedup;support vector machine	Xiang Lin;Ramakant Nevatia	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4563097	support vector machine;computer vision;computer science;theoretical computer science;machine learning;computer graphics (images)	Vision	43.25302431397521	-36.65474810814403	143838
88dd9b38020915927b3c55bcae71a54bc7ae7847	computational design of complex materials using information theory: from physics- to data-driven multi-scale molecular models			computation;information theory	Vagelis Harmandaris;Evangelia Kalligiannaki;Markos A. Katsoulakis	2018	ERCIM News			ML	42.94198353960711	-25.947646624403777	144084
95ac4b75ef1cd300baff6dc979d469bacefb867f	a constraint-based approach to crowd simulation and layout synthesis		Author(s): Weiss, Tomer | Advisor(s): Terzopoulos, Demetri | Abstract: Position-based methods have become popular for real-timesimulation in computer graphics. In contrast to traditional simulationmethods, which are based on Newtonian dynamics, particularly forces, aPosition-Based Dynamics (PBD) method computes the positional changesdirectly, based on a set of well-defined geometric constraints.Therefore, position-based methods are reputed to be more controllable,stable, and faster, which make them well-suited for use in interactiveenvironments. This thesis introduces position-based approaches toaddressing the important tasks of virtual crowd simulation and virtuallayout synthesis.For crowd simulation, we introduce a novel method that runs atinteractive rates for up to hundreds of thousands of agents. Ourmethod enables the detailed modeling of per-agent behavior in aLagrangian formulation. We model short-range and long-range collisionavoidance to simulate both sparse and dense crowds. On the particlesrepresenting agents, we formulate a set of positional constraints thatcan be readily integrated into a standard PBD solver. We augment thetentative particle motions with planning velocities to determine thepreferred velocities of agents, and project the positions onto theconstraint manifold to eliminate colliding configurations. The localshort-range interaction is represented with collision and frictionalcontact between agents, as in the discrete simulation of granularmaterials. We incorporate a cohesion model for simulating collectivebehaviors and propose a new constraint for dealing with potentialfuture collisions. Our method is suitable for use in interactivegames.For layout synthesis, we propose a position-based interior layoutsynthesis method that is able to rapidly synthesize large scalelayouts that were previously intractable. An interior layout modelingtask can be challenging for non-experts, hence the existence ofinterior design professionals. Recent research into the automation ofthis task has yielded methods that can synthesize layouts of objectsrespecting aesthetic and functional constraints that are non-linearand competing. These methods usually adopt a purely stochastic scheme,which samples from a distribution of layout configurations, a processthat is slow and inefficient. We introduce an alternativephysics-based, continuous layout synthesis technique, which results ina significant gain in speed and is readily scalable. We demonstrateour method on a diverse set of examples and show that it achievesresults similar to conventional layout synthesis based on a Markovchain Monte Carlo (McMC) state-search step, but is faster by at leastan order of magnitude and can handle layouts of unprecedented size andtight layouts that can overwhelm McMC.	crowd simulation	Tomer Weiss	2018			newtonian dynamics;discrete event simulation;mathematical optimization;scalability;monte carlo method;markov chain monte carlo;crowd simulation;computer graphics;computer science;solver	EDA	46.876604798840084	-28.005703332593768	144523
d32e53f5b27aa72f7a445e7abbc5878997d30dff	the attentive co-pilot: towards a proactive biologically-inspired advanced driver assistance system	brain;advanced driver assistance system;central behavior control module attentive copilot proactive biologically inspired advanced driver assistance system multiple sensor fusion integrated systems human brain signal processing internal information fusion local control loops human cognition aspects;detection and identification;attention;human characteristics;object tracking;cognition;human factors machine vision vehicles roads visualization control systems;sensor fusion cognition driver information systems;sensor fusion;behavior;driver information systems;modules;driver support systems	Driver assistance functionalities on the market are getting more and more sophisticated, which will lead to integrated systems that fuse the data of multiple sensors (e.g., camera, Photonic Mixer Device, Radar) and internal system percepts (e.g., detected objects and their states, detected road). One important future challenge will be to find smart solutions in system design that allow an efficient control of said integrated systems. A promising way for achieving this is to get inspiration from known signal-processing principles in the human brain. The contribution presents a biologically motivated Advanced Driver Assistance System (ADAS) that uses the generic principle of attention as common front-end of all visual processes. Based on the attention principle an early task-dependent pre-selection of interesting image regions is done, which decreases the scene complexity. Furthermore, internal information fusion increases the system performance (e.g., the attention is used to improve the object tracking, road-detection results can improve the attention). Based on streams of a challenging traffic scenario it is shown, how the system builds up and verifies its environment-related expectations relying on the attention principle. The ADAS is controlled by a central behavior control module that tunes sub-modules and parameters. The behavior control module has a simple structure, but still allows for robustly performing various tasks, since the complexity is distributed over the system in form of local control loops mimicking human cognition aspects.	architecture design and assessment system;artificial intelligence;cognition;complex system;control theory;control unit;diploma;electrical engineering;feature vector;image processing;industrial engineering;information processing;radar;real-time clock;reinforcement learning;robotics;sensor;signal processing;system integration;systems design;time-of-flight camera	Thomas Michalke;Robert Kastner	2009	2009 IEEE Intelligent Vehicles Symposium	10.1109/MITS.2011.941911	embedded system;computer vision;simulation;cognition;attention;computer science;engineering;sensor fusion;behavior	Robotics	47.640963131671555	-33.162370321592086	144747
06e60a9310428f6009f755db315eefb022657bc4	realization of autonomous navigation in multirobot environment	navigation system autonomous navigation multirobot environment local communication cooperative tasks position alignment collision avoidance self localization;navigation collision avoidance robot sensing systems infrared sensors chemicals adaptive systems intelligent robots intelligent sensors environmental management costs;mobile robots;collision avoidance multi robot systems mobile robots computerised navigation;local community;multi robot systems;autonomous navigation;navigation system;collision avoidance;autonomous robot;computerised navigation	In this paper, we discuss autonomous navigation in multirobot environment based o n locc;d communication. To carry out cooperative tasks using multiple autonomous robots, navigation systems should be implemented t o the robots because mutual alignment of their positions and collision avoidance to each other become important. Therefore, it seems a collision avoidance and a self-localization are indispensable as elemental techniques for the navigation. Althomgh these techniques need to recognize situation of robots surrounding environment, conventional devices to acquire information around them can not work suficiently in such environment. For this problem, we propose these techniques based o n the local communication and the navigation system by combining them for multirobot environment. It is shown that robots c m arrive their goal precisely without collision among them through proposed navigation systems.	autonomous robot;elemental	Yoshikazu Arai;Teruo Fujii;Hajime Asama;Hayato Kaetsu;Isao Endo	1998		10.1109/IROS.1998.724904	control engineering;mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;mobile robot navigation	Robotics	53.24776282718418	-33.851217128984835	144804
155033f2f096934042d659d10912ef29aa1cdbd1	visual classification of coarse vehicle orientation using histogram of oriented gradients features	orientation independent classifier;detectors;histograms;autonomous vehicle tracking;image recognition;coarse vehicle orientation;performance evaluation;image databases;autonomous vehicle;radar tracking;visual classification;histograms vehicle detection remotely operated vehicles mobile robots hazards image recognition performance evaluation testing image databases detectors;orientation specific classifiers;training;hazards;vehicle detection;histogram of oriented gradients autonomous vehicle tracking visual classification coarse vehicle orientation vehicle detection orientation specific classifiers orientation independent classifier;image classification;mobile robots;histograms of oriented gradients;testing;remotely operated vehicles;histogram of oriented gradients;feature extraction;driver circuits;traffic engineering computing;vehicles;classification accuracy;tracking;object detection;vehicles feature extraction image classification object detection tracking traffic engineering computing	For an autonomous vehicle, detecting and tracking other vehicles is a critical task. Determining the orientation of a detected vehicle is necessary for assessing whether the vehicle is a potential hazard. If a detected vehicle is moving, the orientation can be inferred from its trajectory, but if the vehicle is stationary, the orientation must be determined directly. In this paper, we focus on vision-based algorithms for determining vehicle orientation of vehicles in images. We train a set of Histogram of Oriented Gradients (HOG) classifiers to recognize different orientations of vehicles detected in imagery. We find that these orientation-specific classifiers perform well, achieving a 88% classification accuracy on a test database of 284 images. We also investigate how combinations of orientation-specific classifiers can be employed to distinguish subsets of orientations, such as driver's side versus passenger's side views. Finally, we compare a vehicle detector formed from orientation-specific classifiers to an orientation-independent classifier and find that, counter-intuitively, the orientation-independent classifier outperforms the set of orientation-specific classifiers.	algorithm;autonomous robot;hazard (computer architecture);histogram of oriented gradients;image gradient;sensor;stationary process	Paul E. Rybski;Daniel Huber;Daniel D. Morris;Regis Hoffman	2010	2010 IEEE Intelligent Vehicles Symposium	10.1109/IVS.2010.5547996	random subspace method;computer vision;simulation;engineering;pattern recognition	Robotics	48.44628721688211	-36.57058787726484	145370
f51a3852795074d86f480f93208f54e95af8e723	fpga implementation of vision-based fingertip-writing digits recognition system	cmos integrated circuits;image color analysis feature extraction cmos integrated circuits skin field programmable gate arrays trajectory image recognition;image recognition;skin;verilog code fpga implementation vision based fingertip writing digits recognition system vision based fingertip writing digits detection system cmos camera real time signature detector image process algorithms;trajectory;image color analysis;feature extraction;gesture detection fpga cmos sensor;field programmable gate arrays;object recognition cmos image sensors computer vision field programmable gate arrays gesture recognition handwriting recognition hardware description languages	This paper presents a vision-based fingertip writing digits detection and recognition system using a CMOS camera and FPGA implementation. It is a real-time signature detector, since the image process algorithms are all executed in Verilog code. The experimental results show that the system can successfully recognize fingertip-numeral-writing with a accuracy rate of 95.8%.	algorithm;cmos;computation;field-programmable gate array;hard coding;preprocessor;real-time clock;robot;system image;time complexity;verilog	Ching-Long Shih;Bing-Lin Yang;Wen-Yo Lee;Ti-Hung Chen;Bo-Jhih Chen;Yi-Chih Fan	2015	2015 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2015.98	embedded system;computer vision;feature extraction;computer science;trajectory;skin;cmos;field-programmable gate array	Robotics	44.64863364903277	-35.20990464173874	146071
8528161e0c40266c48e80369ab8e186f96c15902	urban environment perception and navigation using robotic vision : conception and implementation applied to automous vehicle. (perception de l'environnement urbain et navigation s'appuyant sur la vision robotique : la conception et la mise en oeuvre appliquée au véhicule autonome)		The development of autonomous vehicles capable of getting around on urban roads can provide important benefits in reducing accidents, in increasing life comfort and also in providing cost savings. Intelligent vehicles for example often base their decisions on observations obtained from various sensors such as LIDAR, GPS and Cameras. Actually, camera sensors have been receiving large attention due to they are cheap, easy to employ and provide rich data information. Inner-city environments represent an interesting but also very challenging scenario in this context, where the road layout may be very complex, the presence of objects such as trees, bicycles, cars might generate partial observations and also these observations are often noisy or even missing due to heavy occlusions. Thus, perception process by nature needs to be able to deal with uncertainties in the knowledge of the world around the car. While highway navigation and autonomous driving using a prior knowledge of the environment have been demonstrating successfully, understanding and navigating general inner-city scenarios with little prior knowledge remains an unsolved problem. In this thesis, this perception problem is analyzed for driving in the inner-city environments associated with the capacity to perform a safe displacement based on decision-making process in autonomous navigation. It is designed a perception system that allows robotic-cars to drive autonomously on roads, without the need to adapt the infrastructure, without requiring previous knowledge of the environment and considering the presence of dynamic objects such as cars. It is proposed a novel method based on machine learning to extract the semantic context using a pair of stereo images, which is merged in an evidential grid to model the uncertainties of an unknown urban environment, applying the Dempster-Shafer theory. To make decisions in path-planning, it is applied the virtual tentacle approach to generate possible paths starting from ego-referenced car and based on it, two news strategies are proposed. First one, a new strategy to select the correct path to better avoid obstacles and to follow the local task in the context of hybrid navigation, and second, a new closed loop control based on visual odometry and virtual tentacle is modeled to path-following execution. Finally, a complete automotive system integrating the perception, path-planning and control modules are implemented and experimentally validated in real situations using an experimental autonomous car, where the results show that the developed approach successfully performs a safe local navigation based on camera sensors.		Giovani Bernardes Vitor	2014				Robotics	52.6046054590179	-31.761427529878148	146116
926fabf3f2475425d776b467dc7074d751f7d465	distributed planning of multi-segment soft robotic arms		This paper presents a distributed kinematic planning algorithm for the end-effector of a multi-segment soft robotic arm to reach the goal position in both 2D and 3D spaces. The planning algorithm runs sequentially from the proximal to the distal segments. For each segment, the planning algorithm only requires the information about the position of itself, the end-effector, and the goal. The 2D planning of each segment is parameterized by a bending angle and an inflation ratio, which are determined by checking the overall geometry of the residual arm and the goal position. The same concept is extended to 3D planning, where parameters include inflation ratio, azimuth angle, and elevation angle for each segment. It is demonstrated in this paper that physical limits of the arm and challenging goal position could lead to failure in goal reaching. To account for this, an iterative learning function is proposed, which allows each segment to learn from past trials and be proactive for goal reaching. The proposed distributed planning algorithm, with iterative learning, demonstrates strong scalability, low computation cost, and robust goal reaching performance. Moreover, it does not need training data to pre-learn the configuration of the arm, which makes the algorithm highly applicable to a wide range of multi-segment soft and continuum robots. Both 2D and 3D simulation results are provided to illustrate the efficacy of the proposed algorithm.	2d computer graphics;algorithm;apache continuum;automated planning and scheduling;coat of arms;computation;experiment;iterative method;obstacle avoidance;robot end effector;robotic arm;scalability;simulation	Wenlong Zhang;Panagiotis Polygerinos	2018	2018 Annual American Control Conference (ACC)	10.23919/ACC.2018.8430682	azimuth;iterative learning control;residual;computer science;soft robotics;scalability;computation;kinematics;control theory;robot;algorithm	Robotics	51.14902833878967	-27.01767292697309	146338
75c265460b6bdcaea61b2f82bda27a9cce602443	parallel mechanism for detecting contours in binary images	binary image;parallel mechanism;hardware architecture	We present a parallel mechanism for detecting contours embedded in a binary image. The proposed algorithm can detect contours in parallel in sequential machines with less computational effort. The hardware architecture of the algorithm is also proposed. Experiments with a wide variety of binary images show that the speed of this new technique is much faster than that of other contour detection methods.	binary image;sensor	Kuo-Chin Fan;Chin-Chuan Han	1994	J. Electronic Imaging	10.1117/12.163971	computer vision;binary image;computer science;theoretical computer science;hardware architecture;computer graphics (images)	Vision	44.67517469231014	-34.84479603738381	146515
3490548236a92e7b233506bd0b63792a18c46da9	implementation of a foveal vision mapping	frames per second;mirrors;spatial resolution image resolution maintenance engineering data engineering computer vision hardware computer architecture sensor systems delay streaming media;high resolution;continuous variable;hardware architecture;computer vision;foveal vision mapping;frequency 104 mhz foveal vision mapping computer vision algorithms forward mapping fpga resources variable spatial resolution field programmable gate arrays;frequency 104 mhz;efficient implementation;streaming media;computer vision algorithms;pixel;variable spatial resolution;fpga resources;field programmable gate arrays;field programmable gate arrays computer vision;forward mapping;cameras;spatial resolution	Foveal vision reduces the data volume by utilising a spatially variant resolution. A high resolution is maintained in the fovea where it can be used by computer vision algorithms, while the resolution is reduced in the periphery where it is less important. This work focuses on the hardware architecture of a system that maps a conventional high resolution uniformly sampled sensor to a variable resolution output. The key feature of the proposed architecture is that it employs a separable forward mapping which requires a small amount of FPGA resources. This enables an efficient implementation of a continuously variable spatial resolution, requiring only 1000 LUTs on a Virtex-5, and runs at 104 MHz, enabling the mapping of a 512×512 window at over 300 frames per second.		Donald G. Bailey;Christos-Savvas Bouganis	2009	2009 International Conference on Field-Programmable Technology	10.1109/FPT.2009.5377646	computer vision;image resolution;computer hardware;computer science;hardware architecture;computer graphics (images)	Robotics	44.838421320915835	-33.33401931288019	146905
6ab14fe4c7e24730aa5d4070652e374a5d21ccdd	real time architectures for moving-objects tracking	moving object;real time;optical flow estimation;video tracking;motion estimation;object segmentation;object tracking;colour segmentation;scientific communication;real time image processing;high speed	The problem of object tracking is of considerable interest in the scientific community and it is still an open and active field of research. In this paper we address the comparison of two different specific purpose architectures for object tracking based on motion and colour segmentation. On one hand, we have developed a new multi-object segmentation device based on an existing optical flow estimation system. This architecture allows video tracking of fast moving objects based on high speed acquisition cameras. On the other hand, the second approach consists on real time filtering of chromatic components. Multiobject tracking is performed based on segmentation of pixel neighbourhoods according to a predefined colour. In this contribution we evaluate the two methods, comparing their performance, resource consumption and finally, we discuss which architecture fits better in different working scenarios.	fits;optical flow;pixel;video tracking	Matteo Tomasi;Javier Díaz;Eduardo Ros Vidal	2007		10.1007/978-3-540-71431-6_35	computer vision;simulation;tracking system;computer science;segmentation-based object categorization;video tracking;scale-space segmentation;computer graphics (images)	Vision	44.634792194460125	-37.655157113115415	147005
dcfebe0e47176c58ecc6d7125e0310d0866206b7	efficient centralized track initiation method for multistatic radar	radar tracking monte carlo methods radar signal processing;same source association multistatic radar track initiation false alarm measuring deviation;radar tracking target tracking receivers position measurement multistatic radar radar cross sections;monte carlo simulations centralized track initiation method multistatic radar system same source association technique measurement fusion approach false alarm rate environment	An efficient centralized track initiation method for multistatic radar system is proposed in this paper. The method mainly consists of a same source association technique and a measurement fusion approach. The main advantage of the new method is its good performance of dealing with large measuring deviation and high false alarm rate environment, when compared with other existing track initiation methods. And this is verified by the Monte Carlo simulations.	centralized computing;monte carlo method;radar;simulation;velocity (software development)	Shiyou Xu;Chaojing Tang;Peiliang Jing;Zengping Chen	2014	17th International Conference on Information Fusion (FUSION)		early-warning radar;man-portable radar;computer vision;continuous-wave radar;electronic engineering;radar tracker;radar engineering details;radar lock-on;radar configurations and types;geography;fire-control radar;passive radar;bistatic radar;low probability of intercept radar;pulse-doppler radar;3d radar;radar imaging;remote sensing	Robotics	50.23369064081214	-33.43744559336369	147040
f330cd5b2e0367b9e1a1065058254373d78e4379	an fpga-based architecture for real time image feature extraction	fpga-based architecture;gray level cooccurrence matrix;hardware module;software module;realtime pattern recognition application;feature extraction;real time image;glcm integer feature;novel fpga-based architecture;inverse difference moment;xilinx virtex-e v2000 fpga;proposed architecture;complementary computation;pattern recognition;field programmable gate arrays;image texture;real time systems;hardware description languages;feature vector;image features	We propose a novel FPGA-based architecture for the extraction of four texture features using gray level cooccurrence matrix (GLCM) analysis. These features are angular second moment, correlation, inverse difference moment, and entropy. The proposed architecture consists of a hardware and a software module. The hardware module is implemented on Xilinx Virtex-E V2000 FPGA using VHDL. It calculates many GLCMs and GLCM integer features in parallel. The software retrieves the feature vectors calculated in hardware, and performs complementary computations. The architecture was evaluated using standard grayscale images and video clips. The results show that it can be efficiently used in realtime pattern recognition applications.	angularjs;computation;computer hardware;entropy (information theory);feature (computer vision);feature extraction;feature vector;field-programmable gate array;grayscale;image resolution;modular programming;pattern recognition;real-time computing;vhdl;video clip;virtex (fpga);window function	Dimitris G. Bariamis;Dimitrios K. Iakovidis;Dimitrios E. Maroulis;Stavros A. Karkanis	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334338	image texture;computer vision;feature vector;feature extraction;computer science;theoretical computer science;machine learning;hardware description language;feature;field-programmable gate array;computer graphics (images)	Robotics	43.68909053734237	-35.20845987456159	147161
002a39086433ecdd5829687c06204a4f48c2a7ed	eyes in the back of your head: robust visual teach &amp; repeat using multiple stereo cameras	stereo image processing feature extraction image sensors path planning robot vision;field robotics;localization;cameras robot vision systems navigation lighting pipelines feature extraction;robot field of view robust visual teach multiple stereo cameras autonomous path following robots vision based navigation matching point visual features outdoor environments seasonal variation path following system localization pipeline;computer vision;long term autonomy;navigation;feature extraction;pipelines;autonomous path following;lighting;robot vision systems;cameras;autonomous path following computer vision field robotics localization long term autonomy	Autonomous path-following robots that use vision-based navigation are appealing for a wide variety of tedious and dangerous applications. However, a reliance on matching point-based visual features often renders vision-based navigation unreliable over extended periods of time in unstructured, outdoor environments. Specifically, scene change caused by lighting, weather, and seasonal variation lead to changes in visual features and result in a reduction of feature associations across time. This paper presents an autonomous, path-following system that uses multiple stereo cameras to increase the algorithm field of view and reliably navigate in these feature-limited scenarios. The addition of a second camera in the localization pipeline greatly increases the probability that a stable feature will be in the robot's field of view at any point in time, extending the amount of time the robot can reliably navigate. We experimentally validate our algorithm through a challenging winter field trial, where the robot autonomously traverses a 250m path six times with an autonomy rate of 100% despite significant changes in the appearance of the scene due to lighting and melting snow. We show that the addition of a second stereo camera to the system significantly increases the autonomy window when compared to current state-of-the-art path-following methods.	algorithm;algorithmic efficiency;ampersand;autonomous robot;experiment;rendering (computer graphics);robotics;seasonality;stereo camera;stereo cameras	Michael Paton;François Pomerleau;Tim D. Barfoot	2015	2015 12th Conference on Computer and Robot Vision	10.1109/CRV.2015.16	computer stereo vision;stereo cameras;computer vision;navigation;simulation;internationalization and localization;feature extraction;computer science;lighting;pipeline transport;mobile robot navigation;computer graphics (images)	Robotics	49.4882209713529	-37.680890973323926	147661
1b0999414b3dc4abc8e547b0c197318252122143	unsupervised learning of qualitative motion behaviours by a mobile robot	unsupervised learning;qualitative spatial representations;mobile robotics;machine learning for robotics	The success of mobile robots, in daily living environments, depends on their capabilities to understand human movements and interact in a safe manner. This paper presents a novel unsupervised qualitative-relational framework for learning human motion patterns using a single mobile robot platform. It is capable of learning human motion patterns in real-world environments, in order to predict future behaviours. This previously untackled task is challenging because of the limited field of view provided by a single mobile robot. It is only able to observe one location at any time, resulting in incomplete and partial human detections and trajectories. Central to the success of the presented framework is mapping the detections into an abstract qualitative space, and then characterising motion invariant to exact metric position. This framework was used by a physical robot autonomously patrolling an office environment during a six week deployment. Experimental results from this deployment demonstrate the effectiveness and applicability of the system.	algorithm;high-level programming language;k-means clustering;kinesiology;mobile robot;object type (object-oriented programming);robot control;sensor;software deployment;test data;unsupervised learning	Paul Duckworth;Yiannis Gatsoulis;Ferdian Jovan;Nick Hawes;David C. Hogg;Anthony G. Cohn	2016			unsupervised learning;robot learning;computer vision;simulation;computer science;artificial intelligence;machine learning	Robotics	47.78559497750803	-37.79413460624857	148127
59166e236d44bc7aab8f4841143c619123f361df	neutrosophic hough transform-based track initiation method for multiple target tracking		A neutrosophic Hough transform-based track initiation method (NHT-TI) is proposed to solve the uncertain track initiation problem in a complex surveillance environment. In the proposed method, a neutrosophic set is employed to describe the uncertain association of a measurement with different targets, which is divided into three categories, including the association with real targets, uncertain targets, and false targets, respectively. On this basis, the neutrosophic Hough transform (NHT) method is developed in the framework of the standard Hough transform (HT) method. Based on the sequential processing mode of the sensors, candidate temporary tracks are further defined to directly calculate the parameter points in the parameter space, which are utilized to calculate the contribution for the corresponding vote cells in the accumulation matrix by using the Gaussian membership function. Based on the scheme, the NHT method can avoid large traversal operations and reduce computation complexity of the HT method. Moreover, considering the effects of noises and clutters on track initiation, two constraint conditions related to the velocity information of moving targets and the time information of measurement sequences are introduced to suppress false tracks and reduce vote times. Finally, the real tracks can be determined by detecting the peaks of the global accumulation matrix. The performance of the proposed NHT-TI method is evaluated by using two experiments with simulated data and real data in two complex surveillance environments. The results are found to be better than those of the standard HT-based track initiation (HT-TI) method, the modified HT-TI method based on candidate temporary tracks and the improved HT-TI method in detection reliability and computational complexity.		En Fan;Weixin Xie;Jihong Pei;Keli Hu;Xiaobin Li	2018	IEEE Access	10.1109/ACCESS.2018.2814827	measurement uncertainty;parameter space;computational complexity theory;tree traversal;distributed computing;gaussian;computer science;matrix (mathematics);membership function;artificial intelligence;pattern recognition;hough transform	Vision	48.8102303808872	-33.64249386478931	148164
e224b4f6692df47c1e247aaadafc8707a3407c03	an integrated fibered confocal microscopy system for in vivo and in situ fluorescence imaging - applications to endoscopy in small animal imaging	data acquisition system;software performance;fluorescence;field of view;laser scanning;real time;optical microscopy;microscopy;data acquisition;optical imaging;fluorescence imaging;fluorescence microscopy;in vivo;confocal microscopy;real time systems;image processing;image resolution;real time control	This paper presents a novel fibered confocal fluorescence microscopy system (FCFM) specifically designed for the observation of biological tissues in vivo and in situ, in real time, at the cellular level: the Cell-viZio. The Cell-viZio is made of three main components that are described in this paper: i) FibroScan: an opto-electronic unit controlling a laser scanning and data acquisition system; ii) ProFlex: a set of flexible miniaturized optical probes allowing in situ imaging. iii) ImageCell: a dedicated software performing real-time control and image processing. The Cell-viZio provides images with typical characteristics (varying with the optical probe) as follows: image lateral resolution: 2.5 microns; axial resolution: 20 microns; field of view: 160 /spl times/ 120 microns; optical imaging depth: 80 microns (deeper in transparent tissue); data acquisition frame rate: 12 Hz. Thanks to the miniaturization of flexible optical probes (/spl Phi/: down to 350 /spl mu/m), unprecedented accessibility is made possible. In vivo in situ images of rat bladder and mouse colon obtained endoscopically are presented here for the first time.	accessibility;cell signaling;colon classification;data acquisition;image processing;lateral thinking;preclinical imaging;real-time transcription;video-in video-out	Aymeric Perchant;Georges Le Goualher;Magalie Genet;Bertrand Viellerobe;Frederic Berier	2004	2004 2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro (IEEE Cat No. 04EX821)		pathology;image processing;microscopy;optics;data acquisition	Embedded	44.653824629959566	-33.10015435706806	148186
58cc29de371d5086d26e7813e00e03875d59ac56	touchless hand gesture ui with instantaneous responses	spatiotemporal phenomena correlation methods gesture recognition image motion analysis image sensors mobile computing object detection object tracking;image motion analysis;accurate hand swipe motion detection touchless hand gesture ui instantaneous responses real time touchless hand gesture user interface mobile devices biologically inspired vision sensor dynamic vision sensor dvs moving object detection object edges output events spatiotemporal correlation event driven processing algorithms integrate and fire neurons graphic ui capable finger tip tracking;correlation methods;image sensors;neurons voltage control thumb tracking radio frequency computational efficiency;object tracking;spatiotemporal phenomena;neuromorphic processing gesture recognition user interface;mobile computing;gesture recognition;object detection	In this paper we present a simple technique for real-time touchless hand gesture user interface (UI) for mobile devices based on a biologically inspired vision sensor, the dynamic vision sensor (DVS). The DVS can detect a moving object in a fast and cost effective way by outputting events asynchronously on edges of the object. The output events are spatiotemporally correlated by using novel event-driven processing algorithms based on leaky integrate-and-fire neurons to track a finger tip or to infer directions of hand swipe motions. The experimental results show that the proposed technique can achieve graphic UI capable finger tip tracking with milliseconds intervals and accurate hand swipe motion detection with negligible latency.	algorithm;biological neuron model;dynamic voltage scaling;event-driven programming;gesture recognition;mobile device;real-time clock;user interface	Junhaeng Lee;Paul K. J. Park;Chang-Woo Shin;Hyunsurk Ryu;Byung-Chang Kang;Tobi Delbrück	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467270	computer vision;computer science;video tracking;image sensor;gesture recognition;mobile computing;computer graphics (images)	Robotics	45.099133700858744	-37.462850145216564	148392
2a256d63212d3c6b1cd95ec51090ae9fa89a1789	architecture for a low power image sensor with motion detection based roi	motion estimation cmos image sensors;high resolution;image resolution;embedded steady camera;low power image sensor architecture;recursion operator;power efficiency;low resolution;motion estimation;embedded system;cmos image sensors;computer architecture;embedded systems;artificial neural networks;cmos image sensor;image sensors motion detection spatial resolution energy consumption computational intelligence society image resolution robustness cmos image sensors layout image converters;low power;robust motion detection algorithms;robustness;embedded steady camera low power image sensor architecture embedded systems power consumption cmos image sensor robust motion detection algorithms;power consumption;power demand;motion detection;algorithm design and analysis;image sensor;spatial resolution	Visual tasks for embedded systems are confronted with strong constraints on power consumption. Algorithms have to be both robust and compliant to various environments while being computationally and power efficient. An architecture of CMOS image sensor tailored to implement a class of robust motion detection algorithms based on recursive operations, allowing sensor's acuity adaptation to the scene activity, is exposed. The main interest is that in a context of embedded steady camera, such a system allows to focus on targets with high resolution while keeping background in low resolution. Drastic power consumption reduction is achieved by tremendously reducing the amount of processed data.	algorithm;analog-to-digital converter;application-specific integrated circuit;embedded system;high- and low-level;image resolution;image sensor;motion detector;recursion;region of interest;simulation;systemc	Arnaud Verdant;Patrick Villard;Antoine Dupret;Hervé Mathias	2007	2007 14th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2007.4511167	embedded system;computer vision;electronic engineering;computer science	Robotics	44.11405610170762	-34.65201818771567	148432
a98a0a36fd2ae5324107b7de74010724e3210f22	circumventing robots' failures by embracing their faults: a practical approach to planning for autonomous construction	planning under uncertainty;fault tolerance;autonomous construction mobile robots	This paper overviews our application of state-of-the-art automated planning algorithms to real mobile robots performing an autonomous construction task, a domain in which robots are prone to faults. We describe how embracing these faults leads to better representations and smarter planning, allowing robots with limited precision to avoid catastrophic failures and succeed in intricate constructions.	algorithm;automated planning and scheduling;autonomous robot;failure;mobile robot	Stefan J. Witwicki;Francesco Mondada	2015			fault tolerance;simulation;computer science;artificial intelligence	Robotics	50.265692650777225	-24.832706853585883	148657
c714a3ca931d47068293ab061decbbb3ed73e66e	programmable 2d image filter for aer vision processing	vision system;image filtering;convolution;real time;comunicacion de congreso;computer vision;two dimensional digital filters;address event representation;boundary contour system;address event representation system programmable 2d image filter aer vision processing vlsi architecture real time 2d image filtering convolutional kernel rotated coordinate system signed minimum operation vision system boundary contour system feature contour system bcs fcs model;convolution two dimensional digital filters computer vision vlsi real time systems;vlsi;filters neurons machine vision space vector pulse width modulation eprom computer architecture filtering kernel humans electronic mail;coordinate system;real time systems;vlsi architecture	A VLSI architecture is proposed for the realization of real-time 2D image filtering in an address-event-representation (AER) vision system, The architecture is capable of implementing any convolutional kernel F(x, y) as long as it is decomposable into x-axis and y-axis components, i.e, F(x, y)=H(x)V(y), for some rotated coordinate system {x, y}, and if this product can be approximated safely by a signed minimum operation. The proposed architecture is intended to be used in a complete vision system, known as the boundary-contour-system and feature-contour-system (BCS-FCS) vision model.	composite image filter	Teresa Serrano-Gotarredona;Andreas G. Andreou;Bernab&#x00E9; Linares-Barranco	1999		10.1109/ISCAS.1999.779966	embedded system;computer vision;electronic engineering;machine vision;computer science;theoretical computer science;coordinate system;very-large-scale integration;convolution	Vision	44.83683071935323	-34.410116567485304	148922
a9f996cbbe181d4fbf21ce2d93d18b40873ead0c	a new automatic gait cycle partitioning method and its application to human identification				Sungjun Hong;Euntai Kim	2017	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2017.17.2.51	gait;computer vision;computer science;artificial intelligence	AI	47.21286323068239	-30.53435352053917	149311
8403b7f82e8f9119a36e564e42cc4d939fc9711f	dynamic terrain traversal skills using reinforcement learning	physics simulation;computer animation	The locomotion skills developed for physics-based characters most often target flat terrain. However, much of their potential lies with the creation of dynamic, momentum-based motions across more complex terrains. In this paper, we learn controllers that allow simulated characters to traverse terrains with gaps, steps, and walls using highly dynamic gaits. This is achieved using reinforcement learning, with careful attention given to the action representation, non-parametric approximation of both the value function and the policy; epsilon-greedy exploration; and the learning of a good state distance metric. The methods enable a 21-link planar dog and a 7-link planar biped to navigate challenging sequences of terrain using bounding and running gaits. We evaluate the impact of the key features of our skill learning pipeline on the resulting performance.	approximation;bellman equation;greedy algorithm;reinforcement learning;traverse	Xue Bin Peng;Glen Berseth;Michiel van de Panne	2015	ACM Trans. Graph.	10.1145/2766910	computer vision;simulation;computer science;artificial intelligence;machine learning;computer animation;computer graphics (images)	Graphics	50.54186249952176	-27.149942909917456	149572
e0f0253c7964247daf456f28e57346560f86ef44	solving computational and memory requirements of feature-based simultaneous localization and mapping algorithms	navigation decorrelation mobile robots kalman filters;simultaneous localization and mapping australia random access memory covariance matrix navigation robotics and automation remotely operated vehicles read write memory costs decorrelation;kalman filters;mobile robots;navigation;simultaneous localization and mapping;decorrelation;extended kalman filter;decorrelation solutions memory requirements computational requirements feature based simultaneous localization mapping algorithms compressed extended kalman filter algorithm landmark representation;simultaneous localisation and map building	This paper presents new algorithms to implement simultaneous localisation and map building (SLAM) in environments with very large number of features. The algorithms present an efficient solution to the full update required by the Compressed Extended Kalman Filter algorithm (CEKF). It makes uses of the Relative Landmark Representation (RLR) to develop very close to optimal de-correlation solutions. With this approach the memory and computational requirements are reduced from ~O(N) to ~O(N*Nb), being N and Nb proportional to the number of features in the map and features close to the vehicle respectively. Experimental results are presented to verify the operation of the system when working in large outdoor environments.	extended kalman filter;peterson's algorithm;requirement;simultaneous localization and mapping	José E. Guivant;Eduardo Mario Nebot	2003	IEEE Trans. Robotics and Automation	10.1109/TRA.2003.814500	kalman filter;mobile robot;computer vision;navigation;simulation;decorrelation;computer science;artificial intelligence;machine learning;extended kalman filter;simultaneous localization and mapping	Robotics	53.01705224972947	-35.75079383767572	149632
797a4178bab83be8eb6e2267cfc9fa48f56dbf51	indoor location sensing using geo-magnetism	magnetic field;mobile device;guiding light;systems for location determination;magnetic fingerprint;geomagnetic field;indoor positioning system;positioning system;wearable computer;wearable computing and innovative mobile devices;indoor positioning	We present an indoor positioning system that measures location using disturbances of the Earth's magnetic field caused by structural steel elements in a building. The presence of these large steel members warps the geomagnetic field in a way that is spatially varying but temporally stable. To localize, we measure the magnetic field using an array of e-compasses and compare the measurement with a previously obtained magnetic map. We demonstrate accuracy within 1 meter 88% of the time in experiments in two buildings and across multiple floors within the buildings. We discuss several constraint techniques that can maintain accuracy as the sample space increases.	experiment;indoor positioning system	Jaewoo Chung;Matt Donahoe;Chris Schmandt;Ig-Jae Kim;Pedram Razavai;Micaela Wiseman	2011		10.1145/1999995.2000010	simulation;wearable computer;magnetic field;computer science;operating system;mobile device;hybrid positioning system;earth's magnetic field	HCI	46.96406503053865	-30.054679462738463	149767
51f8c14680e104c598e739754078849cd23f7d07	grid-based online road model estimation for advanced driver assistance systems	roads vehicles estimation semantics sensors trajectory vehicle dynamics;sensor fusion driver information systems intelligent transportation systems object detection road vehicles;german highway scenario grid based online road model estimation advanced driver assistance systems road course information individual lane information automated driving application offline map online sensor measurements ego vehicle perception data lane marking detection dynamic object movement history vehicle environment road boundary detection	The information about the road course and individual lanes is an important requirement in driver assistance systems and for automated driving applications. It is often stored in a highly accurate offline map so that the road and the lanes are known in advance. However, there exist situations where an offline map can become unusable or invalid. This paper presents a novel approach for a road model estimation solely based on online measurements from sensors mounted on the ego vehicle. It combines perception data like detected lane markings, the movement history of dynamic objects in the vehicle's environment and detected road boundaries into a grid-based road model. This approach allows for an estimation of the road model even when one source of information is not available and offers a redundant source of information about the road, which is necessary in critical applications such as automated driving. The presented approach was tested and evaluated with a prototype vehicle and real sensor data from German highway scenarios.	algorithm;autonomous car;existential quantification;information source;online and offline;prototype;sensor;series and parallel circuits;usability	Julian Thomas;Kai Stiens;Sebastian Rauch;Raúl Rojas	2015	2015 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2015.7225665	computer vision;simulation;advanced driver assistance systems;vehicle information and communication system;engineering;transport engineering	Robotics	50.50169831914364	-36.272482256572275	149794
191c161e06d3328dd44e86eabbeaff1f7f458694	fault-tolerant probabilistic sensor fusion for multi-agent systems	sensor fusion fault tolerance multi agent systems multi robot systems;sensor system;uncertainty robot vision systems cameras fuses bayesian methods;multi agent system;fault tolerant;linear opinion pool;uncertainty;fuses;bayesian methods;fault tolerant probabilistic sensor fusion;multirobot multisensor systems;multi agent systems;logarithmic opinion pool;fault tolerance;multi robot systems;p norm opinion pool;sensor fusion;p norm opinion pool fault tolerant probabilistic sensor fusion multiagent systems multirobot multisensor systems linear opinion pool logarithmic opinion pool;robot vision systems;cameras;multiagent systems	In this work we focus on the problem of probabilistic sensor fusion in Multi-Robot Multi-Sensor Systems (MRMS), taking into account that some sensors might fail or produce erroneous information. We study fusion methods that can successfully cope with situations of agreement, partial agreement, and disagreement between sensors. We define a set of specifications for fusion methods appropriate for MRMS environments. In light of these specifications, we review two popular algorithms for probabilistic sensor fusion, Linear Opinion Pool (LOP) and Logarithmic Opinion Pool (LGP). To overcome difficulties of applying them to a MRMS setting, a new method is introduced, p-norm Opinion Pool (POP). Comparing to LOP and LGP, POP is more compatible with the specifications and more flexible, successfully handling situations of agreement and disagreement between sensors. Through simulation and real-world experiments, we check performance of the POP and compare it with LOP and LGP. We also implement a real-world experiment through which the performance of POP is examined.	algorithm;algorithmic efficiency;computation;experiment;language-oriented programming;outlook.com;sensor;sensor web;simulation	Abdolkarim Pahliani;Matthijs T. J. Spaan;Pedro U. Lima	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5650337	fault tolerance;simulation;computer science;engineering;artificial intelligence;multi-agent system;data mining;computer security	Robotics	52.09027137917562	-34.503151388508	149817
1866b6751de441dba80732f080351bfe3205b158	outdoor autonomous navigation using monocular vision	monocular vision;urban vehicles robot navigation monocular vision map building;map building;video signal processing;path planning;robot navigation;mobile robots;three dimensional;navigation;navigation robot vision mobile robots path planning position control vehicles video signal processing image sequences;robot vision;position control;navigation robot sensing systems robot vision systems global positioning system cameras simultaneous localization and mapping trajectory robot kinematics humans remotely operated vehicles;autonomous navigation;ground truth;vehicles;electric vehicle;map building outdoor autonomous navigation monocular vision outdoor robot navigation video sequence robot trajectory urban electric vehicle;image sequences	In this paper, a complete system for outdoor robot navigation is presented. It uses only monocular vision. The robot is first guided on a path by a human. During this learning step, the robot records a video sequence. From this sequence, a three dimensional map of the trajectory and the environment is built. When this map has been computed, the robot is able to follow the same trajectory by itself. Experimental results carried out with an urban electric vehicle are shown and compared to the ground truth.	algorithm;autonomous robot;global positioning system;ground truth;real time kinematic;robotic mapping;sensor;simultaneous localization and mapping	Eric Royer;Jonathan Bom;Michel Dhome;Benoit Thuilot;Maxime Lhuillier;François Marmoiton	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545495	mobile robot;three-dimensional space;computer vision;navigation;simulation;ground truth;computer science;monocular vision;artificial intelligence;motion planning;mobile robot navigation;remote sensing	Robotics	52.60254189118019	-36.82784080349251	150090
dee26c3d434eb24fec8006da653f739109b31578	a new probabilistic path planning algorithm for (dis)assembly tasks	assembly path planning planning probabilistic logic interpolation space exploration robots;robotersysteme	In this paper, a new probabilistic path planning algorithm is described. The algorithm has been developed for assembly planning purposes, but however it can also be used in similar scenarios. Most assembly planning algorithms apply the so called assembly-by-disassembly strategy, therewith planning starts from the goal position of parts and tries to remove single parts or group of parts. Such problems are characterized by the appearance of many narrow passages. Thus, we have developed an probabilistic algorithm able to ??nd paths even if many of such passages exist. The idea of our path planner emanates from the particle In each iteration, it propagates new samples, discards bad evaluated samples and assigns higher weights to good examples. The evaluation functions propagates the samples to explore free space as well as to condensate on the border of obstacles, which leads samples to pass narrow passages. We have evaluated our path planning algorithm with different examples and compared it to the well-known RRT and PRMplanners. We could achieve good execution times for realistic industrial assembly tasks.	algorithm;automated planning and scheduling	Ulrike Thomas;Rene Iser	2010			computer vision;mathematical optimization;any-angle path planning;machine learning;mathematics	Robotics	53.1561702823059	-24.45915758334275	150302
6fc38e3dd859029107c6a0ed1a42f205057c59a1	on the use of sensor fusion to reduce the impact of rotational and additive noise in human activity recognition	metaclassifier;models theoretical;middle aged;adolescent;rotational noise;additive noise;human activities;info eu repo semantics article;artifacts;adult;accelerometer;humans;remote sensing technology;young adult;sensor fusion;rotation;activity recognition	The main objective of fusion mechanisms is to increase the individual reliability of the systems through the use of the collectivity knowledge. Moreover, fusion models are also intended to guarantee a certain level of robustness. This is particularly required for problems such as human activity recognition where runtime changes in the sensor setup seriously disturb the reliability of the initial deployed systems. For commonly used recognition systems based on inertial sensors, these changes are primarily characterized as sensor rotations, displacements or faults related to the batteries or calibration. In this work we show the robustness capabilities of a sensor-weighted fusion model when dealing with such disturbances under different circumstances. Using the proposed method, up to 60% outperformance is obtained when a minority of the sensors are artificially rotated or degraded, independent of the level of disturbance (noise) imposed. These robustness capabilities also apply for any number of sensors affected by a low to moderate noise level. The presented fusion mechanism compensates the poor performance that otherwise would be obtained when just a single sensor is considered.	activity recognition;additive model;additive white gaussian noise;behavioral tic;calibration;consortium;displacement mapping;floating-point unit;gilles de la tourette syndrome;heart rate variability;human activities;linear algebra;noise (electronics);preparation;psychologic displacement;reliability engineering;sensor web;utility functions on indivisible goods;wearable computer;sensor (device)	Oresti Baños;Miguel Damas;Héctor Pomares;Ignacio Rojas	2012		10.3390/s120608039	embedded system;simulation;young adult;telecommunications;rotation;computer science;engineering;electrical engineering;nanotechnology;sensor fusion;forensic engineering;accelerometer;physics;quantum mechanics;activity recognition	Robotics	46.34255746879685	-29.89249742877908	150485
8c937522c3071b6adab2d0483333100459062424	context-enhanced information fusion for tracking applications	context navigation target tracking estimation radar tracking roads noise measurement;radar tracking;information fusion context awareness target tracking bayesian estimation;noise measurement;context fusion context enhanced information fusion tracking applications context inference sensor modeling motion constraints physical condition operational condition indoor waterway navigation inland waterway navigation indoor positioning cascaded extended kalman filter ekf particle filter architecture pf architecture stance phase detection floor map measurement models context information fusion global navigation satellite system gnss context based criteria position estimation;navigation;estimation;roads;target tracking kalman filters nonlinear filters particle filtering numerical methods sensor fusion;target tracking;nautische systeme;context	Over the last decade, context has become a key source of information for tracking problems. Context inference allows refining sensor modeling and target dynamics as well as the creation of motion constraints according to the physical and operational conditions of the scenario. This work presents two example applications: indoor and inland waterway navigation where the context information is employed to reduce the uncertainty of the tracking and enhance the navigation solution. For indoor positioning a cascaded Extended Kalman Filter (EKF) and Particle Filter (PF) architecture is proposed. The system uses stance phase detection and the available floor map to construct the measurement models for the KF and motion constraints for the PF respectively. For navigating in inland waterways it is shown how to benefit from context information fusion by inferring the operating condition of the Global Navigation Satellite System (GNSS). In this scenario, context based criteria are derived for the selection of the best position estimation amongst several positioning solvers running in parallel. This work presents the basics for context fusion in tracking applications, illustrating the theory with two application examples. The preliminary results already demonstrate a performance improvement compared to state-of-the-art approaches.	extended kalman filter;galileo (satellite navigation);information source;particle filter;satellite navigation	Daniel Arias Medina;Jesus Garciaz;Michailas Romanovas;Ralf Ziebold;Manuel Schwaab	2016	2016 19th International Conference on Information Fusion (FUSION)		computer vision;simulation;tracking system;geography;remote sensing	Robotics	50.57590898406048	-34.24226183219676	150489
f69d917b2e062c4d3135ed6d72c09f6abd5d1f71	improved indoor positioning using the baum-welch algorithm	hidden markov models buildings markov processes wireless communication delays estimation measurement uncertainty;parameter estimation indoor positioning baum welch algorithm pedestrians life threatening delays response times hidden markov model location estimation particle filter odometry information position error distributions learning process;measurement uncertainty;wireless communication;hidden markov models;estimation;markov processes;buildings;delays;pedestrians hidden markov models indoor navigation indoor radio parameter estimation particle filtering numerical methods	In this paper, we examine the exploitation of individual patterns of behavior to enhance indoor positioning of pedestrians.We make use of the fact that, due to habits and needs, a person is likely to be in some locations more often than others. For example, at their work or in their home, a person is likely to spend more time in some rooms than in others. Therefore, it seems natural to take advantage of established behavior patterns when performing indoor localization in an effort to improve accuracy. Such improvement is particularly beneficial during emergencies, where location inaccuracies may lead to life-threatening delays in response times. In this work, habitual behavior is modeled and learned using a hidden Markov model. It is shown that, applying the Markov model for location estimation results in more accurate estimates when compared to using a standard particle filter with odometry information. Additionally, transition probabilities as well as position error distributions do not need to be known a priori since they can be learned using the Baum-Welch algorithm. Results show how the Baum-Welch algorithm can even learn the distributions of biased estimates. On the other hand, it is shown how user feedback can help accelerate the learning process, while guaranteeing good parameter estimation accuracies.	baum–welch algorithm;estimation theory;forward algorithm;global positioning system;hidden markov model;markov chain;odometry;particle filter;welch's method	Noha El Gemayel;Javier Schloemann;R. Michael Buehrer;Friedrich Jondral	2015	2015 IEEE Globecom Workshops (GC Wkshps)	10.1109/GLOCOMW.2015.7414025	forward algorithm;estimation;simulation;machine learning;markov process;markov model;hidden markov model;wireless;statistics;measurement uncertainty	Robotics	48.12402877129376	-28.692291276941038	150826
a9778a9f10027bab5c1aa5b54ef4398eb13e4189	appearance-based navigation and homing for autonomous mobile robot	visual potential;visual navigation;nonholonomic mobile robot;dominant plane;optical flow;visual homing	In this paper, we develop an algorithm for navigating a mobile robot using the visual potential. The visual potential is computed from an image sequence and optical flow computed from successive images captured by a camera mounted on the robot, that is, the visual potential for navigation is computed from appearances of the workspace observed as an image sequence. The direction to the destination is provided at the initial position of the robot. The robot dynamically selects a local pathway to the destination without collision with obstacles and without any knowledge of the robot workspace. Furthermore, the guidance algorithm to destination allows the mobile robot to return from the destination to the initial position. We present the experimental results of navigation and homing in synthetic and real environments.	autonomous robot;mobile robot;multiple homing	Naoya Ohnishi;Atsushi Imiya	2013	Image Vision Comput.	10.1016/j.imavis.2012.11.004	mobile robot;computer vision;simulation;computer science;optical flow;visual servoing;mobile robot navigation	Robotics	53.43617147447005	-32.48901510536134	150928
816b08764318f5eba2ba125b6be816076cfcb859	learning navigational maps by observing human motion patterns	robot sensing systems;probability;gaussian processes;uncertainty;path planning;navigational maps human motion patterns social robots pedestrian positional traces continuous probabilistic function gaussian process learning noise filtering planning strategies statistical method spline function regression cluttered office open forum environments laser sensing modalities vision sensing modalities;motion estimation;human robot interaction;splines mathematics;navigation;robot vision;trajectory;human motion;splines mathematics gaussian processes human robot interaction motion estimation path planning probability regression analysis robot vision;regression analysis;humans;gaussian process;navigation trajectory humans robot sensing systems gaussian processes uncertainty;conference proceeding	Observing human motion patterns is informative for social robots that share the environment with people. This paper presents a methodology to allow a robot to navigate in a complex environment by observing pedestrian positional traces. A continuous probabilistic function is determined using Gaussian process learning and used to infer the direction a robot should take in different parts of the environment. The approach learns and filters noise in the data producing a smooth underlying function that yields more natural movements. Our method combines prior conventional planning strategies with most probable trajectories followed by people in a principled statistical manner, and adapts itself online as more observations become available. The use of learning methods are automatic and require minimal tuning as compared to potential fields or spline function regression. This approach is demonstrated testing in cluttered office and open forum environments using laser and vision sensing modalities. It yields paths that are similar to the expected human behaviour without any a priori knowledge of the environment or explicit programming.	algorithm;gaussian process;image noise;information;kinesiology;map;marginal model;mathematical optimization;motion planning;robotic mapping;social robot;spline (mathematics);tracing (software)	Simon Timothy O'Callaghan;Surya P. N. Singh;Alen Alempijevic;Fabio Tozeto Ramos	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5980478	human–robot interaction;computer vision;simulation;computer science;artificial intelligence;machine learning;gaussian process;statistics	Robotics	50.76329620631616	-35.06909733743463	151058
0b6d43513cf95286f6506cbfeb75348a1ffe7121	a parallel deconvolution algorithm in perfusion imaging	parallel computing;patient diagnosis;nvidia geforce gtx 280;cerebral blood volume;brain;haemodynamic quantity;paper;computed tomography;high performance computing;performance;haemodynamics;mean transit time;tumours;gpgpu perfusion imaging deconvolution parallelization;parallel deconvolution algorithm;biology;graphics processing unit deconvolution matrix decomposition blood arrays computed tomography;coprocessors;cerebral blood flow;tesla c1060;arterial input functions;brain tumor;cuda;arrays;gpgpu;tumours blood vessels brain coprocessors deconvolution diseases haemodynamics haemorheology medical image processing parallel algorithms patient diagnosis;graphics generation dedicated coprocessor;perfusion imaging;arterial input functions parallel deconvolution algorithm perfusion imaging brain perfusion quantification gpgpu general purpose graphics processor units cuda programming model graphics generation dedicated coprocessor parallel computing high performance computing parametric map haemodynamic quantity cerebral blood flow cerebral blood volume mean transit time patient diagnosis stroke brain tumor;matrix decomposition;general purpose graphics processor units;medical image processing;blood;parallelization;deconvolution;nvidia;diseases;algorithms;medicine;graphics processing unit;parametric map;cuda programming model;brain perfusion quantification;blood vessels;haemorheology;stroke;parallel algorithms	In this paper, we will present the implementation of a deconvolution algorithm for brain perfusion quantification on GPGPU (General Purpose Graphics Processor Units) using the CUDA programming model. GPUs originated as graphics generation dedicated co-processors, but the modern GPUs have evolved to become a more general processor capable of executing scientific computations. It provides a highly parallel computing environment due to its huge number of computing cores and constitutes an affordable high performance computing method. The objective of brain perfusion quantification is to generate parametric maps of relevant haemodynamic quantities such as Cerebral Blood Flow (CBF), Cerebral Blood Volume (CBV) and Mean Transit Time (MTT) that can be used in diagnosis of conditions such as stroke or brain tumors. These calculations involve deconvolution operations that in the case of using local Arterial Input Functions (AIF) can be very expensive computationally. We present the serial and parallel implementations of such algorithm and the evaluation of the performance gains using GPUs.	algorithm;cuda;central processing unit;computation;deconvolution;general-purpose computing on graphics processing units;graphics processing unit;hemodynamics;map;parallel computing;programming model;supercomputer;usb hub	Fan Zhu;David Rodriguez Gonzalez;Trevor Carpenter;Malcolm P. Atkinson;Joanna M. Wardlaw	2011	2011 IEEE First International Conference on Healthcare Informatics, Imaging and Systems Biology	10.1109/HISB.2011.6	parallel computing;computer science;theoretical computer science;computer graphics (images)	HPC	40.87195796423778	-34.06893760929684	151161
1995005d0707fdf4a82d014a167e05669c25aad2	robust estimation with faulty measurements using recursive-ransac	state estimation autonomous aerial vehicles estimation theory feedback iterative methods kalman filters sensors;micro air vehicles robust estimation faulty measurements recursive ransac autonomous platforms low level state estimation finite probability sensor faults gated kalman filter state estimation measurement noise standard deviation feedback loop;sensors noise noise measurement logic gates kalman filters clutter current measurement	Many autonomous platforms, such as micro air-vehicles, are increasingly relying on cheap, lightweight sensors to improve the low-level state estimation for navigation and control. Unfortunately, these and all sensors have a finite probability of returning spurious measurements that do not follow the classical zero-mean Gaussian models of measurement noise. A classical heuristic used to mitigate the effects of sensor faults is the gated-Kalman filter. We show that the gated- Kalman filter estimate diverges from the true states when the probability of detection is low or when the measurement noise standard deviation increases above the expected value. The main contribution of this paper is to utilize the recently developed recursive-RANSAC algorithm in a feedback loop to robustly estimate the true states when the probability of a sensor fault is high, when the measurement noise characteristics abruptly change, and during brief occlusions of the true signal, while maintaining real-time performance.	algorithm;autonomous robot;feedback;heuristic;high- and low-level;image noise;kalman filter;random sample consensus;real-time clock;recursion (computer science);sensor	Peter C. Niedfeldt;Randal W. Beard	2014	53rd IEEE Conference on Decision and Control	10.1109/CDC.2014.7040037	control engineering;electronic engineering;engineering;control theory;extended kalman filter;moving horizon estimation	Robotics	49.65682123506636	-32.68093412113894	151183
27bdab1ae4bd88fb186c96f84e5d1f249010cd00	learning predictive terrain models for legged robot locomotion	sparse approximation technique;kernel;predictive terrain model learning;probability;legged locomotion;gaussian processes;path planning;model adaptation;foot;sparse approximation;legged robot locomotion;predictive terrain model learning legged robot locomotion probabilistic technique gaussian process sparse approximation technique motion planning;probability approximation theory gaussian processes learning systems legged locomotion path planning;approximation theory;learning systems;robots adaptation model predictive models foot leg gaussian processes kernel;adaptation model;laser range finder;robots;motion planning;predictive models;terrain modeling;gaussian process;probabilistic technique;quadruped robot;legged robot;leg;cost model	Legged robots require accurate models of their environment in order to plan and execute paths. We present a probabilistic technique based on Gaussian processes that allows terrain models to be learned and updated efficiently using sparse approximation techniques. The major benefit of our terrain model is its ability to predict elevations at unseen locations more reliably than alternative approaches, while it also yields estimates of the uncertainty in the prediction. In particular, our nonstationary Gaussian process model adapts its covariance to the situation at hand, allowing more accurate inference of terrain height at points that have not been observed directly. We show how a conventional motion planner can use the learned terrain model to plan a path to a goal location, using a terrain-specific cost model to accept or reject candidate footholds. In experiments with a real quadruped robot equipped with a laser range finder, we demonstrate the usefulness of our approach and discuss its benefits compared to simpler terrain models such as elevations grids.	analysis of algorithms;bayesian network;experiment;gaussian process;microsoft outlook for mac;obstacle avoidance;process modeling;randomized algorithm;reinforcement learning;robot locomotion;simultaneous localization and mapping;smoothing;sparse approximation;sparse matrix;star height	Christian Plagemann;Sebastian Mischke;Sam Prentice;Kristian Kersting;Nicholas Roy;Wolfram Burgard	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4651026	computer vision;simulation;computer science;engineering;artificial intelligence;machine learning;gaussian process;motion planning;statistics	Robotics	49.941178790928824	-26.217224386074328	151421
538bc98c32b25687751d5ad225af134215b90972	trinocular disparity processor using a hierarchic classification structure	image segmentation;clocks;image matching;image classification;fpga;confidence metric;trinocular camera;computer vision;streaming media;system on chip;image color analysis;classification algorithms;stereo vision;field programmable gate arrays;real time matching;cameras;cameras streaming media image color analysis clocks real time systems classification algorithms hardware;conferences;fpga trinocular camera real time matching confidence metric computer vision system on chip;microprocessor chips;hardware;real time systems	This paper presents a real-time trinocular disparity processor. The core module performs a pairwise segmented window matching for both the center-right and center-left image pair as their scaled down image pairs. The resulting cost functions are combined which results into nine different curves. A hierarchical classifier is presented which selects the most promising disparity value using information provided by the calculated cost curves and the pixels spatial neighborhood using a two level classification architecture. The disparity processor has been evaluated with an indoor dataset and with a real-time implementation using an FPGA and three cameras. Special care has been taken to reduce the memory footprint so that the processor doesn't need external memory.	algorithm;binocular disparity;field-programmable gate array;hierarchical classifier;memory footprint;pixel;real-time clock;window function	Andy Motten;Luc Claesen;Yun Pan	2012	2012 IEEE/IFIP 20th International Conference on VLSI and System-on-Chip (VLSI-SoC)	10.1109/VLSI-SoC.2012.6379038	embedded system;computer vision;computer science;computer graphics (images)	Robotics	43.98429656547684	-35.702920134404174	151734
2577ffd4ae134346511e6e40fbceac1d4b463557	topological exploration of subterranean environments	topology;building block;coal mines;robots;exploration;system development;algorithms;mapping;coal lignite and peat;topological mapping;coal mining	The need for reliable maps of subterranean spaces too hazardous for humans to occupy has motivated the development of robotic mapping tools suited to these domains. As such, this work describes a system developed for autonomous topological exploration of mine environments to facilitate the process of mapping. The exploration framework is based upon the interaction of three main components: Node detection, node matching, and edge exploration. Node detection robustly identifies mine corridor intersections from sensor data and uses these features as the building blocks of a topological map. Node matching compares newly observed intersections to those stored in the map, providing global localization during exploration. Edge exploration translates topological exploration objectives into locomotion along mine corridors. This article describes both the robotic platform and the algorithms developed for exploration, and presents results from experiments conducted at a research coal mine near Pittsburgh, PA.		David Silver;Dave Ferguson;Aaron Morris;Scott Thayer	2006	J. Field Robotics	10.1002/rob.20130	biology;computer science;coal mining;geotechnical engineering	Robotics	53.37392187481821	-33.930719325521366	152175
320679667897d6a5cec8dbd10934edd894d8bc3f	development of a nerve model of eyeball motion nerves to simulate the disorders of eyeball movements for neurologic examination training	cranial;medical robotics biomechanics eye medical disorders;training;medical services;systematic training eyeball movement disorder neurologic examination training eyeball motion nerve model wkh 2 waseda kyotokagaku head robot no 2 cranial examination motor nerve model eyeball nerve structure;head;robot kinematics;muscles;muscles training medical services cranial head robot kinematics	Cranial nerve examination takes an important role in the physical examination. All the medical staffs need to be trained to master the skills for examination. By now, several training methods have been carried out for medical training including training with the simulated patient (SP). However, considering the characters of eyeball, the involuntary actions cannot be simulated by Sp. With the developments of technology, more and more robot heads have been launched. However, these simulators only focus on mimicking the healthy human abilities, not simulating disorders for medical training. In this paper, we propose a novel eyeball motion nerve model which is used in the head robot named WKH-2(Waseda Kyotokagaku Head Robot No.2). This robot is designed to simulate the disorders of eyeball movements to give the trainee a full training on cranial examination. The motor nerve model is carried out from mimicking the real eyeball nerve structure. And the functions of each part are designed from analysis of the real functions of each organ. In this robot, the effects of eyeball motion nerve system, and various symptoms are simulated. Making use of this robot, a systematic training on the skills as well as the understanding of medical knowledge is provided. Finally, several experiments are carried out to verify our proposed system. The experimental results show that the approach is worth following in further research.	database schema;experiment;robot;sacral nerve stimulation;simulation;stereoscopy	Lin Wang;Chunbao Wang;Lihong Duan;Ai Niibori;Yusaku Miura;Yurina Sugamiya;Weisheng Kong;Qing Shi;Hiroyuki Ishii;Salvatore Sessa;Massimiliano Zecca;Atsuo Takanishi;Zhengzhi Wu;Jian Qin;Weiguang Li	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090414	simulation;computer science;artificial intelligence;head;robot kinematics	Robotics	39.75766173106242	-37.18904225456294	152442
981965db5cdc10a36b38a70687da3d9b7a2ef973	three novell analog-domain algorithms for motion detection in video surveillance	signal image and speech processing;video surveillance;biometrics;pattern recognition;image processing and computer vision;motion detection	As to reduce processing load for video surveillance embedded systems, three low-level motion detection algorithms to be implemented on an analog CMOS image sensor are presented. Allowing on-chip segmentation of moving targets, these algorithms are both robust and compliant to various environments while being power efficient. They feature different trade-offs between detection performance and number of a priori choices. Detailed processing steps are presented for each of these algorithms and a comparative study is proposed with respect to some reference algorithms. Depending on the application, the best algorithm choice is then discussed.		Arnaud Verdant;Patrick Villard;Antoine Dupret;Hervé Mathias	2011	EURASIP J. Image and Video Processing	10.1155/2011/698914	computer vision;feature detection;speech recognition;image processing;computer science;archaeology;digital image processing;pattern recognition;biometrics;computer graphics (images)	EDA	44.305205076324036	-36.61789869534715	152512
01a5af15300adf07a30990735099d77e6f6a3a66	office waste cleanup: an application for service robots	nonlinear filters;object recognition;mail delivery;motion control;stationary delivery;image segmentation;video signal processing;path planning;kalman filters;service robots;mobile robots;orbital robotics;navigation;postal services;robot vision;position control;image colour analysis;service robot;robustness;explosions;humans;space technology;flexible functional architecture office waste cleanup service robots david mail delivery stationary delivery;office waste cleanup;floods;kalman filters mobile robots path planning object recognition robot vision tree searching image segmentation image colour analysis motion control position control video signal processing nonlinear filters;service robots cleaning orbital robotics postal services humans navigation space technology robustness floods explosions;tree searching;flexible functional architecture;david;cleaning	Waste cleanup is a task which occurs in a great variety of domains and contexts. In spite of the variety of the causes and the types of waste, cleaning it up typically involves dirty, dull, strenuous and sometimes hazardous actions. Thus waste cleanup constitutes a natural task for service robots. In this paper, we describe an office robot, DAVID, who in addition to collecting and delivering mail and stationary, cleans up offices from garbage. We describe, the system's flexible functional architecture and its various components. We identify some functionalities critical in waste-cleanup tasks and describe how they are accomplished in DAVID. The paper closes with the discussion of an experiment which demonstrates the performance of the system, and with a set of conclusions we drew from this work.	robot	Erwin Prassler;Eleni Stroulia;Matthias Strobel	1997		10.1109/ROBOT.1997.619059	kalman filter;motion control;mobile robot;computer vision;navigation;simulation;computer science;engineering;artificial intelligence;cognitive neuroscience of visual object recognition;control theory;motion planning;space technology;image segmentation;robustness	Robotics	50.209167205450946	-37.71958492363384	152841
32dd328efd54b04e113fbcceadcea6ccee9acbd9	mutual learning or unsupervised interactions between mobile robots	unsupervised learning;robot sensing systems;trajectory planner;game theory;cognitive robotics;unsupervised interactions;neural networks;neural nets;mobile robot;path planning;trajectory planner unsupervised interactions mobile robots robot vision mutual learning dynamic map simulated odometric system pursuit strategies neural network;mobile robots;delta modulation;distance measurement;navigation;robot vision;pursuit strategies;cooperative systems;simulated odometric system;dynamic map;dynamic capabilities;mutual learning;robotic assembly;mobile robots robot vision systems robot sensing systems vehicle dynamics neural networks cameras cognitive robotics delta modulation robotic assembly game theory;robot vision systems;vehicle dynamics;cameras;neural network	This paper presents an overview of an ongoing project about the study of vision-based interactions between mobile robots. We first introduce the concept of Dynamic Map, which is a uni$ed representation of the dynamic capabilities of a robot along with its interactions with its environment. An overview of the experimental setup is given, presenting the robot architecture, including the vision-based simulated odometric system. We address the problems of the learning and execution of pursuit strategies using a neural networkbased trajectory planne,:	interaction;mobile robot	Christian Zanardi;Jean-Yves Hervé;Paul Cohen	1996		10.1109/ICPR.1996.547230	mobile robot;computer vision;simulation;computer science;machine learning;artificial neural network	Robotics	50.20529275532278	-29.96661396197163	152873
100917034e7c947e6f40ba6451d4200c68030970	modeling of moisture test for grain based on bp neural network and dielectric loss factor	tms320f2812;gradient descent method;nonlinear mapping;agricultural products;nonlinear mapping ability;phase sensitive detection;neural nets;online test;training;grain centering;neural nets agricultural products backpropagation moisture;generalization ability bp neural network dielectric loss factor multiple sensing mechanism online moisture test system grain centering tms320f2812 phase sensitive detection multi input single output model gradient descent method forgetting factor nonlinear mapping ability;backpropagation;generalization ability;forgetting factor;moisture measurement;multi input single output model;artificial neural networks;moisture;bp neural network;dielectric loss factor;mathematical model;bp neural network dielectric loss factor grain moisture online test;grain moisture;dielectric loss;dielectric measurements;temperature measurement;dielectrics;online testing;online moisture test system;dielectric losses moisture neural networks dielectric loss measurement system testing dielectric measurements loss measurement mathematical model neural network hardware phase measurement;multiple sensing mechanism;neural network;multi input single output	This paper introduces the multiple sensing mechanism of online moisture test for grain and improved BP neural network, and gives hardware structure of moisture test system for grain centering on TMS320F2812. In this paper, dielectric loss factor is measured by the method of orthogonal separation on phase-sensitive detection. Though measuring three parameters of dielectric loss factor, power fluctuations and temperature changes, using BP neural network to construct multi-input single-output model, applying gradient descent method with forgetting factor for parameters adjustment of BP neural network, utilizing the nonlinear mapping ability and learning generalization ability of the BP neural network, and using high precision samples for the training of BP neural network, the mathematic model of moisture test system for grain based on BP neural network was established finally. The sample testing experiment shows that the measurement accuracy obtains a great enhancement comprehensively considering the effect on testing output with the aim sensor characteristic and non-aim parameter.	artificial neural network;gradient descent;nonlinear system;propagation constant	Jishun Jiang;Hua Ji	2009	2009 Second International Symposium on Computational Intelligence and Design	10.1109/ISCID.2009.114	gradient descent;moisture;dielectric loss;temperature measurement;computer science;artificial intelligence;backpropagation;machine learning;mathematical model;artificial neural network;dielectric	ML	40.80900264416503	-24.202935279936572	153039
73183c059f694ab23852fac631f07047b619ee37	a combined monte-carlo localization and tracking algorithm for robocup	monte carlo localization;mobile robot;path planning;mobile robots;pose estimation monte carlo localization tracking algorithm robocup self localization method mobile robotics position estimation;omnidirectional camera;position control;robot kinematics cameras data mining iterative algorithms intelligent robots mobile robots computer architecture mobile computing table lookup force measurement;position control mobile robots monte carlo methods multi robot systems path planning pose estimation;multi robot systems;position estimation;monte carlo methods;pose estimation	Self-localization is a major research task in mobile robotics for several years. Efficient self-localization methods have been developed, among which probabilistic Monte-Carlo localization (MCL) is one of the most popular. It enables robots to localize themselves in real-time and to recover from localization errors. However, even those versions of MCL using an adaptive number of samples need at least a minimum in the order of 100 samples to compute an acceptable position estimation. This paper presents a novel approach to MCL based on images from an omnidirectional camera system. The approach uses an adaptive number of samples that drops down to a single sample if the pose estimation is sufficiently accurate. We show that the method enters this efficient tracking mode after a few cycles and remains there using only a single sample for more than 90% of the cycles. Nevertheless, it is still able to cope with the kidnapped robot problem	3d pose estimation;algorithm;experiment;global optimization;iteration;kidnapped robot problem;local search (optimization);lookup table;macintosh common lisp;mathematical optimization;mobile robot;monte carlo localization;monte carlo method;object detection;omnidirectional camera;real-time clock;robotics	Patrick Heinemann;Jürgen Haase;Andreas Zell	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.281985	mobile robot;monte carlo localization;computer vision;simulation;computer science;artificial intelligence	Robotics	53.7571458233027	-36.20394978255514	153152
b311374bb692b8aaa72f534de50ffd268f516505	genetic tracker with adaptive neuro-fuzzy inference system for multiple target tracking	assignment problem;joint probabilistic data association filter;data association;genetics;multiple target tracking;genetic algorithm;adaptive neuro fuzzy inference system;neural network	In this paper, a genetic tracker with adaptive neuro-fuzzy inference system (GT-ANFIS) is presented for multiple target tracking (MTT). First, the data association problem, formulated as an N-dimensional assignment problem, is solved using the genetic algorithm (GA), and then the inaccuracies in the estimation are corrected by the adaptive neuro-fuzzy inference system (ANFIS). The performances of the GT-ANFIS, the joint probabilistic data association filter (JPDAF), the genetic tracker (GT), and the genetic tracker with neural network (GT-NN) are compared with each other for six different tracking scenarios. It was shown that the tracks estimated by using proposed GT-ANFIS agree better with the true tracks than the tracks predicted by the JPDAF, the GT, and the GT-NN.	adaptive neuro fuzzy inference system;inference engine;neuro-fuzzy	Ilke Turkmen;Kerim Guney	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.08.065	genetic algorithm;joint probabilistic data association filter;adaptive neuro fuzzy inference system;computer science;artificial intelligence;machine learning;pattern recognition;data mining;assignment problem;artificial neural network	Vision	48.486060536537025	-33.98628651268577	153222
5145f68a74b12280e03213d7a863d560f3de0252	obstacle detection in mobile outdoor robots - a short-term memory for the mobile outdoor platform ravon	distributed minimal world model;biologically inspired robotics;short-term memory;obstacle avoidance;mobile robotics;behaviour-based navigation;algorithms;obstacle detection;mobile robot;short term memory	In this paper a biologically inspired approach for compensating the limited angle of vision in obstacle detection systems of mobile robots is presented. Most of the time it is not feasible to exhaustively monitor the environment of a mobile robot. In order to nonetheless achieve safe navigation obstacle detection mechanisms need to keep in mind certain aspects of the environment. In mammals this task is carried out by the creature’s short-term memory. Inspired by this concept an absolute local map storing obstacles in terms of representatives has been introduced in the obstacle detection and avoidance system of the outdoor robot RAVON. That way the gap between the fields of vision of two laser range finders can be monitored which prevents the vehicle from colliding with obstacles seen some time ago.	long short-term memory;mobile robot	Bernd-Helge Schäfer;Martin Proetzsch;Karsten Berns	2007				Robotics	50.10925027112163	-31.04804160809862	153364
bcf8df4da0844c77148abf8da3ccbf1c011103d4	predictive tracking across occlusions in the icub robot	on line estimation;computer graphics;robot vision computer graphics humanoid robots object detection position control;robot vision;humanoid robots;position control;smooth pursuit;robots;biologically plausible behavior icub robot visual moving target rls algorithm online estimation target trajectory prediction vision based object tracker;object detection	In humans the tracking of a visual moving target across occlusions is not made with continuous smooth pursuit. The tracking stops when the object is occluded and one or two saccades are made to the other side of the occluder to anticipate when and where the object reappears. This paper describes a methodology for the implementation of such a behavior in a robotic platform - the iCub. We use the RLS algorithm for the on-line estimation and prediction of the target trajectory and a vision based object tracker capable of detecting the occlusion and the reappearance of an object. This system demonstrates predictive ability for tracking across an occlusion with a biologically-plausible behavior.	algorithm;hidden surface determination;icub;online and offline;recursive least squares filter;robot;sensor	Egidio Falotico;Matteo Taiana;Davide Zambrano;Alexandre Bernardino;José Santos-Victor;Paolo Dario;Cecilia Laschi	2009	2009 9th IEEE-RAS International Conference on Humanoid Robots	10.1109/ICHR.2009.5379534	robot;computer vision;simulation;computer science;humanoid robot;artificial intelligence;smooth pursuit;robot control;computer graphics	Robotics	49.88936715159406	-34.99746524171161	153740
c1b1eb73cf5d56bbbf5c25c4be17594d4517bea0	neural model of a grid-based map for robot sonar	robot sensing systems;ucl;neural networks;neural model;neural nets;mobile robotics;grid based;occupancy grid;biological system modeling;discovery;sonar robot sensing systems robot kinematics orbital robotics cells biology neurons artificial neural networks space technology biological neural networks biological system modeling;map construction;theses;mobile robots;conference proceedings;orbital robotics;neural nets mobile robots sonar navigation computerised navigation;recurrent connections;navigation;artificial neural networks;grid based mapping method neural model robot sonar occupancy grid integrate and fire neurons artificial neural net receptive fields recurrent connections;digital web resources;ucl discovery;open access;integrate and fire neurons;ucl library;mapping;artificial neural net;receptive field;space technology;robot sonar;integrate and fire neuron;neurons;book chapters;open access repository;grid based mapping method;receptive fields;biological neural networks;cells biology;robot kinematics;computerised navigation;ucl research;sonar	A functional similarity is described between cells of an occupancy grid for robot sonar, and integrate-and-re neurons of an artiicial neural net. Using this analogy, a new grid-based mapping system for robot sonar is described, which makes use of the neural concepts of receptive elds and recurrent connections. The performance of the new network is compared to that of a previous Bayesian grid-based mapping method, and a previous feature-based mapping method.	artificial neural network;robot;sonar (symantec)	Kenneth D. Harris;Michael Recce	1997		10.1109/CIRA.1997.613835	computer vision;simulation;computer science;artificial intelligence;machine learning;occupancy grid mapping;receptive field;artificial neural network	Robotics	49.95644891969816	-30.125026519880493	153753
9137a73ee239fd7bd25ddc218c4114f94d0a73eb	a second generation low cost embedded color vision system	intelligent sensor;vision system;microcontrollers;frames per second;costs machine vision robots cmos image sensors embedded system intelligent sensors cmos process color cameras microcontrollers;edge detection;color;color histogram;cmos process;embedded system;chip;cmos image sensors;machine vision;robots;intelligent sensors;cameras;color vision	In this paper we describe a low cost embedded vision system, the CMUcam2. The CMUcam2 is the second generation of the CMUcam system and attempts to overcome the shortcomings of the original system as well as improve upon and add to its functionality. The goal of the system is to provide simple vision capabilities to small embedded systems in the form of an intelligent sensor. The system utilizes a low cost CMOS color camera module, a frame buffer chip and all image data is processed by a low cost microcontroller. The system includes the original functionality of color blob tracking, but improves upon it with tracking speeds of up to 50 frames per second. New functionality includes frame differencing, edge detection, and color histogramming. Other improvements were also made to facilitate communication with slower speed processors, as is often the case in a variety of robotics applications, including miniature robotics, hobby robotics and aerial robots.	arm7;aerial photography;algorithm;autoregressive integrated moving average;blob detection;cmos;cmucam;camera module;central processing unit;color vision;computer vision;edge detection;embedded system;firmware;framebuffer;microcontroller;microprocessor;open-source software;pattern recognition;prototype;robot;robotics;second generation multiplex plus	Anthony Rowe;Charles R. Rosenberg;Illah R. Nourbakhsh	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops	10.1109/CVPR.2005.396	embedded system;computer vision;machine vision;computer science;intelligent sensor	Robotics	44.869876610305	-35.366153919588264	154360
54f443fbaa19f7a73cfaa108ece790b53c0f8317	implementation of visual motion detection in analog “neuromorphic” circuitry—a case study of the issue of circuit precision	visual processing analog integrated circuits motion detection neural networks neuromorphic precision;visualization neuromorphics optical sensors feature extraction motion detection insects optical imaging motion detection neuromorphic engineering visual analytics analog integrated circuits;statistical analysis analogue circuits mixed analogue digital integrated circuits;neuromorphic design philosophy visual motion detection analog neuromorphic circuitry circuit precision biological nervous systems neuromorphic integrated circuits computational precision engineering applications statistical approach electrical properties autonomous robotics analog mixed signal design	Many animals rely on visual motion to infer their relation to the surrounding environment while moving around in it. Motion processing in biological nervous systems, particularly in the insects, has been a subject of active research for over 50 years. With the advent of interest in “neuromorphic” integrated circuits in the late 1980s, this mode of sensory processing has also been the subject of various analog silicon modeling efforts. The author discusses the background of certain models for motion detection in insects, and then the implementation of a particular model in analog integrated circuitry. The paper does not focus on such circuits themselves, however, but on the issue of computational precision in the analog domain: it is argued that insufficient attention to this issue has been an impediment to the development of neuromorphic circuits that are practically useful in engineering applications. A statistical approach is described for assessing one of the designs discussed herein, and the degradation of its performance due to variations in the electrical properties of its constituent devices, with an eye toward suitability for application in autonomous robotics. This approach relies on tools and models routinely used in conventional analog/mixed signal design. The author argues that adoption of such techniques, along with a deeper consideration of computational precision, i.e., a shift in the neuromorphic design philosophy, would be an important step in moving the field forward.	autonomous robot;computation;electronic circuit;elegant degradation;mixed-signal integrated circuit;neuromorphic engineering;robotics	Patrick A. Shoemaker	2014	Proceedings of the IEEE	10.1109/JPROC.2014.2345570	control engineering;computer vision;electronic engineering;computer science;neuromorphic engineering	EDA	46.341910771665376	-32.56960537862941	154367
d7c8f12aba2112e5363cf026b10a8adecfb514dd	multipass target search in natural environments	branch and bound;coverage planning;information gathering;path planning;target search	Consider a disaster scenario where search and rescue workers must search difficult to access buildings during an earthquake or flood. Often, finding survivors a few hours sooner results in a dramatic increase in saved lives, suggesting the use of drones for expedient rescue operations. Entropy can be used to quantify the generation and resolution of uncertainty. When searching for targets, maximizing mutual information of future sensor observations will minimize expected target location uncertainty by minimizing the entropy of the future estimate. Motion planning for multi-target autonomous search requires planning over an area with an imperfect sensor and may require multiple passes, which is hindered by the submodularity property of mutual information. Further, mission duration constraints must be handled accordingly, requiring consideration of the vehicle's dynamics to generate feasible trajectories and must plan trajectories spanning the entire mission duration, something which most information gathering algorithms are incapable of doing. If unanticipated changes occur in an uncertain environment, new plans must be generated quickly. In addition, planning multipass trajectories requires evaluating path dependent rewards, requiring planning in the space of all previously selected actions, compounding the problem. We present an anytime algorithm for autonomous multipass target search in natural environments. The algorithm is capable of generating long duration dynamically feasible multipass coverage plans that maximize mutual information using a variety of techniques such as ϵ -admissible heuristics to speed up the search. To the authors' knowledge this is the first attempt at efficiently solving multipass target search problems of such long duration. The proposed algorithm is based on best first branch and bound and is benchmarked against state of the art algorithms adapted to the problem in natural Simplex environments, gathering the most information in the given search time.	agent-based model;algorithmic efficiency;anytime algorithm;autonomous robot;average-case complexity;benchmark (computing);branch and bound;depth-first search;direction finding;entropy (computing);file spanning;floods;greedy algorithm;hl7publishingsubsection <operations>;heuristics;motion planning;mutual information;numerous;path dependence;rewards;short;solutions;solver;supercomputer;survivors;telecommunications network;transcutaneous electric nerve stimulation;unmanned aerial vehicle;weighted voronoi diagram;workspace	Michael Kuhlman;Michael W. Otte;Donald Sofge;Satyandra K. Gupta	2017		10.3390/s17112514	real-time computing;engineering;electronic engineering;anytime algorithm;speedup;mutual information;motion planning;mathematical optimization;heuristics;branch and bound	AI	52.44291952076346	-25.607530761789288	154502
0634be0f68db46be911e47c2fe2e1945659d7a74	beyond lowest-warping cost action selection in trajectory transfer	libraries;trajectory robots libraries mathematical model three dimensional displays robustness optimization;trajectory;three dimensional displays;robots;mathematical model;robustness;optimization;end effector markov decision process max margin q function estimation learning from demonstrations trajectory transfer warping cost action selection;markov processes end effectors learning by example	We consider the problem of learning from demonstrations to manipulate deformable objects. Recent work [1], [2], [3] has shown promising results that enable robotic manipulation of deformable objects through learning from demonstrations. Their approach is able to generalize from a single demonstration to new test situations, and suggests a nearest neighbor approach to select a demonstration to adapt to a given test situation. Such a nearest neighbor approach, however, ignores important aspects of the problem: brittleness (versus robustness) of demonstrations when generalized through this process, and the extent to which a demonstration makes progress towards a goal. In this paper, we frame the problem of selecting which demonstration to transfer as an options Markov decision process (MDP). We present max-margin Q-function estimation: an approach to learn a Q-function from expert demonstrations. Our learned policies account for variability in robustness of demonstrations and the sequential nature of our tasks. We developed two knot-tying benchmarks to experimentally validate the effectiveness of our proposed approach. The selection strategy described in [2] achieves success rates of 70% and 54%, respectively. Our approach performs significantly better, with success rates of 88% and 76%, respectively.	action selection;approximation;automated planning and scheduling;baseline (configuration management);bellman equation;benchmark (computing);cobham's thesis;computer science;dylan;experiment;heart rate variability;ibm notes;markov chain;markov decision process;robot;simulation;eric	Dylan Hadfield-Menell;Alex X. Lee;Chelsea Finn;Eric Tzeng;Sandy H. Huang;Pieter Abbeel	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139644	robot;simulation;computer science;artificial intelligence;trajectory;machine learning;mathematical model;control theory;robustness	Robotics	50.07978280906772	-26.935107960559133	154665
a103db9b10e0bf8d3d58706b1a193ca62d9089bc	autonomous exploration with exact inverse sensor models		This paper is focused on probabilistic occupancy grid mapping and motion planning such that a robot may build a map and explore a target area autonomously in real time. The desired path of the robot is developed in an optimal fashion to maximize the information gain from the sensor measurements on its path, thereby increasing the accuracy and efficiency of mapping, while explicitly considering the sensor limitations such as the maximum sensing range and viewing angle. Most current exploration techniques require frequent human intervention, often developed for omnidirectional sensors with infinite range. The proposed research is based on realistic assumptions on sensor capabilities. The unique contribution is that the mapping and autonomous exploration techniques are systematically developed in a rigorous, probabilistic formulation. The mapping approach exploits the probabilistic properties of the sensor and map explicitly, and the autonomous exploration is designed to maximize the expected map information gain, thereby improving the efficiency of the mapping procedure and the quality of the map substantially. The efficacy of the proposed optimal approach is illustrated by both numerical simulations and experimental results.		Evan Kaufman;Kuya Takami;Taeyoung Lee;Zhuming Ai	2018	Journal of Intelligent and Robotic Systems	10.1007/s10846-017-0710-7	control engineering;occupancy grid mapping;probabilistic logic;motion planning;omnidirectional antenna;viewing angle;engineering;mathematical optimization	Robotics	51.826937742760784	-25.824329437127457	155031
a10eaaa1ddf1ec5e53b1bc8896511765becbb902	the multi-robot coverage problem for optimal coordinated search with an unknown number of robots	robot sensing systems;path planning cooperative systems deterministic algorithms mobile robots multi robot systems;shortest path;time dependent;approximate algorithm;approximation algorithms;path planning;deterministic coverage algorithm multirobot coverage problem optimal coordinated search randomly initialized position minimum coverage time;mobile robots;deterministic algorithms;travel cost;optimal path;cooperative systems;robot sensing systems robot kinematics search problems optimization approximation algorithms;multi robot systems;search cost;optimization;search problems;robot kinematics	This work presents a novel multi-robot coverage scheme for an unknown number of robots; it focuses on optimizing the number of robots and each path cost. Coverage problems traditionally deal with how a given number of robots covers the entire environment. This work, however, presents solutions of not only (i) how to cover the area (locations of interest) within the minimum time, but simultaneously (ii) how to find the optimal number of robots for a given time. Also, we consider the worst but realistic case of all robots starting at the same location instead of assuming randomly initialized positions. The minimum coverage time depends upon the number of robots used. Our research specifies (iii) how to find the minimum coverage time without knowing the number of robots. Finally, we present a deterministic coverage algorithm based on finding the shortest paths in order to optimize the number of robots and corresponding paths.	algorithm;best, worst and average case;randomness;robot;shortest path problem	Hyeun Jeong Min;Nikolaos Papanikolopoulos	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5979995	mobile robot;mathematical optimization;simulation;computer science;artificial intelligence;machine learning;search cost;mathematics;motion planning;shortest path problem;approximation algorithm;robot kinematics	Robotics	53.64481143057449	-24.697055632695616	155101
29faf6ffd4521255a2c66bd2f27ed7a4e7ebf6fc	robot semantic mapping through wearable sensor-based human activity recognition	simultaneous localization and mapping slam;robot sensing systems;wearable computers human robot interaction indoor environment intelligent robots mobile robots pose estimation slam robots;measurement;semantic map;intelligent robots;mobile robot;hidden markov model;slam robot semantic mapping human activity recognition semantic information indoor human robot coexisting environment intelligent mobile robot platform 2d metric map motion data wearable motion sensors activity to furniture type association robot pose estimates simultaneous localization and mapping;learning model;wearable computers;humans semantics robot sensing systems hidden markov models measurement;semantics;mobile robots;human robot interaction;simultaneous localization and mapping slam semantic map human activity recognition wearable sensor;human subjects;semantic mapping;semantic information;hidden markov models;indoor environment;simultaneous localization and mapping;humans;wearable sensor;human activity recognition;slam robots;human activity;pose estimation	Semantic information can help both humans and robots to understand their environments better. In order to obtain semantic information efficiently and link it to a metric map, we present a semantic mapping approach through human activity recognition in an indoor human-robot coexisting environment. An intelligent mobile robot platform can create a 2D metric map, while human activity can be recognized using motion data from wearable motion sensors mounted on a human subject. Combined with pre-learned models of activity-to-furniture type association and robot pose estimates, the robot can determine the distribution of the furniture types on the 2D metric map. Simulations and real world experiments demonstrate that the proposed method is able to create a reliable metric map with accurate semantic information.	activity recognition;experiment;mobile robot;semantic mapper;sensor;simulation;wearable computer	Gang Li;Chun Zhu;Jianhao Du;Qi Cheng;Weihua Sheng;Heping Chen	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6225305	mobile robot;computer vision;simulation;computer science;artificial intelligence;hidden markov model	Robotics	49.89582353002517	-34.855657776882566	155436
82d187016d34b96b0c5fd9108515b699bfa4f120	a novel augmented reality simulator for minimally invasive spine surgery	artificial soft tissues;artificial vertebrae;cement augmentation technique;augmented reality simulator	Cement augmentation techniques cover minimally invasive surgical interventions used to treat painful vertebral compression fractures. These fractures are mainly caused by decreased bone strength as a result of osteoporosis. The current training options for novice surgeons concerning these techniques are insufficient. High costs, specimen availability as well as ethical concerns lead to poor patient safety. Therefore, an augmented reality simulator was developed allowing novice surgeons to train in a safe environment. Key feature of this simulator is the physical patient phantom with artificial vertebrae and soft tissue. Measurements on formalin-fixed as well as fresh-frozen specimens were performed to get a reference. Based on these results, appropriate material compositions were used to imitate the haptic feedback during instrument insertions. The resulting artificial structures showed reaction forces close to their references.	augmented reality	David Fuerst;Marianne Hollensteiner;Andreas Schrempf	2014			simulation;engineering;biological engineering;surgery	HCI	39.97954469806192	-37.33404350881164	155662
44037b6bdb846a49b57fb259d0c0431bc73d41b7	wave-aware trajectory planning for unmanned surface vehicles operating in congested environments		Many practical operations like search and rescue operations are time-sensitive missions. In this paper, we present a wave-aware deliberative planner that avoids collisions with traffic vessels and also opportunistically traverses wavefields generated by these vessels while minimizing the execution time and the risk of failure. The planner performs a search over a 4D pose-time lattice to generate a collision-free, minimum-risk sequence of motion goals. It addresses motion uncertainty, failure risk and perception uncertainty. We also present heuristics and adaptive motion goals to speed-up computation. We present simulation results showing that our wave-aware planner produces plans that execute faster compared to the typical reactive planning scheme. We also present results from physical experiments showing the feasibility of the proposed approach.	adaptive sampling;c++;computation;experiment;graphics processing unit;heuristic (computer science);matlab;monte carlo method;multi-core processor;reactive planning;risk aversion;run time (program lifecycle phase);sampling (signal processing);simulation;unmanned aerial vehicle;vii	Pradeep Rajendran;Travis Moscicki;Jared Wampler;Brual C. Shah;Karl von Ellenrieder;Satyandra K. Gupta	2018	2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)	10.1109/SSRR.2018.8468656	simulation;computation;reactive planning;heuristics;trajectory;computer science	Robotics	53.285373415690486	-25.42756182106437	156090
505a551585b5809506e292340153a79d8f6d1810	a haptic interface for interventional radiology	radiology;interventional radiology;instruments;minimal invasive surgery;x ray imaging;haptic interfaces radiology instruments minimally invasive surgery virtual reality force feedback catheters computational modeling biomedical imaging medical diagnostic imaging;force feedback devices computer assisted surgery simulator interventional radiology haptic interface;degree of freedom;virtual reality;biomedical imaging;force feedback;computational modeling;catheters;minimally invasive surgery;computer assisted surgery simulator;haptic interfaces;medical diagnostic imaging;computer assisted surgery;haptic interface;force feedback devices	Low trauma, reduced costs, fast recovery are few factors why minimally invasive surgery (MIS) is taking over classical surgical methods. Interventional Radiology (IR) is a MIS technique where thin tubular instruments are steered through the patient’s vascular system under the X-ray imaging. These procedures demand highly trained and experienced specialists. A computer-based haptic interface is proposed as a quality training environment. The system is composed of a virtual reality (VR) environment and force feedback units for the surgical instruments. This paper describes the 4 degrees of freedom (DOFs) haptic interface that has been developed for this purpose.	haptic technology;minimally invasive education;radiography;radiology;virtual reality	Dejan Ilic;Thomas Moix;Blaise Fracheboud;Hannes Bleuler;Ivan Vecerina	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570559	interventional radiology;simulation;computer science;artificial intelligence;virtual reality;biological engineering;haptic technology	Robotics	39.54455939753748	-37.94515568262788	156117
d9f78012d1f65f04f6709243865a92252eeb625b	face detection algorithm using haar-like feature for gpu architecture	gpu opencv haar like features cuda;resource allocation;gpu threads face detection algorithm haar like feature gpu architecture parallel algorithm opencv library computational structure scheduling algorithm workload balancing;face recognition;scheduling face recognition graphics processing units haar transforms object detection parallel algorithms resource allocation;scheduling;graphics processing units;graphics processing units face detection classification algorithms instruction sets real time systems kernel feature extraction;haar transforms;object detection;parallel algorithms	This article describes parallel algorithm of face detection on images for GPU architecture. This algorithm is an extension of an algorithm from OpenCV library. A computational structure is presented for the developed algorithm. Also, scheduling algorithm was developed to balance a workload among GPU's threads.	central processing unit;computation;face detection;graphics processing unit;haar wavelet;mathematical optimization;opencv;parallel algorithm;scheduling (computing)	Dmitry Pertsau;Andrey A. Uvarov	2013	2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS)	10.1109/IDAACS.2013.6663020	facial recognition system;parallel computing;resource allocation;computer science;viola–jones object detection framework;theoretical computer science;operating system;parallel algorithm;scheduling;computer graphics (images)	Robotics	43.08777820737448	-36.53886285862098	156501
af2aae9f60514372cb95e95d565748296790350d	conquering the storage pyramid - implementing the dynamic library			dynamic linker	Daniel Kaberon;Henry Steinhauer	1989			pyramid;computer graphics (images);engineering	PL	42.73240425149388	-31.89599247398793	156662
f25cee32f1df2b36ea560df8334f69d5ccd2c451	an ultra-low-power contrast-based integrated camera node and its application as a people counter	silicon;wide dynamic range;ultra low power;video streaming;image segmentation;image processing;people counter;video segmentation contrast based integrated camera node people counter binary cmos imaging sensor dynamic range wireless camera network flash based fpga processor data readout image processing self standing camera node feature extraction markov chain video stream;pixel cameras markov processes image edge detection silicon field programmable gate arrays radiation detectors;radiation detectors;wireless camera network;video segmentation;cmos image sensors;cmos image sensor;flash based fpga processor;image edge detection;feature extraction;pixel;field of view;low power electronics;dynamic range;self standing camera node;data readout;cross validation;markov processes;binary cmos imaging sensor;field programmable gate arrays;camera network;video stream;low power consumption;contrast based integrated camera node;object detection cmos image sensors feature extraction field programmable gate arrays image segmentation image sequences low power electronics markov processes;cameras;object detection;image sequences;markov chain	We describe the implementation in a self-standing systemof a novel contrast-based binary CMOS imaging sensor.This sensor is characterized by very low power consumptionand wide dynamic range, which makes it attractive forwireless camera network applications. In our implementation,the sensor is interfaced with a Flash-based FPGA processor,which handles data readout and image processing.This self-standing camera node is configured as a system forcounting persons walking through a corridor. Simple featuresare extracted from each image in a video stream at 30fps. A classifier is designed based on the temporal evolutionof these features, which is modeled as a Markov chain. Thevideo stream is then segmented into intervals correspondingto individual persons crossing through the field of view. Experimentalresults are shown in cross-validated tests overreal sequences acquired by the camera.	adobe flash;algorithm;cmos;data acquisition;dynamic range;field-programmable gate array;flash memory;image processing;image sensor;low-power broadcasting;markov chain;people counter;statistical model;streaming media;transceiver	Leonardo Gasparini;Roberto Manduchi;Massimo Gottardi	2010	2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2010.26	embedded system;computer vision;camera auto-calibration;markov chain;dynamic range;field of view;image processing;feature extraction;computer science;image sensor;image segmentation;markov process;silicon;particle detector;pixel;cross-validation;field-programmable gate array;low-power electronics;statistics	Robotics	45.3068942269173	-36.08393840021268	157132
0def99e6c436c9eca2efc81fb7723b238aad3435	hybrid parallel bundle adjustment for 3d scene reconstruction with massive points	gpu;compute unified device architecture;sparse bundle adjustment;structure from motion	Bundle adjustment (BA) is a crucial but time consuming step in 3D reconstruction. In this paper, we intend to tackle a special class of BA problems where the reconstructed 3D points are much more numerous than the camera parameters, called Massive-Points BA (MPBA) problems. This is often the case when high-resolution images are used. We present a design and implementation of a new bundle adjustment algorithm for efficiently solving the MPBA problems. The use of hardware parallelism, the multi-core CPUs as well as GPUs, is explored. By careful memory-usage design, the graphic-memory limitation is effectively alleviated. Several modern acceleration strategies for bundle adjustment, such as the mixed-precision arithmetics, the embedded point iteration, and the preconditioned conjugate gradients, are explored and compared. By using several high-resolution image datasets, we generate a variety of MPBA problems, with which the performance of five bundle adjustment algorithms are evaluated. The experimental results show that our algorithm is up to 40 times faster than classical Sparse Bundle Adjustment, while maintaining comparable precision.	3d reconstruction;algorithm;bundle adjustment;business architecture;central processing unit;conjugate gradient method;embedded system;experiment;fermi (microarchitecture);graphics processing unit;hardware acceleration;hybrid system;image resolution;iteration;jacobi method;multi-core processor;parallel computing;run time (program lifecycle phase);sparse	Xin Liu;Wei Gao;Zhanyi Hu	2012	Journal of Computer Science and Technology	10.1007/s11390-012-1303-3	mathematical optimization;structure from motion;computer science;theoretical computer science;bundle adjustment;algorithm	Vision	41.31722785363318	-32.85920141944676	157718
075de767411566221af49458bb58aad8151bf335	maneuver-adaptive multi-hypothesis tracking for active sonar systems		In undersea surveillance, active sonar systems are commonl y used to detect submarines. These sonar systems allow high detection r anges, but the interaction of sound with the sea bottom may lead to a high number of false a larms as well, especially in shallow-water environments. Therefore, autom atic detection and tracking procedures are needed to provide helpful assistance to sona r operators. The MultiHypothesis Tracking approach presented in this paper is one of these procedures. It is based on nonlinear Kalman Filtering. In Kalman Filtering the assumption on underlying target dyn amics is essential and has considerable impact on the overall tracking performance. A s targets usually maneuver, their dynamics are varying and hidden. To include varia ble target dynamics, a Multi-Hypothesis tracking algorithm is adapted to conside r target maneuvers by estimating and adjusting the process-noise level in the Kalma n Filter equations. The level of process noise is determined for every track hypothe sis individually based on the estimated velocities of the target. The impact on the tra cking result is shown by applying the presented approach to different multistatic s onar datasets and comparing it to results gained by tracking with one global level of proc ess noise. Tracking results are quantified by several tracking-performance metrics.	algorithm;kalman filter;noise (electronics);nonlinear system;sonar (symantec)	Kathrin Seget;Arne Schulz;Ulrich Heute	2010			operator (computer programming);computer vision;kalman filter;sonar;marine mammals and sonar;nonlinear system;artificial intelligence;computer science	Robotics	49.62020511959906	-32.63595749796106	157961
96040168b896637258fb1b09f445b8d0de53eca2	computation of mutual information metric for image registration on multiple gpus	juser;websearch;publications database	Because of their computational power, GPUs are widely used in the field of image processing. Registration of brain images has already been successfully accelerated with GPUs, but registration of high-resolution human brain images presents new challenges due to large amounts of data and images not fitting in the memory of a single device.	computation;graphics processing unit;image registration;mutual information	Andrew V. Adinetz;Jiri Kraus;Markus Axer;Marcel Huysegoms;Stefan Köhnen;Dirk Pleiter	2013		10.1007/978-3-642-54420-0_21	computer science;theoretical computer science;machine learning;data mining	Vision	40.7398470187734	-34.195108150662875	158571
9c5c5de6bd7711da8e66ebdc18922103dda4feb6	real-time terrain classification for rescue robot based on extreme learning machine		Full autonomous robots in urban search and rescue (USAR) have to deal with complex terrains. The real-time recognition of terrains in front could effectively improve the ability of pass for rescue robots. This paper presents a real-time terrain classification system by using a 3D LIDAR on a custom designed rescue robot. Firstly, the LIDAR state estimation and point cloud registration are running in parallel to extract the test lane region. Secondly, normal aligned radial feature (NARF) is extracted and downscaled by a distance based weighting method. Finally, an extreme learning machine (ELM) classifier is designed to recognize the types of terrains. Experimental results demonstrate the effectiveness of the proposed system.	real-time transcription;rescue robot	Yuhua Zhong;Junhao Xiao;Huimin Lu;Hui Zhang	2016		10.1007/978-981-10-5230-9_38	urban search and rescue;point cloud;terrain;robot;extreme learning machine;computer vision;classifier (linguistics);machine learning;weighting;artificial intelligence;computer science;rescue robot	Robotics	49.46267860318835	-36.066045001537255	159126
bb77851512c4a6ebc34e1574925b86e88d57aa19	flyar: augmented reality supported micro aerial vehicle navigation	automatic flight path planning;autonomous flying;high resolution cameras;2d map positions;vehicles cameras three dimensional displays visualization navigation data visualization robots;space vehicles aerospace computing augmented reality collision avoidance image sensors;spatial relationship;visualization techniques flyar augmented reality micro aerial vehicle navigation high resolution cameras aerial reconstructions automatic flight path planning autonomous flying aerial vehicle position 2d map positions physical environment flight planning process flight planning spatial relationship virtual waypoints;image sensors;flyar;aerial reconstructions;navigation;visualization;visualization techniques;flight planning process;aerospace computing;three dimensional displays;micro aerial vehicle navigation;robots;data visualization;aerial vehicle position;physical environment;collision avoidance;vehicles;augmented reality;micro aerial vehicles;flight planning;cameras;virtual waypoints;space vehicles	Micro aerial vehicles equipped with high-resolution cameras can be used to create aerial reconstructions of an area of interest. In that context automatic flight path planning and autonomous flying is often applied but so far cannot fully replace the human in the loop, supervising the flight on-site to assure that there are no collisions with obstacles. Unfortunately, this workflow yields several issues, such as the need to mentally transfer the aerial vehicle's position between 2D map positions and the physical environment, and the complicated depth perception of objects flying in the distance. Augmented Reality can address these issues by bringing the flight planning process on-site and visualizing the spatial relationship between the planned or current positions of the vehicle and the physical environment. In this paper, we present Augmented Reality supported navigation and flight planning of micro aerial vehicles by augmenting the user's view with relevant information for flight planning and live feedback for flight supervision. Furthermore, we introduce additional depth hints supporting the user in understanding the spatial relationship of virtual waypoints in the physical world and investigate the effect of these visualization techniques on the spatial understanding.	aerial photography;area striata structure;augmented reality;automated planning and scheduling;autonomous car;autonomous robot;body dysmorphic disorders;depth perception;doxorubicin/melphalan/teniposide regimen (mav);drug vehicle;estimated;global positioning system;graphical user interface;gray platelet syndrome;image resolution;imagery;inspiration function;interactivity;interface device component;motion planning;navigation;physical object;shadowing (histology);unmanned aerial vehicle;usability testing;video tracking;waypoint;collision	Stefanie Zollmann;Christof Hoppe;Tobias Langlotz;Gerhard Reitmayr	2014	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2014.24	spatial relation;robot;computer vision;augmented reality;navigation;simulation;visualization;computer science;image sensor;creative visualization;data visualization;statistics;computer graphics (images)	Visualization	51.01582892963353	-35.99208928032649	159292
e33ef0cc88ab40e0dbf57688730d3c2d69e3fe38	parameter estimation for target tracking with uncertain sensor positions	passive sensor;parameter estimation target tracking acceleration acoustic sensors radar tracking trajectory sensor arrays mathematical model geometry passive radar;radar tracking;geometry;maximum likelihood estimation;acceleration;maximum likelihood estimation target tracking parameter estimation;uncertain position;trajectory;mathematical model;map estimation;passive radar;parameter estimation;maneuvering acoustic target trajectory parameter estimation target tracking uncertain position passive sensor maximum a priori algorithm;acoustic sensors;target tracking;maneuvering acoustic target trajectory;maximum a priori algorithm;sensor arrays	This paper investigates the problem of estimating parameters necessary to predict the trajectory of a maneuvering acoustic target using passive sensors of uncertain position. A batch oriented maximum a-priori (MAP) estimation scheme is used to determining both the sensor movement and the target trajectory from the data. A typical simulation is presented, and the results show that the algorithm permits estimation of the sensor positions along with that of the target trajectory.		R. Barsanti;M. Tummala	2001		10.1109/ISCAS.2001.921056	acceleration;computer vision;mathematical optimization;radar tracker;trajectory;passive radar;mathematical model;control theory;mathematics;maximum likelihood;estimation theory;statistics	Robotics	52.034458025081115	-34.401538062380894	159359
54a9e174b0dbc73399a213979ca85ab26eb6a1f0	self-localization of intelligent vehicles based on environmental contours		High-precision and robust self-localization is one of basic requirements for intelligent vehicles and related applications, because it provides necessary location information for path planning, behavioral decisions. In this paper a localization method based on IBEO LUX scanners, vehicle sensors and priori maps is proposed. The environmental contour features such as trees, street lamps, green belts and building outlines which are fused by the laser scanners and vehicle information are served as localization information. These features are associated with priori feature maps and the optimal vehicle position estimate is obtained by the Monte Carlo Localization framework. Experimental results show that the mean lateral error is less than 10cm and the mean longitudinal error is less than 20cm. So the localization algorithm introduced can meet the requirements of automatic driving demand.		Jian Fang;Zhuping Wang;Hao Zhang;Wenhao Zong	2018	2018 3rd International Conference on Advanced Robotics and Mechatronics (ICARM)	10.1109/ICARM.2018.8610687	computer vision;motion planning;artificial intelligence;monte carlo localization;computer science	Robotics	51.28934689737668	-36.96048853764679	159662
c780de93616d22553d3b67fb86f255f21cd523dc	live demonstration: a hardware system for emulating the early vision utilizing a silicon retina and spinnaker chips	bioelectric phenomena;eye;neurophysiology;parallel processing;vision;spinnaker chips;hardware retina emulator;hardware system;multiple parallel processing;neural activities;personal computer display;real-time emulator;silicon retina;simulated spikes;visual cortex;spinnaker;real time;retina;simulation	In this live demonstration, we will show a real-time emulator for reproducing neural activities in the retina and the visual cortex. The emulator comprises a hardware retina emulator and SpiNNaker chips. Taking advantages of multiple parallel processing techniques, the emulator generates simulated spikes with 1 ms precision. Visitors can observe simulated neural response displayed on a personal computer display.	computer monitor;emulator;parallel computing;personal computer;real-time clock;simulated annealing;spinnaker	Takumi Kawasetsu;Ryoya Ishida;Tadashi Sanada;Hirotsugu Okuno	2014	2014 IEEE Biomedical Circuits and Systems Conference (BioCAS) Proceedings	10.1109/BioCAS.2014.6981691	computer vision;engineering;optics	EDA	45.71362592144519	-31.634889944340742	159727
49868260fafd6691ac3ee28bb16169524cc600b7	an evaluation of sampling path strategies for an autonomous underwater vehicle	autonomous underwater vehicle;sampling methods autonomous underwater vehicles mobile robots path planning;evaluation function;systematics;path planning;mobile robots;estimating equation;estimation;energy consumption;autonomous underwater vehicles;energy consumption systematics mathematical model spirals estimation equations vehicles;spirals;mathematical model;adaptive sampling;scalar field;sampling densities autonomous underwater vehicle auv sampling path planning problem scalar field estimation stored energy capacity utilization adaptive sampling approaches real time environmental data a priori environmental data cost evaluation function utilization systematic spiral sampling path strategy;vehicles;experimental evaluation;sampling methods	A critical problem in planning sampling paths for autonomous underwater vehicles is balancing obtaining an accurate scalar field estimation against efficiently utilizing the stored energy capacity of the sampling vehicle. Adaptive sampling approaches can only provide solutions when real-time and a priori environmental data is available. Through utilizing a cost-evaluation function to experimentally evaluate various sampling path strategies for a wide range of scalar fields and sampling densities, it is found that a systematic spiral sampling path strategy is optimal for high-variance scalar fields for all sampling densities and low-variance scalar fields when sampling is sparse. The random spiral sampling path strategy is found to be optimal for low-variance scalar fields when sampling is dense.	adaptive sampling;autonomous robot;evaluation function;experiment;nyquist–shannon sampling theorem;real-time locating system;sampling (signal processing);sparse matrix	Colin Ho;Andres Mora;Srikanth Saripalli	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6225231	control engineering;mobile robot;sampling;scalar field;mathematical optimization;estimation;simulation;computer science;artificial intelligence;evaluation function;mathematical model;mathematics;motion planning;systematics;statistics;estimating equations;spiral	Robotics	52.0360522964484	-25.706710655510445	159999
1a2c16d34363bb9e7be2ef9317585e073421eb3a	compression of the images of ancient arab manuscript documents based on segmentation	graphics blocks;document image segmentation;compression wavelet image historical document color image segmentation;image coding;image segmentation;data compression;graphics blocks image compression ancient arab manuscript documents characteristic extractor wavelet transform statistical characteristics document image segmentation image classification;historical document;image classification;image;ancient arab manuscript documents;wavelet transforms data compression document image processing image classification image coding image segmentation statistical analysis;wavelet transforms;wavelet transform;statistical analysis;image compression;statistical characteristics;document image processing;compression;image coding image segmentation pixel graphics wavelet transforms color machine intelligence ink moisture writing;characteristic extractor;wavelet;color image segmentation	This paper presents our contribution for the compression of images of old Arabic manuscripts. Our method of compression is based on the segmentation of the images in three blocks: text, graphics and background, then each block will undergo a different compression method. We present two parts: first, while developing a characteristic extractor based on the wavelet transform and statistical characteristics, we propose an algorithm of segmentation based on the classification of old documents images. Second, we present the education and the implementation of a compression method by using the wavelet transform. This method comprises three steps: the compression of the text blocks, the background blocks, and the graphics blocks.	algorithm;graphics;randomness extractor;wavelet transform	Walid Elloumi;Mohamed Chakroun;Moncef Charfi;Adel M. Alimi	2008	2008 IEEE/ACS International Conference on Computer Systems and Applications	10.1109/AICCSA.2008.4493634	computer vision;computer science;pattern recognition;texture compression;statistics;wavelet transform;computer graphics (images)	Vision	39.48106646228191	-35.36760076936634	160017
247f2a8e762d008441e44110f94c1936a1d63cc8	real-time 3d rendering processor for 2d-to-3d conversion of stereoscopic displays	image resolution;full hd resolution real time 3d rendering processor 2d to 3d conversion stereoscopic display consumer electronic product 3d display technology 3d camera 3d projector 3d tv 2d video content fractional precision image warping hole filling architecture;consumer electronics;three dimensional displays;stereo image processing;visual perception consumer electronics image resolution stereo image processing three dimensional displays;visual perception;three dimensional displays computer architecture rendering computer graphics filling real time systems streaming media engines	Since consumer electronic products grow rapidly, 3D display technology becomes popular in recent years. Now, there are more and more 3D products such as 3D camera, 3D projector, and 3D-TV. However, how to convert traditional 2D video contents to 3D one is a vital issue to solve it. This paper proposes a real-time 3D rendering processor for view synthesis in 2D-to-3D conversion. Moreover, the proposed fractional precision image warping and hole filling architecture can effectively decrease the output buffer size about 94.4% than the traditional. The real-time processor throughput can achieve up to 60 frames per second and full HD resolution (1920×1080) on stereoscopic displays.	2d to 3d conversion;3d rendering;3d television;display device;image warping;real-time clock;real-time transcription;stereo camera;stereoscopy;throughput;video projector;view synthesis	Yeong-Kang Lai;Yu-Chieh Chung	2014	2014 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2014.6775933	computer vision;image resolution;visual perception;computer science;multimedia;computer graphics (images)	Visualization	43.46365198189695	-33.782008856928485	160018
8f2e6b6c6235034b9d7d7018758d5fe1691641c2	low complex, programmable fpga based 8-channel ultrasound transmitter for medical imaging researches	ultrasonic imaging;transducers;biomedical imaging;arrays;electricity and electronics;electricity;field programmable gate arrays;delays	In commercial ultrasound systems, the transmit module typically generates the time delayed excitation pulses to steer and focus the acoustic beam. However, the ultrasound transmitter module in these systems has limited access to medical ultrasound researchers. In this paper, we have presented the development of a programmable architecture for 8-channel ultrasound transmitter for medical ultrasound research activities. The proposed architecture consists of 8 transmit channels and Field Programmable Gate Array (FPGA) based configurable delay profile to steer acoustic beam, transmit frequency and pulse pattern length depending on the medical application. Our system operates in pulse-echo mode, with ultrasound transmit frequency up to 20 MHz, excitation voltage up to 100 Vpp, and individual channel control with single high speed Serial Peripheral Interface (SPI). Pre-calculated delay profiles per scanline are generated in Matlab, based on physical parameters of 8 element linear transducer array which are used to steer and focus the ultrasound beam. An experiment is carried with our transmit module to transmit ultrasound into gelatin phantom, acquired echoes and processed for B-mode imaging. The results show that this transmit platform can be used for ultrasound imaging researches and also for medical diagnosis.	acoustic cryptanalysis;advanced transportation controller;algorithm;beamforming;clock rate;computational human phantom;field-programmable gate array;image quality;instruction unit;matlab;mathematical optimization;medical imaging;medical ultrasound;phantom reference;pradeep tapadiya;primitive recursive function;principle of good enough;propagation constant;prototype;scan line;secure multi-party computation;serial peripheral interface bus;signal processing;transducer;transmitter;user interface	Chandrashekar Dusa;Pachamuthu Rajalakshmi;Suresh Puli;Uday B. Desai;S. N. Merchant	2014	2014 IEEE 16th International Conference on e-Health Networking, Applications and Services (Healthcom)	10.1109/HealthCom.2014.7001850	electronic engineering;telecommunications;engineering;electrical engineering	EDA	43.9628351108627	-29.95869437301867	160102
1d26b3aec3cf29bccb4ee64732116374402d5560	robust view matching-based markov localization in outdoor environments	object recognition;support vector machines markov processes navigation path planning;support vector machines;path planning;training;markov localization;windows;support vector machine view based localization markov localization outdoor navigation;navigation;outdoor navigation markov localization support vector machine view based localization;view based localization;parameter tuning;image edge detection;local features;seasonality;visual features;markov processes;support vector machine;meteorology;buildings;outdoor navigation;object model;support vector machines training object recognition meteorology windows buildings image edge detection	This paper describes a view-based localization method in outdoor environments. An important issue in view-based localization is to cope with the change of object views due to changes of weather and seasons. We have developed a two-stage SVM-based localization method which exhibits a high localization performance with few parameter tunings. In this paper, we extend the method in the following two ways: (1) adding new object models and visual features to deal with various urban scenes and (2) introducing a Markov localization strategy to utilize the history of movements. The new method can achieve a 100% localization performance in an urban route under a wide variety of conditions. The comparison with local feature-based methods is also discussed.	markov chain;robot	Jun Miura;Koshiro Yamamoto	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4650607	support vector machine;computer vision;simulation;computer science;machine learning;mathematics;statistics	Robotics	50.722313875381566	-37.35398076080628	160204
bf7ba8eec1ed2056d244972cb3237a1c9732ef08	a hough transform system based on neural networks	digital circuit;image recognition;vision ordenador;neural nets;implementation;image recognition hough transform system neural networks image to parameter space mapping modified hopfield optimization network autonomous navigation tracking multiple targets curve following mensuration;neural networks sensor arrays sensor phenomena and characterization computer networks concurrent computing equations machine vision noise robustness degradation artificial neural networks;circuito analogico;peak detection;neural nets hough transforms;transformacion hough;detecteur crete;computer vision;circuit numerique;input output interface;ejecucion;analog circuit;operational amplifier;interface entree sortie;parameter space;circuito numerico;interfase entrada salida;hough transforms;vision ordinateur;hough transformation;autonomous navigation;hough transform;transformation hough;reseau neuronal;red neuronal;circuit analogique;neural network	Neural-like analog circuitry is suggested for image-to-parameter-space mapping, and a modified Hopfield optimization network is proposed for the parameter space peak detection. Solution time under 50 mu s is obtainable with general-purpose operational amplifiers. Example system applications include autonomous navigation, tracking multiple targets, curve following, mensuration, and image recognition. >	artificial neural network;hough transform	Gary L. Dempsey;Eugene S. McVey	1992	IEEE Trans. Industrial Electronics	10.1109/41.170971	hough transform;computer vision;computer science;artificial intelligence;machine learning;artificial neural network	Robotics	45.152410174905874	-32.43404337303446	160309
fed3f3b4bf301512d7d7d499f22d620fd049e3f7	filtering data based on human-inspired forgetting	robot sensing systems;signal strength;cognitive science;intelligent robots;mobile robot;mobile robots;mobile robots data filtering human inspired forgetting method episodic data actsimple forgetting algorithm robotic systems wi fi signal strength estimation task wireless fidelity;wireless lan control engineering computing data handling mobile robots;dynamic environment;cognitive science intelligent robots mobile robots filtering algorithms robot sensing systems algorithm design and analysis;filtering algorithms;robot sensing cognitive science intelligent robots mobile robots;control engineering computing;wireless lan;data handling;algorithm design;algorithm design and analysis;robot sensing	Robots are frequently presented with vast arrays of diverse data. Unfortunately, perfect memory and recall provides a mixed blessing. While flawless recollection of episodic data allows increased reasoning, photographic memory can hinder a robot's ability to operate in real-time dynamic environments. Human-inspired forgetting methods may enable robotic systems to rid themselves of out-dated, irrelevant, and erroneous data. This paper presents the use of human-inspired forgetting to act as a filter, removing unnecessary, erroneous, and out-of-date information. The novel ActSimple forgetting algorithm has been developed specifically to provide effective forgetting capabilities to robotic systems. This paper presents the ActSimple algorithm and how it was optimized and tested in a WiFi signal strength estimation task. The results generated by real-world testing suggest that human-inspired forgetting is an effective means of improving the ability of mobile robots to move and operate within complex and dynamic environments.	algorithm;architecture as topic;cognitive architecture;filter (signal processing);flaw hypothesis methodology;inspiration function;large;mathematical optimization;memory disorders;mental recall;mobile robot;population parameter;reading (activity);real-time web;reasoning;relevance;robot (device);simulation;theory;virtual reality	Sanford T. Freedman;Julie A. Adams	2011	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2011.2157142	mobile robot;algorithm design;simulation;computer science;artificial intelligence;machine learning	Robotics	48.669235060559814	-29.81012982430372	160418
ebef5cd34a4ca76a32cdee38126de04b00f3054d	a comprehensive 3-dimensional random mobility modeling framework for airborne networks		To design and evaluate airborne networks (ANs), it is crucial to utilize random mobility models (RMMs) that capture the physical movement patterns of different aerial vehicles in real scenarios. Compared to expensive flight field tests, RMM-based modeling, simulation, and emulation is cost-effective with a large set of RMM-generated flight trajectories. Despite the importance of RMMs, we notice that most existing models focus on the 2-D movement, and do not consider the temporal and 3-D spatial correlation of aerial mobility patterns. In this paper, we propose a comprehensive 3-D smooth turn (ST) modeling framework for fixed-wing aircraft, which can serve as a design and evaluation foundation for future ANs. In the proposed framework, we develop two realistic 3-D ST RMMs that capture the diverse mobility patterns of fixed-wing aircraft, through coupling stochastic forcing with physical laws that govern the 3-D aerial maneuvers. We also develop two boundary models to determine the movement of aerial vehicles when they approach simulation boundaries. Moreover, we propose an approach to estimate the optimal 3-D ST RMMs, with which we can produce rich trajectory ensembles with statistical mobility patterns that match with the real trajectory data.	aerial photography;airborne ranger;emulator;simulation	Junfei Xie;Yan Wan;Baoqian Wang;Shengli Fu;Kejie Lu;Jae Heung Kim	2018	IEEE Access	10.1109/ACCESS.2018.2819600	simulation;spatial correlation;aerodynamics;atmospheric model;trajectory;computer science;solid modeling;distributed computing;mobility model	Vision	46.831156278533825	-28.108353069838504	160487
134464a66fb2dfe3401d98dd5ffd11401b70eb7e	localization in urban environments by matching sensor data to map information	sensors;inertial measurement unit urban environments sensor data outdoor localization algorithm wheelchairs walkers gps map information odometry monte carlo particle filter kalman filter;robot sensing systems global positioning system roads vehicles estimation wheelchairs;distance measurement;global positioning system;vehicles;vehicles distance measurement global positioning system monte carlo methods sensors;monte carlo methods	This paper presents an outdoor localization algorithm for assistive devices such as wheelchairs or walkers in urban environments. By fusing GPS, map information, and odometry with the help of a Monte Carlo particle filter, we provide adequate pose estimates for the implementation of device - specific navigation systems. We demonstrate the robustness and precision of the presented solution by experimental test runs in a municipal scenario, and compare the achieved results against a Kalman filter based localizer that integrates odometry and rate of turn data coming from a sophisticated inertial measurement unit. The core contribution of this work is given by the extension of commonly used map matching techniques in the sense that we not only evaluate the road network, but also different kinds of mapped entities representing obstacles for the vehicle.	algorithm;assistive technology;cluster analysis;computation;converge;discrepancy function;entity;global positioning system;kalman filter;map matching;minimum bounding rectangle;modality (human–computer interaction);monte carlo method;norm (social);odometry;openstreetmap;particle filter;sensor	Christian Mandel;Oliver Birbach	2013	2013 European Conference on Mobile Robots	10.1109/ECMR.2013.6698819	embedded system;computer vision;simulation;global positioning system;sensor;statistics;monte carlo method	Robotics	53.68380108024449	-35.883336012534365	160664
e06d40f8763b6e59824688f3b9ff11e8ccec0d8b	3d object recognition using a mobile laser range finder	sweeping mirror;robot sensing systems;mobile laser range finder;optimum sensor placement;object recognition;sensor phenomena and characterization;intelligent robots;mobile robots;data mining;3d object recognition;ccd image sensors;distance measurement;object recognition intelligent sensors sensor phenomena and characterization robot sensing systems intelligent robots robot vision systems cameras mobile robots laser modes data mining;laser range finder;optical radar;sensor placement;mobile 3d range sensor;pattern recognition;pattern recognition ccd image sensors laser beam applications;laser beam applications;ccd image sensor;robot vision systems;laser modes;intelligent sensors;cameras;sweeping mirror ccd image sensor pattern recognition optimum sensor placement distance measurement optical radar mobile laser range finder 3d object recognition mobile 3d range sensor	Presents a 3D object recognition system with emphasis on an optimum placement strategy for a mobile 3D range sensor. The theoretical aspects leading to the development of the 3D sensor placement paradigm are presented and described. The authors use a triangulation type laser of range finder in which a CCD based imaging device is utilized to view a plane of laser light bounced from a sweeping mirror. The sensor characteristics are described and modeled. A set of criteria for use in constraining the possible sensor site location for future scans is presented. These constraint conditions are essential to drive the actual sensor placement algorithm for 3D object recognition. >	3d single-object recognition;outline of object recognition	Ren C. Luo;Ralph S. Scherp	1990		10.1109/IROS.1990.262482	mobile robot;computer vision;electronic engineering;computer science;engineering;cognitive neuroscience of visual object recognition;image sensor;remote sensing;intelligent sensor	Robotics	52.659159640963416	-34.083101273381516	161182
fb3b12ae494acea5c680df28bf06b3d6230237c3	robot learning	robot learning	Robot learning consists of a multitude of machine learning approaches, particularly reinforcement learning, inverse reinforcement learning, and regression methods, that have been adapted su ciently to domain so that they allow learning in complex robot systems such as helicopters, apping-wing ight, legged robots, anthropomorphic arms and humanoid robots. While classical arti cial intelligence-based robotics approaches have often attempted to manually generate a set of rules and models that allows the robot systems to sense and act in the real-world, robot learning centers around the idea that it is unlikely that we can foresee all interesting real-world situations su ciently accurate. Hence, the eld of robot learning assumes that future robots need to be able to adapt to the real-world, and domain-appropriate machine learning might o er the most approach in this direction.	coat of arms;humanoid robot;machine learning;reinforcement learning;robot learning;robotics	Jan Peters;Russ Tedrake;Nick Roy;Jun Morimoto	2017		10.1007/978-1-4899-7687-1_738	mobile robot;robot learning;tico robot;social robot;robot control	AI	49.93277279710377	-27.419043384110978	161676
4656e7845ed3a9bd1d14149dc5e74b7d75af48a4	high-resolution lidar using random demodulation		Recently emerging applications, such as autonomous navigation, mapping, and home entertainment, have increased the demand for inexpensive and high quality depth sensing. In this paper we fundamentally re-examine the problem, considering recent advances in photoelectric devices, increased availability of fast electronics, reduced computation cost, and developments in sensing theory. Our main contribution is a real-time hardware architecture for time-of- flight (ToF) depth sensors that exploits random modulation to significantly reduce the acquisition burden. The proposed design is able to acquire compressive, critical, or redundant measurements, without requiring any hardware modifications, at the expense of small reduction in the system frame rate. The architecture we propose is sufficiently flexible to be operable in a variety of conditions and with a variety of reconstruction algorithms.		Petros Boufounos	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451424	iterative reconstruction;computer vision;computer hardware;computation;modulation;frame rate;hardware architecture;electronics;artificial intelligence;computer science;lidar;demodulation	Robotics	46.846158094180794	-33.09355721407528	161857
bfaf28b5d1544072fa49a0a3d0449c945c684f9c	sensor registration for robotic applications	prior knowledge;multi sensor data fusion;mutual information;conference proceeding	Multi-sensor data fusion plays an essential role in most robotic applications. Appropriate registration of information from different sensors is a fundamental requirement in multi-sensor data fusion. Registration requires significant effort particularly when sensor signals do not have direct geometric interpretations, observer dynamics are unknown and occlusions are present. In this paper, we propose Mutual Information (MI) based sensor registration which exploits the effect of a common cause in the observed space on the sensor outputs that does not require any prior knowledge of relative poses of the observers. Simulation results are presented to substantiate the claim that the algorithm is capable of registering the sensors in the presence of substantial observer dynamics.	algorithm;computation;image sensor;mobile robot;mutual information;requirement;robot;sensor;signal-to-noise ratio;simulation;the australian;tree accumulation	Alen Alempijevic;Sarath Kodagoda;Gamini Dissanayake	2007		10.1007/978-3-540-75404-6_22	computer vision;simulation;soft sensor;computer science;data mining;mutual information	Robotics	53.191288526497246	-35.950955318954634	161873
ac820482a2d2c498b617857cb2f374216967476f	a deep q network for robotic planning from image		In this paper, the Deep Reinforcement Learning (DRL) is proposed to address a planning issue which is different from the traditional SLAM algorithm. It is an end-to-end trainable framework in which we do not need to build a map for the agent in advance. We apply a Q-CNN model whose policy is to combine the Convolutional Neural Network (CNN) and the Q functions. The raw images are used as inputs and outputs action directly. To achieve the task, we develop a simulation environment in Gazebo, which provides indoor 3D scenes with physics engine and high-quality pictures. In the environment, agent can be controlled conveniently and interacted with objects freely. Our experiment is to find the shortest trajectories from the initial location to the target. After iterative training, we have trained a set of parameters to approximate Q function well and made it have a better performance.	approximation algorithm;automated planning and scheduling;convolutional neural network;driven right leg circuit;end-to-end principle;iterative method;landweber iteration;motion planning;physics engine;raw image format;reinforcement learning;robot;simulation;simultaneous localization and mapping	Jianhui Han;Huaping Liu;Bowen Wang	2017	2017 2nd International Conference on Advanced Robotics and Mechatronics (ICARM)	10.1109/ICARM.2017.8273235	convolutional neural network;simultaneous localization and mapping;reinforcement learning;motion planning;physics engine;machine learning;solid modeling;computer science;artificial intelligence;mechatronics	Robotics	48.929522277427495	-29.426869179712234	162018
a30355acd1b61032ceb5805ff06d09d1d8d3d889	on-line road boundary estimation by switching multiple road models using visual features from a stereo camera	stereo image processing navigation object detection particle filtering numerical methods;particle filter on line road boundary estimation multiple road models visual features stereo camera road boundary estimation method autonomous navigation parallel lines model transition mechanisms multiple sensory feature based road boundary estimation;mobile robot;navigation;shape;estimation;image edge detection;roads;particle filter;image color analysis;roads estimation image color analysis shape robot kinematics image edge detection;stereo image processing;particle filter outdoor navigation mobile robot road boundary estimation stereo vision;stereo vision;road boundary estimation;object detection;particle filtering numerical methods;outdoor navigation;robot kinematics	This paper describes a road boundary estimation method for autonomous navigation. We consider navigation in a campus environment, where roads (or traversable regions) are not necessarily modeled by a typical road model with a pair of parallel lines, but have a variety of shapes. We therefore use a set of flexible road models with model transition mechanisms for a robust road boundary estimation. This new modeling is incorporated into our multiple sensory feature-based road boundary estimation framework using a particle filter. The proposed method has been successfully applied to various scenes in our campus to realize autonomous navigation.	autonomous robot;experiment;global positioning system;mobile robot;norm (social);online and offline;particle filter;stereo camera	Takeshi Chiku;Jun Miura	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6385746	mobile robot;computer vision;estimation;navigation;simulation;particle filter;shape;computer science;stereopsis;artificial intelligence;robot kinematics;remote sensing	Robotics	50.57482033889486	-37.47374270236336	162021
58f3edefa50c6b82c8871bfc570192fde415bc53	real-time path planning for long-term information gathering with an aerial glider	path planning;information gathering;long-term;energy constraint;unmanned aerial vehicle (uav)	Autonomous thermal soaring offers an opportunity to extend the flight duration of unmanned aerial vehicles (UAVs). In this work, we introduce the informative soaring problem, where a gliding UAV performs an information gathering mission while simultaneously replenishing energy from known thermal energy sources. We pose this problem in a way that combines convex optimisation with graph search and present four path planning algorithms with complementary characteristics. Using a target-search task as a motivating example, finite-horizon and Monte Carlo tree search methods are shown to be appropriate for situations with little prior knowledge, but suffer from either myopic planning or high computation cost in more complex scenarios. These issues are addressed by two novel tree search algorithms based on creating clusters that associate high uncertainty regions with nearby thermals. The cluster subproblems are solved independently to generate local plans, which are then linked together. Numerical simulations show that these methods find high-quality nonmyopic plans quickly. The more promising cluster-based method, which uses dynamic programming to compute a total ordering over clusters, is demonstrated in hardware tests on a UAV. Fifteen-minute plans are generated in less than four seconds, facilitating online replanning when simulated thermals are added or removed in-flight.	aerial photography;glider (conway's life);motion planning;real-time path planning;real-time transcription	Joseph L. Nguyen;Nicholas R. J. Lawrance;Robert Fitch;Salah Sukkarieh	2016	Auton. Robots	10.1007/s10514-015-9515-3	computer vision;simulation;artificial intelligence	Robotics	52.69209065783871	-25.998158555584403	162256
4e837b86cb638c0f0fccb3099dbcb4ab6078a982	filter design for simultaneous localization and map building (slam)	iterative method;filtering;iterative algorithms;covariance matrices multi robot systems mobile robots position control path planning sensor fusion laser ranging filtering theory iterative methods;path planning;self localization simultaneous mapping filtering cross covariances iterative method localization multiple robot systems global map laser range scanning covariance matrix sensor fusion;localization;cross covariances;filters;self localization;random variables;mobile robots;laser ranging;symmetric matrices;iterative methods;filter design;laser range scanning;real world application;position control;global map;covariance matrices;robots;simultaneous localization and mapping;multi robot systems;random variable;sensor fusion;simultaneous mapping;simultaneous localization and map building;filtering theory;buildings;covariance matrix;multiple robot systems;covariance intersection;filters buildings simultaneous localization and mapping covariance matrix random variables equations symmetric matrices iterative methods iterative algorithms robots	This paper deals with the fusion of random variables when cross covariances are unknown. This is a vital problem in nearly every real world application since cross covariances are often impossible to obtain but also cannot be ignored. We provide a rigorous derivation of the fusion equations which are also known as covariance intersection. This approach allows us to derive an iterative scheme for simultaneous mapping and localization. The algorithm can also be used for multi-robot explorations where highly correlated decentralized maps have to be fused to form a consistent global map. We show mapping and localization results based on dense laser range scans.	algorithm;covariance intersection;cross-covariance;filter design;internationalization and localization;iterative method;map;robot;simultaneous localization and mapping	Christian Schlegel;Thomas Kämpke	2002		10.1109/ROBOT.2002.1013646	random variable;computer vision;mathematical optimization;computer science;artificial intelligence;control theory;mathematics;iterative method;statistics	Robotics	52.62526302772966	-34.900516604313964	162490
d8ba1cb820b14a41b25c42ef5d81ae944794a60b	autonomous adaptive exploration using realtime online spatiotemporal topic modeling	aqua;rost;topic modeling;summarization;lda;exploration;surprise	The exploration of dangerous environments such as underwater coral reefs and shipwrecks is a difficult and potentially life-threatening task for humans, which naturally makes the use of an autonomous robotic system very appealing. This paper presents such an autonomous system, which is capable of autonomous exploration, and shows its use in a series of experiments to collect image data in challenging underwater marine environments. We present novel contributions on three fronts. First, we present an online topic-modeling-based technique to describe what is being observed using a lowdimensional semantic descriptor. This descriptor attempts to be invariant to observations of different corals belonging to the same species, or observations of similar types of rocks observed from different viewpoints. Second, we use the topic descriptor to compute the surprise score of the current observation. This is done by maintaining an online summary of observations thus far, and then computing the surprise score as the distance of the current observation from the summary in the topic space. Finally, we present a novel control strategy for an underwater robot that allows for intelligent traversal, hovering over surprising observations, and swimming quickly over previously seen corals and rocks.	autonomous robot;autonomous system (internet);control theory;converge;experiment;high- and low-level;local-density approximation;randi j. rost;topic model;underwater robotics	Yogesh A. Girdhar;Philippe Giguère;Gregory Dudek	2014	I. J. Robotics Res.	10.1177/0278364913507325	computer vision;simulation;exploration;computer science;artificial intelligence;automatic summarization;topic model;remote sensing	AI	47.89027278124605	-36.65432142639175	162898
c3fb9de05cac307ffc5ceaa356bb080d282961c4	study on performance improvement of oil paint image filter algorithm using parallel pattern library		This paper gives a detailed study on the performance of oil paint image filter algorithm with various parameters applied on an image of RGB model. Oil Paint image processing, being very performance hungry, current research tries to find improvement using parallel pattern library. With increasing kernelsize, the processing time of oil paint image filter algorithm increases exponentially.	c++;composite image filter;computer engineering;computer performance;image processing;mobile app;multiprocessing;peterson's algorithm;pixel;signal processing;software development	Siddhartha Mukherjee	2014	CoRR		computer graphics (images)	Robotics	43.32480722477092	-32.74432314258853	163048
72706d6329684948c0b32b3d07888fb652ae3614	anadaptive embedded architecture for real-time particle image velocimetry algorithms	lasers computer architecture algorithm design and analysis abstracts embedded systems vectors measurement by laser beam;image processing;particle image velocimetry;cross correlation;real time;method of image;design flow;image processing adaptive embedded architecture realtime particle image velocimetry algorithm piv method flow imaging flow analysis motion vector adaptive fpga based system field programmable gate array cross correlation technique parallel processing strategy;motion vector;velocity measurement computerised instrumentation embedded systems field programmable gate arrays image processing parallel processing;parallel processing	Particle Image Velocimetry (PIV) is a method of imaging and analysing fields of flows. The PIV techniques compute and display all the motion vectors of the field in a resulting image. Speeds more than thousand vectors per second can be required, each speed being environment-dependent. Essence of this work is to propose an adaptive FPGA-based system for real-time PIV algorithms. The proposed structure is generic so that this unique structure can be re-used for any PIV applications that uses the cross-correlation technique. The major structure remains unchanged, adaptations only concern the number of processing operations. The required speed (corresponding to the number of vector per second) is obtained thanks to a parallel processing strategy. The image processing designer duplicates the processing modules to distribute the operations. The result is a FPGA-based architecture, which is easily adapted to algorithm specifications without any hardware requirement. The design flow is fast and reliable.	algorithm;cross-correlation;design flow (eda);embedded system;field-programmable gate array;image processing;parallel computing;real-time clock	Alain Aubert;Nathalie Bochard;Virginie Fresse	2006	2006 14th European Signal Processing Conference		embedded system;parallel processing;computer vision;electronic engineering;image processing;computer science;design flow;cross-correlation;digital image processing;particle image velocimetry;statistics;computer graphics (images)	Robotics	44.13429904244076	-34.96670873150868	163354
73899e0ca39908dc4194d0996e62f4ff0ed206b3	designing mappings for musical interfaces using preset interpolation		A new method for interpolating between presets is described. The interpolation algorithm called Intersecting N-Spheres Interpolation is simple to compute and its generalization to higher dimensions is straightforward. The current implementation in the SuperCollider environment is presented as a tool that eases the design of many-to-many mappings for musical interfaces. Examples of its uses, including such mappings in conjunction with a musical interface called the sponge, are given and discussed.	algorithm;interpolation;many-to-many;supercollider	Martin Marier	2012			interpolation;musical;theoretical computer science;computer science	Graphics	46.490667256842706	-29.0948389199758	163638
0d0ee5afe47b1482a30ece179fee039b2d28167c	model-based environmental visual perception for humanoid robots	humanoid robot;object recognition and detection;object recognition;model based vision;humanoid robots;cognitive vision;pure data;visual perception;experimental evaluation;model coupling	An autonomous environmental visual perception approach for humanoid robots is presented. The proposed framework exploits the available model information and the context acquired during global localization by establishing a vision-model coupling in order to overcome the limitations of purely data-driven approaches in object recognition and surrounding status assertion. The exploitation of the model-vision coupling through the properceptive components is the key element to solve complex visual assertion-queries with proficient performance. An experimental evaluation with the humanoid robot ARMAR-IIIa is presented.	assertion (software development);autonomous robot;dynamic dispatch;gps block iiia;humanoid robot;mathematical optimization;outline of object recognition;region of interest;spatial–temporal reasoning	David Israel Gonzalez-Aguirre	2011	Pattern Recognition Letters	10.1016/j.patrec.2010.09.028	computer vision;simulation;computer science;humanoid robot	Robotics	51.1769874228628	-35.93046953966941	164084
1fedbe380de343cb83cadb1feba418b553ccc8ee	on-board control in the robocup small robots league	embedded hardware;heterogeneous agents;robotic edutainment;robocup;autonomic system;autonomous systems	We present the most recent version of our RoboCube system, a special robot-controller hand-tailored for players in the small robots league. The RoboCube is conceptualized to implement players with as many on-board features as possible in an extremely exible way. For this purpose, the RoboCube provides quite some computation power and memory as well as a multitude of I/O-interfaces within the space-constraints. As it facilitates the use of many sensors and eeectors, including their on-board processing, the RoboCube allows to explore a large space of diierent robots and team setups. In addition to the basic system, we present the option to enhance RoboCube with high-resolution color vision to a so-called VisionCube. In doing so, we exploit the possibilities of a completely new technology in imaging based on C-MOS chips. Among other advantages, the particular imager used in our design allows a selective read-out of pixels. Unlike CCDs where a complete frame has to be read, the VisionCube can read subsets of the 512512 colored pixels of its imager in an extremely fast way. This makes it feasible to explore the challenging option of local vision with on-board processing.	color vision;computation;image resolution;image sensor;input/output;on-board data handling;pixel;robot	Andreas Birk;Holger Kenn;Thomas Walle	2000	Advanced Robotics	10.1163/156855300741410	embedded system;simulation;autonomous system;engineering;artificial intelligence	Robotics	46.80081077421389	-33.13899114941817	164156
23e9e0d2d30418854bd6216f4de530c97cc46c8d	small field-of-view star identification using bayesian decision theory	positional accuracy;science camera;instruments;imaging stars;bayes methods;standard deviation;simulation;bayesian methods;image sensors;narrow field of view;bayesian decision theory;sensor orientation bayesian decision theory autonomous star identification algorithm narrow field of view science camera simulation positional accuracy brightness;brightness;astronomy computing;identification;decision theory;field of view;position measurement;autonomous star identification algorithm;sensor orientation;sun;space missions;bayes methods decision theory identification astronomy computing;propulsion;bayesian methods decision theory space vehicles instruments sun propulsion cameras image sensors position measurement space missions;false positive;cameras;space vehicles	In this document, we describe a simple autonomous star identification algorithm which is effective using a narrow field of view (2 degrees), making the use of a science camera for star identification feasible. This work extends that of Padgett and Kreutz-Delgado [8] by setting decision thresholds using Bayesian decision theory. Our simulations show that when positional accuracy of imaged stars is 0.5 pixel (standard deviation) and the apparent brightness deviates by 0.8 unit stellar magnitude, the algorithm correctly identifies 96.0% of the sensor orientations, with less than a 0.3% rate of false positives.	algorithm;autonomous robot;decision theory;pixel;simulation;stellar (payment network)	Daniel S. Clouse;Curtis W. Padgett	2000	IEEE Trans. Aerospace and Electronic Systems	10.1109/7.869495	identification;computer vision;simulation;propulsion;bayes estimator;decision theory;field of view;type i and type ii errors;bayesian probability;computer science;space exploration;image sensor;mathematics;optics;standard deviation;brightness;physics;statistics	Mobile	52.58944153042998	-36.061433872031806	164223
211e77565d010f15bb0ddc238079a66648940e69	3d robot mapping: combining active and non active sensors in a probabilistic framework	active sensor;3d mapping;laser omnivision fusion;info eu repo semantics article;artificial intelligent;bayesian units	Map reconstruction and robot location are two essential problems in the field of robotics and artificial intelligence. A robot could need a model of the environment that can be incomplete and therefore the robot must work considering the uncertainty. Bayesian Units consider the uncertainty and allow the fusion of information from different sensors. In this paper a map reconstruction system in 3D based on Bayesian Units is presented. The reconstruction is carried out integrating the data obtained by a laser and by an omnivision system. In addition, to improve the quality of the reconstruction, the fusion of several Bayesian Units is defined using a competitive fusion operator. Finally, the obtained results as well as the validity of the system are shown.	robot;sensor	Fidel Aznar Gregori;Mireia Sempere;Maria Del Mar Pujol López;Ramón Rizo Aldeguer;Rafael Molina	2005		10.1007/11881216_2	computer vision;engineering;artificial intelligence;data mining	Robotics	52.4611386374908	-34.98499334980621	164292
54d10eda55a7e8ec9c78237ac2bdfec2a2175bd3	robust mapless outdoor vision-based navigation	visual field occlusions;spatial constellation;visual field occlusions robust mapless outdoor vision based navigation sensory motor learning biologically inspired place cells spatial constellation online learned landmarks place action associations visual path following mobile obstacles;biologically inspired place cells;place action associations;mobile robots;online learning;robustness navigation robot sensing systems mobile robots personal communication networks simultaneous localization and mapping biological system modeling biomimetics intelligent robots gaussian processes;robot vision;online learned landmarks;place cell;sensory motor learning;mobile obstacles;robot vision collision avoidance mobile robots;collision avoidance;visual path following;motor learning;robust mapless outdoor vision based navigation	This article presents an efficient and mature vision-based navigation algorithm based on sensory-motor learning. Neither Cartesian nor topological map are required, but a set of biologically inspired place cells. Each place cell defines a location by a spatial constellation of online learned landmarks. Their activity provides an internal measure of localization. A simple set of place-action associations enable a robot to go back to a learned location or to follow an arbitrary visual path. The system is able to achieve sensory-motor tasks in indoor as well as in large outdoor environments with similar computation load. The behavior is robust to kidnapping, object and landmark addition or removal, presence of mobile obstacles and severe visual field occlusions	algorithm;cartesian closed category;computation;internationalization and localization;robot;simple set	Christophe Giovannangeli;Philippe Gaussier;Gael Désilles	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.282501	mobile robot;computer vision;simulation;motor learning;computer science	Robotics	50.23261052808869	-36.00453886583755	164575
0e80487ac65e3e8f77794e07e5a436e83f416f6c	monte carlo tree search with macro-actions and heuristic route planning for the multiobjective physical travelling salesman problem	marine vehicles fuel storage games fuels radiation detectors tuning cost function;evolutionary computation;path planning;covariance matrices;travelling salesman problems;tree searching covariance matrices evolutionary computation monte carlo methods path planning space vehicles travelling salesman problems;alternate fitness function monte carlo tree search macro actions heuristic route planning multiobjective physical travelling salesman problem mo ptsp ieee cig 2013 conference simulated spaceship 2 d plane consumed fuel incurred damage preplanning stage classical tsp solver path cost measure steering controller mcts turn based game go covariance matrix adaptation evolution strategy algorithm cma es automatic parameter tuning method hand tuned parameters poor quality local optimum;tree searching;monte carlo methods;space vehicles	This paper describes our entry to the Multiobjective Physical Travelling Salesman Problem (MO-PTSP) competition at the IEEE CIG 2013 conference. MO-PTSP combines the classical Travelling Salesman Problem with the task of steering a simulated spaceship on the 2-D plane, requiring that the controller minimises the three objectives of time taken, fuel consumed and damage incurred. Our entry to the MO-PTSP competition builds upon our winning entry to the previous (single-objective) PTSP competitions. This controller consists of two key components: a pre-planning stage using a classical TSP solver with a path cost measure that takes the physics of the problem into account, and a steering controller using Monte Carlo Tree Search (MCTS) with macro-actions (repeated actions), depth limiting and a heuristic fitness function for nonterminal states. We demonstrate that by modifying the two fitness functions we can produce effective behaviour in MO-PTSP without the need for major modifications to the overall architecture. The fitness functions used by our controller have several parameters, which must be set to ensure the best performance. Given the number of parameters and the difficulty of optimising a controller to satisfy multiple objectives in a search space which is many orders of magnitude larger than that encountered in a turn-based game such as Go, we show that informed hand tuning of parameters is insufficient for this task. We present an automatic parameter tuning method using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, which produced parameter settings that dominate our hand tuned parameters. Additionally we show that the robustness of the controller using hand tuned parameters can be improved by detecting when the controller is trapped in a poor quality local optimum and escaping by switching to an alternate fitness function.	algorithm;cma-es;evolution strategy;fitness function;heuristic;inscriptiones graecae;local optimum;monte carlo method;monte carlo tree search;nanosat mo framework;sensor;solver;terminal and nonterminal symbols;travelling salesman problem	Edward Jack Powley;Daniel Whitehouse;Peter I. Cowling	2013	2013 IEEE Conference on Computational Inteligence in Games (CIG)	10.1109/CIG.2013.6633658	mathematical optimization;simulation;computer science;artificial intelligence;machine learning;motion planning;3-opt;monte carlo method;evolutionary computation	Robotics	53.46991990459162	-25.173715676040235	164632
fe08fa05c725f21589a4a9797012d68b5c1f3fcf	just-in-time shader program generation for fixed function graphics pipeline emulation	computer graphics;pipelines graphics emulation consumer electronics graphics processing units smart phones three dimensional displays;software architecture application program interfaces computer graphics graphics processing units;software architecture;graphics processing units;application program interfaces;programmable pipelines just in time shader program generation fixed function graphics pipeline emulation consumer electronics devices smart phones table pc 3d graphics architectures	On various consumer electronics devices including smart phones and table PC's, the 3D graphics architectures are rapidly changing from fixed function pipelines to programmable pipelines. To execute the traditional graphics applications on the modern programmable pipelines, shader programs are used for their emulations. In this paper, we present an efficient way of the most suitable shader program generation for the given configurations. Comparing to the previously used shader programs, our just-in-time approach shows remarkable speed-ups, to give much more convenience for their consumers.	3d computer graphics;emulator;graphics pipeline;just-in-time compilation;shader;smartphone;table computer	Nakhoon Baek	2014	2014 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2014.6776104	embedded system;software architecture;graphics pipeline;computer hardware;computer science;operating system;texture mapping unit;computer graphics lighting;real-time computer graphics;graphics software;computer graphics;graphics address remapping table;graphics hardware;general-purpose computing on graphics processing units;software rendering;3d computer graphics;computer graphics (images)	EDA	43.43334687869514	-32.2440514777811	164641
1c69a9c92c557054c55f58770bd49d5f9b154750	memory-efficient computation of high-dimensional integral images	image processing;memory efficient computation integral image high dimensional array;memory efficient computation high dimensional integral images;high dimensional array;integral image;memory efficient computation;arrays memory management vectors tensile stress indexes histograms pattern recognition	We propose a memory-efficient method for computing high-dimensional integral images. The proposed method generates exactly the same integral images as that generated by the conventional method, while using less memory. Experimental results show the effectiveness of the proposed method.	computation;experiment;resultant	Kohei Inoue;Kenji Hara;Kiichi Urahama	2013	2013 2nd IAPR Asian Conference on Pattern Recognition	10.1109/ACPR.2013.41	computer vision;discrete mathematics;theoretical computer science;mathematics	Robotics	41.43514679520861	-35.34679724138544	164828
1d6752c847dd6b7376e3e3964ae5202102579ca2	fast motion planning from experience: trajectory prediction for speeding up movement generation	machine learning;motion planning;articulated robotics;motion representation	Trajectory planning and optimization is a fundamental problem in articulated robotics. Algorithms used typically for this problem compute optimal trajectories from scratch in a new situation, without exploiting similarity to previous problems. In effect, extensive data is accumulated containing situations together with the respective optimized trajectories—but this data is in practice hardly exploited. This article describes a novel method to learn from such data and speed up motion generation, a method we denote Trajectory Prediction. The main idea is to use demonstrated optimal motion trajectories to quickly predict appropriate trajectories for novel situations. These can be used to initialize and thereby drastically speed-up subsequent optimization of robotic movements and improve the convergence behavior of a conventional motion optimizer. Our approach has two essential ingredients. First, to generalize from previous situations to new ones we need an appropriate situation descriptor – we construct features for such descriptors and use a sparse regularized feature selection approach to find well-generalizing features of situations. Second, the transfer of previously optimized trajectories to a new situation should not be made in joint angle space – we propose a more efficient task space transfer of old trajectories to new situations. We present extensive results in simulation to illustrate the benefits of the new method, and demonstrate it also with real robot hardware. Our experiments with a reaching and obstacle avoidance task, and an object grasping task, show that we can predict good motion Nikolay Jetchev and Marc Toussaint are with Machine Learning and Robotics Lab, FU Berlin, Arnimallee 7, 14195 Berlin, Germany E-mail: nikolay.jetchev@fu-berlin.de, marc.toussaint@fuberlin.de trajectories in new situations for which the refinement is much faster than an optimization from scratch.	algorithm;converge;experiment;feature selection;godfried toussaint;inverse kinematics;machine learning;mathematical optimization;motion planning;noether's theorem;obstacle avoidance;refinement (computing);robot;robotics;simulation;sparse matrix;trajectory optimization;workspace	Nikolay Jetchev;Marc Toussaint	2013	Auton. Robots	10.1007/s10514-012-9315-y	computer vision;simulation;computer science;artificial intelligence;machine learning;motion planning	Robotics	48.59632749984646	-27.2938695931601	165291
752f7749a24a864ab8a2cb10e3fe74e8cc1592a1	automatic real-time extraction of focused regions in a live video stream using edge width information	fpga implementation focused region extraction edge width computation vlsi architecture;fpga implementation vlsi architecture video sequence focused region extraction virtex 5 edge width based scheme video surveillance application live video stream standard pal size color video;image edge detection streaming media field programmable gate arrays cameras real time systems computer architecture image color analysis;vlsi feature extraction field programmable gate arrays image sequences video retrieval video streaming video surveillance	This paper presents the design of a dedicated VLSI architecture for focused region extraction in a video sequence and its implementation on Virtex-5 (ML510) FPGA platform. Edge width based scheme is used for focused region extraction. The proposed architecture is designed to meet the real-time requirements of video surveillance applications. It is capable of robustly extracting the focused regions in a live video stream in real-time for standard PAL size color video.	closed-circuit television;field-programmable gate array;pal;real-time clock;real-time locating system;requirement;streaming media;very-large-scale integration	Sanjay Singh;Sumeet Saurav;Ravi Saini;Anil K. Saini;Chandra Shekhar;Anil Vohra	2014	18th International Symposium on VLSI Design and Test	10.1109/ISVDAT.2014.6881046	computer vision;real-time computing;computer science;video capture;video tracking;video processing;multiview video coding;computer graphics (images)	Arch	43.98107034224532	-34.921476673440715	165306
edfcebc6a020ad2d222eb8e8c8ae42cf3a54fff0	moving object tracking using object segmentation	motion analysis;moving object;surveillance system;feature matching;object segmentation;object tracking;image sequence;traffic monitoring	  Research in motion analysis has evolved over the years as a challenging field, such as traffic monitoring, military, automated  surveillance system and biological sciences etc. Tracking of moving objects in video sequences can offer significant benefits  to motion analysis. In this paper an approach is proposed for the tracking of moving objects in an image sequence using object  segmentation framework and feature matching functionality. The approach is amenable for SIMD processing or mapping onto VLIW  DSP. Our C implementation runs at about 30 frames/second with 320x240 video input on standard Window XP machine. The experimental  results have established the effectiveness of our approach for real world situations.    		Sanjay Singh;Srinivasa Murali Dunga;A. S. Mandal;Chandra Shekhar;Anil Vohra	2010		10.1007/978-3-642-15766-0_122	computer vision;deep-sky object;tracking system;computer science;viola–jones object detection framework;segmentation-based object categorization;video tracking;scale-space segmentation	Robotics	44.277015190111165	-37.681458940954954	165572
ad518422e8eb812974d71bfbe6351a070cf28728	learning to combine motor primitives via greedy additive regression	cognitive science;cost function;numerical optimization;artificial intelligent;computational complexity;local features;sparse representation;motor control	The computational complexities arising in motor control can be ameliorated through the use of a library of motor synergies. We present a new model, referred to as the Greedy Additive Regression (GAR) model, for learning a library of torque sequences, and for learning the coefficients of a linear combination of sequences minimizing a cost function. From the perspective of numerical optimization, the GAR model is interesting because it creates a library of “local features”—each sequence in the library is a solution to a single training task—and learns to combine these sequences using a local optimization procedure, namely, additive regression. We speculate that learners with local representational primitives and local optimization procedures will show good performance on nonlinear tasks. The GAR model is also interesting from the perspective of motor control because it outperforms several competing models. Results using a simulated two-joint arm suggest that the GAR model consistently shows excellent performance in the sense that it rapidly learns to perform novel, complex motor tasks. Moreover, its library is overcomplete and sparse, meaning that only a small fraction of the stored torque sequences are used when learning a new movement. The library is also robust in the sense that, after an initial training period, nearly all novel movements can be learned as additive combinations of sequences in the library, and in the sense that it shows good generalization when an arm’s dynamics are altered between training and test conditions, such as when a payload is added to the arm. Lastly, the GAR model works well regardless of whether motor tasks are specified in joint space or Cartesian space. We conclude that learning techniques using local primitives and optimization procedures are viable and potentially important methods for motor control and possibly other domains, and that these techniques deserve further examination by the artificial intelligence and cognitive science communities.	additive model;analysis of algorithms;artificial intelligence;cartesian closed category;coefficient;cognitive science;computation;global optimization;gradient;greedy algorithm;iteration;jones calculus;machine learning;mathematical optimization;nonlinear system;numerical analysis;payload (computing);principal component analysis;sparse matrix;synergy;test set;triune continuum paradigm;utility functions on indivisible goods	Manu Chhabra;Robert A. Jacobs	2008	Journal of Machine Learning Research	10.1145/1390681.1442782	motor control;simulation;computer science;artificial intelligence;machine learning;sparse approximation;mathematics;computational complexity theory;statistics	ML	48.09444053875932	-26.288189066909126	165811
1908dd91c6bcf1b7ae096714b86e312936fa668b	fbram: a new form of memory optimized for 3d graphics	read modify write;caching;chip;parallel graphics algorithms;3d graphics hardware;dynamic memory;memory optimization;dynamic random access memory;3d graphics;rendering	FBRAM, a new form of dynamic random access memory that greatly accelerates the rendering of Z-buffered primitives, is presented. Two key concepts make this acceleration possible. The first is to convert the read-modify-write Z-buffer compare and RGBα blend into a single write only operation. The second is to support two levels of rectangularly shaped pixel caches internal to the memory chip. The result is a 10 megabit part that, for 3D graphics, performs read-modify-write cycles ten times faster than conventional 60 ns VRAMs. A four-way interleaved 100MHz FBRAM frame buffer can Z-buffer up to 400 million pixels per second. Working FBRAM prototypes have been fabricated.	3d computer graphics;dynamic random-access memory;framebuffer;megabit;pixel;random access;read-modify-write;z-buffering	Michael F. Deering;Stephen A. Schlapp;Michael G. Lavelle	1994		10.1145/192161.192194	cuda pinned memory;interleaved memory;parallel computing;dynamic random-access memory;computer hardware;computer science;operating system;graphics address remapping table;3d computer graphics;computer graphics (images)	Graphics	43.45961080979261	-31.958172555919717	166056
df9d7b23bb0d3e9a86f72c03abcf3a51577ea852	an integrated model of visual attention for homecare robot with self-awareness	bayes rule homecare robot top down visual attention bottom up visual attention human attention process robot self awareness model fuzzy decision making system object based biasing robot memory;medical robotics bayes methods decision making fuzzy set theory;robot sensing systems visualization semantics computational modeling biological system modeling	This paper proposes an integrated model of top-down and bottom-up visual attention with self-awareness for a homecare robot. For mimicking the human attention processes, a robot self-awareness model with fuzzy decision making system is developed and utilized, which is an important improvement on the existing robot attention models. Besides the task-driven object-based biasing, a robot self-awareness model can generate other parts of top-down biases in a robot visual attention model. Some results from the self-awareness model are obtained here. In order to update the weights in robot memory, the learning process is carried out through the Bayes' rule. Three images are tested to evaluate the developed methods. Four types of saliency maps are compared to see how self-awareness can affect robot attention selection.	biasing;kinect;map;microsoft windows;object-based language;outline of object recognition;robot;self-awareness;simulation;top-down and bottom-up design	Yu Du;Clarence W. de Silva;Ming Cong;Dong Liu;Wenlong Qin	2015	2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2015.7419025	robot learning;computer vision;computer science;artificial intelligence;social robot;mobile robot navigation;personal robot	Robotics	48.4103418208988	-32.2308751044329	166396
f693ed60f51d610ce8e09185ef02bc3673a638c8	enhancing the accuracy of parking assistant sensors with bayesian filter		Sonar distance sensors are commonly used for obstacle detection and distance measurement, providing input information for different applications, such as collision avoidance algorithms and vehicle parking assistants. However, they have a wide range of quality and accuracy, resulting in prices ranging from $0.55 to over $100 per unit. As it is often necessary to use a few units in parking assistants and those are deployed on a largescale vehicle production, the unit price is a critical factor. However, the simple choice of the lowest price sensors directly impacts on the measurements reliability, since they have high levels of noise in the values of their measurements. Therefore, this presents the results of the experiments using the Bayesian Recursive Estimation technique – also known as Bayesian Filtering – to increase the accuracy and reliability of low-cost sonar sensor measurements. A prototype is implemented and evaluated in simulated and real (physical) experimental environments. Using this approach, a significant accuracy improvement on distance measurements was observed compared to the raw data obtained from sensors. The results suggest this approach can be an alternative to be considered to reduce costs when equipping vehicles with parking assistants.	algorithm;bayesian network;experiment;prototype;recursion (computer science);sonar (symantec);sensor;traffic collision avoidance system	Alexandre M. Nascimento;Paulo Sérgio Cugnasca;Lucio Flavio Vismari;João Battista Camargo Junior;Jorge Rady de Almeida	2018	2018 IEEE International Conference on Vehicular Electronics and Safety (ICVES)	10.1109/ICVES.2018.8519512		Robotics	53.57491659725521	-34.928035375393044	166505
39f1b88fb929ddf1df1e9b522c0a2926d50a8c15	robot navigation with distance queries	navegacion;obstaculo;medio ambiente;range query;position;current;robot navigation;exploracion;pregunta documental;posicion;robotics;68t05;question documentaire;algorithme;analysis of algorithms;robot exploration;captador medida;algorithm;navigation;measurement sensor;proximite;capteur mesure;proximidad;corriente;environment;proximity;borne inferieure;distancia;query;exploration;robotica;navigation robot;courant;environnement;robotique;distance query;exploration robot;robot;interrogation distance;lower bound;distance;obstacle;cota inferior;algoritmo	We consider the problem of online robot navigation in an unfamiliar two-dimensional environment, using comparatively limited sensing information. In particular, the robot has local sensors to detect the proximity of obstacles and permit boundary-following, and is able to determine its current distance and relative bearing to its nal destination (via distance queries). By contrast, most previous algorithms for online navigation have assumed that the robot knows its exact current position. Because determining exact location is prone to error that accumulates over time, the usefulness of such algorithms may be limited. In contrast, distance queries give less information, but the accuracy of each query is independent of the number of queries, which means distance queries can be more robust. We formally deene our model and give new, eecient navigation algorithms and lower bounds for this setting.	algorithm;robot;robotic mapping;sensor	Dana Angluin;Jeffery Westbrook;Wenhong Zhu	2000	SIAM J. Comput.	10.1137/S0097539797330057	computer vision;simulation;artificial intelligence;mathematics;geometry;robotics;distance;mobile robot navigation;algorithm	Theory	51.93774887210986	-29.896485410201556	166762
0c4c4ddd51e9f023414fed769ec065e9bf6acbef	learning occupancy grid maps with forward sensor models	tecnologia industrial tecnologia mecanica;high dimensionality;forward model;sensor model;mobile robot;mobile robotics;slam;occupancy grid;grupo de excelencia;robot navigation;probabilistic inference;expectation maximization;expectation maximization algorithm;mapping;tecnologias;bayesian techniques	This article describes a new algorithm for acquiring occupancy grid maps with mobile robots. Existing occupancy grid mapping algorithms decompose the high-dimensional mapping problem into a collection of one-dimensional problems, where the occupancy of each grid cell is estimated independently. This induces conflicts that may lead to inconsistent maps, even for noise-free sensors. This article shows how to solve the mapping problem in the original, high-dimensional space, thereby maintaining all dependencies between neighboring cells. As a result, maps generated by our approach are often more accurate than those generated using traditional techniques. Our approach relies on a statistical formulation of the mapping problem using forward models. It employs the expectation maximization algorithm for searching maps that maximize the likelihood of the sensor measurements.	expectation–maximization algorithm;map;mobile robot;sensor	Sebastian Thrun	2003	Auton. Robots	10.1023/A:1025584807625	simulation;expectation–maximization algorithm;computer science;artificial intelligence;machine learning;occupancy grid mapping	Robotics	47.92318486879239	-26.870303303255202	167541
382362cb1a2046491f5d54b907dadded10222d56	implementing planar motion tracking algorithms on cmos+fpga vision system	vision system;fpga vision realtime visual feedback cmos image sensor;image features;image motion analysis;high resolution;fpga;machine vision image sampling feedback control systems target tracking mechanical systems mechanical sensors sensor phenomena and characterization image resolution signal resolution;motion tracking;cmos image sensors;control system;cmos image sensor;feedback;feature extraction;1000 hz planar motion tracking cmos fpga vision system visual feedback visual feature computation cmos image detector;visual features;cmos imager;visual feedback;image motion analysis cmos image sensors feature extraction feedback field programmable gate arrays;field programmable gate arrays;matched filter;mechanical systems;vision;realtime	This paper describes a 1,000 Hz visual feedback using the CMOS+FPGA vision. It is required to obtain positional and angular signals around 1,000 Hz to control a mechanical system. A vision sensor must obtain visual features of a target object, synchronizing its sampling rate to the sampling rate of the control. Thus, we need 1) image capturing over 1,000 Hz with high resolution, 2) visual feature computation at the capturing rate, and 3) visual feature transmission to a control system with little delay. We propose the CMOS+FPGA vision system to realize 1,000 Hz visual feedback. This system consists of a CMOS image detector to capture images at 1,000 Hz and an FPGA to compute image features at this sampling rate. We implement two vision algorithms to track planar motion on the CMOS+FPGA vision system. First we introduce the CMOS+FPGA vision system. Second, we show the implementation of the computation of the image gravity center. Then, we show the implementation of matched filter	algorithm;angularjs;cmos;computation;control system;field-programmable gate array;image resolution;matched filter;sampling (signal processing)	Kazuhiro Shimizu;Shinichi Hirai	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.281924	embedded system;computer vision;electronic engineering;computer science;control system;field-programmable gate array	Robotics	45.389562886079034	-35.13577864100915	167568
bc35e623b7e13ab6b89ec56643d85491e14b7a97	a computationally efficient technique for real-time detection of particular-slope edges	edge detection;motion tracking;wavelet transform;real time implementation	Identification of oblique lines of a particular slope is needed for various applications such as motion tracking for smart cameras. Wavelets and gradient-based techniques, such as Sobel and Canny, do not classify edges based on their slopes. The Hough transform (HT) does classify edges based on their slopes but with high computational complexity, even using its most improved versions. This paper presents a computationally efficient technique for detecting edges of a particular slope. The angle of the required edges is converted into pixel increments over rows and columns. Using these two simple parameters, parallel, oblique lines of a particular slope are formed. A first-order, orthonormal Haar low-pass filter (LPF) is used over the formed lines to filter out undesired edges. The hardware architecture of the proposed technique is fully described, including processing time, based on the number of clock cycles, and fixed-point implementation. A line-based memory mechanism was used to minimize the memory requirements to two simple registers. To demonstrate the computational advantage of the proposed technique, it is compared to the Sobel, Canny and HT detectors.	algorithmic efficiency;canny edge detector;clock signal;column (database);computational complexity theory;convolution;edge detection;first-order predicate;fixed-point arithmetic;gradient;haar wavelet;hough transform;low-pass filter;oblique projection;parallel computing;pixel;processor register;real-time clock;requirement;sensor;sobel operator	Ahmed Mabrouk;Norhidayah Hassim;Ibrahim Elshafiey	2013	Journal of Real-Time Image Processing	10.1007/s11554-013-0346-1	computer vision;edge detection;computer science;wavelet transform;computer graphics (images)	Vision	44.11476170002089	-34.080894374725965	167934
2c610a0b68d69ee36d637062797cd456e297ef5d	mutual information computation and maximization using gpu	image registration computer vision;mutual information entropy retina computer science image registration computer architecture workstations information theory random variables image processing;gpu architecture mutual information computation maximization exponential computations image registration;image processing;approximation algorithms;maximization;mutual information computation;random variables;computer vision;computer architecture;exponential computations;computational modeling;retina;image registration;pixel;workstations;mathematical model;mutual information;entropy;computer science;gpu architecture;information theory	We present a GPU implementation to compute both mutual information and its derivatives. Mutual information computation is a highly demanding process due to the enormous number of exponential computations. It is therefore the bottleneck in many image registration applications. However, we show that these computations are fully parallizable and can be efficiently ported onto the GPU architecture. Compared with the same CPU implementation running on a workstation level CPU, we reached a factor of 170 in computing mutual information, and a factor of 400 in computing its derivatives.	approximation;central processing unit;computation;expectation–maximization algorithm;experiment;graphics processing unit;image registration;mutual information;parallel computing;performance;shared memory;time complexity;workstation	Yuping Lin;Gérard G. Medioni	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4563101	random variable;computer vision;entropy;workstation;image processing;information theory;computer science;image registration;theoretical computer science;machine learning;mathematical model;mutual information;computational model;pixel;statistics	Vision	40.83008170922738	-34.20875534706849	168120
812a9f8238f7351f7123c6c31b3b98b68a522a9b	cello-em: adaptive sensor models without ground truth	nonparametric statistics;sensors;sensors nonparametric statistics regression analysis robot dynamics;regression analysis;robot sensing systems kernel vectors hidden markov models vehicles approximation methods;robot dynamics;cello em covariance model rgb d camera odometry laser based scan matching odometry nonparametric kernel regressor sensor model predictor dynamic model adaptive sensor model	We present an algorithm for providing a dynamic model of sensor measurements. Rather than depending on a model of the vehicle state and environment to capture the distribution of possible sensor measurements, we provide an approximation that allows the sensor model to depend on the measurement itself. Building on previous work, we show how the sensor model predictor can be learned from data without access to ground truth labels of the vehicle state or true underlying distribution, and we show our approach to be a generalization of non-parametric kernel regressors. Our algorithm is demonstrated in simulation and on real world data for both laser-based scan matching odometry and RGB-D camera odometry in an unknown map. The performance of our algorithm is shown to quantitatively improve estimation, both in terms of consistency and absolute accuracy, relative to other algorithms and to fixed covariance models.	approximation;discrete cosine transform;domain-specific language;estimation theory;expectation–maximization algorithm;ground truth;kerrison predictor;mathematical model;odometry;sensor;simulation	William Vega-Brown;Nicholas Roy	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6696609	nonparametric statistics;computer vision;econometrics;sensor;machine learning;odometry;mathematics;regression analysis;statistics	Robotics	39.65874343348761	-27.104622643866197	168362
2ad03000520464f9b4b12c08175bb673a8956576	reactive navigation and opportunistic localization for autonomous underground mining vehicles	scanning range laser;mining;nodal map;localization;relative navigation;reactive navigation;weak localization;control architecture;autonomous navigation;navigation system;wall following;automation	This paper describes an autonomous navigation system for a large underground mining vehicle. The control architecture is based on a robust reactive wall-following behaviour. To make it purposeful we provide driving hints derived from an approximate nodal-map. For most of the time, the vehicle is driven with weak localization (odometry). This need only be improved at intersections where decisions must be made – a technique we refer to as opportunistic localization. The paper briefly reviews absolute and relative navigation strategies, and describes an implementation of a reactive navigation system on a 30 tonne Load-Haul-Dump truck. This truck has achieved full-speed autonomous operation at an artificial test mine, and subsequently, at a operational underground mine. 2002 Published by Elsevier Science Inc.	approximation algorithm;autonomous robot;cyclic redundancy check;image scanner;leslie speaker;odometry;the australian;windows media center	Jonathan M. Roberts;Elliot S. Duff;Peter I. Corke	2002	Inf. Sci.	10.1016/S0020-0255(02)00227-X	computer vision;mining;simulation;internationalization and localization;computer science;automation;mobile robot navigation;weak localization;quantum mechanics	Robotics	53.3676002445533	-30.00477303879537	168815
62e47978e13413b1412dd4182844f00cc88e40e1	rover science autonomy: probabilistic planning for science-aware exploration	heuristic search;low resolution;value function;over the horizon	Next-generation Mars rovers will have the ability to autonomously navigate for distances of kilometers. At these scales, a day’s traverse takes the rover over its local horizon, into regions where only low-resolution orbital data is available. This improved navigation provides both an opportunity and a challenge: we need new techniques for performing effective science while over the horizon and out of contact. This work deals withscience autonomy (SA), the ability of the rover to reason about science goals and the science data it collects in order to make more effective exploration decisions. The proposed work has two major components. First, we will define a set of rover SA operational modes, and assist in developing and field testing a system that implements these modes. We will create an overall architecture for the SA system, and develop the planning component in that architecture. Research questions include: How do we define the SA operational modes? How useful is each mode, and under what circumstances should it be used? What kind of planner is most appropriate for an SA system? Second, we will extend partially observable Markov decision process (POMDP) planning algorithms in ways that bridge the gap between the current state-of-art and the planning requirements of the SA domain. POMDP planners can generate high-quality plans that take into account action and sensing uncertainty, but realistic problems in the SA domain are far beyond the abilities of existing algorithms. Research questions include: Can heuristic search be combined with efficient representations of the POMDP value function to speed planning? Can we improve scalability when most of the world state is observable? Can we effectively use continuous planning techniques in the POMDP context?	algorithm;automated planning and scheduling;autonomy;bellman equation;heuristic;markov chain;molecular orbital;partially observable markov decision process;partially observable system;planner;requirement;rover (the prisoner);scalability;traverse	Trey Smith	2005			machine learning;partially observable markov decision process;probabilistic logic;autonomy;bellman equation;heuristic;computer science;artificial intelligence	Robotics	51.51693179623744	-25.488476487638245	169161
17c6e5cf9d8a2a63506cbdb33f81b17986186985	pips: planning in perception space		Path planning for mobile robots requires rapidly finding collision-free trajectories in an uncertain and changing environment. Full collision checking with detailed, online-revised representations of the robot and world imposes a delay that undermines reactive obstacle avoidance. As a result, reactive vision-based approaches make various assumptions to arrive at simplified representations, such as circular or spherical robot shapes reducible to point masses, or obstacles that always rise from the ground. We seek to avoid these problems by modeling the robot directly in perception space so that collisionfree trajectories can be sought in a consistent representation with minimal processing needs. Here perception space refers to the depth space image measurements available by modern consumer range sensors. We hallucinate a robot navigating through the world and synthesize depth images of its path for comparison against the directly sensed depth images of the local world. The approach performs collision checking in a 3D volume but only requires 2D image comparisons. Experiments show that an implementation is able to negotiate an obstacle course consisting of miscellaneous objects in real-time.	algorithm;automated planning and scheduling;baseline (configuration management);collision detection;functional reactive programming;mobile robot;motion planning;obstacle avoidance;p.i.p.s.;real-time locating system;sensor	Justin S. Smith;Patricio A. Vela	2017	2017 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2017.7989735	robot;obstacle course;spherical robot;mobile robot;hallucinate;motion planning;computer vision;robot kinematics;obstacle avoidance;artificial intelligence;mathematics	Robotics	52.61427562820986	-31.536437402319603	169201
1b203f1ea96b64632d213c5f5b251c2865483a23	robotic playing for hierarchical complex skill learning	robot sensing systems;grasping;electronic countermeasures;robot kinematics	In complex manipulation scenarios (e.g. tasks requiring complex interaction of two hands or in-hand manipulation), generalization is a hard problem. Current methods still either require a substantial amount of (supervised) training data and / or strong assumptions on both the environment and the task. In this paradigm, controllers solving these tasks tend to be complex. We propose a paradigm of maintaining simpler controllers solving the task in a small number of specific situations. In order to generalize to novel situations, the robot transforms the environment from novel situations into a situation where the solution of the task is already known. Our solution to this problem is to play with objects and use previously trained skills (basis skills). These skills can either be used for estimating or for changing the current state of the environment and are organized in skill hierarchies. The approach is evaluated in complex pick-and-place scenarios that involve complex manipulation. We further show that these skills can be learned by autonomous playing.	autonomous robot;programming paradigm;smt placement equipment;simulation;supervised learning;thinking outside the box	Simon Hangl;Emre Ugur;Sándor Szedmák;Justus H. Piater	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759434	simulation;computer science;artificial intelligence;electronic countermeasure;robot kinematics	Robotics	49.92766547608161	-27.67449360316658	169330
70b23c6d8c713f5663603343b19ea5873b6696cc	machine learning in tracking associations with stereo vision and lidar observations for an autonomous vehicle	ukf machine learning tracking associations stereo vision lidar observations autonomous vehicle obstacles detection road safety applications dangerous situations fusion algorithm data association heterogeneous sensors observations multidimensional structure association costs unscented kalman filter;laser radar sensor fusion radar tracking three dimensional displays reliability cameras;stereo image processing collision avoidance kalman filters learning artificial intelligence mobile robots nonlinear filters object tracking optical radar road safety road vehicles sensor fusion	Obstacles detection is used nowdays for a number of road safety applications, increasing the drivers awareness in potential dangerous situations. A reliable and robust obstacles detection continues to be largely investigated and still remains an open challenge, especially for difficult scenarios and in general cases, with loosened constraints and multiple simultaneous use-cases. This work presents an obstacles detection, tracking and fusion algorithm which allows to reconstruct the environment surrounding the vehicle. While the techniques used for the detection are well-known in literature, the improvements introduced by this paper regard the data association and tracking approach of heterogeneous sensors observations. An innovative multi-dimensional structure based on association costs originating from a classifier provides an optimal solution to the association problem with respect to the total association cost. An Unscented Kalman Filter (UKF) managing a variable number of observations, arbitrarily composable, allows to correctly address the combined tracking and fusion challenge. The results, obtained on a public benchmark, show improvements with respect to state of the art systems.	algorithm;autonomous robot;benchmark (computing);correspondence problem;kalman filter;machine learning;sensor;statistical classification;stereopsis	Marco A Allodi;Alberto Broggi;Domenico Giaquinto;Marco Patander;Antonio Prioletti	2016	2016 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2016.7535456	computer vision;simulation;tracking system;geography;remote sensing	Robotics	49.100184311496925	-36.759816028154006	170148
007f4bde7d2ed0d774e62820de600b2db4707a0d	fusion-layer-based machine vision for intelligent transportation systems	intelligent transportation system;proposed obstacle detection methodology;historic information;related information;dynamic obstacle;obstacle information estimation;fusion-layer-based machine vision;environment understanding ability;timing information;individual information;different information;environment understanding technology	Environment understanding technology is very vital for intelligent vehicles that are expected to automatically respond to fast changing environment and dangerous situations. To obtain perception abilities, we should automatically detect static and dynamic obstacles, and obtain their related information, such as, locations, speed, collision/occlusion possibility, and other dynamic current/historic information. Conventional methods independently detect individual information, which is normally noisy and not very reliable. Instead we propose fusion-based and layered-based information-retrieval methodology to systematically detect obstacles and obtain their location/timing information for visible and infrared sequences. The proposed obstacle detection methodologies take advantage of connection between different information and increase the computational accuracy of obstacle information estimation, thus improving environment understanding abilities, and driving safety. Thesis Supervisor: Berthold K.P. Horn Title: Professor of Electrical Engineer and Computer Science Department	computer science;electrical engineering;information retrieval;machine vision	Yajun Fang	2010				Robotics	48.821474992841694	-35.84520630060693	170569
b944ec089f19993f548a288e40f92fe4b23cfc9b	integrating active perception with an autonomous robot architecture	sensor system;mobile robot;real time;proximity space;markers;autonomous agent;system integration;perceptual memory;indoor environment;stereo vision;agent architecture;autonomous robot	Today’s robotics applications require complex, real-time, high-bandwidth sensor systems. Although many such systems have been developed, integrating them into an autonomous agent architecture remains an area of active research. We have integrated an active stereo vision system with an autonomous agent architecture using a system of perceptual memory. Perceptual memory is an important class of memory because it is designed for the ‘behavior-based’ portion of the agent’s architecture, and not the deliberative portion. This memory maintains current and recent task-dependent perceptual information, as well as expectations about the agent’s immediate environment. Our system of perceptual memory is composed of visual primitives from our stereo system, called proximity spaces. Each proximity space represents a virtual fovea or locus of the agent’s attention. As an application, we present a robot that uses our system of perceptual memory and proximity spaces to ‘attend to’ multiple humans in a complex and unstructured indoor environment.	autonomous robot	Glenn S. Wasson;David Kortenkamp;Eric Huber	1999	Robotics and Autonomous Systems	10.1016/S0921-8890(99)00050-0	agent architecture;mobile robot;computer vision;simulation;proximity space;computer science;stereopsis;artificial intelligence;autonomous agent;system integration	Robotics	47.44984013009627	-32.882122999131404	170783
7ca9b2ca25b16618e6a51e1b2242a20a4aede32e	templates for pre-grasp sliding interactions	humanoid robotics;sliding;object manipulation;pushing;abt schaal;pre grasp interaction	In manipulation tasks that require object acquisition, pre -grasp interaction such as sliding adjusts the object in the environment before grasping. This change in object placement ca improve grasping success by making desired grasps reachable. However, the additional sliding action prior to grasping introduces more complexity to the motion planning process, since the hand pose relative to the object does not need to remain fixed during the pre-grasp interaction. Furthermore, anthropomorphic hands in humanoid robots hav e se eral degrees of freedom that could be utilized to improve the object interaction beyond a fixed grasp shape. We present a framework for synthesizing pre-grasp interactions for high-dimensional anthropomorphic manipulato rs. The motion planning is tractable because information from pre-grasp manipulation examples reduces the search sp a e to promising hand poses and shapes. In particular, we show the value of organizing the example data according to ob ject category templates. The template information focuses the search based on the object features, resulting in i creased success of adapting a template pose and decreased planning time.	cobham's thesis;humanoid robot;interaction;motion planning;organizing (structure)	Daniel Kappler;Lillian Y. Chang;Nancy S. Pollard;Tamim Asfour;Rüdiger Dillmann	2012	Robotics and Autonomous Systems	10.1016/j.robot.2011.07.015	computer vision;simulation;computer science;humanoid robot;artificial intelligence	Robotics	50.605333504212425	-27.03629018679755	170852
41b6d0907f8c0f795d4e498d6f50ebc2b972d9f4	thinning algorithms on rectangular, hexagonal, and triangular arrays	image processing;skeleton;thinning algorithms;rectangular;hexagonal;triangular arrays	In this report three thinning algorithms are developed: one each for use with rectangular, hexagonal, and triangular arrays. The approach to the development of each algorithm is the same. Pictorial results produced by each of the algorithms are presented and the relative performances of the algorithms are compared. It is found that the algorithm operating with the triangular array is the most sensitive to image irregularities and noise, yet it will yield a thinned image with an overall reduced number of points. It is concluded that the algorithm operating in conjunction with the hexagonal array has features which strike a balance between those of the other two arrays.	algorithm;battle of midway;experiment;image;performance;thinning	Edward S. Deutsch	1972	Commun. ACM	10.1145/361573.361583	mathematical optimization;image processing;computer science;mathematics;programming language;hexagonal crystal system;skeleton	EDA	45.370433121390604	-30.066797723036018	171092
763a0b7639a10b9738ee02a8e31b44dce808648e	efficient sequential-hierarchical deployment strategy for heterogeneous sensor networks	radar tracking;approximation algorithms;resource management;resource management optimization noise measurement radar tracking approximation algorithms vectors;noise measurement;vectors;optimization	An efficient strategy solution is developed for a specific deployment problem in which different types of sensors are required to simultaneously cover the same area of interest. The deployment goal is to select the sensor positions and orientations in such a way that the sensor network coverage is optimized. A general challenge within resource allocation problems is that, even with small-scale sensor networks, the number of possible final deployment solutions expands very fast and the problem becomes intractable. We assume that it is acceptable to trade solution optimality against algorithm speed. In this case, algorithms can be based on greedy and/or divide-and-conquer principles, which both results in good computational efficiency. We developed an efficient algorithm in three steps. Firstly, we developed a global search algorithm, but with improvements that reduce the search space significantly without losing optimality. Secondly, we transformed the global algorithm into a sequential and a hierarchical algorithm for more efficiency at the cost of optimality. Thirdly, we combine the sequential and hierarchical principles into one algorithm which results in even higher efficiency. In the end, the algorithms are evaluated with the use of an extensive testing scheme which generates many random cases for solving.	computation;greedy algorithm;heuristic (computer science);search algorithm;sensor;software deployment	Teun H. de Groot;Oleg A. Krasnov;Alexander G. Yarovoy	2014	2014 IEEE Ninth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)	10.1109/ISSNIP.2014.6827612	mathematical optimization;simulation;computer science;machine learning	Robotics	52.544827925909985	-25.990078340006928	171527
d220944bf7633bd8fcfaedc7ec70a740ed6357c2	off the beaten track: predicting localisation performance in visual teach and repeat	vision based localisation appearance based approach vision based localisation system gaussian process regressor interpolation;visualization trajectory robots gaussian processes planning cameras training;robot vision gaussian processes interpolation	This paper proposes an appearance-based approach to estimating localisation performance in the context of visual teach and repeat. Specifically, it aims to estimate the likely corridor around a taught trajectory within which a vision-based localisation system is still able to localise itself. In contrast to prior art, our system is able to predict this localisation envelope for trajectories in similar, yet geographically distant locations where no repeat runs have yet been performed. Thus, by characterising the localisation performance in one region, we are able to predict performance in another. To achieve this, we leverage a Gaussian Process regressor to estimate the likely number of feature matches for any keyframe in the teach run, based on a combination of trajectory properties such as curvature and an appearance model of the keyframe. Using data from real traversals, we demonstrate that our approach performs as well as prior art when it comes to interpolating localisation performance based on a number of repeat runs, while also performing well at generalising performance estimation to freshly taught trajectories.	expectation propagation;gaussian process;interpolation;key frame;language localisation;tree traversal	Julie Dequaire;Chi Hay Tong;Winston Churchill;Ingmar Posner	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487209	computer vision;simulation;computer graphics (images)	Robotics	49.926628216845174	-37.62378948318766	171568
3daa2ed8f9aa42cb6deaca81044ac68e55d8ae92	physarum boats: if plasmodium sailed it would never leave a port	shortest path;oscillations;motility;physarum polycephalum;computer experiment;information processing;proximity graph;biological robots	Plasmodium of Physarum polycephalum is a single huge (visible by naked eye) cell with myriad of nuclei. The plasmodium is a promising substrate for non-classical, nature-inspired, computing devices. It is capable for approximation of shortest path, computation of planar proximity graphs and plane tessellations, primitive memory and decision-making. The unique properties of the plasmodium make it an ideal candidate for a role of amorphous biological robots with massive parallel information processing and distributed inputs and outputs. We show that when adhered to light-weight object resting on a water surface the plasmodium can propel the object by oscillating its protoplasmic pseudopodia. In experimental laboratory conditions and computational experiments we study phenomenology of the plasmodium-floater system, and possible mechanisms of controlling motion of objects propelled by on board plasmodium. K eywords: Physarum polycephalum, motility, biological robots	approximation;cellular automaton;computation;computer simulation;experiment;gradient;information processing;motion capture;propel;robot;shading;shortest path problem;sorting;williams tube	Andrew Adamatzky	2009	CoRR	10.1080/11762320902863890	simulation;computer experiment;information processing;computer science;shortest path problem;oscillation;motility	Robotics	51.78840022663535	-28.929120722838572	171744
6cfb5976ecf1168d36d90fe65bda0bbae145001b	environment-aware sensor fusion for obstacle detection	robot sensing systems;manuals;trajectory;roads;sensor fusion;cameras	Reliably detecting obstacles and identifying traversable areas is a key challenge in mobile robotics. For redundancy, information from multiple sensors is often fused. In this work we discuss how prior knowledge of the environment can improve the quality of sensor fusion, thereby increasing the performance of an obstacle detection module. We define a methodology to quantify the performance of obstacle detection sensors and algorithms. This information is used for environment-aware sensor fusion, where the fusion parameters are dependent on the past performance of each sensor in different parts of an operation site. The method is suitable for vehicles that operate in a known area, as is the case in many practical scenarios (warehouses, factories, mines, etc). The system is “trained” by manually driving the robot through a suitable trajectory along the operational areas of a site. The performance of a sensor configuration is then measured based on the similarity between the manually-driven trajectory and the trajectory that the path planner generates after detecting obstacles. Experiments are performed on an autonomous ground robot equipped with 2D laser sensors and a monocular camera with road detection capabilities. The results show an improvement in obstacle detection performance in comparison with a “naive” sensor fusion, illustrating the applicability of the method.	algorithm;autonomous robot;internationalization and localization;mobile robot;robotics;sensor web	Adrian Rechy Romero;Paulo Vinicius Koerich Borges;Alberto Elfes;Andreas Pfrunder	2016	2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2016.7849476	computer vision;simulation;soft sensor;engineering;remote sensing	Robotics	53.52901110045023	-34.78849867754584	171952
bd26883d4c90d18015b034100c6ac2fa6e11fb63	autonomous viewpoint selection of robots based on aesthetic composition evaluation of a photo	silicon;art;robot vision mobile robots motion control path planning position control;fitting;visualization;monitoring;robots;robots visualization correlation monitoring art fitting silicon;autonomous viewpoint selection moving control path planning monitoring robot observation position searching method evaluation function triangle composition diagonal composition rule of third aesthetic values humans aesthetic evaluation composition rules photography painting aesthetic composition evaluation robots;correlation	In the field of painting and photography, composition rules are widely used under various circumstances in order to arouse humans' aesthetic evaluation. In this paper, we propose a method to evaluate the aesthetic values of a scene according to some certain composition rules, such as Rule of Third, Diagonal Composition and Triangle Composition. We propose an evaluation function with three factors for each kind of compositions, which is reasonable to describe properties of compositions as the result showed in our questionnaire. We develop an observation position searching method by estimating relationships between targets from different viewpoints of the monitoring robot. Then a score will be obtained for each viewpoint using our evaluation function. By these scores, the best observation position can be determined within a reachable field. With path planning and moving control, the monitoring robot arrives at the best observation position. We suppose our research can provide any hint about common understanding between human beings and robots.	algorithm;autonomous robot;evaluation function;humans;image editing;motion planning;viewpoint	Kai Lan;Kousuke Sekiyama	2015	2015 IEEE Symposium Series on Computational Intelligence	10.1109/SSCI.2015.51	computer vision;simulation;engineering;artificial intelligence	Robotics	51.45332422587646	-37.16215774696671	171999
e2f58dc6d809910c0811d4857cc39a320278ba77	slam with conceptualisation	slam robots mobile robots path planning;lasers;robot sensing systems;cognitive map;mobile robot;mapping of cluttered spaces cognitive map robot mapping global map asr shape room shape;path planning;mobile robots;simultaneous localization and mapping mobile robots orbital robotics robot sensing systems robot kinematics robot vision systems automatic speech recognition robotics and automation artificial intelligence shape;robot mapping;room shape;laser range finder;shape;global map;indoor environment;space robotics;robots;simultaneous localization and mapping;asr shape;speech recognition;robot mapping slam robot simultaneous localization and mapping rooms juxtaposing representations corridors juxtaposing representations;slam robots;mapping of cluttered spaces;robot kinematics	In this paper, we presented a novel approach for robot Simultaneous Localization and Mapping (SLAM) in an indoor environment. We observed that such an environment is partitioned into rooms and corridors. We thus propose that the robot should compute a map beginning with a conceptual view of its environment rather than a physical view of it. Our approach thus simplifies the localization problem by not having to consider all surfaces perceived. This is especially useful since in the real world, the environment is often cluttered with objects. Instead, and as shown in our experiments, the SLAM problem is solved by juxtaposing representations of rooms and corridors computed. Our approach was tested using a mobile robot equipped with an odometer and a laser range finder. Our initial result shows our robot successfully juxtaposed three rooms to form a correct global map.	algorithm;automated system recovery;computation;experiment;high- and low-level;local coordinates;mobile robot;simultaneous localization and mapping	Mee-Loong Yang;Wai-Kiang Yeap	2008	2008 10th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2008.4795700	mobile robot;computer vision;simulation;computer science;artificial intelligence	Robotics	51.60656470509734	-36.67230559288576	172048
8f089786572f872eaabe3cb46e0504a2bd315a8c	ergodic coverage in constrained environments using stochastic trajectory optimization		In search and surveillance applications in robotics, it is intuitive to spatially distribute robot trajectories with respect to the probability of locating targets in the domain. Ergodic coverage is one such approach to trajectory planning in which a robot is directed such that the percentage of time spent in a region is in proportion to the probability of locating targets in that region. In this work, we extend the ergodic coverage algorithm to robots operating in constrained environments and present a formulation that can capture sensor footprint and avoid obstacles and restricted areas in the domain. We demonstrate that our formulation easily extends to coordination of multiple robots equipped with different sensing capabilities to perform ergodic coverage of a domain.	algorithm;basis function;dynamical system;encode;ergodic theory;ergodicity;first-order predicate;iterative method;kullback–leibler divergence;mathematical optimization;multi-agent system;nonlinear system;obstacle avoidance;robot;robotics;sampling (signal processing);trajectory optimization	Elif Ayvali;Hadi Salman;Howie Choset	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206410	simulation;control engineering;ergodic theory;trajectory optimization;computer science;robot;artificial intelligence;mathematical optimization;trajectory;robotics	Robotics	51.97494587842977	-25.75471999322911	172164
f1f7b5d0990e0bfaec22d3568cbd651f62ce15a7	visual navigation using place recognition with visual line words	navigation visualization robots bayes methods predictive models educational institutions vocabulary;bayes methods;vocabulary;robot vision image sequences mobile robots motion control object recognition;navigation;visualization;robots;predictive models;bayesian model topological map visions based navigation;mobile robot visual line words place recognition technique linked images world environment image sequence visual navigation system	We propose a navigation framework using the place recognition technique. We represent the world environment as linked images with visual line words. Navigation in the proposed system works by traversing a sequence of images. In attached video, we show our visual navigation system.	machine vision	Yong Nyeon Kim;Dong Wook Ko;Il Hong Suh	2014	2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2014.7057494	robot;computer vision;navigation;simulation;visualization;computer science;artificial intelligence;machine learning;predictive modelling;mobile robot navigation	Robotics	50.20598734777785	-37.61701933167152	172365
d01bbfad0f0894ff31bdd8c1452df71aca43d021	integrating haptics with augmented reality in a femoral palpation and needle insertion training simulation	augmented reality haptics force feedback tactile display medical simulation graphic rendering virtual reality;chroma key techniques augmented reality simulation femoral palpation needle insertion training simulation virtual environment interventional radiology procedures palpsim virtual patient tactile feedback off the shelf force feedback devices hydraulic interface phantom omni end effector;interventional radiology;medical simulation;training;virtual reality;research publications;indexing terms;haptics;graphic rendering;medical computing;force feedback;visualization;virtual patients;tactile feedback;tactile display;solid modeling;computer based training;face;medical computing augmented reality computer based training digital simulation end effectors force feedback haptic interfaces;virtual environment;augmented reality;haptic interfaces;off the shelf;needles;digital simulation;end effectors;needles solid modeling training force feedback visualization face	This paper presents a virtual environment for training femoral palpation and needle insertion, the opening steps of many interventional radiology procedures. A novel augmented reality simulation called PalpSim has been developed that allows the trainees to feel a virtual patient using their own hands. The palpation step requires both force and tactile feedback. For the palpation haptics effect, two off-the-shelf force feedback devices have been linked together to provide a hybrid device that gives five degrees of force feedback. This is combined with a custom built hydraulic interface to provide a pulse like tactile effect. The needle interface is based on a modified PHANTOM Omni end effector that allows a real interventional radiology needle to be mounted and used during simulation. While using the virtual environment, the haptics hardware is masked from view using chroma-key techniques. The trainee sees a computer generated patient and needle, and interacts using their own hands. This simulation provides a high level of face validity and is one of the first medical simulation devices to integrate haptics with augmented reality.	augmented reality;blinded;clinical act of insertion;haptic technology;high-level programming language;insertion mutation;interface device component;interventional radiologic procedures;palpation;patients;phantoms, imaging;radiology;robot end effector;simulation;virtual reality;vision	Timothy Richard Coles;Nigel W. John;Derek Gould;Darwin G. Caldwell	2011	IEEE Transactions on Haptics	10.1109/TOH.2011.32	medical simulation;computer vision;augmented reality;interventional radiology;simulation;computer science;engineering;artificial intelligence;virtual reality;multimedia;haptic technology	Visualization	39.43193125313884	-37.9128085713741	172749
092efeb191e66c231fb2ea36a15c25198914c36b	parallel pattern recognition computations within a wireless sensor network	distributed algorithms;finite element analysis;pattern matching;pattern recognition;wireless sensor networks;artificial nervous system;data storage;distributed algorithm;external loading conditions;finite element model;internal stress state;parallel computation;pattern recognition;sensor output;wireless sensor network	The computational properties of a wireless sensor network (WSN) have been investigated by implementing a fully distributed pattern recognition algorithm within the network. It is shown that the set up allows a physical object to develop a capability, which to some extent may be considered similar to our sense of touch, with the WSN acting as an artificial nervous system in this regard. The effectiveness of the algorithm is inspected by comparing the outputs from the sensors with the stress patterns generated through a simple finite element model and then stored within the network. It is shown that the test object could successfully differentiate between its internal stress states resulting from the changes to its external loading conditions. Suitability of the algorithm is discussed with respect to the data storage requirement per node of the WSN.	algorithm;computation;computer data storage;finite element method;pattern recognition;sensor	Asad I. Khan;Patrik Mihailescu	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334332	distributed algorithm;real-time computing;wireless sensor network;computer science;theoretical computer science;finite element method;brooks–iyengar algorithm;distributed computing;key distribution in wireless sensor networks;visual sensor network	Robotics	45.22788490565141	-28.87146430459194	172930
d656c537b5180320cc7a103c2f3886fd54152c44	on the use of stochastic driver behavior model in lane departure warning	modele comportement;time to line crossing tlc;modelizacion;behavior model;detection erreur;driver correction event;errors;driver adaptation;deteccion error;behavioral analysis;etude experimentale;behavioural sciences computing;computer model;modelo comportamiento;cambio de carril;direct sequence;lane departure warning;probabilistic approach;lane change;vehicle driver;driver information systems behavioural sciences computing;modelisation;computational modeling;adaptation model;trajectory;stochastic processes;directional sequence of piecewise lateral slopes;roads;enfoque probabilista;approche probabiliste;analyse comportementale;equal error rate;mathematical model;changement de voie;warning systems;conductor vehiculo;driver circuits;driver circuits vehicles trajectory adaptation model roads mathematical model computational modeling;analisis conductual;drivers;vehicles;experimental evaluation;error detection;nuisance warning;driver behavior;stochastic driver behavior model;modeling;lane departure;estudio experimental;driver information systems;lane changing;time to line crossing tlc driver adaptation driver behavior model lane departure nuisance warning;lane departures;driver behavior model;directional sequence of piecewise lateral slopes stochastic driver behavior model lane departure warning driver correction event;conducteur vehicule	In this paper, we propose a new framework for discriminating the initial maneuver of a lane-crossing event from a driver correction event, which is the primary reason for false warnings of lane departure prediction systems (LDPSs). The proposed algorithm validates the beginning episode of the trajectory of driving signals, i.e., whether it will cause a lane-crossing event, by employing driver behavior models of the directional sequence of piecewise lateral slopes (DSPLS) representing lane-crossing and driver correction events. The framework utilizes only common driving signals and allows the adaptation scheme of driver behavior models to better represent individual driving characteristics. The experimental evaluation shows that the proposed DSPLS framework has a detection error with as low as a 17% equal error rate. Furthermore, the proposed algorithm reduces the false-warning rate of the original lane departure prediction system with less tradeoff for the correct prediction.	algorithm;behavior model;lateral thinking	Pongtep Angkititrakul;Ryuta Terashima;Toshihiro Wakita	2011	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2010.2072502	behavioral modeling;stochastic process;simulation;error detection and correction;systems modeling;computer science;lane departure warning system;trajectory;warning system;mathematical model;mathematics;computational model;operations research;statistics	Mobile	43.13837683823029	-28.12318204402507	173211
431b38171edc5904b357950d2ac7a0fe366edfa2	autonomous vision-based safe area selection algorithm for uav emergency forced landing		In order to solve forced landing for Unmanned Aerial Vehicle (UAV) in emergency situations, an autonomous vision-based algorithm to select safe forced landing area was proposed. To detect the slowly changing edges and weak edges, the algorithm improved the canny operator to detect safe landing area without obstacles. Then eroding and dilating close operation was adopted to select the area. To analyze its terrain, the DEM data was acquired. By extracting features of interesting area, fast classification and recognition of landing area based on Bayesian theory was carried out. The algorithm presents a fast and accurate result on the forced landing area selection for UAVs.	selection algorithm;unmanned aerial vehicle	Aiying Lu;Wenrui Ding;Jiaxing Wang;Hongguang Li	2012		10.1007/978-3-642-34041-3_37	simulation;engineering;aeronautics;remote sensing	Robotics	49.48454578931554	-36.049909081305096	173236
cd4bab5d6845c2141c9b3b635d99dce1db446028	dense semantic stereo labelling architecture for in-campus navigation.		Interest on autonomous vehicles has rapidly increased in the last few years, due to recent advances in the field and the appearance of semi-autonomous solutions in the market. In order to reach fully autonomous navigation, a precise understanding of the vehicle surroundings is required. This paper presents a novel ROSbased architecture for stereo-vision-based semantic scene labelling. The objective is to provide the necessary information to a path planner in order to perform autonomous navigation around the university campus. The output of the algorithm contains the classification of the obstacles in the scene into four different categories: traversable areas, garden, static obstacles, and pedestrians. Validation of the labelling method is accomplished by means of a hand-labelled ground truth, generated from a stereo sequence captured in the university campus. The experimental results show the high performance of the proposed approach.	algorithm;autonomous car;autonomous robot;computation;computer vision;ground truth;learning classifier system;motion planning;out-of-order execution;plug and play;robot operating system;semiconductor industry;sensor;stereopsis	Jorge Beltrán;Carlos Jaraquemada;Basam Musleh;Arturo de la Escalera;Jose M. Armingol	2017		10.5220/0006131602660273	computer vision	Robotics	53.102814944090724	-31.316235082898412	173354
dcb732b99d34e8c5bffdbff067b5a68288e85977	cooperative construction and maintenance of maps for autonomous navigation	intelligent sensor;navegacion;filtro kalman;incertidumbre;uncertainty;autonomous system;cooperation;real time;filtre kalman;dynamic model;kalman filter;prior knowledge;robotics;silla de ruedas;cooperacion;sistema autonomo;human behavior;transductor ultrasonido;navigation;codificacion;prevencion esquiva colision;indoor environment;prevention esquive collision;systeme autonome;coding;wheel chair;estimacion parametro;robotica;ultrasonic transducer;autonomous navigation;collision avoidance;robotique;incertitude;dead reckoning;parameter estimation;estimation parametre;codage;transducteur ultrason;fauteuil roulant;ultrasonic sensor	Abstract   In this paper, an uncertain representation of an indoor environment is used to provide the robot with an a priori map. The major originality of such a representation comes from its human origin. In our specific disabled-oriented application, the on-board person is used as an “intelligent sensor” which provides the system with some pertinent prior knowledge of its environment. No current sensor is able to provide information such as shape, dimensions and location relative evaluation of obstacles, etc. with such accuracy and rapidity. Using a man-made representation raises the problem of uncertainties in interpretation and evaluation. Our solution consists in using proximity relations between the geometrical primitives of the object, thus simply modeling human behavior when representing the surrounding. The dynamic aspect of multivalue number coding is used for the first time which allows real-time alterations of the encoded objects. The proposed dynamic modeling process has been tested in a real indoor environment using an experimental wheelchair equipped with ultrasonic sensors and a dead-reckoning system.	autonomous robot;map	Olivier Habert;Alain Pruski	1997	Robotics and Autonomous Systems	10.1016/S0921-8890(96)00050-4	computer vision;simulation;computer science;ultrasonic sensor;robotics;human behavior	Robotics	51.85569054556828	-35.608053004587816	173540
dc5e4634fdbbbf8cdaa8ee08d082c29f4a822750	real-time visual sinkage detection for planetary rovers	vision system;soil wheel interaction;planetary robotics	Identifying wheel sinkage for planetary exploration rovers can give a critical insight about the terrain traversability and in particular the characteristics of deformable soils that the rover travels on. This paper presents a monocular vision based approach that can detect and estimate the sinkage of a hybrid legged wheel in real time and robustly, with little sensitivity to changing operational conditions. The proposed method involves color-space segmentation that identifies the leg contour and consequently depth of wheel sinkage into the regolith. In addition, it enables dynamic analysis of the sinkage, hence making detection of non-geometric hazards possible while the rover moves. Extensive field trials have been conducted on natural deformable terrain. The experimental results demonstrate that the average discrepancy against annotated images is less than 1%. © 2015 Elsevier B.V. All rights reserved.	algorithm;automatic image annotation;central processing unit;color space;destructible environment;discrepancy function;high-level programming language;human error;planetary scanner;real-time transcription;region of interest;rover (the prisoner);sensor;usb hub;while	Conrad Spiteri;Said Al-Milli;Yang Gao;Aridane Sarrionandia de León	2015	Robotics and Autonomous Systems	10.1016/j.robot.2015.06.009	computer vision;simulation;machine vision;computer science;artificial intelligence	Robotics	49.579822734377984	-36.96301052619235	173558
9472551cc837d4538770691686ff2698b0d23d28	belief driven manipulator control for integrated searching and tracking	manipulators bayes methods belief networks feedforward;target tracking manipulator dynamics control systems bayesian methods kinematics intelligent robots pulp manufacturing australia robot sensing systems constraint optimization;belief networks;eye in hand sensor;manipulators;belief function;feedforward;sensor model;non linear optimization;bayes methods;probability density function;belief driven manipulator control;robot manipulator;target motion model;configuration space;integrated searching;feedforward control;tracking task;eye in hand sensor belief driven manipulator control integrated searching tracking task feedforward control probability density function recursive bayesian process target motion model;recursive bayesian process;conference proceeding;probability of detection	This paper presents a feedforward control strategy for a robotic manipulator based on a belief function. The belief about a target's next location, as described by a probability density function, is maintained by a recursive Bayesian process that fuses observations with a target motion model. A sensor model that incorporates positive and negative sensor readings allows the single belief function to be used to deliver both searching and tracking behaviors. Constrained non-linear optimization is used to search configuration space for the control action that maximizes the subsequent probability of detection. To demonstrate application of the technique, a simple example is elaborated for a searching and tracking task with an eye-in-hand sensor	control theory;feed forward (control);feedforward neural network;inverse kinematics;joint constraints;linear programming;loss function;mathematical optimization;nonlinear programming;nonlinear system;optimization problem;parsing;recursion;robot;seamless3d;uncontrolled format string;velocity (software development);virtual instrument software architecture	Stephen Webb;Tomonari Furukawa	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.282523	control engineering;computer science;machine learning;control theory;feed forward;statistics	Robotics	51.40950223900733	-30.59188419556704	173631
e7bfeea3baa161d5599e16a13532d82078350405	i.m.o.g.e.n.e.-a solution to the real time animation problem		Current graphics processors are very slow for displaying shaded 3D objects. A lot of work is being done in order to define faster display processors by using massive parallelism and VLSI components. Our proposal goes along this line with the supplemen­ tary aim of displaying images in real time, i.e., 25 or 30 times per second. We choose to design a graphics module without any working memory and thus without frame buffer. A massive parallelism over objects, and thus a pixel pipe-line, are used. Each Object Pro­ cessor handles one 3D object; all the processors work in a synchronous way, processing the same pixel simultaneously at pixel rate. These processors are built from very simple Elementary Processors (2 adders, 2 registers and 6 memory words) computing linear or quadratic expressions V(x,y), where (x,y) are the coordinates of a pixeL A pipelined tree made of basic operators (min, max, or, and, ... ) gathers the results given by the Ob­ ject Processors and makes inter-objects operations, at least hidden part elimination. Such a choice of course involves a high hardware complexity when displaying rather sim­ ple scenes. However, we feel that it is the price to pay for building graphics processors allowing real-time interactive animation (e.g., the graphics unit of a driving simulator).	central processing unit;driving simulator;framebuffer;graphics processing unit;maxima and minima;parallel computing;pixel;real-time transcription;shading;simulation;very-large-scale integration	Christophe Chaillou;Michel Mériaux;Sylvain Karpf	1990		10.2312/EGGH/EGGH90/139-151	real-time computing;computer hardware;computer science;computer graphics (images)	Graphics	43.57056835878822	-31.697691241575445	173641
b09266aa66ed6f7cf7a852e1a2a4908c336f2f24	hdr-artist: high dynamic range advanced real-time imaging system	histograms;dynamic range hardware real time systems imaging streaming media computer architecture histograms;computer architecture;streaming media;imaging;dynamic range;hardware;real time systems	This paper describes the HDR-ARtiSt hardware platform, a FPGA-based architecture that can produce a real-time high dynamic range video from successive image acquisition. The hardware platform is built around a standard low dynamic range (LDR) CMOS sensor and a Virtex 5 FPGA board. The CMOS sensor is a EV76C560 provided by e2v. This 1.3 Megapixel device offers novel pixel integration/readout modes and embedded image pre-processing capabilities including multiframe acquisition with various exposure times. Our approach consists of a hardware architecture with different algorithms: double exposure control during image capture, building of an HDR image by combining the multiple frames, and final tone mapping for viewing on a LCD display. Our video camera system is able to achieve a real-time video rate of 30 frames per second for a full sensor resolution of 1,280 × 1,024 pixels.	algorithm;benchmark (computing);cmos;embedded system;field-programmable gate array;high dynamic range;high-dynamic-range rendering;image quality;image sensor;ldraw;map;microprocessor development board;preprocessor;real-time clock;real-time transcription;requirement;tone mapping;virtex (fpga)	Pierre-Jean Lapray;Barthélémy Heyrman;Matthieu Rossé;Dominique Ginhac	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6271513	medical imaging;embedded system;computer vision;dynamic range;real-time computing;computer science;histogram;computer graphics (images)	Robotics	43.95512643693592	-34.635080167684265	173781
7209f87de23d899efc38b1f139fb3f8812b8d084	robot-aided neurorehabilitation: a novel robot for ankle rehabilitation	frotamiento;rehabilitation robots ankle robot foot drop neurorehabilitation;foot drop robot aided neurorehabilitation massachusetts institute of technology stroke patient ankle rehabilitation baltimore veterans administration medical center ambulatory performance backdriveable device mechanical impedance clinical measurement tool locomotor rehabilitation orthopaedics rehabilitation;stroke patient ankle rehabilitation;caracteristique mecanique;measurement tool;patient rehabilitation biomechanics biomedical measurement medical robotics neurophysiology orthopaedics;rehabilitation;ambulatory performance;backdriveable device;impedancia mecanica;rehabilitacion;nervous system;extremities;medical tests;patient rehabilitation;ambulatory;biomechanics;foot;impedance mecanique;locomotion;reeducacion;intelligence artificielle;locomotor rehabilitation;robotics;usa councils;indexing terms;orthopaedics;tobillo;upper extremity;medical robotics;mechanical engineering;ankle robot;ankle;ambulatoire;guidelines;rehabilitation robotics;neurorehabilitation;frottement;veterans administration;cheville;rehabilitation robotics nervous system medical robotics mechanical engineering medical treatment foot usa councils medical tests guidelines extremities;clinical measurement tool;rehabilitation robots;baltimore veterans administration medical center;mechanical characteristic;robotica;artificial intelligence;robotique;inteligencia artificial;ambulatorio;readaptacion fisica;neurophysiology;caracteristica mecanica;medical treatment;pie;mechanical impedance;friction;readaptation physique;massachusetts institute of technology;locomocion;orthopaedics rehabilitation;biomedical measurement;robot aided neurorehabilitation;foot drop;pied;reeducation;physical rehabilitation	In this paper, we present the design and characterization of a novel ankle robot developed at the Massachusetts institute of technology (MIT). This robotic module is being tested with stroke patients at Baltimore Veterans administration medical center. The purpose of the on-going study is to train stroke survivors to overcome common foot drop and balance problems in order to improve their ambulatory performance. Its design follows the same guidelines of our upper extremity designs, i.e., it is a low friction, backdriveable device with intrinsically low mechanical impedance. Here, we report on the design and mechanical characteristics of the robot. We also present data to demonstrate the potential of this device as an efficient clinical measurement tool to estimate intrinsic ankle properties. Given the importance of the ankle during locomotion, an accurate estimate of ankle stiffness would be a valuable asset for locomotor rehabilitation. Our initial ankle stiffness estimates compare favorably with previously published work, indicating that our method may serve as an accurate clinical measurement tool.	characteristic impedance;robot	Anindo Roy;Hermano Igo Krebs;Dustin J. Williams;Christopher T. Bever;Larry W. Forrester;Richard M. Macko;Neville Hogan	2009	IEEE Transactions on Robotics	10.1109/TRO.2009.2019783	neurorehabilitation;ambulatory;index term;computer science;artificial intelligence;biomechanics;friction;mechanical impedance;nervous system;robotics;neurophysiology;foot	Robotics	40.27576087595715	-37.447234180424275	174021
382539718f3438303bb9a7f590bb76428fb97314	local data fusion algorithm for fire detection through mobile robot	multiple sensors;environment monitoring;probability;sensors;prototype tests;multisensor data fusion;temperature measurement fires mobile robots probability sensor fusion;mobile robots;data fusion;fire detectors;fire detection;signal processing;temperature measurements local data fusion algorithm multisensor data fusion smart buildings environment monitoring industry applications defense applications false alarm minimization detection probability maximization low cost mobile robot fire detection missions luminosity measurements;data fusion applications;trabalho apresentado em evento;algorithms;data integration temperature sensors robot sensing systems fires mobile robots;temperature measurement;sensor fusion;fires;fire occurrences;probability of detection	Multisensor data fusion is a technique that combines the readings of multiple sensors to detect some phenomenon. Data fusion applications are numerous and they can be used in smart buildings, environment monitoring, industry and defense applications. The main goal of multisensor data fusion is to minimize false alarms and maximize the probability of detection based on the detection of multiple sensors. In this paper a local data fusion algorithm based on luminosity, temperature and flame for fire detection is presented. The data fusion approach was embedded in a low cost mobile robot. The prototype test validation has indicated that our approach can detect fire occurrence. Moreover, the low cost project allow the development of robots that could be discarded in their fire detection missions.	algorithm;embedded system;mobile robot;prototype;sensor;test suite	Guilherme Freire Roberto;Kalinka Regina Lucas Jaquie Castelo Branco;José Marcio Machado;Alex R. Pinto	2013	2013 14th Latin American Test Workshop - LATW	10.1109/LATW.2013.6562667	simulation;engineering;computer security;remote sensing	Robotics	52.11465434234392	-32.39834204321108	174097
d7f9480695e6c6eed2089435b3814c104d75ba65	map-free localisation in a partially moving 3d world: the edinburgh feature-based navigator	map-free localisation;edinburgh feature-based navigator	From the study of biological acoustic sensorimotor systems it can be seen that Doppler is a rich source of information which is not exploited by commercial ultrasonic range sensors such as the Polaroid. In this work we present a simple collision detection and convoy controller for RoBat, a mobile robot provided with a biomimetic sonarhead, based on the presence or absence of Doppler shifts in the received echos while navigating in real life scenarios. Preliminary results using another robot moving along RoBat’s path show how exploiting the physics of the robot and the environment before going to higher levels of abstraction results in a simple and efficient way for collision detection and smooth convoy navigation.	acoustic cryptanalysis;biomimetics;collision detection;information source;mobile robot;polaroid (polarizer);principle of abstraction;real life;sensor	John Hallam;P. Forster;J. Howe	1989			performance art;cartography	Robotics	50.92533726693538	-33.31678324935219	174147
2c4277544be8de068284ca473ae1704d17d77301	artificial intelligence learning based on proportional navigation guidance	mathematical model aerospace simulation artificial intelligence design for experiment machine learning;mobile robots;learning systems;navigation;navigation autonomous aerial vehicles learning artificial intelligence learning systems mobile robots;reduced miss distance value artificial intelligence learning proportional navigation guidance unmanned aerial vehicle uav maneuvering target ai agent training using machine learning algorithm pn guidance algorithm orthogonal array based training strategy;switches artificial intelligence acceleration oscillators;learning artificial intelligence;autonomous aerial vehicles	Artificial Intelligence (AI) is a vast domain with variety of applications. This research proposes the use of AI for guidance of an Unmanned Aerial Vehicle (UAV) to a maneuvering target. The AI agent is trained using machine learning algorithm. The PN guidance algorithm is used as a basis for training the agent. An orthogonal array based training strategy is proposed that provides a better training and reduced miss distance values. The AI based guidance provides performance equal to the PN Guidance law and in some cases it outperforms PN guidance law.	aerial photography;algorithm;artificial intelligence;best, worst and average case;experiment;machine learning;mike lesser;neural oscillation;portable network graphics;q-learning;random number generation;simulation;state space;state-action-reward-state-action;unmanned aerial vehicle;user agent	Chethan Chithapuram;Yogananda V. Jeppu;Cherukuri Aswani Kumar	2013	2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2013.6637338	mobile robot;computer vision;navigation;simulation;computer science;engineering;artificial intelligence;artificial intelligence, situated approach;hyper-heuristic	AI	50.50611626357808	-28.12301853465171	174339
697d594e84bb177e96041f8c471dd41a589deb5a	bayesian filtering methods for target tracking in mixed indoor/outdoor environments		We propose a stochastic filtering algorithm capable of integrating radio signal strength (RSS) data coming from a wireless sensor network (WSN) and location data coming from the global positioning system (GPS) in order to provide seamless tracking of a target that moves over mixed indoor and outdoor scenarios. We adopt the sequential Monte Carlo (SMC) methodology (also known as particle filtering) as a general framework, but also exploit the conventional Kalman filter in order to reduce the variance of the Monte Carlo estimates and to design an efficient importance sampling scheme when GPS data are available. The superior performance of the proposed technique, when compared to outdoor GPS-only trackers, is demonstrated using experimental data. Synthetic observations are also generated in order to study, by way of simulations, the performance in mixed indoor/outdoor environments.	algorithm;computer simulation;global positioning system;importance sampling;interaction;kalman filter;monte carlo method;particle filter;rss;radar tracker;radio wave;sampling (signal processing);seamless3d;state space;stochastic control;synthetic intelligence	Katrin Achutegui;Javier Rodas;Carlos J. Escudero;Joaquín Míguez	2011		10.1007/978-3-642-29479-2_13	embedded system;simulation;telecommunications	Robotics	40.59010628966984	-26.954421687614428	174647
d5e43dcb3067789ec74922867920967f7a1c01e8	adaptive smart imager for pulse structured light vision	obstacle detection;autonomous mobile robots adaptative smart imager pulse structured light vision optical impulse detection analog processor effective temporal reception window artificial vision problems fast sequenced structured light ambient light obstacle detection intelligent automobiles;structured light;mobile robots;autonomous mobile robot;image sensors;smart sensor;robot vision;natural environment;mobile robots intelligent sensors image sensors object detection robot vision analogue processing circuits;optical pulses image sensors optical sensors optical detectors intelligent sensors charge coupled devices artificial intelligence intelligent robots intelligent vehicles automobiles;artificial vision;analogue processing circuits;image sensor;intelligent sensors;object detection	Many obstacles exist for building a general purpose passive artificial vision system as like humane eye. A practical method is to use some structured light sources in order to simplify vision tasks [2]. Two kinds of structured light can be used: spatial structured light and temporal structured one, of course they can be combined. In the nature1 environment the temporal structured light would be more interesting and it is often in form of pulsed light sequences. This pulsed light source coupled with fast differential imager can produce easy-to-use high contrast images. But the standard CCD imager cannot satisfy this need because of the sequential scanning and the absence of intemal processing ability. Some smart sensors have been investigated [3] [4], but these chips use only spatially structured light. In this paper, we present an experimental adaptative differential image sensor for the temporal structured light sensing, in which each pixel has a photoreceptor and a tiny analog processor. This tiny analog processor adjusts constantly the local sensitivity according to the ambiant lighting level, so it is insensible to the slow ambient light change, but very sensitive to the fast artificial light pulse. The adaptation speed is programmable by changing the adaptation frequency in order to detect differently sequenced temporal light sources and eliminate the ambient fluctuations. The main application is obstacle detection for the intelligent automobile, such as the signalization beacon, detection and identification.	charge-coupled device;image sensor;pixel;structured light;web beacon	F. Lavainne;Yang Ni;Francis J. Devos;P. de Came	1994		10.1109/ISCAS.1994.409294	embedded system;computer vision;simulation;computer science;engineering;image sensor	Vision	46.19937948424426	-33.55135860333075	174674
1a17336064836bb1d9aca8ee6eb7f82953f79128	a constraint-based solver for the military unit path finding problem	optimal solution;articulo;search algorithm;a constraint based solver for the military unit path finding problem;presentation;path finding;mission planning;autonomic system;google earth;military unit path finding problem;spring simulation multiconference 2010;scenario planning;rural urban military operations;constraint satisfaction problem;rural and urban military operations;autonomous systems	We describe the first phase development of a path finding simulation in a military environment. This concept demonstrator can be used for mission planning by constructing what-if scenarios to investigate trade-offs such as location of deployment and mode of transport.  The Military Unit Path Finding Problem (MUPFP) is the problem of finding a path from a starting point to a destination where a military unit has to move, or be moved, safely whilst avoiding threats and obstacles and minimising costs in a digital representation of the real terrain [1]. Although significant research has been done on path finding, the success of a particular technique relies on the environment and existing constraints. The MUPFP is ideal for a constraint-based approach because it requires flexibility in modelling.  We formulate the MUPFP as a constraint satisfaction problem and a constraint-based extension of the A* search algorithm. The concept demonstrator uses a provided map, for example taken from Google Earth, on which various obstacles and threats can be manually marked. Our constraint-based approach to path finding allows for flexibility and ease of modelling. It has the advantage of modelling new environments or additional constraints with ease, and it produces near-optimal solutions if solving is halted prematurely.	a* search algorithm;constraint satisfaction problem;google earth;pathfinding;simulation;software deployment;solver;while	Louise Leenen;Johannes Vorster;Hermanus le Roux	2010		10.1145/1878537.1878564	mathematical optimization;simulation;autonomous system;computer science;pathfinding;engineering;scenario planning;artificial intelligence;constraint satisfaction problem;algorithm;search algorithm	AI	53.650589142133555	-24.564551311911938	174883
3ff45667ee70aa196234080554d91c85f3e5be3f	a method for estimating the 3d rendering performance of the soc in the early design stage	benchmark;estimation;soc;3d rendering performance		3d rendering	Zhen Xie;Yonghui Zhang;Longxing Shi	2014	IEICE Electronic Express	10.1587/elex.11.20140386	system on a chip;estimation;computer architecture;real-time computing;benchmark;computer science;statistics;computer graphics (images)	HCI	43.34989810099338	-32.86452224505185	175072
d20fe9b6df31adba88fb377f2e36b0594a23b648	immune response inspired spatial-temporal target detection algorithms with cnn-um	computing machine;circuit theory;teoria circuito;cnn universal machine;evaluation performance;phenomene spatiotemporel;formation image tridimensionnelle;deteccion blanco;scellular non linear neural networks cnn;performance evaluation;realite virtuelle;theorie circuit;realidad virtual;cellular non linear neural networks cnn;3d imaging;implementation;evaluacion prestacion;analogic algorithms;virtual reality;intelligence artificielle;cellular neural nets;experimental result;detection cible;algorithme;reseau neuronal cellulaire;algorithm;sistema analogico;estudio caso;reponse immune;systeme non lineaire;resultado experimental;etude cas;spatiotemporal phenomena;pattern recognition;machine calcul;artificial intelligence;systeme analogique;inteligencia artificial;reconnaissance forme;respuesta inmune;reconocimiento patron;implementacion;resultat experimental;formacion imagen tridimensional;sistema no lineal;target detection;maquina calculo;artificial immune systems;non linear system;immune response;analog system;algoritmo	Abstract#R##N##R##N#In this paper we show that, similar to the nervous system and the genetic system, the immune system provides a prototype for a ‘computing mechanism.’ We are presenting an immune response inspired algorithmic framework for spatial–temporal target detection applications using CNN technology (IEEE Trans. Circuits Syst. II 1993; 40(3):163–173; Foundations and Applications. Cambridge University Press: Cambridge, 2002). Unlike most analogic CNN algorithms (IEEE Trans. Circuits Syst. 1988; 35(10):1257–1290; Foundations and Applications. Cambridge University Press: Cambridge, 2002) here we will detect various targets by using a plethora of templates. These algorithms can be implemented successfully only by using a computer upon which thousands of elementary, fully parallel spatial–temporal actions can be implemented in real time. In our tests the results show a statistically complete success rate, and we are presenting a special example of recognizing dynamic objects. Results from tests in a 3D virtual world with different terrain textures are also reported to demonstrate that the system can detect unknown patterns and dynamical changes in image sequences. Applications of the system include in explorer systems for terrain surveillance. Copyright © 2006 John Wiley & Sons, Ltd.	algorithm	György Cserey;András Falus;Tamás Roska	2006	I. J. Circuit Theory and Applications	10.1002/cta.341	stereoscopy;immune system;network analysis;computer science;engineering;artificial intelligence;virtual reality;implementation;algorithm	Theory	42.37198907205668	-30.23682646168978	175571
30b3ae3b1da3838bf0ec68d84fbcbe9efd0f2306	grasp planning using low dimensional subspaces		In this chapter we explore the concept of low-dimensional posture subspaces for artificial hands. Recent advances in neuroscience research have shown that control of the human hand during grasping is dominated by movement in a configuration space of highly reduced dimensionality. This has led our group to explore how artificial hands may take advantage of similar subspaces to perform complex grasping tasks. Subspaces are important not only because they are biologically motivated but because they allow us to create computational frameworks that are tractable for difficult problems, characterized by a large number of degrees of freedom, such as dexterous grasping. The work described in this chapter allows us to compute metrics on thousands of grasps quite efficiently, leading to a sampling-based approach that can adequately cover the space of grasps for a number of robotic hands as well as a human hand model. This approach is based on a hand posture subspace defined by a small number of basis vectors which we call eigengrasps. The implied dimensionality reduction has allowed us to perform online dexterous grasp planning both for robots needing to find a correct grasp for an object and for prosthetic devices in which the human provide a subset of the necessary Degrees of Freedom (DOFs), allowing the planner to work in real-time to find a stable grasp. Further, the ability to pre-compute thousands and thousands of stable grasps for dexterous hands over a large class of objects has motivated a new direction in grasp synthesis, which we call data driven grasping. If the number of objects to be grasped in the database is very large and comprehensive then robotic grasping becomes a pre-computed indexed database lookup which is extensible to grasping novel objects not in the database.	basis (linear algebra);cobham's thesis;computation;computational complexity theory;dimensionality reduction;lookup table;poor posture;precomputation;real-time clock;robot;robotic arm;sampling (signal processing)	Peter K. Allen;Matei T. Ciocarlie;Corey Goldfeder	2014		10.1007/978-3-319-03017-3_24	machine learning	Robotics	49.03767905332905	-26.58559914045472	175655
bdaae8331aa3cf62bb6f8a1d9128486c24c47ef5	estimating motion from sparse range data without correspondence	motion analysis;motion estimation navigation mobile robots current measurement equations motion analysis computer vision data mining motion measurement bayesian methods;time varying;range data;mobile robot;piecewise smooth;real time;bayesian methods;robot navigation;motion estimation;mobile robots;prior knowledge;data mining;estimation algorithm;computer vision;robot manipulator;navigation;robot control;current measurement;gradient descent;weighted sums;mobile robot navigation;robust method;terrain mapping;depth estimation;depth map;motion measurement;inertial navigation system	"""Estimating observer motion from time-varying range &ita and fusing this data into a coherent map of the environment are two impor- tant problems in robot navigation. Current methods first detenmine a correspondence between range measurements acquired from different viewpoints, and then compute a motion estimate from this correspon- dence. In this paper, we present an alternative technique which does not assume that any such correspondence exists. Instcad, a smooth surface assumption is used, i.e., the sensed points are imedl to lie on some piecewise smooth surface. A motion estimate is obtained by finding the geometric transformation which makes it most likely (in a Bayesian sense) that the points came from the same surface. We derive an energy equation which measures the distance bctween the new data pints and the dense interpolated depth map wllich is being incrementally rehed. The shape of the energy equation in the neighborhood of the optimal motion estimate is used to com- pute the uncertainty in the estimate. The resulting modon estimation algorithm can be used in conjunction with other motion stirnation systems, and provides a flexible and robust method for computing motion ffom sparse range data. sets of points so that they may be integrated into an updated surface estimate. Our metliad is based on a smooth surface usswnption, i.e., the points which are sensed with the range finder (from two or more viewpoints) are assumed to lie on a piecewise smooth surface:. Our algorithm finds the motion which makes it most likely that these sets of points are from the same piecewise smooth scene. In practice, the logarithm of the likelihood measure itums out to be closely related to the weighted sum of squan:s distance between the new data points and the current surface estimate. This method thus fits well with incremental sensing strategies, where dense depth estimates ~IC obtained by integrating measurements taken from a moving camera (Matthies88). The method presented in this paper shows how to mea- sure the the likelihood of a particular collection of transformed points being properly """"regisxered"""", and how to find a locally optimal motion estimate using gradient descent. This paper does not, however, address tlhe issue of how to search the large space of possible transformations for the """"best"""" motion. Our method is thus meant to b, used in conjunction with some other motion estimation system - such as an inertial navi- gation system - which is used to start the gradient descent algorithm in the vicinity of the solution. Our method will also work if the range of possible motions is small, which can be ensured by sampling the data sufficiently rapidly (as is the case in real-time robot control). The motion estimation algorithm we develop can be ap- plied to both mobile robot navigation and robot manipulation. As part of a mobile robot syslem, the algorithm is used to refine or improve motion estimates obtained from other sources such as inemal navigation, dead retkoning or landmark recognition. The algorithm also builds up and maintains a dense depth map of the environment which can, be used for integration with other sensors. This map can either be a retinotopic (image-based) depth map or a terrain-based elevation map (Olin88). Our al- gorithm is particularly well suited for terrain maps since it can handle data points that are irn:gularly spaced (from perspective de-projection), incaporate prior knowledge from cartographic data, and fuse data with only limited mas of overlap. In robot manipulation, our algorithm can be used to determine object or observer motion from sparse tactile data. The general approach uised in this paper is to incremen- tally build up a dense depth map by interpolating and integrat- ing sparse range data, and to match new points to this surface to perform the motion estimation. We thus start by reviewing"""	sparse	Richard Szeliski	1988		10.1109/CCV.1988.589992	mobile robot;computer vision;simulation;computer science;motion estimation;control theory;mathematics;motion field	Robotics	52.15255444705684	-36.22316485332694	175679
eabd41986acf1ec714bcac58fdaea24902afd3f4	robot motion planning in unknown time varying environments based on a massively parallel processing paradigm			motion planning;parallel computing;programming paradigm;robot	Erwin Prassler	1996				Robotics	53.04154561577295	-28.399527448496567	175946
3cc3928d194e8a2be36bdb5ed2b9bb059314af81	a little more, a lot better: improving path quality by a path-merging algorithm	dynamic programming;graph theory;sampling based motion planning motion control path quality;cluster algorithm;motion control;high dimensionality;dynamic programming algorithm path quality path merging algorithm sampling based motion planning collision free motion paths;degree of freedom;dynamic programming algorithm;robots heuristic algorithms motion planning probabilistic logic length measurement clustering algorithms;path quality;length measurement;artificial intelligent;configuration space;heuristic algorithms;robots;motion planning;graph theory collision avoidance dynamic programming;clustering algorithms;collision free motion paths;collision avoidance;path merging algorithm;quality index;sampling based motion planning;quality measures;probabilistic logic;heuristic algorithm	Sampling-based motion planners are an effective means to generate collision-free motion paths. However, the quality of these motion paths (with respect to quality measures, such as path length, clearance, smoothness, or energy) is often notoriously low, especially in high-dimensional configuration spaces. We introduce a simple algorithm to merge an arbitrary number of input motion paths into a hybrid output path of superior quality, for a broad and general formulation of path quality. Our approach is based on the observation that the quality of certain subpaths within each solution may be higher than the quality of the entire path. A dynamic-programming algorithm, which we recently developed to compare and cluster multiple motion paths, reduces the running time of the merging algorithm significantly. We tested our algorithm in motion-planning problems with up to 12 degrees of freedom (DOFs), where our method is shown to be particularly effective. We show that our algorithm is able to merge a handful of input paths produced by several different motion planners to produce output paths of much higher quality.	algorithm;dynamic programming;gibbs sampling;motion planning;time complexity	Barak Raveh;Angela Enosh;Dan Halperin	2011	IEEE Transactions on Robotics	10.1109/TRO.2010.2098622	mathematical optimization;any-angle path planning;computer science;artificial intelligence;graph theory;theoretical computer science;machine learning;dynamic programming;mathematics	Graphics	52.026818770261485	-24.053716932784567	176158
271422eb10447afa3a6b2b6029a1570bd3ab7f2e	multiscale edges detection algorithm implementation using fpga devices	image edge detection wavelet transforms field programmable gate arrays real time systems smoothing methods;wavelet transforms;smoothing methods;image edge detection;field programmable gate arrays;real time systems	One of the way to extract edges uses the fast wavelet transform algorithm. This technique allows the detection of multiscale edge and is used to detect all the details which are in a picture by modifying the scale. The real time application for edge detection involves the implementation of the algorithm on an integrated circuit like an FPGA and the development of an appropriated board. This article deals about the implementation of a wavelet transform algorithm onto a FPGA and the development of an electronic board to detect multiscale edges.	algorithm;edge detection;fast wavelet transform;field-programmable gate array;integrated circuit	Michel Paindavoine;Sarifuddin;Claude Milan;J. C. Grapin	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)	10.5281/zenodo.36058	computer vision;electronic engineering;computer science;theoretical computer science;cascade algorithm	Robotics	43.923617383615614	-34.432561535587425	176306
7a92b4c90dd38f4ee0e1b1f4159e0065975d490d	hierarchical map building and self-positioning with monalysa	robustness to noise;cognitive map;map building;topological information;self positioning;control architecture;metric information;hierarchical map;structural control;dead reckoning;landmarks	This paper describes how an animat endowed with the MonaLysa control architecture can build a cognitive map that merges into a hierarchical framework not only topological links between landmarks but also higher level structures control information and metric distances and orientations The paper also describes how the animat can use such a map to locate itself even if it is endowed with noisy dead reckoning capacities MonaLysa s mapping and self positioning capacities are illustrated by results obtained in three di erent environments and four noise level conditions These capacities appear to be gracefully degraded when the environment grows more challenging and when the noise level increases In the discussion the current approach is compared to others with similar objectives and directions for future work are outlined	animat;cognitive map;dead reckoning;noise (electronics)	Jean-Yves Donnart;Jean-Arcady Meyer	1996	Adaptive Behaviour	10.1177/105971239600500103	dead reckoning;computer vision;cognitive map;artificial intelligence;communication	Robotics	51.441866072966704	-36.24015665912729	176336
3e368bca15799794e1b91d827ce4befa4ba51e2f	thematic mapping from sensor formations	thematic maps;instruments;fusion;earth;image sensors;sensor formations;sensor fusion decision making geophysical techniques remote sensing;sensor network;fusion sensor networks thematic mapping;intelligent sensors sensor fusion image sensors remote sensing fault tolerance decision making instruments costs earth communication system control;sensor networks;decision making thematic mapping sensor formations remote sensing decision level fusion methods onboard processing;remote sensing;fault tolerance;thematic mapping;decision level fusion methods;sensor fusion;communication system control;onboard processing;intelligent sensors;geophysical techniques	Based on the expectation that remote sensing in the future is likely to be served by large swarms or formations of smaller, simpler platforms and sensors than at present, it is suggested that thematic mapping under that scenario is most likely to be based on decision level fusion methods. Individual platforms are likely to include a degree of on-board processing or intelligence to permit both neighbourhood and global decision making. That will provide redundancy against platform failure and the ability to reconfigure imaging strategies adaptively. Candidate fusion techniques are reviewed.	neighbourhood (graph theory);on-board data handling;sensor	John A. Richards	2007	2007 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2007.4423094	computer vision;wireless sensor network;computer science;artificial intelligence;thematic map;physics;remote sensing	Embedded	52.14405378479826	-33.15413883551523	176363
d095f5cf83af646572386f10f1995bee46160b59	determination of an unmanned mobile object orientation by natural landmarks		The paper is dedicated to determination of equipped with a video camera unmanned mobile object (e.g., a mobile robot) orientation by natural landmarks. The problem is relevant for solving the problem of autonomous movement of the mobile object at a given point of navigating using natural landmarks linked to the map location. The algorithm for determining the orientation of an unmanned mobile object by natural landmarks in view of system conditioning at the point of calculation is proposed. The results of physical experiments of determining the orientation of an unmanned mobile object by natural landmarks in dynamics are presented.	algorithm;autonomous robot;experiment;mobile robot;unmanned aerial vehicle	Anton Korsakov;Ivan Fomin;Dmitry Gromoshinsky;Aleksandr Bakhshiev;Dmitrii Stepanov;Ekaterina Smirnova	2016			computer vision;object-orientation;artificial intelligence;computer science	Robotics	52.109472593135344	-35.775364460082436	176403
95462cd79db6c69c11f3ae0fe62ab23247f0abbd	path efficient level set estimation for mobile sensors		The interest in using robotic sensors for monitoring spatial phenomena is steadily increasing. In the context of environmental analysis, operators typically focus their attention where measurements belong to a region of interest (e.g., when monitoring a body of water we might want to determine where the pH level is above a critical threshold). Most of the previous work in the literature represents the environmental phenomena with a Gaussian Process model, and then uses such a model to determine the best locations for measurements [3, 7]. In this paper we consider a specific scenario where a mobile platform with low computational power can continuously acquire measurements with a negligible cost. In this scenario, we seek to reduce the distance traveled by the mobile platform as it gathers information and to reduce the computation required by this path selection process. Starting from the LSE algorithm [7], we propose two novel approaches, PULSE and PULSE-batch, that exploit a new fast path selection procedure. We evaluate the effectiveness of our approaches on two datasets: a dataset of the pH level of the water, acquired with a mobile watercraft, and a publicly available dataset that represents CO2 maps. Results show that our techniques can compute informative paths with a computation time that is an order of magnitude lower than other techniques.	algorithm;computation;fast path;gaussian process;information;map;mobile operating system;region of interest;robot;robotic sensors;sensor;spatial analysis;time complexity	Lorenzo Bottarelli;Jason Blum;Manuele Bicego;Alessandro Farinelli	2017		10.1145/3019612.3019707	level set;operator (computer programming);fast path;computation;region of interest;environmental analysis;machine learning;gaussian process;exploit;artificial intelligence;computer science	AI	48.15691983072139	-28.955983615524325	176610
53408af91303762ded19473d624df4776b621ad3	research on fast atr algorithms applied to aam at endgame	parameter estimation missiles military equipment target tracking;real time;adaptive control;military equipment;satisfiability;missiles;adaptive control fast atr algorithms air to air missiles target detection automatic target recognition endgame engagement parameters burst point missile models;active appearance model target recognition missiles image segmentation signal processing algorithms target tracking laser radar sensor arrays radar tracking parameter estimation;parameter estimation;target tracking	ATR has been the most important part of the research plan for detecting and recognizing targets in many countries. In this paper, we address fast ATR algorithms for air-to-air missiles (AAMs) flying at endgame. These algorithms can be used to detect, recognize and track a target as well as determine the vulnerable components of an object. It can estimate engagement parameters such as Tgo (time to go) and adaptively control burst-point in various missile and target encounter models. Experimental results have showed that the performance and the real-time of algorithms are both satisfied to general demand of missiles.	active appearance model;algorithm;automatic target recognition	Jiangping Tu;Yingning Peng	2002		10.1109/ICARCV.2002.1234846	control engineering;computer vision;simulation;adaptive control;engineering;control theory;mathematics;estimation theory;statistics;satisfiability	Theory	48.93349589343215	-34.72853400769476	176837
6cbb82293c96dc37e0147857bbf3a8916adf1e41	a deep-learning approach for the detection of overexposure in automotive camera images		To bring autonomous driving onto public roads, autonomous vehicles must be able to make safe driving decisions. In order to achieve this, they need to be self-aware, meaning they have to be aware of their current capabilities at all times. One step towards self-awareness is the assessment of the quality of the available sensor data and the estimation of its impact on the processing chain. Knowing when sensor data is compromised will contribute to safer driving decisions of the vehicle. In this contribution, we present a novel, deep-learning approach for overexposure detection in camera images as one step towards sensor data quality monitoring.		Inga Jatzkowski;Daniel Wilke;Markus Maurer	2018	2018 21st International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2018.8569692		Robotics	50.12742989316398	-35.685722521727406	177067
71543de7692deb2695da5fbd4c199ed228feda8c	directional fuzzy data association filter	fuzzy logic	In this paper, a new multi-target tracking algorithm based on fuzzy logic for tracking in clutter is developed, it is called directional fuzzy data association (DFDA) filter. The new algorithm incorporates the directional information of the targets for data association with the Mahalanobis distance. Firstly, the directional information, called pseudo-direction, is defined; the method of how to calculate the pseudo-direction has been introduced. Then the state incorporating with the pseudo-direction is updated using the cubature Kalman filter (CKF). At last the fuzzy logic inference method is used for data association. Simulation results are used to evaluate the performance of this new algorithm comparing with the nearest neighbor standard filter (NNSF) and joint probability data association filter (JPDAF), the final results show that the proposed DFDA filter an efficient and effective approach for real application.	algorithm;clutter;correspondence problem;fuzzy logic;inference engine;joint probabilistic data association filter;kalman filter;numerical integration;simulation	Pengfei Li;Jingxiong Huang;Lixin Ye;Yi Wang;Zhijun Li;Dongwei Li	2012	JSW		fuzzy logic;joint probabilistic data association filter;computer science;artificial intelligence;machine learning;pattern recognition;data mining	Robotics	48.807394493247145	-33.66570141465327	177309
23ac7de0608b72333cb37d9d9c735ccce4f244a2	two-stage focused inference for resource-constrained minimal collision navigation	navigation;collision avoidance;uncertainty;simultaneous localization and mapping;roads;mobile robots	The operation of mobile robots in unknown environments typically requires building maps during exploration. As the exploration time and environment size increase, the amount of data collected and the number of variables required to represent these maps both grow, which is problematic since all real robots have finite resources. The solution proposed in this paper is to only retain the variables and measurements that are most important to achieve the robot's task. The variable and measurement selection approach is demonstrated on the task of navigation with a low risk of collision. Our approach has two stages: first, a subset of the variables is selected that is most useful for minimizing the uncertainty of navigation (termed the “focused variables”). And second, a task-agnostic method is used to select a subset of the measurements that maximizes the information over these focused variables (“focused inference”). Detailed simulations and hardware experiments show that the two-stage approach constrains the number of variables and measurements. It can generate much sparser maps than existing approaches in the literature, while still achieving a better task performance—in this case (fewer collisions). An incremental and iterative approach is further presented, in which the two-stage procedure is performed on subsets of the data, and thus, avoids the necessity of performing a resource-intensive batch selection on large datasets.	experiment;iterative method;map;mobile robot;simulation	Beipeng Mu;Liam Paull;Ali-akbar Agha-mohammadi;John J. Leonard;Jonathan P. How	2017	IEEE Transactions on Robotics	10.1109/TRO.2016.2623344	computer vision;artificial intelligence;machine learning	Robotics	52.03364684232958	-25.85685291826185	178079
5523d501f294ba188175cd4341d97aa6e846f543	auv pipeline following using reinforcement learning	pipelines learning gaussian distribution cameras robots underwater vehicles algorithm design and analysis;chapter	This paper analyzes the application of several reinforceme nt learning techniques for continuous state and action spac es to pipeline following for an autonomous underwater vehicle (A UV). Continuous space S ARSA is compared to the actor-critic CACLA algorithm [19], and is also extended into a supervised reinf orcement learning architecture. A novel exploration method using the skew-normal stochastic distribution is pr oposed, and evidence towards advantages in the case of tabul a rasa exploration is presented. Results are validated on a re alistic simulator of the AUV, and confirm the applicability o f reinforcement learning to optimize pipeline following beh avior.		Sigurd Aksnes Fjerdingen;Erik Kyrkjebø;Aksel Andreas Transeth	2010			computer vision;simulation;engineering;machine learning	Robotics	50.34514514527018	-28.556031067446362	178135
1dd399f9bf54c09c4e3de4b8b3fc806beff24365	now or later? predicting and maximising success of navigation actions from long-term experience	analytical models;probability mobile robots path planning;h670 robotics and cybernetics;mobile robots;mdp based planning framework navigation actions deliberation planning navigation planning real world robotic systems periodic events opened door closed door planned action success probability superimposed periodic processes long term data real life offices probabilistic planning framework spectral model nonuniform sampling predictive power analysis long term datasets;navigation;spatio temporal representations long term topological map path planning mobile robotics;h671 robotics;predictive models;planning;navigation planning predictive models data models analytical models mobile robots;data models	In planning for deliberation or navigation in real-world robotic systems, one of the big challenges is to cope with change. It lies in the nature of planning that it has to make assumptions about the future state of the world, and the robot's chances of successively accomplishing actions in this future. Hence, a robot's plan can only be as good as its predictions about the world. In this paper, we present a novel approach to specifically represent changes that stem from periodic events in the environment (e.g. a door being opened or closed), which impact on the success probability of planned actions. We show that our approach to model the probability of action success as a set of superimposed periodic processes allows the robot to predict action outcomes in a long-term data obtained in two real-life offices better than a static model. We furthermore discuss and showcase how this knowledge gathered can be successfully employed in a probabilistic planning framework to devise better navigation plans. The key contributions of this paper are (i) the formation of the spectral model of action outcomes from non-uniform sampling, the (ii) analysis of its predictive power using two long-term datasets, and (iii) the application of the predicted outcomes in an MDP-based planning framework.	cost efficiency;expectation–maximization algorithm;mobile robot;newton–cotes formulas;nonuniform sampling;quasiperiodicity;real life;sampling (signal processing);scheduling (computing);spatial variability;timeshift;topological graph	Jaime Pulido Fentanes;Bruno Lacerda;Tomás Krajník;Nick Hawes;Marc Hanheide	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139315	planning;mobile robot;data modeling;computer vision;navigation;simulation;computer science;engineering;artificial intelligence;predictive modelling;mobile robot navigation	Robotics	50.84041338353811	-25.522298240829592	178164
91c73fa38a5d7414cba3eebc040d795d539f1829	gpu optimization of convolution for large 3-d real images	image processing;convolution;gpu;3 d	In this paper, we propose a method for computing convolution of large 3-D images with respect to real signals. The convolution is performed in a frequency domain using a convolution theorem. Due to properties of real signals, the algorithm can be optimized so that both time and the memory consumption are halved when compared to complex signals of the same size. Convolution is decomposed in a frequency domain using the decimation in frequency (DIF) algorithm. The algorithm is accelerated on a graphics hardware by means of the CUDA parallel computing model, achieving up to 10× speedup with a single GPU over an optimized implementation on a quad-core CPU.	convolution;graphics processing unit	Pavel Karas;David Svoboda;Pavel Zemcík	2012		10.1007/978-3-642-33140-4_6	overlap–add method;computer vision;parallel computing;image processing;computer science;theoretical computer science;convolution;kernel;computer graphics (images)	Vision	41.39709121480939	-32.550922179340866	178614
dc475fd6c7c3b12be64a768b0bfb0566d4a39cb8	entropy-based gaze planning	bayes approach;entropy map;camera motion;gaze planning;optical flow;large classes	This paper describes an algorithm for recognizing known objects in an unstructured environment (e.g. landmarks) from measurements acquired with a single monochrome television camera mounted on a mobile observer. The approach is based on the concept of an entropy map, which is used to guide the mobile observer along an optimal trajectory that minimizes the ambiguity of recognition as well as the amount of data that must be gathered. Recognition itself is based on the optical flow signatures that result from the camera motion signatures that are inherently ambiguous due to the confounding of motion, structure and imaging parameters. We show how gaze planning partially alleviates this problem by generating trajectories that maximize discriminability. A sequential Bayes approach is used to handle the remaining ambiguity by accumulating evidence for different object hypotheses over time until a clear assertion can be made. Results from an experimental recognition system using a gantry-mounted television camera are presented to show the effectiveness of the algorithm on a large class of common objects.	algorithm;antivirus software;assertion (software development);monochrome;optical flow;type signature	Tal Arbel;Frank P. Ferrie	2001	Image Vision Comput.	10.1016/S0262-8856(00)00103-7	computer vision;simulation;computer science;artificial intelligence;machine learning;optical flow;mathematics	Vision	49.197877687753135	-37.02607052668068	178645
2ab2661e1c877286607c70d8e2224b6ea6241e62	hardware implementation of a full hd real-time disparity estimation algorithm	image resolution;estimation hardware image resolution real time systems throughput algorithm design and analysis software algorithms;estimation;software algorithms;algorithm design and analysis;l hrm fpga stereo matching disparity estimation;throughput;hardware;real time systems	Disparity estimation is a common task in stereo vision and usually requires a high computational effort. High resolution disparity maps are necessary to provide a good image quality on autostereoscopic displays which deliver stereo content without the need for 3D glasses. In this paper, an FPGA architecture for a disparity estimation algorithm is proposed, that is capable of processing high-definition content in real-time. The resulting architecture is efficient in terms of power consumption and can be easily scaled to support higher resolutions.	application-specific integrated circuit;autostereoscopy;binocular disparity;computer data storage;field-programmable gate array;genetic algorithm;hdmi;image quality;map;real-time clock;requirement;stereopsis;stereoscopy	Martin Werner;Benno Stabernack;Christian Riechert	2014	IEEE Transactions on Consumer Electronics	10.1109/TCE.2014.6780927	algorithm design;computer vision;estimation;throughput;real-time computing;image resolution;computer science;statistics;computer graphics (images)	Visualization	43.53543907905335	-34.19843668173188	178899
83626fd0925616438245502ac7275a75e604a98a	real time implementation of spatial filtering on fpga		Field Programmable Gate Array (FPGA) technology has gained vital importance mainly because of its parallel processing hardware which makes it ideal for image and video processing. In this paper, a step by step approach to apply a linear spatial filter on real time video frame sent by Omnivision OV7670 camera using Zynq Evaluation and Development board based on Xilinx XC7Z020 has been discussed. Face detection application was chosen to explain above procedure. This procedure is applicable to most of the complex image processing algorithms which needs to be implemented using FPGA.	algorithm;face detection;field-programmable gate array;image processing;parallel computing;video processing	Chaitannya Supe	2015	CoRR		embedded system;real-time computing;computer hardware;computer science	Robotics	44.09650043833615	-35.835941710868276	179332
e2a4e2040703cb3d6dd53e4f3fe4db2a48d7328f	sensor architecture and data fusion for robotic perception in urban environments at the 2007 darpa urban challenge	sensor system;urban environment;laser scanner;data fusion;dempster shafer;extended kalman filter	We will demonstrate the sensor and data fusion concept of the 2007 DARPA Urban Challenge vehicle assembled by Team CarOLO, Technische Universitat Braunschweig. The perception system is based on a hybrid fusion concept, combining object and grid based approaches in order to comply with the requirements of an urban environment. A variety of sensor systems and technologies is applied, providing a 360 degree view area around the vehicle. Within the object based subsystem, obstacles (static and dynamic) are tracked using an Extended Kalman Filter capable of tracking arbitrary contour shapes. Additionally, the grid based subsystem extracts drivability information about the vehicle's driveway by combining the readings of laser scanners, a mono and a stereo camera system using a Dempster-Shafer based data fusion approach.	darpa grand challenge (2007)	Jan Effertz	2008		10.1007/978-3-540-78157-8_21	computer vision;simulation;engineering;artificial intelligence;sensor fusion	Robotics	53.018646043536734	-36.56249917830999	179352
7e698cb335ec6fabba78d1f4a8f25e8ce0f919c8	a novel multithreaded rendering system based on a deferred approach	deferred rendering;multi threading;deferred approach;dual core machine;central processing unit engines rendering computer graphics graphics hardware pipeline processing artificial intelligence parallel processing physics;physics;directx 11;rendering system;engines;multithreaded rendering;thread pool;command buffer;artificial intelligence;dual core machine multithreaded rendering system deferred approach;rendering computer graphics;thread pool multithreaded rendering deferred rendering directx 11 command buffer;parallel processing;multithreaded rendering system;graphics;rendering computer graphics multi threading;central processing unit;pipeline processing;hardware	This paper presents the architecture of a rendering system designed for multithreaded rendering. The implementation of the architecture following a deferred rendering approach shows gains of 65% on a dual core machine.	application programming interface;deferred shading;graphics;multi-core processor;multithreading (computer architecture);overhead (computing);rendering (computer graphics);scalability;speedup;thread (computing)	Jorge Alejandro Lorenzon;Esteban Walter Gonzalez Clua	2009	2009 VIII Brazilian Symposium on Games and Digital Entertainment	10.1109/SBGAMES.2009.27	tiled rendering;parallel computing;computer hardware;computer science;parallel rendering;alternate frame rendering;computer graphics (images)	Arch	43.43117631228544	-31.81374274853001	179473
e438cb4d72a7cd6570e6a2f446f6c3c404262419	animated particle rendering in dsp and fpga	field programmable gate array;oriented space;scalar product;vector product;3d point cloud;computer graphic;chip;rendering system;n dimensional spaces;digital signal processor;point cloud	This paper presents an algorithm for rendering and animation of 3D point-clouds using FPGA (Field Programmable Gate Array) chip coupled with a DSP (Digital Signal Processor). The point-clouds are sets of graphical data (particles) in 3D space suitable for and perspective for potentially many purposes in modern computer graphics. The paper describes the architecture of the proposed rendering system and a novel approach to rendering and simple animation of large of 3D point-clouds (large sets of particles).	algorithm;computer graphics;digital signal processing;digital signal processor;field-programmable gate array;graphical user interface;particle system;rendering (computer graphics)	Adam Herout;Pavel Zemcík	2004		10.1145/1037210.1037246	chip;embedded system;cross product;tiled rendering;digital signal processor;dot product;computer hardware;rendering;computer science;operating system;parallel rendering;point cloud;real-time rendering;texture memory;alternate frame rendering;field-programmable gate array;software rendering;computer graphics (images)	Graphics	43.320435206270815	-33.306082024744676	179491
313423abebfc6f6809dbb06faac5e68e44875fdc	video segmentation for transporting pre-stored video across networks				Huirong Fu;Liren Zhang	2003			video tracking;distributed computing;segmentation;computer science	Vision	45.2433380488376	-37.295017251741	179653
a6cd50d2dad3a885b0cef8c05318a48d5379adf9	multimodal learning for autonomous underwater vehicles from visual and bathymetric data	robot vision autonomous underwater vehicles feature extraction image classification image sampling image texture learning artificial intelligence marine engineering mobile robots oceanographic techniques;sampling task multimodal learning autonomous underwater vehicles auv visual data utilization bathymetric data visual imagery marine ecosystem monitoring marine ecology benthic habitat mapping ocean floor region classification shipborne acoustic multibeam data textural cues visual feature utilization shipborne multibeam bathymetry utilization;visualization training encoding correlation vectors image reconstruction oceans	Autonomous Underwater Vehicles (AUVs) gather large volumes of visual imagery, which can help monitor marine ecosystems and plan future surveys. One key task in marine ecology is benthic habitat mapping, the classification of large regions of the ocean floor into broad habitat categories. Since visual data only covers a small fraction of the ocean floor, traditional habitat mapping is performed using shipborne acoustic multi-beam data, with visual data as ground truth. However, given the high resolution and rich textural cues in visual data, an ideal approach should explicitly utilise visual features in the classification process. To this end, we propose a multimodal model which utilises visual data and shipborne multi-beam bathymetry to perform both classification and sampling tasks. Our algorithm learns the relationship between both modalities, but is also effective when visual data is missing. Our results suggest that by performing multimodal learning, classification performance is improved in scenarios where visual data is unavailable, such as the habitat mapping scenario. We also demonstrate empirically that the model is able to perform generative tasks, producing plausible samples from the underlying data-generating distribution.	acoustic cryptanalysis;algorithm;autonomous robot;backscatter (email);bathymetry;ecology;ground truth;habitat;image resolution;marine ecosystem;multimodal interaction;multimodal learning;sampling (signal processing)	Dushyant Rao;Mark De Deuge;Navid Nourani-Vatani;Bertrand Douillard;Stefan B. Williams;Oscar Pizarro	2014	2014 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2014.6907413	computer vision;marine engineering;remote sensing	Robotics	48.5562874524156	-34.891852314409505	179709
c41d663ae110f9c91f4c0160420682df60170dbd	pipelined virtual camera configuration for real-time image processing based on fpga	dedicated hardware architecture;fpga image processing virtual camera;image processing;cameras image processing field programmable gate arrays hardware layout real time systems computer architecture reconfigurable architectures synchronization clocks;reconfigurable architectures;fpga;hardware architecture;reconfigurable architecture;pixel clock pipelined virtual camera configuration real time image processing fpga dedicated hardware architecture reconfigurable architecture;field programmable gate arrays;real time image processing;pipelined virtual camera configuration;quick response;pixel clock;reconfigurable architectures field programmable gate arrays image processing;virtual camera	Real-time image processing is important for many application areas which require a quick response from events in a scene. Since real-time image processing involves large amount of computations, many approaches have been proposed to solve this problem especially using a dedicated hardware system. However, they are not sufficiently adapted to practical use because their dedicated hardware architecture is not suitable to carry out multiple tasks even in the case of a reconfigurable architecture. This paper proposes a pipelined virtual camera configuration which can perform several image processing tasks, especially at the low and intermediate levels, through the reconfiguration of the system. By synchronizing the entire system with the pixel clock, each processing modules can regard the others as a virtual camera. As a result, both the performance and the degree of reconfiguration are significantly increased.	application-specific integrated circuit;computation;dataflow;field-programmable gate array;image processing;pixel;real-time clock;real-time locating system;real-time transcription;robot;video graphics array;virtual camera system	Seunghun Jin;Jung Uk Cho;Jae Wook Jeon	2007	2007 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2007.4522157	embedded system;real-time computing;computer hardware;image processing;computer science;digital image processing;hardware architecture;field-programmable gate array	Robotics	44.32825506119605	-35.111521055864706	179726
741e228df4c5dca5d0a1230ea4752bf1a2b12896	efficient incorporation of optical flow into visual motion estimation in tracking.	moving object;video surveillance;video processing;visual motion;computer vision;video coding;law enforcement;video object plane;optical flow;digital video	 Recent developments in digital technology have increased acquisition of digital video data, which in turn have led to more applications in video processing. Video sequences provide additional information about how scenes and objects change over time when compared to still images. The problem of tracking moving objects remains of great research interest in computer vision on account of various applications in video surveillance, monitoring, robotics, and video coding. For instance, MPEG-4 video standard introduced video object plane concept, and a decomposition of sequences into object planes with different motion parameters [1]. Video surveillance systems are needed in traffic and highway monitoring, in law enforcement and security applications by banks, stores, and parking lots. Algorithms for extracting and tracking over time moving objects in a video sequence are hence of importance.		Gözde B. Ünal;Anthony J. Yezzi;Hamid Krim	2005		10.1007/11504634_5	video compression picture types;computer vision;video;uncompressed video;quarter-pixel motion;motion interpolation;video capture;video tracking;motion estimation;block-matching algorithm;multimedia;video processing;motion compensation;video post-processing;multiview video coding;computer graphics (images)	Vision	44.88801074705074	-37.618443933568976	179753
9ee6b8685ac7b0d36b69ffa9675b4aecacf8c7bb	performance and energy characterization of high-performance low-cost cornerness detection on gpus and multicores	desktop system configuration high performance low cost cornerness detection gpu graphics processing unit feature detection feature tracking computer vision image corners multicore architectures embedded systems energy efficiency harris corner detection algorithm harris approximation mobile system configuration;graphics processing units approximation algorithms mobile communication real time systems detectors approximation methods feature extraction;mobile computing harris corner detection cuda;object tracking approximation theory computer vision graphics processing units multiprocessing systems object detection	Feature detection and tracking is an important problem in Computer Vision. Corners in an image are a good indication of features to track. Original algorithms may be expensive even on multicore architectures because they require full convolutions to be performed. Although these can be performed in real time in modern GPUs and multicore CPUs, faster solutions are needed for embedded systems and complex algorithms, given that corner detections is just a step of the analysis process. In this paper we evaluate the performance and energy efficiency of the Harris corner detection algorithm as well as an approximation of it, in both desktop and mobile platforms. The purpose of this paper is three-fold: evaluate the performance gains of GPUs vs. CPUs for several mobile and desktop systems, evaluate whether the Harris approximation provides adequate performance gains to justify its use in mobile and desktop system configurations and, finally, determine which configurations provide real-time performance. According to our evaluation (a) the best GPU solution is 16.3 times faster than the best CPU solution for the desktop case while being 2.6 times more energy efficient and (b) the best GPU solution for the mobile case is 1.2 times faster while being 3.6 times more energy efficient than the respective CPU.	algorithm;approximation;central processing unit;computer case;computer vision;convolution;corner detection;desktop computer;embedded system;emoticon;feature detection (computer vision);graphics processing unit;harris affine region detector;mobile device;multi-core processor;real-time clock;sensor	Apostolos Glenis;Sergios Petridis	2014	IISA 2014, The 5th International Conference on Information, Intelligence, Systems and Applications	10.1109/IISA.2014.6878727	feature detection;parallel computing;computer science;theoretical computer science;general-purpose computing on graphics processing units;computer graphics (images)	EDA	42.908742445798495	-36.30618449989513	179944
e57bb094dd4d8cfed99d8c1e980ce4b18f317be4	a multi-strategy path planner based on space accessibility		The problem of planning a path in the real complex environment remains a difficult challenge although path planning has been studied extensively in the context of autonomous indoor mobile robots. One of the main reasons is lacking semantic information of the robot's working space. In this paper, we introduce the notion of space accessibility into path planning research. Semantic mapping is utilized to identify and recognize room and hallway on an occupancy grid map. A region topological map is built based on physical connection relation between regions and the types of regions. A multi-strategy path planning combining grid and region topological maps is developed. Paths in hallways will be selected preferentially and different planning methods are used for creating paths in different kinds of regions. Experimental evaluation is conducted on dozens of test maps with indoor layouts. Compared with paths produced by other techniques, the paths generated by our method are more like human beings decisions.	accessibility;autonomous robot;experience;map;mobile robot;motion planning;semantic mapper;workspace	Meng Hang;Mengxiang Lin;Shangzhe Li;Zhixin Chen;Rong Ding	2017	2017 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2017.8324738	occupancy grid mapping;control engineering;grid;motion planning;semantics;semantic mapping;engineering;mobile robot;hidden markov model;distributed computing;topological map	Robotics	51.81515668938463	-35.786934396167936	180500
1181644016e3a546cc500f5381ed4084c41675d7	using the distribution theory to simultaneously calibrate the sensors of a mobile robot		This paper introduces a simple and very efficient strategy to extrinsically calibrate a bearing sensor (e.g. a camera) mounted on a mobile robot and simultaneously estimate the parameters describing the systematic error of the robot odometry system. The paper provides two contributions. The first one is the analytical computation to derive the part of the system which is observable when the robot accomplishes circular trejectories. This computation consists in performing a local decomposition of the system, based on the theory of distributions. In this respect, this paper represents the first application of the distribution theory in the frame-work of mobile robotics. Then, starting from this decomposition, a method to efficiently estimate the parameters describing both the extrinsic bearing sensor calibration and the odometry calibration is derived (second contribution). Simulations and experiments with the robot e-Puck equipped with encoder sensors and a camera validate the approach.	computation;computer simulation;distribution (mathematics);encoder;experiment;mobile robot;observable;odometry;robotics;sensor	Agostino Martinelli	2009		10.15607/RSS.2009.V.011	computer vision;simulation;odometry;mathematics	Robotics	52.24406458391075	-35.022608219659745	180953
33aff239a5b709919fcf0fb933b3c2d72ad21c0d	dempster-shafer theory based robust sequential detection in distributed sensor networks		We propose a distributed sequential detector based on the Dempster-Shafer Theory of Evidence. First, we introduce a novel rule for the basic probability assignment. This rule is based on the distribution of the likelihood ratio and is shown to yield better results than existing ones while at the same time avoiding counter-intuitive and contradictory probability assignments. Second, we use the Dempster-Shafer combination rule to design a distributed sequential detection algorithm. Third, we show how to robustify the algorithm against outliers by leveraging neighborhood communication.	algorithm;robustification;sensor	Mark R. Leonard;Christian A. Schroth;Abdelhak M. Zoubir	2018	2018 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2018.8450760	wireless sensor network;robustness (computer science);dempster–shafer theory;outlier;detector;artificial intelligence;pattern recognition;mathematics	ML	39.63910670445535	-28.29962243389621	181125
359b5df379adceff4c4a509725106ba8e2a773de	autonomous uav control: balancing target tracking and persistent surveillance		Persistent surveillance and target tracking are often accomplished through the use of unmanned aerial vehicles (UAVs). In many scenarios both tasks are concurrently required, but are disparate in their area investigation techniques; surveillance requires global exploration, whereas tracking takes a local target-centric view. Thus, the two tend to require different actions when controlling UAVs tasked with their respective objectives. We propose formulating the UAV control problem for concurrent surveillance and target tracking as a partially observable Markov decision process (POMDP) applying a Q-value approximation technique called nominal belief-state optimization (NBO). Using the Probability Hypothesis Density (PHD) filter as the tracking algorithm, we are able to exploit the birth-intensity components combined with non-myopic action selection inspiring the desired accompanying surveillance behaviors.	acclimatization;action selection;aerial photography;algorithm;approximation;behavior;chronic multifocal osteomyelitis;correspondence problem;drug vehicle;endianness;equilibrium;gentamicins;markov chain;mathematical optimization;mathematics;myopia;parsing;partially observable markov decision process;partially observable system;simulation;unmanned aerial vehicle	Lucas W. Krakow;Edwin K. P. Chong	2017	2017 IEEE Conference on Control Technology and Applications (CCTA)	10.1109/CCTA.2017.8062673	process control;real-time computing;partially observable markov decision process;tracking system;action selection;control engineering;exploit;engineering	Robotics	51.960322913044784	-26.34230541554487	181139
911d4666e1aa4ec7ed52daad377aee9fa9e8a090	a novel time decaying approach to obstacle avoidance	map building;mobile robot;cognitive;reactive control;obstacle avoidance;dynamic window;local minima	One of the basic issues in navigation of mobile robots is the obstacle avoidance task which is commonly achieved using reactive control paradigm where a local mapping from perceived states to actions is acquired. The algorithms of this class suffer from a major drawback of exhibiting cyclic behavior when encountered with certain obstacle configurations. This paper presents a cognitive time decaying approach to overcome this cyclic behavior .The Dynamic Window algorithm is taken as an example for implementing this approach. To build a dynamic window based obstacle avoider, we use time decaying heuristic function for history mapping - which innately eliminates local minima even for a cluttered environment and gives the robot an exploratory nature best suited for map building purposes. The algorithm is successfully tested on a simulation, where it is shown to avoid the U bend problem of local minima.	obstacle avoidance	Sankalp Arora;S. Indu	2009		10.1007/978-3-642-11164-8_88	mobile robot;simulation;cognition;computer science;artificial intelligence;maxima and minima;obstacle avoidance	Robotics	48.88412610127481	-29.906412577105513	181146
2fb5469247e709e1ca3837a8267f1c353a30b22d	bounded uncertainty roadmaps for path planning	environment maps;path planning;degree of freedom;robotics;configuration space;planning under uncertainty;motion planning	Motion planning under uncertainty is an important problem in robotics. Although probabilistic sampling is highly successful for motion planning of robots with many degrees of freedom, sampling-based algorithms typically ignore uncertainty during planning. We introduce the notion of a bounded uncertainty roadmap (BURM) and use it to extend samplingbased algorithms for planning under uncertainty in environment maps. The key idea of our approach is to evaluate uncertainty, represented by collision probability bounds, at multiple resolutions in different regions of the configuration space, depending on their relevance for finding a best path. Preliminary experimental results show that our approach is highly effective: our BURM algorithm is at least 40 times faster than an algorithm that tries to evaluate collision probabilities exactly, and it is not much slower than classic probabilistic roadmap planning algorithms, which ignore uncertainty in environment maps.	algorithm;automated planning and scheduling;loss function;map;motion planning;probabilistic roadmap;relevance;robot;robotics;sampling (signal processing);time complexity	Leonidas J. Guibas;David Hsu;Hanna Kurniawati;Ehsan Rehman	2008		10.1007/978-3-642-00312-7_13	computer vision;mathematical optimization;simulation;any-angle path planning;mathematics	Robotics	51.7653604358218	-24.36281519786277	181159
19d7f4f0534b07e589b09f6416f637bf06ea9ea2	maximum likelihood rover localization by matching range maps	mars;range map matching;probability mobile robots robot vision path planning navigation image matching maximum likelihood estimation stereo image processing;probability;mars rovers;roving vehicles;maximum likelihood;uncertainty;image matching;path planning;maximum likelihood estimates;prototypes;terrain;mars prototypes uncertainty robot kinematics propulsion laboratories postal services maximum likelihood estimation image matching mobile robots;mobile robots;maximum likelihood estimation;position location;navigation;postal services;robot vision;navigation maximum likelihood estimation range map matching stereo vision probability image matching mars rovers mobile robots;stereo image processing;stereo vision;estimating;propulsion;rovers range maps;robot kinematics	This paper describes marimunl likelihood estirncr-tion techniques for performirlg rover localizatio?t in natural terrain by matching range ntaps. An OCCUpancy map of the local terrain isgertef'ated usirigstcreo visiou. The positioa of the rover rvith respect to a previously generated occupancy map M then computed by comparing the maps using a probabdistic formulation of [[ausdoz-ff matching iechrtiq~lcs. Our?rtoti?latio~lfor this work is the desire for greater autononly in Mars rozers. These techniques have been applied to data obtain edjrom the Sojouz'aer h!ar's rover and ran on-boardlhe Rocky 7 Mars rover prototype.	map;prototype;rover (the prisoner)	Clark F. Olson;Larry H. Matthies	1998		10.1109/ROBOT.1998.676398	computer vision;simulation;computer science;maximum likelihood;statistics;remote sensing	Robotics	52.746245451040025	-35.70142823488745	181305
aa43eeb7511a5d349af0cc27b1a594b4e9f2927c	fusion of laser and monocular camera data in object grid maps for vehicle environment perception	laser ranging cameras driver information systems image classification image fusion inference mechanisms;vehicles sensors cameras measurement by laser beam uncertainty roads transforms;dempster shafer theory of evidence monocular camera data fusion laser camera data fusion object grid maps vehicle environment perception occupancy grid maps reliable vehicle environmental model data processing range finding sensors autonomous driving process classification ground plane	Occupancy grid maps provide a reliable vehicle environmental model and usually process data from range finding sensors. Object grid maps additionally contain information about the classes of objects, which is crucial for applications like autonomous driving. Unfortunately, they lack the precision of occupancy grid maps, since they mostly process classification results from camera data by projecting the corresponding images onto the ground plane. This paper proposes a modular framework to create precise object grid maps. The presented algorithm creates classical occupancy grid maps and object grid maps. In a combination step, it transforms both maps into the same frame of discernment based on the Dempster-Shafer theory of evidence. This allows fusing the maps to one object grid map, which contains valuable object information and at the same time benefits from the precision of the occupancy grid map.	algorithm;autonomous car;autonomous robot;map;pitch (music);radar;reflection (computer graphics);sensor;state space;stationary process;stereo camera;stereo cameras	Dominik Nuss;Markus Thom;Andreas Danzer;Klaus C. J. Dietmayer	2014	17th International Conference on Information Fusion (FUSION)		computer vision;simulation;geography;remote sensing	Robotics	51.21793909623008	-35.88435523907284	181379
0d09abd4a319cb1f7c59165d1d06f2d9762a4997	sampling-based incremental information gathering with applications to robotic exploration and environmental monitoring		In this article, we propose a sampling-based motion planning algorithm equipped with an informationtheoretic convergence criterion for incremental informative motion planning. The proposed approach allows for a dense map representation and incorporates the full state uncertainty into the planning process. The problem is formulated as a maximization problem with a budget constraint. Our approach is built on rapidly-exploring information gathering algorithms and benefits from advantages of samplingbased optimal motion planning algorithms. We propose two information functions and their variants for fast and online computations. We prove an information-theoretic convergence for the entire exploration and information gathering mission based on the least upper bound of the average map entropy. The convergence criterion gives rise to a natural automatic stopping criterion for information-driven motion control. We demonstrate the performance of the proposed algorithms using three scenarios: comparison of the proposed information functions and sensor configuration selection, robotic exploration in unknown environments, and a wireless signal strength monitoring task in a lake from a publicly available dataset collected using an autonomous surface vehicle. ∗Working paper. mani.ghaffari@gmail.com – http://maanighaffari.com †Centre for Autonomous Systems (CAS), University of Technology Sydney. ar X iv :1 60 7. 01 88 3v 2 [ cs .R O ] 2 7 Ju l 2 01 6	algorithm;automated planning and scheduling;autonomous robot;autonomous system (internet);computation;eisenstein's criterion;entropy maximization;information theory;motion planning;robotic spacecraft;sampling (signal processing)	Maani Ghaffari Jadidi;Jaime Valls Miró;Gamini Dissanayake	2016	CoRR		mathematical optimization;simulation;computer science;artificial intelligence;machine learning	Robotics	51.375452072130365	-25.87424260032537	181792
b7206b07bdf368e3e30ba93489aaaa0dbdc140b5	robust fusion for multisensor multiobject tracking		"""This letter proposes analytical expressions for the fusion of certain classes of labeled multiobject densities via Kullback–Leibler averaging. Specifically, we provide analytical fusion rules for the labeled multi-Bernoulli and marginalized <inline-formula><tex-math notation=""""LaTeX"""">$\delta$</tex-math></inline-formula>-generalized labeled multi-Bernoulli families of labeled multiobject densities. Information fusion via Kullback–Leibler averaging ensures immunity to double counting of information and is essential to the development of effective multiagent multiobject estimation."""	agent-based model;bernoulli polynomials;kullback–leibler divergence	Claudio Fantacci;Ba-Ngu Vo;Ba-Tuong Vo;Giorgio Battistelli;Luigi Chisci	2018	IEEE Signal Processing Letters	10.1109/LSP.2018.2811750	pattern recognition;fusion;artificial intelligence;robustness (computer science);mathematics;fusion rules;expression (mathematics)	DB	39.64968213440983	-28.234984284774637	182080
709776c61348f1a95a8357f97daccffba5eae71b	optimization methodology to fruit grove mapping in precision agriculture	data filtering;info eu repo semantics article;optimization;mapping;agricultural environments;info ar repo semantics articulo;info eu repo semantics publishedversion;agricultural enviroments	A method capable of efficiently mapping a semi-structured environment is presented.Grove mapping based on LiDAR and the GPS locations of the corner trees is given.An optimization tool that adjusts measurements acquired by a mobile robot is used.The technique was tested in an olive grove located in San Juan - Argentina.It is incorporated a novel filtering technique of unlikely data. The mapping of partially structured agricultural environments is a valuable resource for precision agriculture. In this paper, a technique for the mapping of a fruit grove by a mobile robot is proposed, which uses only front laser information of the environment and the exact position of the grove corners. This method is based on solving an optimization problem with nonlinear constraints, which reduces errors inherent to the measurement process, ensuring an efficient and precise map construction. The resulting algorithm was tested in a real orchard environment. For this, it is also developed a data filtering method capable to comply efficiently the observation-feature matching. The maximum average error obtained by the methodology in simulations was about 13cm, and in real experimentation was about 36cm.		Javier Gimenez;Daniel Herrera;Santiago Tosetti;Ricardo O. Carelli	2015	Computers and Electronics in Agriculture	10.1016/j.compag.2015.06.013	simulation;engineering;artificial intelligence;data mining	Robotics	53.66357745898842	-33.22358459344559	182165
da3b6506b6d0eca05785e2ab2561bcc8e476af26	embedding slam algorithms: has it come of age?	slam algorithms;heterogeneous architectures;gpu;cpu;fpga;hardware–software codesign	Development of Simultaneous Localization and Mapping (SLAM) systems in the era of autonomous navigation and the growing demand for autonomous robots have put into question how to reduce the computational complexity and make use of SLAM algorithms to operate in real time. Our work, aims to take advantage of low-power embedded architectures to implement SLAM algorithms. Precisely, we evaluate the promise held by the new modern low power architectures in accelerating the execution time of SLAM algorithms. Throughout this, we map and implement 4 well-known SLAM algorithms that find utility in very different robot applications and autonomous navigation, on different architectures based embedded systems. We present first a processing time evaluation of these algorithms on different CPU based architectures. Results demonstrate that FastSLAM2.0 allows a compromise between the computation time and the consistency of the localization results. The algorithm has been modified to be adapted to large environments. It is then optimized for parallel implementations on GPU and FPGA. A comparative study of the resulting implementations is given. Our results show that an embedded FPGA based SoC architecture is an interesting alternative for a SLAM algorithm implementation using the hardware–software co-design approach. Hence, the system meets performance requirements of a robot to operate in real-time constraints.	algorithm;simultaneous localization and mapping	Mohamed Abouzahir;Abdelhafid Elouardi;Rachid Latif;Samir Bouaziz;Abdelouahed Tajer	2018	Robotics and Autonomous Systems	10.1016/j.robot.2017.10.019	field-programmable gate array;computer vision;artificial intelligence;computer science;theoretical computer science;algorithm;embedding	Robotics	43.795571794615995	-36.93299931738057	182619
08bd6db69b3fcc67b0d4ced7e6a19dba1a3b24a5	synthesis of images of historical documents for web visualization	image coding;image segmentation;history;data compression;image synthesis storage transmission network transmission historical documents image segmentation entropy based segmentation synthetic image image compression web visualization;data visualisation;internet;image coding image segmentation history data visualisation data compression document image processing internet;document image processing;visualization image segmentation ink optical character recognition software image converters network synthesis image storage internet data mining feature extraction;web visualization	This paper describes a system for efficient storage and network transmission of images of historical documents. The images are segmented into two classes (paper and ink) with the use of an entropy-based segmentation algorithm. The information is then reassembled, synthetising an image visually close to the original document. The synthesis process works in both classes, using information acquired in the segmentation phase. All the information needed to rebuild the final synthetic image occupies, in average, 2 Kbytes performing a very efficient compression scheme.	algorithm;binary data;historical document;jpeg;kilobyte;mega man network transmission;plug-in (computing);synthetic data;synthetic intelligence;visual inspection	Carlos A. B. Mello	2004	10th International Multimedia Modelling Conference, 2004. Proceedings.	10.1109/MULMM.2004.1264989	data compression;image texture;computer vision;feature detection;the internet;binary image;image processing;image retrieval;computer science;segmentation-based object categorization;digital image processing;multimedia;image segmentation;scale-space segmentation;automatic image annotation;information retrieval;digital image;data visualization;statistics	Vision	39.485425866722096	-35.423878367166004	182710
529b46a44698ab59963f4f964e2775157e6d95bc	robot navigation in hand-drawn sketched maps	robot sensing systems;navigation robot sensing systems measurement trajectory monte carlo methods;measurement;navigation;trajectory;path planner robot navigation hand drawn sketched maps navigation systems mobile robots metrical map localization planning trajectory planning two dimensional manifold monte carlo localization pose tracker;trajectory control mobile robots monte carlo methods pose estimation robot vision;monte carlo methods	Many navigation systems for mobile robots use a metrical map for localization and trajectory planning. In several situations, however, building such a map upfront is hard or even impossible. In this paper, we present a combined approach for robot localization and navigation solely relying on a hand-drawn sketch of the environment. In our work we model the sketch as a two-dimensional manifold with an unknown metric. We use Monte Carlo Localization for estimating the state of the robot in combination with the local deformation of the sketch. Accordingly, our approach can track both, the pose of the robot and the local metric properties of the drawing. Furthermore, we couple the pose tracker with a path planner that uses the global information of the sketch and the estimated local metric to robustly perform navigation tasks in real world environments.	approximation algorithm;autonomous robot;discrepancy function;experiment;lambda calculus;map;mobile robot;monte carlo localization;monte carlo method;obstacle avoidance;robotic mapping;sensor	Federico Boniardi;Bahram Behzadian;Wolfram Burgard;Gian Diego Tipaldi	2015	2015 European Conference on Mobile Robots (ECMR)	10.1109/ECMR.2015.7324188	monte carlo localization;computer vision;navigation;simulation;artificial intelligence;trajectory;mobile robot navigation;measurement;statistics;monte carlo method	Robotics	51.94149400800947	-36.28208382500845	183109
3525cf2eda67a680b1809808b02e24b7c869da73	conflict evaluation method for grid maps using sonar sensors	robot sensing systems;executable specification;map building;sensors;mobile robot;nickel;filtered readings conflict evaluation method grid maps sonar sensor readings;sonar signal processing filtering theory;evaluation method;robots;transversal filters;filtered readings;sonar sensor readings;sonar signal processing;sonar sensors nickel sonar measurements robot sensing systems transversal filters robots;conflict evaluation method;grid maps;filtering theory;sonar measurements;sonar	We present a novel filtering method for building a grid map using sonar sensors. Recognizing that conflicting information from sonar sensor readings is the main reason of poor map quality, we established a proposition that put the conflicting information in order. Based on the proposition, we effectively filtered sonar readings that cause conflict. In addition, some information of filtered readings were not discarded but reprocessed to enhance the quality of the map. We call this filtering and reprocessing operation the conflict evaluation method. We evaluated the proposed method by experiments under various conditions.	computer;experiment;polaroid (polarizer);sonar (symantec);sensor	Kyoungmin Lee;Il Hong Suh;Sang-Rok Oh;Wan Kyun Chung	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4650882	robot;nickel;mobile robot;computer vision;computer science;engineering;sensor;artificial intelligence;sonar	Robotics	52.24889222525736	-33.494971762251694	183365
dbda7d9f91228ac636db37ad6370ae2cc6adbeff	fast and bounded probabilistic collision detection for high-dof trajectory planning in dynamic environments		We present a novel approach to perform probabilistic collision detection between a high-DOF robot and imperfect obstacle representations in dynamic and uncertain environments. Our formulation is designed for high-DOF robot trajectory planning in dynamic scenes, where the uncertainties are modeled using Gaussian distributions. We present an efficient algorithm to compute collision probabilities between the robot and the obstacles. Furthermore, we present a prediction algorithm for obstacle positions that takes into account spatial and temporal uncertainties and uses that for trajectory optimization. We highlight the performance of our trajectory planning algorithm in challenging simulated and real-world environments with robot arms operating next to dynamically moving human obstacles. Note to Practitioners—This paper suggests a novel trajectory planning approach for dynamic and uncertain environments. Existing planning approaches generally deal with uncertainties by performing collision checks using enlarged bounding shapes for the given confidence levels, which are conservative and tend to compute less optimal trajectories or fail to find feasible trajectories. In this paper, we suggest a new collision probability approximation of a robot and obstacles. Our approach guarantees that the computed probability is an upper bound on the actual probability. We then present a trajectory planning algorithm based on our probabilistic collision detection, and a practical belief space estimation algorithm. In our experimental results, we demonstrate that our approach can compute more efficient trajectories than the prior approaches, while our approach has a similar level of safety. Our experiments assume Gaussian distributions for the environment uncertainties, and we will extend our algorithm to non-Gaussian distributions in future research. Recently, we have extended our approach to general convex polytopes and improved the speed and accuracy of the collision probability computation using bounding volume hierarchies.	algorithm;approximation;automated planning and scheduling;bounding volume hierarchy;coat of arms;collision detection;computation;experiment;mathematical optimization;robot;trajectory optimization	Chonhyon Park;Jae Soon Park;Dinesh Manocha	2018	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2018.2801279	mathematical optimization;computer science;collision;trajectory optimization;collision detection;probabilistic logic;trajectory;bounded function;gaussian;bounding volume	Robotics	51.80602446734505	-24.16268550180836	183484
92608137587b785dfc67206e6f8ca7dca6fac6bf	automatic mountain detection and pose estimation for teleoperation of lunar rovers	mobile robot;topographic map;position estimation;situation awareness;space missions;pose estimation	This paper presents a system aimed at mobile robot operations in space: we discuss an interface which receives and analyzes images sent by a rover operating in a distant environment. We are particularly interested in long-duration space missions, where rovers interact with human operators on Earth. The position estimates are presented to the operator so as to increase situational awareness and prevent loss of orientation. The system detects mountains in images and automatically searches for mountain peaks in a given topographic map. We introduce our mountain detector algorithm and present a large number of illustrative results from images collected on Earth and on the Moon (by the Apollo 17 mission). We present an algorithm for position estimation which uses statistical descriptions of measurements to produce estimates, and discuss results for scenery from Pennsylvania, Califomia, Utah, Atacama desert and the Apollo 17 site. The implemented system achieves better estimation performance than any competing method due to our quantitative approach and better time performance due to our pre-compilation of relevant data.	3d pose estimation	Fábio Gagliardi Cozman;Eric Krotkov	1997		10.1007/BFb0112963	mobile robot;situation awareness;computer vision;topographic map;pose;computer science;artificial intelligence;space exploration	Robotics	53.67818094990392	-35.47519454848456	183757
a78392b2fb311eb4103fedda1d6fd35cae17ed28	a parallel implementation of the thresholding problem by using tissue-like p systems	parallel algorithm;thresholding problem;parallel implementation;compute unified device architecture;bio-inspired algorithm;membrane computing technique;novel device architecture;research line;tissue-like p system	In this paper we present a parallel algorithm to solve the thresholding problem by using Membrane Computing techniques. This bio-inspired algorithm has been implemented in a novel device architecture called CUDA™, (Compute Unified Device Architecture). We present some examples, compare the obtained time and present some research lines for the future.	p system;thresholding (image processing)	Francisco Peña-Cantillana;Daniel Díaz-Pernil;Ainhoa Berciano;Miguel A. Gutiérrez-Naranjo	2011		10.1007/978-3-642-23678-5_32	mathematical optimization;computer science;theoretical computer science;algorithm	Logic	41.598515433305536	-30.634615462572256	183819
ed43cf8c15e7c71a29c756bee26b3a37971fc27a	camera-based vehicle velocity estimation from monocular video		This paper documents the winning entry at the CVPR2017 vehicle velocity estimation challenge. Velocity estimation is an emerging task in autonomous driving which has not yet been thoroughly explored. The goal is to estimate the relative velocity of a specific vehicle from a sequence of images. In this paper, we present a light-weight approach for directly regressing vehicle velocities from their trajectories using a multilayer perceptron. Another contribution is an explorative study of features for monocular vehicle velocity estimation. We find that lightweight trajectory based features outperform depth and motion cues extracted from deep ConvNets, especially for far-distance predictions where current disparity and optical flow estimators are challenged significantly. Our light-weight approach is real-time capable on a single CPU and outperforms all competing entries in the velocity estimation challenge. On the test set, we report an average error of 1.12 m/s which is comparable to a (ground-truth) system that combines LiDAR and radar techniques to achieve an error of around 0.71 m/s.	autonomous car;binocular disparity;central processing unit;end system;end-to-end principle;ground truth;motion estimation;multilayer perceptron;optical flow;real-time clock;real-time computing;test set;velocity (software development)	Moritz Kampelmühler;Michael G. Müller;Christoph Feichtenhofer	2018	CoRR		artificial intelligence;pattern recognition;estimator;radar;monocular;multilayer perceptron;lidar;computer science;trajectory;optical flow;test set	Robotics	48.85458780424276	-37.412829802642364	183982
e18c68de07c09b4cb9fb6633181baf554d1fdb9f	dominant orientation tracking for path following	roads mobile robots intelligent robots machine intelligence humans navigation robustness maximum likelihood estimation image segmentation australia;mobile robot;path planning;mobile robots;markov process dominant orientation tracking mobile robots autonomous navigation vision based path following vanishing point extraction;navigation;robot vision;vanishing point;position control;path following road following vanishing point markov dominant orientation;autonomous navigation;model fitting;path following;tracking;position control mobile robots robot vision path planning tracking navigation	The behaviour that allows mobile robots to follow what humans consider as paths is beneficial for autonomous navigation in a wide range of environments. Paths can be corridors, footpaths, catwalks, roads, etc. In fact roads can be considered as well-structured paths. Traditional approaches to path following has been extensions of road following systems. We present a vision-based path following method different to that of the traditional model fitting approach of road following. The method focuses on tracking the direction in which parallel lines in the environment are aligned. It is similar in principle to methods that extract vanishing points but with better robustness achieved with appropriate simplifying assumptions. Some preliminary results of successful path following experiments are also presented.	autonomous robot;curve fitting;experiment;mobile robot	Alan M. Zhang;R. Andrew Russell	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545227	mobile robot;computer vision;simulation;computer science;artificial intelligence;mobile robot navigation	Robotics	50.71403029021174	-37.04786197612091	184047
6383b6c3e32359a7bb164135d47fa5043560ab5d	probabilistic harmonic-function-based method for robot motion planning	robot configuration space probabilistic harmonic function based method robot motion planning random sampling scheme potential field approach;probability;intelligent robots;path planning;random sampling;potential field;explicit knowledge;configuration space;intelligent robots probability path planning sampling methods harmonic analysis;robot motion planning;sampling methods;harmonic function;robot motion motion planning orbital robotics sampling methods path planning intelligent robots manipulators service robots prognostics and health management aerospace industry;harmonic analysis	This paper presents a robot motion planning method, called PHM, that uses a random sampling scheme together with a potential-field approach based on harmonic functions. The combination of both results in an efficient path planner that is both resolution and probabilistic complete. On one hand, random sampling allows the use of the harmonic functions approach without the explicit knowledge of the robot’s Configuration Space. On the other hand, harmonic functions allow an intelligent sampling of Configuration Space by introducing a bias towards the more promising regions.	biasing;monte carlo method;motion planning;regular grid;robot;sampling (signal processing);spaces	Pedro Iñiguez;Jan Rosell	2003		10.1109/IROS.2003.1250658	control engineering;sampling;computer vision;simulation;any-angle path planning;harmonic analysis;mathematics;motion planning;statistics	Robotics	52.19471793043545	-24.588615604473194	184110
b26445440892d31f3546903633a73b17aa14114b	radar tracking for air surveillance in a stressful environment using a fuzzy-gain filter	detectors;two stage kalman filter;f 18 fighters;radar tracking;surveillance;air surveillance;fuzzy rules;search radar;filters;kalman filter;tracking filters;indexing terms;statistical model;fuzzy sets;noise measurement;acceleration;fuzzy logic;stressful environment;statistical analysis;radar tracking surveillance target tracking filters fuzzy logic acceleration fuzzy sets noise measurement radar detection detectors;two stage kalman filter radar tracking air surveillance stressful environment fuzzy gain filter fuzzy logic spl alpha spl beta filter fuzzy if then rules f 18 fighters;prediction accuracy;fuzzy if then rules;radar detection;target tracking;α β filter;tracking filters radar tracking search radar filtering theory target tracking fuzzy logic statistical analysis surveillance;fuzzy gain filter;filtering theory;measurement noise	We present a fuzzy-gain filter for target tracking in a stressful environment where a target may accelerate at nonuniform rates and may also complete sharp turns within a short time period. Furthermore, the target may be missing from successive scans even during the turns, and its positions may be detected erroneously. The proposed tracker incorporates fuzzy logic in a conventional /spl alpha/-/spl beta/ filter by the use of a set of fuzzy if-then rules. Given the error and change of error in the last prediction, these rules are used to determine the magnitude of /spl alpha/ and /spl beta/. The proposed tracker has the advantage that it does not require any assumption of statistical models of process and measurement noise and of target dynamics. Furthermore, it does not need a maneuver detector even when tracking maneuvering targets. The performance of the fuzzy tracker is evaluated using real radar tracking data generated from F-18 and other fighters, collected jointly by the defense departments of Canada and the United States. When compared against that of a conventional tracking algorithm based on a two-stage Kalman filter, its performance is found to be better both in terms of prediction accuracy and the ability to minimize the number of track losses.	radar	Keith C. C. Chan;Vika Lee;Henry Leung	1997	IEEE Trans. Fuzzy Systems	10.1109/91.554452	fuzzy logic;acceleration;kalman filter;statistical model;computer vision;detector;radar tracker;index term;computer science;noise measurement;artificial intelligence;control theory;fuzzy set;statistics	Robotics	48.70085301856357	-34.59181091848863	184635
12a685a1164cbee722c160b09cca19f8c3e02f78	adaptive image sensing and enhancement using the cellular neural network universal machine	adaptive image enhancement;reseau capteur;adaptive sensing;image processing;adaptive control;cellular neural network;procesamiento imagen;traitement image;reseau neuronal cellulaire;captador medida;measurement sensor;red sensores;capteur mesure;control adaptativo;commande adaptative;sensor array;cellular neural networks;cnn	As an attempt to introduce Interactive, Content Dependent Adaptive (ICDA) image processing a simple but powerful active image sensing and two image-enhancement methods are introduced via adaptive CNN-UM sensorcomputers. Thus the method ICDA can be used for adaptive control of image sensing and for subsequent on-line or offline image enhancement as well. The algorithms use both intensity and contrast content. The image sensing technology can be realized with the current CNN-UM chip [1],[2]. Our first image enhancement method is also executable on this chip, but it is more suitable for the Adaptive Cellular Neural Network Universal Machine (ACNN-UM) architecture [3]. Some results of simulator and chip experiments and an adaptive extended cell is presented. Our second, dynamical image enhancement method is planned to be executable on a multi-layer, complex cell CNN architecture [3]. In [15] a 3-layer architecture is described which is capable to realize the main part of the second enhancement method. The main issues of our paper are as follows: the novel outlook of the ICDA framework, 3 new methods for two key application-area of CNN-UM, the notion of “regional” adaptive computing, the novelty of application of equilibriumcomputing in the third method. However, the key novelty of our work is not just a new method and a new realization: by combining sensing and computing, dynamically and pixelwise, a new quality becomes practical.	algorithm;artificial neural network;cellular neural network;executable;experiment;image editing;image processing;image sensor;layer (electronics);microsoft outlook for mac;online and offline;reconfigurable computing;simulation;turing machine	Mátyás Brendel;Tamás Roska	2002	I. J. Circuit Theory and Applications	10.1002/cta.201	electronic engineering;cellular neural network;adaptive control;telecommunications;image processing;computer science;artificial intelligence;machine learning	ML	42.182054822264575	-30.124223779970254	184859
776b632a259d54a02564feb258bbf83b3e187e08	cartesian genetic programming for image processing tasks	image processing;ob- ject localization;genetic programming;image analysis	This paper presents experimental results on image analysis for a particular form of Genetic Programming called Cartesian Genetic Programming (CGP) in which programs use the structure of a graph represented as a linear sequence of integers. The efficency of this approach is investigated for the problem of Object Localization in a given image. This task is usually carried out by applying a series of well known image processing operators and commonly relies on the skills and expertise of the researchers. In this work, we present results from a number of runs on actual camera images, in which a set of fairly simple primitives were investigated.	cartesian closed category;computer vision;crossover (genetic algorithm);evolutionary algorithm;experiment;genetic algorithm;genetic programming;image analysis;image processing	Hector A. Montes;Jeremy L. Wyatt	2003			cartesian coordinate system;image processing;genetic program;operator (computer programming);theoretical computer science;genetic programming;computer science;graph;integer	AI	46.66477536520589	-37.0597511267451	184871
de1775412f12c91d4be4f75939fa7e8155898c0d	challenges of image-sensor development	audio electronic products;semiconductor technology;image resolution;data processing;digital camera;digital camera market;video electronic products;charge coupled devices;ccd image sensors;cmos image sensors;digital cameras image sensors charge coupled image sensors data processing consumer electronics pixel image resolution image quality cmos image sensors high definition video;sensitivity;cmos image sensor;memory capacity;image quality;pixel;digital camera market ccd image sensors semiconductor technology memory capacity high speed data processing audio electronic products video electronic products cmos image sensor high definition;lenses;high speed data processing;cmos image sensors cameras ccd image sensors;high speed;high definition;image sensor;cameras	Due to steady advancements in semiconductor technology, greatly-enhanced memory capacity and high-speed data processing are now available, creating many evolving types of audio and video electronic products. The digital camera represents this trend well, where image sensor evolution has been responsible for enabling strong market growth. CCD image sensors have contributed to the miniaturization of cameras by their continuous march toward smaller pixel size and improved resolution, leading to more megapixels and enhanced image quality. Recently, the image-sensor shift from CCDs to CMOS has enabled faster capture speeds and convenient HD (High Definition). The result of this evolution has been the impressive growth of the digital camera market. This presentation will illustrate the achievements of image-sensor development, and will outline the corresponding advancements in digital cameras. Further, the future of digital cameras will be elaborated, illustrating the way in which image sensors are leading camera growth.	cmos;charge-coupled device;digital camera;image quality;image sensor;pixel;semiconductor	Tomoyuki Suzuki	2010	2010 IEEE International Solid-State Circuits Conference - (ISSCC)	10.1109/ISSCC.2010.5434065	computer vision;electronic engineering;data processing;computer science;image sensor;physics;computer graphics (images)	Robotics	43.29116277802373	-24.853499555576818	185021
52c5b45752c7f87f06e8669840cd765ffd0563e4	adaptive gradient-based analog hardware architecture for 2d under-sampled signals reconstruction		Abstract The paper proposes an analog hardware solution for the implementation of two-dimensional gradient-based algorithm. The algorithm employs the discrete cosine transform and performs missing samples reconstruction by using the compressive sensing principles. Although there is a number of algorithms for solving two-dimensional compressive sensing problems, for many of them a real-time application is a challenging task. Therefore, this paper observes an algorithm whose real-time application has relatively low complexity. Also, the reconstruction accuracy is comparable to the commonly used compressive sensing algorithms. The algorithm is observed within the several parts. The implementation of each part is considered in details, with provided discussion on the computational complexity of each part.	gradient	Nedjeljko Lekic;Nikola Zaric;Irena Orovic;Srdjan Stankovic	2018	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2018.07.009	parallel computing;discrete cosine transform;compressed sensing;computer science;computational complexity theory;hardware architecture	EDA	41.30093000853477	-32.497626542370554	185310
a51cbfd637d0b756afa03202f869c88e27e1cb00	fast human detection for indoor mobile robots using depth images	on board computational resources;graph theory;human presence;linear svm;fast human detection algorithm;clutter;histogram of oriented depth;image segmentation;measurement;hod descriptor;support vector machines;depth cameras;training;segmented region merging;segmented region filtering;gpu;mobile robots;robotics;graph based segmentation algorithm;support vector machines cameras clutter filtering theory graph theory image segmentation mobile robots object detection robot vision;hod based algorithm;parameterized heuristics;cluttered environments;robot vision;cluttered cafe environment;indoor mobile robot;image segmentation training measurement cameras support vector machines mobile robots;cpu core;robot motion;raw depth image segmentation;occlusions;filtering theory;cameras;object detection;segmented region filtering indoor mobile robot occlusions cluttered environments robot motion on board computational resources fast human detection algorithm depth cameras raw depth image segmentation graph based segmentation algorithm parameterized heuristics histogram of oriented depth hod descriptor human presence linear svm cluttered cafe environment cpu core hod based algorithm gpu segmented region merging	A human detection algorithm running on an indoor mobile robot has to address challenges including occlusions due to cluttered environments, changing backgrounds due to the robot's motion, and limited on-board computational resources. We introduce a fast human detection algorithm for mobile robots equipped with depth cameras. First, we segment the raw depth image using a graph-based segmentation algorithm. Next, we apply a set of parameterized heuristics to filter and merge the segmented regions to obtain a set of candidates. Finally, we compute a Histogram of Oriented Depth (HOD) descriptor for each candidate, and test for human presence with a linear SVM. We experimentally evaluate our approach on a publicly available dataset of humans in an open area as well as our own dataset of humans in a cluttered cafe environment. Our algorithm performs comparably well on a single CPU core against another HOD-based algorithm that runs on a GPU even when the number of training examples is decreased by half. We discuss the impact of the number of training examples on performance, and demonstrate that our approach is able to detect humans in different postures (e.g. standing, walking, sitting) and with occlusions.	algorithm;central processing unit;computation;computational resource;computer performance;concatenation;experiment;feature vector;graphics processing unit;headroom (audio signal processing);heuristic (computer science);mobile robot;nonlinear system;on-board data handling;parameterized complexity;region of interest;scale space	Benjamin Choi;Çetin Meriçli;Joydeep Biswas;Manuela M. Veloso	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630711	multi-core processor;mobile robot;support vector machine;computer vision;simulation;computer science;artificial intelligence;graph theory;machine learning;clutter;image segmentation;robotics;measurement	Robotics	44.11224748341317	-36.65072548570461	185536
e3e6a1016b54bc7f2fb446beb57cb523bf4077eb	lateral escape capability analysis with probabilistic pilot model for microburst avoidance	lateral escape;microburst;monte carlo;pilot model;wind shear	Low altitude wind shear presents a significant hazard to aircraft during takeoff and landing operations. Much of the prior research on aircraft escape procedures during microburst encounters has assumed that the aircraft penetrates the center of the microburst in straight flight. Based on a parameterized three-dimensional microburst model and flight dynamics model with wind effects, the lateral escape was studied. The escape strategy and occasion was studied by changing the parameters. Since single pilot's control behavior cannot stand for a group of pilots, a parameterized human pilot model was developed to simulate pilots' characters. A pilot-aircraft-microburst environment model was constructed for further study. To study the safety of lateral escape, the Monte Carlo Simulation was adopted to obtain a numerical approximation of the probability density function of the minimum altitude and F-factor. The results show that although lateral escape capability is affected by pilots' characters and microburst parameters, escape from the weaker side of the wind field is favorable than longitudinal escape.	lateral thinking	Zhenxing Gao;Hongbin Gu;Zheng Gao	2011		10.1007/978-3-642-27503-6_77	meteorology;simulation;engineering;aeronautics	Logic	44.9276812228849	-25.806024295643773	185604
1862d8ba40cc39ecaea63a0630d6e4aa73020a9f	vision-based 2.5d terrain modeling for humanoid locomotion	robot kinematics legged locomotion humanoid robots mobile robots robot vision systems intelligent robots foot stereo vision cameras robot sensing systems;humanoid robot;legged locomotion;path planning;research platform vision based 2 5d terrain modeling humanoid locomotion online terrain modeling system stereo vision 2 5d probabilistic description 3d depth map stereo camera images correlation based localisation planar walking surfaces probabilistic map online footstep planning system intelligent humanoid robotics humanoid robot h7;correlation methods;robot vision;stereo image processing;stereo vision;correlation methods legged locomotion robot vision terrain mapping stereo image processing path planning;terrain modeling;terrain mapping;depth map	We present an integrated humanoid locomotion and online termin modeling system using stereo vision. From a 9D depthmap, a 8.50 probablistic description of the nearby terrain is generated. The depthmap is calculated from a pair of stereo camero images, correlation-based localization is performed, and candidate planar walking surfaces are extmcted. The results are used to update a probabilistic map of the terrain, which is input to an online footstep planning system. Experimental results are shown using the humanoid robot H7, which was designed as a research platform for intelligent humanoid robotics.	2.5d;heightmap;humanoid robot;motion planning;robotics;stereopsis	Satoshi Kagami;Koichi Nishiwaki;James J. Kuffner;Kei Okada;Masayuki Inaba;Hirochika Inoue	2003		10.1109/ROBOT.2003.1241910	stereo cameras;computer vision;simulation;computer science;stereopsis;humanoid robot;artificial intelligence;motion planning;depth map;computer graphics (images)	Robotics	52.380350823451764	-37.47419450411163	185687
16dde3c0260c850175ccd6824724880cb0fdb5af	multiple source localization based on biased bearings using the intensity filter - approach and experimental results	intensity filter;target tracking filtering theory monte carlo methods;source localization;three dimensional;multiple target tracking;bias estimation;systematic bearing error multiple source localization biased bearings intensity filter three dimensional localization problem multiple emitters airborne array sensor sequential monte carlo method smc;position measurement vectors systematics particle measurements atmospheric measurements antenna measurements arrays;bias estimation source localization multiple target tracking intensity filter;direction finding;target tracking;monte carlo methods;filtering theory;sequential monte carlo	This paper investigates the three-dimensional localization problem for multiple emitters using a realistic airborne array sensor. In order to achieve improved results systematic and statistical direction finding errors are considered in a unified algorithm. The task is solved using a sequential Monte Carlo (SMC) implementation of the intensity filter (iFilter). In this paper, we compare two localization approaches without and with the consideration of systematic bearing errors and verify them with an experimental data set. The comparison of both approaches reveals that the bias consideration offers a superior performance.	airborne ranger;algorithm;direction finding;experiment;internationalization and localization;monte carlo method;real-time clock;real-time computing	Marek Schikora;Marc Oispuu;Wolfgang Koch;Daniel Cremers	2011	2011 4th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)	10.1109/CAMSAP.2011.6136046	monte carlo localization;econometrics;mathematical optimization;mathematics;statistics	Robotics	50.290126696239945	-33.52553707426247	186038
d8d9b9acb539140f12b058cd845d760df8ad6092	research on computer-based simulation for the dynamics of land system	dls;function modules;model inference;simulation	The Dynamics of Land System (DLS) model is a collection of programs that simulates pattern changes of land uses by conducting scenario analysis of the area of land use change. The Dynamic Land System (DLS) model provides an important way to predict scenarios for future land productivity. The DLS model is a powerful tool for simulating the dynamics of land use changes, which is theoretically based on restrictions of the distribution of land use types. In this paper, we introduce the main function modules, model inference, model estimation, and model test.	simulation	XiangZheng Deng;Xin Wen	2011		10.1007/978-3-642-23756-0_41	simulation;geography;hydrology	Robotics	40.83834927932173	-29.32327642952553	186305
60404f4d1962e6cfe72ea996b89d0de99688a050	differential evolution and bayesian optimisation for hyper-parameter selection in mixed-signal neuromorphic circuits applied to uav obstacle avoidance		The Lobula Giant Movement Detector (LGMD) is a an identified neuron of the locust that detects looming objects and triggers its escape responses. Understanding the neural principles and networks that lead to these fast and robust responses can lead to the design of efficient facilitate obstacle avoidance strategies in robotic applications. Here we present a neuromorphic spiking neural network model of the LGMD driven by the output of a neuromorphic Dynamic Vision Sensor (DVS), which has been optimised to produce robust and reliable responses in the face of the constraints and variability of its mixed signal analogue-digital circuits. As this LGMD model has many parameters, we use the Differential Evolution (DE) algorithm to optimise its parameter space. We also investigate the use of Self-Adaptive Differential Evolution (SADE) which has been shown to ameliorate the difficulties of finding appropriate input parameters for DE. We explore the use of two biological mechanisms: synaptic plasticity and membrane adaptivity in the LGMD. We apply DE and SADE to find parameters best suited for an obstacle avoidance system on an unmanned aerial vehicle (UAV), and show how it outperforms state-of-the-art Bayesian optimisation used for comparison.	aerial photography;algorithm;artificial neural network;central processing unit;control system;differential evolution;digital electronics;dynamic voltage scaling;loss function;mathematical optimization;mixed-signal integrated circuit;network model;neuromorphic engineering;neuron;obstacle avoidance;optimization problem;reduction (complexity);robot;sensor;spatial variability;spiking neural network;synaptic package manager;unmanned aerial vehicle;usability	Llewyn Salt;David Howard;Giacomo Indiveri;Yulia Sandamirskaya	2017	CoRR		machine learning;spiking neural network;mixed-signal integrated circuit;differential evolution;hyperparameter;looming;computer science;artificial intelligence;obstacle avoidance;neuromorphic engineering;bayesian probability	Robotics	46.586247180123785	-31.631855102262804	186321
51734dc46eafdb0b4c782a1f0c9d45978a480f6a	rapid unsupervised connectionist learning for backing a robot with two trailers	unsupervised learning;control systems;eligibility traces;motion control;three dimensional mapping;autonomous mini robot;three dimensions;application software;path planning;robots unsupervised learning learning systems control systems computer science application software books navigation feedback wheels;mobile robots;books;inter neural cooperation rapid unsupervised connectionist learning connectionist control learning system autonomous mini robot three dimensional mapping temporal domains eligibility traces;learning systems unsupervised learning motion control mobile robots path planning self organising feature maps neurocontrollers;learning systems;learning system;navigation;feedback;rapid unsupervised connectionist learning;self organising feature maps;connectionist control learning system;robots;temporal domains;neurocontrollers;computer science;inter neural cooperation;wheels	This paper presents an application of a connec-tionist control-learning system designed for use on an autonomous mini-robot. This system was formerly shown to form useful two-dimensional mappings rapidly when applied to backing a car with a single trailer. In the current paper the learning system is extended to three dimensions and applied to a similar but signiicantly more diicult problem. The system is shown to be capable of rapid unsupervised learning of output responses in temporal domains through the use of eligibility traces and inter-neural cooperation within topologically deened neighborhoods.	autonomous robot;connectionism;tracing (software);unsupervised learning	Dean F. Hougen;Maria L. Gini;James R. Slagle	1997		10.1109/ROBOT.1997.606735	unsupervised learning;robot;motion control;mobile robot;three-dimensional space;computer vision;navigation;application software;computer science;control system;artificial intelligence;machine learning;feedback;motion planning	Robotics	50.21330655073998	-30.021589715390576	186343
838a723eb950db17d8f7e97f4c4cff6210e44ad8	a new landmark framework for mobile robot localization and asynchronous sensor fusion	robot sensing systems;range sensors;landmark framework;mobile robots sensor fusion robot sensing systems robot kinematics sensor systems indoor environments robot vision systems wheels gaussian distribution orbital robotics;sensor systems;map building;mobile robot;mobile robots;orbital robotics;incomplete information;distance measurement;position control;obstacle vertical edge;indoor environment;asynchronous sensor fusion;indoor environments;mobile robot localization;position control mobile robots distance measurement filtering theory sensor fusion gaussian distribution;dead reckoning;sensor fusion;indoor environment landmark framework mobile robot localization asynchronous sensor fusion obstacle vertical edge gaussian distribution fusion strategy range sensors map building;fusion strategy;robot vision systems;gaussian distribution;filtering theory;robot kinematics;wheels	This paper presents a new landmark framework for mobile robot localization in which we use a combination of obstacle vertical edge and its corresponding plane angle, that appear in practically all indoor environments, to realize mobile robot localization. Sensor data is assumed to be provided in the form of a Gaussian distribution over the space of the robot poses. We propose a new fusion strategy asynchronous sensor fusion (ASF), which either creates complementaries or adds more redundancy for asynchronous sensor data set will also be presented in this paper. This strategy, combined with dead reckoning, enables mobile robots to asynchronously utilize incomplete information captured by range sensors to localize themselves and simultaneously accomplish map building in a partially known indoor environment.	mobile robot;robotic mapping	Tao Bai;Jetic Gu;Gong Cheng;O. Majdalawieh;Peter Xiaoping Liu	2003		10.1109/CIRA.2003.1222211	mobile robot;computer vision;simulation;computer science;artificial intelligence	Robotics	52.78997296738286	-34.41534201696946	186653
f936a87572ddc807ba89b2bd37974c71acc24976	variational bayesian data fusion of multi-class discrete observations with applications to cooperative human-robot estimation	softmax model;robot sensing systems;bayesian network;hybrid likelihood function;gaussian processes;continuous sensor observations;bayes methods;discrete multicategorical state dependent information;mixture model variational bayesian data fusion multiclass discrete observations cooperative human robot estimation continuous sensor observations discrete multicategorical state dependent information cooperative human robot interaction problem hybrid likelihood function softmax model closed form gaussian solution joint human robot target localization hybrid bayesian network;variational techniques;joint human robot target localization;bayesian methods;hybrid bayesian network;variational bayesian;data fusion;usa councils;human robot interaction;cooperative human robot interaction problem;joints;state dependence;real time data;state estimation;orbital robotics;discrete observation;bayesian methods humans robot kinematics robot sensing systems position measurement state estimation usa councils orbital robotics humanoid robots robotics and automation;multiclass discrete observations;variational techniques bayes methods gaussian processes human robot interaction sensor fusion;cooperative human robot estimation;humanoid robots;mixture model;target localization;position measurement;approximation methods;humans;sensor fusion;variational bayesian data fusion;likelihood function;robotics and automation;closed form gaussian solution;robot kinematics	A new method is presented for fusing conventional continuous sensor observations with discrete multi-categorical state-dependent information, which can be furnished by humans in many cooperative human-robot interaction problems. The hybrid likelihood function for mapping between continuous hidden states and categorical observations are specified via softmax models. Although softmax models avoid discretization of continuous states, they are challenging to implement for real-time data fusion since they are not analytically integrable. An approximation based on variational Bayesian (VB) methods is presented here to obtain fast closed-form Gaussian solutions to the desired posteriors in cases where the hidden continuous states have Gaussian pdfs. A joint human-robot target localization example illustrates the properties and utility of the VB hybrid fusion strategy, which also applies more generally to inference in hybrid Bayesian networks and mixture models.	approximation;bayesian network;discretization;human–robot interaction;mixture model;real-time clock;real-time data;softmax function;variational principle	Nisar R. Ahmed;Mark E. Campbell	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509521	human–robot interaction;mathematical optimization;simulation;computer science;engineering;artificial intelligence;machine learning;sensor fusion	Robotics	39.66909826764961	-26.876117138578504	187387
644feecdb539e468e9605f87e84b479d74137125	implementation of grid mapped robot planning algorithm in a continuous map for fire fighting robot	robots planning fires hardware;service robots markov processes mobile robots path planning;robots;planning;fires;autonomous mobile robot grid mapped robot planning algorithm continuous map fire fighting robot navigation robotic competitions markov decision planning problem mdp continuous map wall following algorithm;hardware	Fire-fighting robot is still one of the fields in robotic competitions held these days. This paper is aimed to see the implementation of the Markov Decision Planning (MDP) problem in a fire-fighting robot's navigation. The MDP algorithm evolves planning of the actions the robot should take according to the policy. This planning is mapped into a grid map. Yet in the implementation, this planning is applied in a continuous map. Using a fire-fighting robot the succession of this planning implementation is undertaken. The result shows that the implementation of grid mapped in a continuous map yields significant impacts that lead the MDP to be able to solve the limitation of wall following algorithm. This algorithm is also applied in the real autonomous mobile robot.	algorithm;automated planning and scheduling;autonomous robot;markov chain;mobile robot;succession	Sumarsih Condroayu Purbarani;Qurrotin A'yunina Moa;Grafika Jati;Muhammad Anwar Ma'sum;Hanif Arif Wisesa;Wisnu Jatmiko	2015	2015 International Symposium on Micro-NanoMechatronics and Human Science (MHS)	10.1109/MHS.2015.7438292	mobile robot;computer vision;simulation;engineering;artificial intelligence;robot control;mobile robot navigation	Robotics	53.49430734360131	-27.45818616470213	187485
2e51611da8199238d40919c7e850fe3d1c29a75c	concurrent visual multiple lane detection for autonomous vehicles	traffic control mobile robots object detection predictive control road traffic road vehicles robot vision;monocular vision;predictive control;parametric transform;autonomous vehicle;road traffic;concurrent visual multiple lane detection;vehicle detection;urban road;traffic control;mobile robots;remotely operated vehicles;layout;navigable region;robot vision;image edge detection;roads;voting;pixel;transforms;vehicle detection remotely operated vehicles mobile robots voting roads algorithm design and analysis predictive control layout robustness design methodology;accumulator voting scheme;robustness;vehicles;algorithm design and analysis;cameras;object detection;predictive control concurrent visual multiple lane detection autonomous vehicle monocular vision navigable region urban road accumulator voting scheme parametric transform;road vehicles;design methodology	This paper proposes a monocular vision solution to simultaneous detection of multiple lanes in navigable regions / urban roads using accumulator voting. Unlike other approaches in literature, this paper first examines the extent of lane parameters required for continuous control of any vehicle manually or autonomously. The accumulator-based algorithm is designed using this fundamental control knowledge to vote for the required lane parameters (position of lanes and steering angle required) in the image plane. The novel accumulator voting scheme is called “Parametric Transform for Multi-lane Detection.” This paper not only adapts predictive control in the image plane, but also detects multiple lanes in the scene concurrently in the form of multiple peaks in the accumulator. This method is robust to shadows and invariant to color, texture, and width of the road. Finally, the method is designed for dashed/continuous lines.	accumulator (computing);adobe air;algorithm;automation;image plane;lotus 1-2-3;real-time computing;real-time locating system	Rachana A. Gupta;Wesley E. Snyder;Wallace Shep Pitts	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509389	remotely operated underwater vehicle;layout;mobile robot;algorithm design;computer vision;simulation;voting;design methods;computer science;engineering;monocular vision;artificial intelligence;computer security;model predictive control;pixel;robustness	Robotics	49.198509696789934	-36.292388914379345	187601
9acff321f5aa3c3305ced07618d3366184c65ec3	air-borne approaching target detection and tracking in infrared image sequence	image motion analysis;filter bank;surveillance;surveillance infrared imaging target tracking tracking filters image sequences channel bank filters image motion analysis;tracking filters;infrared imaging;channel bank filters;image sequence;object detection target tracking infrared imaging infrared detectors image sequences filter bank infrared surveillance proposals motion detection logic;a priori information;motion analysis air borne target detection infrared image sequence target tracking surveillance application irst system filter bank single step decision logic;target tracking;infrared;target detection;article;image sequences	Detection and tracking of approaching targets in an infrared (IR) image sequence is important for surveillance applications. In this paper an algorithm is proposed which provides a complete solution (track while scan) for detection and tracking for IRST system. The proposal method uses only motion as a cue to detect the target. Detection is followed by tracking. In a real scenario, the movement of a target is arbitrary and no a priori information is available. We propose a tracking method which tracks maneuvering and nonmaneuvering targets simultaneously using a filter bank. The switch-over amongst the filters is based on a single-step decision logic.	algorithm;filter bank	Mukesh A. Zaveri;S. N. Merchant;Uday B. Desai	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1419476	computer vision;simulation;infrared;tracking system;filter bank	Robotics	49.03133078947922	-34.88199562063484	187608
df19df21d2cd669d973f354a968b37aa24370492	hilbert maps: scalable continuous occupancy mapping with stochastic gradient descent	cognitive robotics;learning and adaptive systems;mapping;mobile and distributed robotics slam;range sensing;sensing and perception computer vision		map;scalability;stochastic gradient descent	Fabio Tozeto Ramos;Lionel Ott	2015		10.15607/RSS.2015.XI.002	mathematical optimization;control theory	Robotics	52.846846687092395	-28.508265357599893	187626
8d210d0e6fbdc8ceba138d9b0769f60f26ec2df2	high-speed image feature detection using fpga implementation of fast algorithm	image features	Many of contemporary computer and machine vision applications require finding of corresponding points across multiple images. To that goal, among many features, the most commonly used are corner points. Corners are formed by two or more edges, and mark the boundaries of objects or boundaries between distinctive object parts. This makes corners the feature points that used in a wide range of tasks. Therefore, numerous corner detectors with different properties have been developed. In this paper, we present a complete FPGA architecture implementing corer detection. This architecture is based on the FAST algorithm. The proposed solution is capable of processing the incoming image data with the speed of hundreds of frames per second for a 512 × , 8-bit gray-scale image. The speed is comparable to the results achieved by top-of-the-shelf general purpose processors. However, the use of inexpensive FPGA allows to cut costs, power consumption and to reduce the footprint of a complete system solution. The paper includes also a brief description of the implemented algorithm, resource usage summary, resulting images, as well as block diagrams of the described architecture.	8-bit;algorithm;central processing unit;corner case;diagram;feature detection (computer vision);field-programmable gate array;grayscale;machine vision;sensor	Marek Kraft;Adam Schmidt;Andrzej J. Kasinski	2008			computer vision;artificial intelligence;feature (computer vision);field-programmable gate array;architecture;machine vision;frame rate;computer science;algorithm;feature detection;detector;block diagram	Vision	44.302597240594444	-35.03096864962536	188291
30117d9e2fc2cf5ac901f6e8088c082935dfe1c3	characterization of infrared range-finder pbs-03jn for 2-d mapping	target surface;distance measure;incidence angle infrared range finders distance measurement errors target color target surface;indexing terms;distance measurement errors;target color;infrared sensors sensor phenomena and characterization costs distance measurement mobile robots laser beams azimuth laser modes energy consumption power system modeling;infrared range finders;laser range finder;indoor environment;incidence angle;cost effectiveness;power consumption;infrared	This paper presents a characterization study of the HOKUYO PBS-03JN Infrared range-finder and compares it to the characterization of the SICK LMS-200 laser range-finder for use in indoor 2-D mapping. Many parameters that could affect the performance of the sensor including warm-up time, divergence of the detection beam, usable detection range in the azimuth, target surface, color, and size properties, incidence angle at the target, and the mixed pixels problem have been studied. This characterization, quantification of errors, and 3-D confidence in the distance readings of the sensor is vital for practical applications. These characteristics are compared to the counterpart characteristics of the laser range-finder. The PBS-03JN is a cost effective alternative to laser range-finders in indoor environments. The sensor is attractive due to lower power consumption, and its lightweight.	assistive technology;incidence matrix;mobile device;pixel;robot;sensor;uptime	Majd Alwan;Matthew Wagner;Glenn S. Wasson;Pradip Sheth	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570722	computer vision;cost-effectiveness analysis;index term;infrared;optics;angle of incidence;remote sensing	Robotics	53.46181957744455	-35.101258435961675	188446
c6ddbfd331f3db29dac9603de36b6729dc587379	dual-module data fusion of infrared and radar for track before detect	dynamic programming;data fusion;particle filter;track before detect;probability of detection	A track before detect method based on data fusion of infrared and radar is proposed to increase the probability of correct track initiation and shorten initiation time. Track before detect is a new technique for dim target detection and tracking which is useful when the signal-tonoise ratio of target is low. Particle filter and dynamic programming for track before detect are currently proposed for detecting and tracking dim targets in low signal-to-noise ratio background. In this paper, we apply them in dual-module data fusion of infrared and radar. Particle Filter is applied to process the acquired data from radar. Dynamic programming is applied to process the acquired data from Infrared. Sensor receives data and generates the first stage decision. The decisions are subsequently transmitted to the fusion center where they are combined into a final decision on distributed fusion architectures. The proposed method is applied to simulate track before detect in dual module system of infrared and radar. The simulation results show that the proposed method increase the probability of correct track initiation and shorten initiation time.	dynamic programming;modular programming;particle filter;radar;sensor;signal-to-noise ratio;simulation;track-before-detect	Anfu Zhu;Yunfei Li;Lingling Lv;Hongtao Zhang	2012	JCP	10.4304/jcp.7.12.2861-2867	track-before-detect;computer vision;simulation;particle filter;computer science;machine learning;dynamic programming;statistical power;sensor fusion;statistics	Mobile	48.79194934003638	-33.87610552289785	188569
54a5ae12c327725fa2aa5204bdf8145b26a470dc	improved shape parameter estimation in k clutter with neural networks and deep learning	tecnologias generalidades;tecnologias	The discrimination of the clutter interfering signal is a current problem in modern radars� design, especially in coastal or offshore environments where the histogram of the background signal often displays heavy tails. The statistical characterization of this signal is very important for the cancellation of sea clutter, whose behavior obeys a K distribution according to the commonly accepted criterion. By using neural networks, the authors propose a new method for estimating the K shape parameter, demonstrating its superiority over the classic alternative based on the Method of Moments. Whereas both solutions have a similar performance when the entire range of possible values of the shape parameter is evaluated, the neuronal alternative achieves a much more accurate estimation for the lower Fig.s of the parameter. This is exactly the desired behavior because the best estimate occurs for the most aggressive states of sea clutter. The final design, reached by processing three different sets of computer generated K samples, used a total of nine neural networks whose contribution is synthesized in the final estimate, thus the solution can be interpreted as a deep learning approximation. The results are to be applied in the improvement of radar detectors, particularly for maintaining the operational false alarm probability close to the one conceived in the design.	artificial neural network;clutter;deep learning;estimation theory;neural network software	José Raúl Machado Fernández;Jesús de la Concepción Bacallao Vidal	2016	IJIMAI	10.9781/ijimai.2016.3714	speech recognition;engineering;artificial intelligence;constant false alarm rate;statistics	ML	39.19255100800657	-33.11478126964594	188571
9ae7cea26c94c10a14fea267e24246f936f6a534	evolving image processing operations for an evolvable hardware environment	mascara;image processing;seuil;implementation;representation fonction;procesamiento imagen;threshold;evolvable hardware;algoritmo genetico;traitement image;control proceso;development environment;function representation;representacion funcion;process control;algorithme genetique;evolution strategy;genetic algorithm;masque;umbral;implementacion;mask;commande processus;fitness function	This paper describes the application of genetic algorithms to evolve new spatial masks for non-linear image processing operations, which are ultimately to be implemented on evolvable hardware. The development environment was custom-built to allow full control over the evolution process and enable the importance of the evolution strategy (including the representation scheme, parameters and fitness function) to be investigated and understood. Results of applying the evolved mask to threshold real-world images are provided and are shown to be an improvement on conventional image processing operations. The envisaged infrastructure for the evolvable hardware is also considered, and the implementation of the image processing operations discussed.		Stephen L. Smith;David P. Crouch;Andrew M. Tyrrell	2003		10.1007/3-540-36553-2_30	simulation;genetic algorithm;image processing;computer science;artificial intelligence;machine learning;process control;development environment;function representation;mask;evolution strategy;programming language;implementation;fitness function;algorithm	Robotics	42.05229968433686	-30.133234107244032	188621
58204492364be9a15c78c0caf57a16d7bbfd7457	an illumination identification system for the aibo robot	vision system;280200 artificial intelligence and signal and image processing;legged locomotion;illumination identification system;color;rule based system;robot vision image colour analysis knowledge based systems mobile robots;mobile robots;robot vision system;information and communication services;robocup;computer vision;rule based system illumination identification system aibo robot four legged league robocup robot vision system color classification;robot vision;image edge detection;280000 information computing and communication sciences;aiborobotimage processing;image colour analysis;machine vision;pixel;robots;information and communication services not elsewhere classified;classification system;four legged league;lighting;switches;table lookup;color classification;information and computing sciences;calibration;artificial intelligence and image processing;knowledge based systems;other information and communication services;lighting robots switches calibration pixel machine vision legged locomotion image edge detection table lookup color;aibo robot	The Four Legged League is a division of the RoboCup initiative that uses Sony AIBOtrade robots to further robotics research. Most participants implement vision systems that use the color of objects to perform identification. Calibration of the color classification system must be done and any changes to the lighting of the environment after calibration reduces the accuracy of the system, often to a point at which the robot is effectively blind. This study investigates the relationships in the color data of image pixels between lighting conditions in an effort to identify trends that can be used as the basis of a rule-based system. The aim of the system is to identify the current lighting level as one of a set of known conditions. The proposed systems uses the color data of image pixels and information about the AIBO 's location and orientation to identify lighting levels, allowing a vision system to switch to an appropriate pre-configured calibration.	aibo;color;pixel;robot;robotics;rule-based system	Michael S. Zehmeister;Yang Sok Kim;Byeong Ho Kang	2007	2007 International Conference on Multimedia and Ubiquitous Engineering (MUE'07)	10.1109/MUE.2007.78	robot;mobile robot;computer vision;calibration;simulation;machine vision;network switch;computer science;artificial intelligence;lighting;pixel;image-based lighting	Robotics	48.097680393441074	-37.08207707538991	189706
36793acef84858f8aafc054f137350f0756b0dee	dynamic and probabilistic estimation of manipulable obstacles for indoor navigation	histograms;robots navigation heuristic algorithms mathematical model probabilistic logic histograms planning;probabilistic representation dynamic estimation probabilistic estimation manipulable obstacle indoor navigation indoor mobile robotics platform gamma distributed cost visual feature data color histogram lower confidence bound estimate dynamic replanning search algorithm d lite;navigation;heuristic algorithms;robots;mathematical model;planning;search problems estimation theory gamma distribution mobile robots path planning;probabilistic logic	In this paper we derive and implement an algorithm for an indoor mobile robotics platform to estimate the manipulability of initially unknown obstacles while navigating through its environment to a pre-specified goal. The environment is represented by an evidence grid, where each cell contains a gamma-distributed cost as well as visual feature data in the form of a color histogram. While navigating, the robot associates visual features of objects occupying a given cell with manipulability cost estimates of that cell, learning whether an object or obstacle can be moved or not in the robot's attempt to reach the goal. We derive and utilize a lower confidence bound (LCB) estimate for the cost of each cell in order to incorporate an exploration (versus pure exploitation) element to the robot's search for the lowest-cost path. Combining the LCB cost estimates with the dynamic replanning search algorithm D*-Lite, we can quickly compute optimal navigation paths regardless of the numerous changes occurring in the robot's environmental belief state. We explain the probabilistic representation of cost in the evidence grid and provide simulation and real-world results for our algorithm in a navigation scenario with static and movable objects.	color histogram;experiment;feature data;mobile robot;motion planning;robotics;search algorithm;simulation	Christopher Clingerman;Peter J. Wei;Daniel D. Lee	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7354249	planning;robot;computer vision;navigation;simulation;computer science;machine learning;mathematical model;histogram;probabilistic logic;mobile robot navigation;statistics	Robotics	51.30008431202012	-26.26972571974475	189807
ac6807b67243a789d86628934fb036b4dfbe795b	a lidar and vision-based approach for pedestrian and vehicle detection and tracking	monocular vision;in vehicle lidar;bayes methods;automated highways;vehicle detection;bayesian sum decision rule vehicle detection vehicle tracking sensorial cooperative architecture intelligent vehicles in vehicle lidar monocular vision object classification method;sensorial cooperative architecture;object classification method;bayesian sum decision rule;gaussian mixture model;optical radar;space use;laser radar vehicle detection object detection cameras bayesian methods intelligent transportation systems intelligent vehicles laser modes machine vision space technology;intelligent vehicles;object classification;vehicle tracking;optical radar automated highways bayes methods object detection;decision rule;object detection	This paper presents a sensorial-cooperative architecture to detect, track and classify entities in semi-structured outdoor scenarios for intelligent vehicles. In order to accomplish this task, information provided by in-vehicle Lidar and monocular vision is used. The detection and tracking phases are performed in the laser space, and the object classification methods work both in laser space (using a Gaussian Mixture Model classifier) and in vision spaces (AdaBoost classifier). A Bayesian-sum decision rule is used in order to combine the results of both classification techniques, and hence a more reliable object classification is achieved. Experiments confirm the effectiveness of the proposed architecture.	adaboost;bayesian network;color;entity;experiment;mixture model;modal logic;semiconductor industry;sensor;sum rule in quantum mechanics;xbox live vision	Cristiano Premebida;Gonçalo Monteiro;Urbano Nunes;Paulo Peixoto	2007	2007 IEEE Intelligent Transportation Systems Conference	10.1109/ITSC.2007.4357637	computer vision;simulation;engineering;remote sensing	Robotics	48.79684997167624	-36.46228633128685	189889
99b50ca194d9eb7247e0d38f2c8e8ad753f7126b	multi-target tracking using a vision chip and its applications to real-time visual measurement	vision system;real time visualization;image processing;vision chip;rotation measure;chip;visual measurement;multi target tracking;target tracking;real time image processing;high speed	Real-time image processing at high frame rates could play an important role in various visual measurement. Such image processing can be realized by using a high-speed vision system imaging at high frame rates and having appropriate algorithms processed at high speed. We introduce a vision chip for high-speed vision and propose a multi-target tracking algorithm for the vision chip utilizing the unique features. We describe two visual measurement applications, target counting and rotation measurement. Both measurements enable excellent measurement precision and high flexibility because of high-frame-rate visual observation achievable. Experimental results show the advantages of vision chips compared with conventional visual systems.	algorithm;image processing;real-time clock;system image;weak measurement	Yoshihiro Watanabe;Takashi Komuro;Shingo Kagami;Masatoshi Ishikawa	2005	JRM	10.20965/jrm.2005.p0121	computer vision;simulation;computer science;human visual system model;computer graphics (images)	Robotics	45.30599408633501	-35.63260677717712	190464
aa37127a266af252218581437e473f6300050d33	learning to navigate from limited sensory input: experiments with the khepera microrobot	unsupervised learning;relative position;microrobots;state space methods;position coordinates autonomous robot navigation limited sensory input khepera microrobot state space encoding direct sensor readings state representation reinforcement learning algorithms relative positional information inference unsupervised learning optimal sensor fusion;navigation robot sensing systems robot kinematics orbital robotics state space methods clustering algorithms unsupervised learning sensor fusion encoding electronic mail;reinforcement learning;mobile robots;sensor fusion microrobots mobile robots computerised navigation learning artificial intelligence state space methods;state space;sensor fusion;learning artificial intelligence;autonomous robot;computerised navigation	The goal of this work is to augment reinforcement learning techniques for autonomous robot navigation with a state space encoding more representative of the actual state of the robot in its environment, than available from direct sensor readings. A second goal is to demonstrate the approach in a real-world setting, using the microrobot Khepera (K-Team, Lausanne, Switzerland). The choice of state representation is one of the most critical factors in the performance of reinforcement learning algorithms. The technique of inferring relative positional information indirectly from sensor readings, through unsupervised learning, is an important novel contribution of this work. As demonstrated in the robot experiments, the technique allows to optimally perform sensor fusion and avoids the need of more elaborate sensors conveying explicit information on position coordinates.	algorithm;autonomous robot;experiment;khepera mobile robot;machine learning;microbotics;reinforcement learning;robotic mapping;sensor;state space;switzerland;unsupervised learning	Roman Genov;Srinadh Madhavapeddi;Gert Cauwenberghs	1999		10.1109/IJCNN.1999.832703	unsupervised learning;mobile robot;robot learning;computer vision;computer science;state space;artificial intelligence;social robot;machine learning;sensor fusion;reinforcement learning;mobile robot navigation	Robotics	50.758778079480585	-29.922575618879943	190697
e249e873d5cb65f0ba881e5bb2b90a7fd22d9111	stereo vision embedded system for augmented reality	system on a chip;field programmable gate arrays augmented reality cameras graphics processing unit streaming media system on a chip pipelines;embedded systems;streaming media;system on chip;pipelines;stereo image processing;field programmable gate arrays stereo vision embedded system augmented reality scene depth map computer generated objects real life video boundary precision gige vision board altera stratix iv fpga acadia ii system on chip power requirement reduction size requirement reduction cpu workload reduction graphics card latency user mobility stereo algorithm;visual perception;field programmable gate arrays;augmented reality;graphics processing unit;visual perception augmented reality embedded systems field programmable gate arrays stereo image processing system on chip;cameras	Stereo Vision processing is a critical component of Augmented Reality systems that rely on the precise depth map of a scene to properly place computer generated objects with real life video. Important aspects of the stereo processing are the creation of a dense depth map, high boundary precision, low latency and low power. We present an embedded system for Stereo Vision Processing based on a custom GigE vision board with an Altera Stratix IV FPGA and the Acadia® II System-On-Chip that replaces an existing GPU/PC based system. By porting the stereo algorithm to an FPGA, we reduced the size and power requirements by reducing the workload of the CPU and eliminated the need of a high-end graphics card. The embedded system processes the same algorithm as the GPU/PC based system, but at 10× lower power and lower latency. Placed in a small enclosure, the overall system enables more user mobility for a more compelling user experience.	algorithm;atom (standard);augmented reality;binocular disparity;central processing unit;depth map;embedded system;field-programmable gate array;gige vision;graphics processing unit;parallel computing;personal computer;real life;requirement;single-board computer;stereopsis;stratix;system on a chip;user experience;video card;visual odometry	Eduardo Gudis;Gooitzen S. van der Wal;Sujit Kuthirummal;Sek M. Chai;Supun Samarasekera;Rakesh Kumar;Vlad Branzoi	2012	2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2012.6238889	system on a chip;embedded system;stereo cameras;computer vision;augmented reality;computer science;computer graphics (images)	Vision	43.50737198264597	-34.86789768568339	190807
aab882c8aaa134cf255d907858d8b7901d63b441	the use of optical flow for the autonomous navigation	autonomous navigation;optical flow	Insects and a lot of other animals use the optical flow to control the direction of their motion and to avoid obstacles. This paper describes experiments suggesting the possible use of the optical flow for the navigation of a robot moving in indoor and outdoor environments. In indoor scenes, such as corridors, offices and laboratories, the optical flow is used to detect and localize obstacles. These routines are based on the computation of a reduced optical flow. Almost real time performance was obtained with standard workstations, such as SUN 3 or SUN Sparcstation 1. The mobile vehicle is usually able to avoid large obstacles such as a chair or a human, but it is not able to avoid thin obstacles such as a rod or a bar. The avoidance performances of the proposed algorithm critically depend on the feedback loop between the vision module and the motor system. In outdoor scenes the optical flow can be used to understand the egomotion, that is to obtain information on the absolute velocity of the moving vehicle. The optical flow is corrected for shocks and vibration present during image acquisition. Regions of the image are extracted, where the optical flow is reliable, and the information on egomotion is recovered from the optical flow here obtained. These results suggest that the optical flow can be successfully used by biological and artificial systems for controlling their motion and for avoiding obstacles.	optical flow	Antonio Malisia;Andrea Baghino;Marco Campani;Marco Straforini;Vincent Torre	1992	Int. J. Neural Syst.	10.1142/S0129065792000450	computer vision;simulation;computer science;optical flow	Robotics	48.4572406748195	-37.778252853156346	190993
a3a71e8d64051377e3b10796071f4184a056c757	vision-based maritime surveillance system using fused visual attention maps and online adaptable tracker	video surveillance;neural nets;video surveillance marine systems neural nets object detection sea ports;venetian port vision based maritime surveillance system fused visual attention map online adaptable tracker moving ptz camera local authority low level image online adaptable neural network tracker limassol port;image edge detection image color analysis visualization surveillance cameras tracking training;object detection;sea ports;marine systems	This paper presents a vision-based system for maritime surveillance using moving PTZ cameras. This system is intended to be used as an early warning system by local authorities. It fuses a visual attention method that exploits low-level image features appropriately selected for maritime environment, with an online adaptable neural network tracker, without making any assumptions about environmental or visual conditions. Systems performance was evaluated with videos from cameras placed at Limassol port and Venetian port of Chania and concerns robustness compared to dynamically changing visual conditions and different kinds of vessels, all in real time.	artificial neural network;high- and low-level;map;norm (social);pan–tilt–zoom camera;window blind	Konstantinos Makantasis;Anastasios D. Doulamis;Nikolaos D. Doulamis	2013	2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)	10.1109/WIAMIS.2013.6616150	computer vision;simulation;computer science;machine learning;artificial neural network	Robotics	47.83854920976791	-37.230266738027154	191008
5232c4a38ea2a8c8d87ac3ad1e87f9306aa461b5	cooperative coevolution fusion for moving object detection	environmental conditions;moving object detection;sensor fusion;cooperative coevolution	In this paper we introduce a novel sensor fusion algorithm based on the cooperative coevolutionary paradigm. We develop a multisensor robust moving object detection system that can operate under a variety of illumination and environmental conditions. Our experiments indicate that this evolutionary paradigm is well suited as a sensor fusion model and can be extended to different sensing modalities.	algorithm;cooperative coevolution;experiment;object detection;programming paradigm;sensor web	Sohail Nadimi;Bir Bhanu	2004		10.1007/978-3-540-24854-5_61	computer vision;simulation;computer science;sensor fusion	Robotics	48.58156130050459	-33.424421462381225	191106
523e3fd6792dec3aa04314e8cd1533eb699de5cb	abnormal movement state detection and identification for mobile robots based on neural networks	modelizacion;robot movil;informatique mobile;estimation etat;mobile robot;cinematica;aplicacion espacial;intelligence artificielle;robotics;probabilistic approach;state estimation;kinematics;modelisation;simulation experiment;wheeled mobile robot;planetary exploration;robot mobile;enfoque probabilista;approche probabiliste;cinematique;robotica;artificial intelligence;robotique;inteligencia artificial;reseau neuronal;mobile computing;probabilistic neural network;modeling;estimacion estado;red neuronal;application spatiale;moving robot;space application;neural network	Movement state estimation plays an important role in navigating and movement controlling for wheeled mobile robots (WMRs), especially those in unknown environments such as planetary exploration. When exploring in unknown environments, mobile robot suffers from many kinds of abnormal movement state, such as baffled by an obstacle, slipping, among others. This paper employs neural network method to detect abnormal movement states. Specifically, it exploits the kinematics of the normal and abnormal movement states of the monitored robot. Several residuals are exploited and four probabilistic neural networks are used to classify the residuals. Simulation experiments show that the methods can detect and identify most abnormal movement states.	neural networks;robot	Zhuohua Duan;Zixing Cai;Xiaobing Zou;Jinxia Yu	2005		10.1007/11427469_45	mobile robot;computer vision;kinematics;probabilistic neural network;simulation;systems modeling;computer science;artificial intelligence;robotics;mobile computing;artificial neural network	Robotics	52.372656143775785	-30.155025916607027	191120
5692441eccd0298566febad11cf8e2c1d8861d4e	sensation preserving simplification for haptic rendering	haptic display;complex objects;construccion arquitectura tecnologia ambiental;computacion informatica;object interaction;computer model;grupo de excelencia;haptics;polyhedral model;performance improvement;collision detection;haptic rendering;ciencias basicas y experimentales;level of detail algorithms;tecnologias;haptic perception	"""We introduce a novel """"sensation preserving"""" simplification algorithm for faster collision queries between two polyhedral objects in haptic rendering. Given a polyhedral model, we construct a multiresolution hierarchy using """" filtered edge collapse"""", subject to constraints imposed by collision detection. The resulting hierarchy is then used to compute fast contact response for haptic display. The computation model is inspired by human tactual perception of contact information. We have successfully applied and demonstrated the algorithm on a time-critical collision query framework for haptically displaying complex object-object interaction. Compared to existing exact contact query algorithms, we observe noticeable performance improvement in update rates with little degradation in the haptic perception of contacts."""	haptic technology;level of detail	Miguel A. Otaduy;Ming C. Lin	2005		10.1145/1198555.1198607	computer simulation;computer vision;simulation;computer science;artificial intelligence;haptic technology;collision detection;computer graphics (images)	Visualization	41.45211495160309	-37.387334107191066	191687
73489ebdc62d29f0aa1fee3658ed6e751243d43c	practical application of parallel coordinates for climate model analysis	environmental variables;climate models;climates;climate model;information visualization;data mining;water visual analytics;model evaluation;visualization technique;environmental sciences;statistics;evaluation;visual analytics;climate variability;community land model;correlation analysis;parallel coordinates;runoff	The determination of relationships between climate variables and the identification of the most significant associations between them in various geographic regions is an important aspect of climate model evaluation. The EDEN visual analytics toolkit has been developed to aid such analysis by facilitating the assessment of multiple variables with respect to the amount of variability that can be attributed to specific other variables. EDEN harnesses the parallel coordinates visualization technique and is augmented with graphical indicators of key descriptive statistics. A case study is presented in which the focus is on the Harvard Forest site (42.5378N Lat, 72.1715W Lon) and the Community Land Model Version 4 (CLM4) is evaluated. It is shown that model variables such as land water runoff are more sensitive to a particular set of environmental variables than a suite of other inputs in the 88 variable analysis conducted. The approach presented here allows climate-domain scientists to focus on the most important variables in the model evaluations.	climate model;parallel coordinates;spatial variability;typset and runoff;visual analytics	Chad A. Steed;Galen M. Shipman;Peter E. Thornton;Daniel M. Ricciuto;David Erickson;Marcia L. Branstetter	2012		10.1016/j.procs.2012.04.094	visual analytics;simulation;information visualization;computer science;artificial intelligence;machine learning;data mining;statistics;climate model	ML	40.836999884545214	-29.31843751054431	191972
b159e26f587954fd1f0faf89fffd514373905803	active binocular gaze control inspired by superior colliculus	robotic vision system active binocular gaze control visual maps binocular saccade vergence movements active vision system binocular disparity selective neurons superior colliculus isotropic disparity energy;neural nets;gaze control;robot vision;orientation selectivity;superior colliculus;binocular disparity;robot vision active vision neural nets;hardware implementation;active vision;neurons eyes control systems visual system machine vision sampling methods hardware robot control robot vision systems centralized control	We describe the construction of visual maps used to guide binocular saccade-vergence movements in an active vision system. The model neurons in the maps are inspired by binocular disparity selective neurons found in the superior colliculus (SC). We modify the standard disparity energy model so that it is non-orientation selective (isotropic), like the neurons in SC. We analyze the differences between the standard and isotropic models, arguing that the isotropic disparity energy is well suited for gaze control. Finally, we describe the hardware implementation of this model, which we use to demonstrate that these model neurons can be effective in guiding binocular gaze control of a robotic vision system.	active vision;binocular disparity;binocular vision;computation;computational resource;escuela superior latinoamericana de informática;map;neuromorphic engineering;neuron;robot;vergence	Eric K. C. Tsang;Bertram E. Shi	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246652	binocular disparity;computer vision;simulation;active vision;computer science;artificial neural network	Vision	47.297377263334496	-32.14286116605732	192444
25a686e141239e1a38ef294afc368b979db9d85d	uwgsp4: merging parallel and pipelined architectures for imaging and graphics	conference		graphics;pipeline (computing)	H. W. Park;T. Alexander;K. S. Eo;Yongmin Kim	1991			computer hardware;computer science;theoretical computer science;computer graphics (images)	Visualization	42.908121186203886	-32.09144888242013	192499
48eadbf196da949ae469d667d2a569d0c950ecfd	an enhanced occupancy map for exploration via pose separation	phantom obstacles sensor measurement bearing occupancy map incident poses pose map robot;path planning;robot sensing systems robot kinematics humans fuses surveillance reconnaissance sensor fusion sonar measurements navigation;array signal processing;quality of information;robots;microsensors;array signal processing path planning robots microsensors	We develop a new occupancy map that respects the role of the sensor measurement bearing and how it relates to the resolution of the existing occupancy map. We borrow an idea from Konolige for recording and tracking, in an occupancy-like map, the bearing at which sensor readings originate with respect to a given cell. Our specific contribution is in the way we process the sensor pose information, which is the bearing of the sensor readings when it indicates the presence of an obstacle in a particular cell. For each cell in the occupancy map, we calculate the greatest separation of incident poses, and then store that information in a new two-dimensional array called a pose map. A cell in the pose map measures the quality of information contained in the corresponding cell of the occupancy map. We merge the new pose map with the existing map to generate an enhanced occupancy map. Exploration plans derived from the enhanced occupancy map are more efficient and complete in that they do not guide the robot around phantom obstacles nor incorrectly classify narrow openings as closed commonly found in conventional occupancy maps.	array data structure;imaging phantom;map	Robert Grabowski;Pradeep K. Khosla;Howie Choset	2003		10.1109/IROS.2003.1250712	robot;computer vision;simulation;computer science;artificial intelligence;occupancy grid mapping;motion planning;information quality	Robotics	52.59727317400148	-33.52223572663308	192717
0d61f102044d1cc3bf0f3a5c22174f18e983424e	two-stage focused inference for resource-constrained collision-free navigation	article	Long-term operations of resource-constrained robots typically require hard decisions be made about which data to process and/or retain. The question then arises of how to choose which data is most useful to keep to achieve the task at hand. As spacial scale grows, the size of the map will grow without bound, and as temporal scale grows, the number of measurements will grow without bound. In this work, we present the first known approach to tackle both of these issues. The approach has two stages. First, a subset of the variables (focused variables) is selected that are most useful for a particular task. Second, a task-agnostic and principled method (focused inference) is proposed to select a subset of the measurements that maximizes the information over the focused variables. The approach is then applied to the specific task of robot navigation in an obstacle-laden environment. A landmark selection method is proposed to minimize the probability of collision and then select the set of measurements that best localizes those landmarks. It is shown that the two-stage approach outperforms both only selecting measurement and only selecting landmarks in terms of minimizing the probability of collision. The performance improvement is validated through detailed simulation and real experiments on a Pioneer robot.	experiment;information gain in decision trees;kullback–leibler divergence;pioneer;robot;robotic mapping;simulation	Beipeng Mu;Ali-akbar Agha-mohammadi;Liam Paull;Matthew C. Graham;Jonathan P. How;John J. Leonard	2015		10.15607/RSS.2015.XI.004	computer science;data mining;operations research	Robotics	51.89519648833574	-25.828380004767478	193005
3dc715c7d1ed4004ec68bc789f66e3a128c00c0d	online camera calibration and node localization in sensor networks	self calibration of imagers;sensor network;low power;target recognition;camera sensor networks;camera calibration;image sensor	The use of low-cost imagers in sensor network applications is imminent. Small low-power imagers have many potential applications in visual confirmation of events, target recognition, security and entertainment. To make the autonomous operation of imager sensors in such applications possible, smart imager sensors should be able to calibrate their views with respect to other imagers in the network and should become aware of the locations of other non-imager sensors. Such calibration will allow cameras with multiple vantage points and sensors distributed in physical space to jointly undertake collaborative tasks.	autonomous robot;captcha;camera resectioning;image sensor;low-power broadcasting	Andrew Barton-Sweeney;Dimitrios Lymberopoulos;Eugenio Culurciello;Andreas Savvides	2005		10.1145/1098918.1098965	embedded system;computer vision;camera resectioning;wireless sensor network;computer science;image sensor;visual sensor network	Mobile	51.18339238048323	-32.58503295920433	193126
fe46518b399b5ae9729c88c17c040a42bc0185b3	incremental and adaptive multi-robot mapping for human scene observation	cooperation;mobile robots;navigation;multi robot mapping;human scene observation;robot vision systems;cameras;robot kinematics	This paper aims to use a fleet of mobile robots, each embedding a camera, to optimize the observation of a human dynamic scene. The scene is defined as a sequence of activities, performed by a person in a same place. Mobile robots have to cooperate to find a spatial configuration around the scene that maximizes the joint observation of the human pose skeleton. It is assumed that the robots can communicate but have no map of the environment and no external localisation. This paper presents a concentric navigation topology allowing to keep easily each robot camera towards the scene. This topology is combined with an incremental mapping of the environment in order to limit the complexity of the exploration state space. We also introduce the marginal contribution of each robot observation, to facilitate stability in the search, while the exploration is guided by a meta-heuristics. We developped a simulator that uses skeleton data from real human pose captures. It allows to compare the variants of the approach and to show its features such as adaptation to the dynamic of the scene and robustness to the noise in the observations.	activity recognition;anytime algorithm;brute-force search;experiment;heuristic (computer science);language localisation;marginal model;mobile robot;robustness (computer science);state space;turtle (robot);videotelephony	Jonathan Cohen;Laëtitia Matignon;Olivier Simonin	2016	2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2016.0108	mobile robot;computer vision;navigation;simulation;computer science;artificial intelligence;cooperation;mobile robot navigation;robot kinematics	Robotics	51.38325023120302	-35.596543450090536	193789
504f4affc504cb019825faebd18bd789e5147504	learning occupancy grids with forward models	estimation theory;probability;high dimensionality;forward model;mobile robot;path planning;bayes methods;occupancy grid;probability mobile robots path planning bayes methods estimation theory;mobile robots;expectation maximization algorithm;uncertainty occupancy grids forward models mobile robots high dimensional mapping problem statistical formulation expectation maximization algorithm laplacian approximation;mobile robots uncertainty sonar navigation robot sensing systems inverse problems computer science path planning collision avoidance sonar measurements noise measurement	This paper presents a new way to acquire occupancy grid maps with mobile robots. Virtually all existing occupancy grid mapping algorithms decompose the highdimensional mapping problem into a collection of onedimensional problems, where the occupancy of each grid cell is estimated independently of others. This induces conflicts that can lead to inconsistent maps. This paper shows how to solve the mapping problem in the original, highdimensional space, thereby maintaining all dependencies between neighboring cells. As a result, maps generated by our approach are often more accurate than those generated using traditional techniques. Our approach relies on a rigorous statistical formulation of the mapping problem using forward models. It employs the expectation maximization algorithm for estimating maps, and a Laplacian approximation to determine uncertainty.	approximation;expectation–maximization algorithm;map;mobile robot	Sebastian Thrun	2001		10.1109/IROS.2001.977219	mobile robot;computer vision;mathematical optimization;computer science;artificial intelligence;machine learning;occupancy grid mapping;statistics	Robotics	47.86843921695514	-26.87188102500877	193907
71f5f14e56f53385763f2fd00228f6b449b2cb23	accommodating memory latency in a low-cost rasterizer	homogeneous coordinates;video game;image generation;clipping;rasterization;scan conversion;memory bandwidth;memory latency	This paper describes design tradeoffs in a very low cost raster&r circuit targeted for use in a video game console. The greatest single factor affecting such a design is the character of memory to which the image generator is connected. Low costs generally constrain the memory dedicated to image generation to be a single package with a single set of address and data lines. While overnll memory bandwidth determines the upper limit of performance in such a small image generator, memory latency has n fnr greater effect on the design. The use of Rambus memory provides more than enough aggregate bnndwidth for a frame buffer as long as blocks of pixels nre moved in each transfer, but its high latency can stall nny processor not matched to the memory. The design described here utilizes a long pixel pipeline to match its internal processing latency to the external fmme buffer memory latency. 1. Small Scale Image Generator High performance graphics which was once limited to hugely expensive flight simulators, and which eventually made its way into small lnboratories has rapidly become a standard fixture on the desktop and in video game consoles. However, designs of graphics processors for the low end bear little resemblance to their larger predecessors. As with any mass-market product, the overriding constraint in a low-end graphics system is cost. Given even modest performance targets, the need to minimize costs, and the consequent need to interface to commodity memory parts, devising an appropriate architecture is something of a trick. ‘Princeton, New Jersey, USA banderson@samoff.com ‘Royston, Cambridgeshire, UK rob@benchees.demon.co.uk %‘owcester, Northamptonshire, UK andys@phoenixvlsi.co.uk 4Chnprl Hill, North Carolina, USA jtw@cs.unc.edu I’r’nllis\ioll~~, 11~1l;r: digitnl~lmrd copies ofnll or pa11 oftlk mntwi~l for pcnollal or r’lan\room ~1: ic grmtcd wilhout I& providsd Ihnt the copis are 1101 lllndc or dislrihtcd Ibr pralil or commtrcinl advantage. (111: cop)‘riglit &cc:. I!T [ilIe ofthe puhlicntion nnd its dnlc appear, alld 110tice is givw tllnl copyright k by ptmkioll OI’IIIZ ACM. 111~ TO copy ohwisr, 10 rcpublisll, IO posl OII smwx or IO redistribute lo lisls. requires specilic peniiiwioii nndlor It2 I99 7 ,SIGGl?wl PWEwogrnphics ~Sorkshop Copyrigllt 1997 AChI O-S379l-C)~I-0~97/S..SB.S0 Rob MacAulay Ben Cheese Electronic Design* Turner Whitted The University of North Carolina4 In this paper we start from a design decision to use high bandwidth, high latency, physically compact, commodity Rambus memory. Given that choice, we demonstrate that a long pixel pipeline makes most effective use of the available memory bandwidth, and we show the results of simulating a specific implementation of the architecture. That such a mechanism is effective is not surprising since several current designs use it. We make no claim of originality; we merely wish to examine the performance of this type of design. In the following sections, we review some of the more common raster&r design approaches, including two new low-cost ones, delve into a more detailed discussion of matching rasterizer processing latency to off-chip memory latency, pursue this design approach with an extremely low-cost rasterizer example, and finish with simulation results which illustrate the effectiveness of the technique. (To be clear, our definition of extremely low cost means a combination of graphics processor and memory costing in the low tens of dollars.) While the performance benefits of matched latencies for best case geometry may be obvious on paper, we have found it useful to simulate the example raster&r to see how badly performance degrades for less than optimal geometry. 2. The Memory Interface Bottleneck DRAM access is a bottleneck in graphics system design. The effects of limited memory bandwidth and ways to overcome it have been discussed previously [IO]. However, the limitations imposed by memory latency have not been widely described. A fundamental limitation of current designs stems from choice of algorithm. Almost all polygon image generators support the zbuffer visibility algorithm. This requires that data representing a single pixel be read from memory, processed, and then conditionally written. Because of latency in memory accesses, data read from the z-buffer is not immediately available to the display processor. Contention for memory further aggravates the latency problem. While the alternative of back-to-front pre-sorting of polygons can avoid the overhead of memory read latency suffered by the zbuffer visibility test, pre-sorting has its own overhead. Furthermore, it is hard to resist employing translucency effects which are enabled by the drawing in back-to-front order. Like the z-buffer algorithm, translucency effects require a read before write and share the latency problem with the z-buffer. For a number of years, the trend in the design of image generators has been to gain greater performance through the use	aggregate data;algorithm;best, worst and average case;bottleneck (engineering);cas latency;central processing unit;computer memory;desktop computer;dynamic random-access memory;electronic design;flight simulator;framebuffer;glossary of computer graphics;graphics pipeline;graphics processing unit;interrupt latency;memory bandwidth;overhead (computing);pixel;rasterisation;simulation;sorting;systems design;test fixture;z-buffering	Bruce Anderson;Rob MacAulay;Andy Stewart;Turner Whitted	1997		10.1145/258694.258725	homogeneous coordinates;rasterisation;interleaved memory;parallel computing;cas latency;computer hardware;computer science;clipping;flat memory model;memory bandwidth;computer graphics (images)	Arch	43.5337936458749	-31.406116078447916	194401
80aa671e0ebdc63b324428977b5427c81877d316	bayesian active edge evaluation on expensive graphs		Robots operate in environments with varying implicit structure. For instance, a helicopter flying over terrain encounters a very different arrangement of obstacles than a robotic arm manipulating objects on a cluttered table top. State-of-the-art motion planning systems do not exploit this structure, thereby expending valuable planning effort searching for implausible solutions. We are interested in planning algorithms that actively infer the underlying structure of the valid configuration space during planning in order to find solutions with minimal effort. Consider the problem of evaluating edges on a graph to quickly discover collision-free paths. Evaluating edges is expensive, both for robots with complex geometries like robot arms, and for robots with limited onboard computation like UAVs. Until now, this challenge has been addressed via laziness i.e. deferring edge evaluation until absolutely necessary, with the hope that edges turn out to be valid. However, all edges are not alike in value - some have a lot of potentially good paths flowing through them, and some others encode the likelihood of neighbouring edges being valid. This leads to our key insight - instead of passive laziness, we can actively choose edges that reduce the uncertainty about the validity of paths. We show that this is equivalent to the Bayesian active learning paradigm of decision region determination (DRD). However, the DRD problem is not only combinatorially hard, but also requires explicit enumeration of all possible worlds. We propose a novel framework that combines two DRD algorithms, DIRECT and BISECT, to overcome both issues. We show that our approach outperforms several state-of-the-art algorithms on a spectrum of planning problems for mobile robots, manipulators and autonomous helicopters.	active learning (machine learning);algorithm;autonomous robot;bisection (software engineering);coat of arms;encode;heuristic (computer science);machine learning;mobile robot;motion planning;possible world;programming paradigm;real-time clock;robotic arm;shortest path problem;unmanned aerial vehicle;world online	Sanjiban Choudhury;Siddhartha S. Srinivasa;Sebastian Scherer	2018		10.24963/ijcai.2018/679	robot;configuration space;discrete mathematics;computation;computer science;active learning;mobile robot;exploit;motion planning;robotic arm	AI	51.77296892304992	-24.99201376968248	194594
eefaf77615fe1b717691b45f33868e3117f236ef	3d video applications and intelligent video surveillance camera and its vlsi design	intelligent video surveillance camera;moving object;intelligent video surveillance;video surveillance;video on the fly 3d video applications intelligent video surveillance camera vlsi design;3d image capturing;3d imaging;video on the fly;very large scale integration;vlsi video cameras video surveillance;vlsi design;stereo video coding;multimedia systems;chip;development tool;video coding;object segmentation;engines;video cameras;video surveillance smart cameras very large scale integration engines hardware rendering computer graphics multimedia systems chip scale packaging video coding object segmentation;smart cameras;surveillance camera 3d image capturing depth image based rendering stereo video coding moving object segmentation;vlsi;on the fly;surveillance camera;moving object segmentation;3d video applications;depth image based rendering;process engineering;rendering computer graphics;3d video;chip scale packaging;hardware	In this demonstration, the core processing engines of two video applications, 3D video and intelligent video surveillance, are demonstrated. The developed algorithms and its VLSI design results are shown with hardware prototypes processing input video on-the-fly. In addition to the processing engine design, the development tools for efficiently designing these chips are also demonstrated.	algorithm;closed-circuit television;programming tool;very-large-scale integration;video	Shao-Yi Chien;Chi-Sheng Shih;Mong-Kai Ku;Chia-Lin Yang;Yao-Wen Chang;Tei-Wei Kuo;Liang-Gee Chen	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284571	computer vision;uncompressed video;computer science;video tracking;multimedia;video processing;very-large-scale integration;computer graphics (images)	Robotics	43.976881736975464	-34.78345469832	195113
55763f6b38485a6e14e76c69691b858fef575998	actor-q based active perception learning system	focusing;signal generators;sensor systems;active vision multilayer perceptrons learning artificial intelligence;neural networks;actor q based active perception learning system;multilayer perceptrons;reinforcement learning;error backpropagation;actor s output signals;backpropagation;actor critic;computer networks;learning systems;learning system;computational modeling;target recognition;retina;pattern recognition;q value surface;q value surface actor q based active perception learning system reinforcement learning actor critic actor s output signals neural network error backpropagation nonuniform visual cells;nonuniform visual cells;learning artificial intelligence;learning systems neural networks sensor systems computer networks signal generators computational modeling pattern recognition retina target recognition focusing;neural network;active vision	An active perception learning system based on reinforcement learning is proposed. A novel reinforcement architecture, called Actor-Q, is employed in which Qlearning and Actor-Critic are combined. The system decides its actions according to Q-values. One of the actions is to move its sensor, and the others are to make an answer of its recognition result, each of which corresponds to each pattern. When the sensor motion is selected, the sensor moves according to the actor's output signals. The Q-value for the sensor motion is trained by Q-learning, and the Actor is trained by the Q-value for the sensor motion on behalf of the critic. When one of the other actions is selected, the system outputs the recognition result. When the recognition answer is correct, the Q-value is trained to be the upper limit of the Q-value, and when the answer is not correct, it is trained to be 0.0. The module to compute Q-value and the actor module are both consisted of a neural network, and are trained by Error Back Propagation. The training signals are generated based on the above reinforcement learning. It was con rmed by some simulations using a visual sensor with non-uniform visual cells that the system moves its sensor to the place where it can recognize the presented pattern correctly. Even though the Q-value surface as a function of the sensor location has some local peaks, the sensor was not trapped and moved to the appropriate direction because the Q-value for the sensor motion becomes larger.	actor model;algorithm;artificial neural network;backpropagation;naruto shippuden: clash of ninja revolution 3;q-learning;reinforcement learning;sensor;simulation;software propagation;value (computer science)	Katsunari Shibata;Tetsuo Nishino;Yoichi Okabe	2001		10.1109/ROBOT.2001.932680	computer vision;active vision;computer science;artificial intelligence;backpropagation;machine learning;computational model;artificial neural network;signal generator	Robotics	45.95062224032014	-31.406002657721707	195337
8736e39abd93f401a7a0915dc69b1322507bc2cb	cognitive multistatic auv networks	acoustic model data fusion anti submarine warfare multi static active sonar bayesian target tracking underwater wireless sensor networks autonomous underwater vehicles cognitive systems;target tracking bayes methods optimization receivers trajectory acoustics;wireless sensor networks acoustic signal detection autonomous underwater vehicles bayes methods cognitive systems intelligent sensors sensor fusion target tracking underwater acoustic communication;underwater wireless sensor networks cognitive multistatic auv networks autonomous underwater vehicle low cost devices cognitive tracking systems cognitive detection systems anti submarine warfare asw collaborative multisensor data fusion smart multisensor data fusion single auv units multistatic configuration acoustic model bayesian model bayesian full posterior	Autonomous underwater vehicle (AUV) platforms are low-cost devices with respect to conventional detection and tracking systems for the purpose of anti-submarine warfare (ASW). Unfortunately, the increased level of manageability is often paid in terms of capabilities, e.g., limited speed and endurance, inferior sensor payloads, and so on. This work exploits two fundamental concepts aimed at filling the consequent performance gap. First, a multistatic network of AUVs is considered, where a smart and collaborative multi-sensor data fusion allows going beyond the individual sensors limitations. Then, we focus on the cognitive paradigm, where the single AUV units optimize their future actions (i.e., their path planning) in view of the final inference purpose of the network, and based on the evidence collected up to the present. A multistatic configuration of the platforms and the corresponding acoustic model are considered, and taken into account in order to derive a proper Bayesian model. Using the information contained in the Bayesian full posterior, cognitive detection and tracking algorithms are designed. Seeing them at work in practical scenarios shows the benefits of the cognitive network paradigm.	acoustic cryptanalysis;acoustic model;algorithm;bayesian network;cognition;cognitive network;computational resource;interdependence;mathematical optimization;motion planning;multi-objective optimization;multilateration;obstacle avoidance;programming paradigm;sensor;tracking system	Paolo Braca;Ryan Goldhahn;Kevin D. LePage;Stefano Maranò;Vincenzo Matta;Peter Willett	2014	17th International Conference on Information Fusion (FUSION)		acoustics;telecommunications;engineering;remote sensing	Robotics	51.686156905087785	-32.074949754613	195555
87b3bd202a8ab9d56d5dfee26bba55364a83f7ba	low-rank forward models: a path to the self-organization of visuo-motor systems	robot sensing systems;biological system modeling;low cost energy efficient autonomous robots low rank forward models visuo motor systems self organization ecological advantages motor command corollary discharge sensorimotor coupling low rank approximation matrix factorization off the shelf solvers magnitude speed up ecologically adapted sensorimotor systems;matrix decomposition;robot sensing systems matrix decomposition couplings approximation methods biological system modeling discharges electric;discharges electric;approximation methods;couplings;robot vision matrix decomposition	Sensorimotor coupling is ubiquitous in living organisms. Sensory and motor systems are utterly useless if left without the presence of the other. One crucial faculty that organisms have developed with tremendous ecological advantages is the ability to discern between the origins of perceptual input as being originated by the environment or the organism itself, provided by resource efficient sensor and motor systems. This ability has been shown to be implemented through a specialized circuit (forward model) receiving a copy of the motor command (corollary discharge). We propose a fast method to derive a resource constrained forward model by framing sensorimotor coupling as a low-rank approximation of an overly detailed forward model. By framing the problem as a factorization approach we can resort to currently available off-the-shelf solvers for matrix factorization. We experimentally show that by solving the problem as a low-rank approximation we obtain more than an order of magnitude speed up relatively to minimizing the objective function with gradient descent methods. The development of resource constrained and ecologically adapted sensorimotor systems is essential for the deployment of low-cost energy efficient autonomous robots for the execution of specific tasks in particular environments.	autonomous robot;discharger;ecology;experiment;framing (world wide web);gradient descent;loss function;low-rank approximation;optimization problem;self-organization;sensor;software deployment	Ângelo Cardoso;Ricardo Ferreira;Ricardo Santos;Alexandre Bernardino	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353571	computer vision;simulation;artificial intelligence;machine learning;control theory;mathematics;coupling;matrix decomposition	Robotics	48.21425760054209	-26.306768265658746	195682
21e3be7427bd6704ad98c1e2b3c47afd9eaaf0b0	autonomous mobile robot localization and navigation using a hierarchical map representation primarily guided by vision		While impressive recent progress has been achieved with autonomous vehicles both indoors and on streets, autonomous localization and navigation in less constrained and more dynamic environments, such as outdoor pedestrian and bicycle-friendly sites, remains a challenging problem. We describe a new approach that utilizes several visual perception modules — place recognition, landmark recognition, and road lane detection — supplemented by proximity cues from a planar Laser Range Finder for obstacle avoidance. At the core of our system is a new hybrid topological/grid-occupancy map which integrates the outputs from all perceptual modules, despite different latencies and timescales. Our approach allows for real-time performance through a combination of fast but shallow processing modules that update the map’s state while slower but more discriminating modules are still computing. We validated our system using a ground vehicle that autonomously traversed several times three outdoor routes, each 400m or longer, in a university campus. The routes featured different road types, environmental hazards, moving pedestrians, and service vehicles. In total, the robot logged over 10km of successful recorded experiments, driving within a median of 1.37m laterally of the center of the road, and localizing within 0.97m (median) longitudinally of its true location along the route.	autonomous car;autonomous robot;experiment;internationalization and localization;mobile robot;obstacle avoidance;real-time clock	Christian Siagian;Chin-Kai Chang;Laurent Itti	2014	J. Field Robotics	10.1002/rob.21505	computer vision;simulation	Robotics	53.27166075266169	-31.30031828048642	196408
ab8e369d14a46d195dc02effa14e397887781fe6	correlated orienteering problem and its application to persistent monitoring tasks	spatial correlation orienteering problem persistent monitoring situation awareness;correlation monitoring robot kinematics planning mobile robots unmanned aerial vehicles;mobile robots;mixed integer quadratic programming correlated orienteering problem spatiotemporal field autonomous mobile robots quadratic cop instantiation qcop quadratic utility function;quadratic programming integer programming mobile robots;monitoring;planning;correlation;unmanned aerial vehicles;robot kinematics	We propose the correlated orienteering problem (COP) as a novel nonlinear extension to the classic orienteering problem (OP). With the introduction of COP, it becomes possible to model the planning of informative tours for the persistent monitoring of a spatiotemporal field with time-invariant spatial correlations using autonomous mobile robots, in which the robots are range- or time-constrained. Our focus in this paper is QCOP, a quadratic COP instantiation that looks at correlations between neighboring nodes in a node network. The main feature of QCOP is a quadratic utility function capturing the said spatial correlation. We solve QCOP using mixed integer quadratic programming, with the resulting anytime algorithm capable of planning multiple disjoint tours that maximize the quadratic utility. In particular, our algorithm can quickly plan a near-optimal tour over a network with up to 150 nodes. Beside performing extensive simulation studies to verify the algorithm's correctness and characterize its performance, we also successfully applied QCOP to two realistic persistent monitoring tasks: 1) estimation over a synthetic spatiotemporal field and 2) estimating the temperature distribution in the state of Massachusetts in the United States.	anytime algorithm;autonomous robot;correctness (computer science);information;mobile robot;nonlinear system;quadratic programming;simulation;synthetic intelligence;time-invariant system;universal instantiation;utility	Jingjin Yu;Mac Schwager;Daniela Rus	2016	IEEE Transactions on Robotics	10.1109/TRO.2016.2593450	planning;mobile robot;mathematical optimization;simulation;computer science;engineering;artificial intelligence;correlation;robot kinematics	AI	53.511941306221196	-26.53885054372429	196822
70c1018ae7d9fb9bad1903d60e5a61f378812b9b	computer vision systems		Although many different vision algorithms and systems have been developed so far, integration into a complex intelligent control architecture of a mobile robot is in most cases an open problem. In this paper we describe the integration of different vision based behaviours into our architecture for sensorimotor systems. By means of different scenarios like person tracking and searching of different objects, the structure of the vision system and the interaction with the overall architecture is explained. Especially the interaction of vision based modules with the task level control and the symbolic world model is an important topic. The architecture is successfully used on different mobile robots in natural indoor environments.	algorithm;intelligent control;mobile robot	Henrik I. Christensen	1999		10.1007/3-540-49256-9	computer vision;machine vision	Robotics	49.49097975981094	-31.31534398196771	196871
00c7eab8bf3211a38635a7c6f923d6736aefd772	fpga-based image processing system for quality control and palletization applications	frequency 50 mhz fpga based image processing system industrial automation problems quality control and palletization applications qcp applications intelligent four bar mechanism mechanical palletizer singular quadrilateral mechanism field programmable gate array real time processing system matlab simulink model serial pixel data generator thresholder binarization simulink system blocks fpga architecture fpga configurable logic blocks hardware description language codes;quality control control engineering computing field programmable gate arrays hardware description languages image processing industrial robots palletising;robotics and automation fpga mechanical palletizer edge detection blob;field programmable gate arrays robots shift registers object recognition algorithm design and analysis software packages computer architecture	This paper proposes a new approach for solving well-known industrial automation problems such as Quality Control and Palletization (QCP). An intelligent four-bar mechanism has been designed as a mechanical palletizer. It has been modelled as a singular quadrilateral mechanism whose intelligence is sourced from an image processing algorithm targeted for Field Programmable Gate Array (FPGA) real-time processing system. In this proposed approach the algorithms are implemented using MATLAB and Simulink packages. The critical system blocks of the Simulink model are the serial pixel data generator and the thresholder whose functions is to compute threshold value of all pixels for binarization. All the Simulink system blocks have been designed based on the proposed FPGA architecture and mapped onto the Configurable Logic Blocks of the FPGA. The hardware description language (HDL) codes generated from the Simulink model show no behavioral deviation from the original MATLAB version of the algorithm. The recognition rate results are high and the whole system is very fast at 50 MHz clock frequency.	algorithm;automation;clock rate;code;critical system;field-programmable gate array;hardware description language;image processing;matlab;pixel;real-time clock;simulink	Abubakar M. Ashir;Atef A. Ata;Mohammad Shukri Salman	2014	2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)	10.1109/ICARSC.2014.6849800	embedded system;electronic engineering;real-time computing;reconfigurable computing;computer science	Robotics	44.38603735495804	-34.35647555910822	197170
b7b98fb817b0e57689dccab715ebdd78d658ff8a	implicit crowds: optimization integrator for robust crowd simulation		Large multi-agent systems such as crowds involve inter-agent interactions that are typically anticipatory in nature, depending strongly on both the positions and the velocities of agents. We show how the nonlinear, anticipatory forces seen in multi-agent systems can be made compatible with recent work on energy-based formulations in physics-based animation, and propose a simple and effective optimization-based integration scheme for implicit integration of such systems. We apply this approach to crowd simulation by using a state-of-the-art model derived from a recent analysis of human crowd data, and adapting it to our framework. Our approach provides, for the first time, guaranteed collision-free motion while simultaneously maintaining high-quality collective behavior in a way that is insensitive to simulation parameters such as time step size and crowd density. These benefits are demonstrated through simulation results on various challenging scenarios and validation against real-world crowd data.	crowd simulation;interaction;mathematical optimization;multi-agent system;nonlinear system	Ioannis Karamouzas;Nick Sohre;Rahul Narain;Stephen J. Guy	2017	ACM Trans. Graph.	10.1145/3072959.3073705	computer vision;integrator;artificial intelligence;simulation;animation;nonlinear system;crowds;crowd simulation;computer science	Graphics	46.883974702459646	-27.978608530325456	197481
da05169fed13f8c19c399ade0aeb105891e7a6ee	obstacle detection from ipm and super-homography	obstacle detection methods;urban environment;obstacle detection;ipm tranformation;super homography;autonomous guided vehicle;path planning;road marker tracking;embedded camera;free space;mobile robots;tracking collision avoidance feature extraction mobile robots road vehicles robot vision;free space estimator;robot vision;feature extraction;stereo vision;mobile robots obstacle detection methods super homography embedded camera road marker tracking free space estimator autonomous guided vehicle path planning ipm tranformation complex urban environments;roads layout cameras mobile robots remotely operated vehicles shape radar detection laser radar vehicle detection optical distortion;collision avoidance;ipm transformation;tracking;complex urban environments;road vehicles	We present in this article a simple method to estimate an IPM view from an embedded camera. The method is based on the tracking of the road markers assuming that the road is locally planar. Our aim is the development of a free-space estimator which can be implemented in an Autonomous Guided Vehicle to allow a safe path planning. Opposite to most of the obstacle detection methods which make assumptions on the shape or height of the obstacles, all the scene elements above the road plane (particularly kerbs and poles) have to be detected as obstacles. Combined with the IPM tranformation, this obstacle detection stage can be viewed as the first stage of a free-space estimator dedicated to AGV in the complex urban environments.	embedded system;motion planning;planar graph;radar chart;regional lockout;stereopsis	Nicolas Simond;Michel Parent	2007	2007 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2007.4399253	control engineering;mobile robot;computer vision;simulation;feature extraction;computer science;stereopsis;artificial intelligence;motion planning;tracking;quantum mechanics	Robotics	52.5548872021278	-36.75022816207482	197494
4bd9c863e86e8c21617560f35148e98423cd9510	a framework for active vision-based robot control using neural networks	neural network;calibration-free spatial representation;camera configuration change;active camera system;active vision;assembly robot;active camera control;active vision-based robot control;robot control loop;robot control;new framework;camera configuration	Assembly robots that use an active camera system for visual feedback can achieve greater flexibility, including the ability to operate in an uncertain and changing environment. Incorporating active vision into a robot control loop involves some inherent difficulties, including calibration, and the need for redefining the servoing goal as the camera configuration changes. In this paper, we propose a novel self-organizing neural network that learns a calibration-free spatial representation of 3D point targets in a manner that is invariant to changing camera configurations. This representation is used to develop a new framework for robot control with active vision. The salient feature of this framework is that it decouples active camera control from robot control. The feasibility of this approach is established with the help of computer simulations and experiments with the University of Illinois Active Vision System (UIAVS).	active vision;artificial neural network;robot control	Rajeev Sharma;Narayan Srinivasa	1998	Robotica		robot;smart camera;computer vision;calibration;simulation;active vision;image processing;computer science;engineering;artificial intelligence;feedback;assembly;robot control;visual servoing	Robotics	49.71545385196963	-30.80508012012843	197517
73b1b19280594d9837dbeda477d7efe94269d7c6	using probabilistic movement primitives for striking movements		Due to its strong requirements in motor abilities, robot table tennis is an important test bed for new robot learning approaches. Learning approaches have to generalize a complex hitting behavior from relatively few demonstrated trajectories, which neither cover all ball trajectories nor all desired hitting directions. Therefore, past approaches that only modeled a deterministic mean behavior without capturing the variability of the movement have been fairly limited. Recent work on capturing trajectory distributions using probabilistic movement representations opens important new possibilities for robot table tennis. In this paper, we present two new methods to adapt probabilistic movement primitives. First we present a method to adapt a probability distribution of hitting movements learned in joint space to have a desired end effector position, velocity and orientation. Subsequently, we present a method to find the initial time and duration of the movement primitive in order to intercept a moving object like the table tennis ball. The resulting methods rely on simple operations from probability theory. Providing a more principled approach to solve some of the challenges of robot table tennis compared to previous approaches. Additionally, the presented method has the potential of generalizing to many other motor tasks.	heart rate variability;requirement;robot end effector;robot learning;testbed;velocity (software development);visual intercept	Sebastián Gómez-González;Gerhard Neumann;Bernhard Schölkopf;Jan Peters	2016	2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids)	10.1109/HUMANOIDS.2016.7803322	computer vision;simulation;computer science;artificial intelligence	Robotics	49.799206264496426	-26.997750646464823	197542
157fed33fa0cc2a2711b0c54258e4b75849f03ce	canopy closure estimates with greenorbs: sustainable sensing in the forest	canopy closure;deployment;early experience;ecosystem management;real time;data collection;wireless sensor network;dynamic environment;design;software design	Motivated by the needs of precise forest inventory and real-time surveillance for ecosystem management, in this paper we present GreenOrbs [2], a wireless sensor network system and its application for canopy closure estimates. Both the hardware and software designs of GreenOrbs are tailored for sensing in wild environments without human supervision, including a firm weatherproof enclosure of sensor motes and a light-weight mechanism for node state monitoring and data collection. By incorporating a pre-deployment training process as well as a distributed calibration method, the estimates of canopy closure stay accurate and consistent against uncertain sensory data and dynamic environments. We have implemented a prototype system of GreenOrbs and carried out multiple rounds of deployments. The evaluation results demonstrate that GreenOrbs outperforms the conventional approaches for canopy closure estimates. Some early experiences are reported in this paper.	motorola canopy;prototype;real-time clock;software deployment;the forest	Lufeng Mo;Yuan He;Yunhao Liu;Jizhong Zhao;Shaojie Tang;Xiang-Yang Li;Guojun Dai	2009		10.1145/1644038.1644049	embedded system;ecosystem management;design;simulation;wireless sensor network;computer science;software design;data collection	Mobile	51.88819296312429	-32.05029087482052	198313
7d9bdef9c76b2a3168c97ac437a69ecf80cf792d	overlapping motion hints with polynomial motion for video communication	teleconferencing;video surveillance;motion compensation teleconferencing video surveillance video signal processing image communication;motion compensation;video signal processing;image communication;video sequences overlapping motion polynomial motion video communication region of applicability roa motion hints closed loop prediction open loop prediction probabilistic multiscale framework parallel processing;merging polynomials motion compensation servers noise transform coding conferences;video signal processing image motion analysis image sequences polynomials	In a recent work, we propose the use of motion hints for communicating motion. A motion hint describes motion that is accurate (describes the actual motion) for only a region inside a domain associated with that motion hint; it is the job of the client or decoder to decide the exact region of applicability (ROA). The motion described by a motion hint is invertible and global; that is, it allows the prediction of the ROA associated with a motion hint from any frame that has that hint. Motion hints are applicable to closed-loop prediction, but they are more useful in open-loop prediction scenarios, such as remote browsing of surveillance footage, communicated by a JPIP server, which is the focus of this work. This work proposes a probabilistic multi-scale framework to identifying the ROA; the framework is applicable to multiple overlapping motion hints. The proposed approach is localized (and therefore amenable to parallel processing) and robust to noise, quantization, and changes in contrast. We show that motion hints can be used for real video sequences, and we also present results for the case of three overlapping motion hints (one background and two overlapping foregrounds).	closed-circuit television;jpip;motion compensation;parallel computing;polynomial;quantization (signal processing);resource-oriented architecture;server (computing)	Aous Thabit Naman;David S. Taubman;Rui Xu	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7026208	computer vision;match moving;structure from motion;simulation;teleconference;quarter-pixel motion;computer science;motion interpolation;motion estimation;block-matching algorithm;motion field;motion compensation;algorithm;computer graphics (images)	Robotics	40.80995992353844	-35.77209422975024	198372
7f6c70516cf9bcd4c1477ac3c89615a4b3bb3451	cooperative search by uav teams: a model predictive approach using dynamic graphs	graph theory;detectors;predictive control;travel time;probability density function;unmanned aerial vehicle;search algorithm;prediction algorithms;mobile robots;remotely operated vehicles;receding horizon;graphs;autonomous agent;aerospace control;remotely piloted vehicles;dynamics;optimal routing;heuristic algorithms;probability distribution;gimbaled video cameras receding horizon cooperative search algorithm uav teams unmanned aerial vehicles model predictive approach dynamic graphs route optimization sensor orientations autonomous agents target probability continuous search problem graph vertices nonuniform graphs;sensor orientation;algorithms;heuristic algorithms prediction algorithms unmanned aerial vehicles probability distribution predictive models probability density function;predictive models;search problems;prediction model;unmanned aerial vehicles;heuristic algorithm;search problems aerospace control graph theory mobile robots predictive control remotely operated vehicles	A receding-horizon cooperative search algorithm is presented that jointly optimizes routes and sensor orientations for a team of autonomous agents searching for a mobile target in a closed and bounded region. By sampling this region at locations with high target probability at each time step, we reduce the continuous search problem to a sequence of optimizations on a finite, dynamically updated graph whose vertices represent waypoints for the searchers and whose edges indicate potential connections between the waypoints. Paths are computed on this graph using a receding-horizon approach, in which the horizon is a fixed number of graph vertices. To facilitate a fair comparison between paths of varying length on nonuniform graphs, the optimization criterion measures the probability of finding the target per unit travel time. Using this algorithm, we show that the team discovers the target in finite time with probability one. Simulations verify that this algorithm makes effective use of agents and outperforms previously proposed search algorithms. We have successfully hardware tested this algorithm in two small unmanned aerial vehicles (UAVs) with gimbaled video cameras.	aerial photography;autonomous robot;byte;centralized computing;computer simulation;line-of-sight (missile);low-power broadcasting;mathematical optimization;requirement;routing;sampling (signal processing);search algorithm;search problem;sensor;unmanned aerial vehicle;upload;waypoint	James R. Riehl;Gaemus E. Collins;João Pedro Hespanha	2011	IEEE Transactions on Aerospace and Electronic Systems	10.1109/TAES.2011.6034656	mathematical optimization;simulation;computer science;engineering;graph theory;machine learning;mathematics;predictive modelling;statistics	Robotics	53.681853414306	-25.941204243368848	198533
206ebf41083c7a872b73a3c4a7f5e122549bf4b9	obstacle avoidance through reinforcement learning	reinforcement learning;obstacle avoidance	A method is described for generating plan-like. reflexive. obstacle avoidance behaviour in a mobile robot. The experiments reported here use a simulated vehicle with a primitive range sensor. Avoidance behaviour is encoded as a set of continuous functions of the perceptual input space. These functions are stored using CMACs and trained by a variant of Barto and Sutton's adaptive critic algorithm. As the vehicle explores its surroundings it adapts its responses to sensory stimuli so as to minimise the negative reinforcement arising from collisions. Strategies for local navigation are therefore acquired in an explicitly goal-driven fashion. The resulting trajectories form elegant collisionfree paths through the environment	algorithm;andrew barto;aqua;experiment;mobile robot;obstacle avoidance;reinforcement learning	Tony J. Prescott;John E. W. Mayhew	1991			simulation;computer science;artificial intelligence;obstacle avoidance;reinforcement learning	Robotics	50.446477083123305	-29.08499068055807	198654
06d37532fdb24cfe9ddb605c3d740fdb47a0b44a	the application of force sensing to skills assessment in minimally invasive surgery	force sensors;force force measurement instruments time measurement correlation surgery;medical computing;surgery biomedical education computer based training force measurement force sensors medical computing position measurement professional aspects;professional aspects;computer based training;position measurement;surgery;force measurement;biomedical education;force based metrics force sensing skills assessment minimally invasive surgery reduced access conditions interaction forces mis sensorized instruments instrument position measurement tissue interaction force measurement	The reduced access conditions present in Minimally Invasive Surgery (MIS) affect the feel of interaction forces between the instruments and the tissue being treated. This loss of haptic information compromises the safety of the procedure and must be overcome through training. Determining the skill level of trainees is critical for ensuring patient safety. The objective of this work was to evaluate the usefulness of force information for skills assessment during MIS. Experiments were performed using a set of sensorized instruments capable of measuring instrument position and tissue interaction forces. The results show that experience level has a strong correlation with force-based metrics. The proposed metrics can be automatically computed, are completely objective, and measure important aspects of performance.	computer performance;experiment;haptic technology;interpreter (computing);minimally invasive education;simulation	Ana Luisa Trejos;Rajnikant V. Patel;Michael D. Naish;Richard Malthaner;Christopher Schlachta	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6631196	simulation;engineering;biological engineering	Robotics	39.92287242612111	-37.317922437403894	198887
0c452561915100b954f64ec7f0d6009892900f2a	cognitive personal positioning based on activity map and adaptive particle filter	adaptive particle filter;possibility measure;activity map;computational complexity;particle filter;mobility tracking;bayesian filtering;wearable computer;adaptive filter;personal positioning	This paper presents a cognitive approach for a reliable yet battery-friendly personal positioning. A user's position is learned from both historical log and possible measurements. Firstly, user's past activities recorded in the log are summarized into an activity map. Accordingly, a user-habit guided particle filtering algorithm is presented for position prediction. Specifically, our algorithm makes reference to the map to determine the most probable correct position, smoothed with occasional measurement. User's current position is modeled probabilistically by a collection of particles and her future moves are modeled with a tendency to follow a familiar path on the map; The estimate is then smoothed by Bayesian filtering. We also allow the number of particles to vary according to user's position in the map. Thus, along with better insights about user's movement experience, our approach can learn from the past and potentially improve the quality of estimates. Our experiments show that this adaptive filtering model using the activity map can deal with non-linear behaviors rather effectively. The new cognitive scheme can indeed track the user's position with a high degree of accuracy. Moreover, the algorithms exhibit low computational complexities, making them well suited for applications on wearable computers.	adaptive filter;algorithm;analysis of algorithms;computation;experiment;location-based service;map;nonlinear system;particle filter;sampling (signal processing);smoothing;wearable computer	Hui Fang;Wen-Jing Hsu;Larry Rudolph	2009		10.1145/1641804.1641873	adaptive filter;computer vision;simulation;particle filter;wearable computer;computer science;machine learning;computational complexity theory	HCI	48.20067040241114	-28.618626543661275	199286
dd92a3aea5de1c573e28968c63a93aff3d688eab	enabling robots to find and fetch objects by querying the web	web;robotics;cyber physical systems;object search	This paper describes an algorithm that enables a mobile robot to find an arbitrary object and take it to a destination location. Previous approaches have been able to search for a fixed set of objects. In contrast, our approach is able to dynamically construct a cost function to find any object by querying the web. The performance of our approach has been evaluated in a realistic simulator, and has been demonstrated on a companion robot, which can successfully execute plans such as finding a“coffee”and taking it to a destination location like, “Gates-Hillman Center, Room 7002.”	algorithm;loss function;mobile robot;simulation	Thomas Kollar;Mehdi Samadi;Manuela M. Veloso	2012			computer vision;simulation;computer science;artificial intelligence;robotics;cyber-physical system;world wide web	Robotics	51.674879849138435	-27.997506296230924	199603
3f9de12d3c054d57ba704a86d9fc897076f858bb	caddy underwater stereo-vision dataset for human-robot interaction (hri) in the context of diver activities		In this article we present a novel underwater dataset collected from several field trials within the EU FP7 project “Cognitive autonomous diving buddy (CADDY)”, where an Autonomous Underwater Vehicle (AUV) was used to interact with divers and monitor their activities. To our knowledge, this is one of the first efforts to collect a large dataset in underwater environments targeting object classification, segmentation and human pose estimation tasks. The first part of the dataset contains stereo camera recordings (≈ 10K) of divers performing hand gestures to communicate and interact with an AUV in different environmental conditions. These gestures samples serve to test the robustness of object detection and classification algorithms against underwater image distortions i.e., color attenuation and light backscatter. The second part includes stereo footage (≈ 12.7K) of divers free-swimming in front of the AUV, along with synchronized IMUs measurements located throughout the diver’s suit (DiverNet) which serve as ground-truth for human pose and tracking methods. In both cases, these rectified images allow investigation of 3D representation and reasoning pipelines from low-texture targets commonly present in underwater scenarios. In this paper we describe our recording platform, sensor calibration procedure plus the data format and the utilities provided to use the dataset.		Arturo Gomez Chavez;Andrea Ranieri;Davide Chiarella;Enrica Zereik;Anja Babic;Andreas Birk	2018	CoRR		climatology;computer vision;inertial measurement unit;geology;statistical classification;underwater;pose;stereo camera;human–robot interaction;stereopsis;artificial intelligence;gesture	Vision	48.579448805826175	-34.91666388512038	199845
9903fc0ec813d89d7b9803ad6522099f2251709e	a two-neuron cross-correlation circuit with a wide and continuous range of time delay	detectors;coincidence detectors;cross correlation;enhanced motion detectors;delay effects;circuits delay effects neurons detectors motion detection autocorrelation systems engineering and theory fires acoustic noise noise level;mathematical analysis;a priori fixed delay;detector circuits;time delay;systems engineering and theory;two neuron cross correlation circuit;spiking neurons;noise level;a priori fixed delay two neuron cross correlation circuit time delay spiking neurons signal cross correlations coincidence detectors enhanced motion detectors;acoustic noise;delay circuits;mathematical analysis delay circuits detector circuits;circuits;neurons;signal cross correlations;fires;motion detection;autocorrelation	We describe a circuit of two spiking neurons which extracts mathematically accurate cross-correlations from the signal inputs. It differs from prior circuits such as coincidence detectors or enhanced motion detectors in that it does not require an a priori fixed delay between input signals to be selected. The output, in the form of a differential spike histogram, displays a mathematical cross-correlation in the conventional correlation vs. time form.	accumulator (computing);broadcast delay;coincidence rangefinder;cross-correlation;motion detector;neuron;sensor;spiking neural network	Jonathan Tapson;Mark P. Vismer;Craig T. Jin;André van Schaik;Fopefolu O. Folowosele;Ralph Etienne-Cummings	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541444	electronic circuit;detector;electronic engineering;autocorrelation;electrical engineering;cross-correlation;noise;control theory;mathematics;statistics	Arch	45.40391934249416	-32.285829301313996	199967
