id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
9063c68bd8c01fc0f09c7235f53ed9527f2ba4dc	high-speed inter-view frame mode decision procedure for multi-view video coding	mvc;mb energy;candidate modes;hbp;rdcost;inter view;jmvm;mode similarity;md;hsmd	With the amazing development of visual identification systems, multi-view video which is one of the main types of three-dimensional (3D) video signals, captured by a set of video cameras from various viewpoints, has attracted much interest recently. The multi-view video coding (MVC) uses the joint multi-view video model (JMVM) established on H.264/AVC as the core codec to achieve excellent coding efficiency. However, the concomitant problem is the huge encoding complexity especially due to the heavy Rate-to-Distortion Cost (RDCost) computational load of the mode decision (MD) process, which limits the applications of MVC for mobile terminals and embedded systems. In this paper, a high-speed MD (HSMD) algorithm is proposed for the inter-view frames of multi-view video signal. The redundant candidate modes of each macroblock (MB) in the inter-view frames are eliminated to reduce the original RDCost computational load by utilizing multiple efficiency parameters. The quantitative analysis verifies our proposal in theory and the simulation results show that the proposed HSMD algorithm can reduce the encoding time by over 90% for the inter-view video frames with negligible quality loss compared with the original JMVM codec. The proposed algorithm can be widely employed for real-time multi-view signal encoding especially under the mobile and embedded environments to realize the real-time encoding process.	data compression;decision problem	Xingang Liu;Laurence Tianruo Yang;Kwanghoon Sohn	2012	Future Generation Comp. Syst.	10.1016/j.future.2011.05.013	real-time computing;simulation;computer science;multimedia;programming language;model–view–controller	Arch	47.070701790586035	-19.332205191994824	54106
39d9395b263da8495f6a5a9fd247aeb43c915299	encoder-unconstrained user interactive partial decoding scheme	partial decoding;object of interest;h.264/avc		encoder	Chen Liu;Tianruo Zhang;Satoshi Goto	2012	IEICE Transactions		theoretical computer science;multimedia	Crypto	43.92176734746724	-21.52956439495059	54234
4335161b4cc392e85e04f5a32bd80774f0aa2801	robust 3d lut estimation method for shvc color gamut scalability	color table lookup three dimensional displays estimation image color analysis interpolation scalability;video coding image colour analysis image enhancement least squares approximations table lookup;scalable video coding;color gamut scalability;3d lut;color gamut;inter layer prediction;uneven sample distribution robust 3d lut estimation method shvc color gamut scalability high efficiency video coding scalable coding layer conveying hdtv video bt 709 color space enhancement layer 3d look up table color conversion process 3d lut parameter estimation method parameter estiamation least square method matrix sparsity;3d lut scalable video coding color gamut scalability inter layer prediction color gamut	Color gamut scalability (CGS) in scalable extensions of High Efficiency Video Coding (SHVC) supports scalable coding with multiple layers in different color spaces. Base layer conveying HDTV video in BT.709 color space and enhancement layer conveying UHDTV video in BT.2020 color space is identified as a practical use case for CGS. Efficient CGS coding can be achieved using a 3D Look-up Table (LUT) based color conversion process. This paper proposes a robust 3D LUT parameter estimation method that estimates the 3D LUT parameters globally using the Least Square method. Problems of matrix sparsity and uneven sample distribution are carefully handled to improve the stability and accuracy of the estimation process. Simulation results confirm that the proposed 3D LUT estimation method can significantly improve coding performance compared with other gamut conversion methods.	3d lookup table;3d modeling;color space;estimation theory;high efficiency video coding;scalability;simulation;sparse matrix	Yuwen He;Yan Ye;Jie Dong	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051510	scalable video coding;computer vision;3d lookup table;color depth;telecommunications;computer science;theoretical computer science;high color;computer graphics (images)	Robotics	44.31892176948927	-18.901040911656533	54298
66b3ad154c0bb3163eb276fa78fd4776b8328552	cross-view down/up-sampling method for multiview depth video coding	image sampling;interpolation;image coding;iterative decoding;image resolution;decoding;multiview depth video;three dimensional;video coding;view synthesis cross view down up sampling multiview depth video;view synthesis;cross view;three dimensional displays;down up sampling;feature extraction;image reconstruction;signal processing;期刊论文;sampling methods;signal processing algorithms;signal processing algorithms image reconstruction image coding image resolution three dimensional displays interpolation decoding;view synthesis crossview down up sampling method multiview depth video coding decoder cdu odd even interlaced extraction crossview information depth video reconstruction iterative interpolation process;video coding feature extraction image reconstruction image sampling interpolation iterative decoding	In this letter, we propose a cross-view down/up-sampling (CDU) method for the framework of reduced resolution multiview depth video coding, which exploits cross-view information to assist the up-sampling at the decoder. In the down-sampling procedure of CDU, the odd-even interlaced extraction is employed to preserve more confident information of the original depth video with reduced resolution. In the decoder, the cross-view information is exploited for up-sampling the reconstructed depth video. An iterative interpolation process is proposed to eliminate the effect of compression distortion on this up-sampling. Experimental results demonstrate the gains of up to 3.88 dB for the proposed algorithm and better quality of synthesized views.	algorithm;control display unit;data compression;distortion;encoder;image resolution;interlaced video;interpolation;iteration;iterative method;sampling (signal processing)	Qiong Liu;You Yang;Rongrong Ji;Yue Gao;Li Yu	2012	IEEE Signal Processing Letters	10.1109/LSP.2012.2190060	iterative reconstruction;three-dimensional space;sampling;computer vision;image resolution;feature extraction;interpolation;computer science;theoretical computer science;signal processing;mathematics;statistics;multiview video coding;computer graphics (images)	Vision	44.67224059574827	-17.558513126785943	54316
27f95150fd91c7baa829561950e8bc4f3c239770	parallelization of vq codebook generation by two algorithms: parallel lbg and aggressive pnn [image compression applications]	algorithm elapsed time vq codebook generation parallelization parallel lbg aggressive pnn image compression vq compression k mean method pairwise nearest neighbor algorithm pc cluster system compressed image quality training vector number;image coding;parallel algorithm;k means;concurrent computing clustering algorithms costs image coding parallel algorithms nearest neighbor searches velocity measurement local area networks linux degradation;image compression;pc cluster;pairwise nearest neighbor;vector quantisation;parallel processing;parallel processing image coding vector quantisation	Summary form only given. We evaluate two parallel algorithms for the codebook generation of the VQ compression: parallel LBG and aggressive PNN. Parallel LBG is based on the LBG algorithm with the K-mean method. The cost of both latter algorithms mainly consists of: a) the computation part; b) the communication part; and c) the update part. Aggressive PNN is a parallelized version of the PNN (pairwise nearest neighbor) algorithm, whose cost mainly consists of: a) the computation part; b) the communication part; and c) the merge part. We measured the speedups and elapsed times of both algorithms on a PC cluster system. When the quality of images compressed by both algorithms is the same, the number of training vectors required by the aggressive PNN is much less than that by the parallel LBG, and the aggressive PNN is superior in terms of the elapsed time.	automatic parallelization;codebook;computation;emoticon;image compression;k-nearest neighbors algorithm;linde–buzo–gray algorithm;location-based game;parallel algorithm;parallel computing;vector quantization	Akiyoshi Wakatani	2005	Data Compression Conference	10.1109/DCC.2005.69	parallel processing;image compression;computer science;theoretical computer science;machine learning;pattern recognition;parallel algorithm;k-means clustering	HPC	43.80412831221794	-12.74687251908584	54617
468bbb2f5e8398ab743dd2832961c1550ba26b16	provably correct continuous control for high-level robot behaviors with actions of arbitrary execution durations	formal specification;robots control system synthesis formal specification;continuous low level controllers correct continuous control high level robot behaviors discrete abstraction controller synthesis framework continuous behaviors robot actions;control system synthesis;robots;cameras robot vision systems automata safety turning	Formal methods have recently been successfully applied to construct verifiable high-level robot control. Most approaches use a discrete abstraction of the underlying continuous domain, and make simplifying assumptions about the physical execution of actions given a discrete implementation. Relaxing these assumptions unearths a number of challenges in the continuous implementation of automatically-synthesized hybrid controllers. This paper describes a controller-synthesis framework that ensures correct continuous behaviors by explicitly modeling the activation and completion of continuous low-level controllers. The synthesized controllers exhibit desired properties like immediate reactiveness to sensor events and guaranteed safety of physical executions. The approach extends to any number of robot actions with arbitrary relative timings.	algorithm;correctness (computer science);formal methods;formal verification;game controller;high- and low-level;overhead (computing);programming paradigm;responsiveness;robot control;speech synthesis	Vasumathi Raman;Nir Piterman;Hadas Kress-Gazit	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6631152	robot;control engineering;real-time computing;simulation;computer science;artificial intelligence;formal specification;control theory	Robotics	52.85038331570761	-21.283025858621194	55198
c63fc210d11aad54a6548b3807c0299d2dcdfc74	sub-band coding of images with quadtree-guided recursive polynomial decomposition	quadtree guided recursive polynomial decomposition;image coding;image segmentation;subband coder;tree data structures entropy image coding image segmentation polynomials recursive functions;transform coding;tree data structures;polynomials;recursive polynomial transform coder;entropy constrained scalar quantizers image segmentation quadtree guided recursive polynomial decomposition recursive polynomial transform coder subband coder orthonormal basis set;scalar quantization;thesis;discrete transforms;discrete cosine transforms;image coding polynomials discrete transforms discrete cosine transforms image segmentation entropy signal to noise ratio transform coding frequency merging;peak signal to noise ratio;merging;recursive functions;major electrical engineering;entropy;electrical engineering;signal to noise ratio;frequency;entropy constrained scalar quantizers;orthonormal basis set	The authors develop a recursive polynomial transform coder and incorporate it in a 16-band subband coder (SBC). The polynomial coder segments an image into subblocks of different sizes and approximates the pixel intensities in a block with a 2-D polynomial. An orthonormal basis set that spans the space of sampled polynomials is determined, and the coefficients of these basis polynomials are then quantized using entropy-constrained scalar quantizers. The lowest band of the SBC is encoded using the polynomial coder, and the upper bands are encoded using uniform threshold entropy-constrained quantizers. The system is then simulated in software and is shown to have a gain of 1.3 dB in peak-to-peak signal-to-noise ratio (PSNR) at 0.25 bits/pixel over P. Strobach's recursive plane decomposition coder (IEEE Trans. vol.SP-39, p.1380-97, June 1991). >		Emil Ramirez;Vinay Vaishampayan	1993		10.1109/ICASSP.1993.319877	computer vision;entropy;mathematical optimization;dictionary coder;discrete mathematics;transform coding;peak signal-to-noise ratio;computer science;electrical engineering;theoretical computer science;frequency;mathematics;image segmentation;tree;signal-to-noise ratio;polynomial	Vision	46.82448582821891	-11.867535957148977	55343
0d98e93a976fe51afca7b62a1a2cc013544c536b	minimally non-linear integer wavelets for image coding	image coding;data compression;image coding wavelet transforms transform coding data compression;integer wavelet transform;lossless image compression;lifting scheme;transform coding;wavelet transforms;lossy image compression minimally nonlinear integer wavelets image coding high performance factorizations wavelet filters lifting scheme framework integer wavelet transform factorization iterated graphic function lossless image compression;image coding discrete wavelet transforms filters wavelet transforms wavelet packets performance loss psnr electronic mail graphics availability;high performance	In this paper we deal with the problem of finding high performance factorizations of wavelet filters to be employed within the lifting scheme framework to yield the integer wavelet transform (IWT). A method is proposed, based on the search for the factorization yielding the minimally non-linear iterated graphic function. Results are reported, referring to a set of popular wavelet filters, which show that the obtained implementations lead to IWTs achieving very satisfactory results for both lossy and lossless image compression.	nonlinear system;wavelet	Marco Grangetto;Enrico Magli;Gabriella Olmo	2000		10.1109/ICASSP.2000.859234	data compression;wavelet;computer vision;discrete mathematics;transform coding;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;image compression;theoretical computer science;cascade algorithm;mathematics;lossless compression;wavelet packet decomposition;stationary wavelet transform;context-adaptive binary arithmetic coding;discrete wavelet transform;lifting scheme;gabor wavelet;statistics;wavelet transform	Vision	43.045404781846514	-15.5392539961613	55499
883893cfd955039282a05b9b339bffda7205f17c	complexity reduction algorithm for region-of-interest based h.264 encoding	unequal coding efficiency degradation scheme;degradation;complexity theory;roi boundary enhancement;detection algorithms;bit rate;visual quality;roi boundary enhancement region of interest h 264 encoding computation complexity reduction algorithm higher visual quality inter mode selection unequal coding efficiency degradation scheme;computation complexity reduction algorithm;higher visual quality;h 264 complexity reduction region of interest;video coding;h 264 encoding;complexity reduction;computational complexity;region of interest;detection algorithm;encoding complexity theory degradation bit rate algorithm design and analysis detection algorithms face;h 264;video coding computational complexity;face;bit allocation;encoding;inter mode selection;algorithm design and analysis	This paper proposes a computation complexity reduction algorithm for region-of-interest (ROI) based H.264 encoding. Smaller partition is preferred in both higher visual quality and ROI situation so that inter mode selection is applied according to the ROI based quality difference. An unequal coding efficiency degradation scheme is proposed according to the unequal bits allocation in ROI based video coding. Our ROI detection algorithm can detect accurate ROI boundary and an ROI boundary enhancement scheme is applied to further reduce the computation complexity in ROI. Experimental results show that 76.09% simulation time can be saved. Bit rate increases for only 1.07% while PSNR decreases for 0.05dB.	algorithm;algorithmic efficiency;computation;data compression;elegant degradation;h.264/mpeg-4 avc;inter-process communication;peak signal-to-noise ratio;reduction (complexity);region of interest;simulation	Tianruo Zhang;Minghui Wang;Chen Liu;Satoshi Goto	2010	2010 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2010.5774979	face;algorithm design;computer vision;degradation;computer science;theoretical computer science;machine learning;computational complexity theory;algorithm;reduction;encoding;region of interest	EDA	46.56721331377775	-18.51574569636814	55709
4ab06db3655b98434549eaa309b24abdc4c064cd	lossless compression of cfa-sampled images using ydgcocg transforms with cdf wavelets		This paper discusses reversible color transforms, in which a raw camera image captured using a color filter array (CFA) is from red, green, and blue (RGB) color space to YDgCoCg color space for lossless compression. We found that conventional reversible color transforms (YDgCoCg transforms) are composed of three Haar wavelets, which are simple type of wavelet. Since the finding means that the YDgCoCg transforms on the basis of other wavelets that can more accurately predict pixels of interest in the predict steps have the potential to generate more sparse signals, we replaced Haar wavelets in the YDgCoCg transforms with Cohen-Daubechies-Feauveau (CDF) 5/3 and 9/7 wavelets, which were customized on the basis of the original pixel positions in two-dimensional (2D) space. The experimental results show that the extended YDgCoCg (YDgCoCg-X) transforms, which achieve higher sparsity, outperformed the conventional transforms when they are applied to the color transform parts in the JPEG 2000 and JPEG extended range (XR) lossless modes.		Taizo Suzuki	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451678	wavelet transform;computer vision;wavelet;rgb color model;transform coding;color filter array;computer science;artificial intelligence;pattern recognition;jpeg 2000;jpeg;color space	Robotics	43.15020493224431	-15.57747095721707	55784
192d61c45a4b8f1ee211eafc3a070526bd5d091e	concealability-rate-distortion tradeoff in image compression anti-forensics	image coding transform coding forensics q factor forgery discrete cosine transforms rate distortion theory;image coding;data compression;image forensics;multimedia systems;rate distortion theory;jpeg fingerprints image compression antiforensics digital multimedia content forgeries detection antiforensic operations fingerprints editing data rate data distortion concealability rate distortion surface c r d surface double jpeg compression antiforensics flexible antiforensic;rate distortion theory data compression fingerprint identification image coding image forensics multimedia systems;rate and distortion digital forensics anti forensics jpeg compression concealability;fingerprint identification	Due to the ease with which digital multimedia content can be modified, a number of techniques to forensically detect forgeries have been developed. Meanwhile, anti-forensic operations have been developed to defeat forensic techniques. When anti-forensics is applied, a forger must balance between the amount that editing fingerprints have been concealed and the distortion introduced to the content. Additionally, the forger may compress the forgery for storage or transmission, which introduces a tradeoff between data rate and distortion. In this paper, we define a measure of an anti-forensic technique's effectiveness which we call concealability and examine the tradeoff between concealability, rate, and distortion. We then characterize the concealability-rate-distortion (C-R-D) surface for double JPEG compression anti-forensics. To do this, we propose a new technique known as flexible anti-forensic dither to hide double JPEG fingerprints. From our experiments, we identify two surprising results related to the C-R-D surface.	anti-computer forensics;distortion;dither;experiment;fingerprint;image compression;jpeg;uncompressed video	Xiaoyu Chu;Matthew C. Stamm;Yan Chen;K. J. Ray Liu	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638221	data compression;fingerprint;computer vision;rate–distortion theory;computer science;theoretical computer science;mathematics;multimedia;statistics	Robotics	40.87401012811923	-11.706941474007971	55908
c58cf474124c7276ff73351a8b48233ca6855ad2	a unified approach to restoration, deinterlacing and superresolution of mpeg-2 decoded video	4 to 8 mbit s mpeg 2 decoded video spatial resolution video quality superresolution picture chrominance upsampling restoration deinterlacing hdtv motion compensation mean squared error ringing artifacts mosquito noise suppression visual quality;decoding spatial resolution hdtv motion compensation bit rate streaming media image restoration motion estimation tv quantization;image resolution;motion compensation;decoding;interference suppression video coding decoding high definition television image resolution image restoration motion compensation;image restoration;visual quality;interference suppression;video coding;mean square error;spatial interpolation;high definition television;spatial resolution	The quality and the spatial resolution of video can be improved by combining multiple pictures to form a single superresolution picture. We address the special problems associated with pictures of variable but somehow parameterized quality such as MPEG-decoded video. Our algorithm provides a unified approach to restoration, chrominance upsampling, deinterlacing and superresolution as e.g. HDTV. The algorithm is mainly targeted at improving MPEG-2 decoding at high bit rates (4-8 Mbit/s). The mean squared error is reduced, compared to the directly decoded sequence, and annoying ringing artifacts including mosquito noise are effectively suppressed. The superresolution pictures obtained by the algorithm are of much higher visual quality and has lower mean squared error than superresolution pictures obtained by simple spatial interpolation.	circuit restoration;deinterlacing;mpeg-2;super-resolution imaging	Bo Martins;Søren Forchhammer	2000		10.1109/ICIP.2000.901132	computer vision;image resolution;computer science;computer graphics (images)	Vision	44.50787720787961	-17.753482525962614	55943
d72dc265fac55ec996e2c841612f6b554270ba79	novel error concealment method with adaptive prediction to the abrupt and gradual scene changes	interpolation;spatial dependence;error concealment;detection algorithms;layout video compression decoding change detection algorithms detection algorithms interpolation extrapolation transform coding wireless communication b isdn;decoding;video signal processing;bit errors;gradual scene changes;adaptive prediction;video compression;extrapolation;low complexity;low complexity scene change detection algorithm;layout;transform coding;wireless communication;image sequences video signal processing interpolation extrapolation;linear property error concealment method adaptive prediction abrupt scene changes gradual scene changes low complexity scene change detection algorithm macroblock type information corrupt blocks bit errors extrapolation interpolation gradual scene change sequence;linear property;corrupt blocks;error concealment method;b isdn;scene change detection;macroblock type information;abrupt scene changes;gradual scene change sequence;change detection algorithms;image sequences	In this paper, the impact of scene change on the conventional error concealment method is addressed and a novel error concealment method is proposed to improve the insufficiency of the conventional temporal error concealment algorithm due to the occurrence of scene change. Combining with the low complexity scene change detection algorithm using macroblock type information, the corrupt blocks resulting from bit errors are concealed either temporally or spatially depending on whether or not an abrupt scene change is found. In the case of gradual scene change, a novel error concealment method of interpolation and extrapolation is proposed to utilize the linear property of gradual scene change sequence, and effectively reduce the concealment error in comparison with the conventional algorithm. A great improvement of about 3 to 5 dB PSNR on average and 6 to 8 dB in some cases is obtained with very little memory and computation overhead.	algorithm;computation;convergence insufficiency;dspace;decibel;error concealment;extrapolation;group of pictures;interpolation;macroblock;overhead (computing);peak signal-to-noise ratio;propagation of uncertainty;simulation;software propagation	Soo-Chang Pei;Yu-Zuong Chou	2002	IEEE Transactions on Multimedia	10.1109/ICPR.2002.1048150	data compression;layout;computer vision;transform coding;speech recognition;spatial dependence;interpolation;computer science;theoretical computer science;extrapolation;wireless;statistics	Visualization	47.49783800786448	-16.641314125157432	56055
b1ad84c9292f1067d58c6b27d77028cea872a0d6	power-scalable multi-layer halftone video display for electronic paper	quantization;power saving;image coding;video halftoning;dolphins;video signal processing;non uniform sampling;flicker;power demand dolphins pixel quantization video sequences visualization gray scale;multilayer halftone video display;display instrumentation;video sequences;flicker rate reduction;gray scale;layer coding;visualization;electronic paper;pixel;flicker rate reduction power scalable multilayer halftone video display electronic paper video halftoning layer coding non uniform sampling;video halftoning electronic paper flicker;layered coding;power demand;power scalable;video signal processing display instrumentation image coding	Video halftoning is a key technology for use in the new display device, electronic paper (e-paper). One challenging issue is how to save the limited power of mobile e-paper device when a halftone video is displayed with various frame rates. In this paper, we propose a power-scalable multi-layer halftone video display scheme, which is composed of layer coding, non-uniform sampling, and flicker rate reduction. Our method not only efficiently save power over the state of the art video halftoning technology but also keep the quality of halftone video nearly unchanged when power saving is additionally considered. Experimental results demonstrate the effectiveness of the proposed method.	digital video;display device;electronic paper;flicker (screen);layer (electronics);nonuniform sampling;sampling (signal processing);scalability	Chao-Yung Hsu;Chun-Shien Lu;Soo-Chang Pei	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607717	flicker;computer vision;visualization;quantization;computer science;video tracking;multimedia;error diffusion;algorithm;pixel;grayscale;computer graphics (images)	EDA	43.73087756222304	-18.01018993962379	56093
4226630710f0e07acf28ae9b5c3c8b194384674e	analysis of hierarchical b pictures and mctf	optimisation;video coding code standards data compression filtering theory motion compensation optimisation;data compression;motion compensation;code standards;coding gain;motion compensated temporal filtering;video coding;h 264 mpeg4 avc;hierarchical b picture coding;encoder optimization;image coding delay decoding displays video coding buffer storage image analysis image processing telecommunication control open loop systems;mctf based coding;filtering theory;encoder optimization h 264 mpeg4 avc hierarchical b picture coding mctf based coding motion compensated temporal filtering	In this paper, an investigation of H.264/MPEG4-AVC conforming coding with hierarchical B pictures is presented. We analyze the coding delay and memory requirements, describe details of an improved encoder control, and compare the coding efficiency for different coding delays. Additionally, the coding efficiency of hierarchical B picture coding is compared to that of MCTF-based coding by using identical coding structures and a similar degree of encoder optimization. Our simulation results turned out that in comparison to the widely used IBBP...structure coding gains of more than 1 dB can be achieved at the expense of an increased coding delay. Further experiments have shown that the coding efficiency gains obtained by using the additional update steps in MCTF coding are generally smaller than the losses resulting from the required open-loop encoder control	algorithmic efficiency;encoder;experiment;mathematical optimization;requirement;simulation	Heiko Schwarz;Detlev Marpe;Thomas Wiegand	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262934	data compression;sub-band coding;computer vision;real-time computing;harmonic vector excitation coding;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;speech coding;coding gain;coding tree unit;tunstall coding;context-adaptive binary arithmetic coding;motion compensation;statistics	Robotics	46.783285026625244	-17.334422483965398	56621
77fbf0b8eb75fb6a5f5f06322ab98aa18350c406	an iwt based blind and robust image watermarking scheme using secret key matrix		In this paper, the authors have proposed a binary watermark embedding approach for protecting the copyright ownership of the gray-scale images. The proposed watermark embedding process is realized in integer wavelet transform (IWT) domain to defend the robustness property. Instead of inserting the watermark bits directly in the coefficients of cover media, an indirect embedding mechanism is proposed with the reference to a logistic map based secret key matrix which enhance the secrecy of the proposed embedding approach. Initially, the approximate sub band of the IWT transformed cover image is selected with the intention to embed the watermark. Later, a secret key matrix of size corresponding to the approximate sub band of the cover image is formed using the logistic map with secret parameters. During the watermark embedding process, the approximate sub band is modified indirectly with reference to the secret key matrix and a proposed division table. The scheme is tested on a set of standard images and satisfactory results are achieved. In addition, the proposed schemes is also able to extract the watermark information in blind manner. Also, the scheme is comparable with some other related schemes. Finally, the proposed watermarking scheme is able to survive the watermark even after performing certain types of image manipulation attacks.	approximation algorithm;coefficient;digital watermarking;distortion;embedded system;grayscale;key (cryptography);logistic map;peak signal-to-noise ratio;scheme;watermark (data file);wavelet transform;x.690	Kshiramani Naik;Saswati Trivedy;Arup Kumar Pal	2017	Multimedia Tools and Applications	10.1007/s11042-017-4986-1	digital watermarking;logistic map;watermark;wavelet transform;computer science;artificial intelligence;robustness (computer science);secrecy;pattern recognition;theoretical computer science;embedding;matrix (mathematics)	ML	40.602799746355196	-10.715757217106487	56705
aa0c8b3ded569b3df906385303707101748e8cc4	video encryption based on data partitioning and scalable coding - a comparison	outil logiciel;data transmission;software tool;transformation cosinus;confidencialidad;video streaming;multimedia;encryption;video a peticion;securite;complexite calcul;data stream;authentication;video a la demande;telecommunication network;multimedia application;data encryption;video codec;data partitioning;confidentiality;authentification;confidentialite;complejidad computacion;temporal resolution;autenticacion;internet;senal video;signal video;cryptage;computational complexity;herramienta controlada por logicial;criptografia;red telecomunicacion;cryptography;transmission donnee;video on demand;safety;transformacion coseno;reseau telecommunication;video transmission;video signal;cryptographie;cosine transform;seguridad;partitionnement donnee;transmision datos;scalable coding;base layer	Many o f today's multimedia applications require connden-tial video transmission over the Internet. Appropriate encryption methods require a high computational complexity and are likely to become a performance bottleneck within software-only applications. To reduce the computational encryption eeort, partial video encryption methods have been proposed in the past. Promising approaches are based on data partitioning where the encoded video stream is partitioned into two streams, one containing the most important data, the other one containing the least important data. Encrypting the most important data only can reduce the required computational complexity to 10-50 compared to encryption of the whole data stream. Besides the known standardized DCT based video codecs, scalable codecs become more and more popular. Scalable codecs have the advantage that no additional eeort is needed to obtain the required data partitioning. In this paper, a novel approach to partial video encryption based on data partitioning applicable to every DCT-based video codec is presented. It is compared to base layer encryption of a video stream encoded with a scalable codec based on a spatio-temporal resolution pyramid. Besides partial encryption, transparent encryption is discussed as well.	codec;computation;computational complexity theory;data compression;discrete cosine transform;encryption;internet;intra-frame coding;mpeg-1;moving picture experts group;parsing;partition (database);scalability;simulation;streaming media;video compression picture types	Thomas Kunkelmann;Uwe Horn	1998		10.1007/BFb0055308	disk encryption theory;40-bit encryption;telecommunications;computer science;theoretical computer science;authentication;disk encryption hardware;computer security;computer network	DB	47.42260523930666	-14.312759659811846	57040
2dab8dff84d33d84f392acb8f4d77ee081f4084e	fast sao estimation algorithm and its vlsi architecture	vlsi adaptive codes video coding;picture quality improvement fast sao estimation algorithm vlsi architecture video encoding parameters determination phase;hevc;sao;estimation;estimation very large scale integration algorithm design and analysis encoding complexity theory hardware software algorithms;vlsi;vlsi hevc sao estimation	SAO estimation is the process of determining SAO parameters in video encoding. There are two difficulties for VLSI implementation of SAO estimation. The first is that there are huge amount of samples to deal with in statistic collection phase. The other is that the complexity of RDO in parameters determination phase is very high. In this article, a fast SAO estimation algorithm and its corresponding VLSI architecture are proposed. For the first difficulty, we use bitmaps to collect statistic of all the 16 samples in one 4×4 block simultaneously. For the second difficulty, we simplify a series of complicated procedures in HM to balance the complexity and BD-rate performance. Experimental results show that the proposed algorithm maintains the picture quality improvement. The VLSI design based on this algorithm can be implemented by 156.32K gates, 8832 bits SPRAM, 400MHz @ 65nm technology and is capable of 8Kx4K @ 120fps encoding.	algorithm;bitmap;blu-ray;data compression;fast fourier transform;image quality;raster document object;sword art online: progressive;very-large-scale integration	Jiayi Zhu;Dajiang Zhou;Shinji Kimura;Satoshi Goto	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025255	estimation;real-time computing;computer science;theoretical computer science;mathematics;very-large-scale integration;statistics	EDA	48.450834084841446	-19.67740562149856	57042
9960eecabe52e35027f4be5d8cea522cb9817b26	morphological segmentation applied to displaced frame difference coding	quantization;mathematical morphology;image numerique;transformation cosinus;cuantificacion;estimation mouvement;image coding;morfologia matematica;image processing;simulation;procesamiento imagen;simulacion;motion estimation;segmentation;quantification;traitement image;codificacion;morphological segmentation;transformacion coseno;imagen numerica;coding;digital image;cosine transform;lts1;displaced frame difference;segmentacion;codage;morphologie mathematique	Abstract This paper describes a segmentation algorithm based on morphological operators, which is applied to code the motion compensated prediction error images or the displaced frame differences (DFD). The DFDs result from various motion compensation techniques, e.g., block matching and pel-recursive techniques. Due to the low correlation in a DFD image, a segmentation based coding is more efficient than a transform based coding. The proposed method segments the DFDs using morphological operations. Various morphological operators are applied to eliminate isolated points and small regions of high energy and merge the small inner low-energy region to its neighboring high-energy region, resulting in a final image with a relatively small number of regions. Contours and regions are coded separately using entropy coding techniques. High compression ratios around 80:1 are achieved on DFDs with very good visual quality for the reconstructed sequences.		Wei Li;M. Kunt	1994	Signal Processing	10.1016/0165-1684(94)90056-6	computer vision;mathematical morphology;quantization;image processing;computer science;discrete cosine transform;motion estimation;coding;segmentation;digital image;computer graphics (images)	Vision	45.91663631386962	-14.03432146474645	57126
523cf1afd6238ef3c235055bce678708e7dc96ee	mvc real-time video encoder for full-hdtv 3d video	real time;video processing;video coding code standards high definition television high definition video;code standards;video coding;3d video transmission hdtv 3d video real time video encoder mvc immersive video services h 264 3d video distribution;high definition video;3d video;three dimensional displays streaming media real time systems encoding video coding hdtv multiplexing;high definition television	3D video technologies such as 3D cameras, displays, and video-processing have become more important for achieving high quality and immersive video services. We propose an H.264 MVC encoder architecture for real-time 3D video distribution and transmission. We also present the first-ever successful development of a full-HDTV real-time MVC encoder.	360-degree video;3d computer graphics;asp.net mvc;display resolution;encoder;h.264/mpeg-4 avc;multiview video coding;real-time clock;real-time computing;real-time transcription	Mitsuo Ikeda;Takayuki Onishi;Takashi Sano;Atsushi Sagata;Hiroe Iwasaki;Yasuyuki Nakajima;Koyo Nitta;Yasuko Takahashi;Kazuya Yokohari;Daisuke Kobayashi;Kazuto Kamikura;Hirohisa Jozawa	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6161791	video compression picture types;scalable video coding;composite video;microsoft video 1;real-time computing;video;h.263;bink video;uncompressed video;computer science;electrical engineering;video capture;video tracking;multimedia;video processing;smacker video;pevq;h.261;s-video;multiview video coding;computer graphics (images)	Robotics	43.45968099057041	-20.712531754437197	57158
ee3c4e376aa273f417a8776024118166c51ddd4e	multitemporal hyperspectral image compression	hyperspectral imagery;geophysical image processing;image coding hyperspectral imaging decorrelation pixel principal component analysis;image coding;data compression;spectral image concatenation;reference image;image coding geophysical image processing;multitemporal imagery data compression hyperspectral imagery;change removal process;principal component analysis;pixel;rate distortion performance;decorrelation;linear prediction;change removal process multitemporal hyperspectral image compression reference image image coding linear prediction spectral image concatenation rate distortion performance;multitemporal imagery;hyperspectral imaging;hyperspectral image;multitemporal hyperspectral image compression	The compression of multitemporal hyperspectral imagery is considered, wherein the encoder uses a reference image to effectuate temporal decorrelation for the coding of the current image. Both linear prediction and a spectral concatenation of images are explored to this end. Experimental results demonstrate that, when there are few changes between two images, the gain in rate-distortion performance is achieved over the independent coding of the current image. In addition, a strategy that explicitly removes salient temporal changes and stores them losslessly in the bitstream is proposed, and it is observed that this change-removal process results in a slight decrease in the rate-distortion performance with the benefit of perfect representation of the changed pixels.	bitstream;concatenation;decorrelation;distortion;encoder;image compression;lossless compression;pixel	Wei Zhu;Qian Du;James E. Fowler	2011	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2010.2081661	computer vision;computer science;hyperspectral imaging;pattern recognition;mathematics;statistics;remote sensing	Vision	44.13068218243284	-15.397060244716634	57236
119f0d45a9eb0d9c6e76ef4d4f415ccbb4739593	ultra high-definition video coding using bit-depth reduction with image noise reduction and pseudo-contour prevention	decoding;video coding;image reconstruction;high definition video	We propose a novel ultra high-definition video coding method with bit-depth reduction before encoding procedure and bit-depth reconstruction after decoding procedure. The bit-depth reduction is performed by Lloyd-Max quantization; considering ultra high-definition video noise reduction for high coding efficiency and gradation conservation for pseudo-contour prevention. The bit-depth reconstruction is carried out accurately using side information which is determined by comparing a local-decoded bit-depth reconstructed image and an original image on encoder side. Experiments show that the proposed method has a pseudo-contour prevention effect and a better PSNR in comparison with conventional video coding methods.	algorithmic efficiency;data compression;encoder;experiment;hdmi;noise reduction;peak signal-to-noise ratio	Yasutaka Matsuo;Toshihiko Misu;Shunsuke Iwamura;Shinichi Sakaida	2013	2013 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2013.6706360	iterative reconstruction;scalable video coding;computer vision;computer science;coding tree unit;multimedia;context-adaptive binary arithmetic coding;h.261;video denoising;multiview video coding;computer graphics (images)	Vision	44.639662528428936	-18.0022292308166	57527
c0310a03bcba50d02185e58c4af9c6cafc9185b7	multiple description coding using multiple reference frame for robust video transmission	matching pursuit algorithms;video sequence;probability video coding visual communication image sequences image motion analysis decoding;image motion analysis;probability;reference frame mismatch;motion compensation;decoding;transmission error;macroblock loss rates;visual communication;reference frame;video quality;video sequences;probability multiple description coding multiple reference frame motion prediction transmission error decoding video quality macroblock loss rates robust video transmission error resilient coding method video sequence reference frame mismatch side information transmission;side information transmission;robustness decoding encoding video sequences video coding matching pursuit algorithms discrete cosine transforms error correction motion compensation;video coding;error resilient coding method;discrete cosine transforms;error correction;video transmission;multiple description coding;error resilience;robustness;robust video transmission;side information;encoding;multiple reference frame;image sequences;motion prediction	Multiple description coding is an error-resilient coding method that encodes a video sequence into several bitstreams, called descriptions. The main problem of multiple description coding is the mismatch of reference frames in encoder and decoder, when the descriptions are lost during transmission. The mismatch is also referred to as drift and can be alleviated efficiently by transmitting side information along with each description. We introduce a new generation scheme for side information, which is based on multiple reference frames. For each encoding frame, two previously encoded frames are used as the reference frame's I motion prediction. The prediction results of two reference frames are encoded as descriptions and side information respectively. When transmission errors occur in either reference frame, the decoder can use the correctly received reference frame in decoding. Simulation results prove that the proposed method can improve video quality obviously along different macroblock loss rates.	encoder;internet;jumbo frame;macroblock;multiple description coding;reference frame (video);simulation;transmitter;video	Jinghong Zheng;Lap-Pui Chau	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465509	reference frame;inter frame;frame;residual frame;computer vision;real-time computing;error detection and correction;computer science;video quality;theoretical computer science;multiple description coding;probability;motion compensation;encoding;statistics;robustness;visual communication	Vision	48.247367010636836	-16.735828922417287	57599
04195e35ab909939a3c38f8b4f1e8985299d3367	simultaneous encryption and compression of digital images based on secure-jpeg encoding		Confidentiality and efficient bandwidth utilization requires compression and encryption of digital images. Both of these parameters are necessary for most communication systems. Encryption and compression done separately sometimes result in decreased performance or reduced reconstruction quality. The paper presents a simultaneous encryption and compression scheme for digital images. It modifies standard JPEG compression in a way to encrypt data during compression. The encryption steps are based on a JPEG compressible image encryption scheme. The proposed Secure-JPEG algorithm provides the benefits of encryption along with the ability to provide lossless compression. This scheme results in improved performance and better reconstruction quality than existing schemes utilizing the similar approach.	encryption;jpeg	Saqib Maqbool;Nisar Ahmad;Aslam Muhammad;Ana María Martínez Enríquez	2016		10.1007/978-3-319-39393-3_15	packbits	Crypto	39.69102510451666	-11.912824603631675	57738
2ed31c05d8ae806b72f269351f8fa7dd5b68cd20	objective measurement scheme for perceived picture quality degradation caused by mpeg encoding without any reference pictures	computer programming;data hiding;video;data transmission	The rapid progress in digital transmission technology has spurred demand for developing a technology that supports monitoring of video transmissions. Automating the picture quality assessment process is in particular demand because it currently depends on subjective assessments by human operators and places a heavy burden on them. The authors therefore propose an objective picture quality measurement method that works without reference pictures. In this proposed method, invisible markers are embedded into original pictures by use of the spread spectrum data hiding method. Since the markers are widely spread over the frequency domain, the degradation caused by MPEG compression can be estimated by detecting the extent of marker degradation. Our method is usable regardless of the kind of picture, bitrate, and number of stages of tandem codec connections. The degradation in picture quality caused by the embedded markers is quite small and not perceivable by the human eye. The proposed method is therefore applicable to a wide range of visual transmission services.© (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	elegant degradation;image quality	Osamu Sugimoto;Ryoichi Kawada;Masahiro Wada;Shuichi Matsumoto	2001			subjective video quality;simulation;video;telecommunications;computer science;computer programming;multimedia;spread spectrum;frequency domain;data transmission	Arch	49.6351367243568	-15.351098465022144	58009
beb242ccd60fa594415448c8c78913ce79aea410	improving still image coding by an som-controlled associative memory	artefacto;base donnee;memoire associative;image coding;image processing;imagen fija;image databank;speech processing;database;tratamiento palabra;procesamiento imagen;traitement parole;base dato;hombre;qualite image;traitement image;reduction donnee;artefact;codage image;fixed image;traitement document;image quality;banco imagen;banque image;human;pattern recognition;autoorganizacion;associative memory;memoria asociativa;reduccion datos;self organization;image fixe;calidad imagen;data reduction;document processing;reconnaissance forme;reconocimiento patron;autoorganisation;self organising map;tratamiento documento;homme	Archiving of image data often requires a suitable data reduction to minimise the memory requirements. However, these compression procedures entail compression artefacts, which make machine processing of the captured documents more difficult and reduce subjective image quality for the human viewer. A method is presented which can reduce the occurring compression artefacts. The corrected image yields as output of an auto-associative memory that is controlled by a Self-Organising Map (SOM).	content-addressable memory	Gerald Krell;R. Rebmann;Udo Seiffert;Bernd Michaelis	2003		10.1007/978-3-540-24586-5_70	image quality;computer vision;data reduction;self-organization;document processing;image processing;image compression;computer science;artificial intelligence;pattern recognition;speech processing;database	Vision	45.47103255829319	-12.592983430645338	58168
7ca09ef0609bc4493888fe4c875d913d0a4a2376	development of a cpld based wireless remote control system of pruning machine for plantation forest	automatic control;agricultural machinery;telecontrol agricultural machinery forestry interference signal programmable logic devices;programmable logic devices;digital signal processing;receiving system;control systems;cpld;remote control;plantation forest;motion control;forestry;wireless remote control system;manufactured automatic pruning machine wireless remote control system plantation forest hand controller receiving system mechanical system complex programmable logic device digital signal processing electromechanical interface transmitting system anti interference function;transmitting system;electromechanical interface;circuit simulation automatic pruning machine wireless remote control cpld;hand controller;manufactured automatic pruning machine;circuit simulation;control system;petroleum;interference signal;engines;fuels;mechanical system;telecontrol;control systems automatic control dc motors motion control fuels engines mechanical systems petroleum forestry digital signal processing;complex programmable logic device;dc motors;automatic pruning machine;mechanical systems;anti interference function;wireless remote control	A wireless remote control pruning machine was developed for plantation forest. The new designed system mainly consists of two parts: a transmitting system in the form of a hand controller and a receiving system mounted on the frame of the machine to drive the mechanical system. The idea of using the complex programmable logic device (CPLD) to realize the digital signal processing for the control system was discussed in details. The structure of the electromechanical interface, the transmitting system and the receiving system were all presented in this paper. The point of technology for each part was analyzed. The whole remote control system has the advantages of simplicity, reliability of performance and function of anti-interference. It has been successfully used in the manufactured automatic pruning machine and the expected result is obtained.	complex programmable logic device;control system;digital signal processing;game controller;interference (communication);logic gate;random forest;remote control;technical support;transmitter	Junmei Zhang;Wenbin Li;Chao Sa;Deming Wang;Patrick S. K. Chua;F. L. Tan	2007	2007 11th International Conference on Computer Supported Cooperative Work in Design	10.1109/CSCWD.2007.4281590	embedded system;control system;complex programmable logic device;mechanical system	Robotics	53.4195764867807	-11.48166851409419	58199
c5054aa8968dc069ee2d0a2fc8b5951a742537a6	istree digital watermarking algorithm based on contourlet transform	discrete wavelet transforms;digital watermarking;watermarking discrete wavelet transforms discrete transforms wavelet transforms discrete fourier transforms fourier transforms robustness wavelet domain image processing image analysis;watermarking;image coding;image processing;digital watermark;trees mathematics;wavelet transforms;istree digital watermarking contourlet transform;discrete transforms;fourier transforms;image sequence;watermark image sequence istree digital watermarking contourlet transform image copyright subtree digital watermarking image decomposed image scrambling;contourlet transform;robustness;image analysis;istree;wavelet domain;discrete fourier transforms;wavelet transforms image coding image sequences trees mathematics watermarking;image sequences	In order to protect the copyright of the image, in this paper proposed a novel important sub-tree (Istree) digital watermarking algorithm based on contourlet transform. First, Shuffling is applied by watermarking image for increasing robustness. Second, the original image are decomposed three levels by contourlet transform, and then analysis sub-bands in all directions, according to various levels sub-bands in all direction structure like tree. And find important sub-tree, then Scrambling after the watermark image sequence is embedded in important sub-tree. The experimental results show that the algorithm has a certain degree of robustness and can resistance attack.	algorithm;contourlet;digital watermarking;embedded system	Fan Zhang;Yuli Xu;Rui Li	2010	2010 International Conference on Machine Vision and Human-machine Interface	10.1109/MVHI.2010.196	computer vision;contourlet;image analysis;image processing;digital watermarking;computer science;electrical engineering;theoretical computer science	EDA	41.33496769503434	-10.957639856326876	58201
340706bced5734d8e5c7955b82c2faa4c47bc3dd	fast mode decision for depth video coding using h.264/mvc		Multi-view video plus depth (MVD) is presented as one of 3D video formats. With color and depth videos, virtual views can be synthesized using depth image based rendering. Due to high complexity of 3D video coding, reduction of the time complexity of depth video coding is essential to success of MVD. Thus, this paper proposes a fast mode decision scheme for depth video coding, aiming at speeding up both mode decision and synthesized-view distortion estimation simultaneously. The proposed scheme includes two stages. First at all, features of color and depth videos are referred to reduce the candidates for mode decision with high prediction accuracy and negligible degradation of rate-distortion (RD) performance. For a depth macroblock (MB) having the co-located MB with high complexity of vertical texture in the reconstructed color video, the second stage further selects the optimal mode using the synthesized-view distortion directed RD cost to reduce the synthesized-view distortion. For fast estimation of synthesized-view distortions, a mathematical approximation of a previous synthesized-view distortion function that mimics view rendering is proposed. Experimental results conducted by JMVC 6.0.3, the reference software of H.264/MVC, show that the proposed scheme achieves significant time reduction with negligible RD performance degradation.	algorithm;approximation;color depth;computation;data compression;distortion;elegant degradation;image processing;macroblock;model–view–controller;ruby document format;stereoscopic video coding;time complexity;video file format	Chih-Hung Lu;Han-Hsuan Lin;Chih-Wei Tang	2015	J. Inf. Sci. Eng.		coding (social sciences);computer science;theoretical computer science;distributed computing;multiview video coding	AI	45.70330144131186	-19.472904276586046	58292
851071c520f4b3f3a15431e1138556d43903362d	splined-based motion vector encoding scheme		We present a new motion vector encoding scheme based on a curve fitting algorithm; the motion vectors of collocated blocks in a video sequence are represented by a set o f keypoints that are the coefficients of the best fitted curve into the motion vectors. Motion vectors are mapped into four different categories; each corresponding to the condition for recovering each motion vector from the curve losslessly. Using a proposed adaptive motion estimation method, rather than selecting the motion vector corresponding to the block of minimum residual energy, a set of motion vectors is chosen such that each candidate set contains motion vectors o f residual blocks of a number of bits less than a threshold. We utilize a rate-distortion technique to estimate the number of bits per candidate residual block to avoid computational complexity for selecting the best key point for the curve in terms of residual data. Experimental results show a significant bitrate reduction up to 43% for encoding the motion vector data for the curves’ and block residuals in comparison to the H.264/AVC codec.	algorithm;codec;coefficient;computational complexity theory;curve fitting;distortion;h.264/mpeg-4 avc;line code;lossless compression;motion estimation	Parnia Farokhian;Chris Joslin	2013			artificial intelligence;encoding (memory);pattern recognition;computer vision;computer science;motion vector	Vision	46.527182364988185	-18.300466021302956	58875
62fcfe4fd1648b456ce2a1c6bdfe4a165f04b145	two-stage compression for fast volume rendering of time-varying scalar data	lempel ziv;frames per second;time varying;paper;data compression;volume rendering;spatial coherence;lossless compression;large scale;visualization;image generation;large scale data visualization;graphics hardware;peak signal to noise ratio;data visualization;nvidia;nvidia geforce 6800 gto;time varying data;opengl;computer science;temporal coherence;data transfer;rendering	This paper presents a two-stage compression method for accelerating GPU-based volume rendering of time-varying scalar data. Our method aims at reducing transfer time by compressing not only the data transferred from disk to main memory but also that from main memory to video memory. In order to achieve this reduction, the proposed method uses packed volume texture compression (PVTC) and Lempel-Ziv-Oberhumer (LZO) compression as a lossy compression method on the GPU and a lossless compression method on the CPU, respectively. This combination realizes efficient compression exploiting both temporal and spatial coherence in time-varying data. We also present experimental results using scientific and medical datasets. In the best case, our method produces 56% more frames per second, as compared with a single-stage (GPU-based) compression method. With regard to the quality of images, we obtain permissible results ranging from approximately 30 to 50 dB in terms of PSNR (peak signal-to-noise ratio).	best, worst and average case;central processing unit;coherence (physics);computer data storage;experiment;glossary of computer graphics;graphics processing unit;java platform, standard edition;lempel–ziv–oberhumer;lempel–ziv–welch;lossless compression;lossy compression;out-of-core algorithm;peak signal-to-noise ratio;real-time clock;scalability;speedup;test engineer;texture compression;video ram (dual-ported dram);volume rendering	Daisuke Nagayasu;Fumihiko Ino;Kenichi Hagihara	2006		10.1145/1174429.1174478	data compression;lossy compression;data compression ratio;adaptive scalable texture compression;visualization;peak signal-to-noise ratio;computer hardware;rendering;image compression;computer science;theoretical computer science;lossless compression;texture compression;graphics hardware;frame rate;volume rendering;data visualization;statistics;computer graphics (images)	Visualization	41.53177534066743	-17.18596325390856	58895
20a3530f8dd30a55b96df1eaed682b043f5cce7e	predictive image compression using conditional averages	loco i;image coding;med predictor;conditional average;data compression;predictive image compression;calic;image compression;loco i predictive image compression conditional average gap predictor calic med predictor jpeg ls;gap predictor;data compression image coding;jpeg ls	This paper describes the predictive image compression using conditional averages. compare the performance of the proposed predictor with the GAP predictor used in CALIC and the MED predictor used in JPEG-LS and LOCO-I.	image compression;jpeg;kerrison predictor	S. Derin Babacan;Khalid Sayood	2004	Data Compression Conference, 2004. Proceedings. DCC 2004	10.1109/DCC.2004.1281500	data compression;speech recognition;image compression;computer science;machine learning;pattern recognition;mathematics;algorithm	Robotics	43.57340930525451	-16.537254401648312	59226
451924b82e43d59fa8fda34482d8116550318255	visual cryptography technique based on fft	image encryption;security analysis;image coding;data compression;visual secret sharing;fft;fast fourier transform;visual cryptography;image compression;cryptography;image quality;fast fourier transforms;visaul cryptography;image compression visaul cryptography visual secret sharing fft image encryption;image coding cryptography data compression fast fourier transforms;encryption process visual cryptography technique fft image sharing security approach internet pixel expansion image quality grayscale images fast fourier transform multilayer security method image compression	Visual cryptography is a powerful approach for image sharing security through unsecured network such as Internet. Pixel expansion and bad image quality are the most cons of this approach. Therefore, visual cryptographic technique is proposed for grayscale images to overcome these drawbacks by Fast Fourier Transform (FFT). FFT is used for image sharing where each share is encrypted via Multilayer Security Method (MSM). Image compression is also basically used to eliminate the pixel expansion which is produced before the encryption process. The experimental results and security analysis state that the proposed visual cryptographic technique leads to active levels of image security and quality without pixel expansion.	encryption;fast fourier transform;grayscale;image compression;image quality;internet;markov switching multifractal;pixel;visual cryptography	Ali Makki Sagheer;Salah Sleibi Al-Rawi;Laith Hamid Abed	2011	2011 Developments in E-systems Engineering	10.1109/DeSE.2011.73	computer vision;computer science;theoretical computer science;internet privacy	Vision	40.23395922852253	-11.101615457691219	59376
fbf2ae6cc4d7779854d47380cbc7ce01602950a8	fractal color image compression using vector distortion measure	image storage;fractals;image coding;adaptive fractal color image coding;data compression;grayscale space;quadtree partition;color space;fractal color image compression;color;coding errors;root mean square error;vector distortion measure;fixed image partition;distortion measurement;fractals color image coding distortion measurement extraterrestrial measurements redundancy gray scale image reconstruction image storage partitioning algorithms;color image coding;gray scale;3 dimensional color space;adaptive signal processing;redundancy;image compression;image colour analysis;image reconstruction;root mean square error distortion measure;spectral redundancy;compression ratio;rgb components coding;24 bit;3 dimensional;image fidelity;extraterrestrial measurements;experimental results;24 bit fractal color image compression vector distortion measure rgb components coding spectral redundancy root mean square error distortion measure grayscale space 3 dimensional color space experimental results compression ratio fixed image partition adaptive fractal color image coding quadtree partition image fidelity image reconstruction;image reconstruction fractals data compression image coding image colour analysis coding errors adaptive signal processing quadtrees;quadtrees;color image;partitioning algorithms;color image compression	In this paper, fractal monochrome image compression technique proposed by Jacquin is investigated for 24-bit true color image by encoding the RGB components' images independently. To exploit the spectral redundancy in RGB components, the root mean square error distortion measure in grayscale space is extended to 3-dimensional color space for fractal-based color image coding. Experimental results show that 1.5 compression ratio improvement can be obtained using the vector distortion measure in fractal coding with fixed image partition as compared to separate fractal coding in RGB images. In addition, adaptive fractal color image coding based on quadtree partition is also proposed which can obtain a good trade-off between compression ratio and image fidelity.	24-bit;channel (digital image);color depth;color image;color space;distortion;fractal;grayscale;image compression;lossy compression;mean squared error;monochrome;quadtree	Lai-Man Po	1995		10.1109/ICIP.1995.538544	data compression;iterative reconstruction;adaptive filter;color histogram;three-dimensional space;computer vision;color depth;color image;fractal;image compression;computer science;compression ratio;pattern recognition;mathematics;mean squared error;color balance;fractal transform;redundancy;color space;fractal compression;grayscale;computer graphics (images)	Robotics	44.05217328103606	-14.387112428158035	59502
851b38ac33ff7034186726f090934c77afa4acfd	a wavelet-based two-stage near-lossless coder	transformation ondelette;discrete wavelet transforms;desciframiento;iterative method;theorie vitesse distorsion;traitement signal;methode section divisee;image coding;algorithms data compression image enhancement image interpretation computer assisted numerical analysis computer assisted signal processing computer assisted;arithmetic coding;data compression;lossy medium;decodage;decoding;debit information;information transmission;circuit sans perte;medio dispersor;lossless compression;transformation ondelette discrete;transform coding;critical distortion;indice informacion;arithmetic code;metodo iterativo;rate distortion theory;codigo aritmetico;wavelet transforms;lossy plus residual coding;codificacion;image coding biomedical imaging medical diagnostic imaging bit rate discrete wavelet transforms image reconstruction wavelet domain arithmetic iterative decoding rate distortion;arithmetic codes;wavelet transform;methode iterative;signal processing;near lossless compression;coding;information rate;lossless circuit;transformation inverse;compression sans perte;code arithmetique;transformacion ondita;transmision informacion;near lossless compression critical distortion lossless compression lossy plus residual coding;error bound;transmission information;inverse transformation;circuito sin perdida;rate distortion theory wavelet based coder two stage near lossless coder compression scheme wavelet based lossy layer arithmetic coding;procesamiento senal;wavelet transforms arithmetic codes data compression image coding transform coding;compresion sin perdida;wavelet transformation;codage;transformacion inversa;milieu dissipatif;multistage method	"""In this paper, we present a two-stage near-lossless compression scheme. It belongs to the class of """"lossy plus residual coding"""" and consists of a wavelet-based lossy layer followed by arithmetic coding of the quantized residual to guarantee a given L infin  error bound in the pixel domain. We focus on the selection of the optimum bit rate for the lossy layer to achieve the minimum total bit rate. Unlike other similar lossy plus lossless approaches using a wavelet-based lossy layer, the proposed method does not require iteration of decoding and inverse discrete wavelet transform in succession to locate the optimum bit rate. We propose a simple method to estimate the optimal bit rate, with a theoretical justification based on the critical rate argument from the rate-distortion theory and the independence of the residual error"""	lossless compression;wavelet	Sehoon Yea;William A. Pearlman	2006	IEEE Trans. Image Processing	10.1109/TIP.2006.877525	discrete mathematics;theoretical computer science;signal processing;mathematics;statistics;wavelet transform	Visualization	46.61069637508287	-13.43373607917326	59869
83c790c460952ae6a03bee23dedef5541b0e764f	asymmetric coding scheme for 3d frame-compatible formats	image sampling;video coding decoding image reconstruction image sampling;3d frame compatible format asymmetric coding h 264 avc;decoding;peak signal to noise ratio asymmetric coding scheme 3d frame compatible formats encoding process up sampling process decoder symmetric coding scheme psnr human visual system visual quality;h 264 avc;video coding;encoding bit rate psnr decoding visualization receivers humans;technology and engineering;image reconstruction;3d frame compatible format;asymmetric coding	This paper proposes an asymmetric coding scheme for 3D frame-compatible formats. The aim is to improve the coding efficiency while maintaining the same visual performance. In the encoding process, prediction is performed between the reconstructed left samples and the original right samples. At the receiver, the samples of the left view and the residuals of the right view are decoded. However, the right views could be reconstructed in the up-sampling process by combining the reconstructed left samples and the residuals of the right view. Thus, no modification is required to the decoder. It is shown that the proposed method significantly reduces the bitrate compared to the symmetric coding scheme. Although the PSNR of the right view is about 1.5dB lower than the left view on average, the visual quality is considered tolerable due to the properties of the human visual system.	algorithmic efficiency;peak signal-to-noise ratio;sampling (signal processing)	Jin Li;Jan De Cock;Peter Lambert;Rik Van de Walle	2012	2012 Fourth International Workshop on Quality of Multimedia Experience	10.1109/QoMEX.2012.6263878	iterative reconstruction;sub-band coding;computer vision;shannon–fano coding;telecommunications;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;context-adaptive binary arithmetic coding	Vision	44.898914166651835	-17.581993535229316	60217
16352e19bd3bdf680bf9c7af55ebabade444b23d	perceptual coding of stereo endoscopy video for minimally invasive surgery	pacs;minimal invasive surgery;endoscopy;wavelet transforms;video coding;data storage;wavelet transform;computational complexity;wavelet based coding;stereo vision;surgery;stereo;algorithms;memory allocation;perception;video;elastic registration;vector field;wavelets;model based coding	In this paper, we propose a compression scheme that is tailored for stereo-laparoscope sequences. The inter-frame correlation is modeled by the deformation field obtained by elastic registration between two subsequent frames and exploited for prediction of the left sequence. The right sequence is lossy encoded by prediction from the corresponding left images. Wavelet-based coding is applied to both the deformation vector fields and residual images. The resulting system supports spatio temporal scalability, while providing lossless performance. The implementation of the wavelet transform by integer lifting ensures a low computational complexity, thus reducing the required run-time memory allocation and on line implementation. Extensive psychovisual tests were performed for system validation and characterization with respect to the MPEG4 standard for video coding. Results are very encouraging: the PSVC system features the functionalities making it suitable for PACS while providing a good trade-off between usability and performance in lossy mode.© (2007) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	psychoacoustics	Guido Bartoli;Gloria Menegaz;Guang-Zhong Yang	2007		10.1117/12.708956	computer vision;simulation;computer science;multimedia;wavelet transform	Vision	45.089300161232806	-19.523581908504113	60371
d5a0d1c9890e42fb425f3106a785f54047447e37	toward a robust image authentication method surviving jpeg lossy compression.	digital watermarking;lossy compression;authentication;compresion senal;image;compression signal;authentification;image authentication;autenticacion;imagen;signal compression;compression ratio;algorithms;image watermarking	This paper presents a robust image authentication approach that distinguishes malicious attacks from JPEG lossy compression. The authentication procedure calculates the relationships between important DCT coefficients in each pair of DCT blocks and predefined thresholds to form the authentication message, and then embeds the encryption of the authenticated message into other DCT coefficients. The message calculation and embedding procedure are based on two proposed quantization properties that always exist under different JPEG quantization tables. Therefore, the proposed image authentication approach can tolerate JPEG compression efficiently. Experimental results demonstrate the effectiveness of the proposed image authentication approach.	authentication;coefficient;discrete cosine transform;encryption;jpeg;lossy compression	Ching-Yung Lin;Shih-Fu Chang	1998	J. Inf. Sci. Eng.	10.1117/12.298462	data compression;lossy compression;computer vision;transparency;image compression;computer science;theoretical computer science;jpeg;authentication;lossless compression;internet privacy;quantization	Vision	40.24203322115811	-11.231759523843134	60584
17066f2c65ab7931826d3a3627e2c47412ee8d4a	unsupervised learning approach to adaptive differential pulse code modulation	unsupervised learning;quantization;convergence;data compression;probability density function;pulse modulation unsupervised learning modulation coding laplace equations data compression bayesian methods parameter estimation mean square error methods convergence testing;bayesian methods;testing;unsupervised learning adaptive differential pulse code modulation dpcm adaptive quantization bandwidth compression decision directed estimates;data mining;differential pulse code modulation;estimation algorithm;decision directed estimates;density functional theory;adaptive quantization;laplace equations;bandwidth compression;mean square error;modulation coding;mean square error methods;variance estimation;orthogonal transformation;parameter estimation;adaptive differential pulse code modulation dpcm;pulse modulation;decision directed;adaptive differential pulse code modulation;modulation	This research is concerned with investigating the problem of data compression utilizing an unsupervised estimation algorithm. This extends previous work utilizing a hybrid source coder which combines an orthogonal transformation with differential pulse code modulation (DPCM). The data compression is achieved in the DPCM loop, and it is the quantizer of this scheme which is approached from an unsupervised learning procedure. The distribution defining the quantizer is represented as a set of separable Laplacian mixture densities for two-dimensional images. The condition of identifiability is shown for the Laplacian case and a decision directed estimate of both the active distribution parameters and the mixing parameters are discussed in view of a Bayesian structure. The decision directed estimators, although not optimum, provide a realizable structure for estimating the parameters which define a distribution which has become active. These parameters are then used to scale the optimum (in the mean square error sense) Laplacian quantizer. The decision criteria is modified to prevent convergence to a single distribution which in effect is the default condition for a variance estimator. This investigation was applied to a test image and the resulting data demonstrate improvement over other techniques using fixed bit assignments and ideal channel conditions.	adaptive differential pulse-code modulation;algorithm;coder device component;convergence (action);data compression;default;estimated;laplacian matrix;mean squared error;mixture model;pulse rate;quantization (signal processing);sample variance;standard test image;unsupervised learning;density	Norman C. Griswold;Khalid Sayood	1982	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1982.4767269	data compression;unsupervised learning;probability density function;convergence;quantization;bayesian probability;computer science;machine learning;pattern recognition;pulse-width modulation;mathematics;mean squared error;software testing;estimation theory;density functional theory;statistics;orthogonal transformation;modulation	ML	48.628654078890186	-11.921778749579376	60596
f55a30fe944ee9108ffc0b9181704c88972e58a4	very low bitrate video: a statistical analysis in the dct domain	content management;filigranage numerique;analisis imagen;offre service;digital watermarking;analisis contenido;red sin hilo;streaming;informatique mobile;video streaming;steganographie;critical point;analisis estadistico;robust watermarking;image processing;reseau sans fil;transmision continua;transformation cosinus discrete;wireless network;point critique;procesamiento imagen;gestion contenido;probabilistic approach;curva gauss;qualite image;traitement image;discrete cosine transform;steganography;content analysis;transmission en continu;esteganografia;mobile service;senal video;statistical analysis;signal video;discrete cosine transforms;enfoque probabilista;approche probabiliste;image quality;filigrana digital;analyse statistique;loi normale;gestion contenu;video signal;punto critico;image analysis;calidad imagen;analyse contenu;mobile computing;analyse image;proposals;gaussian distribution;mobile network	While generally the watermarking methods are designed to protect high quality video (e.g. DVD), a continuous increasing demand of protecting very low bitrate video (VLBV) e.g. in mobile networks, the video stream may be coded at 64kbit/s is also met nowadays. In this respect, a special attention should be paid to the statistical behaviour of the video content. At our best knowledge, this paper presents the first statistical investigation on VLBV in the DCT (Discrete Cosine Transform) domain. The major contribution identifies the particular way in which the VLBV obeys to the very popular Gaussian law. It also points to critical behaviour differences between the high and very low bitrate videos. These theoretical results are validated under the framework of watermarking experiments carried out in collaboration with the SFR mobile service provider in France (Vodafone group).	digital video;digital watermarking;discrete cosine transform;display resolution;experiment;streaming media	Mihai Mitrea;Françoise J. Prêteux;Mihai Petrescu	2005		10.1007/11738695_14	normal distribution;image quality;average bitrate;cellular network;image analysis;content analysis;telecommunications;image processing;digital watermarking;content management;computer science;video quality;wireless network;discrete cosine transform;variable bitrate;steganography;constant bitrate;critical point;mobile computing;statistics;computer graphics (images)	Vision	39.61005291211945	-17.369204328036023	60622
3a63b365d15a5e5840a05e440a5fb0149b81f834	employing a novel cross-diamond search in a modified hierarchical search motion estimation algorithm for video compression	video compression;motion estimation;video coding;hierarchical search	The large amount of bandwidth that is required for the transmission or storage of digital videos is the main incentive for researchers to develop algorithms that aim at compressing video data (digital images) whilst keeping their quality as high as possible. Motion estimation algorithms are used for video compression as they reduce the memory requirements of any video file while maintaining its high quality. Block matching has been extensively utilized in compression algorithms for motion estimation. One of the main components of block matching techniques is search methods for block movements between consecutive video frames whose aim is to reduce the number of comparisons. One of the most effective searching methods that yield accurate results but is computationally very expensive is the Full Search algorithm. Researchers try to develop fast search motion estimation algorithms to reduce the computational cost required by full-search algorithms. In this research, the authors present a new fast search algorithm based on the hierarchical search approach, where the number of searched locations is reduced compared to the Full Search. The original image is sub-sampled into additional two levels. The Full Search is performed on the highest level where the complexity is relatively low. The Enhanced Three-Step Search Algorithm and a new proposed searching algorithm are used in the consecutive two levels. The results show that by using the standard accuracy measurements and the standard set of video sequences, the performance of the proposed hierarchal search algorithm is close to the full search with 83.4% reduction in complexity and with a matching quality over 98%.	algorithmic efficiency;computational complexity theory;data compression;digital image;digital video;display resolution;experiment;maxima and minima;motion estimation;requirement;search algorithm;while	Nijad Al-Najdawi;M. Noor Al-Najdawi;Sara Tedmori	2014	Inf. Sci.	10.1016/j.ins.2013.08.009	video compression picture types;data compression;beam search;computer vision;simulation;beam stack search;quarter-pixel motion;computer science;theoretical computer science;video tracking;motion estimation;incremental heuristic search;iterative deepening depth-first search;block-matching algorithm;rate–distortion optimization;motion compensation;binary search algorithm;search algorithm	Vision	48.17599308912598	-19.039728954846634	60819
6229e8f732448d32ecd725962754b6e8d6868c0c	wavelets for waveform coding of digital symbols	funcion haar;fonction echelle;scaling function;codecs;fonction haar;forme onde;estudio comparativo;banda base;bande base;haar function;wavelet base;base ondita;transform coding;analog pulse shaping concepts digital symbols wavelet based coder decoder codec baseband waveform coding bandwidth efficiency nyquist pulse shaping condition;base band;funcion escala;wavelet transforms;etude comparative;pulse shaping;forma onda;finite impulse response filter signal processing filter bank speech processing lagrangian functions image reconstruction algorithm design and analysis signal processing algorithms design methodology image coding;comparative study;codec;nyquist criterion wavelet transforms transform coding codecs;waveform;forme impulsion;forma impulsion;pulse shape;base ondelette;nyquist criterion	A wavelet-based coder-decoder (codec) structure is defined for baseband waveform coding. Numerical results for bandwidth efficiency are given, and a comparison between several different wavelets is presented. Moreover, it is shown that wavelets obey the Nyquist pulse shaping condition and provide a unified framework for analog pulse shaping concepts of communications.	baseband;codec;noise shaping;nyquist rate;pulse shaping;spectral efficiency;unified framework;waveform;wavelet	Prashant P. Gandhi;Sathyanarayan S. Rao;Ravikanth Pappu	1997	IEEE Trans. Signal Processing	10.1109/78.622962	codec;speech recognition;telecommunications;computer science;mathematics;statistics	DB	47.10074592809065	-11.299302669376011	60967
6e372639eaddf93cb075860b7f8ba35ff9efdd7f	hiding multitone watermarks in halftone images	digital watermarking;watermarking;watermarking stochastic resonance gray scale printing computational complexity decoding humans low pass filters filtering books;ordered dither;image coding;sid gnbedf;digital watermark;data encapsulation;error analysis;least squares;digital communication;computational complexity;digital halftoning;error diffusion;least square;print and scan attacks multitone watermark hiding halftone images low computational complexity;watermarking computational complexity data encapsulation image coding;gnbedf;least squares methods;sid gnbedf digital watermarking digital halftoning error diffusion ordered dither least squares gnbedf	The authors propose a method for embedding a multitone watermark using low computational complexity. The proposed approach can guard against reasonable cropping or print-and-scan attacks.	computational complexity theory	Jing-Ming Guo;Yun-Fu Liu	2010	IEEE MultiMedia	10.1109/MMUL.2010.14	computer vision;digital watermarking;computer science;theoretical computer science;computer graphics (images)	Vision	41.58939719178327	-10.986258939672073	61052
5c41f4294fdae23b947cbf3967be7cbd4539ef61	a reversible information hiding algorithm for 2d vector maps	vectors principal component analysis sorting data mining accuracy standards solid modeling;blind source separation;object detection affine transforms blind source separation cartography data encapsulation feature extraction geographic information systems mean square error methods;data encapsulation;geographic information systems;feature extraction;affine transforms;mean square error methods;cartography;information hiding steganographic algorithms two dimensional vector maps capacity reversible;affine transformation reversible information hiding algorithm geographic information system gis application data precision 2d vector map information hiding algorithm embedding capacity recovery map distortion distortion information hiding algorithm cover vector map stego vector map root mean square error secret message extraction virtual reproduction human visual system blind detection;object detection	Vector maps have been heavily utilized in many geographic information system (GIS) applications. In such applications, the precision of data is a critical issue. Hence, as we developed a 2D vector map information hiding algorithm, in addition to the embedding capacity, the reversible mechanism and the distortion of the recovery maps were very important factors in our considerations. This paper proposes a reversible, high capacity, and very low distortion information hiding algorithm for 2D vector maps. Our algorithm performs with the embedding capacity of 2(n-2) bits, where n represents the number of vertices in the cover vector map. To the best of our knowledge, this is the highest capacity in the liteature of information hiding for vector maps so far. Our scheme produces a stego vector map with what we believe is the lowest distortion achieved to date, only 0.000012 of the root mean square error. Once the secret message is extracted, our method provides the capacity to leave the map in a recovered form that is a virtual reproduction of the original vector map. The original and the recovered maps have insignificantly small differences, less than 4.79E-09 of the root mean square error, a level imperceptible to the human visual system. Our algorithm belongs to the scheme of blind detection, where the secret message can be extracted without referring to the original cover vector map. The proposed algorithm is also secure and robust against affine transformation. We conclude that our algorithm provides a feasible solution for 2D vector map information hiding with reversibility and very low distortion.	algorithm;authentication;distortion;geographic information system;image scaling;markup language;mean squared error;principal component analysis;robustness (computer science);steganography;vector map	Sheng-Ming Wang;Ching-Sheng Chiu	2012	2012 International Symposium on Intelligent Signal Processing and Communications Systems	10.1109/ISPACS.2012.6473527	computer vision;theoretical computer science;data mining;mathematics	Visualization	39.968642037950296	-10.768612001835953	61087
4450cad187aae9becfc3cbfc9feffbaf0c9e4896	filter bank optimization for high-dimensional compression of pre-stack seismic data	discrete wavelet transforms;microwave integrated circuits;quantization;optimisation;bit rate filter bank optimization high dimensional compression pre stack seismic data multi dimensional variable length subband coder separable near perfect reconstruction filter bank 3 d separable near perfect reconstruction filter bank correlation properties directional dependent autoregressive processes quantization entropy compression efficiency high dimensional filter bank methods 3 d subband coding common shot gathers;variable length codes;image coding;high dimensionality;filter bank;data compression;seismology;filter bank optimization;correlation properties;filter bank image coding data compression discrete wavelet transforms quantization entropy signal to noise ratio image reconstruction microwave integrated circuits autoregressive processes;separable near perfect reconstruction filter bank;autoregressive process;correlation methods;bit rate;high dimensional compression;coding gain;two dimensional digital filters;multi dimensional variable length subband coder;multi dimensional;error statistics geophysical signal processing data compression entropy codes seismology autoregressive processes two dimensional digital filters optimisation variable length codes correlation methods channel bank filters;autoregressive processes;geophysical signal processing;channel bank filters;image reconstruction;entropy codes;directional dependent autoregressive processes;compression ratio;3 d subband coding;error statistics;common shot gathers;3 d separable near perfect reconstruction filter bank;entropy;pre stack seismic data;compression efficiency;signal to noise ratio;high dimensional filter bank methods;perfect reconstruction;subband coding	A multi-dimensional variable-length subband coder for prestack seismic data was presented. A 2and 3-D separable near perfect reconstruction filter bank was optimized to maximize the coding gain, assuming that the correlation properties of pre-stack seismic data can be modeled by directional dependent autoregressive processes. Identical quantization and entropy coder allocation strategies were utilized to isolate the compression efficiency of the different high-dimensional filter bank methods. An example, with compression ratios ranging from 160: 1 to 320: 1, showed that 3-D subband coding of common shot gathers performed 50 % better in terms of bit rate at a given signal-to-noise ratio compared to 2-D subband coding of common shot gathers.	autoregressive model;coding gain;entropy encoding;filter bank;mathematical optimization;reconstruction filter;signal-to-noise ratio;sub-band coding	Tage Røsten;Viktor A. Marthinussen;Tor A. Ramstad;Andrew Perkis	1999		10.1109/ICASSP.1999.757510	data compression;iterative reconstruction;sub-band coding;entropy;mathematical optimization;speech recognition;quantization;computer science;compression ratio;coding gain;filter bank;mathematics;autoregressive model;signal-to-noise ratio;statistics	ML	48.10533562171077	-11.747351488653715	61138
da83d10e58c05a0b791b4bc683e71c710a56c696	high quality image compression using the wavelet transform	image coding;nonzero coefficients;data compression;performance;hvs based processing module;transform coding;adaptive codes;wavelet transforms;wavelet transform;image compression;image coding wavelet transforms multiresolution analysis transform coding wavelet analysis image quality filters image resolution humans wavelet coefficients;high quality image compression;human visual system;image quality;digital filters;compression ratios;compression ratios high quality image compression wavelet transform human visual system performance wavelet filter nonzero coefficients hvs based processing module;compression ratio;digital filters data compression image coding wavelet transforms transform coding adaptive codes;wavelet filter	A simple image compression scheme based on the wavelet transform and properties of the human visual system (HVS) is proposed. The scheme achieves better performance results than those achieved with the well established JPEG compression technique for very high image quality levels. In achieving those results, the wavelet scheme tries to identify the wavelet filter that would result in the smallest number of nonzero coefficients (thus giving a high compression ratio), uses an HVS based processing module to adaptively quantize wavelet coefficients, and efficiently codes the processed wavelet transform. The results show that the scheme outperforms JPEG in terms of the achievable compression ratios, and image quality.		I. Rabinovitch;Anastasios N. Venetsanopoulos	1997		10.1109/ICIP.1997.647760	data compression;wavelet;computer vision;data compression ratio;speech recognition;second-generation wavelet transform;image compression;computer science;compression ratio;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;set partitioning in hierarchical trees;wavelet transform	Vision	43.485058517602184	-15.05505408458271	61147
cd1831d8c4cabd98d87672a84a5103a3bdeb3d8f	reversible video data hiding using neighbouring similarity	hidden information;information security;network technology;neighbouring similarity;digital multimedia distribution applications;reversible video data hiding;video signal processing data encapsulation multimedia computing security of data;video frame;computer technology;reversible video data hiding hidden information video frame information security digital multimedia distribution applications computer technology network technology neighbouring similarity	Recently, digital multimedia has become widely distributed in computer and network technology. For digital multimedia distribution applications, issues surrounding information security have received significant attention. Data hiding has been one of most researched issues in information security. In this paper, reversible video data hiding based on neighbouring similarity is proposed. Prediction encoding was used to compute the prediction errors. All prediction errors were explored to develop a histogram-based reversible video data hiding algorithm. The results show that the proposed approach has a higher capacity and similar embedding distortion compared with other related schemes. Also, the original video frame could be recovered after the hidden information was extracted.		Hsiu-Lien Yeh;Sun Ha Jee;Piyu Tsai;Wei-Kuan Shih	2014	IET Signal Processing	10.1049/iet-spr.2012.0233	video compression picture types;computing;computer science;information security;theoretical computer science;video tracking;video processing;internet privacy;world wide web	OS	39.69800585141185	-12.870913359953956	61190
95b553e7430c7627a70af6e2b5a4bd2c0d4ddbb5	coding and cell-loss recovery in dct-based packet video	compression gain;adaptive interpolation atm networks image coding temporal domain cell loss recovery dct based packet video discrete cosine transform video coding asynchronous transfer mode compression gain system complexity processing delay error concealment capability reconstruction quality joint photographic experts group jpeg motion picture experts group mpeg block interleaving spatial domain dct coefficient segmentation frequency domain;video compression discrete cosine transforms asynchronous transfer mode image reconstruction transform coding frequency domain analysis interpolation image coding delay systems motion pictures;interpolation;transformation cosinus;dct based packet video;image coding;error concealment;image processing;spatial domain;motion pictures;data compression;error concealment capability;cell loss recovery;reconstruction quality;frequency domain analysis;packet loss;block interleaving;packet video;motion picture experts group;video compression;compresion senal;procesamiento imagen;segmentation;transform coding;traitement image;discrete cosine transform;compression signal;atm networks;transmision asincronica;packet loss rate;algorithme;algorithm;video coding;codificacion;senal video;mode transmission;signal video;jpeg;discrete cosine transforms;image reconstruction;processing delay;signal compression;joint photographic experts group;transformacion coseno;coding;transmission mode;joint photographic expert group;temporal domain;video signal;asynchronous transmission;delay systems;transmission asynchrone;mpeg;video signals;dct coefficient segmentation;cosine transform;frequency domain;modo transmision;segmentacion;system complexity;asynchronous transfer mode;video signals asynchronous transfer mode data compression discrete cosine transforms image coding image reconstruction;codage;adaptive interpolation;algoritmo	This paper considers the applications of DCT-based imageand video-coding methods in the asynchronous transfer mode (ATM) environment. Coding and reconstruction mechanisms are jointly designed to achieve a good compromise among compression gain, system complexity, processing delay, errorconcealment capability, and reconstruction quality. The Joint Photographic Experts Group (JPEG) and Motion Picture Experts Group (MPEG) algorithms for image and video mmpression are modified to incorporate block interleaving in the spatial domain and discrete cosine transform (DCT) coefficient segmentation in the frequency domain to conceal the errors due to packet loss. A new algorithm is developed that recovers the damaged regions by adaptive interpolation in the spatial, temporal, and frequency domains. The weights used for spatial and temporal interpolations are varied according to the motion content and loss patterns of the damaged regions. When combined with proper layered transmission, the proposed coding and reconstruction methods can handle very high packet-loss rates at only slight cost of compression gain, system complexity, and processing delay.	atm turbo;adaptive filter;algorithm;coefficient;discrete cosine transform;forward error correction;interpolation;jpeg;moving picture experts group;network packet;processing delay	Qin-Fan Zhu;Yao Wang;Leonard Shaw	1993	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.224235	computer vision;telecommunications;image processing;computer science;jpeg;algorithm;statistics	Graphics	46.57685332480104	-14.55751249895049	61411
92240ccf28f784d2e2d5d3fd1bcdaa278c214bfb	utilizing soft information in image decoding	wireless channels;image coding;iterative decoding;image coding soft information utilization jpeg2000 image decoder image quality wireless channel iterative decoding;iterative decoding image coding;community networks;image quality;datavetenskap datalogi;datavetenskap;computer science;wireless ip;delay iterative decoding image quality forward error correction physical layer computer errors intelligent networks computer science communication networks application software;source coding	This paper explores the utilization of soft information in image decoding. Error free transmission is not always possible in today's nor future's communication networks without retransmissions. Retransmissions, however, introduce delay. To delay sensitive applications the concept of soft information, i.e. lower layer knowledge about the channel conditions, could reduce the number of retransmissions. It is not sufficient to treat the soft information and the image decoder separately. By combining soft information and the knowledge of the image structure in the decoding process image quality can be improved. A modified JPEG2000 image decoder that utilizes soft information has been developed. Experimental results with images transmitted over a simulated wireless channel show that iterative decoding with soft information can give high gains in image quality.	image quality;iterative method;jpeg 2000;system image;telecommunications network	Hannes Persson;Anna Brunstrom;Tony Ottosson	2003		10.1109/PIMRC.2003.1259218	image quality;list decoding;telecommunications;computer science;theoretical computer science;source code	AI	48.838971645765675	-15.406566815638524	61426
f63c859934bdc7e7b3179c465ef1c3881a47464b	mechatronic prototype of parabolic solar tracker	architectural integration;infrared thermography;concentrating solar thermal power;energetic simulation;mechatronic solar tracker;parabolic	In the last 30 years numerous attempts have been made to improve the efficiency of the parabolic collectors in the electric power production, although most of the studies have focused on the industrial production of thermoelectric power. This research focuses on the application of this concentrating solar thermal power in the unexplored field of building construction. To that end, a mechatronic prototype of a hybrid paraboloidal and cylindrical-parabolic tracker based on the Arduido technology has been designed. The prototype is able to measure meteorological data autonomously in order to quantify the energy potential of any location. In this way, it is possible to reliably model real commercial equipment behavior before its deployment in buildings and single family houses.	cell hybridization;deploy;garbage collection (computer science);laser therapy, low-level;mechatronics;parabolic antenna;prototype;solar tracker	Carlos Morón;Jorge Pablo Díaz;Daniel Ferrández;Mari Paz Ramos	2016		10.3390/s16060882	control engineering;electronic engineering;simulation;parabola;engineering	Mobile	52.76147893129282	-12.7857080990883	61594
03621008ca51382c8e43e78f8beaaa87690c3b34	a concealment based approach to distributed video coding	fmo type macroblock interleaving;mode selection distributed video coding transform domain correlation noise estimation modified b frame quantisation concealment based approach hybrid key wyner ziv frame fmo type macroblock interleaving key sequence i b p frame structure video decoder enhancement;noise estimation;video decoder enhancement;concealment data compression dvc;metadata;data compression;decoding;hybrid key wyner ziv frame;correlation noise estimation;correlation methods;i b p frame structure;mode selection;transform domain;key sequence;video coding;transforms;dvc;h 264;distributed video coding;image denoising;modified b frame quantisation;side information;video coding block codes correlation methods data compression decoding image denoising image sequences transforms;concealment based approach;mode decision;concealment;block codes;mpeg 7;image sequences	This paper presents a concealment based approach to distributed video coding that uses hybrid key/WZ frames via an FMO type interleaving of macroblocks. Our motivation stems from a previous work of ours that showed promising results relative to the more common approach of splitting the sequence in key and WZ frames. In this paper, we extend our previous scheme to the case of I-B-P frame structures and transform domain DVC. We additionally introduce a number of enhancements at the decoder including use of spatio-temporal concealment for generating the side information on a MB basis, mode selection for switching between the two concealment approaches and for deciding how the correlation noise is estimated, local (MB wise) correlation noise estimation and modified B frame quantisation. The results presented indicate considerable improvement (up to 30%) compared to corresponding frame extrapolation and frame interpolation schemes.	data compression;extrapolation;flexible macroblock ordering;forward error correction;frame (networking);motion interpolation;quantization (physics);video compression picture types;winzip	Nantheera Anantrasirichai;Dimitris Agrafiotis;David R. Bull	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712234	data compression;block code;speech recognition;telecommunications;computer science;theoretical computer science;mathematics;metadata;statistics	Vision	45.501014734461116	-18.06487822386521	61606
b873680d0baf08a7f25e47fb2af29e3fa4c5a350	efficient motion vector recovery algorithm for h.264 based on a polynomial model	mesure deplacement;modelo markov oculto;estimation mouvement;audiovisual;traitement parole audiovisuelle;image segmentation;error concealment;standards;motion compensation;modele markov cache;combinaison classificateur;hidden markov model;speech processing;estimacion movimiento;h 264 coding standard;tratamiento palabra;traitement parole;motion estimation;video sequences;testing;correlation methods;indexing terms;h 264 error concealment;polynomials;video coding;correlation methods motion estimation standards video coding vectors image reconstruction motion compensation image segmentation image sequences;mpeg 4 standard;reconocimiento voz;internet;displacement measurement;shape;vectors;polynomials motion estimation shape error correction video sequences video coding predictive models testing internet mpeg 4 standard;audiovisuel;motion vector;error correction;image reconstruction;image segmentation efficient motion vector recovery algorithm h 264 coding standard polynomial model motion estimation scheme video sequences image reconstruction motion compensation;image sequence;h 264;analyse en profondeur;polynomial model;speech recognition;predictive models;secuencia imagen;medicion desplazamiento;reconnaissance parole;sequence image;efficient motion vector recovery algorithm;image sequences;motion estimation scheme	In this paper, we propose an efficient motion vector recovery algorithm for the new coding standard H.264, which is based on a polynomial model. To achieve better coding efficiency, the motion estimation scheme used in H.264 is different from previous coding standards. In H.264, a 16/spl times/16 macroblock can be divided into different block shapes for motion estimation. Each macroblock contains more motion vectors than previous coding standards. For nature video, the blocks within a small area likely belong to the same object, hence the motion vectors of neighboring blocks are highly correlated. Based on the correlation of neighboring motion vectors, we can use the motion vectors that are adjacent to the lost motion vectors to constitute a polynomial model, which can describe the change tendency of motion vectors within a small area. Through this model, the lost motion vectors can be predicted and the lost macroblocks can be reconstructed. Different video sequences are used to test the performance of proposed method. The simulation results show that the quality of corrupted video can be obviously improved by proposed algorithm.	algorithmic efficiency;approximation algorithm;computation;error concealment;h.264/mpeg-4 avc;macroblock;motion estimation;polynomial;real-time clock;real-time computing;simulation	Jinghong Zheng;Lap-Pui Chau	2005	IEEE Transactions on Multimedia	10.1109/TMM.2005.843343	iterative reconstruction;computer vision;polynomial and rational function modeling;the internet;error detection and correction;speech recognition;index term;shape;quarter-pixel motion;computer science;machine learning;coding tree unit;motion estimation;speech processing;mathematics;block-matching algorithm;predictive modelling;software testing;image segmentation;motion field;motion compensation;hidden markov model;polynomial;computer graphics (images)	Vision	47.38941894893543	-15.639490590477697	61661
01ee6119ba8bd622cd8b84ebb58ffd002d323784	on the comparison of audio fingerprints for extracting quality parameters of compressed audio	databases;filigranage numerique;protection information;digital watermarking;base donnee;image processing;metadata;operant conditioning;debit information;information transmission;estudio comparativo;song identification;database;procesamiento imagen;base dato;audio fingerprint;comparison;indice informacion;traitement image;extraccion parametro;parameter extraction;algorithme;etude comparative;algorithm;extraction parametre;monitoring;proteccion informacion;fingerprint recognition;information protection;filigrana digital;comparative study;metadonnee;audio hash;information rate;algorithms;robustness;metadatos;monitorage;transmision informacion;compression;transmission information;monitoreo;algoritmo;audio watermarking	Audio fingerprints can be seen as hashes of the perceptual content of an audio excerpt. Applications include linking metadata to unlabeled audio, watermark support, and broadcast monitoring. Existing systems identify a song by comparing its fingerprint to pre-computed fingerprints in a database. Small changes of the audio induce small differences in the fingerprint. The song is identified if these fingerprint differences are small enough. In addition, we found that distances between fingerprints of the original and a compressed version can be used to estimate the quality (bitrate) of the compressed version. In this paper, we study the relationship between compression bit-rate and fingerprint differences. We present a comparative study of the response to compression using three fingerprint algorithms (each representative for a larger set of algorithms), developed at Philips, Polytechnic University of Milan, and Microsoft, respectively. We have conducted experiments both using the original algorithms and using versions modified to achieve similar operation conditions, i.e., the fingerprints use the same number of bits per second. Our study shows similar behavior for these three algorithms.	algorithm;audio signal processing;cryptographic hash function;data rate units;experiment;fingerprint;precomputation	Peter Jan O. Doets;M. Menor Gisbert;Reginald L. Lagendijk	2006		10.1117/12.642968	fingerprint verification competition;speech recognition;telecommunications;image processing;digital watermarking;computer science;comparative research;operant conditioning;metadata;world wide web;compression;information protection policy;fingerprint recognition;robustness	HCI	44.07499469837888	-10.311306947740942	61823
1325ec3f60e5078ce36aba1f467233cd31714d29	reduced-delay selective arq for low bit-rate image and multimedia data transmission	low bit rate image transmission;packet switching automatic repeat request multimedia communication visual communication;image coding;psnr;packet loss;visual communication;testing;packet switching;data engineering;data communication;similarity check function;explicit knowledge;reduced delay selective arq;packet loss reduced delay selective arq low bit rate image transmission low bit rate multimedia data transmission similarity check function;protection;image reconstruction;automatic repeat request data communication protection image coding throughput image reconstruction testing psnr computer science data engineering;multimedia data;multimedia communication;low bit rate multimedia data transmission;computer science;automatic repeat request;throughput	The paper presents a reduced-delay selective-ARQ scheme based on a similarity check function that measures the degree of corruption in transmitted multimedia data packets. Accordingly, if the packet is found to be badly corrupted based on a specified similarity criterion, it can be considered as a lost packet and retransmitted. The degree of corruption contributed by a particular packet is measured by the proposed similarity check function at the receiver without explicit knowledge of the original source data. Simulation results and comparisons with a conventional ARQ scheme are provided to illustrate the performance of the proposed method.	network packet;simulation;source data	Tuyet-Trang Lam;Lina J. Karam;Rida A. Bazzi;Glen P. Abousleman	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415403	iterative reconstruction;throughput;information engineering;selective repeat arq;peak signal-to-noise ratio;telecommunications;computer science;explicit knowledge;theoretical computer science;software testing;packet loss;automatic repeat request;packet switching;visual communication;computer network	Visualization	48.96365974774521	-15.33062366885939	61991
71737d699eb084d31886a92abcb7d3264c11248c	multi step motion estimation algorithm	diamond search;full search;three step search;our step search;motion estimation;motion vector;most probable point;optimal algorithm;block matching algorithm	We propose a multi step motion estimation algorithm (MSME) that encompasses techniques such as motion vector prediction through initial search, refinement of motion vector to locate true motion vector and early termination criteria that suits to all type of video characteristic. This approach allows us to exploit random distribution of motion vector in successive video frames from which the initial candidate predictors are derived. The derived predictors are the most probable points in search window, which will assure that, the motion vectors in the vicinity of center point and at the edge of the search window does not miss out, as it does for earlier algorithms like Three step search(TSS), Four step search(FSS), Diamond(DS), etc and refinement stage used in the algorithm will allow us to extract true motion vector so that the picture quality is as good as Full search(FS) which is the optimal algorithm. The novelty of the proposed MSME algorithm is that the search pattern derived is not static but can dynamically shrink or enlarge to account for small and large motion. Fixed threshold used improves speed without sacrificing the quality of video. The Simulation result shows that our proposed algorithm outperforms all sub-optimal algorithms in terms of quality and speed up performance and in many cases PSNR of proposed algorithm is comparable to Full Search.	algorithm;iso 10303;image quality;motion estimation;peak signal-to-noise ratio;refinement (computing);simulation;window function	Deepak J. Jayaswal;Mukesh A. Zaveri;R. E. Chaudhari	2010		10.1145/1741906.1742012	interpolation search;beam search;mathematical optimization;simulation;quarter-pixel motion;theoretical computer science;motion estimation;jump search;mathematics;block-matching algorithm;best-first search;binary search algorithm;search algorithm	Vision	48.619433570621645	-19.354862287423305	62301
aaf4ea225cced5df13f537dd16e364a36a772479	embedded zerotree based image coding with low decoding complexity using linear and morphological filter banks	image coding decoding filter bank image reconstruction bit rate computational efficiency;embedded zerotree wavelet;quantization;mathematical morphology;asymmetrical filter banks;image coding;filter bank;decoding;band pass filters;synthesis stage;embedded zerotree wavelet algorithm;bit rate;visual quality;linear filter banks;linear filtering;high compression ratios;wavelet transforms;distortion;morphological subband decomposition;reconstructed images;streaming media;low bit rate image coding;computational complexity;ringing effect;image reconstruction;computational complexity image coding decoding filtering theory mathematical morphology band pass filters image reconstruction wavelet transforms;low decoding complexity;synthesis stage embedded zerotree based image coding low decoding complexity morphological filter banks linear filter banks low bit rate image coding embedded zerotree wavelet algorithm high compression ratios reconstructed images visual quality coded pictures gibbs phenomenon distortion ringing effect asymmetrical filter banks morphological subband decomposition embedded bit stream computational cost reduction decoding;embedded zerotree based image coding;morphological filter banks;compression ratio;signal processing algorithms;computational efficiency;computational cost reduction;filtering theory;gibbs phenomenon;coded pictures;embedded bit stream	"""The problem of low bit rate image coding is addressed. The proposed approach is based on the embedded zerotree wavelet (EZW) algorithm. At high compression ratios, visually annoying artifacts appear in the reconstructed images using the original EZW algorithm. Hence, the first goal of this paper is to improve the visual quality of the coded pictures. The main distortion is due to the Gibbs phenomenon of the linear filter bank and appears as """"ringing effect"""" in the reconstructed images. The ringing effect is shown to be considerably reduced by using asymmetrical filter banks. It can even be completely removed by means of a morphological subband decomposition. The major drawback of the morphological filter bank is the loss of textures at high compression factors. The second objective is to make an optimal use of the embedded bit stream by reducing the computational cost of the decoding process. A new decomposition is proposed which reduces drastically the computational load of the synthesis stage."""	filter bank;mathematical morphology	Olivier Egger;André Nicoulin;Wei Li	1995		10.1109/ICASSP.1995.479922	iterative reconstruction;gibbs phenomenon;computer vision;mathematical morphology;distortion;quantization;computer science;theoretical computer science;compression ratio;linear filter;filter bank;mathematics;band-pass filter;computational complexity theory;wavelet transform	Vision	44.83098547694975	-15.527342067116882	62372
7f1bc99658fe3954e167164c197de98af073a17c	linear programming optimization for video coding under multiple constraints	optimal solution;rate distortion;piecewise linear;piecewise linear approximation;linearity;piecewise linear techniques video coding linear programming minimax techniques variable rate codes rate distortion theory;psnr;constraint optimization;decoding;piecewise linear techniques;frame level distortion;convex rate distortion function;vbr buffer constraint;linear programming optimization;mpeg 2 framework;rate distortion theory;variable rate codes;rate control;video coding;minimax techniques;filtered distortion constraint;multiple constraints;linear programming constraint optimization video coding decoding minimax techniques rate distortion piecewise linear techniques linearity psnr piecewise linear approximation;macroblock level rate control;0 5 to 2 0 db;minmax optimization procedure;linear programming;linear program;0 5 to 2 0 db linear programming optimization lp technique video coding convex rate distortion function piecewise linear technique minmax optimization procedure vbr buffer constraint mpeg 2 framework macroblock level rate control psnr frame level distortion filtered distortion constraint;piecewise linear technique;lp technique	This paper introduces a linear programming (LP) technique that performs ratedistortion based optimization for constrained video coding. Given the assumption of piecewise linear and convex rate-distortion functions, LP is used to generate optimal solutions for the minmax problem under VBR buffer constraints. The LP solution for this problem avoids an explicit search for the minimum distortion value. To verify the linearity and convexity assumptions, this minmax optimization procedure is applied to the MPEG-2 framework with macroblock level rate control under VBR buffer constraints. The results show an improvement in average PSNR of approximately 0.5 dB 2.0 dB over the initial TM5 solution, while maintaining an almost constant PSNR level. As a second LP application, a minimum rate problem with constraints on linear combinations of frame-level distortions is considered. Using the previous assumptions on the rate-distortion functions, it is proven that the LP solution is optimal. The LP technique yields a computationally efficient solution for this filtered distortion problem.	algorithm;algorithmic efficiency;convex function;data compression;distortion;iteration;linear programming;mpeg-2;macroblock;mathematical optimization;minimax;peak signal-to-noise ratio;piecewise linear continuation;rate–distortion theory;simulation;volume boot record	Yegnaswamy Sermadevi;Sheila S. Hemami	2003		10.1109/DCC.2003.1193996	mathematical optimization;combinatorics;discrete mathematics;rate–distortion theory;piecewise linear function;peak signal-to-noise ratio;linear programming;mathematics;linearity	EDA	47.34039172850881	-16.372710492360948	62373
7902d1427c875a590dd021f5200dc458bddf2109	compression of medical images by using artificial neural networks	medical imagery;image processing;data compression;lossy compression;multilayer perceptrons;transformation cosinus discrete;metodo imagen;procesamiento imagen;intelligence artificielle;low pass filter;huffman codes;traitement image;discrete cosine transform;kohonen algorithm;filtro paso bajo;perceptron multicouche;huffman code;algoritmo kohonen;compression image;red multinivel;filtre passe bas;medical image;image compression;algorithme kohonen;codigo huffman;discrete cosine transforms;pixel;code huffman;image method;imagineria medica;imagerie medicale;autoorganizacion;artificial intelligence;self organization;kohonen map;self organized map;multi layer perceptron;compresion dato;inteligencia artificial;multilayer network;reseau multicouche;reseau neuronal;methode image;red neuronal;autoorganisation;compression donnee;artificial neural network;neural network;compresion imagen	This paper presents a novel lossy compression scheme for medical images by using an incremental self–organized map (ISOM). Three neural networks for lossy compression scheme are comparatively examined: Kohonen map, multi-layer perceptron (MLP) and ISOM. In the compression process of the proposed method, the image is first decomposed into blocks of 8(8 pixels. Two-dimensional discrete cosine transform (2D-DCT) coefficients are computed for each block. The dimension of DCT coefficients vectors (codewords) is reduced by low-pass filtering. Huffman coding is applied to the indexes of codewords obtained by the ISOM. In the decompression process, inverse operations of each stage of the compression are performed in the opposite way. It is observed that the proposed method gives much better compression rates.	artificial neural network;neural networks	Zümray Dokur	2006		10.1007/11816157_37	data compression;color cell compression;data compression ratio;block truncation coding;image compression;computer science;artificial intelligence;machine learning;lossless compression;texture compression;artificial neural network;algorithm;huffman coding	ML	45.56539327816816	-12.915551121050974	62380
66812fb975923002c119ba61ba00a763e177e3d3	spatio-temporal video transcoder for streaming over mobile communications networks	tecnologia electronica telecomunicaciones;streaming;low bit rate;mobile radiocommunication;transcodage;resolution spatiale;transcodificacion;resolucion espacial;ring resonator;video signal processing;telecommunication sans fil;information transmission;canal transmision;downsizing;exigence usager;exigencia usuario;metodo imagen;technique video;telecommunication network;resolucion temporal;resonador anillo;refinement;resolution temporelle;tecnica video;communication service mobile;radiocommunication service mobile;mobile communication network;qualite service;velocidad de bit debil;refinement method;video coding;video transcoding;saut trame;transmission en continu;senal video;signal video;codage video;canal transmission;transmission channel;user requirement;red telecomunicacion;telecomunicacion sin hilo;image method;mobile communication;reseau telecommunication;traitement signal video;video signal;video technique;transmision fluyente;transmision informacion;methode raffinement;tecnologias;transmission information;grupo a;radiocomunicacion servicio movil;methode image;transcoding;time resolution;metodo afinamiento;transcoder;debit binaire faible;service quality;resonateur anneau;calidad servicio;frame skipping;wireless telecommunication;spatial resolution	Video transcoding technique is an efficient mechanism to deliver visual contents to a variety of users who have different network conditions or terminal devices with different display capabilities. In this paper, we propose two types of transcoding methods for adapting the bitrate of streaming video to the bandwidth of the transmission channel; spatial resolution reduction (SRR) transcoding and temporal resolution reduction (TRR) transcoding. The two transcoding methods are alternatively operated according to the requirements of users. Experimental results show that the proposed transcoding methods can preserve image quality while transcoding to the low bitrate.		Jae-Won Kim;Goo-Rak Kwon;June-Sok Lee;Nam-Hyeong Kim;Sung-Jea Ko	2006	IEICE Transactions	10.1093/ietcom/e89-b.10.2678	embedded system;transcoding;telecommunications;computer science;operating system;video post-processing;computer graphics (images)	Mobile	47.27411530682512	-14.589090836520494	62635
72401f0292a04f48d45c2d8e61e1d974323fdcca	reduction of gray level disturbances in plasma display panels	plasma displays;driving vectors;plasma displays pulse modulation image quality tv costs eyes motion estimation cathode ray tubes image sequences computer displays;motion estimation;plasma display panel;vectors plasma displays image sequences;gray level disturbances;eyes;vectors;disturbances removal;image quality;image sequence;computer displays;plasma display panels;tv;moving image sequences;optimal subfield;cathode ray tubes;pulse modulation;image sequences;image quality gray level disturbances plasma display panels moving image sequences disturbances removal optimal subfield driving vectors	An effective method to reduce gray level disturbances in plasma display panels (PDPs) is proposed in this work. First, we develop a systematic model for gray level disturbances, which occur when PDPs display moving image sequences. Then, we derive an ideal condition for the disturbances removal. Based on the condition, we propose the optimal subfield and driving vectors to minimize the disturbances. Simulation results show that the proposed algorithm provides a good moving image quality.	algorithm;effective method;grayscale;image quality;plasma display;simulation	Chang-Su Kim;Sang Uk Lee	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)	10.1109/ISCAS.2004.1328903	image quality;cathode ray tube;computer vision;electronic engineering;computer science;motion estimation;pulse-width modulation;computer graphics (images)	EDA	42.55566096860369	-17.819631567821467	62678
4debb190ff2c29e83378183073a200ffcf0e407a	global motion temporal filtering for in-loop deblocking	filtering;rate distortion;global motion temporal filtering;h 264 avc global motion temporal filtering in loop deblocking hybrid video coding motion compensated prediction mcp blocking artifact reduction;motion compensation;deblocking;h 264 avc;information filtering;bit rate;temporal filtering;video coding information filtering motion compensation prediction theory;video coding;in loop deblocking;automatic voltage control;prediction theory;motion compensated prediction h 264 avc video coding deblocking temporal filtering;streaming media;automatic voltage control video coding bit rate entropy motion compensation filtering streaming media;blocking artifact reduction;mcp;spatial filtering;entropy;experimental evaluation;motion compensated prediction;global motion;blocking artifact;hybrid video coding	One of the most severe problems in hybrid video coding is its block-based approach, which leads to distortions called blocking artifacts. These artifacts affect not only the subjective perception at the receiver but also the motion compensated prediction (MCP) that generates a prediction signal from previously decoded pictures. It is therefore directly connected to the amount of data that has to be transmitted. In this paper, we propose a technique called global motion temporal filtering for blocking artifact reduction. Other than common deblocking techniques, this approach does not reduce the blocking artifacts spatially. Filtering is performed temporally using a set of neighboring pictures from the picture buffer. This approach is incorporated into an H.264/AVC reference software. Experimental evaluation shows that the proposed technique significantly improves the quality in terms of rate-distortion performance.	artifact (software development);blocking (computing);data compression;deblocking filter;distortion;h.264/mpeg-4 avc;temporal logic	Alexander Glantz;Andreas Krutz;Thomas Sikora	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5650779	filter;computer vision;entropy;real-time computing;computer science;deblocking filter;theoretical computer science;motion compensation;spatial filter	Robotics	46.0967681298292	-17.573686157671727	62748
133a792ae4166984a1116eeefc796304984e22de	secret communication using jpeg double compression	data hiding;image coding;encryption;hidden data;quality factor;transform coding;quality factor data hiding jpeg;jpeg image;jpeg double compression;steganography;data privacy;jpeg;secret communication;discrete cosine transforms;cryptography;pixel;steganography techniques;telecommunication security;transform coding pixel image coding encryption q factor discrete cosine transforms;jpeg quantization table;telecommunication security cryptography data privacy image coding steganography;jpeg quantization table secret communication jpeg double compression cryptography steganography techniques hidden data jpeg image;q factor	Protecting privacy for exchanging information through the media has been a topic researched by many people. Up to now, cryptography has always had its ultimate role in protecting the secrecy between the sender and the intended receiver. However, nowadays steganography techniques are used increasingly besides cryptography to add more protective layer to the hidden data. In this letter, we show that the quality factor in a JPEG image can be an embedding space, and we discuss the ability of embedding a message to a JPEG image by managing JPEG quantization tables (QTs). In combination with some permutation algorithms, this scheme can be used as a tool for secret communication. The proposed method can achieve satisfactory decoded results with this straightforward JPEG double compression strategy.	algorithm;code;color quantization;cryptography;discrete cosine transform;encryption;jpeg;key (cryptography);privacy;steganalysis;steganography	Jing-Ming Guo;Thanh-Nam Le	2010	IEEE Signal Processing Letters	10.1109/LSP.2010.2066110	lossless jpeg;information privacy;computer science;theoretical computer science;jpeg;internet privacy;q factor;computer security;statistics	Security	40.019624425511395	-11.576823081823926	62924
247d5b677d7ae9f601952cbb6a34941c1f4cea59	a novel distributed compressive video sensing based on hybrid sparse basis	block prediction;temporal correlation distributed compressive video sensing video codec distributed video coding compressive sensing mpeg 4 h 264 image block prediction dct basis adaptive block based prediction;distributed compressed video sensing;sparse basis;compressive sensing;discrete cosine transforms sensors dictionaries silicon image reconstruction proposals encoding;block prediction compressive sensing distributed compressed video sensing sparse basis;video codecs compressed sensing discrete cosine transforms	Distributed compressive video sensing (DCVS) is a new emerging video codec that incorporates advantages of distributed video coding (DVC) and compressive sensing (CS). However, due to the absence of a good sparse basis, the DCVS does not achieve ideal compressing efficiency compared with the traditional video codec, such as MPEG-4, H.264, etc. This paper proposes a new hybrid sparse basis, which combines the image-block prediction and DCT basis. Adaptive block-based prediction is employed to learn block-prediction basis by exploiting temporal correlation among successive frames. Based on linear DCT basis and predicted basis, the hybrid sparse basis can achieve sparser representation with lower complexity. The experiment results indicate that the proposal outperforms the state-of-the-art DCVS schemes on both visual quality and average PSNR. In addition, an iterative fashion proposed in the decoder can enhance the sparsity of the hybrid sparse basis and improve the rate-distortion performance significantly.	codec;compressed sensing;data compression;discrete cosine transform;distortion;distributed concurrent versions system;h.264/mpeg-4 avc;iterative method;key frame;peak signal-to-noise ratio;real-time clock;sparse matrix	Haifeng Dong;Bojin Zhuang;Fei Su;Zhi-Cheng Zhao	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051569	computer vision;computer science;theoretical computer science;machine learning;compressed sensing	Vision	49.11858406432284	-17.566960979210428	63152
91f7cec15630991cf691651c5bffb40e993a2154	real-time lossless compression of mosaic video sequences	traitement signal;evaluation performance;image coding;performance evaluation;image processing;data compression;motion compensation;interframe encoding;video signal processing;debit information;information transmission;real time;evaluacion prestacion;video compression;procesamiento imagen;lossless compression;indice informacion;traitement image;video codec;motion compensated;compensation mouvement;codage predictif;codage image;video coding;compression image;senal video;signal video;image compression;codage video;relacion compresion;signal processing;image sequence;codificacion predictiva;traitement signal video;information rate;video signal;compression ratio;codec;secuencia imagen;compression sans perte;compresion dato;transmision informacion;taux compression;transmission information;procesamiento senal;codage interimage;codificacion entre imagen;predictive coding;compresion sin perdida;sequence image;compression donnee;compresion imagen	This paper presents a simple, fast coding technique for lossless compression of mosaic video data. The design of a video codec needs to strike a balance between the compression performance and the codec throughput. Aiming to make the encoding throughput high enough for real-time lossless video compression, we propose a hybrid scheme of inter and intraframe coding. Interframe predictive coding is invoked only when the motion between adjacent frames is modest and a simple motion compensation operation can significantly improve the compression performance. Otherwise, still frame compression is performed to keep the complexity low. Experimental results show that the proposed scheme achieves higher lossless video compression ratio than existing methods such as JPEG-LS and JPEG-2000. r 2005 Elsevier Ltd. All rights reserved.	algorithmic efficiency;codec;data compression;intra-frame coding;jpeg 2000;lossless compression;motion compensation;motion estimation;real-time transcription;throughput;uncompressed video	Lei Zhang;Xiaolin Wu;Paul Bao	2005	Real-Time Imaging	10.1016/j.rti.2005.07.001	video compression picture types;data compression;lossy compression;inter frame;lossless jpeg;computer vision;data compression ratio;telecommunications;image processing;image compression;quarter-pixel motion;computer science;context-adaptive variable-length coding;signal processing;lossless compression;block-matching algorithm;adaptive coding;context-adaptive binary arithmetic coding;motion compensation;h.261;algorithm;multiview video coding	Vision	46.86321401634986	-14.906520969140779	63165
693e228de9f563a6904a8f043e7609ad9e14fd63	programmable deblocking filter architecture for a vc-1 video decoder	cuantificacion senal;parallelisme;circuit decodeur;video coding entropy filtering theory quantisation signal;quantization;filtering;evaluation performance;filtrage;quantum effect;entropia;image coding;performance evaluation;video filtering deblocking firmware video coding;decoding;vc 1 video decoder;registro rtl;real time requirements vc 1 video decoder programmable deblocking filter quantization entropy encoding interblock correlation coding performance image quality register transfer level;video signal processing;deblocking;implementation;real time;evaluacion prestacion;video filtering;interblock correlation;filtrado;filters;flux donnee;flujo datos;firmware;coding performance;effet quantique;qualite image;circuito desciframiento;algorithme;quantisation signal;algorithm;filtre programmable;video coding;decoding circuit;parallelism;performance improvement;mpeg 4 standard;automatic voltage control;paralelismo;signal quantization;codage video;efecto bloque;image quality;quantification signal;temps reel;entropie;niveau transfert registre;effet bloc;traitement signal video;tiempo real;programmable filters;filters decoding mpeg 4 standard quantization automatic voltage control entropy encoding filtering image coding performance loss;calidad imagen;entropy;real time requirements;data flow;implementacion;efecto cuantico;entropy encoding;encoding;register transfer level;performance loss;filtering theory;blocking artifact;programmable deblocking filter;model simulation;algoritmo	Although the current standards (MPEG1, MPEG2, MPEG4, H.261, H.263, and H.264/MPEG4 AVC) as well as the recent VC-1, present the same basic functional elements, i.e., prediction, transformation, quantization, and entropy encoding, important changes occur in the details of each functional element. One of the details concerns the elimination of the loss in interblock correlation due to block-based prediction, transformation, and quantization. In order to overcome the loss in blocking artifacts, a deblocking filtering method is necessary to maximize coding performance and consequently improve image quality. This letter describes a programmable VC-1 deblocking filter architecture with capabilities to support different standards. The architecture has been modeled, simulated, and implemented at the register transfer level. Results show a threefold performance improvement as compared to solutions where filtering algorithms are otherwise not hardwired. Results also point to parallelism based on existing data flow, and show that real-time requirements can be met.	algorithm;artifact (software development);automatic parallelization;blocking (computing);computer programming;dataflow;deblocking filter;entropy encoding;functional genomics;h.264/mpeg-4 avc;image quality;interlaced video;intra-frame coding;mpeg-1;mpeg-2;megabyte;microcode;parallel computing;random-access memory;real-time clock;register-transfer level;requirement;run time (program lifecycle phase);texture filtering;throughput;video compression picture types;video decoder	Ricardo Citro;Miguel Guerrero;Jae-Beom Lee;Maria Pantoja	2009	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2009.2022699	entropy;real-time computing;telecommunications;computer science;deblocking filter;algorithm	Visualization	46.790235878117606	-14.790043520759594	63327
99b94a5e6a178fe674d40ed071c69720b6dc3dce	perceptual distortion modeling for side-by-side 3d video delivery	digital video broadcasting;streaming media computational modeling degradation three dimensional displays packet loss;three dimensional television;quality of experience;stereo image processing;visual perception;perceptual distortion modeling quality of experience unequal error protection strategies human visual system perceptual features frame level distortion model side by side 3d video delivery systems;visual perception digital video broadcasting quality of experience stereo image processing three dimensional television	A frame-level distortion model based on perceptual features of the human visual system is proposed to improve the performance of unequal error protection strategies and provide better quality of experience to users in Side-by-Side 3D video delivery systems.	distortion;human visual system model	César Diaz;Jesús Gutiérrez-Cillán;Julián Cabrera;Fernando Jaureguizar;Narciso N. García	2014	2014 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2014.6776066	subjective video quality;computer vision;simulation;telecommunications;visual perception;computer science;video quality;multimedia;video processing;digital video broadcasting	Robotics	44.369307546039	-19.904420824809137	63400
376323e6f559a74c8006c99ea540abc652a48dba	an improved distributed video coding with low-complexity motion estimation at encoder	decoding motion estimation complexity theory video coding entropy coding parity check codes;video coding computational complexity data compression decoding motion estimation redundancy;wyner ziv codec;distributed video coding;low complexity motion estimation distributed video coding wyner ziv codec;rate distortion performance improved distributed video coding low complexity motion estimation dvc state of the art video compression standard low complexity encoder redundant decoding time skip mode decoding time reduction computational complexity;low complexity motion estimation	Distributed Video Coding (DVC) is a novel scheme which is different from the state-of-the-art video compression standards. A specific characteristic of DVC is its use of a low complexity encoder. However, there exists an issue on the DVC, which requires redundant decoding time. Therefore, low-complexity motion estimation algorithm and skip mode have been applied to the encoder of a hybrid DVC to reduce half of the decoding time. The computational complexity of the encoder only increases slightly with respect to the hybrid DVC, which is still far less than H.264/AVC no motion. In some cases, the encoding time is even reduced instead of being increased. Besides, the rate-distortion performance is improved in high motion videos. However, the rate-distortion performance does not decrease in the static video, because of the added motion estimation.	algorithm;computational complexity theory;data compression;distortion;encoder;h.264/mpeg-4 avc;motion estimation;video coding format	Hsin-Ping Yang;Hsiao-Chi Hsieh;Sheng-Hsiang Chang;Sao-Jie Chen	2015	2015 28th IEEE International System-on-Chip Conference (SOCC)	10.1109/SOCC.2015.7406923	computer vision;real-time computing;quarter-pixel motion;computer science;theoretical computer science;coding tree unit;block-matching algorithm;context-adaptive binary arithmetic coding;motion compensation;h.261;multiview video coding	Vision	46.91363354785107	-18.192960627906782	63412
2560849a0edf0cd624f25ed24caf45de7b988643	impact of rate control tools on very fast non-embedded wavelet image encoders	transformation ondelette;estensibilidad;systeme temps reel;sistema interactivo;partition method;mobile radiocommunication;personal digital assistant;hierarchized structure;flow rate regulation;telephone portable;cell phones;digital camera;structure hierarchisee;radiocommunication service mobile;assistant numerique personnel;systeme conversationnel;mobile phone;personal digital assistants;digital cameras;computer programming;rate control;data storage;codificacion;telefono movil;methode partition;regulation debit;interactive system;geographic information systems;coding;working memory;jpeg2000;real time system;metodo particion;sistema tiempo real;temps retard;extensibilite;scalability;transformacion ondita;delay time;radiocomunicacion servicio movil;4230v;auxiliar personal digital;tiempo retardo;wavelets;estructura jerarquizada;wavelet transformation;regulacion caudal;scalable coding;codage	ABSTRACT Recently, there has been an increasing interest in the design of very fast wavelet image encoders focused on applications (interactive real-time image&video applications, GIS systems, etc) and devices (digital cameras, mobile phones, PDAs, etc) where coding delay and/or available computing reso urces (working memory and powe r processing) are critical for proper operation. Most of these fast wavelet image encoders are non-embedded in order to reduce complexity, so no rate control tools are available for scalable coding applications. In this work, we analyze the im pact of simple rate control tools for these encoders in order to determ ine if the inclusion of rate control functio nality is worth enough with respect to popular embedded encoders like SPIHT and JPEG2000. We perform the study by adding rate control to the non-embedded LTW encoder, showing that the increase in comp lexity still maintains LTW co mpetitive with respect SPIHT and JPEG2000 in terms of R/D performance, coding delay and memory consumption. Keywords: Wavelet image coders, fast image coding, r ate control, mobile multimedia communications	embedded system;encoder;wavelet	Otoniel López;Miguel Martínez-Rach;José Oliver;Manuel P. Malumbres	2007		10.1117/12.704151	embedded system;scalability;simulation;real-time operating system;telecommunications;computer science;working memory;computer programming;coding;statistics	Robotics	47.20198294179225	-14.441731023944156	63606
5afb3476eaaac65b8cfebf9724583c50835316a9	a new generic texture synthesis approach for enhanced h.264/mpeg4-avc video coding	content management;circuit decodeur;analisis contenido;texture;image coding;image processing;texture synthesis;coupe graphe;procesamiento imagen;gestion contenido;circuito desciframiento;traitement image;codage image;video coding;haute frequence;corte grafo;decoding circuit;content analysis;codage video;graph cut;image sequence;textura;gestion contenu;secuencia imagen;synthetiseur;analyse contenu;sintetizador;alta frecuencia;side information;high frequency;large classes;sequence image;synthesizer	A new generic texture synthesis approach, which is inspired by the work of Kwatra et al. [1], is presented in this paper. The approach is hierarchical, non-parametric, patch-based and for that applicable to a large class of spatio-temporal textures with and without local motion activity. In this work, it is shown, how the new texture synthesizer can be integrated into a content-based video coding framework. That is, the decoder reconstructs textures like water, grass, etc. that are usually very costly to encode. For that, the new texture synthesizer in conjunction with side information that is generated by the encoder is required. The reconstruction of above-mentioned textures at the decoder side basically corresponds to stuffing holes in a video sequence. Spurious edges are thereby avoided by using graph cuts to generate irregular contours at transitions between natural and synthetic textures and preferably place them (the contours) in high-frequency regions, where they are less visible.		Patrick Ndjiki-Nya;Christoph Stüber;Thomas Wiegand	2005		10.1007/11738695_17	computer vision;cut;content analysis;telecommunications;image processing;content management;computer science;high frequency;texture;texture atlas;texture synthesis	Vision	45.734643447729816	-13.234836688716703	63659
cce8b960c450ba837aa3c8a5cc5d3f9793b4584f	mpeg-4 based virtual scenes on embedded system	multimedia technologies;virtual stereoscopic scenes;multimedia technologies mpeg 4 based virtual scenes embedded system virtual stereoscopic scenes;data compression;virtual reality;embedded system;mpeg 4 standard layout embedded system discrete cosine transforms image coding quantization videos embedded software multimedia systems algorithm design and analysis;video coding;embedded systems;mpeg 4 based virtual scenes;virtual reality data compression embedded systems video coding	This paper presents how to design virtual stereoscopic scenes in the embedded system. It also introduces some important multimedia technologies, which are used in the embedded system. In addition, some professional terms and concepts of MPEG-4 are well expounded. This paper shows the process about how to build virtual stereoscopic scenes in the embedded system. Finally, it gives the effect of the virtual stereoscopic scenes, which is shown in the embedded system.	embedded system	Hui Xu;Jianren Lou;Yu Ren	2006		10.1109/ICAT.2006.90	computer vision;computer science;multimedia;computer graphics (images)	EDA	43.13293534456226	-21.07112948175004	63682
92e176c352315ce83252df0284debe08a869684b	subjective video quality metrics based on failure statistics		Subjective and Objective video quality evaluation are important problems in video processing. This work describes some new guidelines for easy and reliable determination of quality metrics that are subjectively meaningful. ‘Mean Time Between Failures’ (MTBF) and ‘Probability of Failure’ (PFAIL) are introduced as new subjective quality metrics based on failure statistics, where failure refers to errors deemed to be perceptually noticeable. These two are shown to be intuitive and scalable in the context of MPEG-2 video. MTBF is a global metric, representing how often a typical viewer observes a noticeable visual error; and PFAIL is an instantaneous metric relating to the fraction of viewers that find a given video portion to be below acceptable quality levels. These subjective quality metrics are evaluated for different video clips at bit rates in the range of 1.5 5 Mbps, and are compared with existing objective quality metrics. A desired end result of this work is the estimation of the subjective quality metrics based on objective measurements. We illustrate this using the example of an objective metric, STJM (Spatial Temporal join metric).	data rate units;h.262/mpeg-2 part 2;mpeg-2;mean time between failures;scalability;software quality assurance;video clip;video processing	Nitin Suresh;Nikil Jayant	2005			pevq;video quality;reliability engineering;subjective video quality;computer science	EDA	45.467529313440345	-22.009398398916417	63770
f17a1d630794f49d94d283b3ec60c67915b0fb1f	next generation video coding for spherical content		Recently, the Joint Video Exploration Team (JVET) issued a Call for Proposals (CFP) for video compression technology that is expected to be successor to HEVC. In this paper, we present some of the technology from our joint response in the 360° video category of CFP. Goal was to keep design as simple as possible, with picture level preprocessing and without 360 specific coding tools. The response is based on a relatively new projection called Rotated Sphere Projection (RSP). RSP splits and surrounds the sphere using two faces that are cropped from Equirectangular Projection (ERP), in the same way as two flat pieces of rubber are stitched to form a tennis ball. This approach allows RSP to get closer to the sphere than Cube Map, achieving more continuity while preserving 3:2 aspect ratio. Our results show an average BDrate Luma coding gain of 10.5% compared to ERP using HEVC.	coding gain;computers, freedom and privacy conference;cube mapping;data compression;erp;high efficiency video coding;next-generation network;preprocessor;scott continuity;usb on-the-go	Adeel Abbas;David Newman;Sri Nitchith Akula;Akhil Konda	2018	2018 Picture Coding Symposium (PCS)	10.1109/PCS.2018.8456281	coding gain;computer vision;theoretical computer science;coding (social sciences);data compression;successor cardinal;artificial intelligence;preprocessor;luma;computer science;equirectangular projection;cube mapping	HCI	41.9210756609509	-21.588852075014103	63787
281fe562bd5d02bb03a4c0e5b06814838aed5074	morphological dilation image coding with context weights prediction	evaluation performance;image coding;quad tree;performance evaluation;image processing;learning;quad arbol;evaluacion prestacion;procesamiento imagen;morphological dilation;traitement image;algorithme;aprendizaje;etat actuel;algorithm;codage image;apprentissage;state of the art;variable length group test coding;weight training;quad arbre;estado actual;group testing;quad tree coding;algoritmo;weights training	This paper proposes an adaptive morphological dilation image coding with context weights prediction. The new dilation method is not to use fixed models, but to decide whether a coefficient needs to be dilated or not according to the coefficient’s predicted significance degree. It includes two key dilation technologies: 1) controlling dilation process with context weights to reduce the output of insignificant coefficients, and 2) using variable-length group test coding with context weights to adjust the coding order and cost as few bits as possible to present the events with large probability. Moreover, we also propose a novel context weight strategy to predict coefficient’s significance degree more accurately, which serves for two dilation technologies. Experimental results show that our proposed method outperforms the state of the art image coding algorithms available today.	algorithm;arithmetic coding;coefficient;dilation (morphology);encoder;entropy encoding;online and offline;quadtree;sparse approximation;sparse matrix;teaching method	Jiaji Wu;Yan Xing;Anand Paul;Yong Fang;Jechang Jeong;Licheng Jiao;Guangming Shi	2010	Sig. Proc.: Image Comm.	10.1016/j.image.2010.10.003	group testing;simulation;image processing;computer science;artificial intelligence;strength training;quadtree;mathematics;algorithm	AI	45.73190224137305	-13.174924643345996	63828
12eb4dd9a3c71c38772c3e3a4d367ea65c457703	one-pass multi-layer rate-distortion optimization for quality scalable video coding	rate distortion;coding efficiency;scalable video coding;rate distortion video coding static var compensators scalability signal processing algorithms gain costs automatic voltage control computational modeling computational complexity;svc;h 264 avc;gain;coding efficiency one pass multilayer rate distortion optimization quality scalable video coding;joints;indexing terms;rate distortion theory;manganese;video coding;multi layer rdo h 264 avc svc quality scalable video coding;enhancement layer;video coding rate distortion theory;static var compensators;optimization;multi layer rdo;quality scalable video coding;rate distortion optimization;encoding;one pass multilayer rate distortion optimization;base layer	In this paper, a one-pass multi-layer rate-distortion optimization algorithm is proposed for quality scalable video coding. To improve the overall coding efficiency, the MB mode in the base layer is selected not only based on its rate-distortion performance relative to this layer but also according to its impact on the enhancement layer. Moreover, the optimization module for residues is also improved to benefit inter-layer prediction. Simulations show that the proposed algorithm outperforms the most recent SVC reference software. For eight test sequences, a gain of 0.35 dB on average and 0.75 dB at maximum is achieved at a cost of less than 8% increase of the total coding time.	algorithm;algorithmic efficiency;computer simulation;data compression;distortion;layer (electronics);mathematical optimization;rate–distortion optimization;scalability;scalable video coding	Xiang Li;Peter Amon;Andreas Hutter;André Kaup	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959664	scalable video coding;real-time computing;gain;computer science;manganese;theoretical computer science;coding tree unit;encoding	EDA	46.94917730063012	-18.13312844472865	63843
5b0bc486a24e59bfe71499f73023e11ad214426d	watermarking through color image bands decorrelation	bayes methods;karhunen-loeve transforms;decision theory;decorrelation;discrete fourier transforms;image colour analysis;message authentication;bayes statistical decision theory;karhunen-loeve transform;additive-multiplicative rule;color image band decorrelation;discrete fourier transform domain;image rgb components;image watermarking;mid-frequency dft coefficient magnitude modification;optimal detection;watermark embedding	Image watermarking has received great attention. Nevertheless, researchers are still faced with several problems: the extension of gray-level watermarking to the color case is one of them. To get rid of the problems raised by the correlation among image color bands, a new approach based on the decorrelation property of the Karhunen-Loeve transform (KLT) is proposed. The KLT is first applied to the RGB components of the image, then watermarking is performed independently in the DFT domain of the KL-transformed bands. Watermark embedding is achieved by modifying the magnitude of mid-frequency DFT coefficients according to an additive-multiplicative rule. On detection, KL decorrelation is exploited to optimally detect the watermark presence by relying on Bayes statistical decision theory	color image;decorrelation	Alessandro Piva;Franco Bartolini;L. Boccardi;Vito Cappellini;Alessia De Rosa;Mauro Barni	2000			message authentication code;computer vision;decorrelation;color image;decision theory;computer science;pattern recognition;mathematics;karhunen–loève theorem;statistics	Vision	41.362998733366865	-10.14514630926603	63934
5a8ceab37adcd3dbf1c4035d4bd9a5144a838dfe	active steganalysis with only one stego image	discrete wavelet transforms;independent component analysis steganalysis stego image bss problem wavelet hidden markov tree model ica algorithm;hidden markov tree;steganalysis;image processing;stego image;hidden markov tree model active steganalysis bss ica;independent component analysis;trees mathematics;active steganalysis;data mining;steganography hidden markov models independent component analysis educational institutions additives signal generators fuzzy systems knowledge engineering programmable logic arrays spread spectrum communication;wavelet transforms;steganography;ica algorithm;computational modeling;hidden markov models;discrete cosine transforms;hidden markov tree model;wavelet hidden markov tree model;bss;wavelet transforms hidden markov models image processing independent component analysis steganography trees mathematics;bss problem;ica	Modeling active steganalysis as a BSS problem we propose two active steganalysis schemes which only use one copy of stego image in this letter. We firstly adopt wavelet hidden Markov tree model to obtain an estimate of cover image and then use ICA algorithm to solve this BSS problem. The schemes overcome the unreasonable presupposition of Chandramouli’s method which needs two copies of stego images. Experiment results show that proposed schemes achieve good performance on estimate of message carrier and acceptable performance on estimate of cover image.	steganalysis;steganography	Wenzhe Lv;Bo Xu;Zhe Zhang;Wenxia Ren	2009		10.1109/FSKD.2009.348	independent component analysis;speech recognition;steganalysis;image processing;computer science;.bss;machine learning;pattern recognition;mathematics;steganography;computational model;statistics;wavelet transform	Vision	42.09531186709517	-11.126370670980263	64016
94a2d7fdbcb04fbd2acc2e5aa8df3e666fc8b604	enhanced stego block chaining (esbc) for low bandwidth channels	steganalysis;steganography;stego block chaining;cryptography;payload	This paper presents a new enhanced stego block chaining (ESBC) technique for data hiding to address the bandwidth and transmission time issues of simple stego block chaining (SBC). In simple SBC data hiding method, the security of the hidden information is increased by increasing the number of stages, but the addition of each stage decreases the hiding capacity. This increases the size of the data needed to transmit the secret message and thus results in an increase in the required bandwidth and transmission time. The proposed ESBC technique addresses the bandwidth and transmission time issues of the simple SBC method. The output of the ESBC is a high quality stego image with peak signal-to-noise ratio above 40 dB and provides the same additional security as simple SBC but reduces the requirements of the bandwidth and transmission time by one half of that of the simple SBC and, thus, solves the hiding capacity issues of SBC for low bandwidth channels. Copyright © 2017 John Wiley u0026 Sons, Ltd.	steganography	Sahib Khan;Muhammad Ismail;Tawab Khan;Nasir Ahmad	2016	Security and Communication Networks	10.1002/sec.1769	payload;steganalysis;cryptography;theoretical computer science;steganography;internet privacy;computer security	Crypto	40.13167019033155	-12.405694071706824	64022
6d71538e92f90cceffa128148c152a51b016e13a	lossless compression of color palette images with one-dimensional techniques	networks;lossless compression;receivers;data storage;visualization;matrices;internet;physical oceanography;image compression;jpeg2000;algorithms;scanning;fractal analysis	Palette images are widely used on the World Wide Web (WWW) and in game-cartridge applications. Many images used on the WWW are stored and transmitted after they are compressed losslessly with the standard graphics interchange format (GIF), or portable network graphics (PNG). Well-known 2-D compression schemes, such as JPEG-LS and JPEG-2000, fail to yield better compression than GIF or PNG due to the fact that the pixel values represent indices that point to color values in a look-up table. To improve the compression performance of JPEG-LS and JPEG-2000 techniques, several researchers have proposed various reindexing algorithms. We investigate various compression techniques for color palette images. We propose a new technique comprised of a traveling salesman problem (TSP)-based reindexing scheme, BurrowsWheeler transformation, and inversion ranks. We show that the proposed technique yields better compression gain on average than all the other 1-D compressors and the reindexing schemes that utilize JPEG-LS or JPEG-2000. © 2006 SPIE and IS&T. DOI: 10.1117/1.2194517	algorithm;burrows–wheeler transform;dhrystone;dither;gif;huffman coding;jpeg 2000;least squares;lookup table;lossless jpeg;lossless compression;palette (computing);pixel;portable network graphics;search engine indexing;sorting;travelling salesman problem;www;world wide web;bzip2	Ziya Arnavut;Ferat Sahin	2006	J. Electronic Imaging	10.1117/1.2194517	data compression;lossy compression;lossless jpeg;computer vision;the internet;visualization;fractal analysis;physical oceanography;image compression;computer science;theoretical computer science;computer data storage;jpeg;jpeg 2000;lossless compression;multimedia;fractal transform;fractal compression;texture compression;algorithm;matrix	Vision	41.15887641115181	-15.955924081854738	64069
870277717add2407cbf7f6f4ed321741f767f867	view-specific based error concealment scheme for multi-view plus depth video		Multi-view video plus depth (MVD) is an efficient format of 3D video. MVD video can be encoded by either H.264/AVC or HEVC standard to gain higher compression ratio, which benefits their broadcasting over the Internet. However, the encoded and transferred MVD video tends to develop worse visual quality degradation caused by lossy network channel. Therefore, error concealment is here to help refrain this problem. In this paper, we propose a classification-related error concealment method for MVD video. Within our method, motion-based classification is used to judge whether the corrupted blocks are static or not. If so, the blocks from reference frames are adopted to conceal the corrupted blocks. Otherwise, view-specific based concealment procedures, which are designed in accordance with view features, are used to conceal the corrupted blocks in different views. Experimental results on AVC-based Test Model (ATM) show the superiority of this concealment scheme to several other error concealment methods in PSNR along with acceptable execution time increase.	error concealment	Ran Ma;Xiangyu Hu;Deyang Liu;Yu Hou;Ping An	2016		10.1007/978-981-10-4211-9_17	real-time computing;lossy compression;reference frame;error concealment;broadcasting;communication channel;computer science	Vision	48.28918481711461	-16.213797169343596	64231
02ae4ad2eaed780421b3dd8f34da0c164a8efb72	rate-distortion optimized motion estimation for error resilient video coding	error resilient video coding;recursive estimation;rate distortion;optimisation;propagation losses;lossy video transmission;rate distortion motion estimation video coding resilience motion compensation source coding propagation losses computer errors recursive estimation bit rate;source channel motion compensation;motion estimation inter intra mode decision lossy packet networks lossy video transmission rate distortion optimized motion compensation error resilient video coding low bit rate video coding end to end distortion motion vector optimization recursive optimal per pixel estimate method rope source channel motion compensation;motion compensation;recursive optimal per pixel estimate;recursive optimal per pixel estimate method;motion estimation;bit rate;motion compensated;rate distortion optimized motion compensation;video coding;resilience;motion vector;inter intra mode decision;lossy packet networks;error resilience;low bit rate video coding;source code;rope;motion vector optimization;recursive estimation motion compensation motion estimation video coding optimisation;rate distortion optimization;mode decision;computer errors;end to end distortion;source coding	Rate-distortion optimized motion compensation was originally proposed to improve the source coding efficiency of low bit-rate video coding. This paper investigates the potential benefits of extensions to explicitly handle transmission over lossy networks. The important impact of appropriate motion compensation on error resilience is identified and exploited to enhance performance. The expected end-to-end distortion, rather than the source coding distortion, is employed in the rate-distortion optimization criterion. The accuracy of the distortion estimate is crucial when optimizing over the motion vectors (as compared to the less sensitive inter/intra mode decision), and is ensured here by adopting the recursive optimal per-pixel estimate (ROPE) method. The main objective is to assess the gains achievable by such source-channel motion compensation, and this is given priority over complexity considerations. Significant performance gains in simulation demonstrate the potential of rate-distortion optimized motion compensation to improve the error resilience of video coding.	algorithmic efficiency;data compression;distortion;end-to-end principle;lossy compression;mathematical optimization;motion compensation;motion estimation;pixel;rate–distortion optimization;recursion;simulation	Hua Yang;Kenneth Rose	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415369	computer vision;real-time computing;quarter-pixel motion;computer science;psychological resilience;source code	Vision	47.93582821812901	-16.337707414515585	64690
885607434694518e5bfcb134ffc8532753ae9761	content analysis based smart macroblock rearrangement for error resilience in wireless video transmission	video streaming;error protection;wireless video;resynchronization mark;visual quality;content analysis;wireless video transmission;error resilience;compressed video	Transmission of compressed video is very sensitive to channel error. The use of resynchronization marks is an efficient method in improving the error resilient performance of a video stream in error-prone environment. In this paper, a smart macroblock rearrangement (SMR) method is proposed to enhance the performance of the resynchronization mark insertion technique for intracoded frames in wireless video transmission. The proposed method makes use of content analysis to rearrange and encode macroblocks between adjacent resynchronization marks. Experiments using the SMR method in both H.263+ and H.264 codecs show that the proposed method outperforms some existing algorithms in both PSNR performance and visual quality.	algorithm;codec;h.264/mpeg-4 avc;image quality;macroblock;overhead (computing)	Yu Chen;Jian Feng;Kwok-Tung Lo;Xu-Dong Zhang	2008	J. Visual Communication and Image Representation	10.1016/j.jvcir.2008.06.002	video compression picture types;real-time computing;content analysis;telecommunications;computer science;video tracking;block-matching algorithm;computer network	Robotics	48.69759396621732	-15.727809844585005	64775
e528482176741d6d08f952d8fb4c86380f640659	personalizing quality aspects in scalable video coding	quantization;information systems;scalable video coding;data compression;video coding video compression bit rate encoding spatial resolution testing quantization video sequences information systems multimedia systems;visual communication;video compression;video sequences;testing;bit rate;null;multimedia systems;visual communication data compression image sequences video coding;video coding;scalable video coding mechanism;technology and engineering;scalable video coding mechanism encoding;encoding;image sequences;spatial resolution	In video coding, certain limitations imposed by the environment, most typically the bit rate, need to be fulfilled. This is achieved by allowing the encoder to reduce the quality in one or several ways, such as the distortion, the resolution and the frame rate. The upcoming scalable video coding mechanisms allow this reduction to take place not exclusively during the encoding step, but at any time. This allows us to reduce the quality in a more personalized way, taking into consideration the preferences of the end user. This paper presents a framework that enables such user dependent quality reductions. We validated this framework by means of a test involving 19 test persons. The results of this mechanism are good, but up to now not sufficiently reliable to use it in commercial applications. At the same time, we still see some room for improvement	data compression;distortion;encoder;image resolution;personalization;scalability;scalable video coding	Sam Lerouge;Robbie De Sutter;Rik Van de Walle	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521420	data compression;computer vision;computer science;theoretical computer science;multimedia;algorithm;statistics	Robotics	44.424971541075905	-20.016136451204915	64929
5c50edc8ed9cb9c4600f0c672a626a1cfad60db5	scalable wavelet video coding using aliasing-reduced hierarchical motion compensation	transformation ondelette;cuantificacion senal;arbre graphe;dynamic programming;organigramme;backward forward mode choice;estimation loop;quantization;interpolation;programacion dinamica;reconstruccion senal;system structure;estimation mouvement;scalable wavelet video coding;selection mode;flowchart;video coding motion estimation motion compensation interpolation low pass filters quantization psnr bit rate wavelet domain wavelet transforms;algorithm performance;scalable representation;psnr;image processing;successive video frames;data compression;motion compensation;filtre reponse impulsion finie;tree graph;concepcion sistema;forme onde;complexite calcul;reconstruction quality;quantization noise;implementation;aliasing;structure arborescente;metodo descomposicion;dynamic programming algorithm;interpolacion;estimacion movimiento;wavelet transform domain;optimal filter;finite impulse response filter;methode decomposition;procesamiento imagen;downsampling;tree structured dynamic programming algorithm;motion estimation;optimization method;aliasing reduced hierarchical motion compensation;transform coding;adaptive codes;wavelet decomposition;bit rate;metodo optimizacion;traitement image;mode selection;adaptive quantization scheme;experimental result;motion compensated;backward forward hybrid motion compensation;wavelet transforms;ejecucion;video coding;decomposition method;codificacion;complejidad computacion;spatially scalable video coding framework;filtro respuesta impulsion acabada;filtro optimal;wavelet transform;forma onda;multiresolutional framework;signal quantization;senal video;signal video;structure systeme;estructura arborescente;computational complexity;resultado algoritmo	We describe a spatially scalable video coding framework in which motion correspondences between successive video frames are exploited in the wavelet transform domain. The basic motivation for our coder is that motion fields are typically smooth and, therefore, can be efficiently captured through a multiresolutional framework. A wavelet decomposition is applied to each video frame and the coefficients at each level are predicted from the coarser level through backward motion compensation. To remove the aliasing effects caused by downsampling in the transform, a special interpolation filter is designed with the weighted aliasing energy as part of the optimization goal, and motion estimation is carried out with low pass filtering and interpolation in the estimation loop. Further, to achieve robust motion estimation against quantization noise, we propose a novel backward/forward hybrid motion compensation scheme, and a tree structured dynamic programming algorithm to optimize the backward/forward mode choices. A novel adaptive quantization scheme is applied to code the motion predicted residue wavelet coefficients, Experimental results reveal 0.3-2-dB increase in coded PSNR at low bit rates over the state-of-the-art H.263 standard with all enhancement modes enabled, and similar improvements over MPEG-2 at high bit rates, with a considerable improvement in subjective reconstruction quality, while simultaneously supporting a scalable representation.	algorithm;aliasing;choice behavior;coder device component;coefficient;data compression;decimation (signal processing);dynamic programming;frame (physical object);interpolation imputation technique;mpeg-2;mathematical optimization;motion compensation;motion estimation;multiresolution analysis;peak signal-to-noise ratio;quantization (signal processing);scalability;scalable video coding;wavelet transform;bit - unit of measure	Xuguang Yang;Kannan Ramchandran	2000	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.841519	computer vision;mathematical optimization;quantization;image processing;interpolation;quarter-pixel motion;computer science;dynamic programming;motion estimation;mathematics;block-matching algorithm;motion compensation;algorithm;wavelet transform	Vision	46.474233372657984	-15.45260000624524	64968
d7f907087fc11c1e587886d60c16a6b74df0eed4	comparison of compression efficiency between hevc/h.265, vp9 and av1 based on subjective quality assessments		The growing requirements for broadcasting and streaming of high quality video continue to trigger demands for codecs with higher compression efficiency. AV1 is the most recent open and royalty free video coding specification developed by Alliance for Open Media (AOMedia) with a declared ambition of becoming the most popular next generation video coding standard. Primary alternatives to AV1 are the VP9 and the HEVC/H.265 which are currently among the most popular and widespread video codecs used in applications. VP9 is also a royalty free and open specification similar to AV1, while HEVC/H.265 requires specific licensing terms for its use in commercial products and services. In this paper, we compare AV1 to VP9 and HEVC/H.265 from rate distortion point of view in a broadcasting use case scenario. Mutual comparison is performed by means of subjective evaluations carried out in a controlled environment using HD video content with typical bitrates ranging from low to high, corresponding to very low up to completely transparent quality. We then proceed with an in-depth analysis of advantages and drawbacks of each codec for specific types of content and compare the subjective comparisons and conclusions to those obtained by others in the state of the art as well to those measured by means of objective metrics such as PSNR.	aomedia video 1;codec;data compression;digital video;display resolution;distortion;high efficiency video coding;next-generation network;peak signal-to-noise ratio;point of view (computer hardware company);rate–distortion theory;requirement;vp9;video coding format	Pinar Akyazi;Touradj Ebrahimi	2018	2018 Tenth International Conference on Quality of Multimedia Experience (QoMEX)	10.1109/QoMEX.2018.8463294	multimedia;real-time computing;compression (physics);codec;data compression;computer science;use case;distortion;ranging;broadcasting	SE	43.68528488080894	-20.280792362250136	65101
9d87edf5f76af0a484eb7f19e88cc25af476626d	adaptive loop filtering based interview video coding in an hybrid video codec with mpeg-2 and hevc for stereosopic video coding	video streaming;three dimensional television;television broadcasting;video coding;adaptive filters;internet;image reconstruction;bd rate gain adaptive loop filtering based interview video coding stereosopic video coding hybrid stereoscopic video codec stereoscopic tv services heterogeneous networks left view sequences mpeg 2 video encoder 2d tv services terrestrial broadcasting networks 3d tv terminal hybrid stereoscopic video streams terrestrial broadcasting networks right view sequence streams hevc data over internet mpeg 2 frame reconstruction alf tool alf on off signaling map hevc bitstreams coding efficiency;stereo image processing;video streaming adaptive filters image reconstruction image sequences internet stereo image processing television broadcasting three dimensional television video codecs video coding;video codecs;transform coding filtering encoding stereo image processing codecs interviews internet;stereoscopic video coding 3dtv hevc alf mpeg 2;image sequences	In this paper, a hybrid stereoscopic video codec is proposed based on MPEG-2 and an extended HEVC with an interview coding scheme for stereoscopic TV services through heterogeneous networks. The left-view sequences are encoded in an MPEG-2 video encoder for conventional 2D TV services via the traditional terrestrial broadcasting networks. On the other hand, the right-view sequences are encoded by an extended HEVC with a proposed interview coding scheme and the resulting bitstreams are transmitted over Internet. So, a 3D TV terminal to support the hybrid stereoscopic video streams receives the MPEG-2 data for the left-view sequences via the terrestrial broadcasting networks, and receives the right-view sequence streams of an extended HEVC data over Internet. The proposed interview coding scheme in an extended HEVC utilizes as reference frames the reconstructed MPEG-2 frames of the left-view sequences to perform predictive coding for the current frames of the right-view sequences. To enhance the texture qualities of the reference frames, an ALF tool is applied for the reconstructed MPEG-2 frames and HEVC as well. The ALF ON/OFF signaling map and ALF coefficients for the MPEG-2 reconstructed frames are transmitted in conjunction with HEVC bitstreams via Internet. The experimental results show that the proposed hybrid stereoscopic codec with ALF-based interview coding improves the coding efficiency with average 16.81% BD-rate gain, compared to a hybrid stereoscopic codec of independent MPEG-2 and HEVC codec without interview coding.	3d television;algorithmic efficiency;blu-ray;codec;coefficient;data compression;encoder;h.262/mpeg-2 part 2;high efficiency video coding;internet;jumbo frame;mpeg-2;stereoscopy;streaming media;terrestrial television	Sangsoo Ahn;Munchurl Kim	2013	IVMSP 2013	10.1109/IVMSPW.2013.6611926	computer vision;video;h.263;telecommunications;computer science;coding tree unit;multimedia;video processing;motion compensation;h.261;multiview video coding	Vision	45.14507005117611	-18.892674797410436	65117
a7934a1d438a56b4c95b892726cba95ca8d01d05	performance analysis of text halftone modulation	performance analysis error analysis watermarking authentication information analysis character generation internet telephony data structures shape humans;watermarking;image coding;text analysis;watermarking document image processing error statistics image coding text analysis;indexing terms;print scan watermarking spectral detection;performance analysis;document image processing;error rate;error statistics;spectral detection;print scan;error rate analysis performance analysis text halftone modulation text hardcopy watermarking method document text characters spectral metric embedded message detection	This paper analyzes the use of text halftone modulation (THM) as a text hardcopy watermarking method. Using THM, text characters in a document have their luminances modified from the standard black to a gray level generated with a given halftone screen, according to a message to be transmitted. The application of THM has been discussed in ((R. Villan et al., 2006), (K. Matsuit and K. Tanaka, 1994)). In this paper, a spectral metric is proposed to detect the embedded message. Based on this metric, an error rate analysis of halftone modulation is presented considering the effects of the print and scan channel. Experiments validate the analysis and the applicability of the method.	embedded system;experiment;grayscale;modulation;profiling (computer programming)	Paulo Vinicius Koerich Borges;Joceli Mayer;Ebroul Izquierdo	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379302	text mining;speech recognition;index term;digital watermarking;word error rate;computer science;multimedia;information retrieval	Robotics	42.78132066104964	-11.439805268974954	65235
4308f979747a8ff69eaf1bd1e1712e04d349c459	image compression and reconstruction using feature extraction	image compression;feature extraction	C o n s i d e r a b l e r e s e a r c h has been d e v o t e d t o t h e deve lopment o f methods f o r r e d u c i n g t h e t r a n s m i s s i o n b a n d w i d t h o f v i d e o s i g n a l s such t h a t t he r e c o n s t r u c t e d image i s o f a h i g h q u a l i t y . T y p i c a l l y , t h e s e methods p e r f o r m a s t a t i s t i c a l e n c o d i n g o f t h e i n t e n s i t y l e v e l s o f a q u a n t i z e d p i c t u r e , o r t hey p e r f o r m some t r a n s f o r m a t i o n and subsequent d a t a r e d u c t i o n i n o r d e r t o a c h i e v e b a n d w i d t h r e d u c t i o n [ 3 ] . Such methods r a r e l y a c h i e v e d a t a comp r e s s i o n r a t i o s i n excess o f t e n t o o n e w i t h o u t n o t i c e a b l e l o s s o f q u a l i t y i n t h e r e c o n s t r u c t e d p i c t u r e .	feature extraction;image compression;offset binary	Robert T. Chien;L. J. Peterson	1977			computer vision;data compression ratio;speech recognition;feature extraction;image compression;computer science;pattern recognition	Robotics	40.885037556605745	-15.644137874135989	65486
c83d1b3d3b56b8baab87e274718e85f5b3c801da	multi band pass filter based one-bit transforms in motion estimation		One-bit transform method can be used for low computational complexity block based motion estimation with respect to hardware implementation in video coding systems. The principal of the one-bit transform method is based on kernel which has multi band pass filtering. In this work, the performance of the one-bit transform methods obtained with band pass filtering on the state of the art video coding standard has been evaluated. Also new kernels, which have different multi band pass filtering, have been proposed to compare the performance of the low bit representation based motion estimation in low and high resolution videos.	computational complexity theory;data compression;image resolution;motion estimation;video coding format	Orhan Akbulut;Ramazan Duvar;Ayhan Kucukmanisa;Oguzhan Urhan	2017	2017 25th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2017.7960514	filter (signal processing);computer vision;kernel (linear algebra);computer science;artificial intelligence;theoretical computer science;low bit;computational complexity theory;motion estimation;band-pass filter	Vision	45.00701087931942	-16.31068745688588	65573
41a1efc86d1ccdafdc95e373b75279f33fe86700	a fast direction predictor of inter frame prediction for multi-view video coding	inter frame prediction;motion compensation prediction;multi view video coding;rate distortion;image coding;motion compensation;geometry;prediction algorithms;motion estimation;bit rate;multiple views;fast direction predictor;rate distortion theory;multiview video coding;video coding;image enhancement;disparity compensation prediction;prediction theory;estimation;base view;computational complexity;fast algorithm;video coding motion compensation cameras bandwidth rate distortion algorithm design and analysis continuous wavelet transforms computational complexity motion estimation geometry;enhancement view;bandwidth;motion compensated prediction;enhancement view fast direction predictor inter frame prediction multiview video coding rate distortion optimization motion compensation prediction disparity compensation prediction base view;signal processing algorithms;rate distortion optimization;real time application;video coding image enhancement motion compensation prediction theory rate distortion theory;algorithm design and analysis;cameras;conferences;continuous wavelet transforms	Multi-view applications provide viewers a whole new viewing experience, and multi-view video coding (MVC) plays a key role in distributing multi-view video contents through networks with limited bandwidth. However, the computational load of a MVC encoder is pretty heavy so that it is hard to be realized in real-time applications. One reason behind this is that a MVC encoder has to make a decision of prediction direction based on rate-distortion optimization from both motion compensation prediction (MCP) and disparity compensation prediction (DCP) for multiple views. Motivated by this, this paper presents a novel fast MVC algorithm where a fast decision strategy of prediction direction of MCP and DCP is designed. Blocks with slow motion (SMBs) of all pictures in the base view and anchor pictures in enhancement views are identified based on MVs from MCP without additional computations. Then, the identification of SMBs in non-anchor frames of an enhancement view will be inferred from the SMBs of base view or the other coded enhancement views. Finally, the fast algorithm is achieved by applying MCP to SMBs of non-anchor pictures in enhancement views within the same GGOP. Experimental results conducted by JMVM 6.0 show that the average time reduction is 20% while the bitrate increase and PSNR loss are less than 0.25% and 0.0045 dB, respectively.	algorithm;binocular disparity;computation;data compression;decision theory;distortion;encoder;kerrison predictor;mathematical optimization;model–view–controller;motion compensation;multiview video coding;peak signal-to-noise ratio;rate–distortion optimization;real-time clock	Jheng-Ping Lin;Angela Chih-Wei Tang	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5118331	algorithm design;computer vision;estimation;real-time computing;simulation;prediction;rate–distortion theory;computer science;motion estimation;mathematics;rate–distortion optimization;computational complexity theory;motion compensation;bandwidth;statistics;multiview video coding	Arch	46.42854896472676	-18.502036650827343	65593
5108ef7a603bb8b99976890c0d9fd5012586023f	block size dependent error model for motion compensation	pixel position block size dependent error model video coding standards block based motion estimation algorithms block based motion compensation algorithms prediction error variance;video coding standards;prediction error;motion compensation;block size dependent error model;pixel position;block based motion compensation algorithms;prediction algorithms;motion estimation;motion compensated;video coding;accuracy;pixel;video coding motion compensation motion estimation;mathematical model equations pixel motion compensation motion estimation prediction algorithms accuracy;mathematical model;block matching;block based motion estimation algorithms;motion compensated prediction;displaced frame difference;prediction error video coding motion compensation block size block matching;block size;prediction error variance	Current video coding standards use block-based motion estimation and compensation algorithms to exploit dependencies between consecutive frames. It is a well-known fact that decreasing the block size reduces the motion-compensated frame difference, and thus reduces the data rate. However, no theoretical evaluations are available to model this relation. This paper derives a model for the prediction error variance of block-based motion compensation algorithms with respect to the block size. It is shown that the variance of the displaced frame difference of a block can be modelled with the pixel position and only three additional parameters. It can be observed that the variance increases almost linearly with the block size.	algorithm;block size (cryptography);data compression;motion compensation;motion estimation;pixel;uncompressed video;video coding format	Sven Klomp;Marco Munderloh;Jörn Ostermann	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5649414	computer vision;prediction;computer science;block size;mean squared prediction error;motion estimation;mathematical model;control theory;mathematics;accuracy and precision;motion compensation;pixel;statistics	Robotics	46.98319190441276	-18.13452590303614	65721
e4dc025ec0b8f759a41a2b0295d729d13fe3d530	graph-based vs depth-based data representation for multiview images	image coding;decoding;geometry;geometry image coding image reconstruction cameras encoding decoding three dimensional displays;three dimensional displays;image reconstruction;encoding;cameras	In this paper, we propose a representation and coding method for multiview images. As an alternative to depth-based schemes, we propose a representation that captures the geometry and the dependencies between pixels in different views in the form of connections in a graph. In our approach it is possible to perform compression of the geometry information and to preserve a direct control of the effect of geometry approximation on view reconstruction. This is not possible with classical depth-based representations. As a results, our method leads to more accurate view prediction, when compared to conventional lossy coding of depth maps operating at the same bit rate. We finally show in experiments that our representation adapts the amount of transmitted geometry to the complexity of the predictions that are performed at the decoder.	approximation;data (computing);experiment;graph (discrete mathematics);lossy compression;pixel	Thomas Maugey;Antonio Ortega;Pascal Frossard	2013	2013 Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2013.6810374	computer vision;discrete mathematics;theoretical computer science;mathematics	Vision	43.45396354263177	-18.6727929992617	65797
c932be3217bb2db3bcc89f395c9f89af8aacda85	efficient intra refreshment and synchronization algorithms for robust transmission of video over wireless networks	optimisation;error concealment;data compression;cost function;mpeg 4 video codec efficient intra refreshment algorithm efficient synchronization algorithm video transmission wireless networks video coding systems error resilience intra inter coding mode selection cost functions synchronization markers data partitioning error concealment decoder rate distortion optimization packetization algorithms mpeg4 standard simple profile qos;visual communication;wireless network;packet radio networks;robustness resilience decoding error correction codes wireless networks video coding mpeg 4 standard visual communication error correction bit error rate;data partitioning;rate distortion theory;synchronisation;video coding;error resilience;video codecs video coding visual communication synchronisation packet radio networks data compression rate distortion theory optimisation;video codecs;rate distortion optimization	This paper proposes efficient intra refreshment and packetization algorithms to enhance error resilience of video coding systems for wireless applications. We attack the two problems separately to alleviate the computational burden, instead of jointly optimizing them. We first enhance error resilience of intra/inter coding mode selection by introducing new cost functions for the two coding modes, and then optimize the placement of synchronization markers by incorporating data partitioning and error concealment at the decoder in the rate-distortion optimization procedure. Experiments show that the proposed techniques in conjunction with the MPEG4 standard (Simple Profile) provide significant improvements in the error resilience performance.	algorithm;intra-frame coding	Kyeong Ho Yang;Dong Wook Kang;A. Farid Faryar	2001		10.1109/ICIP.2001.959201	data compression;synchronization;real-time computing;rate–distortion theory;telecommunications;computer science;wireless network;rate–distortion optimization;statistics;visual communication;computer network	Mobile	48.606035016054186	-16.388755977793423	65873
6d1e97df31e9a4b0255243d86608c4b7f725133b	incremental task and motion planning: a constraint-based approach		We present a new algorithm for task and motion planning (TMP) and discuss the requirements and abstractions necessary to obtain robust solutions for TMP in general. Our Iteratively Deepened Task and Motion Planning (IDTMP) method is probabilistically-complete and offers improved performance and generality compared to a similar, state-of-theart, probabilistically-complete planner. The key idea of IDTMP is to leverage incremental constraint solving to efficiently add and remove constraints on motion feasibility at the task level. We validate IDTMP on a physical manipulator and evaluate scalability on scenarios with many objects and long plans, showing order-of-magnitude gains compared to the benchmark planner and a four-times self-comparison speedup from our extensions. Finally, in addition to describing a new method for TMP and its implementation on a physical robot, we also put forward requirements and abstractions for the development of similar planners in the future.	algorithm;baxter (robot);benchmark (computing);constraint satisfaction problem;motion planning;planner;requirement;robot;scalability;speedup;star trek:	Neil T. Dantam;Zachary K. Kingston;Swarat Chaudhuri;Lydia E. Kavraki	2016		10.15607/RSS.2016.XII.002	motion planning	Robotics	52.51686960642306	-23.053947256651906	65882
554945bcad971138591e35052b882be0dc4f6e0e	machine learning for video compression: macroblock mode decision	optimisation;cost function machine learning video compression macroblock mode decision video encoder optimization bayesian setup classification problem;video encoder optimization;cost function;bayes methods;video compression;macroblock mode decision;video coding;machine learning video compression encoding dvd streaming media video sharing pixel artificial intelligence machinery bayesian methods;machine learning;video coding bayes methods learning artificial intelligence optimisation pattern classification;pattern classification;bayesian setup;classification problem;learning artificial intelligence;mode decision	Video compression currently is dominated by engineering and fine-tuned heuristic methods. In this paper, we propose to instead apply the well-developed machinery of machine learning in order to support the optimization of existing video encoders and the creation of new ones. Exemplarily, we show how by machine learning we can improve one encoding step that is crucial for the performance of all current video standards: macroblock mode decision. By formulating the problem in a Bayesian setup, we show that macroblock mode decision can be reduced to a classification problem with a cost function for misclassification that is sample dependent. We demonstrate how to apply different machine learning techniques to obtain suitable classifiers and we show in detailed experiments that all of these perform better than the state-of-the-art heuristic method	data compression;encoder;experiment;heuristic;loss function;machine learning;macroblock;mathematical optimization;whole earth 'lectronic link	Christoph H. Lampert	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.778	data compression;computer vision;computer science;machine learning;pattern recognition	ML	48.139595236847995	-17.728714193955827	66522
303fc0d204e7dcad821778b328059f963e4f22d2	multiscale inverse reinforcement learning using diffusion wavelets		This work presents a multiscale framework to solve an inverse reinforcement learning (IRL) problem for continuous-time/state stochastic systems. We take advantage of a diffusion wavelet representation of the associated Markov chain to abstract the state space. This not only allows for effectively handling the large (and geometrically complex) decision space but also provides more interpretable representations of the demonstrated state trajectories and also of the resulting policy of IRL. In the proposed framework, the problem is divided into the global and local IRL, where the global approximation of the optimal value functions are obtained using coarse features and the local details are quantified using fine local features. An illustrative numerical example on robot path control in a complex environment is presented to verify the proposed method.	approximation;diffusion wavelets;discrete wavelet transform;markov chain;numerical analysis;optimization problem;reinforcement learning;state space;stochastic process	Jung-Su Ha;Han-Lim Choi	2016	CoRR		mathematical optimization;computer science;artificial intelligence;machine learning;mathematics	Robotics	39.42235754753997	-23.458071367013645	66605
4a3df50308f3e381ff9555dc4d5e498312f4b28d	error resilience and concealment in embedded zerotree wavelet codecs	embedded zerotree wavelet;error correction codes;image coding;error concealment;codecs;decoding;data reconstruction;visual communication;video compression;data communication;image coding error correction codes wavelet transforms video codecs asynchronous transfer mode visual communication interleaved codes data communication variable rate codes;atm networks;wavelet transforms;variable rate codes;interleaved codes;data interleaving;resilience;channel errors;cell loss;image reconstruction;unequal error protection;video transmission;digital image transmission;error resilience;video codecs;rate scalable encoders;intelligent networks;missing data;data loss;resilience codecs asynchronous transfer mode wavelet coefficients decoding intelligent networks digital images image reconstruction computer errors video compression;digital images;wavelet coefficients;computer errors;asynchronous transfer mode;rate scalable encoders error resilience error concealment embedded zerotree wavelet codecs atm networks cell loss channel errors digital image transmission video transmission data reconstruction ezw encoders data loss unequal error protection data interleaving;ezw encoders	In ATM networks cell loss or channel errors can cause data to be dropped in the channel. When digital imagedvideo are transmitted over these networks one must be able to reconstruct the missing data so that the impact of the errors is minimized. In this paper we overview the problem of using EZW encoders in channels where data-loss is possible. We also describe an error resilience scheme based on unequal error protection and data interleaving that addresses the problem of using rate scalable encoders over ATM networks.	atm turbo;codec;embedded zerotrees of wavelet transforms;embedded system;encoder;forward error correction;missing data;scalability	Paul Salama;Ness B. Shroff;Edward J. Delp	2001		10.1109/ICIP.2001.958090	data compression;iterative reconstruction;intelligent network;codec;data loss;missing data;telecommunications;computer science;theoretical computer science;asynchronous transfer mode;mathematics;digital image;psychological resilience;statistics;visual communication;wavelet transform	Mobile	48.57637444206167	-15.316431836405583	66881
86ef1800a36d258a2ed420c73548f784521c123a	reversible data-hiding in encrypted images by redundant space transfer		Reversible data-hiding in an encrypted image (RDHEI) embeds additional data into the encrypted image content, in a manner such that the data-hiding operation does not affect the lossless recovery of the encrypted image content. In previous RDHEI methods, the encrypted images contain little redundant space; thus, these approaches may feature a low embedding rate and burden content owners with too many tasks. In contrast, in this paper, we propose a novel RDHEI by redundant space transfer (RST) scheme, which involves transferring redundant space from the original image to the encrypted image. Then, the encrypted image will necessarily contain redundant space. Thus, reversibly embedding data into this encrypted image becomes easy and efficient, and general reversible data-hiding (RDH) algorithms can be used. The proposed scheme has the advantages of a high embedding rate and requires few tasks of the content owner. The experimental results show that the performance of the proposed scheme outperforms other RDHEI algorithms.	algorithm;arnold;encryption;experiment;intel matrix raid;lossless compression;privacy;pro tools;sparse matrix	Zi-Long Liu;Chi-Man Pun	2018	Inf. Sci.	10.1016/j.ins.2017.12.044	information hiding;mathematics;artificial intelligence;machine learning;theoretical computer science;encryption;embedding;lossless compression	Vision	39.81271778857125	-11.812804076121221	67259
09ab86c4dff89ba534601336a7d4ca7a07926a94	loss concealment using b-pictures motion information	decoding disruption tolerant networking degradation systems engineering and theory motion estimation switching circuits packet switching councils telecommunications laboratories;disruption tolerant networking;degradation;decoding;video signal processing;switching circuits;packet loss;mpeg video;packet video;motion estimation;packet switching;macroblock decisions;systems engineering and theory;corrupted anchor pictures;channel errors;video signal processing motion estimation encoding;motion vector;bidirectionally predicted pictures;councils;loss concealment;b pictures motion information;packet losses;encoding;packet video loss concealment b pictures motion information bidirectionally predicted pictures corrupted anchor pictures packet losses channel errors macroblock decisions motion prediction anchor picture motion estimation mpeg video;anchor picture;telecommunications;motion prediction	In this work, the motion parameters of the bi-directionally predicted pictures (B-pictures) of MPEG-1,2 are exploited for concealment of large portions of corrupted anchor pictures that might arise due to channel errors or packet losses. To further enhance the quality of the concealed anchor pictures, we propose two methods of constraining the motion vectors of the B-pictures that strengthen the tie between them and those of the anchor pictures in the same picture sub-group. In one method, the macroblock decisions on the last B-picture in each sub-group is constrained to be bi-directional if those of the other B-pictures are not, such that the derived motion vectors for the concealment of the anchor picture are always composed from the forward and backward motion vectors of the bi-directional motions. Second, the bi-directional motion vectors of the B-pictures in each sub-group is constrained such that the vectorial sum of their forward and backward motion vectors results in accurate motion prediction of the anchor picture. The experimental results show that while the composed motion vectors improve the quality of concealment over the conventional methods by more than 3-4 dB, another 2 dB improvement can be achieved by constraining the generation of the bi-directional motion	benchmark (computing);bi-directional text;bitstream;encoder;image;linkage (software);loss function;macroblock;motion compensation;moving picture experts group;network packet;overhead (computing);refinement (computing);star trek:	Tamer Shanableh;Mohammed Ghanbari	2003	IEEE Trans. Multimedia	10.1109/TMM.2003.811624	computer vision;degradation;telecommunications;quarter-pixel motion;computer science;delay-tolerant networking;motion estimation;packet loss;packet switching;encoding;computer network	Robotics	48.272547717223546	-16.405062455601733	67284
0f31461717f76784e65451ef7cd166c955469d10	self-folding shape memory laminates for automated fabrication	shape memory effects;laminates;assembling;shape memory effects assembling laminates;laminates heating fabrication shape ink assembly;fold angles self folding shape memory laminates automated fabrication self folding approach small structure assembly cube structure	Nature regularly uses self-folding as an efficient approach to automated fabrication. In engineered systems, however, the use of self-folding has been primarily restricted to the assembly of small structures using exotic materials and/or complex infrastructures. In this paper we present three approaches to the self-folding of structures using low-cost, rapid-prototyped shape memory laminates. These structures require minimal deployment infrastructure, and are activated by light, heat, or electricity. We compare the fabrication of a fundamental structure (a cube) using each approach, and test ways to control fold angles in each case. Finally, for each self-folding approach we present a unique structure that the approach is particularly suited to fold, and discuss the advantages and disadvantages of each approach.	mit engineering systems division;resistive touchscreen;software deployment	Michael T. Tolley;Samuel M. Felton;Shuhei Miyashita;Lily Xu;ByungHyun Shin;Monica Zhou;Daniela Rus;Robert J. Wood	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6697068	structural engineering;engineering drawing	Robotics	52.77118958936572	-12.797300630570886	67424
3d3098102240e559b178d23ca6f33824f8f18e44	optimisation of two-layer snr scalability for mpeg-2 video	quantization;rate distortion;optimisation;layered video coding;dct coefficients;decoding;overall bit rate reduction;optimal coefficient adjustment;transform coding;bit rate;systems engineering and theory;rate distortion theory;video coding;enhancement layer;discrete cosine transforms;dct coefficients optimisation snr scalability mpeg 2 video two layer video coding base quality pictures enhancement layer coding optimal coefficient adjustment overall bit rate reduction;scalability discrete cosine transforms quantization decoding video coding bit rate rate distortion systems engineering and theory optimization methods broadcasting;base quality pictures;scalability;enhancement layer coding;broadcasting;image sequences video coding optimisation transform coding discrete cosine transforms rate distortion theory;snr scalability;mpeg 2 video;optimization methods;image sequences;two layer video coding	SNR scalability, used in two-layer video coding, guarantees good base quality pictures at the expense of increased overall bit-rate. By understanding the inherent ine ciencies of enhancement layer coding we have developed an optimisation method called optimal coe cient adjustment in order to reduce overall bit-rates to levels consistent with single-layer operation.	bundle adjustment;data compression;h.262/mpeg-2 part 2;mpeg-2;mathematical optimization;scalability;signal-to-noise ratio;thresholding (image processing)	David Wilson;Mohammed Ghanbari	1997		10.1109/ICASSP.1997.595330	scalability;transform coding;rate–distortion theory;quantization;telecommunications;computer science;theoretical computer science;broadcasting;statistics	AI	46.75745204904668	-16.71671074619991	67702
c6ff17dab4b5dcb80d4adafb7deecfe88df3aed1	fast 2-dimensional 4 × 4 forward integer transform implementation for h.264/avc	integer transform discrete cosine transform dct fast algorithm;discrete cosine transform dct;real time;automatic voltage control discrete cosine transforms signal processing algorithms video coding quantization two dimensional displays matrix decomposition delay mpeg 4 standard arithmetic;advanced video coding;discrete cosine transform;2 dimensional;integer transform;matrix decomposition;signal processing;fast algorithm;kronecker product	In this paper, the novel two-dimensional (2-D) fast algorithm for realization of 4 $times$ 4 forward integer transform in H.264 is proposed. Based on matrix operations with Kronecker product and direct sum, the efficient fast 2-D 4 $times$ 4 forward integer transform can be derived from the proposed one-dimensional fast 4 $times$ 4 forward integer transform through matrix decompositions. The proposed fast 2-D 4 $times$ 4 forward integer transform design doesn't need transpose memory for direct parallel pipelined architecture. The fast 2-D 4 $times$ 4 forward integer transform requires fewer latency delays than the state-of-the-art methods. With regular modularity, the proposed fast algorithm is suitable for VLSI implementation to achieve real-time H.264/advanced video coding (AVC) signal processing.	h.264/mpeg-4 avc	Chih-Peng Fan	2006	IEEE Trans. on Circuits and Systems	10.1109/TCSII.2005.858748	mathematical optimization;two-dimensional space;discrete mathematics;transform coding;lapped transform;modified discrete cosine transform;computer science;theoretical computer science;fractional fourier transform;discrete sine transform;discrete fourier transform;signal processing;discrete cosine transform;mathematics;discrete fourier transform;kronecker product;matrix decomposition	Security	45.50936071402362	-10.309561385195241	67719
994d05313fe2c68a1a771b42e90675d8f5b7daeb	local prediction for lossless image compression		In predictive coding a group of neighboring picture elements is used to select a suitable prediction value for a current pixel. In this paper, we propose two techniques for lossless images compression based on predictive coding. In the rst technique which called, the predictors, we replace each pixel in the image by the predicted pixel; we use various schemes to predict the value of a pixel. In the second, which is based on predictor technique, and called optimal prediction schemes, we divide the original image into blocks or lines and seek the best predictor for each (among a selected set of eight) that provides the best prediction. The errors image is encoded through arithmetic coding, during the nal step of compression. The gains of compression that we obtained are observed in the lossless image compression.	arithmetic coding;image compression;kerrison predictor;lossless compression;pixel	Ahmad Daaboul	1998			discrete mathematics;predictive coding;pixel;computer science;arithmetic coding;compression (physics);artificial intelligence;lossless compression;pattern recognition	Graphics	44.49064771706077	-16.115690577675693	67770
3c0a3e60b0f85f9e7b1854aafc4087ac9f2002a2	periodic pan compensation for reduced complexity video compression	data compression;motion compensation;video compression;motion estimation;differential pulse code modulation;video coding;spatial scaleability periodic pan compensation reduced complexity video compression video encoder hybrid dpcm transform video compression feedback loop tracking updating image residual energy reconstruction error compensation spatial coder residual image embedded bit stream;feedback;computational complexity;feedback loop;video compression motion compensation feedback loop remote sensing discrete cosine transforms bit rate rate distortion decoding lakes tracking loops;feedback motion compensation motion estimation video coding computational complexity data compression differential pulse code modulation optical modulation tracking;optical modulation;tracking	To reduce the complexity of a video encoder, we introduce a new approach to hybrid DPCM-tranform video compression in which pan compensation is performed outside the feedback loop. While the basic idea is conceptually similar to the pan compensation algorithm proposed by Taubman and Zakhor for their 3D subband coder, our method is different in that it continually tracks and updates the image in the feedback loop in the same way as a conventional hybrid coder. Using both residual energy and reconstruction error as metrics, we show that pan compensation implemented outside the feedback loop compares very favorably to similar compensation implemented within the conventional hybrid-transform framework. Furthermore, if the spatial coder used to compress the residual images outputs an embedded bit stream, then the complete system is spatially scaleable.	algorithm;bitstream;data compression;embedded system;encoder;feedback	Charles D. Creusere	1997		10.1109/ICASSP.1997.595393	data compression;computer vision;real-time computing;quarter-pixel motion;computer science;block-matching algorithm;motion compensation	Vision	46.17924189565028	-17.224940351115162	67791
9cb1fbdc2e85bda869668514c9b7804e862120d3	depth video coding using adaptive geometry based intra prediction for 3-d video systems	video coding three dimensional displays image color analysis materials rendering computer graphics bit rate encoding;geometry;video processing;video coding geometry rendering computer graphics solid modelling;bit rate;materials;visual quality;intra prediction;three dimensional;computer graphic;video coding;adaptive geometry 3 d video processing system depth video coding intra prediction mode boundary information loss reduction statistical characteristics geometric characteristics bit savings rendering quality;three dimensional displays;image color analysis;3 d video system depth video coding geometry based block partitioning intra prediction;3 d video system;depth video coding;rendering computer graphics;encoding;solid modelling;geometry based block partitioning	Depth video coding is an essential part of 3-D video processing systems. Specifically, object boundary regions are important in depth video coding since these regions significantly affect the visual quality of a synthesized view. In this paper, we propose an efficient depth video coding method to determine precise intra prediction modes and thereby reduce the loss of boundary information. To achieve this objective, we analyze and exploit statistical and geometric characteristics of the depth video. Experimental results subsequently show that the proposed method performs better than the original intra prediction of H.264/AVC in terms of bit savings and rendering quality.	3d film;algorithmic efficiency;computational complexity theory;data compression;h.264/mpeg-4 avc;intra-frame coding;video processing	Min-Koo Kang;Yo-Sung Ho	2012	IEEE Transactions on Multimedia	10.1109/TMM.2011.2169238	video compression picture types;scalable video coding;three-dimensional space;computer vision;computer science;video tracking;coding tree unit;multimedia;video processing;context-adaptive binary arithmetic coding;motion compensation;video post-processing;encoding;multiview video coding;computer graphics (images)	Vision	43.851205036553196	-18.79995177954102	67827
fdf021ddb53151f363c6b08200d0e71903aa9d49	real-time and parallel shvc hybrid codec avc to hevc decoder		Scalable High efficiency Video Coding (SHVC) is the scalable extension of the latest video coding standard High Efficiency Video Coding (HEVC). One of the key novelties introduced by SHVC is that it enables hybrid codec scalability. This basically means that the video layers can be encoded with different video standards providing backward compatibility between codecs. In this paper, we propose a software parallel SHVC decoder in hybrid codec scalability configuration. The proposed design consists of an Advanced Video Coding (AVC) decoder for the Base Layer (BL) and a HEVC decoder for the Enhanced Layer (EL). In order to perform Inter Layer Prediction (ILP), a communication of decoding states and outputs is established between the two decoders. While the native frame based parallelism is still allowed within the two decoders, the proposed design also enables the use of frame based parallelism between the two decoders. The proposed software design enables a real time decoding of the HEVC EL at 2160p60 while the AVC base layer is decoded at 1080p60 for ×2 spatial scalability.	bl (logic);backward compatibility;codec;data compression;frame language;h.264/mpeg-4 avc;high efficiency video coding;parallel computing;real-time transcription;scalability;software design;video coding format	Pierre-Loup Cabarat;Wassim Hamidouche;Olivier Déforges	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952716	parallel computing;backward compatibility;computer science;real-time computing;decoding methods;codec;scalability;software;scalable video coding;multiview video coding;software design	Robotics	44.07492485126399	-21.443794219538578	67835
2d3eb18b2dc8c47f2b90c1421d7e348dfcef8e1f	implementation of cartesian grids to accelerate delaunay-based derivative-free optimization	derivative-free optimization;surrogate functions;delaunay triangulation;cartesian grid	This paper introduces a modification of our original Delaunay-based optimization algorithm (developed in JOGO DOI: 10.1007/s10898-015-0384-2) that reduces the number of function evaluations on the boundary of feasibility as compared with the original algorithm. A weaknesses we have identified with the original algorithm is the sometimes faulty behavior of the generated uncertainty function near the boundary of feasibility, which leads to more function evaluations along the boundary of feasibility than might otherwise be necessary. To address this issue, a second search function is introduced which has improved behavior near the boundary of the search domain. Additionally, the datapoints are quantized onto a Cartesian grid, which is successively refined, over the search domain. These two modifications lead to a significant reduction of datapoints accumulating on the boundary of feasibility, and faster overall convergence.	algorithm;benchmark (computing);cobham's thesis;curse of dimensionality;delaunay triangulation;derivative-free optimization;discretization;iteration;mathematical optimization;numerical analysis;overhead (computing);quantization (signal processing);regular grid;sion's minimax theorem;triangulation (geometry)	Pooriya Beyhaghi;Thomas Bewley	2017	J. Global Optimization	10.1007/s10898-017-0548-3	mathematics;derivative-free optimization;cartesian coordinate system;mathematical optimization;delaunay triangulation;constrained delaunay triangulation;regular grid;quantization (physics);convergence (routing)	Robotics	52.87891878994867	-19.36532741464247	67958
360db3bbdacb4f29ac391a32ac967c0b33b966f6	adaptive subband image coding considering spatial relationship on hierarchical pyramid	rate distortion;uniform decomposition adaptive subband image coding spatial relationship hierarchical pyramid band splitting method coding efficiency entropy coding model embedded zero tree wavelet set partitioning hierarchical trees space frequency quantization algorithm quad tree set adaptive decomposition method rate distortion characteristic octave decomposition;wavelet transforms image coding quantisation signal adaptive signal processing;set partitioning in hierarchical trees;image coding;spatial dependence;splitting method;image coding frequency rate distortion wavelet packets quantization space technology partitioning algorithms bit rate entropy coding wavelet coefficients;entropy coding;quantisation signal;wavelet transforms;decomposition method;adaptive signal processing;embedded zero tree wavelet;spatial relationships;high frequency;space frequency quantization	It is well known that the bandsplitting method strongly influences the efficiency for subband image coding. Further, the coding efficiency is very dependent on the entropy-coding model. Especially in the embedded Zerotree wavelet (EZW), the set partitioning in hierarchical trees (SPIHT), or the space-frequency quantization (SFQ) algorithm, the spatial dependency defined by the quad tree set of coefficients is employed and they give good performance. In this work, we describe the adaptive decomposition method based on a rate-distortion Characteristic of the quad tree set of coeficients that is independent of the bandsplitting. Experimental results for some images show that our coder gives a better coding eflciency and it generally tends to be effective to octave-decompose in the lower bit rate application and to uniform-decompose the high frequency subhand in the higher case.	algorithm;algorithmic efficiency;coefficient;distortion;embedded zerotrees of wavelet transforms;embedded system;entropy encoding;network scheduler;quadtree;set partitioning in hierarchical trees;stationary wavelet transform	Tsuyoshi Otake;Akira Kawanaka	1999		10.1109/ICIP.1999.817139	spatial relation;adaptive filter;mathematical optimization;decomposition method;spatial dependence;entropy encoding;theoretical computer science;high frequency;pattern recognition;mathematics;set partitioning in hierarchical trees;statistics;wavelet transform	ML	44.40233285497341	-14.621596971932352	68079
c942d0e6595dcf51f5b41d662b0f55edaca7dfe0	joint coding and embedding techniques for multimedia images	quantization index modulation;data hiding;side match vq;variable rate;data embedding;data communication;image compression;clustering;indexation;vector quantizer;digital image	Hiding data in digital images is a technique for secret data communication. It involves embedding important data into a cover image in digital form with minimal perceptible degradation. In this paper, we first propose an improved data hiding method for sidematch vector quantization (SMVQ) compressed images, which utilizes the codeword-pairing procedure to modulate the index code of each image block. In order to take it a step further and obtain a larger embedding capacity, we remodel the first proposed method into a new one with the power of variable-rate data embedding through the use of a multiplebase notational system. According to the experimental results, the two proposed methods are indeed capable of providing a larger embedding capacity without causing noticeable distortions of stego-images in comparison with previous methods. Moreover, our methods also maintain a low compression bit-rate without auxiliary data. 2008 Elsevier Inc. All rights reserved.	code word;digital image;distortion;elegant degradation;steganography;vector quantization	Chin-Chen Chang;Wen-Chuan Wu;Yi-Hui Chen	2008	Inf. Sci.	10.1016/j.ins.2008.05.003	computer vision;image compression;computer science;theoretical computer science;machine learning;data mining;mathematics;cluster analysis;information hiding;digital image;algorithm	AI	40.69904897618799	-12.570640296766967	68227
f17cdd2a0bfd4b834a8bc4ab798e85b1c8a8cf4a	low-band-shifted hierarchical backward motion estimation, compensation for wavelet-based video coding		A new framework for block-based backward motion compensation in wavelet scalable video coding scheme is proposed. Motion estimation and compensation are hierarchically conducted in wavelet domain using coarser level lowpass subband in the current frame and synthesized next finer level lowpass subband in the reference frame, hence, the motion information does not need to be transmitted. To alleviate the aliasing effect caused by decimation in wavelet decomposition, the lowpass subband in reference frame are shifted to obtain four subbands, which are then used in motion estimation and compensation to generate final prediction. A flexible quantization scheme and arithmetic coding is used to individually encode the motion compensated subbands without exploiting cross-band correlation. Compared to the motion estimation and compensation scheme without shifting of the lowpass subband, the proposed technique provides around 2 dB improvement in PSNR for compression of full motion sequence. The multi-level scalability of the scheme makes it useful for low bandwidth networks, such as satellite or cellular networks.	aliasing;arithmetic coding;data compression;decimation (signal processing);encode;low-pass filter;motion compensation;motion estimation;peak signal-to-noise ratio;reference frame (video);scalability;scalable video coding;wavelet	Yufei Yuan;Mrinal Kanti Mandal	2002			artificial intelligence;reference frame;computer vision;computer science;quarter-pixel motion;block-matching algorithm;motion compensation;rate–distortion optimization;context-adaptive binary arithmetic coding;motion estimation;coding tree unit	Vision	45.975309827179665	-17.285262297988858	68291
985a8ccfbf97dd29c44c959af5569a002fc4d603	digital image watermarking on a special object: the human face	digital watermarking;sensors;skin;psychology;visual quality;skin color;digital content;digital image watermarking	In this paper, we present a method for protection of digital contents by using the watermark embedding in special object, especially, human faces. To insert the watermark signals that are composed of noise like binary signals, we first localize the face regions within images by using the color and edge information. The skin color area is filtered out and then edge detector is applied for skin area to find out face features. These features are used for decision whether the skin area is face region or not. The face region is divide non-overlapping sub-blocks and a watermark bit is inserted into the each sub- block by considering the block activity. We insert a watermark bit in DCT domain of each sub-block. The level of modification of the DCT coefficients is determined considering the block variance. The non-zero coefficients of the DCT are selected and modified according to the robustness levels. Then, inverse DCT is performed. The extraction of the watermark is performed by comparing the original image in DCT domain. The robustness of the watermarking is similar to the other methods in DCT, but it has good visual qualities and less intended external piracy in terms of psychology.© (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	digital image;digital watermarking	Hwang-Seok Oh;DukHo Chang;Choong-Hoon Lee;Heung-Kyu Lee	2000		10.1117/12.385009	computer vision;computer science;multimedia;watermark;computer graphics (images)	Vision	40.62334715614336	-11.012720526710536	68381
2b9078fac15904a9e4fb1e6423a2ee9fbeb34e77	hiding information in images	text;error correction codes;spread spectrum image steganography;image coding;image processing;hidden message recovery;decoding;information hiding;authentication;performance;invisible map overlays;image tamperproofing;embedded control;revision tracking information hiding digital images spread spectrum image steganography covered writing image size dynamic range hidden message recovery image processing error control coding spread spectrum techniques performance text digital signal data hiding scheme in band captioning hidden communication image tamperproofing authentication invisible map overlays embedded control;data hiding scheme;data mining;image size;steganography;spread spectrum communication;error correction;spread spectrum techniques;hidden communication;error control coding;dynamic range;covered writing;digital image;revision tracking;digital signal;spread spectrum communication image coding security of data error correction codes;steganography digital images data mining spread spectrum communication image processing laboratories image coding decoding dynamic range error correction;in band captioning;digital images;security of data	"""In this paper we present a new method of embedding information within digital images, called Spread Spectrum Image Steganography (SSIS). Steganography, which means \covered writing"""" in Greek, is the science of communicating in a hidden manner. SSIS conceals a message of substantial length within digital imagery while maintaining the original image size and dynamic range. The hidden message can be recovered using the appropriate keys without any knowledge of the original image. Image processing, error control coding, and spread spectrum techniques used to conceal the hidden data are described, and the performance of the technique is illustrated. The message embedded by this method can be in the form of text, imagery, or any other digital signal. Applications for such a data-hiding scheme include in-band captioning, hidden communication, image tamperproo ng, authentication, invisible map overlays, embedded control, and revision tracking."""	authentication;digital image;digital signal (signal processing);dynamic range;embedded system;error detection and correction;image processing;image resolution;sql server integration services;steganography;version control	Lisa M. Marvel;Charles T. Retter;Charles G. Boncelet	1998		10.1109/ICIP.1998.723389	computer vision;speech recognition;image processing;computer science;theoretical computer science;digital image processing;spread spectrum;digital image	Graphics	42.253800457545545	-11.143984583827594	68542
c78f106cca431405b7cb2bda433ca22f0a99dfa8	3d video compression with the h.264 codec	3d video encoding;structured light;fringe projection;computer programming;3d scanning;h 264;3d video compression;video;holovideo	Advances in 3D scanning have enabled the real-time capture of high-resolution 3D video. With these advances comes the challenge of streaming and storing this 3D video in a manner that can be quickly and effectively used. To do this, different approaches have been taken, a popular one being image based encoding, which projects from 3D into 2D, uses 2D compression techniques, and then decodes from 2D back to 3D. One such technique that does this is the Holovideo technique, which has been shown to yield great compression ratios. However, the technique was originally designed for the RGB color space and until recently could not be used with codecs that use the YUV color space such as the H.264 codec. This paper addresses this issue, generalizing Holovideo to the YUV color space, allowing it to leverage the H.264 codec. Compression ratios of over 352 : 1 have been achieved when comparing it to the OBJ file format, with mean squared error as low as .204% making it a viable solution for 3D video compression.© (2012) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	codec;data compression;h.264/mpeg-4 avc	Nikolaus Karpinsky;Song Zhang	2012		10.1117/12.907503	video compression picture types;computer vision;video;h.263;structured light;telecommunications;computer science;video capture;video tracking;computer programming;multimedia;video processing;smacker video;h.261;computer graphics (images)	Vision	43.57413855725598	-19.57559590401671	68579
2125762f80e98db643273a2f565cc565484f3dc5	a two-stage fast block matching algorithm using integral projections	integrated project;block matching algorithm	In this paper, a two-stage block matching algorithm (BMA) is proposed. In its first stage, a one-dimensinal distortion measure based on integral projections is introduced to determine the candidate motion vectors, then among which a final motion vector is detected based on the conventional two-dimensional distortion measure. Due to the one-dimensional calculation of a distortion measure, the proposed algorithm combined with the conventional full search (FS) method with a 16 × 16 subblock can reduce computational complexity of the conventional one by a factor of about 4, with its performance almost comparable to that of the conventional one. Simulation results based on the original and noisy image sequences are shown. Also the simulation results of the proposed method combined with the three-step search (TSS) method and Moving Picture Experts Group (MPEG) Simulation Model Three (SM3) are presented. Computer simulation shows that the proposed algorithms combined with conventional ones can reduce computation time significantly with their performances comparable to those of conventional ones.	block-matching algorithm	Joon-Seek Kim;Rae-Hong Park;Byung-Uk Lee	1993	J. Visual Communication and Image Representation	10.1006/jvci.1993.1031	mathematical optimization;discrete mathematics;computer science;theoretical computer science;mathematics;block-matching algorithm;statistics	Vision	48.21957055392508	-19.423747640583482	68603
5759be93f7ec52b3d4b1e255a4387a8139134292	streaming video and rate scalable compression: what are the challenges for watermarking?	internet protocol;digital watermarking;networks;scalable video;video streaming;real time;streaming video;copy protection;video conferencing;video on demand;packet networks;video;video watermarking	Video streaming, or the real-time delivery of video over a data network, is the underlying technology behind many applications including video conferencing, video on demand, and the delivery of educational and entertainment content. In many applications, par- ticularly ones involving entertainment content, security issues, such as conditional access and copy protection, must be addressed. To resolve these security issues, techniques that include encryption and watermarking need to be developed. Since video sequences will often be compressed using a scalable compression technique and transported over a lossy packet network using the Internet Pro- tocol (IP), security techniques must be compatible with the compres- sion method and data transport and be robust to errors. We address the issues involved in the watermarking of rate-scalable video streams delivered using a practical network. Watermarking is the embedding of a signal (the watermark) into a video stream that is imperceptible when the stream is viewed, but can be detected by a watermark detector. Many watermarking techniques have been pro- posed for digital images and video, but the issues of streaming have not been fully investigated. A review of streaming video is pre- sented, including scalable video compression and network trans- port, followed by a brief review of video watermarking and the dis- cussion of watermarking streaming video. © 2004 SPIE and IS&T.		Eugene T. Lin;Christine Podilchuk;Ton Kalker;Edward J. Delp	2004	J. Electronic Imaging	10.1117/1.1632499	internet protocol;video compression picture types;scalable video coding;microsoft video 1;video;h.263;uncompressed video;telecommunications;digital watermarking;computer science;video tracking;multimedia;video processing;smacker video;internet privacy;videoconferencing;video post-processing;computer network;multiview video coding	EDA	42.83397900430937	-20.09269478114825	68670
5e248e002e293e73ceaa6025104b70c146102957	frame error concealment technique using adaptive inter-mode estimation for h.264/avc	adaptive intermode estimation;automatic voltage control motion estimation propagation losses video compression video sequences decoding redundancy payloads extrapolation telecommunication traffic;propagation losses;image motion analysis;error concealment;data compression;frame error concealment technique;adaptive inter mode estimation;motion vectors;h 264 avc;motion vectors frame error concealment technique adaptive inter mode estimation h 264 avc;video coding adaptive estimation error statistics image sequences;extrapolation;video quality;video sequences;indexing terms;video coding data compression image motion analysis image sequences vectors;video coding frame error concealment technique video sequences adaptive intermode estimation spatio temporal redundancy error propagation h 264 avc;video coding;visualization;frame error concealment;automatic voltage control;inter mode estimation;internet;vectors;estimation;error propagation;motion vector;pixel;bidirectional control;compression ratio;error statistics;spatio temporal redundancy;correlation;motion vector extrapolation;adaptive estimation;image sequences;automatic voltage control decoding propagation losses video compression bidirectional control motion estimation filtering extrapolation state estimation redundancy	Since H.264/AVC achieves a high compression ratio by reducing spatio-temporal redundancy in video sequences, the payload of a single packet can often contain a whole frame encoded by H.264/AVC. Therefore, the loss of a single packet does not only cause the loss of a whole frame, but also produce error propagation. In this paper, we propose a novel whole frame error concealment method using adaptive inter-mode estimation for H.264/AVC. Experimental results show that the proposed method exhibits the better PSNR performance as compared with conventional methods.	error concealment;h.264/mpeg-4 avc;network packet;peak signal-to-noise ratio;propagation of uncertainty;software propagation	Min-Cheol Hwang;Jun-Hyung Kim;Hae-Yong Yang;Sung-Jea Ko;Aldo W. Morales	2008	2008 Digest of Technical Papers - International Conference on Consumer Electronics	10.1109/TCE.2008.4470039	data compression;scalable video coding;reference frame;inter frame;residual frame;computer vision;estimation;electronic engineering;the internet;visualization;index term;computer science;video quality;propagation of uncertainty;theoretical computer science;context-adaptive variable-length coding;compression ratio;block-matching algorithm;extrapolation;correlation;pixel;statistics	Robotics	48.15522406784165	-16.58875119856759	68711
ba51974eefc7385098a8e63d421b09ddf8038d07	asymmetric binary tree coding for contour images	image coding;hierarchical coding;video coding;contour coding;binary tree	We consider the coding of featured contours, i.e. texture, object or motion boundaries in images. The MPEG-4 and MPEG-7 video coding standards provide for a content-based representation of video information, so efficient coding of boundaries can play an important role. We propose a new coding scheme that uses an asymmetric binary tree. We show that the new scheme outperforms conventional quadtree, binary tree, contour and READ coding algorithms applied to typical boundary images and also offers competitive performance for general-purpose two-level image coding.	algorithm;binary tree;contour line;data compression;general-purpose modeling;mpeg-7;quadtree;video coding format	Ahsan Shamim;John A. Robinson	2003	Image Vision Comput.	10.1016/S0262-8856(03)00093-3	sub-band coding;computer vision;shannon–fano coding;binary tree;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;pattern recognition;modified huffman coding;tunstall coding;context-adaptive binary arithmetic coding;huffman coding	Vision	44.76866981097482	-14.977482056902831	68776
5587d874f87697d073f8c16b3fcce730d54f6ea9	error concealment for frame losses in mdc	desciframiento;codificacion de video;remolded sample;echantillon remanie;decoding streaming media video compression resilience erbium error correction video coding interpolation forward error correction gold;error recovery;interpolation;video quality error concealment multiple description coding error resilience technique video coding;image coding;error concealment;error resilience technique;decodage;decoding;temporal interpolation error concealment error propagation error resilience mdc multihypothesis;video quality;video coding interpolation;indexing terms;qualite image;muestra modificada;propagation erreur;error analysis;codage image;video coding;description coding;codage video;error propagation;image quality;multihypothesis;multiple description coding;error resilience;calidad imagen;mdc;propagacion error;growth of error;temporal interpolation;propagation	Multiple description coding (MDC) is an effective error resilience (ER) technique for video coding. In case of frame loss, error concealment (EC) techniques can be used in MDC to reconstruct the lost frame, with error, from which subsequent frames can be decoded directly. With such direct decoding, the subsequent decoded frames will gradually recover from the frame loss, though slowly. In this paper we propose a novel algorithm using multihypothesis error concealment (MHC) to improve the error recovery rate of any EC in the temporal subsampling MDC. In MHC, the simultaneous temporal-interpolated frame is used as an additional hypothesis to improve the reconstructed video quality after the lost frame. Both subjective and objective results show that MHC can achieve significantly better video quality than direct decoding.	algorithm;chroma subsampling;data compression;error concealment;interpolation;model of hierarchical complexity;multiple description coding;pixel;propagation of uncertainty;simulation;video	Mengyao Ma;Oscar C. Au;Liwei Guo;Shueng-Han Gary Chan;Peter Hon-Wah Wong	2008	IEEE Transactions on Multimedia	10.1109/TMM.2008.2007282	image quality;residual frame;computer vision;index term;telecommunications;interpolation;computer science;video quality;propagation of uncertainty;multiple description coding;statistics	Vision	47.70153524418274	-15.62715822040794	68954
73c79114c383bca00d7c3633d232326b6c5e3f24	an optimal method for searching uep profiles in wireless jpeg 2000 video transmission	video communication channel coding data compression image coding mathematical analysis source coding;channel coding;science and technology;image coding;ucl;data compression;discovery;theses;conference proceedings;mathematical analysis;digital web resources;uep profiles searching binary symmetric channel mathematical relationship from intermediate rate distortion traces psnr perspective lagrangian optimization memoryless transmission channel embedded image compression standard optimal source channel code allocation wireless jpeg 2000 video transmission;ucl discovery;open access;decision support systems;forward error correction jpeg 2000 wireless video lagrangian optimization;ucl library;book chapters;open access repository;video communication;source coding;ucl research	In this paper, we present a theoretical method to find an optimal source and channel code allocation, by considering an embedded image compression standard such as JPEG 2000 and a memoryless transmission channel. Lagrangian optimization is used to find an optimal error correction profile, from a PSNR perspective, starting from intermediate rate-distortion traces. The resulting mathematical relationship is simple and can be computed in real time. Simulations on a binary symmetric channel show that the achieved performance, in terms of PSNR, is comparable with that of similar methods reported in literature, while keeping a lower complexity.	binary symmetric channel;channel (communications);coding theory;computer simulation;distortion;embedded system;error detection and correction;image compression;jpeg 2000;mathematical optimization;peak signal-to-noise ratio;tracing (software)	Giuseppe Baruffa;Fabrizio Frescura;Paolo Micanti;Barbara Villarini	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467192	data compression;decision support system;channel code;telecommunications;computer science;theoretical computer science;multimedia;statistics;source code;science, technology and society	Robotics	48.24921159553475	-13.395783046453573	68969
0cc3e3b56fbddec388c95210093e1a57e3b8c85a	pattern based robust digital watermarking scheme for images	watermarking;image coding;psnr;digital watermark;image coding watermarking robustness discrete cosine transforms lead laboratories psnr;lead;discrete cosine transforms;image quality;robustness;digital image;visual masking;image similarity	Digital Watermarking is an effective and popular technique to discourage illegal copying and distribution of copyrighted digital image information. The important attributes are the picture quality of the watermarked image (similarity to the original) and robustness to attacks such as cropping. We propose a transform-domain robust digital watermarking technique which uses a pattern-based compression of the watermark image, an intelligent dynamic embedding of the signature bits and a post-watermarking content-based visual masking technique to deliver high image quality and robustness in retaining watermark content against attacks (cropping).	digital image;digital watermarking;image quality;preprocessor;self-information;video post-processing	Aniruddha Sinha;Amitava Das;S Pandith SunilPandith	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745404	computer vision;digital watermarking;computer science;electrical engineering;theoretical computer science;digital image processing;mathematics;multimedia;watermark	EDA	40.59979053986142	-11.192705198821876	68970
83ca353037b49cb4f73c9bb6837e4efad22d9ec6	intra prediction method based on the linear relationship between the channels for yuv 4∶2∶0 intra coding	prediction methods image coding decorrelation color automatic voltage control video compression motion pictures compression algorithms parameter estimation predictive models;prediction method;channel coding;image coding;data compression;decoding;color space;chrominance channels yuv 4 2 0 intra coding rgb transformation yuv color space interchannel correlation linear function model intra chrominance prediction method color image compression chrominance pixels luminance signal reconstruction h 264 avc implicit prediction method luminance channel;intra prediction;linear functionals;inter channel correlation;video coding;automatic voltage control;prediction theory;image color analysis;image colour analysis;image reconstruction;pixel;linear model;inter channel correlation intra coding;intra coding;video coding channel coding data compression image colour analysis image reconstruction prediction theory;correlation;side information;color image compression	In general, the transformation from RGB to YUV color space reduces the correlation between the channels. But some sources in the YUV space still have strong inter-channel correlation, which can be modeled as a linear function. Based on this linear model, we propose a new intra chrominance prediction method for color image compression in YUV 4∶2∶0 color space. A block of chrominance pixels to be encoded is predicted from the reconstructed luminance signal using the proposed prediction scheme and the residual is encoded based on the H.264/AVC. Also, an implicit prediction method that can obviate the transmission of side information is proposed. The experimental results show that there are about 0.35∼1.0dB gains for the luminance (Y) channel, and 0.5∼3.0dB gains for the chrominance (Cb or Cr) channels at the medium to high bit-rates. The proposed method also shows some gains at the low bit-rates.	color image;color space;h.264/mpeg-4 avc;image compression;intra-frame coding;linear function;linear model;pixel	Sang Heon Lee;Nam Ik Cho	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413727	data compression;iterative reconstruction;computer vision;channel code;computer science;linear model;mathematics;color space;correlation;pixel;statistics;computer graphics (images)	Robotics	44.878172400280235	-17.25709217601376	69153
5e2c46b755e739f17d68c7f6ccb621921b294e6a	enhanced cross-diamond-hexagonal search algorithms for fast block motion estimation	diamond search;search algorithm;motion estimation;cross diamond hexagonal search algorithms;indexing terms;video coding;fast block motion estimation;video coding search problems motion estimation;motion estimation shape computational efficiency probability distribution video coding iso standards computational complexity video sequences switches paper technology;cross shaped pattern;cross shaped pattern cross diamond hexagonal search algorithms fast block motion estimation video coding;search problems	This paper proposes two enhanced cross-diamond-hexagonal search algorithms to solve the motion-estimation problem in video coding. These algorithms differ from each other by their second step search only, and both of them employ cross-shaped pattern in first step. Proposed method is an improvement over CDHS, which eliminates some checking points of CDHS algorithm. Experimental results show that the proposed methods perform faster than the diamond search (DS) and CDHS, whereas similar quality is preserved.	data compression;motion estimation;search algorithm	Amir Moradi;Rouhollah Dianat;Shohreh Kasaei;Mohammad T. Manzuri Shalmani	2005	IEEE Conference on Advanced Video and Signal Based Surveillance, 2005.	10.1109/AVSS.2005.1577329	beam search;computer vision;mathematical optimization;index term;quarter-pixel motion;computer science;theoretical computer science;motion estimation;motion compensation;search algorithm	Vision	48.60593315474832	-19.28592492559851	69226
bd0a131602e9cc99517101d037de58eeb8d73e13	fast zero block detection and early cu termination for hevc video coding	transforms encoding video coding standards accuracy transform coding quantization signal;video coding;proceedings paper;coding unit early cu termination hevc video coding fast zero block detection high efficiency video coding sum of absolute difference value sad value preskip detection	This paper proposed a fast zero block detection for various transform size from 32×32 to 4×4 in the new generation of the High Efficiency Video Coding (HEVC) standard. The derivation is based on sum-of-absolute-difference (SAD) value available in the inter prediction computation. The proposed method achieves detection accuracy to 90% in average, and saves transform unit computation by 44% (QP at 22) and 65% (QP at 32) with negligible coding performance loss, when compared with that of HM4.0rc1. Additionally, this pre-skip detection could further help decide the CU inter mode efficiently with about 50% time saving.	computation;high efficiency video coding;tip (unix utility)	Pai-Tse Chiang;Tian-Sheuan Chang	2013	2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)	10.1109/ISCAS.2013.6572177	sub-band coding;real-time computing;telecommunications;computer science;theoretical computer science;coding tree unit;context-adaptive binary arithmetic coding;h.261;multiview video coding	Arch	47.067543548640536	-19.30799485591668	69359
1a00f02fdee4cda7b5a45e953e61f4ede52569e9	fast mode decision based on optimal stopping theory for multiview video coding		Optimal stopping theory is developed to achieve a good trade-off between decision performance and decision efforts such as the consumed decision time. In this paper, the optimal stopping theory is applied to fast mode decision for multiview video coding in order to reduce the tremendous encoding computational complexity, with the benefit of theoretical decision-making expectation and predictable decision performance. The characteristics of encoding modes in multiview video coding are studied to derive an optimal stopping theory-based model to early terminate mode decision and thus a fast mode decision algorithm is designed. Extensive experimental results demonstrate that the proposed algorithm can save a great amount of encoding time for multiview video coding and meanwhile keep the compression performance more or less intact.	algorithm;computation;computational complexity theory;data compression;model–view–controller;multiview video coding;optimal stopping;terminate (software)	Hanli Wang;Yue Heng;Tiesong Zhao;Bo Xiao	2013		10.1007/978-3-642-35728-2_17	real-time computing;simulation;computer science;theoretical computer science	AI	47.25198478530177	-19.19214255343524	69529
91b83318398ae8f01490889cbe5ba9c11ccbc5cb	a reversible watermarking scheme for vector maps based on multilevel histogram modification		To protect the security of vector maps, we propose a novel reversible watermarking scheme for vector maps based on a multilevel histogram modification. First, a difference histogram is constructed using the correlations of adjacent coordinates, and the histogram is divided into continuous regions and discontinuous regions by combining the characteristics of vector map data. Second, the histogram bins that require modification are determined in the continuous regions through the optimal peak value, and the peak values are chosen from the flanking discontinuous regions in both directions; the watermarks are embedded by adopting the multilevel histogram modification strategy. The watermark extraction process is the reverse of the embedding process, and after completing the watermark extraction, the carrier data can be recovered losslessly. The experimental results show that the proposed algorithm has good invisibility and is completely reversible. Compared with similar algorithms reported previously, it achieves higher watermark embedding capacity under the same embedding distortion with lower complexity, thereby having a higher application value.	algorithm;digital watermarking;distortion;embedded system;image histogram;lossless compression;multilevel security;vector map	Xiang Hou;Lianquan Min;Hui Yang	2018	Symmetry	10.3390/sym10090397	combinatorics;digital watermarking;vector map;mathematics;histogram;artificial intelligence;pattern recognition	ML	39.71157616615408	-10.9104885773683	69597
2c46376f4e8a9f4029e3a65eb0b14972b346fc6f	probabilistic mapping of dynamic obstacles using markov chains for replanning in dynamic environments	markov chain model;dynamic objects;risk map;worst case prediction;probability;markov chain model probabilistic mapping dynamic obstacles dynamic environment time efficient navigation worst case prediction obstacle dynamics dynamic objects static objects path planning occlusion estimation risk regions space searching;probabilistic mapping;static objects;path planning;dynamic obstacles;obstacle dynamics;mobile robots;probability collision avoidance markov processes mobile robots;time delay;acceleration;risk regions;robots probabilistic logic path planning computational modeling acceleration robot kinematics trajectory;dynamic environment;computational modeling;trajectory;space searching;occlusion estimation;robots;collision avoidance;markov processes;probabilistic logic;time efficient navigation;robot kinematics;markov chain	Robots acting in populated environments must be capable of safe but also time efficient navigation. Trying to completely avoid regions resulting from worst case predictions of the obstacle dynamics may leave no free space for a robot to move, especially in environments with high dynamic. This work presents an algorithm for a ldquosoftrdquo risk mapping of dynamic objects leaving the complete space free of static objects for path planning. Markov Chains are used to model the dynamics of moving persons and predict their potential future locations. These occlusion estimations are mapped into risk regions which serve to plan a path through potentially obstructed space searching for the trade-off between detour and time delay. The offline computation of the Markov Chain model keeps the computational effort low, making the approach suitable for online applications.	algorithm;best, worst and average case;broadcast delay;computation;markov chain;motion planning;online and offline;population;robot	Florian Rohrmüller;Matthias Althoff;Dirk Wollherr;Martin Buss	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4650952	computer vision;markov chain;simulation;computer science;artificial intelligence;machine learning;statistics	Robotics	52.840170449614476	-23.49940855724476	70045
c9085c16c49840484963dc017e30dbeaa209f1a6	segmentation-based optimized tone mapping for high dynamic range image and video coding	image segmentation image resolution video coding table lookup;tone mapping;hdr mse segmentation based optimized tone mapping high dynamic range image coding hdr displays video coding video compression visible luminance range video codecs low dynamic range displays bright region dark region mean square error;image segmentation;data compression;segmentation;video coding;image compression;hdr;mean square error methods;video coding data compression image segmentation mean square error methods	A core part of the state-of-the art high dynamic range (HDR) image and video compression methods is the tone mapping operation to convert the visible luminance range into the finite bit depths that can be supported by the current video codecs. These conversions are until now optimized to provide backward compatibility to the existing low dynamic range (LDR) displays. However, a direct application of these methods for the emerging HDR displays can result in a loss of details in the bright and dark regions of the HDR content. In this paper, we overcome this limitation by designing a tone mapping operation which handles the bright and dark regions separately. The proposed method first finds the optimal segmentation of the HDR image into two parts, namely dark and bright regions, and then designs the optimal tone mapping for each region in terms of the mean square error between the logarithm of the luminance values of the original and reconstructed HDR content (HDR-MSE). The results indicate the superiority of the proposed method over the state-of-the art HDR coding methods.	backward compatibility;codec;data compression;digital video;h.264/mpeg-4 avc;high dynamic range;high-dynamic-range rendering;ldraw;mean squared error;range imaging;simulation;tone mapping	Paul Lauga;Alper Koz;Giuseppe Valenzise;Frédéric Dufaux	2013	2013 Picture Coding Symposium (PCS)	10.1109/PCS.2013.6737732	data compression;computer vision;tone mapping;telecommunications;image compression;computer science;image segmentation;segmentation;algorithm;computer graphics (images)	Graphics	43.978979502803575	-18.00126414810503	70054
50a0ab8738cb5f9babb663d742eabe9f7cdb2588	standard compliant flicker reduction method with psnr loss control	h 264 flicker temporal filtering;losses;decoding;h 264 avc video coding standard standard compliant flicker reduction method psnr loss control standard decoding implementation luminance block by block basis selective temporal low pass filtering image blurring effect;image restoration;video coding;psnr standards video coding encoding measurement quantization signal decoding;video coding block codes decoding flicker noise image restoration losses low pass filters;low pass filters;flicker noise;block codes	Flicker is a common video coding artifact that occurs especially at low and medium bit rates. In this paper we propose a temporal filter-based method to reduce flicker. The proposed method has been designed to be compliant with conventional video coding standards, i.e., to generate a bitstream that is decodable by any standard decoder implementation. The aim of the proposed method is to make the luminance changes between consecutive frames smoother on a block-by-block basis. To this end, a selective temporal low-pass filtering is proposed that smooths these luminance changes on flicker-prone blocks. Furthermore, since the low-pass filtering can incur in a noticeable blurring effect, an adaptive algorithm that allows for limiting the PSNR loss -and thus the blur- has also been designed. The proposed method has been extensively assessed on the reference software of the H.264/AVC video coding standard and compared to a state-of-the-art method. The experimental results show the effectiveness of the proposed method and prove that its performance is superior to that of the state-of-the-art method.	adaptive algorithm;bitstream;data compression;flicker (screen);flicker noise;h.264/mpeg-4 avc;low-pass filter;peak signal-to-noise ratio;video coding format	Amaya Jimenez-Moreno;Eduardo Martínez-Enríquez;Fernando Díaz-de-María	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637948	flicker noise;scalable video coding;block code;image restoration;computer vision;low-pass filter;computer science;mathematics;computer graphics (images)	Robotics	45.019583398235895	-17.69490305379487	70380
17c2b9c2c23f5f6e7458b1650b89e103e0b61e07	spatially adaptive wavelet thresholding for image watermarking	discrete wavelet transforms;watermarking;dwt;image coding;discrete wavelet transform;probability;jpeg compression;jpeg compression image watermarking spatially adaptive wavelet thresholding discrete wavelet transform dwt image denoising semiblind watermark extraction probability;image segmentation;data compression;spatially adaptive wavelet thresholding;wavelet thresholding;transform coding;data encapsulation;watermarking data compression data encapsulation discrete wavelet transforms feature extraction image coding image denoising image segmentation probability;feature extraction;noise reduction;probability of false alarm;bandwidth;robustness;humans;image denoising;image watermarking;frequency;watermarking discrete wavelet transforms frequency image denoising wavelet coefficients humans noise reduction robustness bandwidth transform coding;semiblind watermark extraction;wavelet coefficients	In this paper, we introduce a new robust image watermarking technique based on the discrete wavelet transform (DWT). The proposed method extends the concept of image denoising to watermarking. A spatially adaptive wavelet thresholding method is used to select the coefficients to be watermarked. A multi-bit watermark is embedded into the discrete wavelet coefficients of the host image. A semi-blind watermark extraction algorithm is presented and the threshold for a given probability of false alarm is derived. The simulation results show that the proposed method outperforms a well-known DWT based watermarking method under most attacks including JPEG compression	algorithm;coefficient;digital watermarking;discrete wavelet transform;embedded system;jpeg;newton's method;noise reduction;semiconductor industry;simulation;thresholding (image processing);watermark (data file)	Mahmood Al-khassaweneh;Selin Aviyente	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262851	data compression;computer vision;transform coding;speech recognition;second-generation wavelet transform;feature extraction;digital watermarking;computer science;frequency;pattern recognition;noise reduction;probability;cascade algorithm;wavelet packet decomposition;stationary wavelet transform;image segmentation;discrete wavelet transform;bandwidth;statistics;robustness;wavelet transform	EDA	41.64699247000639	-10.887934487490014	70525
48daf13b3944ffecbd049374bfd121cbe0cb7633	an efficient rate-distortion optimal shape coding approach utilizing a skeleton-based decomposition	minimisation;camino mas corto;directed graphs;directed acyclic graph;dynamic programming;optimal solution;theorie vitesse distorsion;shortest path;optimisation;object based video compression;programacion dinamica;image coding;image processing;data compression;approximation error;optimizacion;esqueleto;video signal processing;shortest path algorithm;efficient algorithm;metodo descomposicion;search algorithm;methode decomposition;procesamiento imagen;lagrangian relaxation shape coding rate distortion optimality skeleton based decomposition boundary distance approximation error bit budget curve approximation video frame control point location four dimensional dag direct acyclic graph shortest path algorithm computational complexity suboptimal greedy trellis search algorithm relaxed distortion criterion object based video compression skeletonization distortion minimization operational rate distortion optimal approach ord optimal approach;plus court chemin;lagrange multiplier;operational rate distortion;indexing terms;traitement image;skeleton;rate distortion theory;approximation theory;codage image;shape coding;video coding;decomposition method;compression image;image compression;computational complexity;video coding image thinning rate distortion theory search problems data compression minimisation relaxation theory curve fitting approximation theory directed graphs;multiplicateur lagrange;programmation dynamique;boundary coding;traitement signal video;multiplicador lagrange;squelette;rate distortion shape image coding skeleton video compression computational complexity mpeg 4 standard approximation error distortion lagrangian functions;optimization;skeleton decomposition;search problems;skeletonization;curve fitting;image thinning;optimal algorithm;rate distortion optimization;relaxation theory;lagrangian relaxation;admission control;compresion imagen	In this paper, we present a new shape-coding approach, which decouples the shape information into two independent signal data sets; the skeleton and the boundary distance from the skeleton. The major benefit of this approach is that it allows for a more flexible tradeoff between approximation error and bit budget. Curves of arbitrary order can be utilized for approximating both the skeleton and distance signals. For a given bit budget for a video frame, we solve the problem of choosing the number and location of the control points for all skeleton and distance signals of all boundaries within a frame, so that the overall distortion is minimized. An operational rate-distortion (ORD) optimal approach using Lagrangian relaxation and a four-dimensional direct acyclic graph (DAG) shortest path algorithm is developed for solving the problem. To reduce the computational complexity from O(N(5)) to O(N(3)), where N is the number of admissible control points for a skeleton, a suboptimal greedy-trellis search algorithm is proposed and compared with the optimal algorithm. In addition, an even more efficient algorithm with computational complexity O(N(2)) that finds an ORD optimal solution using a relaxed distortion criterion is also proposed and compared with the optimal solution. Experimental results demonstrate that our proposed approaches outperform existing ORD optimal approaches, which do not follow the same decomposition of the source data.	approximation error;choose (action);computational complexity theory;dijkstra's algorithm;directed acyclic graph;distortion;graph - visual representation;greedy algorithm;inguinal hernia, direct;lagrangian relaxation;linear programming relaxation;rate–distortion theory;search algorithm;shape context;short;shortest path problem;source data;trellis quantization	Haohong Wang;Guido M. Schuster;Aggelos K. Katsaggelos;Thrasyvoulos N. Pappas	2003	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2003.816570	data compression;skeletonization;minimisation;mathematical optimization;approximation error;combinatorics;discrete mathematics;directed graph;dijkstra's algorithm;decomposition method;index term;rate–distortion theory;lagrangian relaxation;image processing;image compression;computer science;machine learning;dynamic programming;mathematics;rate–distortion optimization;shortest path problem;lagrange multiplier;computational complexity theory;skeleton;directed acyclic graph;algorithm;statistics;curve fitting;search algorithm;approximation theory	Vision	47.385110861739946	-16.04528090152728	70671
32d3887ba95fd30a7a420739c4ca04376f4af813	tthresh: tensor compression for multidimensional visual data		Memory and network bandwidth are decisive bottlenecks when handling high-resolution multidimensional data sets in visualization applications, and they increasingly demand suitable data compression strategies. We introduce a novel lossy compression algorithm for N -dimensional data over regular grids. It leverages the higher-order singular value decomposition (HOSVD), a generalization of the SVD to 3 and more dimensions, together with adaptive quantization, run-length and arithmetic coding to store the HOSVD transform coefficients’ relative positions as sorted by their absolute magnitude. Our scheme degrades the data particularly smoothly and outperforms other state-of-the-art volume compressors at low-to-medium bit rates, as required in data archiving and management for visualization purposes. Further advantages of the proposed algorithm include extremely fine bit rate selection granularity, bounded resulting l2 error, and the ability to manipulate data at very small cost in the compression domain, for example to reconstruct subsampled or filtered-resampled versions of all (or selected parts) of the data set.	algorithm;archive;arithmetic coding;coefficient;cylinder-head-sector;data compression;emoticon;foremost;image resolution;lossy compression;quantization (signal processing);research data archiving;run-length encoding;singular value decomposition;smoothing	Rafael Ballester-Ripoll;Peter Lindstrom;Renato Pajarola	2018	CoRR		theoretical computer science;lossy compression;quantization (signal processing);data compression;visualization;singular value decomposition;arithmetic coding;computer science;algorithm;data set;bandwidth (signal processing)	Visualization	41.58168596558261	-17.851819494529774	70737
268a9a2638b84712cc528d947577873dbe132d9e	early disparity estimation skipping for multi-view video coding	signal image and speech processing;information systems applications incl internet;communications engineering networks	Multi-view video (MVV) and multi-view plus depth video became typical formats for 3D video systems which offer more realistic visual effects for consumers. The standard for encoding of MVV, known as multi-view video coding (MVC), was produced by ISO/IEC and ITU-T and has exhibited significant compression efficiency for MVV signals. However, the high computational complexity of MVC leads to excessive power consumption and limits its applications. In this study, new fast algorithms for MVC are proposed by skipping disparity estimation prediction. According to the characteristics of the motion and disparity vectors, unnecessary disparity estimation is skipped based on the amount of motion that can be measured through the use of motion activity and SKIP mode on the temporal axis. Experimental results showed that by skipping the disparity estimation, the proposed method reduced the encoding computational complexity by nearly 73% for inter-view prediction and 38% in the overall encoding process of B-views. In addition, no noticeable visual quality degradation was observed.	asp.net mvc;algorithm;apache axis;binocular disparity;computational complexity theory;data compression;elegant degradation;model–view–controller;time complexity;video;visual effects	Jungdong Seo;Kwanghoon Sohn	2012	EURASIP J. Wireless Comm. and Networking	10.1186/1687-1499-2012-32	computer vision;real-time computing;simulation;computer science	Vision	46.78224775772556	-19.12884498638024	70833
93ec0d9f06ab5146841e18cc19306c3cce0dac6d	a generalized vq method for combined compression and estimation	optimisation;image coding;deterministic annealing;image coding vector quantisation source coding computational complexity optimisation approximation theory;probabilistic approach;approximation theory;computational complexity;image compression generalized vector quantization vq quantized estimation data compression codebook random vector source coding noisy sources quantized approximation complexity structurally constrained encoder optimization probabilistic approach deterministic annealing;source code;vector quantizer;vector quantisation;local minima;source coding;semiconductor device noise source coding vector quantization acoustical engineering noise reduction speech distortion measurement information processing annealing digital signal processing	"""In vector quantization, one approximates an input random vector, Y, by choosing from a nite set of values known as the codebook. We consider a more general problem where one may not have direct access to Y but only to some statistically related random vector X. We observe X and would like to generate an approximation to Y from a codebook of candidate vectors. This operation, called generalized vector quantization (GVQ), is essentially that of quantized estimation. An important special case of GVQ is the problem of noisy source coding wherein a quantized approximation of a vector, Y, is obtained from observation of its noise-corrupted version, X. The optimal GVQ encoder has high complexity. We overcome the complexity barrier by optimizing a structurally-constrained encoder. This challenging optimization task is solved via a probabilistic approach, based on deterministic annealing (DA), which overcomes problems of shallow local minima that trap simpler descent methods. We demonstrate the successful application of our method to the coding of noisy sources. Consider the problem of estimating a random vector, Y from a statistically related vector, X. If the estimate V (X) is constrained to take on values from a nite set of N \estimation vectors"""", the mapping from X to Y is called generalized vector quantization (GVQ). Note that GVQ reduces to ordinary VQ in the special case where X = Y. A wide range of applications in source coding can be formulated as GVQ problems, including: (a) noisy source coding problems where Y is the signal of interest, but only a noise-corrupted version, X, is available to the encoder; (b) complexity-constrained source coding problems wherein Y has a large dimensionality and must be replaced by lower dimensional \feature"""" vectors X that are extracted from Y prior to quantization. GVQ Figure 1: Block diagram of a Generalized Vector Quantizer While a quantized estimator is mandated for data compression problems by the rate constraint, for other estimation problems, quantization may also be usefully employed to limit complexity and to facilitate implementability of the estimator, as the optimal estimator may be diicult to evaluate and may require a large complexity of speciication. For N suuciently large, a quantized estimator will closely approximate the optimal estimator, while achieving a practical implementation. Formulation We treat the mapping of a feature vector, X, to a nite-valued estimate of a vector Y as a single operation, called generalized vector quantization (GVQ). Mathematically, …"""	approximation algorithm;codebook;data compression;diagram;emoticon;encoder;estimation theory;feature vector;mathematical optimization;maxima and minima;quantization (signal processing);random access;simulated annealing;vector quantization	Ajit V. Rao;David J. Miller;Kenneth Rose;Allen Gersho	1996		10.1109/ICASSP.1996.544855	mathematical optimization;computer science;theoretical computer science;machine learning;mathematics;linde–buzo–gray algorithm;vector quantization;source code	ML	49.30963035212347	-11.835587177699146	71411
917229eb94be1e1cee4e09a1057c5175fb29a54d	the optimum space-frequency partition for subband image coding in a rate-distortion sense	discrete wavelet transforms;rate distortion;image coding;frequency domain analysis;quantization signal;optimization;entropy	Subband coding is a powerful means for highly efficient image signal compression. We have proposed the optimum frequency band partition (OFBP) to improve the compression performance of subband image coding. OFBP determines an adaptive partition pattern on the 2-dimentional frequency domain considering the characteristics of an input image under the condition of a small number of subbands. However, the OFBP cannot obtain the optimum partition pattern in terms of rate-distortion sense for image compression. This is caused by the partition pattern being fixed, even if the desired bit rate changes. This paper presents an optimum space-frequency partition (OSFP) as a new OFBP. The optimized factors include the band partition on the 2-dimentional frequency domain, the combination of quantizers within subbands and the pruning of coefficient nodes in each subband (also referred to as the spatial domain partition), are optimized in terms of rate-distortion sense in case of an arbitrary given bit rate budget. Experimental results show the outperformance of several conventional image compression methods by the proposed OSFP coder.	algorithm;coefficient;discrete wavelet transform;distortion;frequency band;frequency partition of a graph;image compression;mathematical optimization;network scheduler;optimization problem;peak signal-to-noise ratio;rate–distortion optimization;regular expression;signal compression;sub-band coding	Haruhiko Miyazaki;Masashi Kameda	2016	2016 Picture Coding Symposium (PCS)	10.1109/PCS.2016.7906389	entropy;mathematical optimization;discrete mathematics;mathematics;frequency domain	Vision	44.56910809392379	-13.880251344125377	71698
e810bed0728812d188f5932c74fe316fa42cddbb	a hierarchical representation policy iteration algorithm for reinforcement learning	reinforcement learning;state space decomposition;hrpi	This paper presents a hierarchical representation policy iteration (HRPI) algorithm. It is based on the method of state space decomposition implemented by introducing a binary tree. Combining the RPI algorithm with the state space decomposition method, the HRPI algorithm is proposed. In HRPI, the state space is decomposed into multiple sub-spaces according to an approximate value function, then the local policies are estimated on each sub-space and finally the global near-optimal policy is obtained by combining these local policies. The simulation results indicate that the proposed method has better performance compared to the conventional RPI algorithm.	algorithm;iteration;iterative method;reinforcement learning	Jian Wang;Lei Zuo;Jian Wang;Xin Xu;Chun Li	2012		10.1007/978-3-642-36669-7_89	mathematical optimization;theoretical computer science;machine learning;mathematics	ML	39.38026125258533	-23.460543212045593	71736
a1f571358b1010f3e82f92ce5d71d3fb025e958e	efficient quantization noise reduction device for subband image coding schemes	image coding;data compression;quantization noise;wiener filters;visual quality;quantisation signal;interference signal;adaptive filters;image reconstruction;noise reduction;psnr value efficient quantization noise reduction device subband image coding schemes high compression factors ringing effect high contrast contours blurred false contours large smooth regions first distortion subband filters noise reduction technique dc subband subband decompositions colored quantization noise white noise roberts pseudonoise technique wiener type filter adaptive directional support reconstructed image visual quality;image reconstruction image coding quantisation signal white noise interference signal data compression pseudonoise codes wiener filters adaptive filters;lts1;quantization noise reduction image coding adaptive filters wiener filter psnr image reconstruction colored noise rendering computer graphics;pseudonoise codes;white noise;color quantization	This paper addresses the problem of the quantization noise reduction in subband image coding schemes. Two major artifacts occur for such coding schemes at high compression factors: the ringing effect around highcontrast contours and the blurred false contours in large smooth regioins. The first distortion can be considerably reduced by an appropriate design of the subband filters. The second one can be eliminated by using the noise reduction technique proposed in this paper, which consists of applying a noise reduction filter to the DC subband. Thle advantages of this approach are as follows: First, it can be applied to any kind of subband decompositioins. Second, it removes quantization noise to which the eye is most sensitive and third, it is computationally very efficient due to the small size (typically 64 x 64) of the DC subband. The colored quantization noise in the DC subband is rendered white by using the Roberts pseudonoise technique. The proposed noise reduction filter is a Wiener type filter with adaptive directional support. It has the advantage of reducing the noise without blurring the reconstructed image. It is shown that the proposed noise reduction filter augments the visual quality of the reconstructed image as well as its PSNR value.	distortion;filter (software);noise reduction;peak signal-to-noise ratio;pseudorandom noise;quantization (signal processing);ringing (signal);sub-band coding;wiener filter	Wei Li;Olivier Egger;Murat Kunt	1995		10.1109/ICASSP.1995.479915	data compression;iterative reconstruction;gradient noise;adaptive filter;gaussian noise;median filter;image noise;computer vision;color quantization;speech recognition;quantization;value noise;computer science;noise reduction;mathematics;white noise;statistics;salt-and-pepper noise	EDA	44.98056720558044	-15.291707055452278	71749
17f30b402ed69d97403f9f8e68c0891a2b192002	high fidelity adaptive vector quantization at very low bit rates for progressive transmission of radiographic images	quantization;high resolution;deterministic annealing;adaptive vector quantization;scalar quantization;progressive transmission;medical image;image compression;probability distribution;optimality criteria;generalized gaussian;self organization;visual perception;vector quantizer;bit allocation;neural network	An adaptive vector quantizer (VQ) using a clustering technique known as adaptive fuzzy leader clustering (AFLC) that is similar in concept to deterministic annealing (DA) for VQ codebook design has been developed. This vector quantizer, AFLC-VQ, has been designed to vector quantize wavelet decomposed sub images with optimal bit allocation. The high-resolution sub images at each level have been statistically analyzed to conform to generalized Gaussian probability distributions by selecting the optimal number of filter taps. The adaptive characteristics of AFLC-VQ result from AFLC, an algorithm that uses self-organizing neural networks with fuzzy membership values of the input samples for upgrading the cluster centroids based on well known optimization criteria. By gen- erating codebooks containing codewords of varying bits, AFLC-VQ is capable of compressing large color/monochrome medical images at extremely low bit rates (0.1 bpp and less) and yet yielding high fidelity reconstructed images. The quality of the reconstructed im- ages formed by AFLC-VQ has been compared with JPEG and EZW, the standard and the well known wavelet based compression tech- nique (using scalar quantization), respectively, in terms of statistical performance criteria as well as visual perception. AFLC-VQ exhibits much better performance than the above techniques. JPEG and EZW were chosen as comparative benchmarks since these have been used in radiographic image compression. The superior perfor- mance of AFLC-VQ over LBG-VQ has been reported in earlier pa- pers. © 1999 SPIE and IS&T. (S1017-9909(99)01301-X)	radiography;vector quantization	Sunanda Mitra;Shuyu Yang	1999	J. Electronic Imaging	10.1117/1.482681	probability distribution;computer vision;self-organization;image resolution;quantization;visual perception;image compression;computer science;theoretical computer science;machine learning;mathematics;artificial neural network;algorithm	Vision	43.710496220151285	-13.962987347941395	72125
eb25cf38aa70efa22a5aa7569347118ed9f5b2a2	signal processing techniques for haptic data compression in teleoperation systems	performance measure;prediction method;data compression;signal sampling;virtual reality;operational rate distortion;deadband;rate distortion theory;performance improvement;prediction theory;signal processing;dynamic range;operational rate distortion signal processing haptic compression teleoperation virtual reality haptic interface deadband perception based coding;perception based coding;operational rate distortion measure signal processing technique haptic data compression networked teleoperation system haptic interface rate distortion optimized quantizer prediction method fixed rate down sampling virtual environment;compression ratio;force haptic interfaces bit rate humans quantization decoding;compression;virtual environment;haptic interfaces;haptic;rate distortion optimization;signal sampling data compression haptic interfaces prediction theory rate distortion theory;teleoperation;haptic interface	In this paper, we present various signal processing techniques for haptic data compression in networked teleoperation systems or haptic interfaces. A novel rate-distortion optimized quantizer is introduced to compress haptic data whose dynamic range is limited. Prediction methods, which are primarily based on fixed rate down-sampling, are also presented and combined with the proposed quantizer for performance improvement. Experiments are performed using a haptic interface in a virtual environment to evaluate the proposed signal processing techniques. An objective performance measure, which is called the operational rate-distortion measure, is introduced to evaluate not only the compression ratio, but also the quality of reconstructed haptic data.	computation;data compression;distortion;dynamic range;haptic technology;most significant bit;peak signal-to-noise ratio;propagation of uncertainty;quantization (signal processing);sampling (signal processing);signal processing;software propagation;video;virtual reality	Jae-young Lee;Shahram Payandeh	2012	2012 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTIC.2012.6183817	control engineering;computer vision;simulation;computer science	Robotics	44.103361141544745	-18.17708616898183	72256
7d05bbdcd612654dd1b4e5c9f8cba0091b5efbbc	a new intra prediction method using channel correlations for the h.264/avc intra coding	discrete wavelet transforms;prediction methods automatic voltage control image coding discrete wavelet transforms discrete cosine transforms decorrelation moon telecommunications prediction algorithms streaming media;channel correlation;channel coding;image coding;intra prediction algorithm;weighted neighboring pixel;prediction algorithms;color channel correlation;weighted neighboring pixel intra prediction algorithm color channel correlation 264 avc intra coding chroma prediction tool luminance channel intensity chrominance channel intensity;correlation methods;intra prediction;luminance channel intensity;video coding;chroma prediction tool;prediction methods;video coding channel coding correlation methods image colour analysis prediction theory;automatic voltage control;prediction theory;streaming media;discrete cosine transforms;image colour analysis;weighted sums;moon;intra coding;264 avc intra coding;chroma prediction;decorrelation;chrominance channel intensity;telecommunications;channel correlation image coding chroma prediction intra coding	This paper introduces a new intra prediction algorithm that exploits the correlations between the color channels, for the H.264/AVC intra coding. The main idea is to use the weighted sum of neighboring pixels as a new chroma prediction tool, where the weighting coefficients are determined considering the inter-channel correlations. Specifically the weighting coefficients are determined under the assumption that the similarity of a chrominance channel intensity increases as the distance of the luminance channel intensity decreases. The proposed method is added to the conventional chroma prediction modes with a little modification in the chroma prediction mode binarization. The experiments show that the proposed method provides up to 1.0dB gain for the color-abundant images.	algorithm;channel (digital image);coefficient;experiment;h.264/mpeg-4 avc;intra-frame coding;memory dependence prediction;pixel;weight function	Sang Heon Lee;Jae Won Moon;Jae Woan Byun;Nam Ik Cho	2009	2009 Picture Coding Symposium	10.1109/PCS.2009.5167349	computer vision;decorrelation;prediction;channel code;telecommunications;computer science;natural satellite;mathematics;statistics	Robotics	44.64602113442794	-16.81710554866793	72284
c0ecfba1f6635ab9e63c98c326f3b29d9b885fc8	finite state vector quantization with multipath tree search strategy for image/video coding	finite state machines vector quantisation video coding image coding vlsi tree searching iterative methods parallel algorithms pipeline processing digital signal processing chips cmos digital integrated circuits;image numerique;computational requirements finite state vector quantization multipath tree search strategy video coding image coding minimum distortion codebook multipath search neighboring trees picture quality finite state vq identifier code compression ratio vlsi architectures real time performance iteration parallel computing structure;image coding;image processing;real time;circuit vlsi;procesamiento imagen;search strategy;traitement image;iterative methods;video coding;codificacion;finite state machines;vlsi circuit;cuantificacion vectorial;vector quantization;senal video;signal video;cmos digital integrated circuits;temps reel;indexation;imagen numerica;coding;strategie recherche;parallel computer;vlsi;video signal;tiempo real;compression ratio;vector quantization video coding image coding computational complexity automata chaos very large scale integration computer architecture parallel processing concurrent computing;digital signal processing chips;vector quantizer;digital image;circuito vlsi;tree searching;vector quantisation;article;pipeline processing;codage;estrategia investigacion;vlsi architecture;parallel algorithms;quantification vectorielle	This paper presents a new vector quantization (VQ) algorithm exploiting the features of tree-search as well as finite state VQs for image/video coding. In the tree-search VQ, multiple candidates are identified for on-going search to optimally determine an index of the minimum distortion. In addition, the desired codebook has been reorganized hierarchically to meet the concept of multi-path search of neighboring trees so that picture quality can be improved by 4 dB on the average. In the finite state VQ, adaptation to the state codebooks is added to enhance the hit-ratio of the index produced by the tree-search VQ and hence to further reduce compressed bits. An identifier code is then included to indicate to which output indices belong. Our proposed algorithm not only reaches a higher compression ratio but also achieves better quality compared to conventional Anite-state and tree-search VQs.	apple a5;claire;codebook;data compression;diagram;distortion;emoticon;encoder;fractal dimension;identifier;image quality;jpeg;linde–buzo–gray algorithm;location-based game;multipath propagation;peak signal-to-noise ratio;simulation;tree traversal;vector quantization;very-large-scale integration	Chen-Yi Lee;Shih-Chou Juan;Yen-Juan Chao	1996	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.499837	computer vision;image processing;computer science;theoretical computer science;machine learning;compression ratio;parallel algorithm;iterative method;very-large-scale integration;coding;digital image;vector quantization	ML	45.338884263091856	-13.70831583141526	72449
fab0b8fa29c576d5f3fb446022fcd5da96fba130	uniform and piecewise uniform lattice vector quantization for memoryless gaussian and laplacian sources	cuantificacion senal;lattice truncation;rate distortion;piecewise linear;gaussian sources;vector quantisation lattice theory and statistics piecewise linear techniques;lattices;iterative algorithms;piecewise linear techniques;piecewise linear multidimensional compandor i i d sources memoryless sources gaussian sources lattice truncation lattice scaling lattice vector quantization laplacian sources nonuniform sources;laplacian sources;source sans memoire;memoryless source;lattice scaling;piecewise linear multidimensional compandor;lattice theory and statistics;laplace equations;cuantificacion vectorial;signal quantization;vector quantization;independent and identically distributed;quantizer;memoryless sources;analytical method;quantification signal;quantificateur;aerospace electronics;lattices vector quantization laplace equations multidimensional systems piecewise linear techniques algorithm design and analysis iterative algorithms aerospace electronics block codes rate distortion;vector quantizer;fuente sin memoria;i i d sources;vector quantisation;cuantificador;block codes;algorithm design and analysis;nonuniform sources;multidimensional systems;lattice vector quantization;quantification vectorielle	Lattice vector quantizer design procedures for nonuniform sources are presented. The procedures yield lattice vector quantizers with excellent performance and retaining the structure required for fast quantization. Analytical methods for truncating and scaling lattices to be used in vector quantizations are given, and their utility is demonstrated for independent and identically distributed (i.i.d.) Gaussian and Laplacian sources. An analytical technique for piecewise linear multidimensional compandor designs is evaluated for i.i.d. Gaussian and Laplacian sources by comparing its performance to that of the other vector quantizers. >	vector quantization	Dae-Gwon Jeong;Jerry D. Gibson	1993	IEEE Trans. Information Theory	10.1109/18.256488	block code;independent and identically distributed random variables;algorithm design;mathematical optimization;combinatorics;discrete mathematics;quantization;piecewise linear function;multidimensional systems;vector laplacian;lattice;mathematics;vector quantization;statistics	Theory	49.00076008861702	-11.844336194657187	72478
44388cb7a5d72a6a486d60226b57fa8b440ed761	analysis of compressed depth and image streaming on unreliable networks	protocols;propagation losses;image coding;network protocol;data compression;transform coding;three dimensional;image texture;network protocol image compression image streaming unreliable networks data packets image based rendering scenario texture packets warping operation;data packets;image based rendering scenario;servers;3d model;image generation;warping operation;texture packets;image compression;three dimensional displays;image streaming;three dimensional displays image generation servers transform coding image coding propagation losses protocols;rendering computer graphics data compression image coding image texture;image based rendering;depth map;rendering computer graphics;unreliable networks	This paper explores the issues connected to the transmission of three dimensional scenes over unreliable networks such as the wireless ones. It analyzes the effect of the loss of compressed data packets in a typical image-based rendering scenario, where a set of compressed images together with the corresponding depth maps are transmitted and used to generate the views required from the user at client side. The different impact on the rendered views of the geometry and texture packets is analyzed in detail, taking into account also the position of the lost packets and the warping operation. Finally we will discuss how to exploit this results in the design of an efficient network protocol for the transmission of 3D models.	client-side;color light output;communications protocol;data compression;distortion;in the beginning... was the command line;jpeg 2000;jpip;map;network packet	Pietro Zanuttigh;Andrea Zanella;Guido M. Cortelazzo	2008	2008 IEEE Symposium on Computers and Communications	10.1109/ISCC.2008.4625772	communications protocol;computer vision;computer science;theoretical computer science;operating system;computer network;computer graphics (images)	Embedded	41.92633006847475	-19.854889105965817	72759
20c44e4da02acf97b1f887a6018683a280c76d57	summary: multi-agent path finding with kinematic constraints		Multi-Agent Path Finding (MAPF) is well studied in both AI and robotics. Given a discretized environment and agents with assigned start and goal locations, MAPF solvers from AI find collision-free paths for hundreds of agents with user-provided sub-optimality guarantees. However, they ignore that actual robots are subject to kinematic constraints (such as velocity limits) and suffer from imperfect plan-execution capabilities. We therefore introduce MAPF-POST to postprocess the output of a MAPF solver in polynomial time to create a plan-execution schedule that can be executed on robots. This schedule works on non-holonomic robots, considers kinematic constraints, provides a guaranteed safety distance between robots, and exploits slack to avoid time-intensive replanning in many cases. We evaluate MAPF-POST in simulation and on differential-drive robots, showcasing the practicality of our approach.	artificial intelligence;discretization;online and offline;pathfinding;robot;robotics;simulation;slack variable;solver;time complexity;velocity (software development)	Wolfgang Hönig;T. K. Satish Kumar;Liron Cohen;Hang Ma;Hong Xu;Nora Ayanian;Sven Koenig	2017		10.24963/ijcai.2017/684	machine learning;artificial intelligence;computer science;kinematics	AI	53.173417565186135	-22.414616906519093	72952
54dbd4ed774a28eccbf4fc1803a05fa84d1b38a6	a new and fast real-time implementation of 2-d dct	fast algorithm;real time implementation;algorithm design	A new and fast 4×4 DCT algorithm is proposed in this paper. This algorithm classifies the input 2-D pixel data into four groups, each of which is then further rearranged into 1-D DCT transform. Therefore, the computation of 2-D DCT can be implemented by four 1-D DCTs. As a result, the efficiency of 2-D DCT algorithm is dependent on the 1-D DCT algorithm adopted, and all the existing fast algorithms for 1-D DCT can be directly applied to further optimise the algorithm design. The proposed algorithm can also be extended to compute general 2-D DCT by a recursive procedure. Specific algorithm flowchart and design is also included and described.	discrete cosine transform;real-time transcription	Jianmin Jiang	1995		10.1007/3-540-60298-4_258	algorithm design;computer science	Robotics	45.53009644293543	-10.086211049822317	72970
7185344d58f789c88a308acf83c22cade36a0907	a fast algorithm for dct-domain inverse motion compensation based on shared information in a macroblock	algorithme rapide;modelizacion;optimal solution;optimisation;motion compensation video compression discrete cosine transforms video sharing transform coding streaming media computational efficiency image converters computational modeling delay;transformation cosinus;transformacion discreta;information compression;data compression;motion compensation;etude theorique;fundamental unit;mpeg video;technique video;video processing;code standards;transform coding;indexing terms;tecnica video;compresion informacion;discrete cosine transform;motion compensated;modelisation;video coding;manipulacion;codificacion;sharing;particion;dct matrix factorization motion vector shared information macroblock fast algorithm motion compensated intercoded frames compressed domain video manipulation video composition discrete cosine transform dct based coding mpeg video dct domain inverse motion compensation common blocks computation speedup brute force approaches optimized solution video coding;discrete cosine transforms;motion vector;telecommunication standards;fast algorithm;optimisation discrete cosine transforms transform coding inverse problems motion compensation video coding data compression code standards telecommunication standards;compression information;transformacion coseno;coding;discrete transformation;estudio teorico;video technique;procesador;manipulation;theoretical study;partage;cosine transform;processeur;modeling;transformation discrete;algoritmo rapido;processor;codage;inverse problems	The ability to construct intracoded frame from motion-compensated intercoded frames directly in the compressed domain is important for efficient video manipulation and composition. In the context of motion-compensated discrete cosine transform (DCT)-based coding of video as in MPEG video, this problem of DCT-domain inverse motion compensation has been studied and, subsequently, improved faster algorithms were proposed. These schemes, however, treat each 8 8 block as a fundamental unit, and do not take into account the fact that in MPEG, a macroblock consists of several such blocks. In this paper, we show how shared information within a macroblock, such as a motion vector and common blocks, can be exploited to yield substantial speedup in computation. Compared to previous brute-force approaches, our algorithms yield about 44% improvement. Our technique is independent of the underlying computational or processor model, and thus can be implemented on top of any optimized solution. We demonstrate an improvement by about 19%, and 13.5% in the worst case, on top of the optimized solutions presented in existing literature.	algorithm;best, worst and average case;brute-force search;computation;discrete cosine transform;macroblock;motion compensation;moving picture experts group;speedup	Junehwa Song;Boon-Lock Yeo	2000	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.856453	computer vision;computer science;theoretical computer science;discrete cosine transform;mathematics;block-matching algorithm;macroblock;motion compensation;algorithm;computer graphics (images)	EDA	46.666832116593184	-15.604248206517376	73108
266205d2b3cfd4845f124609370c00b7833f38fe	reduced complexity intra mode decision for resolution reduction on h.264/avc transcoders	rate distortion;resolution reduction;image coding;transcoding h 264 avc intra prediction;complexity theory;image resolution;image converters;pattern directionality;h 264 avc;communication complexity;video compression;prediction algorithms;bit rate;intramode decision complexity reduction;intra prediction;fast mode selection transcoding h 264 avc intra prediction;video coding;video transcoding;automatic voltage control;full rate distortion mode decision intramode decision complexity reduction resolution reduction h 264 avc transcoder video transcoding picture size reduction pattern directionality;h 264 avc transcoder;complexity reduction;automatic voltage control transcoding video compression bit rate rate distortion video coding static var compensators image resolution image converters image coding;picture size reduction;video coding communication complexity transcoding;full rate distortion mode decision;static var compensators;fast mode selection;transcoding;mode decision	Intra frame prediction is one of the most compression efficient tools of the H.264/AVC codec over its predecessor. In video transcoding for picture size reduction, pattern directionality of the output frames may differ from the input, affecting the H.264/AVC intra prediction modes. This paper proposes two simple algorithms for efficient mode decision by exploiting the input modes. To further improve the conversion performance two refinement methods are also proposed. One is to look at the neighboring of the selected modes and the other to look at horizontal, vertical and DC modes, as the most frequently used modes. Compared with the full rate-distortion mode decision, the proposed algorithms achieve more than 50% complexity reduction while showing a negligible PSNR penalty of only 0.1 dB and an excess bitrate of 1-6%.	algorithm;benchmark (computing);codec;distortion;embedded system;encoder;exploit (computer security);full rate;h.264/mpeg-4 avc;image quality;immediate mode (computer graphics);intra-frame coding;jumbo frame;level of detail;offset binary;peak signal-to-noise ratio;reduction (complexity);refinement (computing);video compression picture types;virtual 8086 mode	Sandro Moiron;Mohammed Ghanbari	2009	IEEE Transactions on Consumer Electronics	10.1109/TCE.2009.5174429	computer vision;electronic engineering;real-time computing;transcoding;computer science;statistics	EDA	46.0231596694458	-18.412825785445207	73233
3a6a6d0e450a129b0c53c898031ab203ca00e152	frame-based compression of animated meshes in mpeg-4	temporal correlation;image coding;mesh compression;animation compression;data compression;motion compensation;skinning motion compensation model;photometric attributes frame based compression mpeg 4 3d dynamic meshes frame based animated mesh compression animation framework extension model based motion compensation transform coding residual errors skinning motion compensation model frame based representation discrete cosine transform lifting wavelets layer based predictive coding spatio temporal correlations encoder geometric attributes;layer based predictive coding;video coding computer animation correlation methods data compression discrete cosine transforms encoding geometry mesh generation motion compensation;geometry;transform coding;correlation methods;photometric attributes;discrete cosine transform;motion compensated;residual errors;video coding;afx mesh compression animation compression dynamic mesh compression mpeg 4;progressive transmission;three dimensional displays;discrete cosine transforms;three dimensional displays image coding encoding transforms animation transform coding discrete cosine transforms;frame based animated mesh compression;animation;transforms;geometric attributes;mpeg 4;encoder;spatio temporal correlations;dynamic mesh compression;frame based representation;computer animation;lifting wavelets;afx;mesh generation;encoding;predictive coding;model based motion compensation;3d dynamic meshes;frame based compression;animation framework extension	This paper presents a new compression technique for 3D dynamic meshes, referred to as FAMC - frame-based animated mesh compression, promoted within the MPEG-4 standard as amendment 2 of part 16 AFX (animation framework extension). The FAMC approach combines a model-based motion compensation strategy, with transform/predictive coding of residual errors. First, a skinning motion compensation model is automatically computed from a frame-based representation and then encoded. Subsequently, either 1) DCT/lifting wavelets or 2) layer-based predictive coding is employed to exploit remaining spatio-temporal correlations in the residual signal. The proposed encoder offers high compression performances (gains in bit rate of 60% with respect to the previous MPEG-4 technique and of 20% to 40% with respect to state-of-the-art approaches) and is well suited for compressing both geometric and photometric (normal vectors, colors...) attributes. In addition, the FAMC method supports a rich set of functionalities including streaming, scalability (spatial, temporal and quality) and progressive transmission.	afx windows rootkit 2003;boyce–codd normal form;color;discrete cosine transform;encoder;lifting scheme;lossless compression;motion compensation;performance;scalability;wavelet	Khaled Mamou;Titus B. Zaharia;Françoise J. Prêteux;Nikolce Stefanoski;Jörn Ostermann	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607636	data compression;anime;mesh generation;computer vision;encoder;transform coding;computer science;discrete cosine transform;computer animation;multimedia;motion compensation;mpeg-4;algorithm;encoding;statistics;computer graphics (images)	Robotics	43.51754692621603	-18.92843588943357	73532
4919afdefb407683e8d2dc6e4c96dc714d5d8877	new sorting-based lossless motion estimation algorithms and a partial distortion elimination performance analysis	full search;distortion taylor expansion;sorting;partial distortion elimination pde bounds distortion taylor expansion fast block matching full search lossless motion estimation;image matching;lossless motion estimation;motion estimation;taylor expansion;indexing terms;motion estimation performance analysis encoding information analysis algorithm design and analysis sorting taylor series measurement standards testing motion measurement;video coding;distortion;distortion video coding sorting motion estimation image matching;fast block matching;fast algorithm;performance analysis;taylor series expansion;block matching;fast full search with sorting by gradient algorithm sorting based lossless motion estimation algorithm partial distortion elimination bound video encoding taylor series expansion block motion estimation algorithm fast full search with sorting by distortion algorithm;partial distortion elimination pde bounds;lower bound;search and matching	In video encoding, block motion estimation represents a CPU-intensive task. For this reason, many fast algorithms have been developed to improve searching and matching phases. A milestone within the lossless approach is partial distortion elimination (PDE/SpiralPDE) in which distortion is the difference between the block to be coded and the candidate prediction block. In this paper, (i) we analyze distortion behavior from local information using the Taylor series expansion and show that our general analysis includes other previous similar approaches. (ii) Then, we propose two full-search (lossless), fast-matching, block motion estimation algorithms, based on the PDE idea. The proposed algorithms, called fast full search with sorting by distortion (FFSSD) and fast full search with sorting by gradient (FFSSG), sort the contributions to distortion and the gradient values, respectively, in order to quickly discard invalid blocks. Experimental results show that the proposed algorithms outperform other existing full search algorithms, reducing by up to 20% the total CPU encoding time (with respect to SpiralPDE), while the computation strictly required by the motion estimation is reduced by about 30%. (iii) Finally, we experimentally find an operational lower bound (based on standard test sequences) for the average number of checked pixels in the PDE approach, which measures the performance of the searching and matching phases. In particular, SpiralPDE achieves performances very close to the searching phase bound, while there is still a remarkable margin on the matching phase. We then show that our algorithms, aimed at improving the performances of the matching phase, achieve interesting results, significantly approaching this margin.	central processing unit;computation;data compression;distortion;experiment;gradient;lossless compression;motion estimation;performance;pixel;profiling (computer programming);search algorithm;series expansion;sorting;speedup;time complexity;window function	Bartolomeo Montrucchio;Davide Quaglia	2005	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2004.841689	computer vision;mathematical optimization;combinatorics;computer science;taylor series;theoretical computer science;mathematics	ML	48.62977172826206	-19.348447503017603	73536
6b525b116c38ee87fc41fb05d29ba1ade16f0a27	on the jpeg 2000 ultrafast mode	image coding;huffman codes;quantisation signal;transform coding encoding image coding quantization codecs decoding standards;transcoding huffman codes image coding quantisation signal runlength codes;runlength codes;digital cinema industry jpeg 2000 ultrafast mode jpeg committee jpeg 2000 encoding huffman runlength code prediction step lossless transcoding performance measurements dicom standard medical applications;transcoding	Recently, the JPEG committee discussed the introduction of an “ultrafast” mode for JPEG 2000 encoding. This considered extension of the JPEG 2000 framework replaces the EBCOT coding by a combined Huffman-Runlength code, and adds an optional additional prediction step after quantization. While the resulting codec is not compatible with existing JPEG 2000, it still allows lossless transcoding from JPEG 2000 and back, and performance measurements show that it offers nearly the quality of JPEG 2000 and similar quality than JPEG XR at a much lower complexity comparable to the complexity of the IJG JPEG software. This work introduces the extension, and compares its performance with other JPEG standards and other extensions of JPEG 2000 currently under standardization.	codec;huffman coding;image quality;jpeg 2000;jpeg xr;lossless compression;pegasus	Thomas Richter;Sven Simon	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467406	lossless jpeg;speech recognition;transcoding;telecommunications;computer science;theoretical computer science;jpeg;jpeg 2000;jpeg file interchange format;quantization;statistics;huffman coding	Robotics	44.61632595788355	-19.22990611552924	73959
030efd36223261c0806bb3263a336f8f052c6f11	separable discrete hartley transform based invisible watermarking for color image authentication (sdhtiwcia)		In this paper a novel two-dimensional Separable Discrete Hartley Transform based invisible watermarking scheme has been proposed for color image authentication (SDHTIWCIA). Two dimensional SDHT is applied on each 2 × 2 sub-image block of the carrier image in row major order. Two bits are embedded in second, third and fourth frequency components of each 2 × 2 mask in transformed domain based on a secret key. Second and third bit position in each frequency coefficient has been chosen as embedding position. A delicate re-adjustment has incorporated in the first frequency component of each mask, to keep the quantum value positive in spatial domain without hampering the embedded bits. Inverse SDHT (ISDHT) is applied on each 2 × 2 mask as post embedding operation to produce the watermarked image. At the receiving end reverse operation is performed to extract the stream which is compared to the original stream for authentication. Experimental results conform that the proposed algorithm performs better than the Discrete Cosine Transform (DCT), Quaternion Fourier Transformation (QFT) and Spatio Chromatic DFT (SCDFT) based techniques.	authentication;color image;discrete hartley transform;hartley (unit)	J. K. Mandal;Sudipta Kumar Ghosal	2012		10.1007/978-3-642-31552-7_78	digital watermarking;fourier transform;discrete mathematics;row-major order;discrete hartley transform;mathematics;embedding;color image;least significant bit;discrete cosine transform	Vision	39.54278079119523	-10.665475286462906	74232
ddca176f02f29eb3f050ec4578fddbdd762f4890	mpeg-2 video coding with image partitioning	image resolution;data compression;motion compensation;motion compensated;rate control;video coding;temporal resolution;encoding efficiency mpeg 2 video coding image partitioning motion compensation frame partitioning temporal resolutions video compression bit allocation;bit allocation;image resolution video coding motion compensation data compression;video coding bit rate encoding motion compensation transform coding layout video compression partitioning algorithms motion control mpeg 4 standard	In this paper we present an original motion compensation strategy based on frame partitioning. The proposed method uses different temporal resolutions within a frame to improve compression. We present a new bit allocation and rate control algorithm complementing our motion compensation technique. This unique approach to bit allocation ensures the consistency of quality throughout a single frame and a GOP. For the same picture quality, frame partitioning alone yields an additional increase of up to 20 percent or more of the encoding efficiency.	algorithm;data compression;frame language;group of pictures;h.262/mpeg-2 part 2;image quality;mpeg-2;macroblock;motion compensation;streaming media	Ekaterina G. Barzykina;Panos Nasiopoulos;Rabab Kreidieh Ward	1998		10.1109/ICASSP.1998.678112	video compression picture types;data compression;inter frame;residual frame;computer vision;image resolution;quarter-pixel motion;computer science;temporal resolution;coding tree unit;motion estimation;block-matching algorithm;multimedia;context-adaptive binary arithmetic coding;motion compensation;h.261;statistics;multiview video coding;computer graphics (images)	Robotics	45.25337419809672	-19.213778677901423	74390
a362d3d56776bc75c3ca9cfabe36d35c55e482f6	a segmentation-based coding system allowing manipulation of objects (sesame)	image area selection;rate distortion;image coding;sesame algorithm segmentation based coding system object manipulation image coding image sequence image segmentation rate distortion theory initial regions coding techniques motion criteria spatial criteria image area selection cost image quality content based functions;image segmentation;initial regions;info eu repo semantics conferenceobject;motion criteria;motion estimation;layout;content based functions;sesame algorithm;coding techniques;mpeg 4 standard image segmentation costs layout merging algorithm design and analysis image coding rate distortion video coding proposals;rate distortion theory image segmentation image coding image sequences motion estimation;rate distortion theory;arees tematiques de la upc enginyeria de la telecomunicacio;video coding;mpeg 4 standard;object manipulation;image quality;conference report;image sequence;telecommunication;merging;cost;proposals;algorithm design and analysis;spatial criteria;telecomunicacio;segmentation based coding system;image sequences;info eu repo semantics publishedversion	In this paper we present a coding scheme that achieves for each image in the sequence the best segmentation in terms of Rate Distortion theory It is obtained from a set of initial regions and a set of available coding techniques The segmentation combines spatial and motion criteria It selects at each area of the image the most adequate criterion for de ning a partition in order to obtain the best compromise between cost and quality In addition the proposed scheme is very suitable for addressing content based functionalities	data compression;distortion;rate–distortion theory;sesame	Ferran Marqués;Philippe Salembier;Montse Pardàs;Ramon Morros;Isabelle Corset;Sylvie Jeannin;Beatriz Marcotegui;Fernand Meyer	1996		10.1109/ICIP.1996.560741	image quality;layout;algorithm design;computer vision;rate–distortion theory;computer science;theoretical computer science;segmentation-based object categorization;motion estimation;multimedia;image segmentation;scale-space segmentation	Vision	46.942363058820476	-17.160849871532317	74508
8fc7d8e19c347a48d4d638fc1e3c7ddb0c1702a2	fast block structure determination in av1-based multiple resolutions video encoding		The widely used adaptive HTTP streaming requires an efficient algorithm to encode the same video to different resolutions. In this paper, we propose a fast block structure determination algorithm based on the AV1 codec that accelerates high resolution encoding, which is the bottle-neck of multiple resolutions encoding. The block structure similarity across resolutions is modeled by the fineness of frame detail and scale of object motions, this enables us to accelerate high resolution encoding based on low resolution encoding results. The average depth of a block's co-located neighborhood is used to decide early termination in the RDO process. Encoding results show that our proposed algorithm reduces encoding time by 30.1%-36.8%, while keeping BD-rate low at 0.71%-1.04%. Comparing to the state-of-the-art, our method halves performance loss without sacrificing time savings.	aomedia video 1;algorithm;blu-ray;codec;encode;image resolution;remote data objects;streaming media	Bichuan Guo;Yuxing Han;Jiangtao Wen	2018	2018 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2018.8486492	pattern recognition;computer science;artificial intelligence;computer vision;encoding (memory);codec;image resolution	Visualization	45.92426080175489	-19.956503157563336	74674
b04217892247b2aeddf18429a658beffe844b1c1	an operational rate-distortion optimal single-pass snr scalable video coder	estensibilidad;dynamic programming;theorie vitesse distorsion;evaluation performance;optimisation;programacion dinamica;partition method;scalable video;performance evaluation;data compression;optimizacion;video signal processing;image sequence optimal rate distortion single pass snr scalable video coder signal to noise ratio dct coefficients partitioning displaced frame difference base layer enhancement layers embedded bitstream video quality bit budget lagrangian relaxation dynamic programming scalable video codec video scalability;evaluacion prestacion;transformation cosinus discrete;video quality;operational rate distortion;transform coding;rate distortion theory;video coding;enhancement layer;methode partition;codage video;discrete cosine transforms;methode lagrange;metodo lagrange;programmation dynamique;traitement signal video;lagrangian method;optimization;metodo particion;rapport signal bruit;extensibilite;scalability;relacion senal ruido;displaced frame difference;signal to noise ratio;rate distortion scalability discrete cosine transforms video sequences video compression network servers web server design for disassembly dynamic programming codecs;image sequences video coding data compression rate distortion theory dynamic programming discrete cosine transforms transform coding;base layer;image sequences	In this paper, we introduce a new methodology for signal-to-noise ratio (SNR) video scalability based on the partitioning of the DCT coefficients. The DCT coefficients of the displaced frame difference (DFD) for inter-blocks or the intensity for intra-blocks are partitioned into a base layer and one or more enhancement layers, thus, producing an embedded bitstream. Subsets of this bitstream can be transmitted with increasing video quality as measured by the SNR. Given a bit budget for the base and enhancement layers the partitioning of the DCT coefficients is done in a way that is optimal in the operational rate-distortion sense. The optimization is performed using Lagrangian relaxation and dynamic programming (DP). Experimental results are presented and conclusions are drawn.	bitstream;coder device component;coefficient;dct protein, human;data flow diagram;discrete cosine transform;distortion;dynamic programming;embedded system;embedding;lagrangian relaxation;linear programming relaxation;mathematical optimization;scalability;signal-to-noise ratio;video;anatomical layer;disease transmission	Lisimachos P. Kondi;Aggelos K. Katsaggelos	2001	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.967389	data compression;computer vision;scalability;transform coding;rate–distortion theory;computer science;video quality;theoretical computer science;dynamic programming;mathematics;signal-to-noise ratio;statistics	Vision	47.29571810039049	-13.840497210019452	74738
fa7e2a745121a00522eec02c99106cb5a9968d92	a biologically inspired neural cpg for sea wave conditions/frequencies	adaptive control;central pattern generator;neural network;wave energy	This paper shows that a biology-based neural network (called a central pattern generator (CPG)) can be re-evolved for sea conditions / frequencies. The fish's CPG operates at 1.74Hz to 5.56Hz, whereas we require performance to reach 0.05Hz to 0.35Hz (20s to 3s waves) for an alternative engineering problem. This is to enable adaptive control of wave energy devices, increasing their efficiency and power yield. To our knowledge, this is the first time a bio-inspired circuit will be integrated into the engineering domain (and for a completely different function). This provides great inspiration for utilising other neural network mechanisms for alternative tasks.		Leena N. Patel;Alan F. Murray	2008		10.1007/978-3-642-02490-0_12	central pattern generator;adaptive control;telecommunications;computer science;artificial intelligence;machine learning;artificial neural network	NLP	53.493765784857096	-12.469611562974224	74909
3b31ab56f7e4c4aa310ec115878b91297fdc52b7	scalable coding of depth maps with r-d optimized embedding	image coding;telecommunication network reliability;geometry;geometry encoding scalability image coding rate distortion discrete wavelet transforms;scalable coding breakpoint adaptive transform ebcot scheme pyramid representation jpeg 2000 scalability features subband sample image pyramid structures embedded representation geometry information compression scheme discontinuity boundary geometry r d optimized embedding depth map;image representation;image reconstruction;transforms;transforms geometry image coding image reconstruction image representation telecommunication network reliability;tec mrs com loc	Recent work on depth map compression has revealed the importance of incorporating a description of discontinuity boundary geometry into the compression scheme. We propose a novel compression strategy for depth maps that incorporates geometry information while achieving the goals of scalability and embedded representation. Our scheme involves two separate image pyramid structures, one for breakpoints and the other for sub-band samples produced by a breakpoint-adaptive transform. Breakpoints capture geometric attributes, and are amenable to scalable coding. We develop a rate-distortion optimization framework for determining the presence and precision of breakpoints in the pyramid representation. We employ a variation of the EBCOT scheme to produce embedded bit-streams for both the breakpoint and sub-band data. Compared to JPEG 2000, our proposed scheme enables the same the scalability features while achieving substantially improved rate-distortion performance at the higher bit-rate range and comparable performance at the lower rates.	bitstream;breakpoint;coefficient;data compression;depth map;discrete wavelet transform;distortion;embedding;jpeg 2000;mathematical optimization;microtubule-associated proteins;norm (social);pyramid (image processing);rate–distortion optimization;reflections of signals on conducting lines;scalability;signed number representations	Reji Mathew;David S. Taubman;Pietro Zanuttigh	2013	IEEE Transactions on Image Processing	10.1109/TIP.2013.2240007	iterative reconstruction;computer vision;pyramid;discrete mathematics;theoretical computer science;mathematics	Vision	45.308715885880645	-15.952583070950432	74950
ecc17e34b595711f59d842a3c6d7964593b2bd7d	a novel stereoscopic video coding method based on view warping	view warping;view reconstruction stereoscopic video coding view warping;video coding image reconstruction image representation stereo image processing;three dimensional;video coding;stereo image processing video coding bit rate image reconstruction color encoding decoding;image representation;image reconstruction;stereo image processing;stereoscopic video coding;view reconstruction;3d video;view quality reduction stereoscopic video coding method view warping 3d video technology stereoscopic perception residual map warped right view representation bitrate transmission reduction view quality reconstruction	Three-dimensional (3D) video technology is becoming increasingly popular, as it can provide stereoscopic perception and immersive experience to end users. In this paper, a novel stereoscopic video coding method is proposed based on view warping. In the method, the right view is represented by the residual map between the original view and the warped right views. At the decoder, the right view is reconstructed based on the available information from left view. Thus, the transmitted bitrate can be largely reduced and the reconstructed view quality can be largely improved. Experimental results shows that the proposed method can significantly save the bitrate and can achieve superior reconstruction quality.	data compression;stereoscopic video coding;stereoscopy	Dong Jiang;Feng Shao;Huiying Dong;Gangyi Jiang;Mei Yu	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6233806	iterative reconstruction;image warping;three-dimensional space;computer vision;multimedia;multiview video coding;computer graphics (images)	Robotics	44.23116302460311	-18.882064947877346	75050
40ce29069bba6cc494901c16a9267e77bc1a8177	watermark with dsa signature using predictive coding	watermarking;predictors;dsa signature	This paper presents a predictor-based watermark scheme that embeds secret bit streams and a DSA signature into an image. For the copyrighting of digital media, a DSA signature is appropriate as a watermarking technique. To improve security, we apply an Arnold transform (AT) to secret messages. We propose new predictors, Left-Top, which the predict current pixel values using neighboring pixel values. Our proposed scheme conceals secret messages by using the difference between current pixel values and predictive pixel values. Experimental results show that our method has low complexity and achieves a higher embedding performance with good perceptual quality compared to the earlier arts. Experimental results verified our proposed watermark method in multimedia communications.	arnold;branch predictor;digital media;digital rights management;digital watermarking;experiment;information security;kerrison predictor;peak signal-to-noise ratio;pixel;steganography	Cheonshik Kim;Ching-Nung Yang	2013	Multimedia Tools and Applications	10.1007/s11042-013-1667-6	digital watermarking;computer science;theoretical computer science;internet privacy;computer security	Vision	39.73164776090476	-11.652863498169795	75230
0dcc1acdd1be9693596d8fa8f4569953a3e9698c	fractal image coding as generalized predicitve coding	analysis by synthesis;analysis by synthesis coding fractal image coding generalized predictive coding prediction gain codebooks design iterative decoding prediction residuals reconstruction errors shaping codec complexity prediction gains noncausal predictors long term predictors heuristics codec design short term prediction transform coding vector quantization noise feedback;fractal image coding;fractals;analysis by synthesis coding;fractals image coding predictive coding codecs image reconstruction iterative decoding vector quantization distortion measurement noise shaping transform coding;image coding;codecs;iterative decoding;decoding;noncausal predictors;codec complexity;prediction residuals;distortion measurement;transform coding;noise feedback;iterative methods;codec design;long term predictors;vector quantization;prediction theory;image reconstruction;codebooks design;proceedings paper;short term prediction;noise shaping;prediction gain;vector quantizer;heuristics;generalized predictive coding;prediction gains;vector quantisation;vector quantisation fractals image coding image reconstruction decoding iterative methods codecs transform coding prediction theory;reconstruction errors shaping;predictive coding	We point out that a prevalent form of fractal image coding can be viewed as a kind of generalized predictive coding. Several key issues in predictive coding are the prediction gain, the design of codebooks for predictors and prediction residuals, shaping of reconstruction errors, and codec complexity. Fractal coding can yield higher prediction gains than conventional predictive coding by its use of noncausal predictors and long-term predictors. However, noncausal prediction necessitates it.erative decoding and long-term predictors require search over a large area, both of which increase codec complexity. Design of predictors and prediction codebooks for fractal coding has relied much on heuristics. Drawing on known results about predictive coding, we outline several directions for codec design, among which short-term prediction and transform coding or vector quantization of prediction residuals. Shaping of reconstruction errors by noise-feedback or analysis-by-synthesis coding may also be beneficial. 1. I N T R O D U C T I O N There is significant interest in the recently proposed image coding approach based on theory of iterated function systems (IFS)[1],[2]. The approach is frequently referred to as fractal image coding in the literature and it has been thought as possible of yielding very high compression rat,ios. It, is therefore of interest to examine, from a theoretical point of view, the validity of the last conviction and, if valid, how we may design the encoder to exploit the approach’s full potential (or a large portion thereof) in compression. It is also of interest to lay more clearly the relation between fractal coding and the more well-known waveform coding t,echniques such as predictive coding, transform coding, and vector quantization. This work was performed while with Bellcore. Despite fractal coding’s mathematical grounding, fractal coder design has so far relied heavily on heuristics. We show in this paper that fractal image coding can be viewed as a generalized form of predictive image coding. As such, fractal coding studies can benefit from the established results (and extensions thereof) for predictive coding. illthough conventional predictive image coding usually can only realize moderate coding gains[4], the prediction mechanism in fractal coding is more flexible and can therefore produce higher gains. In what. follows, Section 2 reviews the fractal coding approach to set the stage for subsequent discussion. Section 3 points out the relation between fractal coding and predictive coding. Section 4 draws on established results regarding predictive coding for understanding of fractal coding’s prediction performance and for inspiration toward better fractal codec designs. And Section 5 is the conclusion. 2. F R A C T A L IMAGE C O D I N G Consider a grayscale image of M x N (horizontal x vertical) pels and let g be the NM-vector of pel intensity values arranged in raster-scan order. Fractal image coding involves finding a contractive transformation T on NM-vectors such that 2: is approximately the fixed point of T . Parameters characterizing T can then be used to represent the image and transmitted over a communications channel, in lieu of c. In decoding, T is iterated enough number of times until its output is close to the fixed point. The initial input to T can be arbitrary since the contractive nature of T guarantees convergence to the fixed point from any initial condition. The final output is taken to be the reconstructed image. Diagrammatically, the system is as shown in Fig. 1, where 5 denotes the reconstructed image. Mathematically, the decoder performs where zn is the arbitrary initial input to r and K is the	channel (communications);codebook;codec;encoder;fixed point (mathematics);fractal compression;grayscale;heuristic (computer science);initial condition;iterated function system;iteration;noise shaping;pixel;speech coding;transform coding;vector quantization;waveform;ios	David W. Lin	1994		10.1109/ICIP.1994.413876	iterative reconstruction;codec;transform coding;speech recognition;noise shaping;fractal;computer science;theoretical computer science;heuristics;machine learning;speech coding;mathematics;iterative method;vector quantization;statistics;code-excited linear prediction	AI	50.02247264471578	-14.702277189877853	75312
421d5780a9d5b9aa5ba846961e87d4585347fb1c	ℓ2 optimized predictive image coding with ℓ∞ bound	image coding bit rate transform coding quantization signal context psnr;speckle;data compression;l constrained image compression;optimal scalar quantization;video coding;thesis;signal processing;jpeg 2000 l 2 optimized predictive image coding image video compression l error bound structured errors contours speckles poor rate control l 2 error metric smooth waveforms;video coding data compression speckle;predictive coding	In many scientific, medical and defense applications of image/video compression, an ℓ<sub>∞</sub> error bound is required. However, pure ℓ<sub>∞</sub>-optimized image coding, colloquially known as near-lossless image coding, is prone to structured errors such as contours and speckles if the bit rate is not sufficiently high; moreover, previous ℓ<sub>∞</sub>-based image coding methods suffer from poor rate control. In contrast, the ℓ<sub>2</sub> error metric aims for average fidelity and hence preserves the subtlety of smooth waveforms better than the ℓ<sub>∞</sub> error metric and it offers fine granularity in rate control; but pure ℓ<sub>2</sub>-based image coding methods (e.g., JPEG 2000) cannot bound individual errors as the ℓ<sub>∞</sub>-based methods can. This paper presents a new compression approach to retain the benefits and circumvent the pitfalls of the two error metrics.	data compression;distortion;jpeg 2000;lossless compression;performance;quantization (signal processing);weight function	Sceuchin Chuah;Sorina Dumitrescu;Xiaolin Wu	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637864	data compression;speckle pattern;computer vision;speech recognition;shannon–fano coding;computer science;theoretical computer science;signal processing;coding gain;coding tree unit;mathematics;context-adaptive binary arithmetic coding;statistics	Robotics	45.398659376351105	-15.554370589910944	75553
507985da5b57229b496c4c28b6e3741c0cb8e333	illustration watermarking: an object-based approach for digital images	filigranage numerique;protection information;digital watermarking;data hiding;image numerique;image processing;redundancia;numerical method;procesamiento imagen;useful information;informacion util;digital imaging;segmentation;securite donnee;traitement image;redundancy;metodo numerico;proteccion informacion;object oriented;human visual system;information protection;robustesse;filigrana digital;imagen numerica;oriente objet;robustness;digital image;orientado objeto;security of data;segmentacion;methode numerique;information utile;redondance;robustez	For most applications common watermarking techniques usually spread the data to embed over the entire media, since distributing the watermark information promises an improvement in regard to security aspects, data hiding capacity or rather robustness in terms of redundancy. Distribution is controlled by a a visual or psychoacoustical model that takes limitations of the Human Visual System (HVS) and syntactical information about the signal characteristics into account. Therefore in most cases syntactical and not semantical aspects determine embedding. In our paper we introduce an approach for object based annotation watermarking which respects semantical characteristics of digital images, referred to as model for illustration watermarking. By applying a user-assisted segmentation process regions, representing semantical objects within the image, are identified and prepared for embedding. Providing robustness to typical image processing operations like cropping, scaling, compression and rotation, the proposed technique is applicable for binding additional illustrative information to selected objects within the medium. Moreover we identify the requirements of object based watermarking in consideration of imperceptibility, as well as watermark payload, and present first experimental test results.	digital image;digital watermarking;human visual system model;image processing;image scaling;object-based language;psychoacoustics;requirement	Thomas Vogel;Jana Dittmann	2005		10.1117/12.586977	computer vision;computer science;theoretical computer science;algorithm	Graphics	43.98681095214556	-11.247759092202989	75602
79b4216a143647f5d30c591ee18433a552023be4	large-scale servo control using a matrix wire network for driving a large number of actuators	power control large scale servo control matrix wire network actuators power amplifiers networked actuation architecture power drive;control systems;control algorithm;power amplifier;actuators;large scale systems servosystems wire actuators power amplifiers control systems wiring gold laboratories information systems;power amplifiers;large scale;servomechanisms;servo control;control systems servomechanisms actuators power amplifiers large scale systems;large scale systems	Research MIT, Dept. of Mechanical Engineering Cambridge, MA Experience Advisor: Prof. Harry Asada Fall 2000 to present • Dimensionality Reduction Techniques for actuation: Using concept of synergy, reduced number of controls needed to drive and control large DOF system. • Multi-Axis Actuator Array: Applied Feature extraction algorithm to design and control a large DOF multiaxis actuator array. Built prototypes using shape memory alloy actuators and thermoelectric devices. • Robotic Hand: Designed and fabricated an actuator system, a robotic hand, and the drive amplifiers and the controller for the system. • Control of a SMA actuator: Developed a method of controlling an artificial muscle actuator in a stepper motor style by exploiting the nonlinearity in the model (Segmented Binary Control). Patent issued. • Temperature Controller: Developed a temperature control system using thermoelectric devices and water cooled heat sink to control segmented array of SMA actuators. • Robotic car seat: Developed and delivered a prototype of active car seat for rapid heating/cooling and massaging. Patent pending.	algorithm;amplifier;autonomous car;computer cooling;control system;dimensionality reduction;feature extraction;heat sink;nonlinear system;prototype;robot;servo;synergy;water cooling	Kyu-Jin Cho;Samuel Au;H. Harry Asada	2003		10.1109/ROBOT.2003.1241667	control engineering;electronic engineering;servo control;engineering;control system;amplifier;control theory;servo drive;actuator	Robotics	53.46915944351464	-11.519126901852621	75629
7ade91665e41e34c0ea518b44856f05ea98a2999	object-based rate allocation with spatio-temporal trade-offs	distortion;video	This paper describes a bit allocation algorithm that can achieve a constant bit rate when coding multiple video objects (MVOs), while improving the rate-distortion (R-D) performance over the reference method for MPEG-4 object-based rate control [1]. In object-based coding, bit allocation is performed at the object level and temporal rates of different objects may vary. The proposed algorithm in this paper deals with these two issues. We pay particular attention to maintenance of buffer occupancy levels and propose a new method for spatio-temporal trades-offs for object-based coding. In order to improve the coding efficiency, we also consider several new R-D coding modes. These modes are available at the encoder to determine the best R-D performance under different coding conditions and are chosen automatically. Simulation results demonstrate moderate improvements at low and high bit rates. One important key aspect of the proposed algorithm is that the actual coded bits in the proposed algorithm are similar to the target bits over a wide range of bit rates. Consequently, the proposed algorithm has not experienced the buffer overflow/underflow over the entire range of bit rates.	algorithm;algorithmic efficiency;arithmetic underflow;buffer overflow;crystallographic information file;distortion;encoder;moe;object-based language;simulation	Jeong-Woo Lee;Anthony Vetro;Yao Wang;Yo-Sung Ho	2002		10.1117/12.453078	real-time computing;simulation;bit error rate;harmonic vector excitation coding;computer science;theoretical computer science;coding gain;coding tree unit;bit field;context-adaptive binary arithmetic coding	Networks	47.62651227905637	-17.688230146572955	75963
2a77bc1a80ce7d43bd22330a24a3c0d1d88ab792	rate distortion optimal ecg signal compression	graph theory;rate distortion;time domain analysis medical signal processing data compression electrocardiography graph theory;data compression;time domain analysis;electrocardiography;rate distortion electrocardiography upper bound interpolation monitoring bit rate application software heuristic algorithms graph theory graphics;time domain;rate distortion optimization;medical signal processing;graph theory problem rate distortion optimal ecg signal compression time domain algorithm exact optimization algorithms reconstruction error;electrocardiogram	Signal compression is an important problem encountered in many applications. Various techniques have been proposed over the years for addressing the problem. In this paper we present a time domain algorithm based on the coding of line segments which are used to approximate the signal. These segments are t in a way that is optimal in the rate distortion sense. Although the approach is applicable to any type of signal, we focus, in this paper, on the compression of ElectroCardioGram (ECG) signals. ECG signal compression has traditionally been tackled by heuristic approaches. However, it has been demonstrated [1] that exact optimization algorithms outperform these heuristic approaches by a wide margin with respect to reconstruction error. By formulating the compression problem as a graph theory problem, known optimization theory can be applied in order to yield optimal compression. In this paper we present an algorithm that will guarantee the smallest possible distortion among all methods applying linear interpolation given an upper bound on the number of bits. Compared to many other compression methods, we report superior performance for this method.	approximation algorithm;distortion;graph theory;heuristic;linear interpolation;mathematical optimization;rate–distortion theory;signal compression	Ranveig Nygaard;Gerry Melnikov;Aggelos K. Katsaggelos	1999		10.1109/ICIP.1999.822915	data compression;data compression ratio;mathematical optimization;combinatorics;time domain;computer science;graph theory;theoretical computer science;mathematics;lossless compression;rate–distortion optimization	Theory	46.08484069603763	-11.399121528231616	76087
106f6fab8e7822ea9d380fa4197e74be57442bb0	embedded quadtree-based image compression in dct domain	discrete wavelet transforms;quadtree set partition;data structures image coding transform coding data compression quadtrees discrete cosine transforms;data organization;image recognition;spine;image coding;psnr;data compression;ezdct;dct coefficients reorganization embedded quadtree based image compression discrete cosine transform dct based image coders mrdct eqdct jpeg mr dct peak signal to noise ratio psnr data organization data representation embedded image coder quadtree set partition ezdct;data engineering;transform coding;embedded quadtree based image compression;dct coefficients reorganization;data representation;discrete cosine transform;mrdct;eqdct;image compression;dct based image coders;jpeg;discrete cosine transforms;data structures;peak signal to noise ratio;mr dct;educational programs;computational efficiency;quadtrees;image coding discrete cosine transforms image recognition data engineering transform coding discrete wavelet transforms computational efficiency spine educational programs psnr;embedded image coder	Recent success in discrete cosine transform(DCT) image coding is mainly attributed to recognition of the importance of data organization and representation. In this paper, we proposed an embedded image coder based on quadtree set partition in DCT domain (EZDCT) which is suitable for many kinds of DCT coefficients reorganization schemes. The experimental results show that it is among the state-of-the-art DCT-based image coders when compared with the famous DCT-based image coders, such as EZDCT and MRDCT. For example, for the Barbara image, EQDCT outperforms JPEG, EZDCT and MRDCT by 3.3,1.71,1.70dB in peak-signal-to-noise ratio at 0.2Sbpp, respectively.	coefficient;discrete cosine transform;embedded system;image compression;jpeg;quadtree;signal-to-noise ratio	Xingsong Hou;Guizhong Liu;Yiyang Zou	2003		10.1109/ICASSP.2003.1199226	computer vision;speech recognition;data structure;information engineering;trellis quantization;peak signal-to-noise ratio;computer science;theoretical computer science;mathematics;quantization;statistics	Vision	43.73096349550398	-13.978803767533941	76108
9db7497f2df053dea7798aad620a4c1173d62b71	light field imaging coding: performance assessment methodology and standards benchmarking	software;hevc intra light field imaging coding 3d systems light capturing technologies light field cameras visual scene light intensity data pixel position image coding standard compression performance assessment methodology jpeg 2000 h 264 avc intra;video coding data compression image representation video cameras video codecs;image coding;standards;image coding rendering computer graphics software standards encoding transform coding cameras;transform coding;rendering computer graphics;encoding;cameras;codecs benchmarking light fields image coding assessment methodology	It is well known that the conventional ways to capture the light around us are limited and thus provide a limited user experience, notably in terms of parallax capabilities. As this has been preventing 3D systems to explode in the market, significant advances are emerging in terms of light capturing technologies among which is relevant to highlight the socalled light field cameras which capture a richer representation of the visual scene by measuring the light intensity for each direction and for each pixel position. Considering the huge amount of light intensity data involved, efficient compression becomes a must. In this context, this paper addresses the light field coding challenge by using available image coding standard solutions. With this target in mind, this paper proposes first a compression performance assessment methodology and presents after the compression performance of the direct, fully compatible usage of the main image coding standards available, notably JPEG, JPEG 2000, H.264/AVC Intra and HEVC Intra for a representative set of light field images. While the expected conclusion is that HEVC Intra is the most efficient codec, other less expected conclusions emerge.	codec;h.264/mpeg-4 avc;high efficiency video coding;jpeg 2000;light field;parallax;pixel;user experience	Gustavo Alves;Fernando Pereira;Eduardo A. B. da Silva	2016	2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2016.7574774	data compression;computer vision;transform coding;computer science;multimedia;context-adaptive binary arithmetic coding;encoding;statistics;computer graphics (images)	Visualization	43.97210399462285	-20.01382435120176	76157
53ee53dab5dfe01b054d50c36ed0e55f7ecfe75c	a framework for providing adaptive sports video to mobile devices	video streaming;mobile device;video compression;sports video;video server;content adaptation;multimedia content adaptation;extraction method;highlight extraction;mobile user	In this paper, we present a novel framework that serves adaptive sports video to mobile users. Our framework combines content-based sports highlights extraction and quality-domain video compression technologies in video server, capable of reducing wireless bandwidth consumption more effectively. We develop a robust replay-based highlights extraction method, and propose a content-based video streaming coding scheme to handle the problems of bandwidth and capacity of computation. To validate the practicality and effectiveness of our system, we conduct the experiments on several real soccer videos. The experimental results demonstrate the robustness of our highlights extraction method and more than 77.5% of the bandwidth consumption could be reduced.	computation;data compression;experiment;mobile device;server (computing);streaming media;video server	Cunxun Zang;Qingshan Liu;Xiaofeng Tong;Hanqing Lu	2006		10.1145/1374296.1374337	data compression;computer science;operating system;video tracking;mobile device;multimedia;video processing;internet privacy;world wide web	Mobile	44.64062358796959	-23.42793824385368	76159
540c2c7bb144c2dd6dfa2a10f019d6507fa142db	constrained optimization for neural map formation: a unifying framework for weight growth and normalization	simulation ordinateur;constrained optimization;optimisation;modele mathematique;optimizacion;computer model;modelo matematico;dynamic linking;fonction objectif;dynamical system;algorithme;objective function;optimization problem;systeme dynamique;algorithm;levels of abstraction;mappage;coordinate transformation;mathematical model;funcion objetivo;optimization;mapping;simulacion computadora;sistema dinamico;reseau neuronal;computer simulation;red neuronal;gradient flow;neural network;algoritmo	Computational models of neural map formation can be considered on at least three different levels of abstraction: detailed models including neural activity dynamics, weight dynamics that abstract from the neural activity dynamics by an adiabatic approximation, and constrained optimization from which equations governing weight dynamics can be derived. Constrained optimization uses an objective function, from which a weight growth rule can be derived as a gradient flow, and some constraints, from which normalization rules are derived. In this article, we present an example of how an optimization problem can be derived from detailed nonlinear neural dynamics. A systematic investigation reveals how different weight dynamics introduced previously can be derived from two types of objective function terms and two types of constraints. This includes dynamic link matching as a special case of neural map formation. We focus in particular on the role of coordinate transformations to derive different weight dynamics from the same optimization problem. Several examples illustrate how the constrained optimization framework can help in understanding, generating, and comparing different models of neural map formation. The techniques used in this analysis may also be useful in investigating other types of neural dynamics.	activity recognition;approximation;computation;computational model;constrained optimization;constraint (mathematics);database normalization;dynamic link matching;gradient;mathematical optimization;nonlinear system;optimization problem;principle of abstraction;rule (guideline);cell transformation	Laurenz Wiskott;Terrence J. Sejnowski	1998	Neural Computation	10.1162/089976698300017700	computer simulation;optimization problem;mathematical optimization;computer science;artificial intelligence;dynamical system;coordinate system;mathematical model;mathematics;balanced flow;algorithm	ML	40.2476112264672	-23.243720966113326	76349
64aa940f65f99b0ec545054997488e4451d55d90	fast mode decision algorithm in h.263+/h.264 intra transcoder	algorithme rapide;transcodage;multimedia;transcodificacion;reutilizacion;pixel domain;transformation cosinus discrete;reuse;transform domain;computational complexity;discrete cosine transforms;fast algorithm;cpdt;transcoding;mode decision;algoritmo rapido;reutilisation	In this paper, we proposed a fast mode decision algorithm in transform-domain for H.263+ to H.264 intra transcoder. In the transcoder, the residual signals carried by H.263+ bitstreams are threshold controlled to decide whether we should reuse the prediction direction provided by H.263+ or re-estimate the prediction direction. Then the DCT coefficients in H.263+ bitstreams are converted to H.264 transform coefficients entirely in the transform-domain. Finally, by using the new prediction mode and direction, the H.264 transform residual coefficients are coded to generate the H.264 bitstream. The simulation results show the performance of the proposed algorithm is close to that of a cascaded pixel-domain transcoder (CPDT) while transcoding computation complexity is significantly lower.		Min Li;Guiming He	2006		10.1007/11922162_5	real-time computing;transcoding;computer science;theoretical computer science;reuse;multimedia;computational complexity theory;algorithm	ML	46.50270756251372	-15.626836918516101	76388
c2c803ccb21e4f9f6da5627f8f896f21212f446d	radmpc: a fast decentralized approach for chance-constrained multi-vehicle path-planning		Robust multi-vehicle path-planning is important for ensuring the safety of multi-vehicle systems in applications like transportation, search and rescue, and robotic exploration. Chance-constrained methods like Iterative Risk Allocation (IRA)(Ono and Williams 2008) have been developed for situations where environmental disturbances are unbounded. However, chance-constrained methods for the multi-vehicle case generally use centralized strategies where the vehicle set is planned with couplings between all vehicle pairs. This approach is intractable as fleet size increases because computation time is exponential with respect to the number of vehicles being planned over due to a polynomial increase in coupling constraints between vehicle pairs. We present a faster approach for chance-constrained multivehicle path-planning that relies upon a decentralized pathplanning method called Risk-Aware Decentralized Model Predictive Control (RADMPC) to rapidly approximate a centralized IRA approach. The RADMPC approximation is evaluated for vehicle interactions to determine the vehicle sets that should be planned in a coupled manner. Applying IRA to the smaller vehicle sets determined from the RADMPC approximation rapidly plans safe paths for the entire fleet. A Monte Carlo simulation analysis demonstrates the correctness of our approach and a significant improvement in computation time compared to a centralized IRA approach.	analysis of algorithms;approximation algorithm;autonomous robot;brian;centralized computing;computation;correctness (computer science);decentralised system;interaction;iterative method;monte carlo method;motion planning;polynomial;robotic spacecraft;simulation;time complexity	A. Huang;Benjamin J. Ayton;Brian C. Williams	2018	CoRR			Robotics	53.511934116558	-23.839151500422822	76434
0cd36d93d03773c38412e46188088a78b9748fec	refinement of the pixel-based motion vector extrapolation in h.264 video	video streaming;data compression;pixel based mv correction pixel based motion vector extrapolation h 264 video video communication compressed video stream channel errors error concealment algorithm motion vector recovery algorithm hybrid motion vector extrapolation algorithm hmve algorithm;extrapolation;vectors extrapolation signal processing algorithms video coding artificial intelligence communication systems;video coding;video streaming data compression extrapolation video coding;motion vector extrapolation error concealment error resilient video transmission h 264 avc	In video communications, the compressed video stream is very likely to be corrupted by channel errors. Recently, many error concealment algorithms have been proposed in order to combat channel errors. In this paper, we have combined two state-of-the-art motion vector recovery algorithms into a even better algorithm; we have modified the hybrid motion vector extrapolation (HMVE) algorithm and further improved the accuracy of the derived motion vectors by pixel-based MV correction using neighboring received MVs. Experimental results show that this method is able to provide better performance than existing algorithms subjectively and objectively.	algorithm;data compression;error concealment;extrapolation;h.264/mpeg-4 avc;pixel;streaming media	Ting-Lan Lin;Neng-Chieh Yang;Tsung-En Chang;Wen-Chih Chen	2012	2012 International Symposium on Intelligent Signal Processing and Communications Systems	10.1109/ISPACS.2012.6473511	scalable video coding;computer vision;quarter-pixel motion;computer science;theoretical computer science;video tracking;motion estimation;block-matching algorithm;motion compensation;video denoising;multiview video coding;computer graphics (images)	Robotics	46.89198174492897	-17.123805847373514	76437
c27ba2c0a78efc24c4be9762a8ab2909b1a6ada5	adaptive frame level rate control in 3d-hevc	3d video coding;video coding adaptive codes data compression image sequences quantisation signal;bits allocation;bit rate video coding three dimensional displays quantization signal complexity theory psnr encoding;initial qp;rate control;3d video coding rate control initial qp bits allocation;3d hevc performance improvement adaptive frame level rate control algorithm high efficiency video coding 3d video compression standard 3dv test sequence demonstration quantization parameter decision scheme qp decision scheme bit allocation bitrate fluctuation complexity estimation method computational complexity reduction r d performance htm10 0	In this paper, we propose a new frame level rate control algorithm for the high efficiency video coding (HEVC) based 3D video (3DV) compression standard. In the proposed scheme, a new initial quantization parameter (QP) decision scheme is provided, and the bit allocation for each view is investigated to smooth the bitrate fluctuation and reach accurate rate control. Meanwhile, a simplified complexity estimation method for the extended view is introduced to reduce the computational complexity while improves the coding performance. The experimental results on 3DV test sequences demonstrate that the proposed algorithm can achieve better R-D performance and more accurate rate control compared to the benchmark algorithms in HTM10.0. The maximum performance improvement can be up to 12.4% and the average BD-rate gain for each view is 5.2%, 6.5% and 6.6% respectively.	benchmark (computing);blu-ray;computational complexity theory;data compression;high efficiency video coding;quantum fluctuation;rc algorithm	Songchao Tan;Junjun Si;Siwei Ma;Shanshe Wang;Wen Gao	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051586	data compression;real-time computing;harmonic vector excitation coding;computer science;theoretical computer science;coding tree unit;context-adaptive binary arithmetic coding;h.261	Robotics	46.94281454587236	-18.621631378070912	76533
38a26c3e73c47e33827d5fc0143ea0648e8452b7	sample-parallel execution of ebcot in fast mode	image coding;standards;transform coding;graphics processing units;materials requirements planning;encoding;context	JPEG 2000's most computationally expensive building block is the Embedded Block Coder with Optimized Truncation (EBCOT). This paper evaluates how encoders targeting a parallel architecture such as a GPU can increase their throughput in use cases where very high data rates are used. The compression efficiency in the less significant bit-planes is then often poor and it is beneficial to enable the Selective Arithmetic Coding Bypass style (fast mode) in order to trade a small loss in compression efficiency for a reduction of the computational complexity. More importantly, this style exposes a more finely grained parallelism that can be exploited to execute the raw coding passes, including bit-stuffing, in a sample-parallel fashion. For a latency-or memory critical application that encodes one frame at a time, EBCOT's tier-1 is sped up between 1.1x and 2.4x compared to an optimized GPU-based implementation. When a low GPU occupancy has already been addressed by encoding multiple frames in parallel, the throughput can still be improved by 5% for high-entropy images and 27% for low-entropy images. Best results are obtained when enabling the fast mode after the fourth significant bit-plane. For most of the test images the compression rate is within 1% of the original.	algorithm;analysis of algorithms;arithmetic coding;bit plane;computational complexity theory;encoder;frequency band;general-purpose computing on graphics processing units;graphics processing unit;jpeg 2000;most significant bit;parallel computing;self-propelled particles;throughput;truncation	Volker Bruns;Miguel A. Martínez-del-Amor	2016	2016 Picture Coding Symposium (PCS)	10.1109/PCS.2016.7906388	cognitive psychology;real-time computing;transform coding;computer science;theoretical computer science;material requirements planning;context-adaptive binary arithmetic coding;encoding;statistics;computer graphics (images)	Arch	46.20105927136329	-20.676571444118863	76576
8b7c9066ca28352cc2ca1e330e72a935e815e02f	an information hiding scheme using a pattern-based compression algorithm	histograms;compression algorithm;data hiding;image patterns data hiding image compression;image quality information hiding scheme pattern based compression algorithm data hiding scheme digital image image blocks;image coding;data compression;decoding;information hiding;data hiding scheme;data mining;image blocks;data encapsulation;image compression;image edge detection;image quality;pixel image coding histograms image edge detection data mining digital images decoding;pixel;image patterns;digital image;pattern based compression algorithm;digital images;image coding data compression data encapsulation;information hiding scheme	This paper presents a data hiding scheme that hides the data in the compression domain of a digital image. The pattern-based compression algorithm used for hiding the information, compresses the image according to the visual activity of individual image blocks. The key point of the proposed scheme is to first identify the smooth area of the host image and then embed the data in these areas. Experimental results confirm that the proposed scheme can embed a large amount of data in the compressed file while maintaining satisfactory image quality.	algorithm;data compression;digital image;image quality;information theory	Farhad Keissarian	2007	2007 Third International IEEE Conference on Signal-Image Technologies and Internet-Based System	10.1109/SITIS.2007.105	data compression;computer vision;image compression;computer science;theoretical computer science;pattern recognition;information hiding;digital image;statistics	Vision	40.71048636861851	-12.126073329525438	76606
87a5e9fdb4942d7b83b1a86378c76e7ad513bd74	coding isotropic images	rate distortion;pulse code modulation;image coding;weighted mean squared error;signal distortion;dpcm coding decoding;entropy coding;transform coding;differential pulse code modulation;imaging techniques;discrete cosine transform;2 dimensional;signal encoding;rate distortion theory;transform coding dft dpcm coding decoding discrete fourier transforms dft s entropy coding hadamard transforms image coding rate distortion theory;discrete fourier transforms dft s;hadamard transforms;mean square error;transmission efficiency;entropy;cosine transform;video communication;root mean square errors;dft;coders;autocorrelation	Rate-distortion functions for 2-dimensional homogeneous isotropic images are compared with the performance of five source encoders designed for such images. Both unweighted and frequency weighted mean-square error distortion measures are considered. The coders considered are a) differential pulse code modulation (DPCM) using six previous samples or picture elements (pels) in the prediction--herein called 6-pel DPCM, b) simple DPCM using single-sample prediction, c) 6-pel DPCM followed by entropy coding, d) 8 \times 8 discrete cosine transform coding, and e) 4 \times 4 Hadamard transform coding. Other transform coders were studied and found to have about the same performance as the two transform coders above. With the mean-square error distortion measure, 6-pel DPCM with entropy coding performed best. Next best was the 8 \times 8 discrete cosine transform coder and the 6-pel DPCM--these two had approximately the same distortion. Next were the 4 \times 4 Hadamard and simple DPCM, in that order. The relative performance of the coders changed slightly when the distortion measure was frequency weighted mean-square error. From R = 1 to 3 bits/pel, which was the range studied here, the performances of all the coders were separated by only about 4 dB.		John B. O'Neal;T. Raj Natarajan	1977	IEEE Trans. Information Theory	10.1109/TIT.1977.1055796	discrete mathematics;speech recognition;discrete cosine transform;mathematics;statistics	Theory	47.80641245489752	-10.568027648062637	77102
454d423e38642ff51f63aa9367a784e5441b5386	multiple description image coding based on lagrangian rate allocation	regime optimal;theorie vitesse distorsion;detection erreur;rate distortion characteristics;quantization;evaluation performance;rate distortion;optimisation;deteccion error;propagation losses;multiple description;art;optimal lagrangian rate allocation;image coding;performance evaluation;image processing;optimizacion;redundancia;computer graphics;resource allocation;rate distortion optimization error resilience jpeg 2000 multiple description coding;correction erreur;evaluacion prestacion;image coding lagrangian functions rate distortion redundancy streaming media quantization propagation losses character generation art resilience;simulation;resource management;procesamiento imagen;image multiple;regimen optimo;simulacion;imagen multiple;indexing terms;codigo bloque;traitement image;algorithms artificial intelligence computer graphics image enhancement image interpretation computer assisted information storage and retrieval signal processing computer assisted;multiple image;algorithme;rate distortion theory;asignacion optima;signal processing computer assisted;etat actuel;algorithm;codage image;gestion recursos;jpeg 2000 standard;image enhancement;performance improvement;compression image;redundancy;resilience;image compression;image interpretation computer assisted;streaming media;theoretical analysis;rate allocation;error correction;character generation;state of the art;independently coded blocks;jpeg 2000 standard multiple description image coding optimal lagrangian rate allocation independently coded blocks rate distortion characteristics optimal rate distortion condition;allocation optimale;multiple description coding;gestion ressources;optimal conditions;error resilience;artificial intelligence;algorithms;estado actual;multiple description image coding;code bloc;optimization;jpeg 2000;asignacion recurso;optimal rate distortion condition;correccion error;error detection;allocation ressource;optimal allocation;data consistency;rate distortion optimization	In this paper, a novel multiple description coding technique is proposed, based on optimal Lagrangian rate allocation. The method assumes the coded data consists of independently coded blocks. Initially, all the blocks are coded at two different rates. Then blocks are split into two subsets with similar rate distortion characteristics; two balanced descriptions are generated by combining code blocks belonging to the two subsets encoded at opposite rates. A theoretical analysis of the approach is carried out, and the optimal rate distortion conditions are worked out. The method is successfully applied to the JPEG 2000 standard and simulation results show a noticeable performance improvement with respect to state-of-the art algorithms. The proposed technique enables easy tuning of the required coding redundancy. Moreover, the generated streams are fully compatible with Part 1 of the standard	algorithm;allocation;assumed;backward compatibility;code::blocks;codec;decoder device component;distortion;encoder device component;enumerated type;frame (physical object);generalization (psychology);interdependence;jpeg 2000;license;loss function;macroblock;multiple description coding;rate–distortion theory;simulation;xfig	Tammam Tillo;Marco Grangetto;Gabriella Olmo	2007	IEEE Transactions on Image Processing	10.1109/TIP.2007.891152	computer vision;simulation;error detection and correction;image processing;computer science;resource management;theoretical computer science;mathematics;algorithm	Robotics	47.588162270507226	-14.41337959649711	77125
273237a9dec8dde240b22e22dad942e3b3209117	source and channel coding for audiovisual communication systems	telekommunikation;telecommunications	Topics in source and channel coding for audiovisual communication systems are studied. The goal of source coding is to represent a source with the lowest possible rate to achieve a particular distortion, or with the lowest possible distortion at a given rate. Channel coding adds redundancy to quantized source information to recover channel errors. This thesis consists of four topics. Firstly, based on high-rate theory, we propose Karhunen-Loève transform (KLT)-based classified vector quantization (VQ) to efficiently utilize optimal VQ advantages over scalar quantization (SQ). Compared with codeexcited linear predictive (CELP) speech coding, KLT-based classified VQ provides not only a higher SNR and perceptual quality, but also lower computational complexity. Further improvement is obtained by companding. Secondly, we compare various transmitter-based packet-loss recovery techniques from a rate-distortion viewpoint for real-time audiovisual communication systems over the Internet. We conclude that, in most circumstances, multiple description coding (MDC) is the best packet-loss recovery technique. If channel conditions are informed, channel-optimized MDC yields better performance. Compared with resolution-constrained quantization (RCQ), entropyconstrained quantization (ECQ) produces a smaller number of distortion outliers but is more sensitive to channel errors. We apply a generalized r-th power distortion measure to design a new RCQ algorithm that has less distortion outliers and is more robust against source mismatch than conventional RCQ methods. Finally, designing quantizers to effectively remove irrelevancy as well as redundancy is considered. Taking into account the just noticeable difference (JND) of human perception, we design a new RCQ method that has improved performance in terms of mean distortion and distortion outliers. Based on high-rate theory, optimal centroid density and its corresponding mean distortion are also accurately predicted. The latter two quantization methods can be combined with practical source coding systems such as KLT-based classified VQ and with joint source-channel coding paradigms such as MDC.	algorithm;channel capacity;code-excited linear prediction;companding;computational complexity theory;data compression;distortion;forward error correction;internet;multiple description coding;network packet;quantization (signal processing);real-time clock;signal-to-noise ratio;sound quality;speech coding;transmitter;vector quantization	Moo Young Kim	2004			telecommunications;computer science;multimedia;communication	ML	48.72422693699987	-15.547137941044001	77305
37cde2080f5d9d4899500964edcf42b0d38c8e1d	square-type-first inter-cu tree search algorithm for acceleration of hevc encoder	video encoder;index hevc;inter cu decision;fast encoding algorithm;square type mode	In this paper, a fast inter-coding algorithm is proposed to reduce the computational load of HEVC encoders. The HEVC reference model (HM) employs the recursive depth-first-search (DFS) of the quad-tree search in terms of rate-distortion optimization in selecting the best coding modes for the best CU, PU, TU partitions, and many associated coding modes. The proposed algorithm evaluates the RD costs of the current CU only for its square-type PUs in the top-down search of the DFS. When the CU partition with the square-type PU is better than its sub-level CU partitions in terms of RD cost in bottom-up search of the DFS, the square type of current CU partition, along with its coding mode, is selected as the best partition. Otherwise, non-square-type PUs for the current CU level are evaluated. If the sub-partition is better than the CU with the non-square PUs, the sub-partition is finally selected as the optimum PU. Otherwise, the best non-square PU is selected as the best PU for the current level. Experimental results demonstrate that the proposed square-type-first inter-PU search can reduce the computational load in average encoding time by 66.7 % with 1–2 % BD loss over HM reference software. In addition, the proposed algorithm can yield an additional average time saving of 26.8–35.5 % against the three fast encoding algorithms adopted in HM.	algorithmic efficiency;blu-ray;bottom-up parsing;codec;computational complexity theory;decibel;depth-first search;distortion;encoder;engine control unit;high efficiency video coding;ibm systems network architecture;mathematical optimization;one-class classification;pdf/a;rate–distortion optimization;real-time clock;recursion;reference model;ruby document format;search algorithm;tip (unix utility);top-down and bottom-up design;tree traversal	Yong-Jo Ahn;Dong-Gyu Sim	2015	Journal of Real-Time Image Processing	10.1007/s11554-015-0487-5	encoder;real-time computing;simulation;computer science;theoretical computer science;statistics	AI	47.04661312982703	-19.405686148701733	77322
e419019eeb64261ce4e7a63aa7e5688463578d65	forensics for partially double compressed doctored jpeg images	image forensics;partially double compressed;first quantization matrix;dct coefficient histogram;jpeg	Digital image forensics is required to investigate unethical use of doctored images by recovering the historic information of an image. Most of the cameras compress the image using JPEG standard. When this image is decompressed and recompressed with different quantization matrix, it becomes double compressed. Although in certain cases, e.g. after a cropping attack, the image can be recompressed with the same quantization matrix too. This JPEG double compression becomes an integral part of forgery creation. The detection and analysis of double compression in an image help the investigator to find the authenticity of an image. In this paper, a two-stage technique is proposed to estimate the first quantization matrix or steps from the partial double compressed JPEG images. In the first stage of the proposed approach, the detection of the double compressed region through JPEG ghost technique is extended to the automatic isolation of the doubly compressed part from an image. The second stage analyzes the doubly compressed part to estimate the first quantization matrix or steps. In the latter stage, an optimized filtering scheme is also proposed to cope with the effects of the error. The results of proposed scheme are evaluated by considering partial double compressed images based on the two different datasets. The partial double compressed datasets have not been considered in the previous state-of-the-art approaches. The first stage of the proposed scheme provides an average percentage accuracy of 95.45%. The second stage provides an error less than 1.5% for the first 10 DCT coefficients, hence, outperforming the existing techniques. The experimental results consider the partial double compressed images in which the recompression is done with different quantization matrix.	coefficient;digital image;discrete cosine transform;first quantization;floor and ceiling functions;jpeg;quantization (image processing)	Gurinder Singh;Kulbir Singh	2016	Multimedia Tools and Applications	10.1007/s11042-016-4290-5	computer vision;theoretical computer science;quantization;computer graphics (images)	Vision	40.052352536858315	-10.834021813194626	77734
f68978d251fadc747682c31034c6ea47d9c9bf7e	an application of unified reference picture list for motion-compensated video compression	standards;motion compensation;indexes;high efficiency video coding;entropy;encoding	Modern video compression standards rely on motion-compensated prediction from multiple reference pictures. In past standardisation efforts following the introduction of bidirectional prediction, reference pictures were typically signalled in two independent lists, used for forward- and backward-prediction. Modern video coding standards, such as H.264/MPEG-4 Advanced Video Coding or High Efficiency Video Coding, extend this concept by removing the separation between preceding and succeeding reference pictures, and allow reference frames to be included in the two lists regardless of their temporal index. Therefore, the principle of using two separate lists can be considered obsolete. This paper evaluates an alternative concept, using a single, unified reference picture list. This alternative list, called “list unified” or LU, reduces reference picture signalling, while all functionalities for motion-compensated prediction using multiple references are preserved. Evaluation shows competitive video coding efficiency compared to the usage of two lists, with the advantage of simplified bitstream parsing and much improved reference picture flexibility.	algorithmic efficiency;bitstream;data compression;h.264/mpeg-4 avc;high efficiency video coding;lu decomposition;parsing;video coding format	Sebastian Schwarz;Marta Mrak	2016	2016 Picture Coding Symposium (PCS)	10.1109/PCS.2016.7906324	video compression picture types;scalable video coding;reference frame;cognitive psychology;database index;entropy;computer science;theoretical computer science;coding tree unit;multimedia;motion compensation;h.262/mpeg-2 part 2;algorithm;encoding	AI	45.47399048238494	-18.948923368653283	77767
d113e037495fa195f2550f3d693396be4265b08f	reversible data hiding in jpeg image based on dct frequency and block selection		Abstract The joint photographic experts group (JPEG) is the most popular format of digital image. In this paper, a novel reversible data hiding scheme based on JPEG image is proposed. In JPEG image, quantified DCT (discrete cosine transform) coefficients with different frequencies will yield different capacities and embedding distortions. To reduce the total distortion for the marked image, we select coefficients from frequencies yielding less distortions for embedding, and then an advanced block selection strategy is applied to always modify the block yielding less simulated distortion firstly until the given payloads are completely embedded. Experimental results show that the proposed method can keep good visual quality with small bitstream expansion.	discrete cosine transform;jpeg	Dongdong Hou;Haoqian Wang;Weiming Zhang;Nenghai Yu	2018	Signal Processing	10.1016/j.sigpro.2018.02.002	computer vision;information hiding;mathematical optimization;digital image;discrete cosine transform;bitstream;mathematics;embedding;distortion;jpeg;artificial intelligence	EDA	40.841826660321544	-11.748858173668273	77850
c7d1679f837f93cdba7bf1fc800380c8d496b9dc	estimation of primary quantization matrix for steganalysis of double-compressed jpeg images	cuantificacion senal;contraste;protection information;traitement signal;quantization;image coding;steganographie;steganalysis;image processing;low frequency;transformation cosinus discrete;maquina vector soporte;procesamiento imagen;traitement image;algorithme;algorithm;codage image;accuracy;machine vecteur support;steganography;histogram;esteganografia;compression image;precision;matrices;signal quantization;histogramme;image compression;proteccion informacion;steganalyse;discrete cosine transforms;signal processing;information protection;quantification signal;signal classification;basse frequence;estaganalisis;classification signal;algorithms;etalonnage;baja frecuencia;support vector machine;classification automatique;automatic classification;histograma;procesamiento senal;clasificacion automatica;calibration;algoritmo;compresion imagen	A JPEG image is double-compressed if it underwent JPEG compression twice, each time with a different quantization matrix but with the same 8 × 8 grid. Some popular steganographic algorithms (Jsteg, F5, OutGuess) naturally produce such double-compressed stego images. Because double-compression may significantly change the statistics of DCT coefficients, it negatively influences the accuracy of some steganalysis methods developed under the assumption that the stego image was only single-compressed. This paper presents methods for detection of double-compression in JPEGs and for estimation of the primary quantization matrix, which is lost during recompression. The proposed methods are essential for construction of accurate targeted and blind steganalysis methods for JPEG images, especially those based on calibration. Both methods rely on support vector machine classifiers with feature vectors formed by histograms of low-frequency DCT coefficients.	algorithm;coefficient;discrete cosine transform;existential quantification;jpeg;quantization (image processing);robustness (computer science);steganalysis;steganography;support vector machine	Tomás Pevný;Jessica J. Fridrich	2008		10.1117/12.759155	computer vision;speech recognition;steganalysis;image processing;theoretical computer science;signal processing;accuracy and precision;quantization	Vision	43.91257804364797	-11.040897036680457	77909
a5f8e68ada13c433b3d39e73a93120a77af0a56a	a study on the application of color transfer technique for video compression.	video compression	In this paper, we explore the technique of colorization for video compression. The chrominance of the intermediate frames is discarded during encoding. At the time of decoding, chrominance values, which are dropped, are approximated from colorizing grayscale frames by transferring color from a source color frame to a targeted grayscale frame. So far human intervention is an integral part of colorization and pseudo colorization. But the present work can be looked upon as a novel application of the color transformation methodology without any human intervention for video compression. The videos regenerated at the decoding end demonstrate the potential and the utility of color transferring techniques in video compression domain.	approximation algorithm;data compression;floor and ceiling functions;grayscale	K. Madhu Sudhana Rao;Suman K. Mitra	2004			video compression picture types;data compression;data compression ratio;uncompressed video;motion compensation;texture compression;video denoising;multiview video coding	Vision	44.18332321786438	-18.64537490943223	78219
425a80b8e2362110f6d13ec667143d2c647c1ff1	using lzw compression technique for payload encoding in multiple frequency domain steganography	discrete wavelet transform;information hiding;variable length code;satisfiability;discrete cosine transform;steganography;mean square error;peak signal to noise ratio;binary data;frequency domain;security;high frequency	In this paper the frequency domain steganography is proposed; it uses DWT (discrete wavelets transform) with DCT (discrete cosine transform) techniques, which are applied sequentially on the cover image. Embedding the secret message is done in the high frequency coefficients to provide a high imperceptibility; as it is the most important property of any steganography system. For secret message encoding, variable length encoding is used with a special character codes to encode the characters into binary data, by using the LZW compression technique. This encoding encrypts the secret message and reduces the number of bits used to encode each character, which results in the increment of the capacity. With this new technique, we got very good results through satisfying and improving the most important properties of steganography such as: Imperceptibility; improved by having the MSE (mean square error) → 0 and PSNR (peak signal to noise ratio) → 128 db, security; improved by using an encrypted Stego-key and secret message by the encoding, capacity; improved by encoding the secret message characters with variable length codes and embedding the secret message in the YCbCr components instead of the RGB.	binary data;coefficient;data compression;discrete cosine transform;discrete wavelet transform;encode;encryption;lempel–ziv–welch;mean squared error;peak signal-to-noise ratio;steganography;variable-length code	Raoof Smko;Abdelsalam Almarimi;K. Negrat	2010		10.1145/1967486.1967583	arithmetic;speech recognition;theoretical computer science;mathematics	Visualization	41.07392770726723	-12.203485966429833	78261
befcd4c763dcd3df52e56196e6ec638d364c5e02	6k effective resolution with 4k hevc decoding capability for omaf-compliant 360° video streaming		The recent Omnidirectional MediA Format (OMAF) standard specifies delivery of 360° video content. OMAF supports only equirectangular (ERP) and cubemap projections and their region-wise packing with a limitation on video decoding capability to the maximum resolution of 4K (e.g., 4096x2048). Streaming of 4K ERP content allows only a limited viewport resolution, which is lower than the resolution of many current head-mounted displays (HMDs). In order to take the full advantage of those HMDs, this work proposes a specific mixed-resolution packing of 6K (6144x3072) ERP content and its realization in tile-based streaming, while complying with the 4K-decoding constraint and the High Efficiency Video Coding (HEVC) standard. Experimental results indicate that, using Zonal-PSNR test methodology, the proposed layout decreases the streaming bitrate up to 32% in terms of BD-rate, when compared to mixed-quality viewport-adaptive streaming of 4K ERP as an alternative solution.	4k resolution;blu-ray;cube mapping;digital video;erp;head-mounted display;high efficiency video coding;peak signal-to-noise ratio;set packing;streaming media;video decoder;viewport	Alireza Zare;Alireza Aminlou;Miska M. Hannuksela	2018		10.1145/3210424.3210425	viewport;computer vision;decoding methods;test method;omnidirectional antenna;artificial intelligence;computer science;equirectangular projection;cube mapping	Vision	44.37461058457794	-19.798342095979272	78275
cd68aa7d5e428114ada6c153bf5622f76658a9a7	perceptually weighted distortion measure in low bitrate block-based video coders	video compression;motion estimation;distortion measurement weight measurement bit rate video compression encoding humans video coding motion estimation decoding psnr;block based hybrid coding motion estimation entropy coding frame reconstruction perceptually weighted distortion measure low bitrate block based video coders compressed video visual artifacts pwd measure subjective quality computational complexity;video coding;distortion;computational complexity;image reconstruction;entropy codes;distortion video coding motion estimation entropy codes image reconstruction	Visual artifacts found in videos compressed at low bitrates may be the results of various decisions made in the encoding process. Although many post-processing techniques are available to detect and reduce such artifacts, they often lead to undesired blurriness and require extensive computations. A better approach is to avoid such artifacts in the first place by making better decisions in the encoding process. In this paper, a perceptually weighted distortion (PWD) measure is presented. This distortion measure, which outperforms the conventional distortion measures in terms of subjective quality, leads to an acceptable increase in computational complexity.	artifact (software development);computation;computational complexity theory;distortion;video post-processing;visual artifact	Jaehan In;Ali Jerbi;Foued Ben Amara	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1418794	data compression;iterative reconstruction;computer vision;speech recognition;distortion;computer science;video quality;theoretical computer science;motion estimation;mathematics;rate–distortion optimization;context-adaptive binary arithmetic coding;computational complexity theory;motion compensation;algorithm;multiview video coding	Robotics	45.914161370761796	-17.504592191476416	78420
995b0a13303895c579196053d71e6f5fe37e33dc	an improved discrete particle swarm optimizer for fast vector quantization codebook design	codebook design;maximum descent algorithm;convergence;cost function;iterative algorithms;design engineering;tree structured vector quantizers;convergence rate;trees mathematics;design optimization;vector quantisation particle swarm optimisation trees mathematics;particle swarm optimization design optimization vector quantization partitioning algorithms clustering algorithms iterative algorithms convergence design engineering cost function computational efficiency;binary partitioning scheme discrete particle swarm optimizer codebook design tree structured vector quantizers maximum descent algorithm;vector quantization;particle swarm optimization;tree structure;discrete particle swarm optimizer;clustering algorithms;binary partitioning scheme;vector quantizer;discrete particle swarm optimization;vector quantisation;computational efficiency;particle swarm optimisation;partitioning algorithms	For tree-structured vector quantizers (TSVQ), the codebook quality highly depends on the splitting criterion and the approach by which a specific node is selected and then be partitioned into new ones. Among several proposed TSVQs, maximum descent (MD) algorithm can produce high quality code-books and reduce the computation time simultaneously. In this paper, under the basic structure of MD algorithm, we propose an improved discrete particle swarm optimizer with less computation cost and faster convergence rate than the conventional one, and then, based on which, a novel binary partitioning scheme for MD algorithm is presented. Experimental data show that the newly proposed algorithm can further improve the codebook quality while the computation time is almost equivalent to that of the MD algorithm.	algorithm;book;codebook;computation;cryptographic hash function;display resolution;mathematical optimization;particle swarm optimization;rate of convergence;time complexity;vector quantization	Yuxuan Wang;Qiao-Liang Xiang	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284689	mathematical optimization;multidisciplinary design optimization;convergence;computer science;theoretical computer science;machine learning;mathematics;tree structure;cluster analysis;rate of convergence;linde–buzo–gray algorithm;particle swarm optimization;vector quantization	Robotics	44.18152562856018	-13.153525089470762	78486
4bc9b726e069632fc6cdbd15d2d639e8ffd87e95	reference index-based h.264 video watermarking scheme	watermarking;reference index;h 264;video	Video watermarking has received much attention over the past years as a promising solution to copy protection. Watermark robustness is still a key issue of research, especially when a watermark is embedded in the compressed video domain. In this article, a robust watermarking scheme for H.264 video is proposed. During video encoding, the watermark is embedded in the index of the reference frame, referred to as reference index, a bitstream syntax element newly proposed in the H.264 standard. Furthermore, the video content (current coded blocks) is modified based on an optimization model, aiming at improving watermark robustness without unacceptably degrading the video's visual quality or increasing the video's bit rate. Compared with the existing schemes, our method has the following three advantages: (1) The bit rate of the watermarked video is adjustable; (2) the robustness against common video operations can be achieved; (3) the watermark embedding and extraction are simple. Extensive experiments have verified the good performance of the proposed watermarking scheme.	bitstream;copy protection;data compression;digital video;digital watermarking;embedded system;experiment;h.264/mpeg-4 avc;mathematical optimization;reference frame (video);scheme	Hongmei Liu;Jiwu Huang;Yun Q. Shi	2012	TOMCCAP	10.1145/2344436.2344439	scalable video coding;computer vision;video;telecommunications;digital watermarking;computer science;theoretical computer science;block-matching algorithm;multimedia;watermark;motion compensation	Vision	45.71087507392136	-18.52750642679441	78529
6a3021ba46cf7320051e4853aefb7df1aca780ab	improved error concealment using scene information	image coding;error concealment;codage image;video coding;senal video;signal video;codage video;video signal	Signaling of scene information in coded bitstreams was proposed by the authors and adopted into the emerging video coding standard H.264 (also known as MPEG-4 part 10 or AVC) as a supplemental enhancement information (SEI) message. This paper proposes some improved error concealment methods for intra coded pictures and scene transition pictures using the signaled scene information. Simulation results show that the proposed methods outperform conventional techniques significantly.	color;data compression;error concealment;h.264/mpeg-4 avc;simulation;software engineering institute;video coding format	Ye-Kui Wang;Miska M. Hannuksela;Kerem Caglar;Moncef Gabbouj	2003		10.1007/978-3-540-39798-4_36	scalable video coding;computer vision;telecommunications;computer science;video quality;coding tree unit;multimedia;video processing;video denoising;multiview video coding	Vision	45.190749883599146	-18.75163735657894	78641
23a5af99cefb583bc63a6986638f2f10348dcc71	perceptual rate distortion optimization for block mode selection in hybrid video coding	optimisation;data compression;rate distortion performance video compression technologies mpeg 4 advanced video coding standard high efficiency video coding motion representation residual coding options transform domain coding efficiency lagrangian cost function conventional distortion function perceptual rate distortion optimization algorithm block mode selection content dependent lagrangian multiplier perceptual quality;rate distortion theory;video coding;proceedings paper;optimization encoding video coding rate distortion bit rate psnr indexes;visual perception;encoding;visual perception data compression encoding optimisation rate distortion theory video coding	Video compression technologies developed recently have taken advantage of a hybrid approach for better coding efficiency. For example, in the MPEG-4 Advanced Video Coding standard and also the new High Efficiency Video Coding, there are numerous possibilities of motion representation and residual coding options in transform domain. The coding parameters, including block modes, motion vectors, and indices of reference frames, are often correlated with each other in spatial and temporal directions. The selection of those coding parameters can affect the coding efficiency greatly. In many of the state-of-the-art solutions, the selection of coding options is based on the minimization of a Lagrangian cost function which is constructed of conventional distortion function and produced data rate. However, the conventional distortion function measured as either sum of absolute difference or mean square error does not often represent the observed deficiency in reconstructed videos. In this paper, a perceptual rate distortion optimization algorithm for block mode selection is developed, and the content-dependent Lagrangian multiplier is then derived. The just-noticeable distortion is used in the proposed method such that perceptual quality can be considered. The simulation results show that the rate-perceptual distortion performance is improved substantially without sacrificing the conventional rate-distortion performance.	algorithm;algorithmic efficiency;angular defect;block-oriented terminal;data compression;data rate units;distortion;h.264/mpeg-4 avc;high efficiency video coding;lagrange multiplier;loss function;mathematical optimization;mean squared error;pixel;quantization (signal processing);rate–distortion optimization;rate–distortion theory;simulation;spatial reference system;video coding format	Chen-Chou Huang;Hsu-Feng Hsiao	2013	2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)	10.1109/ISCAS.2013.6571887	data compression;sub-band coding;computer vision;mathematical optimization;rate–distortion theory;visual perception;harmonic vector excitation coding;theoretical computer science;context-adaptive variable-length coding;coding gain;coding tree unit;mathematics;rate–distortion optimization;context-adaptive binary arithmetic coding;motion compensation;h.261;encoding;statistics;multiview video coding	Vision	46.74377509878569	-17.261098736144692	78652
a8d82fa7ff742dc219d3fb90cf77ca3c8b45b2cd	blind image watermark detection algorithm based on discrete shearlet transform using statistical decision theory		Blind watermarking targets the challenging recovery of the watermark when the host is not available during the detection stage. This paper proposes Discrete Shearlet Transform (DST) as a new embedding domain for blind image watermarking. Our novel DST blind watermark detection system uses a nonadditive scheme based on the statistical decision theory. It first computes the Probability Density Function (PDF) of the DST coefficients modeled as a Laplacian distribution. The resulting likelihood ratio is compared with a decision threshold calculated using Neyman–Pearson criterion to minimize the missed detection subject to a fixed false alarm probability. Our method is evaluated in terms of imperceptibility, robustness, and payload against different attacks (Gaussian noise, blurring, cropping, compression, and rotation) using 30 standard grayscale images covering different characteristics (smooth, more complex with a lot of edges, and high detail textured regions). The proposed method shows greater windowing flexibility with more sensitive to directional and anisotropic features when compared against discrete wavelet and contourlets.		Baharak Ahmaderaghi;Fatih Kurugollu;Jesús Martínez del Rincón;Ahmed Bouridane	2018	IEEE Transactions on Computational Imaging	10.1109/TCI.2018.2794065	grayscale;probability density function;contourlet;wavelet;mathematical optimization;mathematics;digital watermarking;gaussian noise;watermark;shearlet	Vision	41.28317779101663	-10.284078269513614	78807
94ffc370e2189531cf534d759d040b0117faf056	a fast two-stage omp algorithm for coding stereo image residuals	orthogonal matching pursuit;rate distortion;rate distortion performance signal decomposition image coding disparity residual image compensation stereo image pair matching pursuit based coding scheme sequential orthogonal subspace updating sosu orthogonal matching pursuit process;image coding;cosine transforms;motion compensation;iterative methods image coding stereo image processing rate distortion theory;correlation methods;rate distortion theory;conference paper;approximation theory;iterative methods;stereo image processing;algorithms;image coding matching pursuit algorithms dictionaries signal processing algorithms computational modeling discrete cosine transforms signal resolution high resolution imaging image resolution hardware;computer simulation;functions;problem solving	Over-complete signal decomposition has been shown to be an efficient technique for coding disparity compensated residual images in stereo image pairs. This paper presents a residual image coding scheme based on a recently proposed MP-based coding scheme called SOW. The proposed scheme decomposes an orthogonal matching pursuit process into two stages. In different stages, dictionaries of different nature are used. Simulation results show that the proposed scheme can significantly reduce the computation effort while maintaining a similar ratedistortion performance as compared with SOSU.	algorithm;binocular disparity;computation;dictionary;matching pursuit;openmp;scheme;simulation;statement of work	Wan-Fung Cheung;Yuk-Hee Chan	2003		10.1109/ICIP.2003.1247071	computer simulation;computer vision;mathematical optimization;rate–distortion theory;computer science;theoretical computer science;mathematics;iterative method;motion compensation;function;algorithm;matching pursuit;approximation theory	Vision	45.68963408560367	-16.467226075135105	78929
f22f087c6a6d915af5fc21d199604a7fd108ada2	palette mode — a new coding tool in screen content coding extensions of hevc	color indexes encoding image color analysis decoding video coding graphics;video coding image colour analysis;hevc palette screen content coding scc high efficiency video coding;lb common test conditions screen content coding extensions high efficiency video coding hevc scc joint collaborative team on video coding jct vc palette mode coding unit representative colours screen contents pixel values all intra common test conditions random access common test conditions low delay b common test conditions	Palette mode, as a new coding tool, is adopted in the Screen Content Coding Extensions of High Efficiency Video Coding (HEVC SCC) that is being defined by the Joint Collaborative Team on Video Coding (JCT-VC). In the palette mode, pixels in a coding unit (CU) are represented by selected representative colours according to the characteristics of screen contents in which pixel values usually concentrate on few colour values. This paper introduces the palette mode in HEVC SCC, and our contributions to the palette mode are also presented. Experimental results show that disabling the palette mode suffers 20.3%, 13.0%, and 7.5% BD-rate increases for “YUV, text & graphics with motion, 1080p & 720p sequences” under all intra (AI), random access (RA), and low-delay B (LB) common test conditions, respectively.	blu-ray;color;graphics;high efficiency video coding;lattice boltzmann methods;palette (computing);pixel;random access;tip (unix utility);traffic collision avoidance system;vc dimension	Yu-Chen Sun;Tzu-Der Chuang;PoLin Lai;Yi-Wen Chen;Shan Liu;Yu-Wen Huang;Shawmin Lei	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351234	computer vision;h.263;computer science;coding tree unit;multimedia;context-adaptive binary arithmetic coding;h.261;multiview video coding;computer graphics (images)	Robotics	44.82491187728705	-19.967073302485286	79320
76a4df1c49df7c93be71d45f992908b31cfdaeb0	capacity improved robust lossless image watermarking	robust lossless image watermarking method;host image recovery;transforms image segmentation image watermarking;robust lossless image watermarking method image thresholding embedding capacity reduction host image recovery pixel value overflow underflow slt slantlet transform domain;slantlet transform domain;image thresholding;slt;pixel value overflow underflow;embedding capacity reduction	Nowadays, the lossless watermarking methods that can resist attacks have attracted more attention. Obtaining a robust lossless watermarking was at the cost of reducing the capacity and the watermarked image quality. This study presents a new robust lossless watermarking scheme in the transform domain where the Slantlet transform (SLT) has been applied to transform the host image and blocks of the SLT coefficients have been selected for the embedding process. The histograms of the selected blocks are modified according to a predefined threshold value to carry the watermark bits. The overflow/ underflow of the pixel values have been avoided by using a pixel adjustment method as a post-processing step. In the proposed scheme, the original host image can be recovered without any distortion after the hidden watermark has been extracted and the watermark can withstand different kinds of attacks. In comparison with the previous methods, the proposed scheme has higher embedding capacity, better robustness against unintentional attacks and improved visual quality. The results of the experiments prove the efficiency of the proposed scheme.	arithmetic underflow;coefficient;digital watermarking;distortion;experiment;image quality;lossless compression;pixel;video post-processing;watermark (data file)	Rasha Thabit Mohammed;Bee Ee Khoo	2014	IET Image Processing	10.1049/iet-ipr.2013.0862	computer vision;theoretical computer science;mathematics	Vision	40.31305447722739	-11.664160855372657	79620
bb795f59638074904c373b1b0d8f1b01c70ed7d6	mptc: video rendering for virtual screens using compressed textures	video compression;video in vr;gpu decoding;texture compression	We present a new method, Motion Picture Texture Compression (MPTC), to compress a series of video frames into a compressed video-texture format, such that the decoded output can be rendered using commodity texture mapping hardware. Our approach reduces the file size of compressed textures by exploiting redundancies in both the temporal and spatial domains. Furthermore, we ensure that the overall rendering quality of the compressed video-textures is comparable to that of compressed image-textures. At runtime, we render each frame of the video decoded from the MPTC format using texture mapping hardware on GPUs. Overall, MPTC improves the bandwidth from CPU to GPU memory up to 4−6× ona desktop and enables rendering of high-resolution videos (2K or higher) on current Head Mounted Displays (HMDs). We observe 3 − 4× improvement in rendering speeds for rendering high-resolution videos on desktop. Furthermore, we observe 9 − 10× improvement in frame rate on mobile platforms using a series of compressed-image textures for rendering high-resolution videos.	central processing unit;data compression;desktop computer;graphics processing unit;image resolution;mobile device;original net animation;rendering (computer graphics);texture compression;texture mapping	Srihari Pratapa;Pavel Krajcevski;Dinesh Manocha	2017		10.1145/3023368.3023375	data compression;computer vision;rendering;computer science;multimedia;texture memory;texture compression;computer graphics (images)	Graphics	45.42743252998554	-20.62007540357127	79713
a420b2feb5e600dfaec7b64667a3870884aba89d	data hiding in a hologram by modified digital halftoning techniques	data hiding;hidden information;record format;holograma;image numerique;steganographie;format enregistrement;intelligence artificielle;methode matricielle;steganography;esteganografia;imagen numerica;matrix method;metodo matriz;artificial intelligence;digital image;inteligencia artificial;formato grabacion;hologramme;hologram	The objective of this research is to design a novel data hiding method for the dot-matrix hologram. The modified error diffusion techniques for addressing the halftone dots in six colors are proposed to make efficient use of the image area and to hide information. After outputting the encoded hologram, which is in analog format, an independent data recovery system is applied to capture the encoded holographic image and to extract the hidden information. The results show that it is feasible to hide data in a dot-matrix hologram. The method proposed in this research can achieve multiple security features for hologram in anti-counterfeiting applications.		Hsi-Chun Wang;Wei-Chiang Wang	2005		10.1007/11553939_152	matrix method;computer vision;computer science;artificial intelligence;steganography;programming language;holography;information hiding;digital image;statistics;computer graphics (images)	HCI	43.268831944651005	-11.284815777075936	79776
8e043f70c8e5513692bf82f77eaadffad1e4893e	two improved codebook search methods of vector quantization based on orthogonal checking and fixed range search	quantization;full search;triangle inequality;search method;image quality;vector quantizer;signal to noise ratio;eigenvectors	We present two improved methods for the codebook search in this article. We call them the improved eigenvector method (IEVM) and the improved triangle inequality elimination (ITIE), respectively. IEVM is a full-search (FS) equivalent method, but ITIE is not. In these two methods we propose some techniques, such as the orthogonal checking and the fixed range search, to speed up their performance. According to our experimental results, IEVM is an efficient method. It is faster than FS, TIE, and EVM. Its execution time and number of operations per pixel were 4.86% and 4.76% of those in FS, respectively. As for ITIE, it is always faster than IEVM even if we limit its peak signal-to-noise ratio degradation to be no greater than 3% of the image quality of FS. Under this limitation, the execution time and the number of operations per pixel of ITIE were only 3.08% and 3.05% of those in FS, respectively.	codebook;range searching;vector quantization	Chin-Chen Chang;Wen-Tsai Li;Tung-Shou Chen	1998	J. Electronic Imaging	10.1117/1.482606	image quality;computer vision;mathematical optimization;combinatorics;discrete mathematics;learning vector quantization;quantization;eigenvalues and eigenvectors;computer science;triangle inequality;mathematics;linde–buzo–gray algorithm;signal-to-noise ratio;vector quantization;algorithm	AI	43.66280041355682	-13.159451547347217	79784
b3f5bb211fe28417f48e08acded9582d46853c42	lossless eeg signal compression	linear prediction techniques lossless eeg signal compression lossless electroencephalograph signal compression arithmetic coder;data compression;electroencephalography brain models image coding standards encoding histograms;electroencephalography mathematics signal processing computer assisted;electroencephalography;medical signal processing;medical signal processing data compression electroencephalography	In this work, we investigate the lossless compression of EEG signals using Burrows-Wheeler Transformation and linear prediction. We show that when compressing EEG signals utilization of linear prediction and Burrows-Wheeler Transformation yield better compression than the well-known techniques.	burrows–wheeler transform;electroencephalography;lossless compression;signal compression;whole earth 'lectronic link	Arda Gumusalan;Ziya Arnavut;Hüseyin Koçak	2009	2009 Fifth International Conference on Soft Computing, Computing with Words and Perceptions in System Analysis, Decision and Control	10.1109/EMBC.2012.6347331	data compression;lossy compression;electronic engineering;neuroscience;speech recognition;electroencephalography;image compression;computer science;theoretical computer science;lossless compression;context-adaptive binary arithmetic coding;statistics	EDA	42.59800766697239	-15.585786022994768	79807
e3803e71e912eea32a077788a7e5f307a47de2c3	lossless video coding using wavelet packet and subband domain motion estimation	lossless video coding;reversible wavelet packet lossless video coding subband domain motion estimation lossless transforms integer transform coefficients inter frame coding intra frame coding subband divisions subband blocks zero tree coding scheme exact bit rate control quality resolution scalability progressive transmission compression performance;zero tree coding scheme;image coding;decoding;lossless transforms;subband divisions;motion estimation;transform coding;trees mathematics;wavelet transforms video coding motion estimation trees mathematics;compression performance;wavelet packet;wavelet transforms;resolution scalability;rate control;video coding;intra frame coding;inter frame coding;wavelet transform;progressive transmission;quality;subband domain motion estimation;image reconstruction;video coding wavelet packets wavelet domain motion estimation wavelet transforms decoding transform coding image coding image reconstruction communication system control;subband blocks;reversible wavelet packet;wavelet packets;communication system control;wavelet domain;exact bit rate control;integer transform coefficients	In this paper, we propose a lossless video coding scheme based on a Reversible Wavelet Packet, and Subband Domain Motion Estimation. In lossless transforms, integer input signals are transformed into integer transform coefficients and losslessly reconstructed. Our method permits both intra and inter frame coding to proceed in the wavelet transform domain. The subband divisions by wavelet packet are varied effectively. And the motions between two successive frames are described as the translation of subband blocks. Each frame is referred to the next frame. The prediction differences are encoded by a zero tree coding scheme to provide some desirable features including exact bit-rate control, quality and resolution scalability, progressive transmission, good compression performance and possibility of lossless coding. This paper offers the availability of our strategy in lossless video coding.	coefficient;data compression;lossless compression;motion estimation;network packet;scalability;wavelet transform	Tomoyoshi Oguri;Y. Indou;Masaaki Ikehara	2002		10.1109/APCCAS.2002.1115125	data compression;lossless jpeg;computer vision;electronic engineering;theoretical computer science;context-adaptive variable-length coding;coding tree unit;mathematics;tunstall coding;wavelet packet decomposition;context-adaptive binary arithmetic coding;wavelet transform	Networks	45.86840206169745	-17.237799521508926	79975
d3db0bee268051af9f577c938b4e3de16e7f9e1a	images reconstruction with use of a genetic algorithm	genetic engineering;rgb color model;image coding;image reconstruction genetic algorithms image coding image colour analysis;decoding;application software;color model;biological cells;binary coding;general population;computer experiment;image colour analysis;image reconstruction;pixel;genetic algorithm;genetic algorithms;genetic mutations;computer science;image reconstruction genetic algorithms biological cells pixel application software genetic mutations computer science image coding decoding genetic engineering;binary coding image reconstruction genetic algorithm colored images black and white images image coding rgb color model;black and white images;color image;colored images	In this paper we investigate the task of images reconstruction with use of a genetic algorithm (GA). We consider two kinds of images: colored and black- and-white. Each kind of images requires different coding for a GA. In the case of colored images we base on the RGB color model. When black-and-white images are considered, we use binary coding. A genetic algorithm evolves an initial randomly generated population of individuals (images). The aim of a GA is to find a solution as similar to a reconstructed image as possible. We show results of computer experiments and formulate conclusions arising from them.	binary number;computer experiment;genetic algorithm;procedural generation;software release life cycle	Anna Piwonska;Urszula Grycuk	2007	6th International Conference on Computer Information Systems and Industrial Management Applications (CISIM'07)	10.1109/CISIM.2007.41	computer vision;genetic algorithm;computer science;artificial intelligence;machine learning;statistics;computer graphics (images)	Robotics	44.157114204448284	-12.472515750103085	80134
73b11db5c16e7b6ad411717e7ceb63dfb0f6cc81	comparisons of file formats for image transmission through networks	png;comparative analysis;file formats;image transmission;jpeg;jpeg2000	This paper presents a comparative analysis of the three most important progressive file formats: PNG, JPEG and JPEG2000.	jpeg 2000;portable network graphics;qualitative comparative analysis	Rafael Dueire Lins;Charlana Rodrigues	2006		10.1145/1141277.1141475	qualitative comparative analysis;data conversion;portable network graphics;computer science;theoretical computer science;operating system;jpeg;jpeg 2000;image file formats;internet privacy;jpeg file interchange format;file format;world wide web	Networks	40.76913593756882	-15.094856430029502	80135
02f9d3494f6ff259e94549efeb7a92580533c640	real-time perceptual coding of wideband speech by competitive neural networks	transformation ondelette;codage parole;arithmetic coding;systeme aide decision;quantifier;real time;acoustic modeling;wide band;recommandation;speech coding;sistema ayuda decision;wavelet packet;competitive neural network;arithmetic code;large bande;codigo aritmetico;decision support system;quantificateur;recomendacion;recommendation;banda ancha;code arithmetique;transformacion ondita;wideband speech;reseau neuronal;probability model;cuantificador;red neuronal;parole large bande;wavelet transformation;neural network	We developed a real-time wideband speech codec adopting a wavelet packet based methodology. The transform domain coefficients were first quantized by means of a mid-tread uniform quantizer and then encoded with an arithmetic coding. In the first step the wavelet coefficients were quantized by using a psycho-acoustic model. The second step was carried out by adapting the probability model of the quantized coefficients frame by frame by means of a competitive neural network. The neural network was trained on the TIMIT corpus and his weights updated in real-time during the compression in order to model better the speech characteristics of the current speaker. The coding/decoding algorithm was first written in C and then optimised on the TMS320C6000 DSP platform.	neural networks;psychoacoustics;real-time transcription	Eros Pasero;Alfonso Montuori	2002		10.1007/3-540-45808-5_18	arithmetic coding;wideband audio;speech recognition;decision support system;telecommunications;computer science;artificial intelligence;machine learning;speech coding;artificial neural network;algorithm	NLP	45.84505084349009	-11.319309863714391	80233
71ba207fd3cdba84e77c88417559f0b776f48cf6	predictive absolute-moment block truncation coding for image compression		Block Truncation Coding is one of the oldest known forms of image compression algorithms, its main attraction being its simple underlying concepts and ease of implementation. In this paper we present a new Predictive Absolute Moment Block Truncation Coding scheme which improves the performance of the existing Block Truncation Coding schemes. The proposed scheme is based on selectively predicting the reconstruction values of a block based on the corresponding values in the neighbouring blocks, as well as predicting the bitplane from the bitplanes of the corresponding blocks in other colour components.	algorithm;bit plane;block truncation coding;data compression;image compression	Sriram Subramanian;Anamitra Makur	2000				AI	41.73129339453486	-13.787525297509598	80240
da68b6cbcf78ed9fd99a673cb623d49d196351bb	an efficient motion strategy to compute expected-time locally optimal continuous search paths in known environments	optimal solution;path planning;probability density function;numerical optimization;pursuit evasion;optimization problem;search;timing optimization;random variable	In this paper we address the problem of finding time optimal search paths in known environments. In particular, we address the problem of searching a known environment for an object whose unknown location is characterized by a known probability density function (pdf). With this formulation, the time required to find the object is a random variable induced by the choice of search path together with the pdf for the object’s location. The optimization problem we consider is that of finding the path that minimizes the expected value of the time required to find the object. Because the complexity of the problem precludes finding an exact optimal solution, we propose a two-level, heuristic approach to finding the optimal search path. At the top level, we use a decomposition of the workspace based on critical curves to impose a qualitative structure on the solution trajectory. At the lower level, individual segments of this trajectory are refined using local numerical optimization methods. We have implemented the algorithm and present simulation results for the particular case when the object’s location is specified by the uniform pdf. keywords: Search, pursuit-evasion, path planning.	algorithm;evasion (network security);heuristic;mathematical optimization;motion planning;optimization problem;path (variable);portable document format;pursuit-evasion;simulation;workspace	Alejandro Sarmiento;Rafael Murrieta-Cid;Seth Hutchinson	2009	Advanced Robotics	10.1163/016918609X12496339799170	random variable;optimization problem;mathematical optimization;probability density function;combinatorics;random search;machine learning;mathematics;motion planning;random optimization;metaheuristic	Robotics	52.3264327663627	-23.0376508784988	80379
bbceebe1c9902d0f16eef893db60e55e85a338e6	blind image watermarking via exploitation of inter-block prediction and visibility threshold in dct domain	blind image watermarking;relative modulation;discrete cosine transform;backward propagation neural network;imperceptibility;inter block prediction;robustness;just noticeable difference	Inter-block relation in DCT domain is exploited to attain blind image watermarking.A neural network offers an appropriate prediction for relative comparison.The relation between a coefficient and its prediction leads to binary embedding.Embedding strength is adjusted subject to a just-noticeable difference model.The proposed scheme exhibits superior robustness and imperceptibility. In this paper, the backward-propagation neural network (BPNN) technique and just-noticeable difference (JND) model are incorporated into a block-wise discrete cosine transform (DCT)-based scheme to achieve effective blind image watermarking. To form a block structure in the DCT domain, we partition a host image into non-overlapped blocks of size 8×8 and then apply DCT to each block separately. By referring to certain DCT coefficients over a 3×3 grid of blocks, the BPNN can offer adequate predictions of designated coefficients inside the central block. The watermarking turns out to be a process of adjusting the relationship between the intended coefficients and their BPNN predictions subject to the JND. Experimental results show that the proposed scheme is able to withstand a variety of image processing attacks. Compared with two other schemes that also utilize inter-block correlations, the proposed one apparently exhibits superior robustness and imperceptibility under the same payload capacity.	digital watermarking;discrete cosine transform	Ling-Yuan Hsu;Hwai-Tsu Hu	2015	J. Visual Communication and Image Representation	10.1016/j.jvcir.2015.07.017	just-noticeable difference;speech recognition;computer science;theoretical computer science;discrete cosine transform;pattern recognition;mathematics;algorithm;robustness	Vision	40.70569313482158	-10.394954131546841	80584
762d3f4a3126e01733b0d06e27ab45a170ceb673	a message-based cocktail watermarking system	watermarking;fault tolerant;backpropagation neural network;hadamard code;spectrum;copyright protection;hamming distance;robustness;inexact matching;neural network	A noise-type Gaussian sequence is most commonly used as a watermark to claim ownership of media data. However, only a 1-bit information payload is carried in this type of watermark. For a logo-type watermark, the situation is better because it is visually recognizable and more information can be carried. However, since the sizes and shapes of logos for different organizations are different, the flexibility of use of a logo-type watermark will certainly be degraded. In this paper, a more flexible type of watermark, i.e., a message, is designed. Since a message is composed of a finite number of ASCII-type characters, it is by nature vulnerable to attacks. Therefore, we propose to choose a set of nonlinear Hadamard-codes that has the maximum Hamming distance between any two constituent codes to replace the original ASCII-type inputs. This design will make our system much more fault-tolerant in comparison with ASCII-code based systems under direct attack. To recover an attacked Hadamard code, we use a trained backpropagation neural network to perform inexact matching. The proposed Hadamard code-based message is embedded using the cocktail-watermarking scheme (IEEE Trans. Multimedia 2 (4) (2000) 209–224) for copyright protection. Experimental results demonstrate that our message-based cocktail watermarking system is superb in terms of robustness and flexibility.	message passing;watermark (data file)	Gwo-Jong Yu;Chun-Shien Lu;Hong-Yuan Mark Liao	2003	Pattern Recognition	10.1016/S0031-3203(02)00106-1	spectrum;fault tolerance;hamming distance;digital watermarking;computer science;theoretical computer science;machine learning;watermark;hadamard code;computer security;artificial neural network;algorithm;statistics;robustness	Vision	40.211583732832885	-10.389636119916027	80678
0cee8a3e5a2b90cfbe75897b0108af73f01d4cf1	robust quality-scalable transmission of jpeg2000 images over wireless channels using ldpc codes	desciframiento;red sin hilo;metodo adaptativo;vision ordenador;wireless channels;image processing;decodage;decoding;reseau sans fil;transmission error;controle parite;codage source;wireless network;procesamiento imagen;control paridad;methode adaptative;error transmision;probabilistic approach;qualite image;traitement image;computer vision;joint source channel coding;image transmission;enfoque probabilista;approche probabiliste;image quality;unequal error protection;adaptive method;ldpc code;jpeg2000;error resilience;vision ordinateur;calidad imagen;low density parity check;transmission image;semi random ldpc codes;erreur transmission;parity check;transmision imagen;source coding	A new error-resilient JPEG2000 wireless transmission scheme is proposed. The proposed scheme exploits the ‘progressive by quality’ structure of the JPEG2000 code-stream and takes into account the effect of channel errors at different quality layers in order to protect the coded bit-stream according to channel conditions using multi-rate low-density parity-check (LDPC) codes, leading to a flexible joint source-channel coding design. The novelty of this adaptive technique lies in its ability to truncate the less important source layers to accommodate optimal channel protection to more important ones to maximize received image quality. Results show that the proposed scheme facilitates considerable gains in terms of subjective and objective quality as well as decoding probability of the retrieved images.	algorithm;bitstream;channel capacity;cognitive dimensions of notations;computational complexity theory;distortion;experiment;forward error correction;image quality;iteration;jpeg 2000;low-density parity-check code;truncation	Abdullah Al Muhit;Teong Chee Chuah	2006		10.1007/11919476_4	computer vision;low-density parity-check code;telecommunications;image processing;computer science;computer security;statistics	Mobile	47.1203062616241	-13.691547274915553	80752
5ee6cbd6f1236d84bdedf99a77f5c687ab73bd07	conditional entropy coding of vq indexes for image compression	image sampling;rate distortion optimality;vq complexity;rate distortion;image coding;address vq;context modeling conditional entropy coding vq indexes image compression vector quantization source coding rate distortion optimality vq complexity high order sample correlations block sizes vq image coders address vq rate distortion performance lossless coding 16d vq codewords signal source memoryless vq codewords lossless compression;lossless compression;rate distortion theory image coding entropy codes vector quantisation source coding image sampling memoryless systems;signal source;vq image coders;memoryless vq codewords;context model;rate distortion theory;16d vq codewords;vector quantization;image compression;entropy coding image coding rate distortion block codes vector quantization source coding discrete cosine transforms transform coding mpeg standards bit rate;entropy codes;indexation;block sizes;rate distortion performance;conditional entropy coding;conditional entropy;lossless coding;vq indexes;source code;vector quantizer;memoryless systems;vector quantisation;rate distortion optimization;context modeling;high order sample correlations;source coding	Vector quantization (VQ) is a source coding methodology with provable rate-distortion optimality. However, despite more than two decades of intensive research, VQ theoretical promise is yet to be fully realized in image compression practice. Restricted by high VQ complexity in dimensions and due to high-order sample correlations in images, block sizes of practical VQ image coders are hardly large enough to achieve the rate-distortion optimality. Among the large number of VQ variants in the literature, a technique called address VQ (A-VQ) by Nasrabadi and Feng (1990) achieved the best rate-distortion performance so far to the best of our knowledge. The essence of A-VQ is to effectively increase VQ dimensions by a lossless coding of a group of 16-dimensional VQ codewords that are spatially adjacent. From a different perspective, we can consider a signal source that is coded by memoryless basic VQ to be just another signal source whose samples are the indices of the memoryless VQ codewords, and then induce the problem of lossless compression of the VQ-coded source. If the memoryless VQ is not rate-distortion optimal (often the case in practice), then there must exist hidden structures between the samples of VQ-coded source (VQ codewords). Therefore, an alternative way of approaching the rate-distortion optimality is to model and utilize these inter-codewords structures or correlations by context modeling and conditional entropy coding of VQ indexes.	conditional entropy;image compression;vector quantization	Xiaolin Wu;Jiang Wen;Wing Hung Wong	1997		10.1109/DCC.1997.582058	speech recognition;computer science;theoretical computer science;pattern recognition;mathematics;context model;statistics;source code	ML	47.95979624797485	-12.131093439581782	80819
0865acb900aefa2a7659cc80b679475af3f70e3b	a reversible color transform for 16-bit-color picture coding	hicolor image;coding efficiency;multimedia application;mobile phone;video coding;picture coding;16 bit color;reversible color transform;low end	This paper proposes a reversible color transform for 16-bit-color (hicolor) picture coding. The work is motivated by the increasing needs of multimedia applications on low-end devices such as mobile phones and PDAs. They have limited resources and up to 16-bit displays. Current image/video coding systems can hardly manage this case effectively. To enhance coding efficiency on this condition, a reversible color transform customized for hicolor systems is derived from Y'CrCb and JPEG2000 Reversible Component Transformation (RCT). The transform proves simple but highly-decorrelating, and able to reduce the computation time of decoding. Comparison experiment demonstrates the effectiveness of this transform with equal or even higher coding efficiency on low-end devices with 16-bit display mode.	16-bit;algorithmic efficiency;color management;computation;computer display standard;data compression;decorrelation;high color;jpeg 2000;mobile phone;personal digital assistant;picture-in-picture;time complexity	Na Li;Jiajun Bu;Chun Chen	2004		10.1145/1027527.1027607	sub-band coding;computer vision;transform coding;computer science;context-adaptive variable-length coding;high color;multimedia;algorithmic efficiency;context-adaptive binary arithmetic coding;computer graphics (images)	HCI	43.17898957290903	-17.775353172121804	80941
097d369ce9abfbd0899fe964ed141058660eadbc	robust message authentication code algorithm for digital audio recordings	hachage;message authentication code;signal audio;localization;audio signal;registro numerico;localizacion;algorithme;algorithm;sound recording;methode fingerprint;hashing;evaluation subjective;localisation;enregistrement son;digital recording;senal numerica;metodo fingerprint;robustesse;registro sonido;enregistrement numerique;authentification message;fingerprint method;signal numerique;signal acoustique;robustness;acoustic signal;message authentication;digital signal;analisis semantico;analyse semantique;subjective evaluation;human perception;senal acustica;senal audio;semantic analysis;robustez;evaluacion subjetiva;algoritmo	Current systems and protocols for integrity and authenticity verification of media data do not distinguish between legitimate signal transformation and malicious tampering that manipulates the content. Furthermore, they usually provide no localization or assessment of the relevance of such manipulations with respect to human perception or semantics. We present an algorithm for a robust message authentication code (RMAC) to verify the integrity of audio recodings by means of robust audio fingerprinting and robust perceptual hashing. Experimental results show that the proposed algorithm provides both a high level of distinction between perceptually different audio data and a high robustness against signal transformations that do not change the perceived information.		Sascha Zmudzinski;Martin Steinebach	2007		10.1117/12.704539	message authentication code;speech recognition;telecommunications;computer science	HCI	43.61013216909015	-10.346495535544687	81039
05772a199871f6c5c132ab52d427d8f0aa6d162a	low-complexity video compression for wireless sensor networks	compression algorithm;change detection;tms320vc5204 board low complexity video compression wireless sensor networks videosurveillance compression jpeg like compression low complexity change detection algorithm mpeg 2 coder design;data compression;video compression;low complexity;wireless sensor network;video coding;computational complexity;region of interest;video compression wireless sensor networks detection algorithms monitoring delay costs clustering algorithms multimedia communication radio communication sequences;digital signal processing chips;computational complexity video coding wireless sensor networks data compression digital signal processing chips image sequences;wireless sensor networks;image sequences	We study the problem of compression of videosurveillance sequences collected by a wireless sensor network. In particular, we propose a low-complexity coding framework based on change detection and JPEG-like compression of regions of interest, along with a suitable low-complexity change detection algorithm. We show that on typical videosurveillance sequences the performance of the proposed compression algorithm is similar to that of MPEG2, at a much less computational cost. Energy profiling results on a TMS320VC5204 board validate the coder design for the proposed application.	algorithm;algorithmic efficiency;data compression;jpeg;mpeg-2;region of interest	Enrico Magli;Massimo Mancin;Luca Merello	2003		10.1109/ICME.2003.1221379	video compression picture types;data compression;image compression;computer science;theoretical computer science;context-adaptive binary arithmetic coding;statistics;computer network;multiview video coding	Mobile	48.954230330834356	-17.413920186665916	81248
34ed62ecc05e4996daf48f0c3e97868cff896d16	classified region algorithm for fast intermode decision in h.264/avc encoder	signal image and speech processing;quantum information technology spintronics	H.264, MPEG-4 Part 10, is the latest digital video coding standard that achieves very high data compression by using several new coding features. One of the new features is variable block sizes for interframe coding to increase compression efficiency. However, to achieve this, the H.264 encoder employs a complex mode decision technique based on rate-distortion optimization (RDO) that requires high computational complexity, which significantly increases the encoder complexity. In this paper, we propose a classified region algorithm (CRA) that analyzes the spatial and temporal homogeneity of the block by using cross differences to reduce the number of modes that are required for RDO calculation in inter mode decision. The proposed low computational complexity algorithm significantly reduces the number of inter modes without affecting the video quality. The experimental results show that the proposed method is able to reduce complexity by up to 67% on average with negligible degradation in both objective and subjective quality.	algorithm;arithmetic coding;computational complexity theory;credit bureau;data compression;digital video;distortion;elegant degradation;encoder;h.264/mpeg-4 avc;mathematical optimization;preprocessor;raster document object;rate–distortion optimization;remote data objects;simulation;video coding format	K. Bharanitharan;Bin-Da Liu;Jar-Ferr Yang	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/150809	real-time computing;telecommunications;computer science;theoretical computer science	EDA	46.37367965310417	-17.857934540062374	81504
188fc22453b51183904e24b7d3222e1fafdfb279	motion vector size-compensation based method for very low bit-rate video coding	traitement signal;evaluation performance;compensacion;motion compensation video coding data compression video codecs;estimation mouvement;performance evaluation;image processing;data compression;motion compensation;cost function;macroblock level;helium;evaluacion prestacion;estimacion movimiento;motion vector size compensation based method;video compression;procesamiento imagen;motion estimation;video coding cost function video compression current measurement testing transform coding motion compensation video codecs bandwidth;testing;transform coding;indexing terms;funcion coste;traitement image;video codec;motion compensated;very low bit rate video coding;headers size compensation;video coding;codificacion;global bit rate reduction;compensation;current measurement;senal video;signal video;motion vector;signal processing;coding;video signal;bandwidth;fonction cout;video codecs;coding modes;compresion dato;h 263 codec motion vector size compensation based method very low bit rate video coding compression efficiency global bit rate reduction macroblock level headers size compensation coding modes cost function video compression low quality video coding;compression efficiency;h 263 codec;procesamiento senal;compression donnee;low quality video coding;codage	In this paper, a new method to achieve better compression efficiency in low bit-rate video coding is proposed. It is based on a global bit-rate reduction at a macroblock level, optimizing the number of bits to code each macroblock as a whole by means of motion vector and headers size compensation. The selection of the best motion vector and different coding modes for each block of the current picture will be made depending not only on trying to choose the best prediction for the block, but also on the number of bits to code the associate headers, introducing some kind of penalization in the cost function. This method improves efficiency on video compression for all qualities, but especially for low-quality video coding, whose efficiency improvement can reach 17%. Its implementation is simple, and compatible with most video-compression standards (H.263, MPEG, etc.). Results of the algorithm in a state-of-the-art H.263+ codec are presented, and demonstrate that the efficiency enhancement is achieved with minimal time-processing increase, and even decrease, in some conditions.	algorithm;bernard greenberg;codec;data compression;digital video;integrated services digital network;interpolation;least squares;list of itu-t v-series recommendations;loss function;mag technology co.;macroblock;motion estimation;moving picture experts group;penalty method;signal processing;videotelephony	Jorge Sastre;Antonio Ferreras;José Félix Hernández-Gil	2000	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.875523	data compression;computer vision;telecommunications;image processing;quarter-pixel motion;computer science;signal processing;coding tree unit;block-matching algorithm;context-adaptive binary arithmetic coding;macroblock;motion compensation;statistics	Vision	46.54177407190308	-15.592852582436889	81617
29b0cc583c7f7feddfd4785a3288f96871049ebd	screen content coding using non-square intra block copy for hevc	video codec screen content coding nonsquare intra block copy hevc intra block copy intrabc mode block matching unit of square coding unit nonsquare prediction unit partition syntax design;non square partition;hevc screen content intra block copy non square partition compound image coding;hevc;screen content;compound image coding;proceedings paper;video coding video codecs;encoding vectors video codecs video coding standards bandwidth syntactics;intra block copy	To achieve high coding performance for screen content, the intra block copy (IntraBC) performs block matching within a limited area of the reconstructed samples inside the current picture. We further extend its notion from the unit of square coding unit (CU) to non-square prediction unit (PU) partitions. This design is then justified by theoretical and empirical analyses which reveal the same fact that blocks coded by IntraBC mode tend to enable more at smaller partition levels. Besides, the syntax design of the proposed method is fully aligned with that of inter partition modes. Therefore the architecture-wise change in video codec design can be minimized. The experimental results justify the effectiveness of the proposed mode for coding of screen content video. In particular, up to 19.5% rate reduction (with an average of 12.7%) relative to the HM-12.0+RExt-4.1 anchor can be achieved on top of the usage of CU-based IntraBC prediction.	codec;high efficiency video coding;one-class classification;tip (unix utility)	Chun-Chi Chen;Xiaozhong Xu;Ru-Ling Liao;Wen-Hsiao Peng;Shan Liu;Shawmin Lei	2014	2014 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2014.6890229	real-time computing;telecommunications;theoretical computer science;coding tree unit;multiview video coding	EDA	45.22025368526786	-18.753002756879894	81861
81cd131705c9cca320d2c11e70c8d28b757e55fa	no-reference video quality assessment using mpeg analysis	video coding analysis methods no reference video quality assessment mpeg analysis nr video quality assessment vqa video decoding nr image quality assessment method video coding parameters h 264 avc mpeg 2;video coding;no reference video quality assessment mpeg 2 analysis h 264 avc analysis quantization estimation;transform coding video coding quantization signal estimation psnr quality assessment databases	We present a method for No-Reference (NR) Video Quality Assessment (VQA) for decoded video without access to the bitstream. This is achieved by extracting and pooling features from a NR image quality assessment method used frame by frame. We also present methods to identify the video coding and estimate the video coding parameters for MPEG-2 and H.264/AVC which can be used to improve the VQA. The analysis differs from most other video coding analysis methods since it is without access to the bitstream. The results show that our proposed method is competitive with other recent NR VQA methods for MPEG-2 and H.264/AVC.	bitstream;codec;complexity;data compression;deblocking filter;frame language;h.264/mpeg-4 avc;image quality;intra-frame coding;mpeg-2;moving picture experts group;noise reduction;numerical recipes	Jacob Søgaard;Søren Forchhammer;Jari Korhonen	2013	2013 Picture Coding Symposium (PCS)	10.1109/PCS.2013.6737708	video compression picture types;scalable video coding;computer vision;telecommunications;video quality;coding tree unit;block-matching algorithm;multimedia;rate–distortion optimization;context-adaptive binary arithmetic coding;motion compensation;h.262/mpeg-2 part 2;pevq;h.261;multiview video coding	Vision	45.0898786764791	-18.763618624024186	81900
8b5f609adf9861c52c8d047e4aecd53522250282	post-processing of coded images by neural network cancellation of the unmasked noise	institutional repositories;fedora;image restoration;vital;lts3;human visual system;multi layer perceptron;network structure;vtls;subband coding;ils;neural network	This paper presents a new application of neural networks for the post-processing of coded images. It is based on a model of the human visual system. The image affected by coding noise is decomposed into perceptual channel components. The image restoration stage is realized by filtering the perceptual components of the channels for which the noise power is not masked by the image power. This operation, referred as cancellation of the unmasked noise, is performed using a multi-layer perceptron (MLP) network. Different network structures have been considered for this purpose. Simulation results of the processing scheme show significant improvements in both visual and objective (SNR) quality for post-processed images affected by DCT or subband coding noise.	artificial neural network;circuit restoration;discrete cosine transform;image restoration;layer (electronics);memory-level parallelism;multilayer perceptron;noise power;signal-to-noise ratio;simulation;sub-band coding;video post-processing	Marco Mattavelli;Olivier Bruyndonckx;Serge Comes;Benoit M. Macq	1995	Neural Processing Letters	10.1007/BF02312351	sub-band coding;image restoration;computer vision;speech recognition;computer science;machine learning;multilayer perceptron;human visual system model;artificial neural network	ML	46.80653270432597	-11.476649049309287	81950
7457647f7e5715167660aa30d2283675adcc2a03	a blind source separation based multi-bit digital audio watermarking scheme	filigranage numerique;digital watermarking;modelizacion;correlacion;frecuencia audible;cle secrete;frequence audible;separacion ciega;signal audio;blind source separation;audio signal;modelisation;blind separation;secret key;clave secreta;senal numerica;filigrana digital;separacion senal;separation aveugle;audiofrequency;signal numerique;signal acoustique;separation source;acoustic signal;digital signal;correlation;reseau neuronal;source separation;modeling;red neuronal;audio acoustics;senal acustica;senal audio;neural network;acoustique audio;audio watermarking	A multi-bit digital audio watermarking scheme based on blind source separation (BSS) is proposed in this paper. The host audio signal is first permuted and divided into frames equally, and the watermark embedding and extraction procedure are executed frame by frame. In embedding procedure, the embedding watermark signal is generated firstly, and then mixed together with the permuted host audio signal through a BSS model. After that, the obtained mixtures are combined and inverse permuted to form the watermarked audio signal. In extraction procedure, watermarked audio signal is permuted and split into two signals, and then BSS technique is applied to obtain the separated embedding watermark signal, by observing the correlation values between it and the embedding watermark signal, the existence of watermark can be determined. With the secret keys, we can recover the watermark image further more. Experimental results show the effectiveness of the proposed method.	blind signal separation;source separation	Xiaohong Ma;Xiaoyan Ding;Chong Wang;Fuliang Yin	2006		10.1007/11760191_45	speech recognition;systems modeling;telecommunications;digital watermarking;digital signal;computer science;machine learning;audio signal;blind signal separation;watermark;correlation;artificial neural network	AI	43.89370005751079	-10.434638788435802	82044
4f10f1757d7fe73039bd10e2751f468f9cf9832a	a wavelet packet image coding algorithm based on quadtree classification and utcq	libraries;decoding algorithm;quantization;basis algorithms;universal trellis coded quantization;image coding;cost function;trellis codes image coding quadtrees wavelet transforms image classification image reconstruction;image classification;entropy coder;wavelet packets image coding entropy wavelet transforms basis algorithms cost function quantization image reconstruction wavelet coefficients libraries;utcq;trellis coded quantization;wavelet packet;decoding algorithm wavelet packet image coding algorithm quadtree classification utcq wavelet packet decomposition cost function wavelet packet coefficients universal trellis coded quantization entropy coder;wavelet transforms;image reconstruction;wavelet packet decomposition;quadtree classification;wavelet packet image coding algorithm;entropy;trellis codes;wavelet packets;quadtrees;wavelet coefficients;wavelet packet coefficients	In this paper, we present a wavelet packet image coding algorithm based on quadtree classification and UTCQ. It is composed of four parts: (1) wavelet packet decomposition and best basis selection based on a new cost function, (2) a quadtree classification procedure, used to classify the wavelet packet coefficients into two sets: a significant one and an insignificant one, (3) the universal trellis coded quantization, used to code the significant coefficients sets, (4) the entropy coder, used to code the indices of the universal trellis coded quantizer output. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm are either comparable to or surpass previous results for texture-rich images.	algorithm;coefficient;color quantization;entropy encoding;loss function;network packet;quadtree;quantization (signal processing);trellis quantization;wavelet packet decomposition	Xingsong Hou;Guizhong Liu	2003		10.1109/ICME.2003.1221676	computer vision;discrete mathematics;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;wavelet packet decomposition	ML	44.485437138361746	-13.993989971921831	82113
c7d8c0ab2bd3e8d11df1c1425f6670c3c69ff47a	online handwriting recognition of tamil script using fractal geometry	fractals encoding accuracy image reconstruction character recognition image coding handwriting recognition;fractals;online handwritten character recognition;tamil handwriting recognition fractal geometry online handwritten character recognition online handwriting recognition fractal coding tamil character recognition;image coding;handwriting recognition;data compression;image matching;fractal geometry;image classification;image reconstruction data compression handwriting recognition handwritten character recognition image classification image coding image matching;online handwriting recognition;tamil handwriting recognition;accuracy;tamil character recognition;image reconstruction;fractal coding;dynamic time warping online handwriting recognition tamil script fractal geometry fractal coding method online handwritten tamil character recognition data redundancy character compression character reconstruction character classification fractal decoding dtw nearest neighbor classifier;electrical engineering;encoding;character recognition;handwritten character recognition	We present a fractal coding method to recognize online handwritten Tamil characters and propose a novel technique to increase the efficiency in terms of time while coding and decoding. This technique exploits the redundancy in data, thereby achieving better compression and usage of lesser memory. It also reduces the encoding time and causes little distortion during reconstruction. Experiments have been conducted to use these fractal codes to classify the online handwritten Tamil characters from the IWFHR 2006 competition dataset. In one approach, we use fractal coding and decoding process. A recognition accuracy of 90% has been achieved by using DTW for distortion evaluation during classification and encoding processes as compared to 78% using nearest neighbor classifier. In other experiments, we use the fractal code, fractal dimensions and features derived from fractal codes as features in separate classifiers. While the fractal code is successful as a feature, the other two features are not able to capture the wide within-class variations.	code;distortion;experiment;fractal dimension;handwriting recognition;mike lesser;nearest neighbour algorithm;redundancy (engineering)	Rituraj Kunwar;A. G. Ramakrishnan	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.279	computer vision;speech recognition;fractal;computer science;pattern recognition;handwriting recognition;fractal compression;statistics	Robotics	42.91592930991118	-13.200487231117885	82116
9da8b719acde33835f093eb305c6c56c4e0f6b8c	factoring a mobile client's effective processing speeed into the image transcoding decision	proxy;mobile;image processing;partitioning;pda;cpu;processing speed;transcoding	An image transcoding proxy decides whether to transcode an image fetched from the Web based either on the criterion of reducing the overall response time (store-and-forward proxies), or the criterion of avoiding buffer overflow (streamed proxies). In this paper, we introduce a new parameter, namely a mobile client’s effective processing speed, into the analytical formulation of both transcoding decisions, and study the practical importance of this parameter when transcoding is applied for standard PDA clients. CPU-intensive operations like image decompression, colorspace conversion and scaling can together add excessive delay to the perceived response time when performed on a mobile client that has severely limited processing capability. Under certain conditions, a transcoding proxy that sends GIF or JPEG images can incur greater delay due to decompression on a PDA client than a better informed transcoding proxy that chooses instead to send bit-mapped equivalents that incur little to no client-side decoding delay. We designed three experiments that partitioned image processing functions between a proxy and a standard PDA in order to assess the importance of a client’s CPU limitations on the image transcoding decision. First, images were fetched by a standard Web browser that decompressed GIF’s on the PDA. This browser also enforced scaling on every image to fit within the PDA’s small screen. Second, we added a transcoding proxy that pre-scaled images, thereby bypassing the browser’s scaling function but still requiring the browser to decompress the scaled GIF’s. Third, we migrated both scaling and decompression off of the PDA on to the proxy. The proxy pre-scaled as before, and also transcoded GIF’s to grayscale bitmaps that required no client-side decompression. We measured response times from each of these three experiments and quantified how much improvement in response time can be achieved when a proxy assists a CPU-limited PDA by performing some or all CPU-intensive image processing tasks on the transcoding proxy.	bitmap;buffer overflow;central processing unit;client-side;color space;data compression;experiment;gif;grayscale;image compression;image processing;image scaling;integer factorization;jpeg;modem;personal digital assistant;proxy server;response time (technology);social inequality;store and forward;streaming media;television;wavelet;world wide web	Richard Han	1999		10.1145/313256.313285	proxy;real-time computing;transcoding;computer hardware;image processing;computer science;operating system;central processing unit;mobile technology;world wide web;computer network	Web+IR	41.17369796105497	-22.41711666611238	82248
6a4aac0f99b74b57978e0eb4aae4fb4040379d03	a fast algorithm for adaptive motion compensation precision in screen content coding	computers;motion compensation;decoding;color;encoding motion compensation color decoding video coding computers correlation;video coding motion compensation;video coding;hash based block matching fast algorithm adaptive motion compensation precision screen content coding fractional pel motion compensation video coding efficiency camera captured content computer desktop motion vectors integer precision multipass encoding;correlation;encoding	Fractional-pel motion compensation is very good at improving video coding efficiency, especially for camera-captured content. But for screen content, which is obtained from a computer desktop, motion vectors with integer-precision may be enough to represent the motion in different pictures. Using fractional-pel motion compensation for such content is a waste of bits. Thus, adaptive motion compensation precision is helpful for improving coding efficiency, especially for screen content coding. Usually, to select suitable motion compensation precision, multi-pass encoding is introduced, which significantly increases the encoding time. This paper presents a fast encoding algorithm for adaptive motion compensation precision used in screen content coding by hash-based block matching. With the proposed method, multi-pass encoding is avoided and most of the benefits brought by adaptive motion compensation precision are preserved. The experimental results show that with the proposed method, up to 7.7% bit saving is obtained without a significant impact on encoding time.	algorithm;algorithmic efficiency;data compression;desktop computer;motion compensation;pixel	Bin Li;Jizheng Xu	2015	2015 Data Compression Conference	10.1109/DCC.2015.17	computer vision;quarter-pixel motion;computer science;motion estimation;mathematics;multimedia;context-adaptive binary arithmetic coding;motion compensation;correlation;encoding;computer graphics (images)	Vision	45.344976696721055	-18.25643412403349	82268
3a4abcd4454a1a2d2c69845bf895eca76dbc7da9	phase-shifting for nonseparable 2-d haar wavelets	2 d haar transform;nonseparable 2d haar wavelet phase shifting;funcion haar;evaluation performance;transformation haar;interpolation;image coding;performance evaluation;haar wavelet;image processing;fonction haar;complexite calcul;phase shifting;defasaje;ondelette;signal design;evaluacion prestacion;signal analysis;haar function;interpolacion;analisis de senal;phase shift;dephasage;image coding wavelet transforms interpolation equations wavelet coefficients computational complexity image processing redundancy signal design computer science;indexing terms;wavelets nonseparable phase shifting 2 d haar transform;wavelet transforms;complejidad computacion;shift invariance;redundancy;computational complexity;wavelet transforms haar transforms image processing;invariance par decalage;computer science;image processing nonseparable 2d haar wavelet phase shifting shift invariance;nonseparable;haar transforms;wavelets;shift invariant;wavelet coefficients;analyse signal;algorithms image enhancement image interpretation computer assisted reproducibility of results sensitivity and specificity signal processing computer assisted	In this paper, we present a novel and efficient solution to phase-shifting 2-D nonseparable Haar wavelet coefficients. While other methods either modify existing wavelets or introduce new ones to handle the lack of shift-invariance, we derive the explicit relationships between the coefficients of the shifted signal and those of the unshifted one. We then establish their computational complexity, and compare and demonstrate the superior performance of the proposed approach against classical interpolation tools in terms of accumulation of errors under successive shifting.	coefficient;computation;computational complexity theory;haar wavelet;interpolation imputation technique;propagation of uncertainty;tree accumulation	Mais Alnasser;Hassan Foroosh	2008	IEEE Transactions on Image Processing	10.1109/TIP.2008.919950	computer vision;mathematical analysis;speech recognition;image processing;interpolation;computer science;mathematics;phase;discrete wavelet transform;statistics	Visualization	45.67054758903924	-15.059300787497182	82399
77f0cbaaa747ee1163ed18f0ff6354491352ef60	a mathematical model for shape coding with b-splines	second order;analisis imagen;video object;concepcion asistida;computer aided design;algorithm performance;modele mathematique;image processing;aproximacion capa limite;data compression;chain code;edge detection;signal distortion;simulacion numerica;procesamiento imagen;orden 2;arbitrary shaped body;distorsion signal;modelo matematico;codigo bloque;traitement image;b spline curve;experimental result;deteccion contorno;detection contour;video coding;codificacion;3d model;senal video;signal video;object oriented;resultado algoritmo;simulation numerique;coding;performance algorithme;resultado experimental;mathematical model;pattern recognition;conception assistee;video signal;oriente objet;image analysis;code bloc;b spline;corps forme arbitraire;compresion dato;reconnaissance forme;ordre 2;reconocimiento patron;cuerpo forma arbitraria;approximation couche limite;resultat experimental;analyse image;orientado objeto;block code;b splin;compression donnee;codage;boundary layer approximation;numerical simulation;distorsion senal	"""A major problem in object-oriented video coding is the e$cient encoding of the shape information of arbitrarily shaped objects. E$cient shape coding schemes are also needed in encoding the shape information of video object (VO) in the upcoming MPEG-4 standard. Furthermore, there are many applications where only the shape needs to be encoded, such as CAD, 3D modeling and signature encoding. In this paper, we present an e$cient method for the lossy encoding of object shapes which are given as 8-connect chain codes using a mathematical model. We approximate a boundary by a second-order B-spline curve and consider the problem of """"nding the curve with the lowest bit-rate for a given distortion. The presented scheme is optimal, e$cient and o!ers complete control over the trade-o! between bit-rate and distortion. It is an extension of our previous research where we used polygons to approximate a boundary. The main reason for using curves rather than polygons is that curves have a more natural appearance than polygons and can give better coding e$cencies. We present results of the proposed scheme using objects boundaries in di!erent shapes and sizes as well as an MPEG-4 test sequence. ( 2000 Elsevier Science B.V. All rights reserved."""	3d modeling;approximation algorithm;b-spline;code;computer-aided design;data compression;distortion;lossy compression;mathematical model;spline (mathematics)	Fabian W. Meier;Guido M. Schuster;Aggelos K. Katsaggelos	2000	Sig. Proc.: Image Comm.	10.1016/S0923-5965(99)00045-4	data compression;block code;b-spline;computer vision;image analysis;simulation;edge detection;image processing;computer science;mathematical model;mathematics;chain code;coding;object-oriented programming;second-order logic;algorithm;statistics	Vision	46.683516423943665	-14.210333415371807	82537
f9a5c86c155aedae47f9e1f9b55632881cae2e70	a secret image sharing scheme based on vector quantization mechanism	desciframiento;distributed system;partage secret;ombre;systeme reparti;calculateur embarque;secret sharing;decodage;decoding;pervasive computing;localization;threshold scheme;localizacion;codigo bloque;n threshold scheme;qualite image;permutation;informatica difusa;reconstruction image;cuantificacion vectorial;sistema repartido;sombra;localisation;vector quantization;reconstruccion imagen;shadow;informatique diffuse;image reconstruction;image quality;indexation;permutacion;t;boarded computer;image sharing;code bloc;calidad imagen;vector quantizer;block code;calculador embarque;reparto secreto;quantification vectorielle	In this paper, we proposed an image sharing method adopting (t, n) threshold scheme. The secret image goes through Vector Quantization (VQ) scheme and permutes the locations of indices to obtain permuted indexed secret image. We select t indices to form a (t-1)-degree polynomial by taking t indices as its coefficients. Then, the values of blocks’ indices in cover image are fed into the (t-1)-degree polynomial to get the output values. After embedding the output values back to the pixels in block, we can obtain meaningful shadow images. Any t out of n shadow images can reconstruct the permuted indexed secret image. By inversing permutation and decoding VQ indices, we can finally reconstruct the secret image. Experimental results show that our method can get high image quality for the resultant shadow images and reconstruct secret images as well.	coefficient;display resolution;existential quantification;image quality;pixel;polynomial;pseudorandom permutation;resultant;secret sharing;shadow volume;system image;vector quantization	Chin-Chen Chang;Chi-Shiang Chan;Yi-Hsuan Fan	2006		10.1007/11802167_48	iterative reconstruction;image quality;block code;shadow;discrete mathematics;internationalization and localization;computer science;theoretical computer science;mathematics;permutation;secret sharing;ubiquitous computing;vector quantization;algorithm;statistics	Vision	44.32387352631906	-11.901721482030348	83142
824c921b3b0a4359dae4caa34a285e56ea91cfc5	high throughput image codec for high-resolution satellite images		The growth in the use of satellite images has generated the need for their fast compression, processing, and distribution. JPEG2000 is a widespread standard for the compression and transmission of such images once they are in the ground. Despite its advanced features and excellent coding performance, JPEG2000 demands significant computational resources. This paper introduces a wavelet-based codec that uses the JPEG2000 framework, but replaces its most computationally demanding coding stage by a highly parallel engine. When executed in Graphics Processing Units to code high-resolution satellite images, the proposed codec achieves speed-ups of up to $8\times$ when compared to the fastest implementation of JPEG2000 executed in a multi-core platform.	codec;computational resource;data compression;fastest;graphics processing unit;image resolution;jpeg 2000;multi-core processor;throughput;wavelet	Carlos de Cea-Dominguez;Pablo Enfedaque;Juan C. Moure;Joan Bartrina-Rapesta;Francese Auli-Llina	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518919	computer vision;throughput;wavelet;satellite;artificial intelligence;transform coding;codec;jpeg 2000;instruction set;computer science;graphics	Arch	42.410624903768074	-17.252669508382223	83163
aa0b551834ad918c61473d27a59c45525d4b20cc	image compression using an enhanced self organizing map algorithm with vigilance parameter	image coding;data compression;activation function;self organising feature maps computational complexity data compression image coding;image compression;self organising feature maps;computational complexity;computational complexity image compression self organizing map algorithm vigilance parameter activation functions pattern distribution performance enhancement;image coding organizing clustering algorithms neurons computational complexity testing image processing color lattices process control;self organized map	In this paper, a new approach for image compression is presented. The enhanced SOM algorithm applies a vigilance parameter in order to test if the maximum value of all activation functions in each training step exceeds the minimum threshold. If vigilance test is not passed, a new cluster will be added to the network; else the winner cluster will be updated. Therefore, the network could be extendable due to pattern distribution. The proposed approach is compared with ART1. The performance of enhanced SOM algorithm does not depend on the input presentation order. As observed through simulations, the new algorithm with vigilance parameter reduces the computational complexity, and presents better quality compared with Kohonen's SOM.	activation function;algorithm;cluster analysis;computation;computational complexity theory;extensibility;image compression;network performance;self-organizing map;simulation	Elham Bavafa;Mohammad Javad Yazdanpanah	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246787	data compression;computer vision;image compression;computer science;theoretical computer science;machine learning;activation function;computational complexity theory	Robotics	44.03543513495902	-13.60901356747298	83197
9733e52a07c7fa83ad19a5e1b2488d32aec6c735	a high-speed pyramid image coding algorithm for a vlsi implementation	digital signal processing;pulse code modulation;image coding;additions;decoding;very large scale integration;efficient algorithm;speech processing;finite impulse response filter;picture processing;very large scale integrated;compression rates;vlsi digital signal processing chips encoding picture processing;nonseparable pyramid method;peak signal to noise ratio;pixel;digital filters;multidimensional signal processing;pulse code modulation pcm image coding algorithm nonseparable pyramid method wavelet expansion compression rates additions shift image pixel decoding vlsi signal to noise ratio entropy;vlsi;wavelet expansion;digital signal processing chips;image coding algorithm;circuits;entropy;shift;signal to noise ratio;signal processing algorithms;pcm;encoding;image pixel;high speed;image coding very large scale integration signal processing algorithms digital filters finite impulse response filter multidimensional signal processing speech processing circuits pixel digital signal processing	A nonseparable pyramid method based on a wavelet expansion is developed for image coding. It achieves high compression rates and at the same time allows a very efficient algorithmic implementation. In particular it uses only two additions and a shift (division by 2) for each image pixel during coding or decoding. Because the operations needed are mostly independent of each other and have a high degree of regularity, it is also possible to design very-large-scale integration (VLSI) hardware to perform this operation using an array of simple basic cells. For a typical 513*513 image one can achieve a peak signal-to-noise ratio of 30 dB with an entropy of 0.133 b/pixel. Transmitted coefficient values can be encoded with pulse code modulation to allow for simpler hardware while still requiring only 0.385 b/pixel, giving a very simple overall coding system. >	algorithm;very-large-scale integration	Haralambos Sahinoglou;Sergio D. Cabrera	1991	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.120776	pulse-code modulation;computer vision;computer science;electrical engineering;theoretical computer science;speech processing;very-large-scale integration	EDA	46.395212978085006	-11.424671561833485	83225
7978d5223dcef649661a4790deef8e0848cc1a37	mode-correlation-based early termination mode decision for multi-view video coding	early termination;multi view video coding;rate distortion;adaptive thresholding;coding efficiency;skip mode;psnr;mode correlation based early termination;early termination chance mode correlation early termination mode decision multiview video coding exhaustive mode decision coding efficiency computational complexity fast mode decision mode correlation based early termination macroblock rate distortion cost skip mode adaptive threshold;adaptive threshold;early termination mode decision;mode correlation;bit rate;multiview video coding;video coding;computational complexity;early termination chance;peak signal to noise ratio;video coding computational complexity;fast mode decision;correlation;rate distortion cost;jmvm;signal processing algorithms;exhaustive mode decision;encoding;mode decision;jmvm multi view video coding mode decision early termination mode correlation;correlation video coding encoding psnr bit rate computational complexity signal processing algorithms;macroblock	Exhaustive mode decision is exploited in multi-view video coding for effectively improving the coding efficiency, but at the expense of yielding higher computational complexity. In this paper, a fast mode decision algorithm, called the mode-correlation-based early termination (MET), is proposed. For each macroblock, the proposed MET algorithm always starts with checking whether the rate-distortion (RD) cost computed at the SKIP mode is below an adaptive threshold for providing a possible early termination chance. This adaptive threshold is calculated by using the mode correlation between the current macroblock and a set of adjacent macroblocks in the current view and its neighboring view. Experimental results have shown that compared with exhaustive mode decision, which is a default approach set in the JMVM reference software, the proposed MET algorithm achieves a reduction of the computational complexity by 65.91% and the total bit rate by 0.98% on average, while incurring only 0.06 dB loss in peak signal-to-noise ratio (PSNR).	algorithm;algorithmic efficiency;computational complexity theory;data compression;distortion;macroblock;peak signal-to-noise ratio;ruby document format	Huanqiang Zeng;Kai-Kuang Ma;Canhui Cai	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653326	computer vision;real-time computing;peak signal-to-noise ratio;computer science;theoretical computer science;coding tree unit;mathematics;algorithm	Robotics	46.914266964347654	-19.089344165871932	83529
5ccb54c61d846c0b83481f2f6f1a5c78cce636a1	improved set-partitioning embedded block scheme for still image compression	set partitions;wavelet transforms;computer programming;image compression;algorithms;wavelets	In the last few years, wavelet-based still image compres- sion schemes have become one of the most popular fields of re- search. Several powerful methods of encoding the wavelet coeffi- cients have been proposed and set-partitioning embedded block (SPECK) is one of the most popular techniques. We propose a method called HSPECK (Huffman SPECK), which is based on SPECK to further improve the compression ratio. By observation, we are able to establish the probability distribution over the signifi- cance pattern of four subblocks generated successively along the wavelet decomposition process. Therefore, the Huffman coding scheme can then be employed for coding wavelet coefficients. The experimental results show that under the same peak signal-to-noise ratio, the bit rates via the proposed approach improve between 6.1% and 14.9%. © 2008 SPIE and IS&T. DOI: 10.1117/1.2952847	embedded system;image compression	Shu-Mei Guo;Bing-Hui Wu;Jason Sheng Hong Tsai	2008	J. Electronic Imaging	10.1117/1.2952847	discrete mathematics;image compression;computer science;theoretical computer science;computer programming;mathematics;wavelet packet decomposition;algorithm;wavelet transform	EDA	43.430456628483434	-14.5309893512948	83566
c666f6e598dbadfcd8b83094d5f9bc81182f810d	fast depth map intra coding based structure tensor data analysis		As a recent 3D video coding standard, ISO/IEC MPEG and ITU-T Video Coding Experts Group (VCEG) establish 3D-HEVC as the most efficient 3D video coding based on Multiview texture Videos plus Depth maps data format. In 3D-HEVC, depth map intra prediction is a key factor in 3D video coding, in which, the encoder utilizes the conventional intra prediction and depth modeling modes together to improve the depth map coding. This improvement of depth map intra prediction increase the coding efficiency significantly, but result in a dramatic computational complexity load, due to the exhaustive searching for the best intra mode. The increase of the intra coding complexity excludes the 3D-HEVC from real time and real world application. To resolve the aforementioned problem, it's imperative to develop solutions that can reduce the complexity meaningfully. In this work, we propose an efficient depth map intra prediction model decision based on tensor features. The simulation experiments prove that the developed model decreases the computational complexity (38.52%) with no performance losses.	depth map;structure tensor	Hamza Hamout;Abderrahmane Elyousfi	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451781	encoder;coding (social sciences);tensor;pattern recognition;structure tensor;artificial intelligence;computational complexity theory;stress (mechanics);algorithmic efficiency;depth map;computer science	Robotics	46.31082006291177	-19.648009781923438	83568
89c61573bcfef336b0759617a87348779382701c	low-complexity encoder framework for window-level rate control optimization	h 264 avc low complexity encoder framework window level rate control optimization visual quality smoothness buffer constraint open loop motion estimation;h 264 avc;telecommunication control;window level rate control optimization;motion estimation;window model;bit rate;buffer constraint;rate control;video coding;visualization;encoding bit rate streaming media visualization computational complexity algorithm design and analysis entropy;streaming media;computational complexity;low complexity encoder framework;window level rate control rate control video coding window model;video coding motion estimation telecommunication control;open loop motion estimation;entropy;window level rate control;visual quality smoothness;encoding;algorithm design and analysis	There exists a tradeoff between visual quality smoothness and buffer constraint in rate control. Thus, a window model and a window-level rate control algorithm were proposed to handle such a tradeoff recently. However, the extra computational complexity and encoding delay were introduced due to a pre-analysis process in the window-level rate control. In this paper, the window model is first improved for a more convenient usage in practical encodings. Second, a new low-complexity encoder framework is proposed based on an open-loop motion estimation (ME). Third, a new memory-efficient algorithm is proposed to eliminate the drift error of open-loop ME. Fourth, the window-level rate control algorithm is improved to adapt to the proposed encoder framework. The experimental results show that the visual quality smoothness is much improved by the proposed algorithm against the traditional one of H.264/AVC. In addition, the improvement of the window-level rate control is better than the previous one in terms of both bit control accuracy and visual quality smoothness. Besides, the proposed encoder could be used to any other rate control algorithms with pre-analysis to reduce computational complexity.	algorithm;computational complexity theory;encoder;h.264/mpeg-4 avc;motion estimation	Long Xu;Sam Kwong;Yun Zhang;Debin Zhao	2013	IEEE Transactions on Industrial Electronics	10.1109/TIE.2012.2190960	algorithm design;entropy;real-time computing;simulation;visualization;computer science;theoretical computer science;motion estimation;computational complexity theory;encoding	Robotics	47.281865990311246	-17.790147964878788	83575
23fd8d6df429fa6dac0fb67596e2b4d193141e14	fcbhs: a fast center-biased hybrid search algorithm for fast block motion estimation	diamond search;compact plus shaped search;motion estimation video coding video sequences software algorithms computer science motion compensation testing streaming media mobile computing application software;full search;fast center biased hybrid search algorithm;motion compensation;application software;image matching;real time;visual communication;search algorithm;software performance evaluation;x shaped search;search point reduction;motion estimation;real time mobile video coding applications;video sequences;testing;motion compensated;h 263 video sequences;video coding;accuracy;image sequences motion estimation search problems image matching vectors software performance evaluation power consumption real time systems mobile communication visual communication video coding motion compensation;h 263 video sequences fcbhs algorithm fast center biased hybrid search algorithm block motion estimation block matching algorithm center biased motion vector distribution compact plus shaped search x shaped search diamond search search point reduction search window search speed suboptimal algorithm processing power power consumption real time mobile video coding applications processing speed accuracy motion compensation;hybrid method;vectors;streaming media;motion vector;mobile communication;suboptimal algorithm;center biased motion vector distribution;software algorithms;block matching;search problems;computer science;power consumption;block motion estimation;processing power;search speed;mobile computing;processing speed;search window;block matching algorithm;mobile video;fcbhs algorithm;real time systems;image sequences	"""Describes a fast block-matching algorithm (BMA) for motion estimation exploiting the center-biased motion vector distribution characteristic more efficiently than the center-biased hybrid search (CBHS) algorithm does. This proposed algorithm, which is called the """"fast center-biased hybrid search"""" (FCBHS), employs a hybrid method of a compact plus-shaped search, an X-shaped search and a diamond search to reduce the search point for motion vectors which are distributed within a 2-pixel radius of the center of the search window. Experimental results show that the FCBHS algorithm is about 44 times faster than a full search and 7-10% faster than the CBHS algorithm. Such a fast sub-optimal motion estimation algorithm is essential for situations which have limited processing power and constraints on the amount of power consumption, like real-time mobile video coding applications. This paper compares the popular sub-optimal block-matching technique with FCBHS, for which both the processing speed and the accuracy of motion compensation are tested over widely-used H.263 test video sequences."""	motion estimation;search algorithm	Su-Bong Hong;Hyoseok Lee;Geun-Young Chun;Hyunki Baik;Myong-Soon Park	2002		10.1109/ITCC.2002.1000396	beam search;computer vision;real-time computing;beam stack search;quarter-pixel motion;computer science;theoretical computer science;motion estimation;incremental heuristic search;binary search algorithm	Vision	48.49923916689494	-19.404007175163326	83597
c2aa43ec3cb62b6eef82be9d3ba302aa2a0009e6	robust streaming of offline coded h.264/avc video via alternative macroblock coding	cuantificacion senal;desciframiento;mismatching;circuit decodeur;theorie vitesse distorsion;detection erreur;error resilient video coding;receiver;evaluation performance;rate distortion;quantum effect;deteccion error;streaming;communication networks;video streaming;performance evaluation;decodage;decoding;transmission error;video signal processing;debit information;information transmission;correction erreur;coding errors;evaluacion prestacion;erroneous channel;offline h 264 avc video coding;receptor;off line;video compression;robust error resilient video streaming;error transmision;bit rate;effet quantique;indice informacion;circuito desciframiento;error propagation detection;propagation erreur;avc;rate distortion theory;quantisation signal;video coding;decoding circuit;transmission en continu;desadaptacion;automatic voltage control;alternative macroblock coding scheme;signal quantization;senal video;signal video;resilience;codage video;bit stream replacement method;error propagation;streaming media;error correction;quantification signal;poursuite cible;recepteur;video streaming block codes coding errors decoding error detection quantisation signal video coding;h 264;traitement signal video;information rate;video signal;error resilience;video streaming avc error resilience h 264 video coding;robustness;transmision fluyente;transmision informacion;desadaptation;correccion error;propagacion error;error detection;growth of error;transmission information;target tracking;fuera linea;robustness streaming media automatic voltage control video compression decoding resilience error correction bit rate video coding communication networks;efecto cuantico;block codes;error propagation detection robust error resilient video streaming offline h 264 avc video coding erroneous channel alternative macroblock coding scheme bit stream replacement method decoder adaptive quantization selection;erreur transmission;decoder;hors ligne;adaptive quantization selection	An error resilient video streaming scheme that transmits offline coded H.264/AVC video through erroneous channels, called the alternative macroblock coding (AMC) scheme, is proposed in this work. In the AMC scheme, each macroblock can be reconstructed from one default and several alternative versions coded using different predictions. During the transmission, the sender tracks ACK or NACK messages from the receiver to detect transmission errors. When the reference used in the default version of a macroblock that has not been transmitted is corrupted by errors, one of its alternative versions that has the correctly received reference is selected to replace the default version in the output bit stream to stop error propagation. The AMC scheme is designed to ensure that the bit stream replacement during streaming will not cause significant mismatch at the decoder end and neither introduce large bit rate overhead to the transmitted bit stream. Furthermore, adaptive quantization selection and bit stream replacement methods are developed to improve the rate-distortion performance of received video. It is demonstrated by experimental results that the AMC scheme is effective in reducing error propagation in offline coded H.264/AVC video.	acknowledgement (data networks);advanced mezzanine card;bitstream;cognitive dimensions of notations;data compression;display resolution;distortion;h.264/mpeg-4 avc;macroblock;online and offline;overhead (computing);propagation of uncertainty;quantization (signal processing);software propagation;stream (computing);streaming media	Xiaosong Zhou;C.-C. Jay Kuo	2008	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2008.918762	real-time computing;error detection and correction;telecommunications;computer science;mathematics;macroblock;psychological resilience;statistics	Arch	48.52878480140105	-15.395163564225319	83599
ecbeeabe436b8af307321be1db5ce27b2da5596b	adaptive block transforms for hybrid video coding	video;distortion;matrices;transform theory	Abstract: Todays standard video coders employ the hybrid coding scheme on a macroblock basis. In these coders blocks of1616 , and 88 pixel are used for motion compensation of non-interlaced video. The Discrete Cosine Transform(DCT) is then applied to the prediction error on blocks of size 88 .	data compression	Mathias Wien;Claudia Mayer	2001			mathematical optimization;transform coding;video;lapped transform;entropy estimation;distortion;theoretical computer science;mean squared prediction error;discrete cosine transform;transform theory;coding tree unit;macroblock;motion compensation;matrix;sum of absolute transformed differences	Vision	44.03943226732626	-16.235489656773222	83611
8edf0755062d368219bfb5180f2a2ad2438d9af3	two-dimensional map based fast mode decision for h.264/avc	rate distortion;h 264 avc;2d map;video quality;video coding;performance improvement;computational complexity;temporal spatial correlation;mode decision	The state-of-the-art video coding standard H.264/AVC achieves significant coding performance improvement by choosing the rate-distortion (RD) optimized encoding mode from a list of candidate modes at the cost of intensive computations, which limits the practical application of H.264/AVC. A number of fast mode decision algorithms have been recently presented in the literature in order to reduce H.264/AVC encoding computations. In this paper, a novel two-dimensional (2D) map based fast mode decision scheme is proposed, which produces a flexible mode checking order for fast mode decision. Experimental results demonstrate that with the proposed algorithm, the computational complexity of H.264/AVC coding is significantly reduced while the video quality and compression efficiency are preserved.	algorithm;coefficient;computation;computational complexity theory;data compression;discrete cosine transform;distortion;h.264/mpeg-4 avc;ruby document format;video coding format	Tiesong Zhao;Hanli Wang;Xiangyang Ji;Sam Kwong	2008		10.1007/978-3-540-89796-5_16	scalable video coding;computer vision;real-time computing;computer science;video quality;theoretical computer science;context-adaptive variable-length coding;machine learning;context-adaptive binary arithmetic coding;computational complexity theory	AI	46.736587236007644	-18.915050539198095	83754
86f6565343835d59aa4590362c3fa9dd47498bb3	motion compensation method for moving pictures with binary shape	moving image;codificacion binaria;correlacion;binary image coding;estimation mouvement;image coding;image processing;data compression;motion compensation;interframe encoding;forme onde;video signal processing;binary image;texture image;estimacion movimiento;simulacion numerica;procesamiento imagen;motion estimation;arbitrary shaped body;imagen movil;traitement image;experimental result;image mobile;motion compensated;image texture;standardisation;compensation mouvement;codage image;video coding;binary coding;forma onda;codage video;motion vector;estructura datos;simulation numerique;image binaire;resultado experimental;mpeg 4;traitement signal video;imagen binaria;structure donnee;corps forme arbitraire;code motion;waveform;compresion dato;correlation;binary shape;cuerpo forma arbitraria;resultat experimental;codage interimage;codificacion entre imagen;data structure;compression donnee;numerical simulation;codage binaire	This paper describes a motion compensation method for arbitrarily shaped moving visual objects. To utilise both the inter-frame correlation and the texture-shape correlation, we use macroblock-based motion compensation for binary shape coding as well as texture coding. To efficiently achieve motion compensation for a moving binary shape, we introduce extra motion vectors for the shape coding in addition to the texture motion vectors, where the shape motion vectors are coded by referring to the texture motion vectors. The proposed method can successfully save more than 50% of the number of bits in comparison with the intra-only method. The effectiveness of the proposed method has been established through MPEG4 standardisation activity, and the proposed motion-compensation framework has been adopted by the MPEG-4 visual coding standard (ISO 14496-2).	image;motion compensation	Shinya Kadono;Choong Seng Boon;Minoru Etoh	2000	Sig. Proc.: Image Comm.	10.1016/S0923-5965(99)00059-4	data compression;image texture;computer vision;binary code;simulation;waveform;data structure;binary image;image processing;quarter-pixel motion;computer science;motion estimation;mathematics;motion compensation;mpeg-4;correlation;standardization;computer graphics (images)	Vision	46.39249763514826	-14.177342395258368	83779
2ffd0bb83601c545a7b24fa4d2874f1f25a0ecdc	a steganographic framework for reference colour based encoding and cover image selection	extraction information;image numerique;image processing;information extraction;etude experimentale;luminance signal;selection;traitement image;demarrage;systeme reference;start up;reference systems;senal luminancia;human visual system;imagen numerica;signal luminance;digital image;seleccion;imagen color;encoding;image couleur;codage;color image;extraction informacion	This is a small versionof a paperpublishedat the ISW2000in Wollongongdescribingonly thenew codecfor embeddingandextractionof theadditional informationandsomeexperimentalresults. 1 An adaptive steganographic framework In this section,we introducea new hybrid steganographicapproach.The presented framework is distinguishedfrom previous approachesby a greaterflexibility facing changesof thedemands. This flexibility canbereachedon two differentlevels:(1) On the level of theusedsystemsandalgorithms, and(2) on the level of a concreteimplementation. Thefirst point canberealisedduringthedesignphaseandimplementation of the steganographicsystemby exchangingof modules.On the level of a concrete implementation, changesarepossibleduringtheexecutiontimeof thesystem. The basicidea of the new approachis to useso called referencecolours, which form thestartingpoint for theembeddingandextractionof theadditionalinformation. Thesereferencecoloursarechosensuchthat if the cover imageis describedonly by thesecolours,thereareno visualdifferencesto theoriginal cover image.Startingfrom animagethatis normalisedin thisway, partsof informationcanbeembeddedinto it by smallchangesof thereferencecolours. Herethediscussedeffectof justnoticeabledifferencesis used.Theextractionitself is basedonthecomparisonbetweenthereference coloursandthecolour informationof thestego image.Therefore,senderandrecei ver mustsharethesamereferencecolourinformationto extracttheinformationcorrectly. In imagedomaintheunequi vocalassignment anddecisionof membershipfor pixel values to partsof theadditionalinformationis basedon a Pseudo-Noise-Function 1. Theused cover, the referencecoloursof the cover andthe parameters, which steerthe random functioncanbesuitablychosendependingon theexisting requirements. The presentedframework consistsof moduleswhich canbe chosenaccordingto thesuitability of componentsfor embeddingandextracting(Fig. 1). Accordingto the propertiesof theindividual requirements(cf. section??) asuitablecandidatecanbeselectedfor eachcodecstep.To embedamessage, wefirst have to choseasuitablecover (module1) which dependson the userrequirementsandis basedon the prepositions givenin section??. After that,wetransformthiscover in amoresuitablecoloursystem 1 Implementationof Kerkhof ’s principle[Sim83].	codi;pixel;requirement;steganography;ver (command)	René Rosenbaum;Heidrun Schumann	2000		10.1007/3-540-44456-4_3	selection;computer vision;color image;image processing;computer science;artificial intelligence;human visual system model;information extraction;digital image;encoding	Vision	44.676862410961945	-11.719379609594991	84038
9533ba05ae0014a20b32326a6db7067c409fdffb	improved separable reversible data hiding in encrypted image based on neighborhood prediction		Recently, separable reversible data hiding in encrypted image attracts more and more attention. Data extraction and image decryption are separable in the separable reversible data hiding method in encrypted image, which makes it possible for cloud server to extract the additional data without knowing the original content. In this paper, we focus on the user side and introduced two improved methods based on neighborhood prediction to obtain the good quality decrypted image without data hiding key. Our experiment results prove both the two methods achieve good performance on decrypted image.		Shu Yan;Fan Chen;HongJie He	2016		10.1007/978-3-319-48671-0_9	theoretical computer science;pattern recognition;data mining	Robotics	39.37973926002045	-11.413706110070253	84472
6b72b962e8d50f1872e2688665e0b4802aa2754d	mode-dependent transforms based on elliptical model for high efficiency video coding	hevc;intra prediction;klt;non separable transform;elliptical model;mode dependent transform	High efficiency video coding (HEVC) defines 35 prediction modes in its intra prediction stage to signal the direction information of residual blocks. Traditionally, separable two-dimension (2-D) transforms (integer DCT and DST) are utilized in a similar manner as in the previous H.264/AVC standards. However, such 2-D transforms cannot yield the best energy compaction for a 2-D directional source where the dominating directional information is other than the horizontal or vertical one. In order to overcome this drawback, we build an elliptical model with directionality and design some non-separable transforms based on the Karhunen-Loeve transform in this paper. Specifically, we derive a non-separable transform in closed-form for each intra-prediction mode and replace the default transform in HEVC. Simulation results reveal that 1.7% and 2.0% on average and up to 7.7% and 8.1% BD-rate reduction can be achieved for luma and chroma component, respectively. In the meantime, the test results show that both the encoding time and decoding time increase only about 5%.	algorithm;algorithmic efficiency;blu-ray;computational complexity theory;data compaction;data compression;discrete cosine transform;discrete sine transform;h.264/mpeg-4 avc;high efficiency video coding;intra-frame coding;iteration;order by;simulation;terabyte	Kaiyuan Jia;Chen Chen;Xiandong Meng;Shuyuan Zhu;Bing Zeng	2016	2016 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2016.7805583	speech recognition;telecommunications;mathematics	EDA	43.28723014155652	-15.928211667923302	84567
85ecbb66ff97db2e8788a305f0a0651ce54257a9	wavelets: a tool for efficient data representation	computer graphics;video compression;data representation;data compression;videoconference;data visualization;application software;noise reduction	More often than not information is embedded in massive amount data corrupted with noise. Efficient representation of data will facilitate processing of information contained in the data. Wavelets are a new mathematical tool for data representation, and are particularly efficient in reducing the number of parameters in the representation, a process known as data compression. This talk is a visual presentation of the applications of wavelets, including image, video and other data compression as well as denoising and signal separation. Some mathematical aspects of wavelets will also be presented.	data (computing);data compression;embedded system;noise reduction;wavelet	S. L. Lee	2004	Proceedings. International Conference on Computer Graphics, Imaging and Visualization, 2004. CGIV 2004.	10.1109/CGIV.2004.1323985	video compression picture types;data compression;computer vision;image compression;computer science;theoretical computer science;lossless compression;computer graphics (images)	Visualization	42.58466026838974	-19.624936847590813	84649
824bdfd3890f321b42e37937682bb34e2f3157db	a fast coding algorithm based on inter-view correlations for 3d-hevc	cu depth decision;interview correlation;3d hevc;cu depth candidate;statistical analysis	The newly published 3D-HEVC has received a remarkable response due to its high compression efficiency which is based on High Efficiency Video Coding (HEVC). However, the complexity of its encoding process is also large as a result of introducing the coding units (CU) size decision process together with the rate distortion optimization (RDO) process. In this paper, a fast coding algorithm making good use of the interview correlations is proposed. With the inter-view correlation statistical analysis, the CU depth candidates of the dependent views can be predicted from the independent view instead of the brute force RDO process in determining CU depth. The experimental results show that the proposed method saves 51% time in texture coding and the loss is negligible.	3d computer graphics;algorithm;blu-ray;brute-force search;distortion;high efficiency video coding;huffman coding;mathematical optimization;raster document object;rate–distortion optimization;rate–distortion theory;real-time clock;remote data objects;tip (unix utility)	Guangsheng Chi;Qionghai Dai	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051584	simulation;theoretical computer science;mathematics;statistics	EDA	46.58753325185852	-19.425375010270432	84683
2050ecfa30af4571c18d842ac736c10b63d9137d	combined issue on high efficiency video coding (hevc) standard and research	special issues and sections video coding mpeg standards iso standards iec standards;special issues and sections;iso standards;video coding;iec standards;mpeg standards	With the emergence of the HEVC project, TCSVT published the first special section of its kind in December 2010. The special section consisted of a number of video coding proposals that were submitted to the ISO/IEC SC29/WG11 (MPEG) and ITU-T SG16/Q6 (VCEG). Now, on the eve of its ratification of formally becoming a new video coding standard, we are very pleased to dedicate the entire issue to the topic of HEVC. The first part will be the special section on HEVC standard and is edited by J.-R. Ohm, G.J. Sullivan, and T. Wiegand. It consists of three invited papers. These excellent papers provide a comprehensive overview of the important aspects of the new standard. The second part is the special issue that was initiated by the Editor-in-Chief in 2012, and the Guest Editors were carefully selected from the members of our regular editorial team. Seventy two submissions were received in response to our call for papers, and unfortunately, we could only accommodate 18 due to our limited space.		Hamid Gharavi	2012	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2012.2226073	telecommunications;computer science;electrical engineering;multimedia;h.262/mpeg-2 part 2;mpeg-4	EDA	42.29714494429719	-20.266310067445716	84763
4d7d558043cfc14562b8f4e99c4946abdd39ca35	a novel steganographic method based on edge detection and adaptive multiple bits substitution	adaptive lsb substitution steganography edge detection the sobel operators;edge detection;random number generation;steganography;cryptography;image edge detection data mining visualization gray scale vectors payloads psnr;steganography cryptography edge detection random number generation;steganographic method prng pseudorandom number generator data embedding route sobel operator gradient magnitude grayscale image steganography adaptive multiple bits substitution edge detection	The present paper proposes a novel method for grayscale image steganography based on edge detection and adaptive multiple bits substitution. Compared with the smooth regions, the pixels located in the edge regions usually present more random characteristics. In proposed method, the gradient magnitude of the pixels of the cover image is computed using the Sobel operators. As a result, all edges of the cover image, both horizontal and vertical, are fully detected. According to the length of secret data, the sharper edges are adaptively preserved and the weaker edges are suppressed. Therefore, the sharper edges will be used in advance of the weaker edges and the smooth regions for data embedding. Next, the data embedding route is determined using a pseudorandom number generator (PRNG) and multiple bits of secret data are adaptively embedded into k-LSBs of the pixels in the route. The value k depends on the gradient magnitude of each pixel. The larger the gradient magnitude, the larger the value k. The experimental results evaluated on 8000 natural images with RS steganalytic algorithm show that our proposed method can significantly enhance the security and can increase the embedding capacity.	algorithm;edge detection;embedded system;gradient;grayscale;pixel;pseudorandom number generator;sobel operator;steganography	Mehran Iranpour	2013	2013 18th International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2013.6622828	computer vision;discrete mathematics;theoretical computer science;mathematics	Vision	40.24398508823284	-11.331366951920865	84769
25696df1fbcc60491d903ad7780807075fde917d	minimum constraint displacement motion planning		This paper formulates a new minimum constraint displacement (MCD) motion planning problem in which the goal is to minimize the amount by which constraints must be displaced in order to yield a feasible path. It presents a sampling-based planner that asymptotically approaches the globally optimal solution as more time is spent planning. Key developments are efficient data structures that allow the planner to select small subsets of obstacles and their displacements that are candidates for improving the current best solution, and local optimization methods to improve convergence rate. The resulting planner is demonstrated to successfully solve MCD problems with dozens of degrees of freedom and up to one hundred obstacles.	algorithm;automated planning and scheduling;data structure;displacement mapping;interaction;magnetic circular dichroism;mathematical optimization;maxima and minima;motion planning;rate of convergence;sampling (signal processing)	Kris K. Hauser	2013		10.15607/RSS.2013.IX.017	pfaffian constraint	Robotics	52.40294855415228	-23.57482658789826	84910
0e11a4c1c03b5ba36352957414f7ff7ab0ab57bc	representation of regions for accurate motion compensation	interpolation;image coding;psnr;bit rate;interpolation bit rate image coding meteorology containers psnr;meteorology;containers	We propose a novel motion compensation technique for the precise reconstruction of regions over several frames within a region-based coding scheme. This is achieved by using a more accurate internal representation of arbitrarily shaped regions than the standard grid structure, thus avoiding repeated approximations for a region at each frame.	approximation;motion compensation	R. Rambaruth;William J. Christmas;Josef Kittler	2000	2000 10th European Signal Processing Conference		computer vision;mathematical optimization;theoretical computer science;mathematics	Vision	45.48608216226141	-17.530304165644043	85075
807e2f0e24b0629be7d7f69855aa3ad89753a856	fast hevc intra coding algorithm based on machine learning and laplacian transparent composite model		Compared with H.264, High Efficient Video Coding (HEVC) improves the coding efficiency by 50% at the price of significant increase in encoding time, due to Rate Distortion Optimization (RDO) on large variations of block sizes and prediction modes. In this paper, a fast intra coding algorithm is proposed to alleviate the high computational complexity of HEVC intra-frame coding. The proposed algorithm is based on machine learning and Laplacian Transparent Composite Model (LPTCM). Features called Summation of Binarized Outlier Coefficient (SBOC) vectors are firstly extracted from original frames by using LPTCM and then fed into online trained Support Vector Machine (SVM). Two SVMs are combined to predict Coding Unit (CU) decisions so that the encoding process can be significantly sped up. Additionally, a performance controller is introduced to ensure the robustness of machine learning models. It is shown by experiments that compared with HM 16.3, the proposed algorithm reduces the encoding time, on average, by 48% with negligible increase in BD-rate.	algorithm;algorithmic efficiency;blu-ray;coefficient;computational complexity theory;distortion;experiment;h.264/mpeg-4 avc;high efficiency video coding;intra-frame coding;midi controller;machine learning;raster document object;rate–distortion optimization;rate–distortion theory;shadow volume;support vector machine;tip (unix utility)	Yi Shan;En-Hui Yang	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952635	theoretical computer science;machine learning;pattern recognition	Robotics	46.865023675893895	-19.85646134399122	85198
fd27ac0396e207413d5be507c854fb3bdf6b33a5	view-dependent progressive mesh coding based on partitioning	3d graphics	A view-dependent progressive mesh coding algorithm is proposed in this work to facilitate interactive 3D graphic streaming and browsing. First, a 3D graphic model is split into several partitions. Second, each partition is simplified independently to generate a base model that can be efficiently encoded. Third, topological and geometrical data are reorganized to enable the view dependent transmission. Before the transmission, the server is informed of the viewing parameters. Then, the server can accordingly transmit visible parts in detail, while cutting off invisible parts. Experimental results demonstrate that the proposed algorithm reduces the required transmission bandwidth, and provides an acceptable visual quality even at low bit rates.	progressive meshes	Sheng Yang;Chang-Su Kim;C.-C. Jay Kuo	2002			computer vision;telecommunications;computer science;theoretical computer science;machine learning;algorithm;3d computer graphics;computer graphics (images)	EDA	41.339923712193375	-18.66833775955498	85577
d69dc588dbbbeb05101f212c124e1e11662f3f84	video denoising using motion compensated 3-d wavelet transform with integrated recursive temporal filtering	mc3dwt framework;video denoising;transformation ondelette;high pass;traitement signal;video denoising motion compensated 3 d wavelet transform mc3dwt;motion compensated 3 d wavelet transform mc3dwt;filtering;evaluation performance;mc3dwt framework video denoising motion compensated 3d wavelet transform integrated recursive temporal filtering sliding window image denoising algorithms inverse 2d spatial wavelet transform;logica temporal;mise a jour;performance evaluation;image processing;motion compensation;inverse 2d spatial wavelet transform;imagen fija;video signal processing;redundancia;filtrado frecuencia espacial;temporal logic;evaluacion prestacion;wavelet base;subband decomposition;base ondita;procesamiento imagen;real time processing;noise reduction wavelet transforms filtering wavelet domain image denoising delay signal processing image reconstruction noise level visual effects;recursive filters;filtrage recursif;buffer system;traitement image;motion compensated;reduccion ruido;sistema amortiguador;algorithme;actualizacion;temporal filtering;wavelet transforms;algorithm;compensation mouvement;tratamiento tiempo real;visual effects;traitement temps reel;integrated recursive temporal filtering;motion compensated 3d wavelet transform;wavelet transform;noise level;redundancy;fixed image;subbanda;descomposicion subbanda;image reconstruction;signal processing;noise reduction;subband;reduction bruit;spatial filtering;image denoising algorithms;image fixe;transformation inverse;suppression bruit image;temps retard;image denoising;filtrado recursivo;transformacion ondita;delay time;decomposition sous bande;inverse transformation;wavelet domain;systeme tampon;spatial frequency filtering;base ondelette;procesamiento senal;tiempo retardo;sous bande;logique temporelle;recursive filtering;wavelet transformation;updating;sliding window;redondance;filtrage frequence spatiale;transformacion inversa;algoritmo	A novel framework of the motion-compensated 3-D wavelet transform (MC3DWT) for video denoising is presented in this paper. The motion-compensated temporal wavelet transform is first performed on a sliding window of video frames consisting of previously denoised frames and the current noisy frame. The 2-D spatial wavelet transform is then performed on the temporal subband frames, thus realizing a 3-D wavelet transform. Any of established wavelet-based still image denoising algorithms can then be applied to the high-pass 3-D subbands. The operation of the inverse 2-D spatial wavelet transform followed by the inverse temporal wavelet transform reconstructs the video frames in the buffer. The denoised current frame may be used as an output for real-time processing; meanwhile, the past frames can be updated, one of which may be used as a delayed output for post-processing or for real-time processing that allows some amount of delay. The proposed MC3DWT framework integrates both the spatial filtering and recursive temporal filtering into the 3-D wavelet domain and effectively exploits both the spatial and temporal redundancies. Experimental results have demonstrated a superior visual and quantitative performance of the proposed scheme for various levels of noise and motion.	additive white gaussian noise;algorithm;bivariate data;coefficient;computational complexity theory;image noise;motion compensation;noise reduction;real-time clock;recursion;stationary process;statistical model;thresholding (image processing);video denoising;video post-processing;wavelet transform;whole earth 'lectronic link	Shigong Yu;M. Omair Ahmad;M. N. Shanmukha Swamy	2010	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2010.2045806	wavelet;computer vision;speech recognition;s transform;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;image processing;computer science;signal processing;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;wavelet transform	Vision	46.63808288922363	-14.709923773160813	85676
44b9b1e3c9bf015e3c7dd18276a71bc57b6088fd	h.264 visually lossless compressibility index: psychophysics and algorithm design	loss measurement;image coding;data compression;support vector machines;video coding data compression;visually lossless threshold;video compression;temporal activity;biomedical imaging;lossless compression;spatial activity;temporal activity h 264 compression compressibility index psychophysics video compression literature psychovisual study visually lossless threshold video spanning spatial activity;video compression literature;video coding;indexes;visualization;psychovisual study;indexation;image coding indexes loss measurement biomedical imaging support vector machines motion measurement visualization;h 264 compression;motion measurement;video spanning;compressibility index;algorithm design;psychophysics	Although the term ‘visually lossless’ (VL) has been used liberally in the video compression literature, there does not seem to be a systematic evaluation of what it means for a video to be compressed visually lossless-ly. Here, we undertake a psychovisual study to infer the visually lossless threshold for H.264 compression of videos spanning a wide range of contents. Based on results from this study, we then propose a compressibility index which provides a measure of the appropriate bit-rate for VL H.264 compression of a video given texture (i.e., spatial activity) and motion (i.e., temporal activity) information. This compressibility index has been made available online at [1] in order to facilitate practical application of the research presented here and to further research in the area of VL compression.	algorithm design;data compression;file spanning;h.264/mpeg-4 avc;lossless compression	Anush K. Moorthy;Alan C. Bovik	2011	2011 IEEE 10th IVMSP Workshop: Perception and Visual Signal Analysis	10.1109/IVMSPW.2011.5970364	data compression;computer vision;computer science;lossless compression;multimedia;computer graphics (images)	Vision	44.65731457880858	-22.116803757391523	85785
5d72d1db8896e81137f2067659dcd15eca88ca7e	fine-granular motion matching for inter-view motion skip mode in multiview video coding	inter view motion prediction;theorie vitesse distorsion;traitement signal;evaluation performance;rate distortion;optimisation;teleconferencing;rate distortion optimization criterion fine granular motion matching algorithm inter view motion skip mode multiview video coding motion correlation motion compensated coding;video coding video compression rate distortion layout cameras tv iso standards iec standards teleconferencing surveillance;image coding;performance evaluation;image processing;motion compensation;optimizacion;video signal processing;motion correlation;surveillance;motion skip mode;image matching;iso standards;evaluacion prestacion;multiview video coding fine granular motion matching inter view motion prediction motion skip mode;video compression;procesamiento imagen;image multiple;inter view motion skip mode;imagen multiple;layout;correlation methods;traitement image;materiau granulaire;motion compensated;multiple image;rate distortion optimization criterion;algorithme;rate distortion theory;motion compensated coding;algorithm;compensation mouvement;codage image;multiview video coding;video coding;video coding correlation methods image matching motion compensation rate distortion theory;iec standards;codage video;signal processing;fine granular motion matching;traitement signal video;optimization;tv;granular material;fine granular motion matching algorithm;rate distortion optimization;procesamiento senal;material granular;cameras;algoritmo	As motion of neighboring views is highly correlated in multiview video systems, the inter-view motion skip mode has been proposed to improve the efficiency of multiview video coding (MVC) by reusing the motion information of neighboring views. To fully exploit the inter-view motion correlation, a fine-granular motion matching algorithm for the motion skip mode is presented in this paper. The proposed algorithm searches for the best matching motion information in neighboring views with fine granularity, and then uses the motion information for the motion-compensated coding in the motion skip mode. The rate-distortion optimization criterion is applied to find the best matching motion information. Experimental results show that the proposed algorithm can improve the performance of the motion skip mode, and further improve the efficiency of MVC.	algorithm;data compression;distortion;h.264/mpeg-4 avc;macroblock;mathematical optimization;model–view–controller;multiview video coding;rate–distortion optimization	Haitao Yang;Yilin Chang;Junyan Huo	2009	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2009.2017410	data compression;layout;computer vision;structure from motion;simulation;teleconference;granular material;rate–distortion theory;image processing;quarter-pixel motion;computer science;signal processing;motion estimation;block-matching algorithm;rate–distortion optimization;motion compensation;algorithm;multiview video coding;computer graphics (images)	Vision	46.38966868214826	-15.167663905975925	85792
9e9b41d2e4b749e6f932133d96b827428fa00e55	winner update on walsh-hadamard domain for fast motion estimation	winner update search;motion estimation video compression distortion measurement computer science tracking video coding degradation frequency mean square error methods object detection;sum of absolute difference;walsh functions data compression hadamard transforms motion estimation video coding;data compression;video compression;motion estimation;null;sum of absolute difference video compression block based motion estimation winner update search walsh hadamard transform domain energy packing capability;video coding;fast motion estimation;walsh functions;walsh hadamard transform;hadamard transforms;walsh hadamard transform domain;energy packing capability;block based motion estimation	Motion estimation (ME) plays an important role in video compression. Block-based ME has been adopted in most video compression standards due to its efficiency. In this paper, we propose a novel and fast block-based ME algorithm that is based on applying the winner-update search in the Walsh-Hadamard transform domain. Due to the energy packing capability of the Walsh-Hadamard transform, the modified winner update procedure is performed only on a small number of most representative transform coefficients. In addition, our algorithm can skip the search at some locations based on the predicted SAD of the current block. The superior performance of the proposed algorithm is demonstrated through experimental comparison with some representative motion estimation methods	algorithm;algorithmic efficiency;coefficient;computation;computational complexity theory;data compression;distortion;flying-spot scanner;hadamard transform;moea framework;motion estimation;peak signal-to-noise ratio;set packing;video coding format;windows me	Shao-Wei Liu;Shou-Der Wei;Shang-Hong Lai	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.1190	data compression;computer vision;mathematical optimization;computer science;theoretical computer science;mathematics;statistics;sum of absolute transformed differences	Vision	48.206959515295466	-19.126426681517838	85928
130bf256f4cc3dded4fb701f74f6a34992be639b	a robust multiwavelet-based watermarking scheme for copyright protection of digital images using human visual system		The contemporary period of information technology facilitates simple duplication, manipulation and distribution of digital data. This enduringly, has insisted the rightful ownership of digital images to be protected efficiently. For content owners and distributors, there emerged a necessary concern in regard to the content authentication of digital images as well as copyright protection. A latent solution to this issue is bestowed by digital watermarking. To certify efficient copyright protection, the watermarking scheme should own the characteristics, such as robustness and imperceptibility. Integration of Human Visual System (HVS) models with in the watermarking scheme helps to attain an effective copyright protection. Currently, wavelet domain based watermarking scheme mainly interested in watermarking researches. An undetectable and proficient wavelet-based watermarking scheme to safe guard the copyrights of images are portrayed here on contrary to the prior works. By effecting few modifications to our prior works, we have presented a new proficient watermarking scheme by incorporating the HVS models for watermark embedding. Additionally, we have applied the GHM multiwavelet transform in the watermarking process. Based on the computed distance using hausdorff distance measure, the image components for embedding are selected and a new procedure is designed for watermark embedding by multiplying the embedding strength with the random matrix that is generated by key image as a primary element and is engaged in both embedding and extraction processes. The correlation coefficient computation is used for extraction of watermark process. The experimental results illustrate the robustness and imperceptibility of the proposed approach. From the results, we can identify that the proposed watermarking process has achieved the correlation value of 0.9848 even if the watermarked image is affected by the Gaussian noise.	authentication;authorization;coefficient;computation;digital data;digital image;digital watermarking;hausdorff dimension;human visual system model;requirement;robustness (computer science);wavelet	Padmanabhareddy Vundela;Varadarajan Sourirajan	2013	Int. Arab J. Inf. Technol.		computer vision;digital watermarking;theoretical computer science;multimedia;watermark;computer security	AI	40.526314972026825	-10.467086987410122	85987
f695118989b3c39857536d6c68bacf334a621319	on-line scene change detection of multicast video	on line processing;ip multicast;real time;video processing;internet video;feature extraction;video scene change detection;scene change detection;mbone	Network-based computing is becoming an increasingly important area of research, whereby computational elements within a distributed infrastructure process/enhance the data that traverses through its path. We refer to these computations as online processing and this paper investigates scene change detection in the context of MBonebased proxies in the network. On-line processing varies from traditional offline processing schemes, where for example, the whole video scope is known as a priori, which allows multiple scans of the stored video files during video processing. The on-line processing system is designed to meet the requirements of real-time video multicasting over the Internet and to utilize the successful video parsing techniques available today. The proposed algorithms do scene change detection and extract key frames from video bitstreams sent through the MBone network. We study several algorithms based on histogram differences and evaluate them with respect to precision, recall, and processing latency.C © 2001 Academic Press	algorithm;codec;computation;digital video;discrete cosine transform;key frame;mbone;multicast;network packet;online and offline;parsing;real-time clock;real-time transcription;requirement;sensitivity and specificity;video file format;video processing;workstation	Wensheng Zhou;Asha Vellaikal;Ye Shen;C.-C. Jay Kuo	2001	J. Visual Communication and Image Representation	10.1006/jvci.2000.0459	video compression picture types;computer vision;real-time computing;ip multicast;feature extraction;mbone;computer science;video tracking;multimedia;video processing;video post-processing;multiview video coding	ML	43.971168569046355	-23.02391657012022	86211
42ec89ae483a00f4192374be0ed05065547334ea	video error correction using steganography	signal image and speech processing;data hiding;error concealment;steganography;quantum information technology spintronics;error correction;mpeg 2	The transmission of any data is always subject to corruption due to errors, but video transmission, because of its real time nature must deal with these errors without retransmission of the corrupted data. The error can be handled using forward error correction in the encoder or error concealment techniques in the decoder. This MPEG-2 compliant codec uses data hiding to transmit error correction information and several error concealment techniques in the decoder. The decoder resynchronizes more quickly with fewer errors than traditional resynchronization techniques. It also allows for perfect recovery of differentially encoded DCT-DC components and motion vectors. This provides for a much higher quality picture in an error-prone environment while creating an almost imperceptible degradation of the picture in an error-free environment.	closing (morphology);codec;cognitive dimensions of notations;discrete cosine transform;elegant degradation;encoder;error concealment;error detection and correction;forward error correction;image quality;mpeg-2;qr code;quality of service;retransmission (data networks);steganography	David L. Robie;Russell M. Mersereau	2002	EURASIP J. Adv. Sig. Proc.	10.1155/S1110865702000562	error detection and correction;speech recognition;soft-decision decoder;telecommunications;computer science;theoretical computer science;mpeg-2;steganography;information hiding;statistics	Graphics	48.54169802401995	-15.749717946010225	86251
8b4a024daf8ec5b71cd054e4fd03e44e565dd5d8	a new watermarking method with obfuscated quasi-chirp transform	embedding;orthogonal transform;quantization;detection;obfuscation	Watermark detection software is obfuscated using a table to hide embedding and detection algorithms. As the table size is limited, the block size is also limited for watermarking. To address this situation, a new quasi-chirp transform is developed to improve embedding efficiency. The quasi-chirp transform is different from the conventional DCT or Fourier transform. It contains multiple frequency components in a single basis of the transform. It disperses image data rather than compressing it, as the DCT does. The dispersed data increases the range for embedding watermarks. The chirp transform is able to embed even on a flat area of an image. Using this chirp transform, embedding and detection experiments for image data with small block sizes were carried out. A high SNR and robust watermark with an evaluated obfuscation were obtained.	chirp;obfuscation (software)	Kazuo Ohzeki;Yuanyu Wei;Yutaka Hirakawa;Kiyotsugu Sato	2011		10.1007/978-3-642-32205-1_7	discrete mathematics;speech recognition;obfuscation;s transform;quantization;computer science;theoretical computer science;embedding;mathematics	EDA	40.75412171885988	-10.385321831752211	86340
ad2f7967b0a813fa56a73d4233d4a16b2cbcee09	transmission error and compression robustness of 2d chaotic map image encryption schemes	signal image and speech processing;systems and data security;image encryption;security science and technology;transmission error;communications engineering networks	This paper analyzes the robustness properties of 2D chaotic map image encryption schemes. We investigate the behavior of such block ciphers under different channel error types and find the transmission error robustness to be highly dependent on the type of error occurring and to be very different as compared to the effects when using traditional block ciphers like AES. Additionally, chaotic-mixing-based encryption schemes are shown to be robust to lossy compression as long as the security requirements are not too high. This property facilitates the application of these ciphers in scenarios where lossy compression is applied to encrypted material, which is impossible in case traditional ciphers should be employed. If high security is required chaotic mixing loses its robustness to transmission errors and compression, still the lower computational demand may be an argument in favor of chaotic mixing as compared to traditional ciphers when visual data is to be encrypted.	baker's map;block cipher;chaos theory;chaotic mixing;computation;encryption;http 404;image quality;iteration;jpeg;lossy compression;pixel;requirement;robustness (computer science);stencil buffer	Michael Gschwandtner;Andreas Uhl;Peter Wild	2007	EURASIP J. Information Security	10.1155/2007/48179	multiple encryption;telecommunications;computer science;theoretical computer science;s-box;computer security	Crypto	39.399094946659574	-11.912593799488072	86569
b2dbcfb0419587c46c2e8f87bfe912c534935d7e	a two-dimensional approach for lossless eeg compression	compression algorithm;arithmetic coding;entropy coding;lossless compression;mental imagery;journal article;image compression;jpeg2000;spiht;relative energy concentration;source memory;electroencephalogram eeg;drntu engineering electrical and electronic engineering control and instrumentation medical electronics;correlation coefficient;electroencephalogram;predictive coding	In this paper, we study various lossless compression techniques for electroencephalograph (EEG) signals. We discuss a computationally simple pre-processing technique, where EEG signal is arranged in the form of a matrix (2-D) before compression. We discuss a two-stage coder to compress the EEG matrix, with a lossy coding layer (SPIHT) and residual coding layer (arithmetic coding). This coder is optimally tuned to utilize the source memory and the i.i.d. nature of the residual. We also investigate and compare EEG compression with other schemes such as JPEG2000 image compression standard, predictive coding based shorten, and simple entropy coding. The compression algorithms are tested with University of Bonn database and Physiobank Motor/Mental Imagery database. 2-D based compression schemes yielded higher lossless compression compared to the standard vector-based compression, predictive and entropy coding schemes. The use of pre-processing technique resulted in 6% improvement, and the two-stage coder yielded a further improvement of 3% in compression performance.	algorithm;arithmetic coding;data compression;electroencephalography;entropy encoding;image compression;jpeg 2000;lossless compression;lossy compression;preprocessor;set partitioning in hierarchical trees	K. Srinivasan;Justin Dauwels;M. Ramasubba Reddy	2011	Biomed. Signal Proc. and Control	10.1016/j.bspc.2011.01.004	data compression;lossy compression;mental image;arithmetic coding;color cell compression;lossless jpeg;data compression ratio;dictionary coder;speech recognition;block truncation coding;transparency;image compression;computer science;entropy encoding;theoretical computer science;context-adaptive variable-length coding;jpeg 2000;mathematics;lossless compression;tunstall coding;adaptive coding;context-adaptive binary arithmetic coding;set partitioning in hierarchical trees;statistics;golomb coding	ML	42.509223577069086	-15.43007956748603	87013
bc78d825a2ede7a9e8eca872f196788f9df8cc8f	recursively weighting pixel domain intra prediction on h.264	transform coding;autoregressive model;prediction error;algorithms;autoregressive models	Intra coding for lossy block base transform video coding and still picture coding has been studied. In H.264, pixel domain prediction is applied, where all pixel values in a block are predicted from decoded images in surrounding blocks. There are some advantages in pixel domain prediction comparing with DCT domain prediction. One thing is that in pixel domain prediction, residual data at block boundaries becomes smaller. On the other hand, in pixel base prediction scheme for lossless coding, each pixel value is predicted from surrounding pixels generally. In Multiplicative Autoregressive Models (MAR) or JPEG-LS, each pixel is predicted from some neighboring pixels. This pixel base prediction scheme is more effective to reduce prediction error than block base prediction. In this paper, the new intra prediction method, Recursively Weighting pixel domain Intra Prediction (RWIP) method for block base transform coding is proposed. The RWIP applies similar approach to pixel base prediction scheme in order to reduce prediction error more than the conventional block base prediction scheme, especially for blur or complicated directional edge images. This paper also demonstrates the efficiency of the RWIP over the normal intra prediction of H.264.	h.264/mpeg-4 avc;intra-frame coding;pixel;recursion	Hideaki Kimata;Masaki Kitahara;Yoshiyuki Yashima	2003			computer vision;speech recognition;signal processing;mathematics;autoregressive model;statistics	Vision	44.715086270453035	-16.551231142451464	87265
131c554efe67f61fe3a078e283ba86994f9ccbd3	a content-adaptive approach for reducing embedding impact in steganography	art;chaotic communication;image coding;embedding impact reduction;information security;information hiding;stego image;gray scale;noise measurement;content adaptive approach;steganography;stego image content adaptive approach embedding impact reduction steganography modification direction neighboring pixel;neighboring pixel;security information hiding steganography;pixel;steganography statistics information security data security digital images entropy pixel chaotic communication computer science art;statistics;content adaptation;entropy;wiener filter;computer science;security;digital images;modification direction;integrated circuits;steganography image coding;noise;data security	In this paper, a content-adaptive steganographic scheme is proposed. The novel scheme can be viewed as an improvement of the conventional LSB matching. In this scheme, we take advantage of embedding redundancy in LSB matching to select modification direction (i.e., increasing or decreasing the pixel value by 1), and the dependency of neighboring pixels is taken into consideration. More specifically, if the secret message bit does not match the LSB of the corresponding cover pixel value, the choice of modification direction is not random but a specific selection, in order to hold the correlation of neighboring pixels as far as possible. The resulting stego image looks more like a natural one, and smooth to some extent. Comparing with LSB matching and other state-of-the-art steganography, higher level security of the proposed scheme is experimentally verified. In addition, the proposed approach can be also applied in LSB-based steganography to enhance the security.	experiment;least significant bit;pixel;steganography	Chao Wang;Xiaolong Li;Bin Yang;Xiaoqing Lu;Chengcheng Liu	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495440	computer vision;entropy;computer science;noise measurement;noise;information security;theoretical computer science;mathematics;data security;steganography;wiener filter;information hiding;computer security;digital image;pixel;grayscale;statistics	Vision	40.54915336407965	-11.68170865924492	87294
9d441463e3cfc1fd1cca510fd0f39e10a28fc2bc	horizon computation on a hierarchical triangulated terrain model	dynamic programming;visibilite;triangulated irregular network;visibilidad;programacion dinamica;data compression;surface representation;modele numerique terrain;modelo numerico terreno;digital terrain model;algorithme;algorithm;aleatorizacion;visibility;level of detail;programmation dynamique;randomized algorithm;randomisation;terrain modeling;compresion dato;randomization;hierarchical models;compression donnee;algoritmo	Hierarchical terrain models describe a topographic surface at different levels of detail, thus providing a multiresolution surface representation as well as a data compression mechanism. We consider the horizon computation problem on a hierarchical polyhedral terrain (in particular, on a hierarchical triangulated irregular network), which involves extracting the horizon of a viewpoint at a given resolution and updating it as the resolution increases. We present an overview of horizon computation algorithms on a nonhierarchical polyhedral terrain. We extend such algorithms to the hierarchical case by describing a method which extracts the terrain edges at a given resolution, and proposing a randomized algorithm for dynamically updating a horizon under insertions and deletions of terain edges	computation;computational problem;data compression;polyhedral terrain;polyhedron;randomized algorithm;topography;triangulated irregular network	Leila De Floriani;Paola Magillo	1995	The Visual Computer	10.1007/BF01898599	data compression;randomization;computer vision;simulation;digital elevation model;visibility;computer science;triangulated irregular network;dynamic programming;level of detail;mathematics;randomized algorithm;algorithm	Robotics	39.37454740226529	-18.790889872018518	87390
b6a1d5a1e6c13f88ee305e8b3720890b1c8aa5d8	a majority-voting based watermarking scheme for color image tamper detection and recovery	tamper detection;data embedding;fragile watermarking;visual quality;tampered image recovery;vector quantizer;majority voting;color image	This paper presents a novel color image watermarking scheme for both tamper detection and tampered image recovery. The proposed scheme embeds watermarks consisting of the authentication data and the recovery data into image blocks. In the tamper detection process, instead of independently examining each embedded authentication data, we take all the authentication data embedded in an image into account and utilize a majority-voting technique to determine the legitimacy of image blocks. Experimental results show that the proposed scheme can effectively thwart collage attack and vector quantization (VQ) attack, while sustaining superior accuracy of tamper localization. Furthermore, the results reveal that the tampered images can be successfully recovered with acceptable visual quality.	color image;digital watermarking	Ming-Shi Wang;Wei-Che Chen	2007	Computer Standards & Interfaces	10.1016/j.csi.2006.11.009	majority rule;computer vision;color image;computer science;internet privacy;watermark;law;computer security	Vision	39.29527397306216	-11.594825165523702	87793
f225c5ce6ce3a9e7bf6aceb144feb8ef95a0a19f	block boundary filtering for intra prediction samples	interpolation;video coding adaptive filters filtering theory interpolation;video coding;adaptive filters;adaptive filtering scheme block boundary filtering intra prediction samples hevc next generation video coding standard iso mpeg itu t vceg mpeg4 avc h 264 like intra prediction scheme adjacent reference samples interpolation reference samples prediction block adjacent reconstructed blocks;filtering theory;encoding maximum likelihood detection nonlinear filters video coding smoothing methods transform coding	This paper presents a new intra prediction technique for HEVC, which is next-generation video coding standard that has collaboratively been developed by ISO/MPEG and ITU-T/VCEG. On intra coding for HEVC, extension of MPEG4-AVC/H.264-like intra prediction scheme has been studied. In this conventional scheme, the prediction block is generated by simply copying adjacent reference samples, interpolating the reference samples toward prediction direction or averaging the reference samples. Thus, signal discontinuity between prediction block and adjacent reconstructed blocks could occur, which causes loss of coding efficiency of residual signal. In this paper, we propose an adaptive filtering scheme for intra prediction samples generated by either DC or directional prediction that solves the above issue. According to our evaluation, the proposed scheme achieves approximately 0.6% bitrate savings on average compared to the conventional AVC-style intra coding.	adaptive filter;algorithmic efficiency;data compression;h.264/mpeg-4 avc;high efficiency video coding;interpolation;intra-frame coding;moving picture experts group;reference implementation;reflections of signals on conducting lines;video coding format	Akira Minezawa;Kazuo Sugimoto;Shun-ichi Sekiguchi	2013	2013 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2013.6487052	adaptive filter;computer vision;electronic engineering;interpolation;computer science;theoretical computer science;mathematics;statistics	Robotics	45.25545680567967	-17.569416982124384	87925
debd2fa29bb430709633b467612115010f3b7e31	lapped spectral decomposition for 3d triangle mesh compression	image coding;data compression;spectral representation;spectral decomposition;progressive transmission lapped spectral decomposition 3d triangle mesh compression mesh geometry processing address transmission pseudofrequential information mesh connectivity piecewise decomposition transform complexity spectral representation;data compression mesh generation image coding;progressive transmission;mesh generation;geometry processing;triangle mesh;laplace equations matrix decomposition eigenvalues and eigenfunctions information geometry collaboration art toy industry biomedical computing encoding discrete cosine transforms	Spectral decomposition of mesh geometry has first been introduced by Taubin for geometry processing purposes. It has been extended to address transmission issues by Karni & Gotsman. Such decompositions give rise to pseudo-frequential information of the geometry defined over the mesh connectivity. For large meshes a piecewise decomposition has to be applied in order to restrict the complexity of the transform. In this paper, we propose to introduce overlap for its spectral representation. We show visual gains obtained in compression and progressive transmission of mesh geometry.	geometry processing;lossless compression;spectral method;triangle mesh;tutte embedding	Patrice Rondao-Alface;Benoit M. Macq;François Cayre;Francis J. M. Schmitt;Henri Maître	2003		10.1109/ICIP.2003.1247078	data compression;mesh generation;mathematical optimization;combinatorics;computer science;theoretical computer science;triangle mesh;mathematics;matrix decomposition;t-vertices;algorithm;statistics	Vision	41.832577761972196	-18.356041854816738	88077
9d5921c2cfa803c2193265eab701f876e818142b	a comparison of fixed-point 2d 9/spl times/7 discrete wavelet transform implementations	discrete wavelet transforms;image coding;discrete wavelet transform;filter bank;data compression;tms320c6201 dsp discrete wavelet transform fixed point implementation quantization error daubechies filter bank polyphase form reduced scaling lifting scheme jpeg2000 image compression;lifting scheme;fixed point;texas instruments;channel bank filters discrete wavelet transforms data compression image coding;channel bank filters;discrete wavelet transforms digital signal processing quantization computational efficiency low pass filters filter bank instruments image processing laboratories robustness;quantization error	In this paper, we describe three 2D Discrete Wavelet Transform fixed-point implementations and compare them in terms of quantization error for the Daubechies 9x7 filter bank. The three implementations are the polyphase form, lifting scheme, and reduced scaling lifting scheme. Experimental results show that the reduced scaling lifting scheme is more robust than other schemes. Also, the numbers of cycles the implementations take on a Texas Instruments TMS320C6201 simulator are given as reference.	discrete wavelet transform;filter bank;fixed point (mathematics);fixed-point arithmetic;image scaling;lifting scheme;polyphase quadrature filter;quantization (signal processing);simulation	Hyung Cook Kim;Edward J. Delp	2002		10.1109/ICIP.2002.1038042	data compression;wavelet;mathematical optimization;harmonic wavelet transform;quantization;second-generation wavelet transform;computer science;theoretical computer science;filter bank;cascade algorithm;mathematics;fixed point;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;polyphase matrix;algorithm;wavelet transform	DB	45.33062481085051	-10.0574706245671	88149
1cf32c03e77944770435213dd7fed02ea24c1fe6	h.263 to h.264 transconding using data mining	data mining h 263 h 264;decision tree;data compression;low complexity;data mining;video coding data compression data mining decision trees learning artificial intelligence table lookup;video coding;data mining decision trees transcoding signal processing algorithms partitioning algorithms machine learning algorithms machine learning classification tree analysis automatic voltage control mpeg 4 standard;inter prediction complexity h 263 transconding h 264 transconding data mining macroblock partition mode decision algorithm inter frame prediction machine learning decision trees h 263 mc residual classification h 264 mb mode computation decision tree lookup;machine learning;data mining algorithm;h 264;learning artificial intelligence;h 263;decision trees;table lookup;high efficiency;mode decision	In this paper, we propose the use of data mining algorithms to create a macroblock partition mode decision algorithm for inter-frame prediction, to be used as part of a high-efficient H.263 to H.264 transcoder. We use machine learning tools to exploit the correlation and derive decision trees to classify the incoming H.263 MC residual into one of the several coding modes in H.264. The proposed approach reduces the H.264 MB mode computation process into a decision tree lookup with very low complexity. Experimental results show that the proposed approach reduces the inter-prediction complexity by as much as 60% while maintaining the coding efficiency.	algorithm;algorithmic efficiency;computation;data mining;decision tree;h.264/mpeg-4 avc;lookup table;machine learning;macroblock	Gerardo Fernández-Escribano;Jens Bialkowski;Hari Kalva;Pedro Cuenca;Luis Orozco-Barbosa;André Kaup	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379959	decision tree learning;computer science;machine learning;decision tree;pattern recognition;data mining	Robotics	46.92092510214682	-19.277514062196776	88225
c25f2a1575cf8ddd90b9958bb76f121207a0d2d1	integrity authentication method for jpeg images using reversible watermarking		In these days, with increasing the importance of multimedia security, various multimedia security techniques are studied. In this paper, we propose a content authentication algorithm based on reversible watermarking which supports JPEG compression commonly used for multimedia contents. After splitting image blocks, a specific authentication code for each block is extracted and embedded into the quantized coefficients on JPEG compression which are preserved against lossy processing. At a decoding process, the watermarked JPEG image is authenticated by extracting the embedded code and restored to have the original image quality. To evaluate the performance of the proposed algorithm, we analyzed image quality and compression ratio on various test images. The average PSNR value and compression ratio of the watermarked JPEG image were 33.13dB and 90.65%, respectively, whose difference with the standard JPEG compression were 2.44dB and 1.62%.	algorithm;authentication;coefficient;digital watermarking;embedded system;image quality;jpeg;lossy compression;peak signal-to-noise ratio	Hyun-Wu Jo;Dong-Gyu Yeo;Hae-Yeoun Lee	2012			computer science;digital watermarking;computer vision;artificial intelligence;jpeg;authentication	EDA	40.00947409928235	-12.028188794828639	88332
5d7ce90d1874a2988201a1aa9dba4179c945de80	a secure keyless image steganography approach for lossless rgb images	authentication;lossless rgb images;keyless steganography;text messaging;storage capacity;password authentication	This paper proposes an improved steganography approach for hiding text messages within lossless RGB images. The objective of this work is to increase the security level and to improve the storage capacity while incurring minimal degradation of the image. The security level is increased by distributing the message over the entire image instead of clustering within specific image portions, as also by including a password authentication scheme to ensure that the message can be retrieved only by the intended recipient. Storage capacity is increased by utilizing all the color channels for storing information instead of reserving one of the channels as pixel indicator. Image degradation is minimized by changing only one LSB bit per color channel for hiding the information thereby incurring the least change in the original image. Experimentations done for analyzing the storage capacity and quality degradation, establish the superiority of the proposed approach vis-à-vis contemporary existing approaches.	authentication;channel (digital image);cluster analysis;elegant degradation;least significant bit;lossless compression;password;pixel;steganography	Sankar Roy;Ranjan Parekh	2011		10.1145/1947940.1948059	computer science;internet privacy;world wide web;computer security	Vision	39.53699165979854	-12.005242684717949	88425
b78b85c3821edf435da2f659592eec8342671d5d	degradation algorithm of compressive sensing for integer dct with application to h. 264/avc	video coding compressed sensing data compression discrete cosine transforms video codecs;compressed sensing;data compression;psnr compressive sensing degradation algorithm sparsity integer 2d dct dimensionality reduction;video coding;discrete cosine transforms;video codecs;discrete cosine transforms sparse matrices degradation quantization signal compressed sensing video coding;compressible representation degradation algorithm compressive sensing h 264 avc integer 2d dct jpeg psnr performance cs approach signal acquisition sparse representation	Compressive sensing (CS) is an innovative approach for the acquisition of signals having a sparse or compressible representation in some basis. In this paper, we extend and modify the previous compression work that uses the degradation algorithm of CS for JPEG with 2-D DCT to the H.264/AVC with the integer 2-D DCT. Through simulation results we show that the complexity with the new proposed scheme can be dramatically reduced, and simultaneously to achieve better PSNR performance compared with the conventional CS approach using the 2-D DCT transform.	algorithm;compressed sensing;computer simulation;data compression;dimensionality reduction;discrete cosine transform;elegant degradation;h.264/mpeg-4 avc;image scaling;jpeg;peak signal-to-noise ratio;simulation;sparse matrix;transformation matrix	Shiunn-Jang Chern;Che-Wei Wu;Ching-Tang Hsieh	2013	2013 International Symposium on Intelligent Signal Processing and Communication Systems	10.1109/ISPACS.2013.6704519	computer vision;electronic engineering;theoretical computer science;mathematics	Arch	44.53153225721419	-16.78283765474133	88598
cb8c84b16f52bad3a7e8d83d5b7406197fb4904f	parallel dithered scalar quantization	scalar quantization		dither;quantization (signal processing)	Vivek K. Goyal	2011	CoRR		quantization;scalar multiplication;scalar	NLP	42.35126181260182	-15.190786081737553	88666
1af15c55df541558b3cf68f4cb3c5fee338ac7f8	automatic construction of nonlinear denoising filter for video coding	maximum likelihood detection nonlinear filters adaptive filters information filters encoding proposals;nonlinear filters;state of the art coding technology nonlinear denoising filter video coding h 264 avc hevc in loop filter deblocking filter sample adaptive offset adaptive loop filter evolutionary method bit rate reduction high efficiency configuration;evolutionary computation;video coding;adaptive filters;image denoising;video coding adaptive filters evolutionary computation image denoising nonlinear filters;high efficiency	Current video coding technologies such as H.264/AVC and HEVC employ in-loop filters such as deblocking filter, sample adaptive offset and adaptive loop filter. This paper aims to develop a new type of loop filter using an evolutionary method. Generated filter is not necessarily linear and, most distinctively, content-specific. Preliminary results show that proposed filter achieves the bit rate reduction of 0.62-1.01 % against HM4.0 (High Efficiency configuration), a test model of HEVC, the state-of-the-art coding technology.	data compression;deblocking filter;h.264/mpeg-4 avc;high efficiency video coding;noise reduction;nonlinear system	Seishi Takamura;Hirohisa Jozawa	2012	2012 Picture Coding Symposium	10.1109/PCS.2012.6213379	adaptive filter;computer vision;kernel adaptive filter;computer science;deblocking filter;root-raised-cosine filter;filter;control theory;filter design;half-band filter;evolutionary computation	EDA	45.74898243829918	-17.050480679633754	88808
25634e95ccc4965079c425c3a60bd4e1f6c8c95b	cooperative path management for mobile systems based on adaptive dynamic programming	dynamic programming;automobiles;online cooperative collision avoidance system cooperative path management mobile system adaptive dynamic programming multiple vehicles navigation robotics cooperative movement management risk aversion movement composition path planning problem state transition process continuous high dimensional system approximate dynamic programming path planning task approximate value function search space ordinary least squares regression mechanism scoring technique discrete system finite states;collision avoidance cooperative path planning adaptive dynamic programming;vehicles collision avoidance least squares approximations robots planning dynamic programming;dynamic programming automobiles collision avoidance;collision avoidance;cooperative path planning;adaptive dynamic programming	Path-planning for multiple vehicles is a topic relevant to many areas of research including navigation and robotics. In this paper, we present a novel approach for cooperative movement management to examine how two objects can orchestrate their movements so as to avoid collisions and retain a good chance of returning to their intended paths. When the objects recognize that they are at risk of a collision, they cooperatively change course to avoid hitting each other and return to their original course when the risk is averted. The paths are generated and selected by the system after learning the approximate reward of each movement composition, which takes into account the smoothness of paths, as well as the distances between, and velocities of, the vehicles. The path-planning problem in this research is treated as a state transition process in a continuous high-dimensional system. Adaptive (or Approximate) Dynamic Programming (ADP) is applied to solve path-planning task. In ADP, an approximate value function for the entire search space defined for the system is heuristically developed according to certain rules. The method of ordinary least squares serves as the regression mechanism to approximate the value function for the entire search space, providing a scoring technique for the discrete system featuring finite states. This paper summarizes the concept and methodologies used to implement an online cooperative collision avoidance system. Different scenarios are tested to assess the performance of the proposed algorithm.	agent-based model;approximation algorithm;bellman equation;collision detection;computer simulation;discrete system;dynamic programming;feature vector;forward–backward algorithm;heuristic;motion planning;ordinary least squares;real-time clock;real-time computing;robotics;state transition table;usb on-the-go	Qichen Wang;Chris Phillips	2013	2013 UKSim 15th International Conference on Computer Modelling and Simulation	10.1109/UKSim.2013.37	mathematical optimization;simulation;computer science;artificial intelligence	Robotics	53.04926408742272	-23.305142002023572	88942
a1498b7586f7b36bfc935c27aeeb92b9eb8c3e60	a generalized algorithm and reconfigurable architecture for efficient and scalable orthogonal approximation of dct	computational complexity;data compression;discrete cosine transforms;field programmable gate arrays;logic design;matrix decomposition;parallel architectures;reconfigurable architectures;video coding;dct basis vectors;fpga;approximation algorithms;approximation methods;arithmetic complexity;discrete cosine transform;hardware complexity;image compression;reconfigurable parallel architecture;recursive sparse matrix decomposition;scalable orthogonal approximation;video compression;algorithm-architecture codesign;dct approximation;discrete cosine transform (dct);high efficiency video coding (hevc);computer architecture;sparse matrices	Approximation of discrete cosine transform (DCT) is useful for reducing its computational complexity without significant impact on its coding performance. Most of the existing algorithms for approximation of the DCT target only the DCT of small transform lengths, and some of them are non-orthogonal. This paper presents a generalized recursive algorithm to obtain orthogonal approximation of DCT where an approximate DCT of length N could be derived from a pair of DCTs of length (N/2) at the cost of N additions for input preprocessing. We perform recursive sparse matrix decomposition and make use of the symmetries of DCT basis vectors for deriving the proposed approximation algorithm. Proposed algorithm is highly scalable for hardware as well as software implementation of DCT of higher lengths, and it can make use of the existing approximation of 8-point DCT to obtain approximate DCT of any power of two length, N > 8. We demonstrate that the proposed approximation of DCT provides comparable or better image and video compression performance than the existing approximation methods. It is shown that proposed algorithm involves lower arithmetic complexity compared with the other existing approximation algorithms. We have presented a fully scalable reconfigurable parallel architecture for the computation of approximate DCT based on the proposed algorithm. One uniquely interesting feature of the proposed design is that it could be configured for the computation of a 32-point DCT or for parallel computation of two 16-point DCTs or four 8-point DCTs with a marginal control overhead. The proposed architecture is found to offer many advantages in terms of hardware complexity, regularity and modularity. Experimental results obtained from FPGA implementation show the advantage of the proposed method.	approximation algorithm;basis (linear algebra);computation;data compression;discrete cosine transform;field-programmable gate array;image quality;marginal model;overhead (computing);parallel computing;power of two;preprocessor;recursion (computer science);scalability;sparse matrix	Maher Jridi;Ayman Alfalou;Pramod Kumar Meher	2015	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2014.2360763	parallel computing;architecture;scalability;discrete cosine transform;modularity;computer science	EDA	45.49133327892884	-10.327893934351154	89000
17b1ebd9c9ccdf1a60212710d8f94980282473af	robust h.264/avc video transmission using data partitioning and unequal loss protection	h 264 avc video transmission;error resilience techniques;propagation losses;group of pictures;data compression;video quality h 264 avc video transmission data partitioning unequal loss protection error resilience techniques video coding video packets group of pictures forward error correction fec parity codes available network bandwidth packet loss rate;parity check codes;forward error correction automatic voltage control markov processes encoding mathematical model propagation losses streaming media;available network bandwidth;h 264 avc;fec assignment optimization h 264 avc adaptive unequal loss protection;adaptive unequal loss protection;video quality;fec parity codes;data partitioning;packet loss rate;video coding;engineering and technology;teknik och teknologier;automatic voltage control;forward error correction;streaming media;video transmission;mathematical model;video coding data compression forward error correction parity check codes;markov processes;unequal loss protection;encoding;fec assignment optimization;video packets	In this work, we present an adaptive unequal loss protection (ULP) scheme for H264/AVC video transmission over lossy networks. This scheme combines erasure coding, H.264/AVC error resilience techniques and importance measures in video coding. The unequal importance of the video packets is identified in the group of pictures (GOP) and the H.264/AVC data partitioning levels. The presented method can adaptively assign unequal amount of forward error correction (FEC) parity across the video packets according to the network conditions, such as the available network bandwidth, packet loss rate and average packet burst loss length. A near optimal algorithm is developed to deal with the FEC assignment for optimization. The simulation results show that our scheme can effectively utilize network resources such as bandwidth, while improving the quality of the video transmission. In addition, the proposed ULP strategy ensures graceful degradation of the received video quality as the packet loss rate increases.	algorithm;bandwidth (signal processing);cobham's thesis;constrained optimization;constraint (mathematics);data compression;distortion;elegant degradation;erasure code;error detection and correction;fault tolerance;forward error correction;group of pictures;h.264/mpeg-4 avc;lossy compression;markov chain;markov model;mathematical optimization;network packet;optimization problem;partition (database);simulation	Xingjun Zhang;Xiaohong Peng;Scott Fowler;Dajun Wu	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.423	data compression;scalable video coding;group of pictures;real-time computing;telecommunications;computer science;video quality;mathematical model;forward error correction;markov process;context-adaptive binary arithmetic coding;encoding;statistics;computer network	Networks	48.13103878438655	-16.201895566377807	89368
793a93a2a6a9619d6ccf1c69bc144bfd7e3222ec	efficient dct-domain prefiltering inside a video encoder	video;block codes;computer programming;wiener filter;computational complexity;discrete cosine transform	Efficient implementation of pre-filtering has been an important issue in video sequence coding, since it improves coding efficiency dramatically by alleviating camera noise. Based on the approximated generalized Wiener filtering and 2D discrete cosine transform (DCT) factorization, this paper introduces a novel pre-filtering scheme that is performed inside a video encoder. The proposed pre-filtering is performed, by scaling the DCT coefficients of original image blocks for intra block coding and those of motion- compensated error blocks for inter block coding, respectively. Even though the pre-filtering operation is embedded in a video encoder, its additional computational complexity is marginal compared to the encoding process, and the overall architecture of the conventional video encoder is maintained. In spite of its simplicity, the proposed pre- filtering scheme gives good filtering and coding performance for noisy video sequences.	discrete cosine transform;encoder	Sung Deuk Kim;Jong Beom Ra	2000			computer vision;computer science;theoretical computer science;discrete cosine transform;coding tree unit;mathematics;context-adaptive binary arithmetic coding;factorization;h.261;algorithm;multiview video coding;computer graphics (images)	Vision	45.03501472075494	-17.19380726697828	89430
627b2750c2ead885557827f5f7fe49c1e6b849dd	adapting model for convergence of optimal minmax bit allocation in intra-frame video coding	quantization;rate distortion;intra frame video coding;optimal minmax bit allocation;convergence;real time;prediction algorithms;performance comparison;operational rate distortion;bit rate;convergence minimax techniques bit rate video coding rate distortion dynamic programming broadband communication information technology africa videoconference;video coding;adapting model algorithm;adaptation model;adapting model algorithm optimal minmax bit allocation intra frame video coding real time video encoders dependent macroblocks operational rate distortion convex hull power law function power law prediction bisection method switches;rate distortion video coding optimal bit allocation;bisection method;bit allocation;dependent macroblocks;power law;real time video encoders;power law prediction;convex hull;switches;encoding;video coding convergence;operational rate distortion convex hull;optimal bit allocation;algorithm design and analysis;power law function	The application of optimal bit allocation in real-time video encoders is a challenging prospect for dependent macroblocks (MBs) where a quantiser is ascribed to each MB within a frame. The operational rate-distortion (R-D) convex hull of INTRA encoded frames may be approximated with a power-law function. However, discontinuities on the surface produce local divergence and therefore an adapting model is proposed that switches between power-law prediction and the bisection method to impose convergence. A performance comparison is made between the bisection method and this adapting model algorithm as applied to the H.263+ video encoder.	approximation algorithm;bisection method;convex hull;data compression;distortion;encoder;macroblock;minimax;network switch;quantization (signal processing);real-time clock	Keith L. Ferguson	2008	2008 Third International Conference on Broadband Communications, Information Technology & Biomedical Applications	10.1109/BROADCOM.2008.16	mathematical optimization;real-time computing;theoretical computer science;mathematics	Robotics	47.178239101398184	-16.829913122805273	89474
3ea073a266420636010e4b1b8a629131fb61005c	modeling and analysis of vector-quantized m-channel subband codecs	quantization;model specification;mse;electronic mail;image coding;codecs;filter bank;optimum fir filter coefficients;band pass filters;gain plus additive noise;scalar nonlinear quantization model;finite impulse response filter;simulation;additive noise;mean square;distortion measurement;two band paraunitary structure;correlation methods;bit rate;optimum lbg vector quantizer;monte carlo simulation vector quantized m channel subband codecs gain plus additive noise scalar nonlinear quantization model accuracy analytic model quantization errors simulation optimum lbg vector quantizer mean squared reconstruction error mse codebook codeword length filter bank coefficients optimum fir filter coefficients bit rate input signal correlation model 4 tap filter two band paraunitary structure;accuracy;band structure;filter bank coefficients;computational modeling;scalar quantization;codebook;mean squared reconstruction error;image reconstruction;fir filter;4 tap filter;codecs quantization filter bank finite impulse response filter bit rate image coding distortion measurement electronic mail computational modeling image reconstruction;quantization errors;signal reconstruction;vector quantizer;fir filters;analytic model;vector quantisation;codeword length;monte carlo simulation;input signal correlation model;vector quantized m channel subband codecs;filtering theory;analytical model;noise;quantization error;modeling and analysis;correlation methods codecs noise band pass filters vector quantisation filtering theory signal reconstruction fir filters	This paper demonstrates that the scalar nonlinear gain-plus-additive noise quantization model can be used to represent each vector quantizer in an M-band subband codec. The validity and accuracy of this analytic model is confirmed by comparing the calculated model quantization errors with actual simulation of the optimum LBG vector quantizer. We compute the mean squared reconstruction error (MSE) which depends on N the number of entries in each codebook, k the length of each codeword, and on the filter bank coefficients. We form this MSE measure in terms of the equivalent scalar quantization model and find the optimum FIR filter coefficients for each channel in the M-band structure for a given bit rate, given filter length, and given input signal correlation model. Specific design examples are worked out for a 4-tap filter in a two-band paraunitary structure. Theoretical results are confirmed by extensive Monte Carlo simulation.	codec	Innho Jee;Richard A. Haddad	1995		10.1109/ICASSP.1995.480483	mathematical optimization;speech recognition;quantization;computer science;finite impulse response;mathematics;statistics	Logic	48.28870526817507	-11.669559161091069	89537
59aa025f268c78d58779c550d133f4a7bdebd2fb	reversible 2d 9-7 dwt based on non-separable 2d lifting structure compatible with irreversible dwt	wavelet;lossless;compatible;image;coding	This paper proposes a reversible two dimensional (2D) discrete wavelet transform (DWT) for lossless coding which is compatible with the irreversible 2D 9-7 DWT for lossy coding in the JPEG 2000. Since all the filters and scalings are factorized into a product of lifting steps, and signal values are rounded into integers, the proposed DWT is reversible and applicable to lossless coding of 2D signals. We replace a part of the separable 2D transfer function of the 2D DWT by a non separable 2D lifting structure, so that the number of rounding operations is decreased. We also investigate performance of the DWT under octave decomposition case and theoretically endorse it. As a result, reduction of the rounding errors due to the replacement was confirmed. It means that compatibility of the reversible DWT to the irreversible 2D 9-7 DWT is improved. key words: lossless, image, wavelet, coding, compatible	discrete wavelet transform;jpeg 2000;lifting scheme;lossless compression;lossy compression;round-off error;rounding;transfer function	Masahiro Iwahashi;Hitoshi Kiya	2011	IEICE Transactions		wavelet;discrete mathematics;second-generation wavelet transform;theoretical computer science;image;mathematics;lossless compression;coding;statistics	Robotics	42.820570008524925	-14.656540774981618	89650
a5a7e483f0e0d70de26504fe890fc200e80df630	fast mvc prediction structure selection for interactive multiview video streaming	exhaustive searching benchmark multiview video coding mvc prediction structure selection interactive multiview video streaming imvs spatial correlation temporal correlation interview correlation prediction coding dependency interview prediction structure ps visual distortion constraint user interactive behavior model computational complexity that;video streaming;video streaming computational complexity correlation methods interactive video video coding;interactive video;correlation methods;encoding quantization signal;video coding;view popularity model interactive multiview video streaming imvs multiview video coding mvc optimal prediction structure;computational complexity	Multiview Video Coding (MVC) has been developed to efficiently compress a set of camera views by exploiting the spatial, temporal and interview correlations among images of the same scene. However, the resulting compressed data has a lot of prediction coding dependencies, which may not suit interactive multiview video streaming (IMVS) systems, where only one view is requested at a time by the end-user. This paper proposes a fast selection mechanism for effective interview prediction structure (PS) in IMVS while minimizing the point-to-point transmission rate, given some storage and visual distortion constraints, and a user interactive behavior model. Simulation results show that our novel fast MVC PS selection algorithm has high efficiency with low computational complexity that is reduced by more than 40% in comparison to the exhaustive searching benchmark.	behavior model;benchmark (computing);brute-force search;computational complexity theory;data compression;distortion;mathematical optimization;model–view–controller;multiview video coding;point-to-point protocol;selection algorithm;simulation;streaming media	Ana De Abreu;Pascal Frossard;Fernando da Cruz Pereira	2013	2013 Picture Coding Symposium (PCS)	10.1109/PCS.2013.6737710	computer vision;computer science;video quality;theoretical computer science;video tracking;multimedia;computational complexity theory;algorithm;multiview video coding	AI	46.35818754707088	-20.668644537947962	89671
6c54fb4f5cb727ad8a5e4626f7707aeeb9a920c5	a method of watermarking with multiresolution analysis and pseudo noise sequences	multiresolution analysis;watermarking		digital watermarking;multiresolution analysis	Junji Onishi;Kineo Matsui	1998	Systems and Computers in Japan	10.1002/(SICI)1520-684X(199805)29:5%3C11::AID-SCJ2%3E3.0.CO;2-Q	multiresolution analysis;speech recognition;telecommunications;digital watermarking;computer science;theoretical computer science;algorithm	EDA	42.21674403777884	-11.802767213582422	90073
79ad29f74e874279e4a5bf7d2bb3fb08f153ba92	some improvements to hvs models for fingerprinting in perceptual decompressors	institutional repositories;experimental tests;luminance masking;image compression fingerprinting perceptual decompressors human visual system models wavelet decompressors luminance masking texture masking insertion gain energy distribution;watermarking;quantization;perceptual decompressors;wavelet decompressors;image coding;fedora;reference model;copy protection;testing;transform coding;texture masking;vital;image texture;transform coding copy protection brightness visual perception wavelet transforms image coding image texture;wavelet transforms;brightness;image compression;fingerprinting;fingerprint recognition;human visual system;energy distribution;human visual system models;fingerprint recognition watermarking frequency humans visual system testing wavelet domain signal resolution quantization wavelet coefficients;signal resolution;visual perception;humans;weight function;vtls;wavelet domain;frequency;visual system;wavelet coefficients;ils;insertion gain	Benoit Macq has made a number of original contributions to the design of algorithms for visual communications systems, and his work has inspired many researchers and made significant industrial impact. His contributions span a range of areas, including video analysis, multiresolution analysis, morpholological descripti ons, and hardware implementations. He is best known for his pioneering work on image c oding and watermarking based on human vision perceptual models as well as for innovative work on watermarking of 3-D meshes, and more recently, key contr ibutions to the digital cinema initiative.	algorithm;cinema 4d;data compression;digital watermarking;fingerprint (computing);human visual system model;multiresolution analysis;video content analysis	Marc Bertran;Jean-François Delaigle;Benoit M. Macq	2001		10.1109/ICIP.2001.958304	image texture;fingerprint;computer vision;transform coding;weight function;reference model;speech recognition;visual system;quantization;visual perception;digital watermarking;image compression;computer science;insertion gain;frequency;software testing;human visual system model;fingerprint recognition;brightness;wavelet transform;computer graphics (images)	Graphics	41.55089155546563	-16.030746404592282	90210
f243e265cb7864db3a14c4e5aaeda754a2d581e3	derivation of prediction equations for blocking effect reduction	mean squared difference of slopes prediction equations blocking effect reduction heuristic optimization problem low bit rate coded images global minimum computational complexity discrete cosine transform coefficients jpeg block smoothing method experimental results dct;equation prediction;heuristic;quadratic programming;quantization;filtering;optimisation;transformation cosinus;transformacion discreta;image coding;global minimum;image processing;optimizacion;decoding;complexite calcul;mean squared difference of slopes;smoothing method;prediction equation;equations discrete cosine transforms transform coding smoothing methods low pass filters filtering quantization quadratic programming decoding computational complexity;minimum global;compresion senal;procesamiento imagen;transform coding;indexing terms;traitement image;discrete cosine transform;compression signal;experimental result;optimization problem;complejidad computacion;smoothing methods;prediction theory;prediction equations;jpeg;computational complexity;discrete cosine transforms;smoothing;smoothing methods prediction theory image coding computational complexity optimisation discrete cosine transforms transform coding;signal compression;image sequence;transformacion coseno;discrete transformation;alisamiento;resultado experimental;block smoothing method;low bit rate coded images;secuencia imagen;optimization;low pass filters;discrete cosine transform coefficients;cosine transform;blocking effect reduction;resultat experimental;experimental results;blocking effect;transformation discrete;effet blocage;lissage;dct;sequence image	A heuristic is proposed by Minami and Zakhor (see IEEE Trans. Circuits Syst. Video Technol., vol.5, p.74-82, 1995) to solve an optimization problem on reduction of blocking effects seen in low bit-rate coded images. This paper presents a novel solution to the same problem. Not only does it compute the global minimum, but its computational complexity is significantly smaller as well. The solution is stated in the form of prediction equations for computation of certain discrete cosine transform coefficients-an approach adapted by JPEG for its block-smoothing method. Experimental results show that our solution outperforms the JPEG's method.	blocking (computing)	Gopal Lakhani;Norman Zhong	1999	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.754770	mathematical optimization;discrete mathematics;heuristic;image processing;computer science;theoretical computer science;discrete cosine transform;mathematics;quadratic programming;algorithm	Arch	46.82773863625943	-15.898782671303456	90378
a44e14263685f0f53578079c845ed215dbe0e3c7	fast vector quantization image coding by mean value predictive algorithm	quantization;image coding;computer programming;distortion;image compression;vector quantizer;value prediction	of d en3 a Abstract. Vector quantization (VQ) is an effective technology for signal compression. In traditional VQ, most of the computation concentrates on searching the nearest codeword in the codebook for each input vector. We propose a fast VQ algorithm to reduce the encoding time. There are two main parts in our proposed algorithm. One is the preprocessing process and the other is the practical encoding process. In preprocessing, we will generate some tables that we need to employ for practical encoding. Because those tables are used for all the images, the time to generate these tables does not increase any time in the practical encoding process. On the second part, the practical encoding process, we use the tables generated previously and other techniques to speed up the encoding time. This paper provides an effective algorithm to accelerate the encoding time. The proposed algorithm demonstrates the outstanding performance in terms of time saving and arithmetic operations. Compared to a full search algorithm, it saves more than 95% searching time. © 2004 SPIE and IS&T. [DOI: 10.1117/1.1666877]	code word;codebook;computation;preprocessor;search algorithm;signal compression;vector quantization	Yung-Gi Wu;Kuo-Lun Fan	2004	J. Electronic Imaging	10.1117/1.1666877	computer vision;learning vector quantization;distortion;quantization;harmonic vector excitation coding;image compression;computer science;theoretical computer science;pattern recognition;computer programming;fractal transform;linde–buzo–gray algorithm;quantization;vector quantization;algorithm	SE	43.23692277792203	-14.346892626045669	90468
9ffa42210305b7696c37df5d2f4c4aea6ab2d269	shape-adaptive dct algorithm - hardware optimized redesign	video object;rate distortion;concepcion circuito;transformation cosinus;transformacion discreta;image processing;algoritmo adaptativo;circuit design;procesamiento imagen;traitement image;video codec;adaptive algorithm;codificacion;algorithme adaptatif;senal video;signal video;transformacion coseno;coding;discrete transformation;video signal;codec;hardware design;conception circuit;cosine transform;transformation discrete;hardware implementation;codage	This article refers to the shape-adaptive DCT ( SA-DCT ) algorithm developed by Sikora and Makai in 1995. It is an important tool for encoding texture of arbitrary shaped video objects and can be included in MPEG-4 video codecs. In this paper a modification of normalized version SA-DCT redesigned for intraframe coding is presented. Simulations results show that this solution outperforms standard SA-DCT in rate-distortion sense. Efficiency is close to improved version SA-DCT for intraframe coding, known as DDC-SA-DCT. But computational overhead is smaller than for DDC-SA-DCT. Therefore, this solution may be attractive for hardware implementations.		Krzysztof Mroczek	2001		10.1007/3-540-44692-3_16	computer vision;codec;image processing;computer science;theoretical computer science;circuit design;discrete cosine transform;coding;statistics;computer graphics (images)	EDA	45.94837230529336	-14.074586766626688	90477
00c23407515faeebeb7dee6afdfdf17e5a6e9dca	bridging the gap between discrete symbolic planning and optimization-based robot control	semantics;runtime;grippers robots semantics monitoring runtime linear programming planning;monitoring;motion control discrete symbolic planning optimization based robot control open drawer scenario constraint optimization;robots;grippers;linear programming;path planning mobile robots motion control optimisation;planning	Symbolic reasoners generate plans which are often not exploiting the robot capabilities and are sensitive to runtime disturbances. This work proposes a scheduler as an interface between a discrete, symbolic plan and a motion control based on constraint optimization. Acting as a local reasoner, the scheduler valuates a set of predicates to decide when an action will be executed. Given a task specification which describes how the action should be realized, the scheduler configures the controller at runtime. A demonstration will be provided considering an “open drawer” scenario.	bridging (networking);cops (software);constrained optimization;context awareness;mathematical optimization;optimization problem;robot control;run time (program lifecycle phase);scheduling (computing);semantic reasoner;symbolic computation	Enea Scioni;Gianni Borghesan;Herman Bruyninckx;Marcello Bonfé	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139905	planning;robot;control engineering;real-time computing;simulation;computer science;linear programming;artificial intelligence;semantics	Robotics	53.06646665454141	-21.06368760432491	90487
c5a3c3a6ee70446951ae3140a3b0f2fdc4a36afc	motion-compensated wavelet packet zerotree video coding on multicomputers	transformation ondelette;distributed memory;algoritmo paralelo;donnee experimentale;distribution donnee;image coding;parallel algorithm;shared memory;algorithm analysis;motion compensation;memoria compartida;architecture memoire;subband decomposition;advanced video coding;dato experimental;technique video;paralelisacion;tecnica video;wavelet packet;data distribution;algorithme parallele;motion compensated;compensation mouvement;codage image;video coding;calculateur mimd;codage video;descomposicion subbanda;memory architecture;mimd architectures;parallelisation;wavelet packet decomposition;parallelization;video technique;analyse algorithme;transformacion ondita;decomposition sous bande;wavelet packets;memoire repartie;analisis algoritmo;wavelet transformation;mimd computer;memoire partagee;parallel algorithms	In this work we describe and analyze algorithms for advanced video coding on distributed memory MIMD architectures. In particular, we consider a wavelet packet based codec using the concept of zerotree encoding. The main contribution of this work is the design of a parallel motion-compensated video coder composed of a wavelet packet decomposition in conjunction with the best basis algorithm followed by zerotree coding. Whereas two sensible parallelization techniques can be employed for the wavelet packet decomposition (subband based partitioning and stripe partitioning), the zerotree coding and motion compensation stages only allow one reasonable parallelization method (stripe partitioning). We investigate the advantages and drawbacks of the resulting different overall data distribution strategies and show experimental results obtained on a Siemens hpcLine cluster and a Cray T3E.	data compression;wavelet	Manfred Feil;Andreas Uhl	2003	Journal of Systems Architecture	10.1016/S1383-7621(03)00058-4	parallel computing;computer science;theoretical computer science;parallel algorithm;wavelet packet decomposition;computer graphics (images)	Networks	46.28026438068778	-14.032686087373031	91063
ca00415b760f9473de2e990f0a0d09c32f90ee2f	comparative performance analysis of optimization techniques on vector quantization for image compression		Linde-Buzo-Gray (LBG) Vector Quantization (VQ), technically generates local codebook after many runs on different sets of training images for image compression. The key role of VQ is to generate global codebook. In this paper, we present comparative performance analysis of different optimization techniques. Firefly and Cuckoo search generate a near global codebook, but undergoes problem when non-availability of brighter fireflies and convergence time is very high respectively. Hybrid Cuckoo Search (HCS) algorithm was developed and tested on four benchmark functions, that optimizes the LBG codebook with less convergence rate by taking McCulloch’s algorithm based levy flight and variant of searching parameters. Practically, we observed that Bat algorithm (BA) peak signal to noise ratio is better than LBG, FA, CS and HCS in between 8 to 256 codebook sizes. The convergence time of BA is 2.4452, 2.734 and 1.5126 times faster than HCS, CS and FA respectively. KeywORdS Bat Algorithm (BA), Cuckoo Search Algorithm (CS), Firefly Algorithm (FA), Hybrid Cuckoo Search Algorithm (HCS), Linde-Buzo-Gray (LBG), Vector Quantization		Karri Chiranjeevi;Umaranjan Jena;Sonali Dash	2017	IJCVIP	10.4018/IJCVIP.2017010102	fractal transform;linde–buzo–gray algorithm;texture compression;quantization;vector quantization	Vision	43.65866856311034	-12.9993334961763	91193
706d90df58227ebaf2b0ef4e9eab7c94de99ccd9	order statistics preserving near-lossless image coding	image coding;order statistic;approximation theory data compression image coding statistical analysis quantisation signal;data compression;lossless image compression;quantisation signal;approximation theory;statistical analysis;statistics image coding biomedical imaging pixel image reconstruction image edge detection space technology multimedia systems computer science psnr;scientific communication;quantization near lossless image coding image compression order statistics preserving near lossless coding psnr context based fidelity measure medical community space community scientific community l sub spl infin approximation	We introduce a new concept of near-lossless image compression called order statistics preserving (OSP) near-lossless coding. Unlike ubiquitous L/sub 2/ (PSNR) and common near-lossless criterion of L/sub /spl infin// the OSP is a context-based fidelity measure that can meet more stringent requirements of high-end users in medical, space, and scientific communities.	lossless compression	Xiaolin Wu;Xuehong Li;Tong Qiu	1998		10.1109/MMSP.1998.738974	data compression;lossy compression;computer vision;order statistic;image compression;theoretical computer science;context-adaptive variable-length coding;mathematics;lossless compression;tunstall coding;context-adaptive binary arithmetic coding;statistics;approximation theory	Vision	48.90844514486343	-13.412726020740612	91235
fdb36182b02b0c2ba94e2a03c96967c665052673	a study about the relationship between frame quality and single video quality	tecnologia electronica telecomunicaciones;image coding;video quality;time series;temporal information;mos;single video quality;digital video;tecnologias;grupo a;subjective assessment;video quality assessment	Digital video encapsulates the time series of a frame (still) images, where overall video quality can be obtained by using the quality of each frame image and the temporal information between the frame image. Coding of video produces degradation of these two types of information. These degradations can be classified as spatial degradation (static degradation) of a frame images and temporal degradation between frame image (dynamic degradation). In the framework of video quality evaluation it is necessary to consider those degradations, because their contents are strongly interdependable and quantification is problematic for these degradations. Therefore, the development of an objective video quality assessment method for single video quality requires to investigate how much static degradation and dynamic degradation affect single video quality. In this research, single video quality was predicted highly accuratly by using frame quality as static degradation and frame rate information as dynamic degradation.		Yoshikazu Kawayoke;Yuukou Horita	2008	IEICE Transactions	10.1093/ietfec/e91-a.6.1443	video compression picture types;reference frame;subjective video quality;computer vision;simulation;computer science;video quality;machine learning;time series;video tracking;mathematics;block-matching algorithm;multimedia;video processing;rate–distortion optimization;motion compensation;video post-processing;statistics;multiview video coding	Arch	45.49569831000857	-21.20299297819871	91377
20f37432b2bf7c912f6f5f0198add325c0c744fd	information improvement and computation of the rate-distortion function under true and inaccurate input distributions			computation;distortion;rate–distortion theory	Yogeshwar Dayal Mathur	1974	Elektronische Informationsverarbeitung und Kybernetik		discrete mathematics;computation;mathematics;distortion function;mathematical optimization	ML	51.21529510029171	-15.095976218912597	91488
3030bb4845146a3a1c7d3b392336f6dbf9f0ebdf	high-compression of chrominance data by use of segmentation of luminance	image segmentation;vector quantization;transform coding;image reconstruction;encoding	The paper describes a very simple technique for compression of chrominance data in colour images and video sequences coded at low bit rates. The technique is based on coding of arbitrarily shaped regions, whereby each region is efficiently represented by one value of chrominance which is averaged over the whole region. As the regions are determined on the basis of reconstructed luminance component, no information upon the segment shapes has to be transmitted. In order to obtain high compression, the set of assigned pairs of chrominance is processed by vector quantization. Experimental results prove high efficiency of the proposed technique.	vector quantization	Maciej Bartkowiak;Marek Domanski	2000	2000 10th European Signal Processing Conference		image texture;color cell compression;computer vision;pattern recognition;mathematics;quantization;vector quantization;computer graphics (images)	Robotics	43.73642340391705	-14.428498719619636	91490
3e62171c1c7762a12e042134b7d0d8fff09e947e	block-adaptive palette-based prediction for depth map coding	temporal correlation;image coding;decoding;video signal processing;edge detection;3d video block adaptive palette based prediction depth map coding representative depth values temporal correlations spatial correlations;depth map coding;bit rate;three dimensional;shape image coding encoding image edge detection bit rate decoding three dimensional displays;shape coding;3d video depth map coding palette based prediction shape coding;shape;image edge detection;three dimensional displays;palette based prediction;depth map;encoding;3d video;video signal processing image coding	This paper proposes a novel prediction scheme for depth map coding. We utilize the fact that depth values are largely dependent on objects and one small block consists of only a small number of objects at most. The proposed method approximates each block with one palette and one object shape map. The palette consists of two representative depth values for foreground object and background object in the target block. The object shape map expresses which object exists at each pixel. These data are encoded only at the blocks where the proposed method is used. In order to reduce the bitrates required to transfer palettes and object shape maps, the proposed method enables their prediction by utilizing spatial and temporal correlations. Experiments are conducted on three kinds of depth maps; estimated from multiview images, captured by special sensors, and generated by computer. The results show that the proposed method reduces the bitrate by up to 40% and about 20% on average for 13 sequences relative to the MPEG-4 AVC/H.264.	depth map;experiment;glossary of computer graphics;h.264/mpeg-4 avc;list of software palettes;palette (computing);pixel;sensor;shape context	Shinya Shimizu;Hideaki Kimata;Shiori Sugimoto;Norihiko Matsuura	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6115640	three-dimensional space;computer vision;edge detection;shape;computer science;mathematics;multimedia;encoding;depth map;computer graphics (images)	Robotics	44.2161356900138	-18.790726454571335	91550
4a50c5adf338de09d8037865cbf469bc02245358	real-time motion planning in changing environments using topology-based encoding of past knowledge		Trajectory planning and replanning in complex environments often reuses very little information from the previous solutions. This is particularly evident when the motion is repeated multiple times with only a limited amount of variation between each run. To address this issue, we propose the DRM-connect algorithm, a combination of dynamic reachability maps (DRM) with lazy collision checking and a fallback strategy based on the RRT-connect algorithm which is used to repair the roadmap through further exploration. This fallback allows us to use much sparser roadmaps. Furthermore, we investigate using an approximate Reeb graph to capture the topology-persistent features of the past solutions of the problem utilising this sparsity. We evaluate DRM-connect with a Reeb graph on reaching tasks, and we compare it to state-of-the-art methods. We show that the proposed method outperforms both RRT-connect and BKPIECE algorithms in the number of collision checks required and we show that our method has the potential to scale to systems with higher number degrees of freedom.		Richard Fisher;Benjamin Rosman;Vladimir Ivan	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593879	computer vision;artificial intelligence;collision;motion planning;robot;theoretical computer science;encoding (memory);maintenance engineering;computer science;reachability;trajectory;reeb graph	Robotics	52.80042461678378	-23.709129309217037	91675
353fbdcae6bd1341f2c2173845cd97bb6dab38d4	a rate-constrained key-frame extraction scheme for channel-aware video streaming	rate constrained;video coding rate constrained key frame extraction channel aware video streaming adaptive key frame selection realtime video streaming channel estimation two step sequential key frame selection scheme video shots video clip feature extraction metadata realtime streaming transcoding complexity;video streaming;error protection;channel aware video streaming;video signal processing;channel estimation;streaming media multimedia systems data mining content based retrieval time factors computer science application software transcoding video coding ip networks;video coding;feature extraction;multimedia communication;transcoding;video signal processing video streaming multimedia communication feature extraction channel estimation transcoding video coding	The paper presents an adaptive rate-constrained key-frame selection scheme for channel-aware realtime video streaming applications. The proposed method dynamically determines the target number of key-frames by estimating the channel conditions according to feedback information. A two-step sequential key-frame selection scheme is then utilized to select a target number of key-frames by first finding the optimal allocation of the key-frame budget among the video shots in a video clip using an analytical model, and then selecting most representative key-frames in each shot according to the allocation. The feature information used for key-frame selection is extracted offline and stored in the server as metadata for realtime streaming and transcoding. Experimental results show that the proposed method can achieve good performance with acceptable complexity.	key frame;mathematical optimization;online and offline;server (computing);single-access key;streaming media;video clip	Yu-Hsuan Ho;Wei-Ren Chen;Chia-Wen Lin	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1418829	video compression picture types;scalable video coding;real-time computing;transcoding;h.263;uncompressed video;feature extraction;computer science;video capture;machine learning;video tracking;block-matching algorithm;multimedia;video processing;smacker video;motion compensation;video post-processing;world wide web;h.261;video denoising;multiview video coding	Robotics	44.74726756605055	-23.27284321549215	91688
247666368c786fa51cc56010ba4b4d53d79f46e0	rate-distortion optimized streaming for 3-d wavelet video	rate distortion;optimisation;video streaming;motion compensation;video quality;convex optimization;rate distortion theory;wavelet transforms;video coding;image reconstruction;rate distortion optimization;motion compensation 3d wavelet video streaming rate distortion optimized framework reconstructed frame convex optimization heuristic scheme video coding;rate distortion streaming media video coding scalability discrete wavelet transforms wavelet analysis video sequences wavelet transforms wavelet coefficients motion compensation;optimisation video streaming rate distortion theory wavelet transforms image reconstruction motion compensation video coding	We propose a rate-distortion optimized framework to stream scalable bitstreams of 3-D wavelet video stored at the sender to a remote receiver. Based on the source rate-distortion profiles, the desired playout deadline, transmission rate, and the network characteristics, the receiver issues customized requests throughout the video playout session in order to retrieve the data that minimize the distortion in the reconstructed frames. Rate-distortion optimized data request is formulated as a convex optimization problem. Experimental results show that the proposed scheme improves the video quality by up to 3.1 dB over the heuristic scheme at the same rate, or correspondingly reduces the required the transmission rate by up to 60% for the same quality.	convex optimization;decibel;distortion;heuristic;mathematical optimization;optimization problem;playout;scalability;wavelet	Chuo-Ling Chang;Sangeun Han;Bernd Girod	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421779	video compression picture types;iterative reconstruction;scalable video coding;computer vision;mathematical optimization;convex optimization;rate–distortion theory;computer science;video quality;theoretical computer science;machine learning;mathematics;block-matching algorithm;wavelet packet decomposition;rate–distortion optimization;motion compensation;video denoising;multiview video coding;wavelet transform	EDA	47.137379212284806	-16.669091533909032	91704
1736dd2edeb0fb8a261142edc27f64a99a1fdeb6	a traceable content-adaptive fingerprinting for multimedia	neural network;neural nets;image reconstruction;watermarking;fingerprint identification;feature extraction	This paper presents a feasible system of multimedia fingerprinting. A c-TA code which can be constructed flexibly is used. To ensure the applicability of both the maximum collusion size and the maximum number of consumers, the fingerprint length should be considerably increased. We design a content-adaptive watermarking scheme using neural networks to adaptively embed the long-length fingerprint without influencing the imperceptibility significantly. Experimental results show the high detection ratio of traitor tracing.	artificial neural network;correctness (computer science);digital watermarking;fingerprint (computing);peak signal-to-noise ratio;traceability;traitor tracing	Yu-Tzu Lin;Ja-Ling Wu	2004	2004 International Conference on Image Processing, 2004. ICIP '04.		iterative reconstruction;fingerprint;feature extraction;digital watermarking;computer science;theoretical computer science;machine learning;internet privacy;computer security;artificial neural network	EDA	39.80846980359429	-12.023998717662357	91732
6c6985b1e0fdcd290f4845098a4052a9c11834b7	lossy to lossless object-based coding of 3-d mri data	discrete wavelet transforms;nuclear magnetic resonance imaging;lossless object based coding;image tridimensionnelle;discrete wavelet transforms biomedical mri medical image processing image coding decorrelation;evaluation performance;head magnetic resonance images lossy object based coding lossless object based coding 3 d mri data diagnostic relevance volumetric data rate allocation 3 d discrete wavelet transform lifting steps scheme integer to integer values object based inverse transform disjoint segments bitstream embedded zerotree coding ezw 3d multidimensional layered zero coding nmzq region of interest based processing roi based processing region boundaries filter length decomposition depth;lts5;disjoint segments;3 d mri data;image coding;discrete wavelet transform;imagineria rmn;performance evaluation;image processing;bitstream;evaluacion prestacion;lossy object based coding;information filtering;volumetric data;procesamiento imagen;transformation ondelette discrete;indexing terms;analyse multiresolution;head magnetic resonance images;traitement image;magnetic resonance image;three dimensional;embedded zerotree coding;decomposition depth;ezw 3d;magnetic resonance imaging discrete wavelet transforms decorrelation discrete transforms image reconstruction multidimensional systems information filtering information filters magnetic separation head;codage image;compression image;image compression;discrete transforms;magnetic separation;integer to integer values;region boundaries;rate allocation;image reconstruction;medical image processing;region of interest;magnetic resonance imaging;ivrg;object based inverse transform;3 d discrete wavelet transform;multiresolution;tridimensional image;decorrelation;objects;imagerie rmn;head;roi based processing;region of interest based processing;lts1;filter length;multidimensional layered zero coding;information filters;nmzq;multiresolution analysis;lifting steps scheme;multidimensional systems;analisis multiresolucion	We propose a fully three-dimensional (3-D) object-based coding system exploiting the diagnostic relevance of the different regions of the volumetric data for rate allocation. The data are first decorrelated via a 3-D discrete wavelet transform. The implementation via the lifting steps scheme allows to map integer-to-integer values, enabling lossless coding, and facilitates the definition of the object-based inverse transform. The coding process assigns disjoint segments of the bitstream to the different objects, which can be independently accessed and reconstructed at any up-to-lossless quality. Two fully 3-D coding strategies are considered: embedded zerotree coding (EZW-3D) and multidimensional layered zero coding (MLZC), both generalized for region of interest (ROI)-based processing. In order to avoid artifacts along region boundaries, some extra coefficients must be encoded for each object. This gives rise to an overheading of the bitstream with respect to the case where the volume is encoded as a whole. The amount of such extra information depends on both the filter length and the decomposition depth. The system is characterized on a set of head magnetic resonance images. Results show that MLZC and EZW-3D have competitive performances. In particular, the best MLZC mode outperforms the others state-of-the-art techniques on one of the datasets for which results are available in the literature.	3d computer graphics;allocation;arabic numeral 0;bitstream;code system;coefficient;discrete wavelet transform;embedded system;embedding;integer (number);lifting scheme;lossless compression;lossy compression;morphologic artifacts;object-based language;open reading frames;performance;physical object;region of interest;relevance;resonance;anatomical layer	Gloria Menegaz;Jean-Philippe Thiran	2002	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2002.802525	iterative reconstruction;multiresolution analysis;sub-band coding;three-dimensional space;computer vision;speech recognition;index term;decorrelation;multidimensional systems;image compression;variable-length code;computer science;object;theoretical computer science;context-adaptive variable-length coding;magnetic resonance imaging;coding tree unit;mathematics;context-adaptive binary arithmetic coding;discrete wavelet transform;head;bitstream;region of interest	Visualization	44.95074272811209	-15.007271351619814	91747
4a7e477cf893ba1f25cea5374a41133e836fe67b	dynamic selection and effective compression of key frames for video abstraction	cluster algorithm;motion compensation;motion compensated;key frame selection;clustering and video abstraction	This paper reports on a new key frame based video abstraction method. With our method, a video sequence is first segmented into a number of video shots. Several key frames are selected in each shot using a dynamic selection technique. For these key frames, a motion-based clustering algorithm is applied so that key frames in the same cluster are alike in sense of motion compensation error, while those from different clusters are quit dissimilar. Then a novel cluster-based coding scheme is developed for efficient representation of the key frames. Simulations show that the proposed method can select key frames according to the dynamics of a video sequence and abstract the video with different levels of scalability.	key frame	Xu-Dong Zhang;Tie-Yan Liu;Kwok-Tung Lo;Jian Feng	2003	Pattern Recognition Letters	10.1016/S0167-8655(02)00391-4	video compression picture types;reference frame;inter frame;computer vision;quarter-pixel motion;computer science;theoretical computer science;video tracking;motion estimation;block-matching algorithm;multimedia;motion compensation;algorithm;multiview video coding	Vision	47.8402177925917	-20.60687303836418	91793
39f0303a0e80b5733f4a6e4150112e16a13f6a73	a low-complexity intra prediction mode selection algorithm in hevc	image coding;complexity theory;prediction algorithms;prediction algorithms encoding algorithm design and analysis complexity theory correlation image coding;correlation;encoding;algorithm design and analysis;35 intra prediction modes in high efficiency video	35 intra-prediction modes in High Efficiency VideoCoding (HEVC) make the prediction image more accurate andsignificantly improve the coding image quality, but numerousintra-prediction modes results in a remarkable rise incomputational complexity compared with H.264.In order toreduce computational complexity, a low-complexity HEVC intracomputation method that utilizes image textural featureinformation was proposed in this paper. Because of thesimilarities of neighboring pixels in an image, Sobel operator isused to predict the texture direction of the image, which reducesthe 35 prediction modes of prediction unit (PU) to no more than4 modes. Compared with the standard HM16.0 algorithm,experimental results show that the proposed algorithm decreasesthe encoding time by approximately 30.6% with minor loss ofcoding efficiency.	computational complexity theory;high efficiency video coding;ibm systems network architecture;image quality;pixel;selection algorithm;sobel operator	Wentao Yang;Yiming Pi	2016	2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC-PICom-DataCom-CyberSciTec.2016.170	computer vision;computer science;theoretical computer science;machine learning	Vision	46.45162118848685	-19.278135180155516	92010
0fdd366cda5b26a70747bb82937e2e5c6556d448	an efficient packetization algorithm for jpeg2000	image coding;entropy coding packetization algorithm jpeg2000 post compression rate allocation still image coding computational efficiency d heap data structure jasper;data compression;efficient algorithm;iso standards;image coding transform coding iso standards wavelet transforms streaming media computer science computational efficiency image resolution rate distortion sun;transform coding;wavelet transforms;computational complexity;data structures;rate allocation;entropy codes;telecommunication standards;computational efficiency;data structure;entropy codes image coding data compression computational complexity data structures iso standards telecommunication standards wavelet transforms transform coding	This paper presents an efficient algorithm for post compress ion optimal rate allocation and packetization within JPEG2000 en coding. JPEG2000, the new ISO/ITU-T standard for still image coding , has been shown to provide superior coding efficiency to the pr evious standard, JPEG. However, the added efficiency of JPEG200 0 comes at the cost of increased computational requirements. To improve the computational efficiency of JPEG2000, we propos e a new algorithm for JPEG2000 rate allocation and packetizat ion utilizing the D-Heapdata structure. Implemented in Jasperand tested on five reference images, this algorithm provides a sp eedup for JPEG2000’s rate allocation and packetization of 15.9 ti mes on average, and enables an average overall speedup of 33% for JPEG2000 encoding.	algorithm;algorithmic efficiency;amdahl's law;jpeg 2000;network packet;requirement;speedup	Wei Yu Fritts;J. Tangting Sun	2002		10.1109/ICIP.2002.1037996	data compression;real-time computing;transform coding;data structure;telecommunications;computer science;theoretical computer science;computational complexity theory;wavelet transform	Networks	46.31544399614411	-16.870274521633576	92261
68fe896243a278f9960c6ecadafacb103c5db03d	lossy hyperspectral images coding with exogenous quasi optimal transforms	discrete wavelet transforms;transform coding hyperspectral image compression;image coding;hyperspectral image compression;exogenous quasioptimal transform;hyperspectral imaging image coding karhunen loeve transforms hyperspectral sensors discrete wavelet transforms vectors image sensors codecs principal component analysis testing;optimal spectral transform;lossy hyperspectral image coding;ost;hyperspectral sensors;transform coding;optimal spectral transform lossy hyperspectral image coding exogenous quasioptimal transform transform coding karhunen loeve transform;data mining;karhunen loeve transforms image coding;karhunen loeve transforms;redundancy;transforms;linear transformation;hyperspectral imaging;encoding;karhunen loeve transform;hyperspectral image	It is well known in transform coding that the Karhunen-Loève Transform (KLT) can be suboptimal for non Gaussiansources. However in many applications using JPEG2000Part 2 codecs, the KLT is generally considered as the optimal linear transform for reducing redundancies between components of hyperspectral images. In previous works, optimal spectral transforms (OST) compatible with the JPEG2000 Part 2 standard have been introduced, performing better than the KLT but with an heavier computational cost. In this paper, we show that the OST computed on a learningbasis constituted of Hyperion hyperspectral images issuedfrom one sensor performs very well, and even better thanthe KLT, on other images issued from the same sensor.	algorithmic efficiency;codec;computation;distortion;hyperion;jpeg 2000;lossy compression;performance;transform coding	Michel Barret;Jean-Louis Gutzwiller;Isidore Paul Akam Bita;Florio Dalla Vedova	2009	2009 Data Compression Conference	10.1109/DCC.2009.8	computer vision;speech recognition;hyperspectral imaging;pattern recognition;mathematics;statistics	Vision	44.36104521753392	-15.323615967372293	92830
942fe9e037303871e0ce9b205b5124738eda6029	fast prediction unit partition mode selection for high-efficiency video coding intercoding using motion homogeneous coding units	confocal fluorescent microscopy;computer programming;video coding;video	The newest video compression standard, high-efficiency video coding (HEVC), obtains a higher coding efficiency than previous video compression standards, at the expense of a significant increase of computational complexity. To reduce computational complexity, implementation-friendly encoding algorithms that efficiently trade off coding efficiency and complexity should be designed. We address this issue by proposing a fast prediction unit (PU) partition mode selection scheme from the perspective of spatial correlation of visual signal. Motion similarity of spatially neighboring blocks is first introduced into the PU partition mode selection for HEVC, and the concept of motion homogeneous coding unit (CU) is first proposed. For the motion homogeneous CU, a new method is proposed to reduce the number of its candidate PU partition modes to speed up the PU partition mode selection. Experimental results show that the proposed scheme can achieve lower computational complexity than the five state-of-the-art fast methods while maintaining a similar rate distortion performance. © 2015 SPIE and IS&T [DOI: 10.1117/1.JEI.24.6.063024]	algorithm;algorithmic efficiency;blu-ray;computational complexity theory;data compression;distortion;encoder;high efficiency video coding;ibm systems network architecture;one-class classification;peak signal-to-noise ratio;rate–distortion theory;ruby document format;spatial reference system;speedup;tip (unix utility);video coding format	Wei Wu;Jiong Liu;Lixin Zhao	2015	J. Electronic Imaging	10.1117/1.JEI.24.6.063024	real-time computing;simulation;video;quarter-pixel motion;computer science;theoretical computer science;coding tree unit;computer programming;context-adaptive binary arithmetic coding;motion compensation;multiview video coding	AI	46.80429831847002	-19.586595433751203	93087
225c606d4412d9b18040dde50d17ddd7c808960a	a model for entropy coding in matching pursuit	orthogonal matching pursuit;optimisation;fractals;image coding;probability;video compression;entropy coding;transform coding;rate distortion theory;probabilistic model;transform coding entropy codes image coding optimisation probability rate distortion theory discrete cosine transforms fractals;discrete cosine transforms;entropy codes;dct entropy coding integer programming optimisation technique image compression video compression probabilistic model matching pursuit algorithm image coding fully orthogonal matching pursuit orthogonal matching pursuit rate distortion optimized matching pursuit quantization transform coding fractal coding;matching pursuit;rate distortion optimization;entropy coding matching pursuit algorithms libraries least squares approximation pursuit algorithms rate distortion greedy algorithms vectors context modeling image coding	Matching pursuit is a powerful and flexible optimisation technique that has found applications in different areas, including image and video compression. This paper proposes an underlying probabilistic model for the entropy coding of the parameters generated by the matching pursuit algorithm in the context of image coding. It also distinguishes between orthogonal and fully-orthogonal matching pursuit, and provides a formulation for rate-distortion optimized matching pursuit.	entropy encoding;matching pursuit	Mohammad Gharavi-Alkhansari	1998		10.1109/ICIP.1998.723617	computer vision;mathematical optimization;basis pursuit denoising;basis pursuit;3-dimensional matching;pattern recognition;mathematics;statistics;matching pursuit	Vision	48.76634486753691	-12.85083478276352	93134
6f629e2eca8d12cf7122db6acad30be8e6aff38b	adaptive reversible video watermarking based on motion-compensated prediction error expansion with pixel selection		In this study, a motion-compensated prediction error expansion-based adaptive reversible video watermarking algorithm is proposed. Blocks of motion-compensated frames are classified as smooth and non-smooth according to their prediction errors. Unlike the current reversible video watermarking methods that apply a single watermarking strategy to all blocks, the proposed method uses two different strategies for smooth and non-smooth blocks. This adaptive strategy is shown to increase watermarking capacity. In addition, an approach is suggested to detect those pixels causing high distortion in the watermarked video and they are not used in watermarking to limit the distortion occurring in the original video. Simulations show that the proposed method is superior to existing methods in terms of capacity and distortion.	pixel	Cabir Vural;Burhan Barakli	2016	Signal, Image and Video Processing	10.1007/s11760-016-0881-x	computer vision;theoretical computer science;mathematics;computer graphics (images)	Vision	44.598346457878364	-17.358773483765074	93273
cb63731cc089ae37313a16f2afb804462e3eeaae	incorporating primal sketch based learning into low bit-rate image compression	image coding;primal sketch;data compression;image coding image resolution frequency low pass filters asia transform coding image storage wavelet transforms image reconstruction computer vision;indexing terms;visual quality;image enhancement;image compression;low bit rate coding image compression primal sketch;learning artificial intelligence data compression image coding image enhancement;image enhancement low bit rate image compression primal sketch learning;low bit rate coding;learning artificial intelligence;high frequency	This paper proposes an image compression approach, in which we incorporate primal sketch based learning into the mainstream image compression framework. The key idea of our approach is to use primal sketch information to enhance the quality of distorted images. With this method, we only encode the down-sampled image and use the primal sketch based learning to recover the high frequency information which has been removed by down-sampling. Experimental results demonstrate that our scheme achieves better objective visual quality as well as subjective quality compared with JPEG2000 at the same bit-rates.	encode;image compression;jpeg 2000;patch (computing);peak signal-to-noise ratio;sampling (signal processing);sketch;sun one	Yang Li;Xiaoyan Sun;Hongkai Xiong;Feng Wu	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379274	data compression;image quality;computer vision;feature detection;index term;image compression;computer science;theoretical computer science;machine learning;high frequency;algorithm	Vision	44.20893552257201	-17.66215310045891	93433
9a6bdaae71ef255f8ef1a6bf41ec3483ab37a258	improved spread transform dither modulation using luminance-based jnd model		In the quantization-based watermarking framework, perceptual just noticeable distortion (JND) model has been widely used to determine the quantization step size to provide a better tradeoff between fidelity and robustness. However, the perceptual parameters computed in the embedding procedure and the detecting procedure are different, as the image has been altered by watermark embedding. In this paper, we incorporate a new DCT-based perceptual JND model, which not only shows better consistency with the HVS characteristics compared to the conventional models, but also can be invariant to the changes in the watermark framework. Furthermore, an improved spread transform dither modulation (STDM) watermarking scheme based on the new JND model is proposed. Experimental results show that the proposed scheme provides powerful resistance against common attacks, especially in robustness against Gauss noise, amplitude scaling and JPEG compression.		Wenhua Tang;Wenbo Wan;Ju Liu;Jiande Sun	2015		10.1007/978-3-319-21963-9_39	watermark;pattern recognition;robustness (computer science);quantization (signal processing);distortion;artificial intelligence;dither;discrete cosine transform;digital watermarking;jpeg;computer science	NLP	41.45973350740822	-10.14656149981894	93435
da201b1119b741456f834a53844f2387a2e53a6e	multispectral-image coding by vector quantization with kronecker-product representation	image coding;full search;image resolution;encoding complexity multispectral image coding vector quantization kronecker product representation vq very low bit rate encoding algorithm generic spatial block shape 3d spatial spectral block spatial shape vector spectral gain vector complexity reduction pixels redundancy numerical experiments high spectral resolution images compression ratios image quality;vector quantisation spectral analysis image coding image resolution image representation;complexity reduction;image representation;multispectral images;image quality;compression ratio;vector quantizer;numerical experiment;spectral analysis;vector quantisation;kronecker product;vector quantization image coding shape encoding multispectral imaging hyperspectral sensors satellites compression algorithms computational complexity pixel;spectral resolution	This paper proposes a new technique based on vector quantization (VQ) for very low bit-rate encoding of multispectral images. The new algorithm relies on the observation that in high spectral-resolution imagery the shape of a generic spatial block does not change Significantly from band to band. Therefore, it is reasonable to represent each 3-d spatial/spectral block as the Kronecker product of a spatial-shape vector and a spectral-gain vector, and to jointly quantize only these representative vectors in place of the original block. Even though such an encoding strategy is suboptimal with respect to full-search VQ, the huge complexity reduction allows one to use much larger blocks and to better exploit the redundancy among close pixels of the image. Numerical experiments carried out on high spectral-resolution images show fully satisfactory results, with compression ratios exceeding 100:1, good image quality and very low encoding complexity.	algorithm;experiment;image quality;multispectral image;neural coding;pixel;quantization (signal processing);reduction (complexity);vector quantization	Gerardo R. Canta;Giovanni Poggi	1996		10.1109/ICIP.1996.561057	image quality;multispectral image;computer vision;image resolution;theoretical computer science;compression ratio;pattern recognition;mathematics;kronecker product;spectral resolution;algorithm;reduction	AI	44.18786362194768	-14.734970463855348	93438
494b9c65bd513d39dabeea6549856fc7dc3e3eee	policy search for the optimal control of markov decision processes: a novel particle-based iterative scheme	convergence;history;aerospace electronics optimal control history convergence stochastic processes optimization probability distribution;optimal control;stochastic processes;probability distribution;state space methods dynamic programming iterative methods markov processes optimal control;aerospace electronics;iterative policy building scheme policy search optimal control markov decision processes particle based iterative scheme classical approximate dynamic programming techniques state space gridding high dimensional problems restricted parameterized policy space discrete action space policy gradient method parameter based exploration;optimization;stochastic optimal control approximate dynamic programming adp markov decision processes mdps policy search reinforcement learning rl	Classical approximate dynamic programming techniques based on state-space gridding become computationally impracticable for high-dimensional problems. Policy search techniques cope with this curse of dimensionality issue by searching for the optimal control policy in a restricted parameterized policy space. We here focus on the case of discrete action space and introduce a novel policy parametrization that adopts particles to describe the map from the state space to the action space, each particle representing a region of the state space that is mapped into a certain action. The locations and actions associated with the particles describing a policy can be tuned by means of a recently introduced policy gradient method with parameter-based exploration. The task of selecting an appropriately sized set of particles is here solved through an iterative policy building scheme that adds new particles to improve the policy performance and is also capable of removing redundant particles. Experiments demonstrate the scalability of the proposed approach as the dimensionality of the state-space grows.	acclimatization;approximation algorithm;clinical act of insertion;curse of dimensionality;dynamic programming;gradient method;insertion mutation;inspiration function;iteration;iterative method;markov chain;markov decision process;mathematical optimization;norm (social);optimal control;particle filter;population parameter;rule (guideline);scalability;state space;stochastic process;mapped	Giorgio Manganini;Matteo Pirotta;Marcello Restelli;Luigi Piroddi;Maria Prandini	2016	IEEE Transactions on Cybernetics	10.1109/TCYB.2015.2483780	markov decision process;probability distribution;stochastic process;mathematical optimization;convergence;optimal control;artificial intelligence;theoretical computer science;machine learning;control theory;mathematics;q-learning;statistics	ML	51.180138949341135	-22.299698310970822	93659
298029f0db1dcd63ac5f9b1fb7a5d1bf8fd79f55	chance-constrained optimal path planning with obstacles	constrained optimization;linear systems;optimal solution;autonomous vehicle;convex programming;gaussian processes;path planning;probabilistic planning autonomous agents chance constraints optimization under uncertainty;tree searching aircraft autonomous aerial vehicles collision avoidance convex programming gaussian processes;probability of failure;linear system;autonomous agents atmospheric modeling optimization mathematical model trajectory linear systems intelligent vehicles;model error;autonomous agent;trajectory;obstacle avoidance;optimization under uncertainty;intelligent vehicles;empirical validation;mathematical model;global optimization;optimization;collision avoidance;probabilistic planning;atmospheric modeling;tree searching;autonomous agents;branch and bound;chance constraints;constrained optimization problem;aircraft obstacle avoidance example chance constrained optimal path planning autonomous vehicles trajectory planning set bounded uncertainty probabilistic vehicle state distribution operator specified region linear gaussian systems nonconvex chance constrained optimization disjunctive convex program branch and bound techniques;autonomous aerial vehicles;aircraft;chance constraint	Autonomous vehicles need to plan trajectories to a specified goal that avoid obstacles. For robust execution, we must take into account uncertainty, which arises due to uncertain localization, modeling errors, and disturbances. Prior work handled the case of set-bounded uncertainty. We present here a chance-constrained approach, which uses instead a probabilistic representation of uncertainty. The new approach plans the future probabilistic distribution of the vehicle state so that the probability of failure is below a specified threshold. Failure occurs when the vehicle collides with an obstacle or leaves an operator-specified region. The key idea behind the approach is to use bounds on the probability of collision to show that, for linear-Gaussian systems, we can approximate the nonconvex chance-constrained optimization problem as a disjunctive convex program. This can be solved to global optimality using branch-and-bound techniques. In order to improve computation time, we introduce a customized solution method that returns almost-optimal solutions along with a hard bound on the level of suboptimality. We present an empirical validation with an aircraft obstacle avoidance example.	approximation algorithm;autonomous car;binary data;branch and bound;computation;constrained optimization;constraint (mathematics);convex function;convex optimization;cyclic redundancy check;disjunctive normal form;fathom;global optimization;goto;heuristic (computer science);language localisation;linear programming;loss function;mathematical optimization;maxima and minima;motion planning;obstacle avoidance;optimization problem;reduced cost;root-finding algorithm;search tree;time complexity;tree (data structure);unmanned aerial vehicle	Lars Blackmore;Masahiro Ono;Brian C. Williams	2011	IEEE Transactions on Robotics	10.1109/TRO.2011.2161160	mathematical optimization;constrained optimization;simulation;computer science;artificial intelligence;autonomous agent;control theory;mathematics;linear system;global optimization	Robotics	52.11917552374452	-22.488275134745166	93664
1e4a2e9628b10b3156018182b0e14527a3b3f444	vector quantization for computer generated phase-shifting holograms	computers;laser beams;quantization signal;interference;vectors;image reconstruction;image reconstruction holography quantization signal vectors laser beams computers interference;holography	Concerning the reconstruction quality, phase-shifting scheme has been proved to be efficient for digital holography. As a benefit from three differently phased reference waves, the reconstructed object image does no longer suffer from the zero-order image or twin image. In order to reduce the obvious storage and transmission burden brought in by extra holograms, in this paper, vector quantization (VQ) based on LGB algorithm (LBG-VQ) for computer generated phase-shifting holograms (CGPSHs) is proposed. The interference patterns at the hologram plane is generated by Fresnel transformation in phase-shifting digital holography (PSDH) scheme. Meanwhile, the performance of adaptive scalar quantization using Lloyd Max algorithm on effective holographic information is also investigated. Simulation results indicate that LBG-VQ outperforms adaptive scalar Lloyd Max quantization.	algorithm;color depth;data compression;digital holography;in-phase and quadrature components;interference (communication);location-based game;numerical analysis;numerical method;oracle advanced queuing;quantization (signal processing);simulation;transmitter;vector quantization	Yafei Xing;Béatrice Pesquet-Popescu;Frédéric Dufaux	2013	2013 Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2013.6810375	computer vision;electronic engineering;quantization;mathematics;optics;vector quantization	EDA	41.321110562095	-19.23202619879908	93782
d85d6633723cbdea61f749e4552a6461f3e704a8	adaptive dimensionality reduction techniques for tree-structured vector quantization	multisubspace tree structured vq;image storage;coding systems;image coding;full search;image processing;spatial domain;cost function;structure arborescente;procesamiento imagen;adaptive dimensionality reduction;1 dimensional;distortion measurement;nonterminal node;tree codebook generation;design optimization;traitement image;subspace distortions;experimental result;process design;fast codebook design algorithm;tsvq;codificacion;cuantificacion vectorial;vector quantization;vq codebook design;estructura arborescente;codebook;table codage;dimension reduced transform domain;tree structure;coding;distortion measures;tree structured vector quantization;resultado experimental;child nodes;real time implementation;vector quantizer;local statistics;resultat experimental;vector quantisation;encoding;dimensional reduction;algorithm design and analysis;codage;partitioning algorithms;real time systems;binary tree;quantification vectorielle	Abstruct- It is well-known that the exorbitant complexity of vector quantization's codebook design and encoding process have hindered the vector quantization based coding systems for real-time implementations. 'he-structured vector quantization (TSVQ) is one of the most efficient techniques to solve these problems. However, the encoder's memory requirement has increased as compared with the conventional full search vector quantization. To tackle this drawback and further reduce the searching complexity of TSVQ, new multisubspace tree-structured vector quantization design and encoding techniques are proposed in this paper. During the tree codebook generation process, each nonterminal node of the tree is associated with a partition (or region) and the child nodes are generated by further partitioning of these regions individually. Thus, different distortion measures can be used in different partitions for the node splitting. Based on this idea, the proposed multisubspace TSVQ design algorithms perform the vector quantization in spatial domain while using specially designed subspace distortions in transform domain as cost functions in the optimization process. Due to the energy compaction property of orthonormal transforms, dimensionality reduction is permitted by disregarding the components of minimal energy. The conventional Euclidean distortion is, therefore, adaptively replaced by dimension reduced transform domain subspace distortions based on the local statistics of the partition. Experimental results show that exceptionally low subspace dimension can be used in multisubspace TSVQ based on a fixed transform domain to obtain a similar performance as the conventional TSVQ or single subspace TSVQ. For example, a 256-level and 4x4 image multisubspace TSVQ using binary tree and 1-dimensional subspace distortions achieved almost 63 times computational reduction and 5 times storage reduction when compared with conventional full search vector quantization. In addition, the proposed generalized multisubspace TSVQ design algorithm is a general TSVQ design algorithm and which can also be utilized as a fast codebook design algorithm.	dimensionality reduction;vector quantization	Lai-Man Po;Chok-Ki Chan	1994	IEEE Trans. Communications	10.1109/26.293676	process design;algorithm design;mathematical optimization;discrete mathematics;multidisciplinary design optimization;binary tree;image processing;computer science;theoretical computer science;codebook;one-dimensional space;mathematics;tree structure;coding;linde–buzo–gray algorithm;vector quantization;encoding	EDA	44.6227599537734	-13.663251348002403	93852
f4bd7d054afc3214136df48b631c28867a185079	an imperceptible digital image watermarking technique by compressed watermark using pca		To provide secure communication, a modified digital watermarking scheme using discrete cosine transform (DCT) and principal component analysis (PCA) has been proposed. This scheme uses DCT for watermarking and PCA for compressing the watermark. In this technique, PCA in addition to DCT is used for watermarking the digital image to improve the quality of the watermarked image.	digital image;digital watermarking	Shaik K. Ayesha;V. Masilamani	2014		10.1007/978-3-319-11218-3_27	watermark	EDA	40.682253725480045	-11.368439388599226	94102
d4e136ce4f6ad2791e7aeaa183be253aced77229	improved image watermarking scheme using fast hadamard and discrete wavelet transforms	discrete wavelet transforms;digital watermarking;discrete wavelet transform;image watermarking	bstract. We propose a robust image watermarking scheme by pplying the fast Hadamard transform (FHT) to small blocks comuted from the four discrete wavelet transform (DWT) subbands. ifferent transforms have different properties that can effectively atch various aspects of the signal’s frequencies. Our approach onsists of four main steps: (1) we decomposed the original image nto four subbands, (2) the four subbands are divided into blocks; (3) HT is applied to each block; and (4) the singular-value decompoition (SVD) is applied to the watermark image prior to distributing he singular values over the DC components of the transformed locks. The proposed technique improves the data embedding sysem effectively, the watermark imperceptibility, and its resistance to wide range of intentional attacks. The experimental results demnstrate the improved performance of the proposed method in comarison with existing techniques in terms of the watermark impereptibility and the robustness against attacks. © 2007 SPIE and S&T. DOI: 10.1117/1.2764466	digital watermarking;discrete wavelet transform;fast walsh–hadamard transform;hadamard transform;lock (computer science);singular value decomposition	Emad Eddien Abdallah;A. Ben Hamza;Prabir Bhattacharya	2007	J. Electronic Imaging	10.1117/1.2764466	computer vision;discrete mathematics;digital watermarking;computer science;theoretical computer science;mathematics;discrete wavelet transform	Vision	40.874015085837975	-10.219582230674682	94145
47caf072666b1980f9fbc7900235b059fbfbafc7	analysis of in-loop denoising in lossy transform coding	inter frame prediction;quantization;quantization effects;high resolution;image coding;lossy transform coding;in loop denoising;h 264 avc;reference frame;transform coding;quantisation signal;noise quantization discrete cosine transforms noise reduction automatic voltage control image coding;video coding;high quality compression;jm15 1 reference software;automatic voltage control;discrete cosine transforms;noise reduction;noisy image sequences;image sequence;inverse transform;jm15 1 reference software in loop denoising lossy transform coding noisy image sequences lossy coded reference frame compression efficiency h 264 avc inverse transform inter frame prediction;image denoising;compression efficiency;quantization effects high quality compression predictor denoising;video coding image denoising image sequences inverse transforms quantisation signal transform coding;inverse transforms;lossy coded reference frame;predictor denoising;noise;image sequences	When compressing noisy image sequences, the compression efficiency is limited by the noise amount within these image sequences as the noise part cannot be predicted. In this paper, we investigate the influence of noise within the reference frame on lossy video coding of noisy image sequences. We estimate how much noise is left within a lossy coded reference frame. Therefore we analyze the transform and quantization step inside a hybrid video coder, specifically H.264/AVC. The noise power after transform, quantization, and inverse transform is calculated analytically. We use knowledge of the noise power within the reference frame in order to improve the inter frame prediction. For noise filtering of the reference frame, we implemented a simple denoising algorithm inside the H.264/AVC reference software JM15.1. We show that the bitrate can be decreased by up to 8.1% compared to the H.264/AVC standard for high resolution noisy image sequences.	algorithm;data compression;h.264/mpeg-4 avc;image resolution;lossy compression;noise power;noise reduction;quantization (signal processing);reference frame (video);transform coding	Eugen Wige;Gilbert Yammine;Peter Amon;Andreas Hutter;André Kaup	2010	28th Picture Coding Symposium	10.1109/PCS.2010.5702584	reference frame;image noise;residual frame;computer vision;transform coding;speech recognition;image resolution;quantization;computer science;noise;noise reduction;mathematics	Vision	45.264179818693634	-17.4731594191925	94190
c6dd62158734dda0d72878ae21bfa0530e7cc774	optimized svd-based robust watermarking in the fractional fourier domain		Digital watermarking is one of the most effective methods for protecting multimedia from different kind of threats. It has been used for many purposes, like copyright protection, ownership identification, tamper detection, etc. Many watermarking applications require embedding techniques that provide robustness against common watermarking attacks, like compression, noise, filtering, etc. In this paper, an optimized robust watermarking method is proposed using Fractional Fourier Transform and Singular Value Decomposition. The approach provides a secure way for watermarking through the embedding parameters that are required for the watermark extraction. It is a block-based method, where each watermark bit is embedded in its corresponding image block. First, the transform is applied to each block, and then the singular values are evaluated through which the embedding modification is performed. The optimum fractional powers, of the transform, and the embedding strength factor are evaluated through a Meta-heuristic optimization to optimize the watermark imperceptibility and robustness. The Artificial Bee Colony is used as the Meta-heuristic optimization method. A fitness function is employed, at the optimization process, through which the maximum achievable robustness can be provided without degrading the watermarking quality below a predetermined quality threshold Qth. The effectiveness of the proposed method is demonstrated through a comparison with recent watermarking techniques in terms of the watermarking performance. The watermarking quality and robustness are evaluated for different quality threshold values. Experimental results show that the proposed approach achieves a better quality compared to that of other existing watermarking methods. On the other hand, the robustness is examined against the most common applied attacks. It is noticed that the proposed method can achieve a higher robustness degree when decreasing the quality threshold value.	digital watermarking;embedded system;fitness function;fractional fourier transform;genetic algorithm;heuristic;image noise;mathematical optimization;metaheuristic;singular value decomposition;watermarking attack	Assem M. Abdelhakim;Mohamad Hanif Md. Saad;M. Sayed;Hassan I. Saleh	2018	Multimedia Tools and Applications	10.1007/s11042-018-6014-5	fractional fourier transform;computer science;fourier transform;robustness (computer science);filter (signal processing);artificial intelligence;digital watermarking;pattern recognition;watermark;embedding;singular value decomposition	EDA	41.024366034768725	-10.259385529213857	94207
609d48ace60c1cb9d5dac607a2f80e50e9d043bf	ct and mri image compression using wavelet-based contourlet transform and binary array technique		Compression techniques are essential for efficient storage and fast transfer of medical image data. In this paper, a rapid 2-D lossy compression technique constructed using wavelet-based contourlet transform (WBCT) and binary array technique (BAT) has been proposed for computed tomography (CT) and magnetic resonance imaging (MRI) images. In WBCT, the high-frequency subband obtained from wavelet transform is further decomposed into multiple directional subbands by directional filter bank to obtain more directional information. The relationship between the coefficients has been changed in WBCT as it has more directions. The differences in parent–child relationships are handled by a repositioning algorithm. The repositioned coefficients are then subjected to quantization. The quantized coefficients are further compressed by BAT where the most frequently occurring value is coded only once. The proposed method has been experimented with real-time CT and MRI images, the results indicated that the processing time of the proposed method is less compared to existing wavelet-based set-partitioning in hierarchical trees and set-partitioning embedded block coders. The evaluation results obtained from radiologists indicated that the proposed method could reproduce the diagnostic features of CT and MRI images precisely.	batch file;ct scan;coefficient;contourlet;embedded system;filter bank;genetic algorithm;image compression;lossy compression;quantization (signal processing);radiology;real-time clock;resonance;set partitioning in hierarchical trees;tomography;wavelet transform	G. Uma Vetri Selvi;R. Nadarajan	2014	Journal of Real-Time Image Processing	10.1007/s11554-014-0400-7	computer vision;speech recognition	Robotics	42.28763780479956	-15.378322017148909	94380
cffa6e9669f1f72802eb17476b5be2178575ea78	a new interpretation of data hiding capacity	data hiding;image coding;data compression;combined source channel coding;copy protection;additive noise;combined source channel coding data encapsulation copy protection image coding data compression random noise;data encapsulation;information embedding data hiding capacity still images jpeg compression additive noise captions copyright notice source coding channel coding;random noise;data encapsulation transmitters source coding multimedia communication laboratories transform coding image coding additive noise art robustness	We presenta new definition of datahiding capacitywhich complementstheestablishedtheoryin thefield andproduces practicalestimatesundermany attacks.We discussthe relation betweentheproposeddefinitionandthecurrenttheoretical work on datahiding capacity. The definition proposed is appliedto still imagesto estimatethehiding capacityof a particularimageunderattackssuchasJPEGcompressionand additivenoise.		Cagatay Candan;Nikil Jayant	2001		10.1109/ICASSP.2001.941339	data compression;sub-band coding;shannon–fano coding;telecommunications;variable-length code;computer science;theoretical computer science;tunstall coding;internet privacy;context-adaptive binary arithmetic coding;information hiding;statistics	NLP	42.41670731471197	-12.006620006675481	94429
2a063a89f7c1c9b22ffcc217fcd88b15a70fa2b9	wavelet-based compression of medical images: protocols to improve resolution and quality scalability and region-of-interest coding	region of interest coding;compression algorithm;telemedicine;client server;resolution and quality scalability;medical image;image compression;wavelet based image compression;image reconstruction;region of interest;embedded zero tree wavelet;graceful degradation;compression ratio	The paper describes a methodology to improve the scalability support of an embedded bit stream, generated with a wavelet-based compression algorithm, and a generic protocol to handle multiple regions-of-interest (ROIs). The generic scheme, illustrated for embedded zero-tree wavelet (EZW) coding, exploits the inherent graceful degradation capabilities of wavelet-based compression methods and ensures an optimal trade-off between the image reconstruction quality and the compression ratio. Additionally, an efficient protocol is proposed to handle multiple ROIs in an interactive client-server set-up for telemedicine applications. The processing of the ROIs takes place in the wavelet domain. c ©1999 Elsevier Science B.V. All rights reserved.	algorithm;bitstream;client–server model;elegant degradation;embedded zerotrees of wavelet transforms;embedded system;fault tolerance;image processing;interactivity;iterative reconstruction;region of interest;scalability;server (computing);thresholding (image processing)	Peter Schelkens;Adrian Munteanu;Jan Cornelis	1999	Future Generation Comp. Syst.	10.1016/S0167-739X(98)00061-2	data compression;iterative reconstruction;computer vision;data compression ratio;fault tolerance;image compression;computer science;theoretical computer science;compression ratio;lossless compression;wavelet packet decomposition;texture compression;set partitioning in hierarchical trees;client–server model;computer graphics (images);region of interest	EDA	41.97801009668612	-15.409334604896666	94491
3815cffbbd9a9143f9b79ab2b4343d6f701de9e8	hybrid video coding based on bidimensional matching pursuit	signal image and speech processing;rate distortion;prediction error;image processing;resource allocation;h 264 avc;greedy algorithms;motion estimation;redundant dictionaries;video coding;quantum information technology spintronics;adaptive coding;greedy algorithm;matching pursuit;spatial locality;lts2	Hybrid video coding combines together two stages: first, motion estimation and compensation predict each frame from the neighboring frames, then the prediction error is coded, reducing the correlation in the spatial domain. In this work, we focus on the latter stage, presenting a scheme that profits from some of the features introduced by the standard H.264/AVC for motion estimation and replaces the transform in the spatial domain. The prediction error is so coded using the matching pursuit algorithm which decomposes the signal over an appositely designed bidimensional, anisotropic, redundant dictionary. Comparisons are made among the proposed technique, H.264, and a DCT-based coding scheme. Moreover, we introduce fast techniques for atom selection, which exploit the spatial localization of the atoms. An adaptive coding scheme aimed at optimizing the resource allocation is also presented, together with a rate-distortion study for the matching pursuit algorithm. Results show that the proposed scheme outperforms the standard DCT, especially at very low bit rates.	adaptive coding;adaptive filter;algorithm;algorithmic efficiency;atom;b-spline;basis function;computation;computer simulation;data compression;data flow diagram;dictionary;discrete cosine transform;distortion;entropy encoding;fits;h.264/mpeg-4 avc;matching pursuit;mathematical optimization;motion estimation;performance;quantization (signal processing);rate–distortion optimization;rate–distortion theory;ruby document format;scalability;sparse approximation;sparse matrix	Lorenzo Granai;Emilio Maggio;Lorenzo Peotta;Pierre Vandergheynst	2004	EURASIP J. Adv. Sig. Proc.	10.1155/S1110865704407136	computer vision;greedy algorithm;image processing;computer science;theoretical computer science;machine learning;statistics;matching pursuit	Vision	46.29092122861013	-17.099632828387893	94627
babb2ae4e711b9f933e7309d3221c7f5f852ae7a	a compression algorithm for medical images and a display with the decoding function	compression algorithm;medical imagery;image processing;data compression;procesamiento imagen;traitement image;algorithme;ecran visualisation;algorithm;reconstruction image;codificacion;pantalla visualizacion;medical image;reconstruccion imagen;image reconstruction;coding;imagerie medicale;imageneria medical;compresion dato;display screen;compression donnee;codage;algoritmo	This paper d e s c r i b e s an e f f i c i e n t compression method f o r medical images and a high-speed d i s p l a y with a decoding funct ion. An image is divided i n t o blocks f i r s t and then whether each block has sharp edges i s examined. The discrete-cosinet ransform coding (DCT), which i s e f f i c i e n t f o r a p a r t having s m a l l gray-level v a r i a t i o n , i s app l i e d t o a block having no sharp edge. The block-truncation coding (BTC) which p rese rves sharp edges i s app l i ed t o a block having edges, and then t h e DCT is appl ied t o t h e i r e r r o r images.	algorithm;block truncation coding;code;data compression;discrete cosine transform;medical imaging	Toshiyuki Gotoh;Yukihiro Nakagawa;Morito Shiohara;Masumi Yoshida	1991	Systems and Computers in Japan	10.1002/scj.4690220210	data compression;computer vision;image processing;computer science;theoretical computer science;algorithm;statistics;computer graphics (images)	AI	45.53975011164455	-13.110890051585928	94739
65ca025529cd317fe885433ed986d5ad10276b29	low-complexity, low-memory entropy coder for image compression on msc8102 dsp	image compression		entropy encoding;image compression	Prasanna Parthasarathy;Mohamed El-Sharkawy	2002			computer vision;image compression;color cell compression;digital signal processing;data compression;distributed computing;texture compression;dictionary coder;artificial intelligence;lossless compression;data compression ratio;computer science	Vision	42.62802772254815	-15.676273529531652	95160
0dffacb8d4739621c77049996c6c2580a178fea9	integration of recursive temporal lmmse denoising filter into video codec	video denoising;desciframiento;noise reduction video codecs video compression wiener filter decoding encoding filtering gold video sequences adaptive filters;circuit decodeur;traitement signal;methode recursive;filtro respuesta impulsion inacabada;filtering;evaluation performance;filtrage;circuit codeur;logica temporal;video encoder;image coding;coding circuit;algorithm complexity;performance evaluation;image processing;data compression;least mean squares methods;decodage;decoding;video signal processing;recursive temporal linear minimum mean squared error;implementation;minimum mean squared error;complejidad algoritmo;temporal logic;evaluacion prestacion;filtrado;erreur quadratique moyenne;metodo recursivo;video compression;procesamiento imagen;recursive method;codec video;video sequences;recursive temporal lmmse denoising filter;circuito desciframiento;traitement image;linear minimum mean square error;video codec;infinite impulse response filter;reduccion ruido;temporal filtering;codage image;video coding;decoding circuit;performance improvement;adaptive filters;compression image;gold;senal video;complexite algorithme;signal video;image compression;codage video;mean square error;signal processing;noise reduction;circuito codificacion;video sequences recursive temporal lmmse denoising filter video codec video compression systems preprocessing module video encoder recursive temporal linear minimum mean squared error;preprocessing module;image sequence;reduction bruit;filtre reponse impulsion infinie;traitement signal video;video signal;video compression systems;video denoising minimum mean squared error temporal filtering video compression;video codecs;secuencia imagen;image denoising;compresion dato;wiener filter;error medio cuadratico;video coding data compression filtering theory image denoising image sequences least mean squares methods;implementacion;minimum mean square error;encoding;procesamiento senal;logique temporelle;filtering theory;sequence image	The presence of noise can dramatically affect the efficiency of video compression systems. For performance improvement, most practical video compression systems adopt a denoising filter as a pre-processing module for the video encoder, or as a post-processing module for the video decoder, but the complexity introduced by denoising can be very high. This paper first presents a recursive temporal linear minimum mean squared error (LMMSE) filter for video denoising. Based on the analysis of the hybrid video compression process, two novel schemes are presented, one for video encoding and the other for video decoding, in which the proposed recursive temporal LMMSE filter is seamlessly integrated into the encoding and the decoding processes, respectively. For both of these two schemes, the denoising is implemented with nearly no extra computation introduced. Experimental results validate the effectiveness of the proposed schemes on encoding and decoding noisy video sequences.	codec;computation;data compression;encoder;estimation theory;mean squared error;motion compensation;noise reduction;preprocessor;recursion (computer science);video decoder;video denoising;video post-processing	Liwei Guo;Oscar C. Au;Mengyao Ma;Peter Hon-Wah Wong	2010	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2009.2031453	data compression;scalable video coding;minimum mean square error;computer vision;image processing;computer science;deblocking filter;theoretical computer science;signal processing;filter;block-matching algorithm;motion compensation;h.261;video denoising;statistics;multiview video coding	Vision	46.63474237268256	-15.095660300588092	95175
2158459838203f44d5621a758429db87aab3586b	a new fast motion estimation algorithm using fast mode decision for high-efficiency video coding standard	motion estimation;hevc;sdsp;hds;fast encoding	High-efficiency video coding is the latest standardization effort of the International Organization for Standardization and the International Telecommunication Union. This new standard adopts an exhaustive algorithm of decision based on a recursive quad-tree structured coding unit, prediction unit, and transform unit. Consequently, an important coding efficiency may be achieved. However, a significant computational complexity is resulted. To speed up the encoding process, efficient algorithms based on fast mode decision and optimized motion estimation were adopted in this paper. The aim was to reduce the complexity of the motion estimation algorithm by modifying its search pattern. Then, it was combined with a new fast mode decision algorithm to further improve the coding efficiency. Experimental results show a significant speedup in terms of encoding time and bit-rate saving with tolerable quality degradation. In fact, the proposed algorithm permits a main reduction that can reach up to 75 % in encoding time. This improvement is accompanied with an average PSNR loss of 0.12 dB and a decrease by 0.5 % in terms of bit-rate.	algorithm;algorithmic efficiency;computation;computational complexity theory;data compression;elegant degradation;high efficiency video coding;motion estimation;peak signal-to-noise ratio;quadtree;recursion;ruby document format;speedup;time complexity;tip (unix utility);video coding format;video processing	Fatma Belghith;Hassan Kibeya;Hassen Loukil;Mohamed Ali Ben Ayed;Nouri Masmoudi	2014	Journal of Real-Time Image Processing	10.1007/s11554-014-0407-0	computer vision;real-time computing;simulation;computer science;theoretical computer science;coding tree unit;motion estimation	Vision	46.96943014556023	-19.152429934440068	95255
e350aa40a410cc59f7e86ca38e8688dfa76a4b20	a viewpoint switching method for multiview videos using the mpeg-4 system	standards organizations;information technology;audio video;layout;transform coding;multimedia systems;mpeg 4 standard;next generation;videos mpeg 4 standard transform coding information technology cameras standards organizations layout multimedia systems holography mpeg 7 standard;holography;3 dimensional;mpeg 7 standard;user interaction;3d video;multimedia services;cameras;videos	The 3D video is the core technology of next generation multimedia service for providing the best quality service to the users. This paper propose the structure of the ObjectDescriptor(OD) steam which can provide the multiview video service using the current 3-Dimensional Audio/Video (3DAV) of MPEG-4. This paper defines and analyzes the structure of OD, which can provide the multiview video service by applying the expanded scope to the existing OD. In addition, it is found out that just the expansion of the existing system is not enough to supply the multiview videos which consider the interrelationship between sending part and receiving part. Accordingly, this paper suggests the structure in which the new OD is added at the sending side to enable the viewpoint switching depending on the relation between viewpoints when the multiview video is transmitted. Through the proposed method, the user interaction becomes possible according to the request of users in the multiview video service.	imperative programming;mpeg-2;model–view–controller;multipoint ground;multiview video coding;next-generation network;quality of service;requirement;sound card;viewpoint	Kyung-Suek Park;Jeong-Hyun Cho;Jae-Hyung Ko;Sung-Ho Kim	2007	Sixth International Conference on Advanced Language Processing and Web Information Technology (ALPIT 2007)	10.1109/ALPIT.2007.73	computer vision;computer science;multimedia;computer graphics (images)	Robotics	42.85295658647034	-21.262463603825775	95306
657cad9601f42d64d576b49502224831de3870fb	video de-interlacing by adaptive 4-field global/local motion compensated approach	metodo adaptativo;block based directional edge interpolation;interpolation;estimation mouvement;motion compensation;video signal processing;interpolation motion detection motion estimation video sequences motion compensation tv image resolution digital signal processing laboratories design engineering;interpolacion;estimacion movimiento;motion estimation;methode adaptative;motion compensated;zooming motion detection video deinterlacing adaptive 4 field global local motion compensation block based directional edge interpolation same parity 4 field motion detection global local motion estimation camera panning detection;algorithme;algorithm;compensation mouvement;peak signal to noise ratio;motion compensation block based directional edge interpolation de interlacing global motion estimation;adaptive method;traitement signal video;motion compensation video signal processing motion estimation;motion detection;global motion estimation;algoritmo;de interlacing	A de-interlacing algorithm using adaptive 4-field global/local motion compensated approach is presented. It consists of block-based directional edge interpolation, same-parity 4-field motion detection, global/local motion estimation and compensation. The edges are sharper when the directional edge interpolation is adopted. The same parity 4-field motion detection and the 4-field local motion estimation detect the static areas and fast motion by four reference fields, and the global motion estimation detects the camera panning and zooming motions. The global and local motion compensation recover the interlaced videos to the progressive ones. Experimental results show that the peak signal-to-noise ratio of our proposed algorithm is 2/spl sim/3 dB higher than that of previous studies and attain the best quality of subjective view.	algorithm;decibel;feathering;global motion compensation;interlaced video;interpolation;motion compensation;motion estimation;peak signal-to-noise ratio	Yu-Lin Chang;Shyh-Feng Lin;Ching-Yeh Chen;Liang-Gee Chen	2005	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2005.858746	computer vision;structure from motion;interpolation;quarter-pixel motion;computer science;motion estimation;control theory;mathematics;block-matching algorithm;motion field;motion compensation;algorithm;computer graphics (images)	Vision	46.30649358811281	-14.914004648172975	95849
49654043b89d382ad9751da884bf5fea999fe493	greedy tree growing algorithms for designing variable rate vector quantizers	eigenvalues and eigenfunctions;image coding;data compression;variable rate;signal design;signal analysis;tree structured vector quantizers;variational techniques;active region;complexity;matrix algebra;tree data structures;bit rate;visual quality;variable rate vector quantizers;complexity greedy tree growing algorithms variable rate vector quantizers image compression tree structured vector quantizers eigenvalue input covariance matrix visual quality;eigenvalue;vector quantization;image compression;computational complexity;largest eigenvalue;tree structure;vector quantizer;vector quantisation;greedy tree growing algorithms;algorithm design and analysis;input covariance matrix;vector quantisation computational complexity eigenvalues and eigenfunctions image coding matrix algebra tree data structures variational techniques;algorithm design and analysis bit rate image coding vector quantization laboratories signal analysis signal design eigenvalues and eigenfunctions covariance matrix data compression;covariance matrix	The performance of vector quantization for image compression can be improved by using a variable-rate code which is able to designate more bits to regions of an image that are active or difficult to code, and fewer bits to less active regions. Two schemes are presented for directly designing variable-rate tree-structured vector quantizers by growing the tree one node at a time. One involves selecting the node with the largest average distortion within one node to split. The other involves splitting the node with the largest eigenvalue of the input covariance matrix. A comparison with the scheme of E.A. Riskin and R.M. Gray (1991) shows that the proposed schemes perform better in terms of visual quality with reduced complexity. >		W. J. Zeng;Y. F. Huang;S. C. Huang	1993		10.1109/ICASSP.1993.319881	data compression;algorithm design;covariance matrix;mathematical optimization;combinatorics;discrete mathematics;complexity;eigenvalues and eigenvectors;image compression;computer science;signal processing;mathematics;tree structure;tree;computational complexity theory;vector quantization	HPC	44.308836668660746	-13.599413493942443	95985
89b6af37461edf2c2b361172c8e7a25dac18599a	encoding spectral parameters using cache codebook		A new efficient approach to quantize the spectral line frequencies (LSF) in a coder is proposed. The use of the full search algorithm in the spectral parameters quantization causes high complexity and large hardware storage. Attempts to reduce the complexity have been performed by lowering the size of the LSF codebook. This option leads to a sub-optimal solution; the number of LSF vectors to be tested affects the performance of the speech coder. Cache codebook (CCB) technique enhances the search of the optimal quantized spectral information. In this technique the size of the main codebook is kept unchanged while the number of closest match searches is reduced. Unlike the classical quantizer design, the CCB method involves one main codebook embedding four disjoint sub-codebooks. The content of the CCB at any time is an exact reproduction of one of the four sub-codebooks. The search for the best match to an input vector is limited to the LSF vectors of the CCB. Some criteria are used to accept or reject this closest match. The CCB is updated whenever the decision is in favor of rejection. The cache codebook was successfully embedded in a CELP coder to enhance the quantization of the spectral information. The comparison simulation results show that the Codebook Caching approach yields to comparable objective and subjective performance to that of the optimal full-search technique when using the same training and testing database.	codebook	Driss Guerchi;Siwar Rekik	2012		10.1007/978-3-642-30507-8_24	parallel computing;theoretical computer science;pattern recognition;linde–buzo–gray algorithm	Robotics	49.18521132889117	-10.157702208862075	96684
f6944aa886e762c0c69f91820cc6e4dc3e10f52c	semi-fragile zernike moment-based image watermarking for authentication	signal image and speech processing;journal;quantum information technology spintronics;zernike moment;image watermarking	We propose a content-based semi-fragile watermarking algorithm for image authentication. In content-based watermarking scheme for authentication, one of the most challenging issues is to define a computable feature vector that can capture the major content characteristics. We identify Zernike moments of the image to generate feature vector and demonstrate its good robustness and discriminative capability for authentication. The watermark is generated by quantizing Zernike moments magnitudes (ZMMs) of the image and embedded into DWT (Discrete Wavelet Transform) subband. It is usually hard to locate the tampered area by using global feature in the content-based watermarking scheme. We propose a structural embedding method to locate the tampered areas by using the separability of Zernike moments-based feature vector. The authentication process does not need the original feature vector. By using the semi-fragilities of the feature vector and the watermark, the proposed authentication scheme is robust to content-preserved processing, while being fragile to malicious attacks. As an application of our algorithm, we apply it on Chinese digital seals and the results show that it works well. Compared with some existing algorithms, the proposed scheme achieves better performance in discriminating high-quality JPEG compression from malicious attacks.	algorithm;authentication;binary image;computable function;computer-aided design;digital rights management;digital watermarking;discrete wavelet transform;distortion;embedded system;experiment;feature vector;jpeg;linear separability;lossy compression;seal (east asia);semiconductor industry;video processing	Hongmei Liu;Xinzhi Yao;Jiwu Huang	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/341856	computer vision;theoretical computer science;mathematics;computer security	Vision	40.47244864766731	-10.882732651501861	96687
98dfd53f54533b5135f45b3474be4949ad9b8b72	selective image authentication tolerant to jpeg compression	zernike moments;jpeg compression;authentication;zernike moments digital images authentication jpeg compression haar wavelet transform;message authentication data compression image coding;future generation services security selective image authentication jpeg compression watermark application integrity maintenance integrity verification image compression exact authentication zernike moments compression factor malicious attacks resistance future generation services privacy;haar wavelet transform;digital images;transform coding image coding authentication quantization signal transforms q factor distortion	A watermark application in the integrity maintenance and verification of associated images is considered. There is a great advantage of watermark use in the context of conventional authentication since it does not require additional storage space for supplementary metadata. However JPEG compression, being a conventional method to compress images, leads to breaking of exact authentication. The usage of occuring selective authentication, tolerant to JPEG compression, is proposed. This technique is based on execution of Zernike moments. The performance evaluation of the proposed method depending on a compression factor is estimated. Simulation results show a good efficiency of this approach, including resistance to malicious attacks rather important within the frame of Security and Privacy in Future Generation Services topics.	authentication;coefficient;hash function;image editing;jpeg;malware;performance evaluation;pixel;simulation;wavelet	Valery I. Korzhik;Aleksey Zhuvikin;Guillermo Morales-Luna	2015	2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA)	10.1109/IISA.2015.7388076	data compression;lossy compression;lossless jpeg;computer vision;computer science;theoretical computer science;jpeg;jpeg 2000;computer security;quantization	Mobile	39.54506733148779	-11.776909116375	96764
484a6986a18c9c991c82efdfe9d210a89453bfaa	hiding a logo watermark in an image for its copyright protection	low frequency;digital library;copyright protection	Due to the urgent need for protecting the copyright of digital products in a digital library, this paper proposes a novel public watermarking scheme for a still image. Firstly, the image is decomposed to the multiwavelet domain, in which there are four subblocks in the coarsest resolution level. Then a logo watermark is embedded into the low frequency band by quantizing the difference value between corresponding coefficients in a pair of subblocks in the coarsest resolution level. Experimental results show that the proposed scheme is superior to the conventional quantization based approaches in terms of robustness.		Jun Zhang;Feng Xiong;Nengchao Wang	2002		10.1007/3-540-36227-4_71	digital library;telecommunications;computer science;low frequency;internet privacy;world wide web;computer security	EDA	40.0652903704978	-10.978525534291458	96772
e168e27611443c6b022e3ec5972caedcc9eef8a8	fast segment-wise dc coding for 3d video compression	image coding;video coding data compression;prediction algorithms;video coding;three dimensional displays;encoding three dimensional displays prediction algorithms image coding video coding rendering computer graphics algorithm design and analysis;rendering computer graphics;encoding;algorithm design and analysis;pu fast segment wise dc coding algorithm 3d video compression 3d hevc depth intra coding intra prediction modes intra candidate modes full rate distortion cost calculation prediction units	A Fast Segment-wise DC Coding (SDC) algorithm is proposed for 3D-HEVC depth intra coding. Since SDC is a supplementary option for intra prediction modes, it is not necessary to check SDC option for all intra candidate modes during full Rate-Distortion (RD) cost calculation. Based on the observation that most Prediction Units (PUs) choose the same intra coding mode with and without SDC option, we propose a fast SDC coding algorithm to avoid unnecessary full-RD cost calculation. The performance evaluation is based on HTM-11.0 and the experimental result for the proposed fast algorithm shows up to 25% encoding time saving with only 0.19% BD-bitrate increase in coded and synthesized view for All-Intra test case.	algorithm;blu-ray;distortion;full rate;high efficiency video coding;intra-frame coding;performance evaluation;ruby document format;smart data compression;test case	Zhouye Gu;Jianhua Zheng;Nam Ling;Philipp Zhang	2015	2015 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2015.7169263	sub-band coding;algorithm design;computer vision;real-time computing;prediction;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;tunstall coding;context-adaptive binary arithmetic coding;algorithm;encoding;statistics;multiview video coding	Arch	46.94313944316287	-19.179408995466336	96945
29f319f049de46ae2c86f77b3fea1a8ce546ceea	natural and synthetic video in mpeg-4	mpeg 4 standard shape scalability facial animation graphics solid modeling geometry resilience robustness decoding;object based standard;computer animation video coding multimedia systems code standards natural scenes image texture motion estimation motion compensation;multimedia coding;3d graphics models;motion compensation;decoding;texture mapping;geometry;arbitrary shape objects;motion estimation;code standards;multimedia systems;image texture;video coding;mpeg 4 standard;shape;resilience;natural video coding;temporal scalability;solid modeling;synthetic video coding;facial animation;video object plane;object animation;mesh geometry;mpeg 4 visual standard;error resilience;robustness;still texture maps;scalability;video object planes;computer animation;3d graphics;graphics;natural scenes;error resilient coding;support function;spatial scalability;facial animation mpeg 4 visual standard synthetic video coding natural video coding object based standard multimedia coding arbitrary shape objects video object planes temporal scalability spatial scalability error resilient coding still texture maps 3d graphics models mesh geometry object animation	The ISO MPEG committee, after successful completion of the MPEG-1 and the MPEG-2 standards, has recently completed the Committee Draft for MPEG-4, its third standard. MPEG-4 is designed to be an object-based standard for multimedia coding. The visual part of the standard specifies coding of both natural and synthetic video. The MPEG-4 visual standard supports coding of natural video not only in a conventional manner (using frames) but also as a collection of arbitrary shape objects (using video object planes). Further, it supports functionalities such as spatial and temporal scalability, both conventional and with arbitrary shape objects. It also supports error resilient coding for delivery of coded video on error prone channels. MPEG-4 visual standard also supports coding of synthetic video which includes still texture maps used in 3D graphics models, mesh geometry for object animation, and parameters for facial animation.	3d computer graphics;cognitive dimensions of notations;mpeg-1;mpeg-2;moving picture experts group;object-based language;scalability;synthetic data;synthetic intelligence;texture mapping	Jörn Ostermann;Atul Puri	1998		10.1109/ICASSP.1998.679713	scalable video coding;image texture;texture mapping;support function;computer vision;scalability;h.263;computer facial animation;shape;computer science;graphics;video tracking;motion estimation;computer animation;multimedia;video processing;solid modeling;motion compensation;h.262/mpeg-2 part 2;h.261;psychological resilience;robustness;multiview video coding;3d computer graphics;computer graphics (images)	Vision	43.27310881959297	-19.730678905468565	97104
fcb1255f412ade10512c73e10a43601323b997a6	content adaptation using mpeg standards			content adaptation;moving picture experts group		2008		10.1007/978-0-387-78414-4_642		NLP	43.4295169162461	-21.39174259422392	97106
13337ecd8ff338f463ab96fe406eefc7f3fefb1a	a digital watermarking scheme based on dct and svd	watermarking;image coding;watermarking discrete cosine transforms matrix decomposition information science sun singular value decomposition robustness image processing protection law;digital watermark;singular value decomposition;matrix algebra;discrete cosine transform;attack robustness digital watermarking discrete cosine transform singular value decomposition matrices;discrete cosine transforms;watermarking discrete cosine transforms image coding matrix algebra security of data singular value decomposition;security of data	This paper presents a digital watermarking scheme based on the discrete cosine transform (DCT) and singular value decomposition (SVD). In the proposed method, selected mid-band DCT coefficients from 8x8 block DCT are selected to form blocks which are later decomposed into three matrices, i.e. U, S and V. Finally the original watermark is also partitioned into blocks and then embedded into the matrix S. Experimental result shows that the presented scheme is robust to several kinds of attacks.	coefficient;digital watermarking;discrete cosine transform;embedded system;enter the matrix;singular value decomposition	Zhe-Ming Lu;Hong-Yi Zheng;Ji-Wu Huang	2007	Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007)	10.1109/IIH-MSP.2007.13	arithmetic;discrete mathematics;transform coding;modified discrete cosine transform;digital watermarking;computer science;electrical engineering;theoretical computer science;discrete cosine transform;mathematics;algorithm	EDA	41.00843614528028	-10.807534713517555	97179
7cb66bd0cb4c201a3cd845f19c5ec99a02c0c541	a comparative study of image correlation models for directional two-dimensional sources	analytical models discrete cosine transforms encoding correlation covariance matrix computational modeling;data compression;euclidean distance model image correlation models directional two dimensional sources 2d nonseparable karhunen loeve transform klt dominant directional information image data image video coding application;euclidean distance;video coding;theoretical analysis;transforms;video coding data compression transforms	The non-separable Karhunen-Loève transform (KLT) has been proven to be optimal for coding a directional 2-D source in which the dominant directional information is neither horizontal nor vertical. However, the KLT depends on the image data, and it is difficult to apply it in a practical image/video coding application. In order to solve this problem, it is necessary to build an image correlation model, and this model needs to adapt to the directional information so as to facilitate the design of 2-D non-separable transforms. In this paper, we compare two models that have been used commonly in practice: the absolute-distance model and the Euclidean-distance model. To this end, theoretical analysis and experimental study are carried out based on these two models, and the results show that the Euclidean-distance model consistently performs better than the absolute-distance model.	data compression;euclidean distance;experiment	Shuyuan Zhu;Bing Zeng	2011	2011 IEEE 13th International Workshop on Multimedia Signal Processing	10.1109/MMSP.2011.6093812	data compression;computer vision;speech recognition;machine learning;euclidean distance;mathematics;statistics	Vision	48.110632035696156	-12.022625443765952	97506
0bb260cf84302138b468a83fe9f0c5207941ba98	representation and compression of multidimensional piecewise functions using surflets	variable discreta;moving object;nonlinear approximation;theorie vitesse distorsion;polynomial discontinuities;approximation asymptotique;evaluation performance;rate distortion;atomic measurements;funcion discreta;sampling rate;piecewise constant techniques;entropia;piecewise smooth approximation;performance evaluation;discrete data;data compression;piecewise smooth functions;methode echelle multiple;aproximacion optima;piecewise smooth;evaluacion prestacion;multidimensional signals;multiresolution predictive coder;simulation;geometry;multiscale representation;video compression;metric;donnee discrete;simulacion;video sequences;metric entropy;metodo escala multiple;1 dimensional;distortion measurement;blanco movil;analyse multiresolution;technique constante par morceau;asymptotic behavior;comportement asymptotique;razon muestreo;polynomials;algorithme;rate distortion theory;function space;approximation theory;algorithm;codage predictif;comportamiento asintotico;distortion;rate x2013;discrete function;dictionnaire;fonction discrete;optimal approximation;discontinuities;approximation optimale;senal video;signal video;estimation;image compression;image edge detection;entropie;image sequence;dictionaries;discrete data approximation;taux echantillonnage;multidimensional piecewise functions representation;codificacion predictiva;approximation scheme;representacion parsimoniosa;video signal;multiscale representations;moving objects;cible mobile;metrico;wedgelets;multidimensional piecewise functions compression;secuencia imagen;multiscale method;approximation methods;entropy;asymptotic approximation;compression;surflets;sparse representation;multiscale geometric tiling;multiresolution analysis;predictive coding;diccionario;wavelets;sparse representations;images edges;moving target;metrique;analisis multiresolucion;sequence image;seismic data;aproximacion asintotica;source coding;representation parcimonieuse;algoritmo	We study the representation, approximation, and compression of functions in M dimensions that consist of constant or smooth regions separated by smooth (M-1)-dimensional discontinuities. Examples include images containing edges, video sequences of moving objects, and seismic data containing geological horizons. For both function classes, we derive the optimal asymptotic approximation and compression rates based on Kolmogorov metric entropy. For piecewise constant functions, we develop a multiresolution predictive coder that achieves the optimal rate-distortion performance; for piecewise smooth functions, our coder has near-optimal rate-distortion performance. Our coder for piecewise constant functions employs surflets, a new multiscale geometric tiling consisting of M-dimensional piecewise constant atoms containing polynomial discontinuities. Our coder for piecewise smooth functions uses surfprints, which wed surflets to wavelets for piecewise smooth approximation. Both of these schemes achieve the optimal asymptotic approximation performance. Key features of our algorithms are that they carefully control the potential growth in surflet parameters at higher smoothness and do not require explicit estimation of the discontinuity. We also extend our results to the corresponding discrete function spaces for sampled data. We provide asymptotic performance results for both discrete function spaces and relate this asymptotic performance to the sampling rate and smoothness orders of the underlying functions and discontinuities. For approximation of discrete data, we propose a new scale-adaptive dictionary that contains few elements at coarse and fine scales, but many elements at medium scales. Simulation results on synthetic signals provide a comparison between surflet-based coders and previously studied approximation schemes based on wedgelets and wavelets.	algorithm;approximation;data compression;dictionary;discrete mathematics;distortion;kolmogorov complexity;measure-preserving dynamical system;polynomial;reflections of signals on conducting lines;sampling (signal processing);simulation;synthetic intelligence;tiling window manager;wavelet	Venkat Chandrasekaran;Michael B. Wakin;Dror Baron;Richard G. Baraniuk	2009	IEEE Transactions on Information Theory	10.1109/TIT.2008.2008153	data compression;entropy;mathematical optimization;combinatorics;discrete mathematics;asymptotic analysis;mathematics;piecewise;statistics	DB	48.91284336433258	-13.809952288135811	97511
9518ca1a1ded7751be6a88e05b64141b50fa5ebd	blind measurement of blocking artifacts in images	quantization;coding errors video coding image coding;image coding;coding errors;luminance;distortion measurement transform coding power measurement humans bit rate image coding video coding video compression quantization algorithm design and analysis;video compression;blocky signal;spectrum;distortion measurement;transform coding;bit rate;texture masking;design optimization;power spectrum;blocking artifacts;video coding;texture masking blind measurement blocking artifacts images design optimization image coding video coding nonblocky image blocky signal luminance;human visual system;image quality;design;optimization;humans;nonblocky image;algorithm design and analysis;images;blocking artifact;blind measurement;power measurement	The objective measurement of blocking artifacts plays an important role in the design, optimization, and assessment of image and video coding systems. We propose a new approach that can blindly measure blocking artifacts in images without reference to the originals. The key idea is to model the blocky image as a non-blocky image interfered with a pure blocky signal. The task of the blocking effect measurement algorithm is then to detect and evaluate the power of the blocky signal. The proposed approach has the flexibility to integrate human visual system features such as the luminance and the texture masking effects.	algorithm;artifact (software development);blocking (computing);data compression;mathematical optimization;ringing artifacts	Zhou Wang;Alan C. Bovik;Brian L. Evans	2000		10.1109/ICIP.2000.899622	image quality;spectrum;computer vision;design;transform coding;quantization;computer science;mathematics;luminance;multimedia;statistics;computer graphics (images)	AI	42.7010958833607	-12.539962487679395	97685
3694503a56f8685aa19a08efe808dddcf81a019e	rate-distortion model and analytical bit allocation for wavelet-based region of interest coding	rate distortion;set partitioning in hierarchical trees;image coding;transform coding;wavelet coefficients rate distortion model analytical bit allocation wavelet based region of interest coding progressive wavelet coder spiht set partitioning in hierarchical trees jpeg2000 mallat model multiple description coding;rate distortion theory;wavelet transforms;rate distortion analytical models bit rate wavelet analysis image coding quantization wavelet coefficients signal processing image processing design engineering;model approximation;region of interest;image coding rate distortion theory wavelet transforms transform coding;multiple description coding;bit allocation	We introduce a novel Rate-Distortion (RD) model for images coded with a progressive wavelet coder, such as SPIHT (Set Partitioning in Hierarchical Trees) 111 or JPEGZOOO [2[. and especially designed to capture RD behavior when different parts of an image are refined at different speeds. Our model is an extension of Mallat's model 131. which takes into account that the rates used are not necessarily the same throughout the image. Because of the different rates. certain modeling approximations (e.g.. those for coarse quantization) can not be used uniformly throughout the image. Allocating different numbers of bits to different regions in an image is a problem that appears in applications such as Region of Interest (ROD coding 121 or Multiple Description Coding WDC) 141. where the wavelet coefficients are divided by different factors before coding to enable different bit allocations to different regions. We show that the proposed RD model provides a more accurate estimate than Mallat's model.	approximation;coefficient;distortion;multiple description coding;quantization (signal processing);rate–distortion theory;region of interest;ruby document format;set partitioning in hierarchical trees;wavelet	Phoom Sagetong;Antonio Ortega	2002		10.1109/ICIP.2002.1038913	multiresolution analysis;wavelet;computer vision;mathematical optimization;transform coding;shannon–fano coding;rate–distortion theory;harmonic vector excitation coding;computer science;theoretical computer science;multiple description coding;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;set partitioning in hierarchical trees;statistics;wavelet transform;region of interest	ML	47.520864096327756	-13.113807526781432	97692
39ad38329588b7f4f723a248058181447aa2bbdd	embedded pyramid coding of displaced frame differences	displaced frame differences;rate distortion;image coding;image resolution;motion compensation;dfd coding;motion estimation;transform coding;bit rate;design for disassembly;rate distortion theory;video coding;low bit rate video coding embedded pyramid coding displaced frame differences dfd coding hybrid video coding laplacian pyramid spatial resolution zero trees rate distortion performance;laplacian pyramid;laplace equations;discrete cosine transforms;rate distortion performance;low bit rate video coding;zero trees;displaced frame difference;proposals;rate distortion theory video coding image resolution;embedded pyramid coding;design for disassembly laplace equations discrete cosine transforms motion compensation bit rate image coding transform coding video coding motion estimation proposals;hybrid video coding;spatial resolution	A new method for coding of displaced frame differences (DFD) is proposed. DFD coding is a necessary task in hybrid video coding. In particular a Laplacian pyramid is employed which decomposes the DFD into several layers with differing spatial resolution. The pyramid is encoded in an embedded fashion using zero-trees. Simulation results show promising rate-distortion performance for low bit rate video coding.	embedded system	Frank Müller;Klaus Illgner	1995		10.1109/ICIP.1995.537619	computer vision;image resolution;telecommunications;computer science;coding tree unit;mathematics;computer graphics (images)	NLP	45.19489821381211	-16.633378087843354	97695
5c8b96e94f5b99bc2ab0146c23495e2fd0cc7d49	wavelet packets optimization using artificial bee colony algorithm	artificial bee colony algorithm image compression wavelet packet decomposition multi level thresholding;artificial bee colony;multi level thresholding;optimisation;image coding;psnr;image resolution;radiation detectors;radiation detector;wavelet packet;multimedia systems;wavelet transforms;approximation theory;image coding psnr wavelet packets radiation detectors image reconstruction;image enhancement;artificial bee colony algorithm;image compression;feature extraction;image reconstruction;wavelet packet decomposition;wavelet transforms approximation theory feature extraction filtering theory image coding image enhancement image reconstruction image resolution multimedia systems optimisation;wavelet packets;image reconstruction wavelet packet optimization artificial bee colony algorithm image size increment image compression technique wavelet transform multimedia files approximation coefficients signal filtering frequency resolution optimum threshold values;filtering theory	The increment in the sizes of the images by the technological advances accompanies high demand for large capacities, high performance devices, high bandwidths etc.,. Therefore, image compression techniques are essential to reduce the computational or transmittal costs. Wavelet transform is one of the compression techniques especially used for images and multimedia files. In wavelet transform, approximation and detail coefficients are extracted from the signal by filtering. Both approximation and detail coefficients are re-decomposed up to some level to increase frequency resolution. Once coefficients are generated, the optimum threshold values are determined to obtain the best reconstructed image, which can be considered as an optimization task. In this study, Artificial Bee Colony algorithm which is a recent and successful optimization tool is used to determine the thresholds to produce the best compressed image in terms of both compression ratio and quality.	approximation;artificial bee colony algorithm;bandwidth (signal processing);coefficient;image compression;mathematical optimization;wavelet packet decomposition;wavelet transform	Bahriye Akay;Dervis Karaboga	2011	2011 IEEE Congress of Evolutionary Computation (CEC)	10.1109/CEC.2011.5949603	computer vision;mathematical optimization;computer science;pattern recognition;mathematics;wavelet packet decomposition;stationary wavelet transform;particle detector	EDA	42.733472619450886	-16.362718445741766	97857
1b6344d650ed7e34d9c0282724875d17eb2a373f	multiple description image coding using natural scene statistics	image coding layout statistics gsm wavelet domain image communication wavelet coefficients random variables rate distortion redundancy;rate distortion;multiple description;image coding;gaussian processes;image communication;natural scene statistics;visual communication;transform coding;gaussian scale mixture;rate distortion theory;wavelet transforms;redundancy;statistical analysis;image reconstruction;redundancy image coding natural scenes statistical analysis wavelet transforms gaussian processes visual communication rate distortion theory image reconstruction transform coding;random variable;multiple description coding;error resilience;image reconstruction multiple description coding multiple description image coding natural scene statistics wavelet domain gaussian scale mixture model denoising watermark detection error resilient image communications rate distortion bound redundancy rate distortion function;natural scenes	The statistics of natural scenes in the wavelet domain are accurately characterized by the Gaussian scale mixture (GSM) model. The model lends itself easily to analysis and many applications that use this model are emerging (e.g., denoising, watermark detection). We present an error-resilient image communications application that uses the GSM model and multiple description coding (MDC) to provide error-resilience. We derive a rate-distortion bound for GSM random variables, derive the redundancy rate-distortion function, and finally implement an MD image communication system.	distortion;multiple description coding;noise reduction;rate–distortion theory;scene statistics;wavelet	Sumohana S. Channappayya;Robert W. Heath;Alan C. Bovik	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415398	iterative reconstruction;random variable;computer vision;transform coding;rate–distortion theory;theoretical computer science;multiple description coding;pattern recognition;gaussian process;mathematics;redundancy;statistics;visual communication;wavelet transform	Robotics	48.569440000597034	-12.810655391699369	98085
7b19e660162f6cf53c8dfac5c2fdb6429eaf8995	hierarchical coding of hdtv	circuit codeur;red numerica integracion servicios;coding circuit;image processing;data compression;subband decomposition;procesamiento imagen;traitement image;experimental result;codificacion;integrated services digital network;circuito codificacion;coding;resultado experimental;television haute resolution;compresion dato;reseau numerique integration services;decomposition sous bande;television alta definicion;resultat experimental;high definition television;compression donnee;codage	The Broadband Integrated Services Digital Network (BISDN) based on lightwave technology is supposed to become the all-purpose exchange area communication network of the future. All digital services are integrated with applications ranging from facsimile, viedophone, teleconferencing to digital standard-resolution TV - sometimes referred to as Extended Quality TV 5EQTV) - and High Definition TV (HDTV). In order to make efficient use of the available network bandwidth hierarchical coding schemes combine the necessary data compressions oif the HDTV and EQTV signals such that the HDTV signal can be transmitted at 135 Mbit/s with the embedded EQTV signal coded in a sub-channel of approximately 35 Mbit/s		Frank Bosveld;Reginald L. Lagendijk;Jan Biemond	1992	Sig. Proc.: Image Comm.	10.1016/0923-5965(92)90027-D	data compression;telecommunications;image processing;computer science;integrated services digital network;coding;statistics	ML	47.85095948549327	-14.111939704203795	98125
ce8358c9daf58d38fcee4bea94a0311b0cbafda0	object-oriented analysis-synthesis coding of moving images	object oriented analysis	An object-oriented analysis-synthesis coder is presented which encodes objects instead of blocks of N x N picture elements. The objects are described by three parameter sets defining the motion, shape and colour of an object. The parameter sets are obtained by image analysis based on source models of either moving 2D-objects or moving 3D-objects. Known coding techniques are used to encode the parameter sets. An object-depending parameter coding allows to introduce geometrical distortions instead of quantization errors. Using the transmitted parameter sets an image can be reconstructed by model-based	data compression;distortion;encode;image analysis;quantization (signal processing)	Hans Georg Musmann;Michael Hötter;Jörn Ostermann	1989	Sig. Proc.: Image Comm.	10.1016/0923-5965(89)90005-2	object-oriented analysis and design;deep-sky object;computer science	Vision	41.986868363078464	-19.00691021803011	98395
e4ca217a62b7bdd467af59eddb3edc565acc5e57	3-d video coding with redundant-wavelet multihypothesis	transformation ondelette;estensibilidad;filtering theory video coding data compression wavelet transforms diversity reception motion compensation;filtering;rate distortion;filtrage;estimation mouvement;scalable video coding;data compression;motion compensation;redundant wavelet transform;video signal processing;signal 3 dimensions;estimacion movimiento;filtrado;compresion senal;motion estimation;video coding wavelet transforms filtering wavelet domain noise reduction motion compensation feedback loop rate distortion scalability tracking;compression signal;diversity reception;motion compensated;motion compensated temporal filtering;temporal filtering;wavelet transforms;compensation mouvement;senal 3 dimensiones;video coding;scalable video coding motion compensated temporal filtering mctf multihypothesis motion compensation redundant wavelet transform;wavelet transform;codage video;affine transformation;signal compression;motion compensated temporal filtering mctf;traitement signal video;three dimensional signal;multihypothesis motion compensation;extensibilite;scalability;transformacion ondita;triangle mesh;filtering theory;wavelet transformation;lifting based temporal transform 3d video coding redundant wavelet multihypothesis phase diversity motion compensated temporal filtering inverse transform affine transform	Multihypothesis with phase diversity is introduced into motion-compensated temporal filtering by deploying the latter in the domain of a spatially redundant wavelet transform. The centerpiece of this redundant-wavelet approach to multihypothesis temporal filtering is a multiple-phase inverse transform that involves an implicit projection significantly reducing noise not captured by the motion model of the temporal filtering. The primary contribution of the work is a derivation that establishes analytically the advantage of the redundant-wavelet approach as compared to equivalent temporal filtering taking place in the spatial domain. For practical implementation, a regular triangle mesh is used to track motion between frames, and an affine transform between mesh triangles implements motion compensation within a lifting-based temporal transform. Experimental results reveal that the incorporation of phase-diversity multihypothesis into motion-compensated temporal filtering improves rate-distortion performance, and state-of-the-art scalable performance is observed.	3d film;data compression;distortion;lifting scheme;motion compensation;scalability;stationary wavelet transform;triangle mesh	Yonghui Wang;Suxia Cui;James E. Fowler	2006	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2005.861940	computer vision;computer science;theoretical computer science;mathematics;wavelet transform	Vision	45.6935819560249	-15.820995797584683	98720
01487cbbe2fbe32ab4dcc9e20ee7254c1bf77268	reversible visible watermarking for h.264/avc encoded video	watermarking;quantization;image coding;residual information;psnr;watermarking image coding transforms quantization streaming media cryptography psnr;advanced encryption standard reversible visible watermarking h 264 avc encoded video residual information zlib deflector algorithm;video quality;video coding;streaming media;cryptography;reversible visible watermarking;transforms;medical application;h 264 avc encoded video;advanced encryption standard;zlib deflector algorithm;watermarking video coding	Visible watermarked images and videos are generally used to convey ownership information. However, the visible watermark is generally irreversible and thus authenticated users cannot recover the original image or video quality after watermark extraction. This poses a limitation in various scenarios including military, law and medical applications. This paper presents a novel reversible visible watermarking scheme for H.264/AVC encoded video sequences. The proposed approach reversibly embeds the residual information that will then be used by the decoder to recover the original image. The residual information is losslessly compressed using the ZLib Deflector algorithm to minimize the information to be embedded. The compressed information is then encrypted using the 128-bit Advanced Encryption Standard (AES). Simulation results clearly demonstrate the superiority of the proposed scheme to current state of the art where Peak Signal-to-Noise Ration (PSNR) gains of up to 7 dB were achieved.	128-bit;algorithm;authentication;decibel;digital watermarking;embedded system;encryption;h.264/mpeg-4 avc;lossless compression;peak signal-to-noise ratio;simulation;zlib	Reuben A. Farrugia	2011	2011 IEEE EUROCON - International Conference on Computer as a Tool	10.1109/EUROCON.2011.5929031	advanced encryption standard;computer vision;quantization;computer science;cryptography;video quality;theoretical computer science;machine learning;multimedia;statistics	Vision	40.094230950054246	-12.448853392718986	98748
874396290d59db23e7f47c76156aad428d18e0ae	enhancing hevc compressed videos with a partition-masked convolutional neural network		In this paper, we propose a partition-masked Convolution Neural Network (CNN) to achieve compressed-video enhancement for the state-of-the-art coding standard, High Efficiency Video Coding (HECV). More precisely, our method utilizes the partition information produced by the encoder to guide the quality enhancement process. In contrast to existing CNN-based approaches, which only take the decoded frame as the input to the CNN, the proposed approach considers the coding unit (CU) size information and combines it with the distorted decoded frame such that the degradation introduced by HEVC is reduced more efficiently. Experimental results show that our approach leads to over 9.76% BD-rate saving on benchmark sequences, which achieves the state-of-the-art performance.		Xiaoyi He;Qiang Hu;Xiaoyun Zhang;Chongyang Zhang;Weiyao Lin;Xintong Han	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451086	convolutional neural network;encoder;visualization;partition (number theory);artificial intelligence;computer science;pattern recognition;feature extraction	Robotics	46.36238579128173	-19.598658482393997	98903
5dd03fda97a7729cd9501b98e828c1d8803ec30b	compression of vector field changing in time	3d field;floating point compression;compression algorithm;004;lossy compression;protein ligand docking vector compression;lossless compression;force field;vector field	One of the problems connected with a real-time protein-ligand docking simulation is the need to store series of precomputed electrostatic force fields of a molecule changing in time. A single frame of the force field is a 3D array of floating point vectors and it constitutes approximately 180 MB of data. Therefore requirements on storage grow rapidly if several hundreds of such frames need to be stored. We propose a lossy compression method of such force field, based on audio and video coding, and we evaluate its properties and performance. Digital Object Identifier 10.4230/OASIcs.MEMICS.2010.40	3d computer graphics;data compression;docking (molecular);force field (chemistry);identifier;lossy compression;megabyte;precomputation;protein–ligand docking;real-time clock;requirement;simulation	Tomas Golembiovsky;Aleš Křenek	2010		10.4230/OASIcs.MEMICS.2010.40	data compression;lossy compression;data compression ratio;electronic engineering;simulation;image compression;computer science;theoretical computer science;lossless compression;vector quantization	Graphics	41.28569678597358	-16.643154737823668	98967
66b3b7a0559229615cf71e9dc6e205102ed596ba	rate-distortion optimal skeleton-based shape coding	skeleton based shape coding;lossy distance data coding;spline;rate distortion;optimisation;image coding;data compression;cost function;bit allocation cost;distance data;operational rate distortion;polynomial approximation rate distortion theory data compression video coding optimisation;skeleton;approximation accuracy;rate distortion theory;object oriented video coding;video coding;boundary distance;mpeg 4 standard;shape information;shape;rate distortion shape skeleton encoding image coding cost function video coding mpeg 4 standard standardization spline;lossless skeleton coding;skeleton data;polygonal approximation;lossless skeleton coding rate distortion optimal coding skeleton based shape coding shape information independent data sets boundary distance approximation accuracy bit allocation cost operational rate distortion polygonal approximation distance data skeleton data object oriented video coding correlation lossy distance data coding;bit allocation;correlation;rate distortion optimization;encoding;rate distortion optimal coding;standardization;polynomial approximation;independent data sets	We present a new shape-coding approach, which decouples the shape information into two independent data sets, the skeleton and the distance of the boundary from the skeleton. The major benefit of this approach is that it allows a more flexible trade-off between accuracy of the approximation and bit-allocation cost, and thus, provides the possibility of better performance in the operational rate-distortion (ORD) optimal sense than other reported techniques. The characteristics of these data sets are studied and various approximation approaches are applied on each of them to reach an ORD optimal result. We apply, for example, polygonal approximation on both the skeleton and distance data. We demonstrate that the resulting approach outperforms existing ORD optimal approaches.	apply;approximation;distortion;rate–distortion theory;shape context	Haohong Wang;Thrasyvoulos N. Pappas;Aggelos K. Katsaggelos	2001		10.1109/ICIP.2001.958665	data compression;spline;computer vision;mathematical optimization;rate–distortion theory;shape;computer science;theoretical computer science;mathematics;rate–distortion optimization;skeleton;correlation;standardization;encoding	ML	49.905183879797015	-16.035806282912084	99005
8e4055122f53a588db8f956e63d6fac66bc500a6	a new jpeg2000 region-of-interest image coding method: foveal shift	region of interest		jpeg 2000	J. Zheng;L. Y. Fan	2003			region of interest;foveal;coding (social sciences);computer vision;artificial intelligence;jpeg 2000;computer science	Vision	42.0392757580762	-16.4413834414474	99048
fa37de85c7fe9eb520bd4bae744d221284c09688	modeling of subband image data for buffer control	dct buffer control subband image data modeling adaptive scheme quantization transform coded frames video sequence coder generalized gaussian model transform coefficient subband coefficients optimum dead zone quantizer entropy quantizer output symbols low bit rates uniform quantizer adaptive procedure quantizer parameters updating channel buffer state constant output rate variable input rate bit rate prediction laplacian model experimental results;modelizacion;metodo adaptativo;quantization;metodo estadistico;optimisation;cuantificacion;image processing;optimizacion;gaussian processes;telecommunication control;buffer control;procesamiento imagen;buffer storage;statistical method;methode adaptative;transform coding;indexing terms;quantification;traitement image;statistical model;entropy image coding quantization discrete cosine transforms predictive models laplace equations video sequences bit rate pulse modulation transform coding;algorithme;quantisation signal;modelisation;algorithm;video coding;codificacion;adaptive signal processing;statistical analysis;channel capacity;methode statistique;discrete cosine transforms;adaptive method;coding;generalized gaussian;quantization parameter;optimization;entropy;telecommunication control video coding image sequences quantisation signal buffer storage adaptive signal processing transform coding gaussian processes channel capacity statistical analysis entropy discrete cosine transforms;modeling;subband coding;codage;image sequences;algoritmo	In this work we develop an adaptive scheme for quantization of subband or transform coded frames in a typical video sequence coder. Using a generalized Gaussian model for the subband or transform coefficients, we present a procedure to determine the optimum deadzone quantizer for a given entropy of the quantizer output symbols. We find that, at low bit rates, the dead-zone quantizer offers better performance than the uniform quantizer. The model is used to develop an adaptive procedure to update the quantizer parameters on the basis of the state of a channel buffer with constant output rate and variable input rate. We compare the accuracy of the generalized Gaussian model in predicting the actual bit rate to that achievable using the simpler and more common Laplacian model. Experimental results show that the generalized Gaussian model has superior performance than the Laplacian model, and that it can be effectively used in a practical scheme for buffer control.	coefficient;ising model;quantization (signal processing)	Giancarlo Calvagno;Cristiano Ghirardi;Gian Antonio Mian;Roberto Rinaldo	1997	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.564117	adaptive filter;sub-band coding;statistical model;entropy;mathematical optimization;transform coding;systems modeling;index term;quantization;image processing;computer science;theoretical computer science;gaussian process;mathematics;coding;channel capacity;statistics	ML	48.094735556376094	-12.853881229936418	99153
d1d05b08aef484e31d3c50b9adb1ba8a5430a2bf	improved residual data coding for high efficiency video coding lossless extension	binarization table selection high efficiency video coding hevc lossless intra coding residual data coding;data compression;video coding data compression;hevc lossless intra coding residual data coding high efficiency video coding lossless extension hevc standard lossless video coding lossy video compression level coding binarization table selection encoded level values bit saving;video coding;video coding standards transforms quantization signal entropy coding throughput	This paper describes an extension of the new high efficiency video coding (HEVC) standard for supporting lossless video coding. Since HEVC is originally designed for lossy video compression, it cannot provide outstanding performances in lossless video coding. In this paper, we propose new residual data coding methods based on the statistical characteristics of lossless intra coding. The proposed method consists of improved level coding and the binarization table selection with the weighted sum of previously encoded level values. Experimental results show that the proposed method achieves approximately 2.41% bit saving and reaches up to 5.34% bit saving, compared to HEVC lossless intra coding.	binary image;binary number;context-adaptive binary arithmetic coding;data compression;high efficiency video coding;intra-frame coding;lossless compression;lossy compression;performance;throughput;weight function	Jung-Ah Choi;Yo-Sung Ho	2013	The 2013 RIVF International Conference on Computing & Communication Technologies - Research, Innovation, and Vision for Future (RIVF)	10.1109/RIVF.2013.6719859	data compression;lossy compression;sub-band coding;lossless jpeg;computer vision;harmonic vector excitation coding;variable-length code;entropy encoding;theoretical computer science;context-adaptive variable-length coding;coding tree unit;modified huffman coding;tunstall coding;multimedia;adaptive coding;context-adaptive binary arithmetic coding;h.261;huffman coding;multiview video coding;golomb coding	Robotics	46.40700283792525	-19.15536498882448	99181
e44ec191b0dba0fedd6d12f5060a4fb329bf439e	recursive optimal pruning with applications to tree structured vector quantizers	available bit rate;optimisation;full search;image processing;data compression;bit error rate;time sharing;trees mathematics;rate distortion theory algorithm design and analysis tree data structures time sharing computer systems books image reconstruction bit rate quantization speech coding;design optimization;vector quantization;codes;optical data processing;tree structure;recursive functions;training sequence recursive optimal pruning tree structured vector quantizers optimal tree structures codebooks convex hull distortion rate function minimum average distortion bit rates statistical mismatch coding data;algorithms;vector quantizer;optimisation data compression encoding;convex hull;encoding	A pruning algorithm of P.A. Chou et al. (1989) for designing optimal tree structures identifies only those codebooks which lie on the convex hull of the original codebook's operational distortion rate function. The authors introduce a modified version of the original algorithm, which identifies a large number of codebooks having minimum average distortion, under the constraint that, in each step, only modes having no descendents are removed from the tree. All codebooks generated by the original algorithm are also generated by this algorithm. The new algorithm generates a much larger number of codebooks in the middle- and low-rate regions. The additional codebooks permit operation near the codebook's operational distortion rate function without time sharing by choosing from the increased number of available bit rates. Despite the statistical mismatch which occurs when coding data outside the training sequence, these pruned codebooks retain their performance advantage over full search vector quantizers (VQs) for a large range of rates.	algorithm;choose (action);codebook;convex hull;distortion;large;recursion (computer science);time-sharing	Shei-Zein Kiang;Richard L. Baker;Gary J. Sullivan;Chung-Yen Chiu	1992	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.136593	data compression;mathematical optimization;multidisciplinary design optimization;bit error rate;image processing;computer science;theoretical computer science;convex hull;machine learning;mathematics;tree structure;code;time-sharing;vector quantization;encoding;statistics	Visualization	46.8694229529661	-12.309262726994598	99187
dc6e304541b91fc74b458afe0a2c02296ecb672f	an efficient vq codebook search algorithm applied to amr-wb speech coding	speech codec;vector quantization vq;immittance spectral frequency isf;adaptive multi rate wideband amr wb;binary search space structured vq bss vq;voip	The adaptive multi-rate wideband (AMR-WB) speech codec is widely used in modern mobile communication systems for high speech quality in handheld devices. Nonetheless, a major disadvantage is that vector quantization (VQ) of immittance spectral frequency (ISF) coefficients takes a considerable computational load in the AMR-WB coding. Accordingly, a binary search space-structured VQ (BSS-VQ) algorithm is adopted to efficiently reduce the complexity of ISF quantization in AMR-WB. This search algorithm is done through a fast locating technique combined with lookup tables, such that an input vector is efficiently assigned to a subspace where relatively few codeword searches are required to be executed. In terms of overall search performance, this work is experimentally validated as a superior search algorithm relative to a multiple triangular inequality elimination (MTIE), a TIE with dynamic and intersection mechanisms (DI-TIE), and an equal-average equal-variance equal-norm nearest neighbor search (EEENNS) approach. With a full search algorithm as a benchmark for overall search load comparison, this work provides an 87% search load reduction at a threshold of quantization accuracy of 0.96, a figure far beyond 55% in the MTIE, 76% in the EEENNS approach, and 83% in the DI-TIE approach.	adaptive multi-rate wideband;adaptive multi-rate audio codec;benchmark (computing);binary search algorithm;code word;codebook;coefficient;color balance;computation;experiment;ink serialized format;lookup table;lossless compression;mobile device;nearest neighbor search;operation time;smartphone;social inequality;speech coding;vector quantization	Cheng-Yu Yeh	2017	Symmetry	10.3390/sym9040054	speech recognition;theoretical computer science;speech coding;voice over ip;mathematics	AI	46.16762608110538	-10.29964669130224	99236
425afe0235d32588f8c6582e12e3496fc96b3790	fast cu size and pu partition decision for avs2 intra coding		AVS2 is the new generation of video coding standard developed by the Audio Video Coding Standard Working Group of China. Similar to HEVC, a flexible partition structure is adopted to improve coding performance in AVS2. For intra coding, a coding unit (CU) is recursively split into sub CUs based on the quad-tree structure. CUs should be further split into prediction unit (PU) with various square or non-square shapes. All these processes increase encoding complexity dramatically. In this paper, a fast intra coding algorithm for CU partition and PU partition is proposed to reduce the complexity in AVS2 intra coding. Specifically, the statistical analysis model between texture complexity and partition mode is established. According to the model, an adaptive online-offline threshold selection algorithm is proposed to determine the early skipping and early termination in CU and PU level. Experimental results show that the proposed fast intra coding algorithm achieves more than a 54% encoding time reduction on average with only a 0.77% BD-rate increase under all-intra configuration for the AVS2 reference software RD18.0.	blu-ray;cuda;data compression;fast fourier transform;high efficiency video coding;ibm systems network architecture;intra-frame coding;one-class classification;online and offline;recursion;selection algorithm;tip (unix utility);tree structure;video coding format	Meng Yuan;Yonglin Xue;S. Ohn;Sae Hyung	2018	2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2018.8436807	real-time computing;selection algorithm;coding (social sciences);computer science;recursion;computational complexity theory;partition (number theory);artificial intelligence;pattern recognition	SE	46.88468340620078	-19.70115346359339	99248
2cef6381d0f28aff31c016f4904ffdb97343083a	passively safe partial motion planning for mobile robots with limited field-of-views in unknown dynamic environments	safety mobile robots path planning;trajectory safety planning collision avoidance dynamics robot sensing systems;braking ics passively safe partial motion planning mobile robots unknown dynamic environments limited sensory field of view upper bounded planning time absolute motion safety passive motion safety passpmp partial motion planner inevitable collision state concept ics concept	This paper addresses the problem of planning the motion of a mobile robot with a limited sensory field-of-view in an unknown dynamic environment. In such a situation, the upper-bounded planning time prevents from computing a complete motion to the goal, partial motion planning is in order. Besides the presence of moving obstacles whose future behaviour is unknown precludes absolute motion safety (in the sense that no collision will ever take place whatever happens) is impossible to guarantee. The stance taken herein is to settle for a weaker level of motion safety called passive motion safety: it guarantees that, if a collision takes place, the robot will be at rest. The primary contribution of this paper is PassPMP, a partial motion planner enforcing passive motion safety. PassPMP periodically computes a passively safe partial trajectory designed to drive the robot towards its goal state. Passive motion safety is handled using a variant of the Inevitable Collision State (ICS) concept called Braking ICS, i.e. states such that, whatever the future braking trajectory of the robot, a collision occurs before it is at rest. Simulation results demonstrate how PassPMP operates and handles limited sensory field-of-views, occlusions and moving obstacles with unknown future behaviour. More importantly, PassPMP is provably passively safe.	mobile robot;motion planning;simulation	Sara Bouraine;Thierry Fraichard;Ouahiba Azouaoui;Hassen Salhi	2014	2014 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2014.6907375	control engineering;simulation;engineering;control theory	Robotics	53.61559068984226	-21.878097563799372	99328
fdfc5c41d62074c5e9b89418e698a11ce62086fb	binary contour coding using bézier approximation	edge boundary;image processing;contour;binary image;procesamiento imagen;intelligence artificielle;qualite image;traitement image;algorithme;algorithm;reconstruction image;codificacion;reconstruccion imagen;image reconstruction;image quality;coding;image binaire;pattern recognition;imagen binaria;artificial intelligence;contorno;calidad imagen;inteligencia artificial;reconnaissance forme;reconocimiento patron;codage;algoritmo	Ahs/raCl: A method for coding of binary image contour using B6zier approximalion is proposed. A set of key pixel (guiding pixels) on the contour is defined which enables the contour to be decomposed into arcs and straight line segments. A set of cleaning operations has been considered as an intermedIate step before producing the final output. The quality of faithful reproduetion of the decoded version has been examined through the objective measures of shape com­ paclness and the percentage error in area. Finally, the bit requirement and the compression ratios for different input images are compared with the existing ones.	approximation error;binary image;bézier curve;contour line;pixel;plasma cleaning	S. N. Biswas;Sankar K. Pal;D. Dutta Majumder	1988	Pattern Recognition Letters	10.1016/0167-8655(88)90031-1	iterative reconstruction;image quality;computer vision;binary image;image processing;computer science;artificial intelligence;mathematics;coding	Vision	45.48910489220861	-12.921397489390063	99527
6b3e6cb21f2a1df0022e452c30f39dde1459fe0b	reversible data hiding by coefficient adjustment algorithm		This chapter presents three reversible data hiding schemes based on coefficient adjustment algorithm. The proposed coefficient adjustment (CA) algorithm mainly consists of three steps, which are block-mean removal, pixel squeezing, and pixel isolation. To provide a large hiding space, the mean removal firstly generates the differenced blocks from an input image. A pixel in the differenced blocks can be obtained by subtracting the original pixel value of the block from either the target codeword that was stored in a predetermined codebook or a prediction mean. Then, the pixel squeezing approach further provides hiding storage, while the pixel isolation approach minimizes distortion. Simulation demonstrated that the proposed method provided a larger payload than existing techniques at various embedding rates, and its corresponding PSNR was competitive. Moreover, a robust version of the proposed method can be achieved by employing the CA algorithm in the integer wavelet transform (IWT) domain. Experiments also confirmed that the resulting (marked) images were robust against manipulations of the image such as JPEG2000, JPEG, brightness adjustment, cropping, and inversion.	algorithm;coefficient	Ching-Yu Yang;Wu-Chih Hu	2013		10.1007/978-3-642-28580-6_3	wavelet transform;pixel;computer science;information hiding;brightness;distortion;codebook;jpeg;algorithm;jpeg 2000	Robotics	40.77712632150795	-11.989667268827704	99706
3fd7f7f9e8453f15255d7721e8f027daffb4a08b	simple method for enhancing the performance of lossy plus lossless image compression schemes	lossless image compression;image compression;algorithms;prediction model	Lossy plus lossless techniques for image compression split an image into a low-bit-rate lossy representation and a residual that represents the difference between this low-rate lossy image and the originaL Conventional schemes encode the lossy image and its lossless residual in an independent manner. We show that making use of the lossy image to encode the residual can lead to significant savings in bit rate. Further, the complexity increase to attain these savings is minimaL The savings are achieved by capturing the inherent structure of the image in the form of a noncausal prediction model that we call a prediction tree. This prediction model is then used to transmit the lossless residuaL Simulation results show that a reduction of 0.5 to 1.0 bit/pixel can be achieved in bit rates compared to the conventional approach of independently encoding the	encode;image compression;lossless compression;lossy compression;pixel;simulation	Nasir D. Memon;Khalid Sayood;Spyros S. Magliveras	1993	J. Electronic Imaging	10.1117/12.148253	data compression;lossy compression;lossless jpeg;data compression ratio;adaptive scalable texture compression;block truncation coding;transparency;image compression;computer science;theoretical computer science;jpeg;lossless compression;predictive modelling;fractal transform;context-adaptive binary arithmetic coding;texture compression;packbits;algorithm	Vision	45.309994269318366	-16.747819806757295	99802
d524c33a6dbaebd82c8a0ffc083bd42b6967cd6f	the effect of applying 2d enhancement algorithms on 3d video content		• A submitted manuscript is the author's version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. • The final author version and the galley proof are versions of the publication after peer review. • The final published version features the final layout of the paper including the volume, issue and page numbers.	active shutter 3d system;algorithm;digital video;elegant degradation;movie projector;online and offline;real-time clock;stereoscopic video game;stereoscopy;video coding format	Sin Lin Wu;Jorge E. Caviedes;Lina J. Karam;Ingrid Heynderickx	2014	J. Electrical and Computer Engineering	10.1155/2014/601392	computer vision;computer science;multimedia;computer graphics (images)	ML	41.96576784976826	-23.270262624250037	99837
49a3bbe217c0dbe9a5de52515aa8e453489796fb	scene content driven fec allocation for video streaming	fec allocation;error concealment;scene transitions;error resiliency;unequal loss protection;hill climbing algorithm	Unequal error protection schemes applied on video data streams, considering varying importance of data packets over a group of pictures (GOP), are more efficient in terms of rate-distortion performance at different loss rates. Importance ordering policy adopted so far, mostly considered frame positions within a GOP. In the present work, we offer significant importance to the packets containing scene-transition frames, as these should be better error protected. We adopt a strategy of Forward Error Correcting (FEC) Code allocation, based on the minimization of end-to-end distortion up to the decoder, assuming that error concealment is adopted at the decoder. Two FEC allocation strategies are proposed within the Block of Packets (BOP) structure - one is an iterative modified hill climbing approach and the other is a reduced complexity heuristic approach. The Gilbert-Elliot model is used for the modeling of transmission channel. The proposed FEC allocation schemes outperform existing FEC allocation schemes in terms of PSNR for sequences with and without transitions, when transmitted over lossy channels.		Abhishek Midya;Rajeev Ranjan;Somnath Sengupta	2014	Sig. Proc.: Image Comm.	10.1016/j.image.2013.11.001	real-time computing;telecommunications;computer science;theoretical computer science;hill climbing	Vision	47.99888024057437	-16.531704125103136	99870
ea572a76449c257d8c8a8882ca27ec4c5398343c	applying interest operators in semi-fragile video watermarking	filigranage numerique;protection information;digital watermarking;traitement signal;image coding;image processing;data compression;securite;video signal processing;interest points;authentication;procesamiento imagen;traitement image;authentification;codage image;video coding;compression image;autenticacion;senal video;signal video;image compression;codage video;proteccion informacion;feature extraction;senal numerica;signal processing;information protection;robustesse;filigrana digital;safety;signal numerique;traitement signal video;video signal;robustness;digital video;compresion dato;digital signal;extraction caracteristique;video;video watermarking;seguridad;procesamiento senal;compression donnee;invariant feature;robustez;compresion imagen	In this article we present a semi-fragile watermarking scheme for authenticating intra-coded frames in compressed digital videos. The scheme provides the detection of content-changing manipulations while being moderately robust against content-preserving manipulations. We describe a watermarking method based on invariant features referred to as interest points. The features are extracted using the Moravec-Operator. Out of the interest points we generate a binary feature mask, which is embedded robustly as watermark into the video. In the verification process we compare the detected watermark with the interest points from the video to be verified. We present test results evaluating the robustness against content-preserving manipulations and the fragility in terms of content-changing manipulations. Beside the discussion of the results we propose a procedure to provide security of the scheme against forgery attacks.	semiconductor industry	Stefan Thiemert;Hichem Sahbi;Martin Steinebach	2005		10.1117/12.586791	computer vision;computer science;multimedia;computer graphics (images)	Theory	43.29831194901464	-10.63434772708138	99993
8371bb6bfe6fa406e333e7fbaa6a9954ff6aceee	on the security of ownership watermarking of digital images based on singular value decomposition	digital watermarking;image coding;singular value decomposition;digital imaging;numerical linear algebra;digital image;false positive;process engineering	We show that the two countermeasures proposed in a paper on the security of ownership watermarking of digital images based on singular value decomposition by Loukhaoukha and Chouinard do not solve the false-positive detection problem in contrast to designers’ claim and therefore should not be used for proof of ownership application. © 2011 SPIE and IS&T. [DOI: 10.1117/1.3534865]	digital image;digital watermarking;singular value decomposition	Huo-Chong Ling;Raphael C.-W. Phan;Swee-Huay Heng	2011	J. Electronic Imaging	10.1117/1.3534865	computer vision;discrete mathematics;type i and type ii errors;digital watermarking;computer science;theoretical computer science;digital imaging;mathematics;numerical linear algebra;singular value decomposition;digital image	EDA	40.702746247704745	-10.10669593474896	100312
6324e4862921862c87290667d34772f105f7baca	improvement speed of fractal image compression through gray level difference and normal variance	decompression;decompression fractal image compression gray level difference normal variance image encoding compression ratio multi resolution properties;fractals;image coding;data compression;image coding data compression fractals;prediction algorithms;data mining;gray level difference;frcatal encoding;frcatal encoding fractal image compression gray level difference normal variance encoding time;pixel;classification algorithms;mathematical model;compression ratio;fractals image coding;multi resolution properties;normal variance;encoding time;image encoding;fractal image compression	Fractal image encoding is attractive due to its potential high compression ratio, fast decompression and multi-resolution properties. However, the encoding time is computationally intensive. In this paper, a new method is proposed to reduce the encoding time based on computing the gray level difference and normal variance of domain and range blocks. Proposed method only compares those domain blocks whose gray level difference and normal variance (obtained from dividing variance on gray level difference) are higher than those of the range blocks. This method reduces the number of comparisons, and thereby the encoding time considerably, while obtaining good fidelity and compression ratio for the decoded image. Experimental results on standard grayscale images (256´256, 8bit) show that the proposed method yields superior performance over conventional fractal encoding.	data compression;fractal compression;grayscale;image compression	Gohar Vahdati;Elham Afarandeh;Mehdi Yaghoubi	2009	2009 International Conference of Soft Computing and Pattern Recognition	10.1109/SoCPaR.2009.68	data compression;statistical classification;computer vision;prediction;fractal;computer science;theoretical computer science;compression ratio;mathematical model;mathematics;fractal compression;pixel;statistics;computer graphics (images)	Robotics	43.61369233778458	-14.966748764227379	100412
e9d48a0f398ae1a4da428b48a68ffe93133ea3a0	reduced complexity superresolution for low-bitrate video compression	reduced complexity downsampled compression schemes reduced complexity superresolution low bitrate video compression sr techniques coding efficiency coding complexity image quality generic decimation quantization compression scheme video codec h 264 advanced video coding high efficiency video coding common rescaling techniques signal to noise ratio quality improvement psnr image rescaling solutions interpolation techniques computational time reduction;super resolution video compression singleimage low complexity codecs high definition video;video coding data compression image resolution interpolation video codecs;encoding complexity theory bit rate codecs psnr decoding image coding	Evolving video applications impose requirements for high image quality, low bitrate, and/or small computational cost. This paper combines state-of-the-art coding and superresolution (SR) techniques to improve video compression both in terms of coding efficiency and complexity. The proposed approach improves a generic decimation-quantization compression scheme by introducing low complexity single-image SR techniques for rescaling the data at the decoder side and by jointly exploring/optimizing the downsampling/upsampling processes. The enhanced scheme achieves improvement of the quality and system's complexity compared with conventional codecs and can be easily modified to meet various diverse requirements, such as effectively supporting any off-the-shelf video codec, for instance H.264/Advanced Video Coding or High Efficiency Video Coding. Our approach builds on studying the generic scheme's parameterization with common rescaling techniques to achieve 2.4-dB peak signal-to-noise ratio (PSNR) quality improvement at low-bitrates compared with the conventional codecs and proposes a novel SR algorithm to advance the critical bitrate at the level of 10 Mb/s. The evaluation of the SR algorithm includes the comparison of its performance to other image rescaling solutions of the literature. The results show quality improvement by 5-dB PSNR over straightforward interpolation techniques and computational time reduction by three orders of magnitude when compared with the highly involved methods of the field. Therefore, our algorithm proves to be most suitable for use in reduced complexity downsampled compression schemes.	algorithm;algorithmic efficiency;autostereogram;codec;data compression;decimation (signal processing);encoder;frame language;h.264/mpeg-4 avc;high efficiency video coding;image quality;image scaling;interpolation;mathematical optimization;mebibyte;megabyte;peak signal-to-noise ratio;requirement;run time (program lifecycle phase);super-resolution imaging;time complexity;upsampling	Georgios Georgis;George Lentaris;Dionysios I. Reisis	2016	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2015.2389431	data compression;scalable video coding;computer vision;data compression ratio;telecommunications;computer science;theoretical computer science;coding tree unit;block-matching algorithm;context-adaptive binary arithmetic coding;motion compensation;h.261;multiview video coding	Vision	45.122184005382344	-17.56561644545375	100548
632f4216eea0c40629251513d1067a4d82b31349	novel watermarked mdc system based on sfq algorithm		A novel watermarked MDC system based on the SFQ algorithm and the sub-sampling method is proposed in this paper. Subsampling algorithm is applied onto the transformed image to introduce some redundancy between different channels. Secret information is embedded into the preprocessed sub-images. Good performance of the new system to defense the noise and the compression attacks is shown in the experimental results. Copyright © 2012 The Institute of Electronics, Information and Communication Engineers.	algorithm;network scheduler;watermark (data file)	Lin-Lin Tang;Jeng-Shyang Pan;Hao Luo;Junbao Li	2012	IEICE Transactions		telecommunications;digital watermarking;computer science;electrical engineering;theoretical computer science;mathematics	Visualization	40.47215880522135	-10.887913963999424	100551
63f0135bdd561828e7de14d559b81bd7a365ad5e	a new approach for hiding image based on the signature of coeficients	tecnologias generalidades;tecnologias	This paper presents a new approach for hiding the secret image inside another image file, depending on the signature of coefficients. The proposed system consists of two general stages. The first one is the hiding stage which consist of the following steps (Read the cover image and message image, Block collections using the chain code and similarity measure, Apply DCT Transform, Signature of coefficients, Hiding algorithm , Save information of block in boundary, Reconstruct block to stego image and checking process). The second stage is extraction stage which consist of the following steps ( read the stego image, Extract information of block from boundary, Block collection, Apply DCT transform, Extract bits of message and save it to buffer, Extracting message).		Tawfiq A. Al-asadi;Israa Hadi Ali;Abdul kadhem Abdul kareem Abdul kadhem	2015	IJIMAI	10.9781/ijimai.2015.352	theoretical computer science;mathematics;world wide web;algorithm	Vision	39.5588385650822	-10.986266363231026	100651
dc7715f80e2a53cddb5bdc55691b8c8beb1eefd8	multi-layer assignment steganography using graph-theoretic approach	multi layer;steganography;期刊论文;graph;palette image;depth first search	This paper proposes a novel multibit assignment steganographic scheme for palette images, in which some colors in the palette are exploited to represent several secret bits. For the proposed scheme, each palette color is treated as a graph vertex, and an edge among any two vertices indicates an adjacent relationship between them. A graph traversal technique named depth-first search is used to accomplish the multibit assignment for the vertices that correspond to the palette colors. The major idea of the proposed data-embedding is to modify colors of image pixels according to the assigned bits and secret message. Image pixels are classified as embeddable pixels and non-embeddable pixels before data-embedding. During the data-embedding, for each embeddable pixel, if the original color of the pixel matches the secret data, the pixel is then kept unchanged; otherwise, a suitable adjacent color of the original color will be used to replace the original color so that the new color matches the secret data. Experimental results show that the secret data can be embedded and extracted successfully without introducing visual artifacts, and the proposed scheme can always achieve a high capacity or maintain good image quality, comparing with the related works.	chaos theory;color;computation;computational complexity theory;concatenation;depth-first search;embedded system;experiment;fisher–yates shuffle;graph theory;graph traversal;hidden surface determination;human visual system model;image quality;layer (electronics);palette (computing);pixel;steganography;transform, clipping, and lighting;tree traversal;universal conductance fluctuations;vertex (geometry);vertex (graph theory);visual artifact	Hanzhou Wu;Hongxia Wang;Hong Zhao;Xiuying Yu	2014	Multimedia Tools and Applications	10.1007/s11042-014-2050-y	color histogram;computer vision;color depth;breadth-first search;theoretical computer science;steganography;graph;8-bit color;computer graphics (images)	Vision	40.6894553308233	-12.330674840312929	100758
8ddbcd9713b774c092de40c73f9a83e522db2bfc	fast inter-mode decision and selective quarter-pel refinement in h.264 video coding	half pel motion estimation step;fast inter mode decision;rate distortion;inter prediction modes;h 264 video coding mode decision quarter pel refinement;quarter pel refinement;mode decision complexity fast inter mode decision selective quarter pel refinement h 264 video coding inter prediction modes macroblock partitions rate distortion optimal coding mode computational complexity rate distortion calculations sub pel motion refinement integer pel search half pel motion estimation step optimal mode estimation error;optimal mode estimation error;integer pel search;motion estimation;variable block size;macroblock partitions;selective quarter pel refinement;video coding;video coding rate distortion motion estimation rate distortion theory estimation error computational modeling cost function computational complexity psnr prediction methods;computational complexity;mode decision complexity;sub pel motion refinement;video coding computational complexity;h 264 video coding;rate distortion calculations;estimation error;rate distortion optimal coding mode;rate distortion optimization;mode decision	In H.264 video coding standard, there exist several inter - prediction modes that use macroblock partitions with variable block sizes. Choosing a rate-distortion optimal coding mode for each macroblock is essential for the best possible coding performance, but also prohibitive due to the heavy computational complexity associated with the required rate-distortion calculations. Likewise, sub-pel motion refinement improves the coding efficiency, but becomes a major computational bottleneck when integer-pel search is executed fast. In this paper, we present a simple strategy to reduce the complexity of quarter-pel refinement and inter-mode decision with minimum loss of coding efficiency. Based on the results of the half-pel motion estimation step, our method evaluates the likelihood of each inter-coding mode being optimal. Then, quarter-pel refinement and actual rate and distortion are computed for only those coding modes with sufficient chance of being optimal. We claim that this method minimizes optimal mode estimation error at a given level of refinement and mode decision complexity. Simulation results show that the algorithm speeds up quarter-pel search and inter-mode selection modules by a factor of about 6 with less than 0.12 dB PSNR loss.	algorithm;algorithmic efficiency;computational complexity theory;data compression;distortion;existential quantification;h.264/mpeg-4 avc;macroblock;motion estimation;peak signal-to-noise ratio;pixel;refinement (computing);simulation;video coding format	Hasan F. Ates	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517792	computer vision;mathematical optimization;real-time computing;computer science;coding tree unit;motion estimation;mathematics;rate–distortion optimization;computational complexity theory	Robotics	47.41374723317645	-19.0333398658962	100777
005737f66ff28ad83d0896967be13171ba16f8d3	image coding using robust quantization for noisy digital transmission	quantization;channel coding;cuantificacion;image coding;image processing;data compression;image coding robustness quantization bit error rate filtering design optimization rate distortion optimization methods psnr degradation;memoryless systems image coding cryptography digital communication source coding channel coding noise all pass filters;bit error rate;digital transmission;compresion senal;procesamiento imagen;indexing terms;quantification;traitement image;compression signal;tratamiento numerico;codificacion;scalar quantization;digital communication;image transmission;channel bit error rate image coding robust quantization noisy digital transmission memoryless sources binary symmetric channel channel optimized scalar quantization all pass filtering binary phase scrambling descrambling method peak signal to noise ratio performance;cryptography;peak signal to noise ratio;signal compression;coding;transmision numerica;binary symmetric channel;digital processing;compresion dato;transmission numerique;memoryless systems;transmission image;traitement numerique;compression donnee;codage;noise;transmision imagen;source coding;all pass filters	A robust quantizer is developed for encoding memoryless sources and transmission over the binary symmetric channel (BSC). The system combines channel optimized scalar quantization (COSQ) with all-pass filtering, the latter performed using a binary phase-scrambling/descrambling method. Applied to a broad class of sources, the robust quantizer achieves the same performance as the Gaussian COSQ for the memoryless Gaussian source. This quantizer is used in image coding for transmission over a BSC. The peak signal-to-noise ratio (PSNR) performance degrades gracefully as the channel bit error rate increases.	binary symmetric channel;bit error rate;fault tolerance;normal statistical distribution;peak signal-to-noise ratio;quantization (signal processing)	Thomas R. Fischer	1998	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.663494	data compression;binary symmetric channel;bit error rate;quantization;telecommunications;image processing;computer science;noise;cryptography;theoretical computer science;mathematics;coding;statistics	Vision	48.04404777134422	-12.639393481520152	100841
4aadd26cfbc9f5c7d128879e7399bc3c3840d76d	3d spiht for multi-lead ecg compression	incart twelve lead arrhythmia database 3d spiht algorithm multilead ecg signal compression 3d set partitioning in hierarchical trees electrocardiogram intrabeat redundancy interbeat redundancy interlead redundancy compression performance beat reordering residual calculation technique 2d ecg array high frequency component;medical signal processing data compression electrocardiography;electrocardiography arrays three dimensional displays encoding heart beat wavelet transforms	In this paper we proposed the implementation of 3D Set Partitioning In Hierarchical Trees (SPIHT) algorithm to a multi-lead ECG signal compression. The implementation of 3D SPIHT decorrelates three types of redundancy that commonly found on a multi-lead electrocardiogram (ECG) signal i.e. intra-beat, inter-beat, and inter-lead redundancies. To optimize overall compression performance we also proposed beat reordering and residual calculation technique. Beat reordering rearranges beat order in 2D ECG array based on the similarity between adjacent beats. This rearrangement reduces variances between adjacent beats so that the 2D ECG array contains less high frequency component. Residual calculation optimizes required storage usage further by minimizing amplitude variance of 2D ECG array. The experiments on selected records from St Petersburg INCART 12-lead Arrhythmia Database show that proposed method gives relatively low distortion at compression rate 8 and 16.	algorithm;distortion;experiment;network packet;set partitioning in hierarchical trees;signal compression;wavelet packet decomposition;wavelet transform	Sani M. Isa;Wisnu Jatmiko;Aniati Murni Arymurthy	2014	2014 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2014.6906900	electronic engineering;speech recognition;computer science;pattern recognition	Robotics	42.454557467365476	-15.510132822852796	100899
62beb5c36b23661bc1080469a496b4ae34d0ec9e	a new dual-bitstream video streaming system with vcr functionalities using sp/si-frames	video streaming;streaming video;video segmentation;dual bitstream system;video coding;digital video cassette recording vcr;digital video;mpeg h 264;motion compensated prediction	With the proliferation of digital video and the popularity of video streaming applications, it is highly desirable to find and access video segments of interest by searching through the content of video at a speed that is faster than a normal playback. The key functions that enable quick browsing of video are fast-forward and fastreverse playbacks. However, motion-compensated prediction adopted in the current video coding standards drastically complicates these operations. One approach to implement the fast-forward/reverse playback is to store an additional reverse-encoded bitstream in the server. Once the client requests a fast-forward/reverse operation, the server can select an appropriate frame for the client from either the forward-encoded bitstream or the reverse-encoded bitstream by considering the cost of network bandwidth and decoder complexity. Unfortunately, these two bitstreams are encoded separately. The frame in one bitstream may not be exactly identical to the frame in another bitstream. If one of these frames is then used as the reference for the requested frame, which is in another bitstream, it induces mismatch errors. In this paper, a novel H.264 dualbitstream system aiming at providing the fast-forward/ reverse playback based on SP/SI-frames is proposed. The proposed system can completely eliminate mismatch errors when the frame in the reverse-encoded bitstream replaces the frame in the forward-encoded bitstream and vice versa. Experimental results confirm that the proposed system is effective in eliminating mismatch errors so as to enhance the performance of the dual-bitstream system.	approximation;bitstream;claire;data compression;digital video;fast forward;fully buffered dimm;h.264/mpeg-4 avc;information engineering;peak signal-to-noise ratio;sp-devs;server (computing);signal processing;streaming media;video coding format;videocassette recorder	Yui-Lam Chan;Tak-Piu Ip;Ki-Kit Lai;Chang-Hong Fu;Wan-Chi Siu	2011	Signal Processing Systems	10.1007/s11265-010-0472-y	video compression picture types;scalable video coding;real-time computing;computer hardware;computer science;video capture;video tracking;block-matching algorithm;multimedia;video processing;bitstream format;motion compensation;s-video;multiview video coding	Mobile	44.09802812581209	-20.818491221103173	100940
4fa82a177a0d76db6448f889bd338cf0cc2b3de6	saliency based perceptual hevc	graph theory;perceptual quality;circuits and systems;video coding graph theory quantisation signal;saliency;human attention;quantization control;quantization signal;transform coding;bit rate;hevc;hevc video coding;quantisation signal;video coding;visualization;quantization parameter saliency hevc;quantization parameter;saliency information;graph based visual saliency;saliency based perceptual hevc;perceptual quality saliency based perceptual hevc human attention saliency information hevc video coding graph based visual saliency quantization control quantization parameter;encoding;video coding bit rate quantization signal encoding visualization circuits and systems transform coding	Saliency represents the probability of human attention over the image therefore is important in understanding the importance of different areas in the image. In this paper, we consider how to utilize the saliency information in HEVC video coding. The graph-based visual saliency is incorporated with the quantization control in HEVC to reduce the bit rate of compressed video by using larger quantization parameter for the coding unit which has lower probability of attention. As shown in our experiments, the proposed method achieves up to 12% bit rate reduction without perceptual quality loss.	data compression;experiment;high efficiency video coding;quantization (signal processing)	Yiming Li;Weihang Liao;Junming Huang;Da He;Zhenzhong Chen	2014	2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2014.6890644	computer vision;transform coding;visualization;graph theory;theoretical computer science;salience;pattern recognition;encoding	Robotics	44.61325241179438	-18.35207241578893	101233
a4bef447aa9a0f568ab943e971c8b962d2a66aa2	hybrid key/wyner-ziv frames with flexible macroblock ordering for improved low delay distributed video coding	desciframiento;evaluation performance;wyner ziv;performance evaluation;decodage;decoding;video signal processing;flow rate regulation;evaluacion prestacion;gestion trafic;traffic control;traffic management;probleme de wyner ziv;problema de wyner ziv;video coding;codificacion;flexible macroblock ordering;performance improvement;codage video;regulation debit;wyner ziv problem;pixel;coding;gestion trafico;traitement signal video;distributed video coding;temps retard;delay time;regulation trafic;side information;tiempo retardo;regulacion trafico;regulacion caudal;codage	This paper proposes a concealment based approach to generating the side information and estimating the correlation noise for low-delay, pixel-based, distributed video coding. The proposed method employs a macroblock pattern similar to the one used in the dispersed type FMO of H.264 for grouping the macroblocks of each frame into intra coded (key) and Wyner-Ziv groups. Temporal concealment is then used at the decoder for concealing the missing macroblocks (estimating the side information - predicting the Wyner-Ziv macroblocks). The actual intra coded/decoded macroblocks are used for estimating the correlation noise. The results indicate significant performance improvements relative to existing motion extrapolation based approaches (up to 25% bit rate reduction).	data compression;flexible macroblock ordering	Dimitris Agrafiotis;Pierre Ferré;David R. Bull	2007		10.1117/12.704310	active traffic management;simulation;speech recognition;telecommunications;computer science;deblocking filter;coding tree unit;block-matching algorithm;coding;pixel;statistics	EDA	47.629196931213855	-15.218245788785838	101248
4aa1ce9cdb18ee204cc188a3a254815b1b7abd24	lossless gray-scale image compression by predictive gdf	lossless image compression;computer programming;spatial correlation;image compression	Gray scale DF-expression (GDF) has raised some research interests recently. GDF-expression exploits data correlation on every bitplane and represents image hierarchically. In this paper, a predictive GDF-expression (PGDF) for lossless image compression is proposed which applies a predictive step to increase the spatial correlation on MSB bit-planes. The predictive GDF is compared with predictive Huffman encoding. Extensive experiments show that PGDF can achieve much better compression results than GDF and slightly better results than predictive Huffman encoding for most of the testing images.© (1993) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	geographic data files;grayscale;image compression;lossless compression	Jun Ma	1993		10.1117/12.150942	data compression;lossy compression;computer vision;spatial correlation;image compression;computer science;theoretical computer science;computer programming;lossless compression	Robotics	43.16732622628685	-16.182405536134528	101387
0f70b5dbb919aaed5c67d57302546311b48dfdd3	adaptive watermarking and performance analysis based on image content	digital watermarking;hvs model;feature blocks;iteration mapping;fractal dimension	This paper presents a novel adaptive watermarking based on iteration mapping and image content. An original image is divided into non-overlapped blocks, which are analyzed by means of fractal dimension, and the feature blocks which contain edges and textures are further classified by variance characteristics into three different parts: edges, weak textures, and strong textures. The original watermark is a distinguishable gray image, which is scrambled and amended to form a watermark signal to be embedded. DCT is applied to all the original image blocks and the formed watermark is embedded into their middle-frequency coefficients with different strength. Performance analysis of adaptive watermark extraction is presented in details. Experimental results show that this algorithm is able to achieve a watermarked image with good perceptual invisibility, high security and strong robustness.	algorithm;coefficient;digital watermarking;discrete cosine transform;embedded system;fractal dimension;human visual system model;iteration;nl-complete;numerical aperture;profiling (computer programming)	Rongrong Ni;Qiuqi Ruan;Jun Lu	2007	IJWMIP	10.1142/S0219691307001690	computer vision;digital watermarking;theoretical computer science;mathematics;fractal dimension;watermark;computer graphics (images)	AI	41.128446865384234	-10.77220911305115	101408
08be7d28e5b40af9aa6776e70c02d22d6a1ba55b	adaptive, robust and blind digital watermarking using bhattacharyya distance and bit manipulation		Ownership identification and copyright protection are two major concerns for digital data. Digital watermarking provides the best solution for these issues. In order to preserve the copyright protection and identify the ownership of digital data, a robust blind watermarking method using Bhattacharyya distance and exponential function is proposed. The proposed method is adaptive since the embedding factor is calculated based on the gray scale pixel distribution of the host image. Insertion of watermark is done in two phases in which the first phase is carried out in wavelet domain, and the second phase in spatial domain. Similarly extraction of watermark is done in two phases. The proposed method is tested using standard benchmark color images and a binary watermark. The proposed method is analyzed with various image and signal processing attacks and compared with other methods. The results show that the proposed method has better performance than other methods.	benchmark (computing);bit manipulation;digital data;digital watermarking;experiment;grayscale;image processing;insertion sort;marginal model;neural correlates of consciousness;peak signal-to-noise ratio;pixel;signal processing;time complexity;wavelet	S. PrasanthVaidya;R. ChandraMouliP.V.S.S.	2017	Multimedia Tools and Applications	10.1007/s11042-017-4476-5	computer vision;grayscale;digital data;watermark;wavelet;computer science;artificial intelligence;digital watermarking;signal processing;pattern recognition;bit manipulation;bhattacharyya distance	EDA	41.04619512308057	-10.804377326254453	101471
e9204949735c681a59ea764fd22f9ad5f8d6d129	two-description distributed video coding for robust transmission	signal image and speech processing;quantum information technology spintronics	In this article, a two-description distributed video coding (2D-DVC) is proposed to address the robust video transmission of low-power capturers. The odd/even frame-splitting partitions a video into two sub-sequences to produce two descriptions. Each description consists of two parts, where part 1 is a zero-motion based H.264-coded bitstream of a sub-sequence and part 2 is a Wyner-Ziv (WZ)-coded bitstream of the other sub-sequence. As the redundant part, the WZ-coded bitstream guarantees that the lost sub-sequence is recovered when one description is lost. On the other hand, the redundancy degrades the rate-distortion performance as no loss occurs. A residual 2D-DVC is employed to further improve the rate-distortion performance, where the difference of two subsequences is WZ encoded to generate part 2 in each description. Furthermore, an optimization method is applied to control an appropriate amount of redundancy and therefore facilitate the tuning of central/side distortion tradeoff. The experimental results show that the proposed schemes achieve better performance than the referenced one especially for low-motion videos. Moreover, our schemes still maintain low-complexity encoding property.	bitstream;data compression;distortion;low-power broadcasting;mathematical optimization;winzip	Anhong Wang;Zhihong Li;Yao Zhao;Wubin Wang;Huihui Bai	2011	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2011-76	scalable video coding;real-time computing;telecommunications;computer science;theoretical computer science	AI	48.164366251516185	-16.93252379021733	101494
59c039c4a5788080d5839d52e1a3d0b30971e464	intra frame encoding using programmable graphics hardware	parallel algorithm;gpu;intra prediction;graphics hardware;graphic processing unit;programmable graphics hardware	In this paper, we propose a parallel algorithm for H.264/AVC intra frame encoding by using the graphics processing unit (GPU). The proposed algorithm can handle 4×4 intra block prediction and reconstruction. By rearranging the encoding order of 4×4 blocks and modifying the architecture of H.264/AVC encoder, thirty times speed up can be achieved which utilizing the computing power of GPU without any loss in coding efficiency.		Man Cheung Kung;Oscar C. Au;Peter Hon-Wah Wong;Chun-Hung Liu	2007		10.1007/978-3-540-77255-2_76	computer architecture;graphics pipeline;parallel computing;computer hardware;computer science;real-time computer graphics;parallel algorithm;graphics hardware;general-purpose computing on graphics processing units	Graphics	45.780438433819256	-20.22184745317911	101510
136ba455310363149052fceb216a3ba41903b2ac	jscc based on adaptive segmentation and irregular ldpc for image transmission over wireless channels	red sin hilo;metodo adaptativo;wireless channels;image segmentation;image processing;reseau sans fil;transmission error;controle parite;canal transmision;codage source;wireless network;procesamiento imagen;control paridad;methode adaptative;error transmision;visual quality;traitement image;calcul analogique;joint source channel coding;image transmission;canal transmission;transmission channel;unequal error protection;adaptive method;segmentation image;ldpc code;error resilience;low density parity check;transmission image;erreur transmission;parity check;transmision imagen;source coding;analog calculus;calculo analogico	In order to improve error resilient capability and transmission efficiency for image transmission over wireless channels, a joint source channel coding (JSCC) scheme was proposed. Adaptive segmentation can segment an image into different size of block with different level of significance; high degree bit nodes within an irregular low-density parity check (LDPC) code can provide unequal error protection (UEP) scheme. So the adaptive segmentation and irregular LDPC coding were combined together to provide JSCC. Simulation results show that the proposed scheme can support robust image transmission in a very effective way with high visual quality.	low-density parity-check code	Rui Guo;Ji-lin Liu	2006		10.1007/11881223_50	low-density parity-check code;telecommunications;image processing;computer science;statistics	Vision	46.94953008406202	-13.600385255870627	101555
efcefd7bf6b95ad915a74e6d0469f4bcb7c0afac	simultaneous edge sensing compression and encryption for real-time video transmission	multimedia;edge detection;video compression;wavelet transforms;video;wavelets	Video compression and encryption became an essential part in multimedia application and video conferencing in particular. Applying both techniques simultaneously is one of the challenges where the size and the quality are important. In this paper we are suggesting the use of wavelet transform in order to deal with the low frequency coefficients when undertaking the encryption on the wavelet high frequency coefficients while accomplishing the compression. Applying both methods simultaneously is not new. In this paper we are suggesting a way to improve the security level of the encryption with better computational performance in both encryption and compression. Both encryption and compression in this paper are based on edges extraction from the wavelet high frequency sub-bands. Although there are some research perform the edge detection on the spatial domain, but the number of edges produced based on wavelet can be dynamic which have an effect on the compression ratio dynamically. Moreover, this kind of edge detection in wavelet domain will add different level of selective encryption. © (2014) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	encryption;real-time clock	Nazar Al-Hayani;Naseer Al-Jawad;Sabah Jassim	2014		10.1117/12.2052630	video compression picture types;data compression;wavelet;computer vision;video;edge detection;computer science;theoretical computer science;multimedia;wavelet transform	Mobile	42.02475782216712	-15.296662264886617	101603
13f8dd34deb7db989ecb9f1b07203d9b0fe408cb	multiresolution fragile watermarking using complex chirp signal for content integrity verification	quantization index modulation;watermarking;signal resolution watermarking authentication frequency modulation chirp modulation amplitude modulation pulse modulation software tools protection matched filters;image coding;image coding watermarking chirp modulation quantisation signal embedded systems frequency modulation image resolution;frequency modulation;image resolution;quantisation signal;embedded systems;frequency modulated;blind authentication process multiresolution fragile watermarking content integrity verification quantization index modulation embedding technique embedding zone false detection rate frequency modulated complex chirp signal fm;detection rate;chirp modulation	This paper proposes a wavelet-domain multiresolution fragile watermarking scheme using an improved quantization-index-modulation (QIM) embedding technique. A secure embedding zone is exploited in our proposed scheme to reduce the false detection rate of Kundur's scheme. The frequency modulated (FM) complex chirp signal is employed as watermark. Both the real and the imaginary parts of the chirp signal are embedded simultaneously in a hierarchical manner. Unlike the conventional schemes, the proposed scheme does not require the original watermark for content integrity verification. The blind authentication process allows embedding of arbitrary FM chirp watermarks.	authentication;chirp;digital watermarking;embedded system;fm broadcasting;imaginary time;modulation;multiresolution analysis;wavelet	Dan Yu;Farook Sattar;Braham Barkat	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421851	frequency modulation;speech recognition;image resolution;telecommunications;digital watermarking;computer science;mathematics;analog transmission;chirp	EDA	41.51179019575103	-9.91544904295363	101736
def81fab10bd4fc74b44721dfed883ccdce15601	quadtree-structured variable-size block-matching motion estimation with minimal error	motion estimation video compression image coding distortion measurement computer science image quality local activities bit rate dynamic programming heuristic algorithms;efficient dynamic programming;dynamic programming;evaluation performance;programacion dinamica;variable sized square blocks;prediction error;estimation mouvement;image coding;algorithm performance;video coding motion estimation image matching quadtrees adaptive signal processing image representation dynamic programming image sequences data compression;quad tree;performance evaluation;image processing;data compression;motion compensation;forme onde;quad arbol;image matching;codigo longitud variable;estudio comparativo;evaluacion prestacion;local activities;estimacion movimiento;heuristic method;variable size block matching motion estimation;variable length code;selection of variables;erreur quadratique moyenne;optimal selection;video compression;procesamiento imagen;variable size block matching;metodo heuristico;motion estimation;local motion information;dynamic program;distortion measurement;local image activity;indexing terms;bit rate;codigo bloque;quadtree based algorithms;traitement image;bit rate variation;etude comparative;video coding;displacement representation;estimation erreur;forma onda;adaptive signal processing;code longueur variable;error estimation;resultado algoritmo;image representation;mean square error;heuristic algorithms;image quality;image sequences quadtree based algorithms variable size block matching motion estimation variable size block matching motion estimation local image activity adaptive bit allocation displacement representation residual data bit rate variation optimal selection prediction error quadtree based vsbm efficient dynamic programming variable sized square blocks local motion information experiments optimal algorithm motion compensation video compression;estimacion error;comparative study;programmation dynamique;performance algorithme;experiments	We report two techniques for variable size block matching (VSBM) motion compensation. Firstly an algorithm is described which, based on a quad-tree structure, results in the optimal selection of v ariable-sized square blocks. It is applied in a VSBM scheme in which the total mean squared error (MSE) is minimized. This provides the best-achie vable performance for a quad-tree based VSBM technique. Although it is computationally demanding and hence impractical for real-time codecs, it does provide a yardstick by which the performance of other VSBM techniques can be measured. Secondly, a new VSBM algorithm which adopts a ‘bottom-up’ approach is described. The technique starts by computing sets of ‘candidate’ motion vectors for fixed-size small blocks. Blocks are then ef f ctively merged in a quad-tree manner if the y have similar motion vectors. Theresult is a computationally-efficient VSBM technique which attempts to estimate the ‘true’ motion within the image. Both methods ha ve been tested on a number of real image sequences. In all cases the ne w ‘bottom-up’ technique was only marginally worse than the optimal VSBM method but significantly better than fix ed-size block matching and other kno wn VSBM implementations. Ke ywords: motion estimation, block matching, interframe coding	algorithm;codec;mean squared error;motion compensation;motion estimation;probabilistic turing machine;quadtree;real-time clock;tree structure	Injong Rhee;Graham R. Martin;S. Muthukrishnan;Roger A. Packwood	2000	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.825857	data compression;mathematical optimization;image processing;computer science;theoretical computer science;mathematics;algorithm;statistics	Vision	48.1914335825176	-18.623119748559407	101737
1db0fcb97659046597f6eefa8f9bb82b42e63ec2	research on image compression algorithm based on rectangle segmentation and storage with sparse matrix	compression algorithm;sparse matrices data compression image coding image segmentation;image coding;image segmentation;data compression;rssms compression algorithm image compression algorithm quarter tree decomposition image pixel points consistency condition image blocks compression ratio improvement rectangle segmentation and sparse matrix storage compression algorithm;image coding image segmentation sparse matrices image reconstruction compression algorithms psnr gray scale;satisfiability;image compression;sparse matrix storage image compression rectangle segmentation quarter tree decomposition;compression ratio;tree decomposition;sparse matrix;sparse matrices	The Quarter-tree decomposition of image compression method characters with relative simplicity and fast calculation, however compression ratio is not very high. In order to overcome this flaw, one new segmentation method named the Rectangle Segmentation is proposed, in which adjacent pixel points satisfying consistency condition are viewed as the same image block. Also, without the restriction of square which abides to 2n, the image block can be rectangle which reduces the amount of block, and improves the compression ratio. Image compression ratio can be further augmented by combining the storage method of sparse matrix. Therefore, a new image compression algorithm is proposed named the Rectangle Segmentation and Sparse Matrix Storage(RSSMS) compression algorithm. Simulation results indicate that the compression ratios of images using the new algorithm is 25.19% higher than those using the Quarter-tree decomposition method.	algorithm;data compression;flaw hypothesis methodology;image compression;pixel;simulation;sparse matrix;tree decomposition	Shengli Chen;Xiaoxin Cheng;Jiapin Xu	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6233969	data compression;lossy compression;image texture;color cell compression;computer vision;adaptive scalable texture compression;block truncation coding;sparse matrix;image compression;computer science;theoretical computer science;segmentation-based object categorization;pattern recognition;mathematics;lossless compression;fractal transform;image segmentation;scale-space segmentation;texture compression	Robotics	43.59982032873401	-16.4926154915522	101868
4df422d623b97a4e7b1eb59e766c22ed97e31489	hybrid fractal/jpeg image compression: a case study.	image compression			William A. Stapleton;Krirkkrit Sripaipan;David Jeff Jackson;Kenneth G. Ricks	2004			data compression;lossy compression;lossless jpeg;data compression ratio;jpeg;lossless compression;compression artifact;fractal transform;fractal compression;texture compression;quantization	HCI	42.504750585221025	-15.92507147486683	102110
6a8add9fec5496603da013c8c81573c38e66bea5	lossless image compression using gradient based space filling curves (g-sfc)		In most classical lossless image compression schemes, images are scanned line by line, and so, only horizontal patterns are effectively compressed. The proposed approach attempts to better explore image correlation in different directions by adopting a context-based adaptive scanning process. The adopted scanning process aims to generate a compact one-dimensional image representation by using an image gradient based scan process. This process tries to find the best space-filling curve that ensures scanning the image according to the direction where minimal pixels’ intensity change is found. Such scan process would reduce high frequency data. It is used in order to provide an easily compressible smooth and highly correlated mono-dimensional signal. The suggested representation acts as a pre-processing which transforms the image source into some strongly correlated representation before applying coding algorithms. Based on this representation, a new lossless image compression method is designed. Our experimental results show that the proposed image representation is able to significantly improve the signal proprieties in terms of correlation and monotony and then compression performances. The suggested coding scheme shows a competitive compression results compared to conventional lossless coding schemes such as PNG and JPEG 2000.	algorithm;algorithmic efficiency;dictionary;embedded system;encoder;human visual system model;image compression;image gradient;image retrieval;jpeg 2000;lenna;lossless compression;lossy compression;nonlinear system;performance;portable network graphics;preprocessor;requirement;space-filling curve;synthetic intelligence	Tarek Ouni;Arij Lassoued;Mohamed Abid	2015	Signal, Image and Video Processing	10.1007/s11760-013-0435-4	data compression;lossy compression;lossless jpeg;computer vision;data compression ratio;mathematical optimization;pyramid;feature detection;image compression;theoretical computer science;mathematics;lossless compression	Vision	43.087755008934174	-15.09798461148096	102222
060476163b2b4b035c79fb166fb225f3fbb0978b	pattern-based compression of text images	image coding;data compression;text documents;lossiness;decompression time;document image processing image coding data compression;text images;lossiness pattern based compression text images text documents font prototype implementation decompression time lossy scheme;document image processing;prototype implementation;image coding prototypes gray scale encoding digital systems frequency computer science grid computing books turning;font;pattern based compression;lossy scheme	We suggest a novel approach for compressing images of text documents based on building up a simple derived font from patterns in the image, and present the results of a prototype implementation based on our approach. Our prototype achieves better compression than most alternative systems, and the decompression time appears substantially shorter than other methods with the same compression rate. The method has other advantages, such as a straightforward extension to a lossy scheme that allows one to control the lossiness introduced in a well-de ned manner. We believe our approach will be applicable in other domains as well.	color image;data compression;dictionary;html element;image compression;lossy compression;prototype	Andrei Z. Broder;Michael Mitzenmacher	1996		10.1109/DCC.1996.488335	data compression;lossy compression;computer vision;speech recognition;image compression;computer science;theoretical computer science;mathematics;lossless compression;context-adaptive binary arithmetic coding;statistics	ML	39.75091345560006	-14.889398863885106	102223
ceeb021faaa3fe45eef101fbeb496d15584346b3	an image watermarking scheme in wavelet domain with optimized compensation of singular value decomposition via artificial bee colony	artificial bee colony;invariant wavelet transform;optimization;entropy;image watermarking	Digital image watermarking is the process of authenticating a digital image by embedding a watermark into it and thereby protecting the image from copyright infringement. This paper proposes a novel robust image watermarking scheme developed in the wavelet domain based on the singular value decomposition (SVD) and artificial bee colony (ABC) algorithm. The host image is transformed into an invariant wavelet domain by applying redistributed invariant wavelet transform, subsequently the low frequency sub-band of wavelet transformed image is segmented into non-overlapping blocks. The most suitable embedding blocks are selected using the human visual system for the watermark embedding. The watermark bits are embedded into the target blocks by modifying the first column coefficients of the left singular vector matrix of SVD decomposition with the help of a threshold and the visible distortion caused by the embedding is compensated by modifying the coefficients of the right singular vector matrix employing compensation parameters. Furthermore, ABC is employed to obtain the optimized threshold and compensation parameters. Experimental results, compared with the related existing schemes, demonstrated that the proposed scheme not only possesses the strong robustness against image manipulation attacks, but also, is comparable to other schemes in term of visual quality.	artificial bee colony algorithm;digital watermarking;singular value decomposition;wavelet	Musrrat Ali;Chang Wook Ahn;Millie Pant;Patrick Siarry	2015	Inf. Sci.	10.1016/j.ins.2014.12.042	computer vision;entropy;mathematical optimization;theoretical computer science;mathematics;wavelet packet decomposition	AI	40.637771069683	-10.56713865815011	102484
51b194e68138a832845755577a1b5b4767ea7cc2	low-complexity, near-lossless coding of depth maps from kinect-like depth cameras	free viewpoint video;video coding cameras codecs data compression image segmentation image sensors;image segmentation;codecs;data compression;code optimization;low complexity;image sensors;face tracking;video coding;sensors cameras accuracy quantization psnr entropy coding;compression ratio;word length 16 bit low complexity coding near lossless coding depth maps kinect like depth cameras rgb foreground background segmentation face tracking activity detection free viewpoint video rendering near lossless codec video frames code optimization;depth map;cameras;foreground background	Depth cameras are gaining interest rapidly in the market as depth plus RGB is being used for a variety of applications ranging from foreground/background segmentation, face tracking, activity detection, and free viewpoint video rendering. In this paper, we present a low-complexity, near-lossless codec for coding depth maps. This coding requires no buffering of video frames, is table-less, can encode or decode a frame in close to 5ms with little code optimization, and provides between 7:1 to 16:1 compression ratio for near-lossless coding of 16-bit depth maps generated by the Kinect camera.	16-bit;algorithm;codec;data compression;depth map;digital camera;encode;front and back ends;hdmi;kinect;lossless compression;lossy compression;mathematical optimization;program optimization;scott continuity;usb	Sanjeev Mehrotra;Zhengyou Zhang;Qin Cai;Cha Zhang;Philip A. Chou	2011	2011 IEEE 13th International Workshop on Multimedia Signal Processing	10.1109/MMSP.2011.6093803	data compression;computer vision;facial motion capture;codec;computer science;foreground-background;program optimization;compression ratio;image sensor;multimedia;image segmentation;statistics;depth map;computer graphics (images)	Vision	43.34927489408749	-20.276831371553623	102595
d6db59382c796183a100161ce774c2bdac7e25df	motion classified 3d vector quantization for sequence coding	temporal correlation;interpolation;vector quantization interpolation mesh generation image reconstruction image sequences image coding algorithm design and analysis computational modeling psnr bit rate;classified multi codebook approach;image coding;image segmentation;psnr;trilinear interpolation;motion compensation;motion classified 3d vector quantization;image classification;motion estimation;average bit rate;correlation methods;bit rate;image classification image sequences vector quantisation motion estimation motion compensation image coding correlation methods image segmentation interpolation video coding;temporally changing regions;interframe coding;video coding;computational modeling;vq;spatial correlation;vector quantization;intraframe coding;image reconstruction;motion classifier;image sequence;image sequence coding;vector quantizer;salesman sequence;vector quantisation;mesh generation;computer simulation;algorithm design and analysis;mc3dvq;bits per pixel;temporal change;image sequences;mc3dvq image sequence coding motion classified 3d vector quantization interframe coding intraframe coding temporally changing regions temporal correlation spatial correlation motion classifier classified multi codebook approach salesman sequence psnr average bit rate trilinear interpolation vq	This paper proposes a new strategy to code image sequences using Motion Classified 3D Vector Quantization (MC3DVQ). Typical VQ based sequence coding schemes perform interframe coding by means of label replenishment only at temporally stationary regions, while intraframe coding is applied to the temporally changing (with motion) regions. The proposed algorithm differs from these methods in such a way that it attempts to perform intraframe and interframe coding concurrently at the temporally changing regions by exploiting the spatial and temporal correlation at the same time. A novel motion classifier is also developed to provide models for various types of motion, leading to a classified multi-codebook approach with better performance and less computations. Simulation results show that the reconstructed sequence “Salesman” has an average PSNR of 30.7 dB with good fidelity, at an average bit rate of 0.126 bits per pixel (bpp).	algorithm;codebook;color depth;computation;intra-frame coding;peak signal-to-noise ratio;pixel;simulation;stationary process;vector quantization	Hung-Kai Cliff;C. K. Chan	1996		10.1109/ICIP.1996.560470	iterative reconstruction;computer simulation;algorithm design;mesh generation;computer vision;contextual image classification;spatial correlation;color depth;peak signal-to-noise ratio;interpolation;computer science;theoretical computer science;pattern recognition;motion estimation;mathematics;image segmentation;motion compensation;computational model;vector quantization;trilinear interpolation	ML	46.1656046614232	-18.383468304507502	102702
95827af1454159bb734321ca52701507d01295d5	distortion caused by dct-based image compression techniques on spatial-domain watermark	image compression		discrete cosine transform;distortion;image compression	Vo Hao Tu;Tom Hintz	2005			image compression;watermark;computer network;computer science;computer vision;distortion;data compression;discrete cosine transform;texture compression;artificial intelligence	Vision	41.65651245421369	-12.195537338018013	103023
08b6408131fc6f6781ccc4168d8fa455d47fbb54	perceptual preprocessing techniques applied to video compression: some result elements and analysi	data compression;video compression;code standards;physics based approach perceptual preprocessing video coding picture quality low bit rate video compression high quality video compression decoding compression efficiency luminance chrominance color preprocessing h 26l encoder itu standard;video coding;video compression decoding video coding bit rate image coding color testing videoconference humans eyes;telecommunication standards;visual perception;visual perception video coding data compression code standards telecommunication standards	Recent developments in video coding research deal with solutions to improve picture quality while decreasing bit rates. However, no major breakthrough in compression emerged and low bit rate high quality video compression is still an open issue. The compression scheme is generally decomposed into two stages: coding and decoding. In order to improve compression efficiency, a complementary solution may consist in introducing a preprocessing stage before the encoding process or/and a post-processing step after decoding. For this purpose, instead of using the usual ( ) V U Y , , representation space to compress the video signal, where the video is encoded along different separate channels (luminance Y, chrominance U, chrominance V), we propose to choose other channels by means of a color pre-processing based upon perceptual and physics-based approaches. In this manner, each original image is transformed into a new space. The encoding/decoding stage is performed in this new representation space. Then, the inverse transform at the decoder side permits to recover the color image in the (Y,U,V) space. A linear transformation, called Opponent color space, has already been published by Watson et al. We propose non linear transformations driven by our experience or extrapolated from the literature: a first general idea consists in normalizing the (Y,U,V) space to obtain a new space such as ( ) Y V Y U Y , , or Y b C s Y t C s Y , , . Another idea consists in using the HSL	code;color image;computer programming;data compression;display resolution;emoticon;extrapolation;image quality;preprocessor;telecommunications link;video post-processing	Gwenaëlle Marquant	2002		10.1109/DCC.2002.1000006	video compression picture types;data compression;scalable video coding;computer vision;data compression ratio;h.263;block truncation coding;computer science;video quality;video tracking;coding tree unit;mathematics;lossless compression;block-matching algorithm;multimedia;video processing;smacker video;context-adaptive binary arithmetic coding;motion compensation;h.262/mpeg-2 part 2;h.261;statistics;multiview video coding;computer graphics (images)	Vision	43.94880085430345	-19.14166419644995	103058
ff56e10b449d6ce16a59afc080f25e5bc8505aba	error concealment in video communications using dpcm bit stream embedding	stress;watermarking;perceptual quality;video transmission defects;low gain spread spectrum watermarking;data hiding;propagation losses;dpcm bit stream embedding;error concealment;low resolution;differential pulse code modulation video coding error correction watermarking data encapsulation discrete cosine transforms spread spectrum communication;ecdh;data mining;differential pulse code modulation;video communication error concealment;data encapsulation;video coding;spread spectrum communication;streaming media;discrete cosine transforms;error correction;image reconstruction;cox watermarking algorithm;streaming media data encapsulation data mining computer errors discrete cosine transforms spread spectrum communication watermarking image reconstruction stress propagation losses;lexicographic order;spread spectrum watermarking;video end user perceptual quality;block based 2d dct;lexicographically ordered extracted dpcm bits;video communication;side information cox watermarking algorithm video communication error concealment dpcm bit stream embedding data hiding ecdh video end user perceptual quality video transmission defects block based 2d dct dpcm encoded video frame low gain spread spectrum watermarking lexicographically ordered extracted dpcm bits;side information;dpcm encoded video frame;computer errors	An error concealment using data hiding (ECDH) technique is proposed to improve the end user perceptual quality of videos that are affected by transmission defects. A 2/spl times/2 set of coefficients from each block of block-based 2D DCT of the video frame is encoded using DPCM and embedded in the frame itself using a low gain spread spectrum watermarking technique. The DPCM bit stream is ordered into a binary block-image, which is approximately 4 times smaller than the video frame, and embedded in the frame's mid-frequencies. At the receiver, the extracted DPCM bits are lexicographically ordered and a reference is reconstructed for error concealing the channel loss affected video frame. The technique is closely compared with a similar approach which embeds halftoned bits of a low resolution version of the video frame in itself. Also, a comparison is done when the DPCM encoded bits are transmitted as side information through the channel rather than as embedded data. Experimental results show that the technique that uses DPCM bit stream embedding is more effective than the other two ECDH techniques as well as most other error concealment algorithms.	algorithm;bitstream;coefficient;discrete cosine transform;embedded system;error concealment;image resolution;lexicographical order;transmission (bittorrent client)	Chowdary Adsumilli;Sanjit K. Mitra	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415368	iterative reconstruction;residual frame;computer vision;elliptic curve diffie–hellman;error detection and correction;image resolution;telecommunications;digital watermarking;computer science;theoretical computer science;lexicographical order;stress;information hiding;spread spectrum	Embedded	40.83350764053601	-13.714873591819886	103189
7d18e5d726ad8f260adab3975fe77c8588b4c7f3	nested object watermarking: comparison of block-luminance and blue channel lsb wet paper code image watermarking	filigranage numerique;protection information;digital watermarking;herencia;anotacion;modulation phase;image coding;modulacion fase;image processing;data compression;hierarchized structure;phase modulation;heritage;structure arborescente;luminance;procesamiento imagen;structure hierarchisee;photography;annotation;photographic image;traitement image;algorithme;algorithm;codage image;photographie;compression image;image compression;proteccion informacion;image photographique;estructura arborescente;information protection;robustesse;filigrana digital;tree structure;fotografia;imagen fotografica;transparency;robustness;image watermarking;compresion dato;experimental evaluation;inheritance;object relational;estructura jerarquizada;compression donnee;robustez;algoritmo;compresion imagen;luminancia	Annotation watermarking (sometimes also called caption or illustration watermarking) denotes a specific application of watermarks, which embeds supplementary information directly in the media, so that additional information is intrinsically linked to media content and does not get separated from the media by non-malicious processing steps such as image cropping or compression. Recently, nested object annotation watermarking (NOAWM) has been introduced as a specialized annotation watermarking domain, whereby hierarchical object information is embedded in photographic images. In earlier work, the Hierarchical Graph Concept (HGC) has been suggested as a first approach to model object relations, which are defined by users during editing processes, into a hierarchical tree structure. The original HGC method uses a code-book decomposition of the annotation tree and a block-luminance algorithm for embedding. In this article, two new approaches for embedding nested object annotations are presented and experimentally compared to the original HGC approach. The first one adopts the code-book scheme of HGC using an alternative embedding based on Wet Paper Codes in blue-channel LSB domain, whereas the second suggests a new method based on the concept of intrinsic signal inheritance by sub-band energy and phase modulation of image luminance blocks. A comparative experimental evaluation based on more than 100 test images is presented in the paper, whereby aspects of transparency and robustness with respect to the most relevant image modifications to annotations, cropping and JPEG compression, are discussed comparatively for the two code-book schemes and the novel inheritance approach.	channel (digital image);digital watermarking;least significant bit	Claus Vielhauer;Jana Dittmann	2007		10.1117/12.703848	data compression;computer vision;image processing;digital watermarking;image compression;photography;theoretical computer science;phase modulation;luminance;tree structure;transparency;information protection policy;robustness;computer graphics (images)	Robotics	44.65475489731212	-11.644618201538359	103260
b9c7484178e97aa6ec5e1c709c08b7de6304a074	adaptive linear prediction for block-based lossy image coding	rate distortion;image coding;least mean squares methods;least square error;training;linear predictive;intra prediction;coding gain;video coding least mean squares methods linear predictive coding rate distortion theory;rate distortion theory;video coding;linear predictive coding;automatic voltage control;pixel;least square error method adaptive linear prediction model block based lossy image coding video coding framework h 264 avc line based linear prediction model intra prediction modes rate distortion coding efficiency;transforms;image coding automatic voltage control predictive models video coding prediction methods design methodology image reconstruction least squares methods signal processing decoding;predictive models;encoding	Linear prediction model has been well investigated and applied in lossless image and video coding. In this paper, we investigate the linear prediction method for block-based lossy image coding and propose a method that merges linear prediction technique into H.264/AVC video coding framework. A block-based linear prediction method is designed instead of pixel-based one in order to cooperate with transform module. Furthermore, line-based linear prediction with 1D transform is developed by considering coding gain tradeoff between prediction and transform. Linear prediction model coefficients are derived by using neighboring reconstructed data with least square error method. The model coefficients implicitly embed the local texture characteristics and no bits overhead is needed for signaling the coefficients since we can derive them with same process at decoder side. We insert block-based and line-based linear prediction modes into H.264/AVC as additional intra prediction modes and select the best mode by minimum rate-distortion sense. Experimental results show that the proposed technique improves coding efficiency of H.264/AVC intra picture with average 4.3% bit saving and up to 7.0% bit saving.	algorithmic efficiency;coding gain;coefficient;data compression;distortion;h.264/mpeg-4 avc;intra-frame coding;lossless compression;lossy compression;overhead (computing);pixel	Jianle Chen;Woojin Han	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414476	linear predictive coding;rate–distortion theory;computer science;theoretical computer science;context-adaptive variable-length coding;machine learning;coding gain;predictive modelling;context-adaptive binary arithmetic coding;pixel;encoding;statistics;code-excited linear prediction	Vision	44.94174138159868	-16.23222969920454	103375
538e36f1a9ae1851dfdf1e38c5ba03054f3a6b10	dct computation based on variable complexity fast approximations	data compression;computational complexity;encoding;video compression;approximation theory;image quality;degradation;transform coding;discrete cosine transform;floating point;arithmetic;quantization	We investigate input dependent, variable complexity algorithms to compute the DCT. The basic goal of these algorithms is to not compute those DCT coefficients that will be quantized to zero. These algorithms exploit the fact that for compression applications (i) most of the energy is concentrated in a few DCT coefficients and (ii) as the quantization step size increases an increased number of coefficients is set to zero and reduced precision computation of the DCT may be tolerable. Thus we propose two classes of algorithms, the first one selectively prunes the DCT computation while the second uses an approximate computation, without floating point multiplications, that is matched to the quantization level selected.	approximation;computation;discrete cosine transform	Krisda Lengwehasatit;Antonio Ortega	1998			data compression;mathematical optimization;discrete mathematics;transform coding;trellis quantization;computer science;floating point;theoretical computer science;discrete cosine transform;mathematics;context-adaptive binary arithmetic coding;computational complexity theory;quantization;algorithm;statistics;approximation theory	Robotics	44.77402356517296	-13.973710853304603	103978
5718a30674a2fd9efaecaa820b947b7adaac4913	an adaptable spatial-temporal error concealment method for multiple description coding based on error tracking	burst packet loss simulation adaptable spatial temporal error concealment method multiple description coding error tracking combat error prone channel adaptable temporal spatial error concealment channel model;decoding image coding conferences bridges encoding correlation;image coding;error concealment;decoding;gilbert model;error tracking;packet loss;bridges;p2p;video coding;channel model;gilbert model multiple description coding error concealment error tracking;multiple description coding;correlation;encoding;conferences	Multiple Description Coding (MDC) is one of the most efficient methods to combat error-prone channels especially when retransmission is unacceptable. In applications involving scalable, multicast and P2P environments, it is advantageous to use more than two descriptions. In this paper, we present an adaptable temporal-spatial error concealment method based on error tracking to improve the performance of MDC. A Gilbert model is used as the channel model for burst packet loss simulation. Experimental results demonstrate the efficacy of the proposed method.	channel (communications);cognitive dimensions of notations;error concealment;gilbert cell;multicast;multiple description coding;network packet;peer-to-peer;retransmission (data networks);scalability;simulation	Meilin Yang;Mary L. Comer;Edward J. Delp	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116434	speech recognition;telecommunications;computer science;multiple description coding;peer-to-peer;packet loss;correlation;encoding	Robotics	48.648075333288745	-16.29639055733775	104102
0c63b5c14eb2ef40879d0f718fe2e7f77ab202b0	in-loop denoising of reference frames for lossless coding of noisy image sequences	compression gain;lossless video coding;adaptive wiener filter;image coding;wiener filters adaptive filters encoding image denoising image sequences video coding;in loop denoising;motion compensated prediction lossless video compression predictor denoising;additive noise;video compression;reference frame;wiener filters;bit rate;noisy image sequence;intra prediction;adjacent frame;video coding;adaptive filters;automatic voltage control;h 264 avc standard;noise reduction;interprediction;noise noise reduction image coding bit rate encoding image sequences automatic voltage control;image sequence;lossless video compression;image denoising;wiener filter;motion compensated prediction;temporal prediction;encoding;intrapredicted block;interprediction in loop denoising reference frame noisy image sequence image coding temporal prediction adjacent frame lossless video coding additive noise adaptive wiener filter compression gain h 264 avc standard intrapredicted block;predictor denoising;noise;image sequences	The major gain in video coding applications compared to single image coding is the use of temporal prediction, which exploits the correlation between adjacent frames. However, in high quality video coding, especially lossless video coding, the compression gain of P-frames over I-Frames becomes very small. The reason for that is that the reference frame for inter prediction is not good enough, and therefore the amount of inter-predicted blocks in a P-Frame becomes relatively small compared to the number of intra-predicted blocks. In order to generate a better predictor for inter-prediction, we propose to remove additive noise from the reference frame using an adaptive Wiener filter. This way, we could achieve a maximum compression gain of 4.6% and an average compression gain of 3.3% in contrast to the H.264/AVC standard for lossless coding of high quality image sequences without affecting the encoding time noticeably.	additive white gaussian noise;autostereogram;data compression;display resolution;h.264/mpeg-4 avc;kerrison predictor;lossless compression;noise reduction;principle of good enough;reference frame (video);utility functions on indivisible goods;wiener filter	Eugen Wige;Peter Amon;Andreas Hutter;André Kaup	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5654136	data compression;reference frame;lossy compression;inter frame;adaptive filter;sub-band coding;lossless jpeg;computer vision;computer science;entropy encoding;noise;theoretical computer science;context-adaptive variable-length coding;coding tree unit;noise reduction;mathematics;lossless compression;wiener filter;adaptive coding;context-adaptive binary arithmetic coding;motion compensation;encoding;statistics;golomb coding	Vision	45.56848337468095	-17.338956362004012	104103
3c197a33cb9bd1e036bd1b7a91fcabb3fc3dda5d	performance enhancement of sum of absolute difference (sad) computation in h.264/avc using saturation arithmetic	sum of absolute difference;sad;h 264 avc;reference frame;video quality;video processing;motion estimation;video coding;saturation arithmetic;computational complexity	Sum of Absolute Difference (SAD) Computation is commonly used for motion estimation in video coding. It is usually the computationally intensive part in video processing. Therefore, a method to reduce the computational complexity is strictly required. In this paper, the effectiveness of saturation arithmetic on SAD computation is presented. Our goal is to use saturation arithmetic to reduce the complexity of SAD computation for the encoding process while the accuracy in finding the best matching block from the reference frame is still maintained. Experiment results show that the computational complexity of SAD computation is reduced efficiently by saving a number of bits for SAD values representation while the video quality is kept.	computation;h.264/mpeg-4 avc;saturation arithmetic	Trung Hieu Tran;Hyo-Moon Cho;Sang-Bock Cho	2009		10.1007/978-3-642-04070-2_45	reference frame;discrete mathematics;real-time computing;computer science;video quality;theoretical computer science;saturation arithmetic;motion estimation;mathematics;video processing;context-adaptive binary arithmetic coding;computational complexity theory;algorithm	NLP	47.68919811821859	-19.034871103244647	104964
7fbeac8a6a3cf6bab1afea2e44f52e8ae71dfd7d	fast intra prediction mode decision for h.264/avc	peak signal to noise ratio	In this letter, we present a simple but efficient intra prediction mode decision for H.264/AVC. Based on our investigation, the DC mode appears to be the superior prediction mode among the various candidates. We propose an intra-mode decision algorithm where the DC mode is chosen as a candidate for the best prediction mode. By experimental results, on average, the proposed algorithm significantly saves 81.905% of the entire encoding time compared to the H.264 reference software; besides, it reduces negligible peak signal-to-noise ratio (PSNR) values and slightly increases bitrates. key words: H.264/AVC, video coding, intra-mode decision	algorithm;data compression;h.264/mpeg-4 avc;intra-frame coding;peak signal-to-noise ratio	Do Quan;Yo-Sung Ho	2010	IEICE Transactions		computer vision;real-time computing;simulation;peak signal-to-noise ratio;computer science;theoretical computer science;statistics	AI	46.91478545225093	-19.11048363860382	105168
7f16dfd1d58860dd9814d692df01245579e2e89e	a secure fragile watermarking scheme based on chaos-and-hamming code	authentication;fragile watermarking;logistic map;chaotic map;burst bits;vector quantizer;hamming code;vq attack	In this work, a secure fragile watermarking scheme is proposed. Images are protected and any modification to an image is detected using a novel hybrid scheme combining a two-pass logistic map with Hamming code. For security purposes, the two-pass logistic map scheme contains a private key to resist the vector quantization (VQ) attacks even though the embedding scheme is block independent. To ensure image integrity, watermarks are embedded into the to-be-protected images which are generated using Hamming code technique. Experimental results show that the proposed scheme has satisfactory protection ability and can detect and locate various malicious tampering via image insertion, erasing, burring, sharpening, contrast modification, and even though burst bits. Additionally, experiments prove that the proposed scheme successfully resists VQ attacks.	hamming code	Chin-Chen Chang;Kuo-Nan Chen;Chin-Feng Lee;Li-Jen Liu	2011	Journal of Systems and Software	10.1016/j.jss.2011.02.029	logistic map;computer science;theoretical computer science;authentication;hamming code;computer security	Embedded	39.466928588880315	-11.338937269401962	105481
e71f4cffbe901452198a2e3814e0d21c39442d73	fast h.264 inter mode decision based on inter and intra block conditions	motion analysis;probability table;homogeneous condition analysis;h 264 interframe coding;iso standards;video compression;advanced video coding;motion estimation;transform coding;variable block size;interblock conditions;acceleration;video coding;automatic voltage control;motion vector;fast algorithm;motion vector relation;h 264 interframe coding fast h 264 inter mode decision interblock conditions intrablock conditions video coding compression efficiency probability table homogeneous condition analysis motion vector relation;bandwidth;intrablock conditions;compression efficiency;video coding motion estimation;fast h 264 inter mode decision;mode decision;automatic voltage control motion estimation costs video compression bandwidth transform coding video coding acceleration iso standards motion analysis	In H.264 advanced video coding (AVC), the inter prediction with variable block sizes plays an important role to achieve compression efficiency. Although it actually raises the coding efficiency in H.264, it also increases the complexity due to the decision of the best inter mode. In this paper, we will propose a fast algorithm based on neighboring information and homogeneous condition to select the best mode effectively. For each macroblock, we first select the initial mode by using probability table and then use homogeneous condition analysis with motion vector relation to accelerate the inter mode decision process. Experimental results show that our proposed algorithm can save about 67~70% mode computation with negligible PSNR loss, which is very helpful for H.264 interframe coding.	algorithm;algorithmic efficiency;computation;data compression;h.264/mpeg-4 avc;macroblock;peak signal-to-noise ratio	Hung-Ming Wang;Ji-Kun Lin;Jar-Ferr Yang	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378543	data compression;acceleration;inter frame;computer vision;real-time computing;transform coding;computer science;theoretical computer science;motion estimation;mathematics;bandwidth;statistics	EDA	47.15411993214151	-19.010899966436998	105789
863de61d64af916089fdbdf8bfda8a8db63543cb	image-compression for wireless world wide web browsing: a neural network approach	information resources;web documents;learning algorithm;image coding;telecommunication computing;satisfiability;data clustering;hypermedia;interactive application;internet;image compression;telecommunication computing information resources internet image coding vector quantisation art neural nets learning artificial intelligence hypermedia;web sites image coding clustering algorithms ip networks internet bandwidth humans vector quantization resonance neural networks;input data clusters wireless world wide web browsing intermediary proxy network heterogeneity internet infrastructure image compression web documents bandwidth constraints compression rates image semantics modified adaptive resonance learning algorithm modified art2 learning algorithm;world wide web;vector quantizer;quality standard;internet application;art neural nets;learning artificial intelligence;vector quantisation;human perception;neural network	The implementation of an intermediary proxy is a common approach to the problem of network heterogeneity in the Internet infrastructure. Due to the hypertext nature of the most popular Internet application the World Wide Web, image compression is considered to be one of the fundamental functions of such a proxy. It has been observed that most images embedded into Web documents are of 'information-delivery' type, so an algorithm intended for their compression has to satisfy some specific requirements. First, in order to support network (bandwidth) constraints for an arbitrary case, the algorithm should be inherently adaptive, i.e. able to provide a wide range of compression rates. Second, as dealing with images that are integral parts of an interactive application (such as a Web browser), the algorithm should be capable of preserving a sufficient level of image semantics according to the quality standards of human perception. Vector quantization (VQ) technique, in its general form, is proven to satisfy the first requirement. On the other hand, a modified adaptive resonance (modified ART2) learning algorithm (which we employ in this paper) more properly belongs to the family of NN algorithms whose main goal is the discovery of input data clusters, without considering their actual size. This feature makes the modified ART2 algorithm satisfy the second requirement. Thus, the discussion and results presented in this paper are intended to show that modified ART2 underlying the general VQ procedure is an appropriate techniques for image compression purposes in a bandwidth-constrained environment.	algorithm;artificial neural network;browsing;cluster analysis;codebook;embedded system;general-purpose modeling;hypertext;image compression;image quality;jpeg;requirement;resonance;rich internet application;vector quantization;web page;world wide web	Natalija Vlajic;Thomas Kunz;Howard C. Card	2000		10.1109/IJCNN.2000.857832	quality control;the internet;image compression;computer science;theoretical computer science;machine learning;data mining;cluster analysis;perception;world wide web;artificial neural network;satisfiability	ML	40.41957565089231	-21.793461475058635	105802
1922707876506724a77965855f11eec57f13dcfb	motion estimation with the redundant wavelet transform	motion compensation;multiscale representation;reference frame;adaptive dynamics;motion estimation;motion estimation wavelet transforms motion compensation video compression algorithm design and analysis channel capacity pixel contracts mathematics video sequences;optic flow motion estimation redundant wavelet transform motion flow computation redundant haar transform video sequence multiscale representation video encoding multiscale correlation haar block channel capacity motion compensation;wavelet transforms;video coding;wavelet transform;channel capacity;image representation;image representation motion estimation motion compensation video coding haar transforms wavelet transforms channel capacity image sequences;haar transforms;high efficiency;image sequences	The weakness of the state of the art encoders lies at two levels. The first is the inability of these encoders to capture motion occurring within their decomposition blocks. The second is their fixed allocation of bits to describing the motion field. It would be better to dynamically allocate bits to motion and residual in a progressive streaming environment. Addressing these deficiencies would have two effects. Firstly, it would make full and optimal use of channel capacity. Secondly it would allow the user to select a bit rate depending on the bandwidth available to him. Thereby, the user able to purchase more bandwidth will receive better quality video.	channel capacity;encoder;motion estimation;motion field;stationary wavelet transform	Ronald A. DeVore;Alexander Petukhov;Robert C. Sharpley	2002		10.1109/DCV.2002.1218743	computer vision;theoretical computer science;motion estimation;mathematics;discrete wavelet transform;motion compensation;computer graphics (images)	Vision	45.4782468425122	-16.950403623855365	105911
643776a27bb0082c248addc18e9c42fad2c19b41	avc intraprediction mode decision based on 4x4 integer transform coefficients	rate distortion;decoding;intraprediction size decision;video coding;audio coding;audio video encoding avc intraprediction mode integer transform coefficient intraprediction size decision;audio video encoding;automatic voltage control;integer transform coefficient;discrete cosine transforms;feature extraction;transforms;video coding audio coding transforms;automatic voltage control encoding video coding discrete cosine transforms computational efficiency rate distortion costs decoding entropy feature extraction;entropy;avc intraprediction mode;computational efficiency;encoding;mode decision	In order to achieve AVCs fall potential in intracoding efficiency an encoder must choose an appropriate coding option for each block in every frame. This leads to a significant computational effort. In this paper, we investigate employing the AVC 4times4 integer transform in order to decide intraprediction size and mode for AVC encoding in an efficient manner. The transform is used to constrain the encoding options with a feasible overhead, as it can be implemented using shifts and additions only.	algorithm;coefficient;computation;computational complexity theory;cost efficiency;distortion;encoder;h.264/mpeg-4 avc;macroblock;mathematical optimization;overhead (computing);pixel;rate–distortion optimization	Florian Obermeier;Marko Durkovic;Michael Zwick;Klaus Diepold	2007	Eighth International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS '07)	10.1109/WIAMIS.2007.25	computer vision;entropy;feature extraction;computer science;theoretical computer science;context-adaptive variable-length coding;machine learning;mathematics;encoding	Robotics	46.290978548635195	-18.19231263002543	105974
ef157daf273613bfd735948fcb7947a6b2a40f12	fast scalable coding based on a 3d low bit rate fractal video encoder		Video transmissions usually occur at a fixed or at a small number of predefined bit rates. This can lead to several problems in communication channels whose bandwidth can vary along time (e.g. wireless devices). This work proposes a video encoding method for solving such problems through a fine rate control that can be dynamically adjusted with low overhead. The encoder uses fractal compression and a simple rate distortion heuristic to preprocess the content in order to speed up the process of switching between different bit rates. Experimental results show that the proposed approach can accurately transcode a preprocessed video sequence into a large range of bit rates with a small computational overhead.	algorithm;bandwidth (signal processing);block code;data compression;distortion;encoder;fractal compression;heuristic;overhead (computing);preprocessor;rate–distortion theory	Vitor de Lima;Thierry Pinheiro Moreira;Hélio Pedrini;William Robson Schwartz	2017		10.5220/0006100400240033	harmonic vector excitation coding	Networks	45.88059642991204	-16.093976006007516	106113
55d58a1df02d7c0715705367d518d445fdc036ff	adaptively embedding binary data in an image	information hiding;lsb;binary data	In this paper, we propose an embedding algorithm, of high visual quality, that can adaptively embed a binary message into an image. The binary message to be embedded is divided into two segments, each of which is then decomposed into n + 1 or n types of sub-messages (where n g 3 enables each pixel to embed a sub-message), respectively, according to the desired embedding capacity. Embedding is done by leaving the pixel value unchanged or changing it into one of its n or n − 1 neighboring values according to the type of the sub-message. From the results of this study, each pixel may not embed a fixed number of message bits and the adjustment of the pixel value is minimal, thus the image quality is significantly improved by adaptively decomposing the message into sub-messages and embedding them into the host image.	binary data	Ching-Chiuan Lin;Nien-Lin Hsueh;Wen-Hsiang Shen	2008	Fundam. Inform.		least significant bit;computer vision;discrete mathematics;computer science;theoretical computer science;mathematics;programming language;information hiding	ML	41.118505851176465	-12.970804446971552	106289
96167ced5ec2aff466e6748698185813e4e82bc0	dadu-p: a scalable accelerator for robot motion planning in a dynamic environment		As a critical operation in robotics, motion planning consumes lots of time and energy, especially in a dynamic environment. Through approaches based on general-purpose processors, it is hard to get a valid planning in real time. We present an accelerator to speed up collision detection, which costs over 90% of the computation time in motion planning. Via the octree-based roadmap representation, the accelerator can be reconfigured online and support large roadmaps. We in addition propose an effective algorithm to update the roadmap in a dynamic environment, together with a batched incremental processing approach to reduce the complexity of collision detection. Experimental results show that our accelerator achieves 26.5X speedup than an existing CPU-based approach. With the incremental approach, the performance further improves by 10X while the solution quality is degraded by 10% only.	motion planning;robot;scalability	Shiqi Lian;Yinhe Han;Xiaoming Chen;Ying Wang;Hang Xiao	2018		10.1109/DAC.2018.8465785	real-time computing;software;computation;octree;scalability;speedup;collision detection;motion planning;robotics;computer science;artificial intelligence	Robotics	50.71398411322538	-23.84125447082153	106313
da59f8ff391b9cf089cf0a0afbf92fc09dace192	efficient low-bit-rate adaptive mesh-based motion compensation technique	low bit rate;estimation mouvement;quad tree;image processing;quad arbol;estimacion movimiento;procesamiento imagen;motion estimation;traitement image;velocidad de bit debil;triangulacion;quad arbre;computing systems;triangulation;debit binaire faible	This paper proposes a two-stage global motion estimation method using a novel quadtree block-based motion estimation technique and an active mesh model. In the first stage, motion parameters are estimated by fitting block-based motion vectors computed using a new efficient quadtree technique, that divides a frame into equilateral triangle blocks using the quad-tree structure. Arbitrary partition shapes are achieved by allowing 4-to-1, 3-to-1 and 2-1 merge/combine of sibling blocks having the same motion vector . In the second stage, the mesh is constructed using an adaptive triangulation procedure that places more triangles over areas with high motion content, these areas are estimated during the first stage. finally the motion compensation is achieved by using a novel algorithm that is carried by both the encoder and the decoder to determine the optimal triangulation of the resultant partitions followed by affine mapping at the encoder. Computer simulation results show that the proposed method gives better performance that the conventional ones in terms of the peak signal-to-noise ration (PSNR) and the compression ratio (CR).	algorithm;computer simulation;data compression ratio;encoder;motion compensation;motion estimation;peak signal-to-noise ratio;quadtree;resultant;tree structure	Hanan A. Mahmoud;Magdy A. Bayoumi	2001		10.1117/12.438252	simulation;telecommunications;image processing;triangulation;quarter-pixel motion;quadtree;motion estimation;computer graphics (images)	Vision	46.18392242286196	-14.893908624974019	106448
d9bfaa23673bc9b1bf99d36dd9033684f0e577e1	the minimum detectable capacity of digital image information hiding	teoria imagen;documento electronico;attracteur;informacion numerica;image numerique;image theory;steganographie;information hiding;theorie image;attractor;atractor;document electronique;digital information;steganography;esteganografia;imagen numerica;borne inferieure;information capacity;attraction;information numerique;digital image;reseau neuronal;atraccion;red neuronal;lower bound;electronic document;cota inferior;neural network	Information hiding capacity of digital image is the maximum information that can be hidden in an image. But the lower limit of information hiding, the minimum detectable information capacity is also an interesting problem. This paper proposes a method to analyze the minimum detectable capacity of information hiding in digital images based on the theories of attractors and attraction basin of neural network. The results of research show that the attractors of neural network decide the lower limit of information hiding.	artificial neural network;channel capacity;digital image	Fan Zhang;Ruixin Liu;Xinhong Zhang	2006		10.1007/11760191_41	computer science;artificial intelligence;machine learning;mathematics;steganography;upper and lower bounds;programming language;information hiding;attractor;digital image;artificial neural network;algorithm;statistics	AI	44.758703259958594	-11.126875620638948	106461
efb89ef5f5eac969713f0abe3200b607ad3fe757	novel efficient architecture for jpeg2000 entropy coder	multimedia;image compression;discrete wavelet transform;computer programming;internet;jpeg2000	With the continual expansion of multimedia and Internet applications, the needs and requirements of advanced technologies, grew and evolved. With the increasing use of multimedia technologies, image compression techniques require higher performance as well as new features. Significant progress has recently been made in image compression techniques using discrete wavelet transforms. The overall performance of these schemes may be further improved by properly designing of efficient entropy coders. In this paper, we describe an efficient architecture for JPEG2000 entropy coder, which is a new standard to address the needs in the specific area of still image encoding. Our proposed architecture consists of two main parts, the coefficient bit modeler (CBM) and the binary arithmetic coder (BAC), which communicate through a FIFO buffer. Optimizations have been made in our proposed architecture to reduce accesses to memories. Our Proposed architecture is fast and modular and is suitable for real-time applications.	entropy encoding;jpeg 2000	Omid Fatemi;Parvin Asadzadeh Birjandi	2003			data compression;computer vision;simulation;telecommunications;image processing;image compression;computer science;theoretical computer science;signal processing;computer programming;discrete wavelet transform;algorithm	Arch	42.23579406453501	-16.28584098594996	106626
56af5484d88609f13ed52ba74c0eb0ca8076bb3b	harmonic postprocessing to conceal for transmission errors in dwt transmitted images	approximation rationnelle;discrete wavelet transforms;traitement signal;fonction harmonique;evaluation performance;discrete wavelet transform;performance evaluation;laplace equation;transmission error;ondelette;evaluacion prestacion;signal analysis;rational approximation;transformation cosinus discrete;subband decomposition;ecuacion laplace;coons;loss recovery;armonica;analisis de senal;transformation ondelette discrete;harmonic;error transmision;nurbs;funcion armonica;transform coding;qualite image;visual quality;aproximacion esplin;harmonique;descomposicion subbanda;spline approximation;discrete cosine transforms;approximation spline;signal processing;image quality;controle qualite;calidad imagen;b spline;rapport signal bruit;relacion senal ruido;decomposition sous bande;signal to noise ratio;quality control;aproximacion racional;procesamiento senal;harmonic function;wavelets;analyse signal;equation laplace;b splin;erreur transmission;codage transformation;control calidad	The discrete wavelet transform has become a dominant method in transform coding, but transmission of the resulting coefficients has lacked efficient postprocessing to recover from block loss in the lowest frequency wavelet subband. This paper relates the lost coefficients to the surrounding valid ones by the Laplace equation on a square grid. The properties of its solutions, viz. harmonic functions, ensure uniform and stable performance of the recovery scheme.#R##N##R##N#The method is compared with interpolative functions already used for block loss recovery, such as the bicubic Coons surface (the only reported method to recover lowest frequency subband block loss) and NURBS, used in DCT domain block loss recovery. Experiments on standard test images show uniform superiority in both visual quality and PSNR measurement.#R##N##R##N#The method can reconstruct both coefficients in the wavelet domain and pixels in the image domain. We compare results experimentally and analytically, to guide practical application.	discrete wavelet transform	Yan Niu;Tim Poston	2006	Sig. Proc.: Image Comm.	10.1016/j.image.2006.02.001	image quality;b-spline;wavelet;computer vision;harmonic function;quality control;transform coding;non-uniform rational b-spline;computer science;calculus;signal processing;harmonic;mathematics;geometry;discrete wavelet transform;signal-to-noise ratio;laplace's equation	HCI	45.5231716104246	-13.483833454931816	106724
88d99a499202bc41a22639db0c2bc4d9bd90e63b	fast motion vector re-estimation for transcoding mpeg-1 into mpeg-4 with lower spatial resolution in dct-domain	estimation mouvement;transcodage;motion compensation;transcodificacion;video signal processing;mpeg video;transformation cosinus discrete;estimacion movimiento;motion estimation;discrete cosine transform;motion compensated;compensation mouvement;video coding;codage video;discrete cosine transforms;motion vector;image quality;traitement signal video;mpeg;transcoding;spatial resolution	In this paper, we propose a fast motion vector re-estimation for transcoding MPEG-1 to MPEG-4 with lower spatial resolution. This task can be performed in the pixel-domain or in the discrete cosine transform (DCT) domain. In this paper, we concentrate on the DCT-domain approach, which requires lower delay and complexity than those in the pixel-domain. For the DCT-domain transcoding to lower spatial resolution pictures, DCT-domain down-sampling filter is applied and a base motion vector (BMV) for the down-sampled MPEG-4 macroblock is to be calculated from the input motion vectors operating on the higher spatial resolution image. Quality can be significantly improved by refining the BMV. Starting with the BMV, the motion vector refinement (MVR) scheme searches for a delta motion vector within a significantly reduced search area. We propose a fast MVR scheme for video down-sampling in the DCTdomain based on minimizing the number of required check points, and a computationally efficient method for extracting motion compensated DCT block. We also show an efficient scheme for selecting a coding mode for a macroblock in the lower resolution video from those of the corresponding higher resolution video. r 2003 Elsevier B.V. All rights reserved.	algorithmic efficiency;computation;decimation (signal processing);discrete cosine transform;mpeg-1;macroblock;pixel;refinement (computing);sampling (signal processing);search algorithm;simulation	Kwang-deok Seo;Jae-Kyoon Kim	2004	Sig. Proc.: Image Comm.	10.1016/j.image.2003.11.001	image quality;computer vision;transcoding;image resolution;quarter-pixel motion;computer science;discrete cosine transform;motion estimation;motion compensation;algorithm;computer graphics (images)	Vision	46.48047278435712	-15.808010285183098	107035
cfb5de46a26e9cd8aa6360ec68242244b52e7886	fast coding unit depth decision for hevc	support vector machines;encoding bit rate support vector machines classification algorithms video coding prediction algorithms educational institutions;prediction algorithms;bit rate;hevc;svm coding unit depth decision high efficiency video coding hevc prediction unit transform unit mode decision encoder complexity rate distortion cost mean square error methods mse content adaptation binary classification problem offline trained support vector machine;video coding;classification algorithms;content adaptation;encoding;video coding and processing;content adaptation video coding and processing hevc;video coding mean square error methods rate distortion theory support vector machines	High Efficiency Video Coding (HEVC) achieves high efficiency by introducing a new coding structure in adoption of coding unit (CU), prediction unit (PU) and transform unit (TU). However, it also imposes great computation burden on the mode decision of encoders. In this paper, we propose a fast CU depth decision scheme to reduce the encoder complexity for HEVC. Firstly, the relationship between rate-distortion (R-D) cost and CU depth is explored carefully with Mean Squared Error (MSE) and Number of Encoded Bits (NEB) metrics. Then CU splitting is modeled as a binary classification problem and resolved by an offline trained Support Vector Machine (SVM) model. The experimental results show that the proposed algorithm achieves up to 59% running-time reduction with negligible loss in terms of PSNR and bit rate.	algorithm;binary classification;computation;distortion;encoder;high efficiency video coding;mean squared error;one-class classification;online and offline;peak signal-to-noise ratio;support vector machine;tip (unix utility)	Fangshun Mu;Li Song;Xiaokang Yang;Zhenyi Luo	2014	2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2014.6890647	statistical classification;support vector machine;prediction;harmonic vector excitation coding;computer science;theoretical computer science;machine learning;coding tree unit;pattern recognition;context-adaptive binary arithmetic coding;h.261;encoding;multiview video coding	Robotics	46.72809289981951	-19.53998632732943	107224
f5c2a48c20485e3cc416604e2473becd427ab865	embedded wavelet coder with multistage vector quantization		An embedded image coder based on wavelet transform coding and multistage vector quantization (VQ) is proposed in this research. We have examined several critical issues to make the proposed algorithm practically applicable. They include the complexity of embedded VQ, design of the successive vector quantizer, signi cance evaluation of a vector symbol, and integration of wavelet transform coding and vector quantization. It is shown in experiments that the new method achieves a superior rate-distortion performance.	algorithm;arithmetic coding;biorthogonal wavelet;codebook;distortion;encode;embedded system;experiment;jbig;multistage amplifier;multistage interconnection networks;quantization (signal processing);transform coding;vector quantization;wavelet transform	Kai-Chieh Liang;Jin Li;C.-C. Jay Kuo	1997	Int. J. Imaging Systems and Technology	10.1002/(SICI)1098-1098(1997)8:5%3C444::AID-IMA5%3E3.0.CO;2-A	computer science;linde–buzo–gray algorithm;vector quantization	Robotics	46.713300523084456	-10.886496526169909	107264
b7e2912f5e48ebc9a7590136ae7da03cc8fd22cf	an efficient image retrieval method under dithered-based quantization scheme	image coding;standards;quantization signal;bit rate;feature extraction;image retrieval	Recent research works have shown the impact of the quantization techniques on the performance of standard image retrieval systems when datasets are compressed in a lossy mode. In this work, we propose to design an efficient retrieval method well adapted to wavelet-based compressed images. Our objective is to recover features of the original image (herein the moments of the unquantized subbands) directly from the quantized coefficients. To this end, we propose to apply a dithered quantization technique satisfying some specific conditions. Then, the estimated moments of the wavelet subbands are used in an appropriate way to construct the feature vectors of the database images. Experimental results show the interest of the proposed image retrieval method compared to the state-of-the-art ones.	coefficient;dither;feature vector;image retrieval;lossy compression;quantization (signal processing);wavelet	Amani Chaker;Mounir Kaaniche;Amel Benazza-Benyahia;Marc Antonini	2016	2016 24th European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2016.7760578	computer vision;computer science;theoretical computer science;information retrieval	Vision	41.880228315664766	-15.156267811667721	107284
aa8e2901339dff9a2a67d7cbbe795de52741b661	rlfc: random access light field compression using key views		We present a new hierarchical compression scheme for encoding light field images (LFI) that is suitable for interactive rendering. Our method (RLFC) exploits redundancies in the light field images by constructing a tree structure. The top level (root) of the tree captures the common high-level details across the LFI, and other levels (children) of the tree capture specific low-level details of LFI. Our decompressing algorithm corresponds to tree traversal operations and gathers the values stored at different levels of the tree. Furthermore, we use bounded integer sequence encoding which provides random access and fast hardware decoding for compressing the blocks of children of the tree. We have evaluated our method for 4D two-plane parameterized light fields. The compression rates vary from 0.08 − 2.5 bits per pixel (bpp), resulting in compression ratios of around 200:1 to 10:1 for a PSNR quality of 40 to 50 dB. The decompression times for decoding the blocks of LFI are 1 − 3 microseconds per channel on NVIDIA GTX-960 and we can render the LFIs at 200 fps. Our overall scheme is simple to implement and involves only bit manipulations and integer arithmetic operations.	algorithm;bit manipulation;cluster analysis;color depth;data compression;davis–putnam algorithm;exploit (computer security);file inclusion vulnerability;filter (signal processing);graphics processing unit;htc vive;headset (audio);high- and low-level;high-level programming language;image resolution;image warping;indirection;light field;mobile device;motion compensation;oculus rift;overhead (computing);peak signal-to-noise ratio;pixel;progressive meshes;random access;rendering (computer graphics);tree structure;tree traversal	Srihari Pratapa;Dinesh Manocha	2018	CoRR		compression ratio;rendering (computer graphics);tree structure;tree traversal;encoding (memory);color depth;decoding methods;algorithm;random access;computer science	Graphics	41.624178413705714	-19.8779328742471	107418
d48d73b8a38aa87d64f6548cfc82bc8186ec1736	web based teleoperation architecture and h.264 video encoder	peak signal to noise ratio teleoperations h 264 avc video compression;computers;human perception survey web based teleoperation architecture h 264 video encoder teleoperations robotics surgical robots space robots master side user interface video compression video transmission multiple collaborator system master interface adobe flash cross operating system compatible video broadcasting h 264 avc encoder intel ipp multicore capability high definition videos over internet slow moving objects compression ratio encoding time video encoders compression algorithm psnr analysis;psnr;h 264 avc;video compression;teleoperations;video coding;internet;streaming media;robots computers streaming media video coding internet psnr encoding;peak signal to noise ratio;robots;high definition video;telerobotics;video coding control engineering computing high definition video internet operating systems computers telerobotics user interfaces;control engineering computing;encoding;user interfaces;operating systems computers	Teleoperations is an important field in robotics. From surgical robots to space robots, it has found applications in different areas. This paper discusses the development of master side user interface and video encoder for video compression and transmission. It is a single robot, single user but multiple collaborator system. The master interface is developed using Adobe Flash and is cross operating system compatible. For video broadcasting purposes, a H.264/AVC encoder is developed using Intel IPP which uses latest multicore capabilities of new processors and can encode and transmit high definition videos over internet in real time. Encoder is designed to be used for video with slow moving objects in it. Use of Intel IPP provides better compression ratio, shorter encoding time and improved PSNR than existing video encoders. The results of our compression algorithm have been verified using PSNR analysis and human perception survey.	adobe flash;algorithm;central processing unit;data compression;encode;encoder;h.264/mpeg-4 avc;integrated performance primitives;multi-core processor;operating system;peak signal-to-noise ratio;robot;robotics;user interface	Mohsin Khan;Jason Jianjun Gu	2012	2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2012.6334958	video compression picture types;embedded system;computer vision;encoder;real-time computing;peak signal-to-noise ratio;computer science;artificial intelligence;operating system;video tracking;multimedia;video processing;statistics;multiview video coding	Robotics	43.27152476022963	-21.204158125812704	107434
749f9d76a6d4890bdb44a12574c72901b94340a4	memory efficient multilevel discrete wavelet transform schemes for jpeg2000	discrete wavelet transforms;image coding;memory management;transform coding;image coding block codes data compression discrete wavelet transforms;memory efficient multilevel discrete wavelet transform scheme image size image compression multilevel block based wavelet transform multilevel stripe based wavelet transform memory usage block coding buffer jpeg2000 coding;discrete wavelet transforms transform coding image coding memory management wavelet coefficients;code block discrete wavelet transform lifting scheme line based method jpeg2000;wavelet coefficients	When working with the line-based wavelet transform, JPEG2000 needs to buffer many sub-band lines for future block coding. This coding buffer occupies the majority of the memory usage in a JPEG2000 system. In this paper, we first use the multilevel block-based wavelet transform, and then utilize the multilevel stripe-based wavelet transform to realize JPEG2000 coding. The system schemes of the multilevel block-based wavelet transform and the multilevel stripe-based wavelet transform for JPEG2000 are presented. The proposed schemes can effectively control the wavelet coefficient output pattern, and thus reduce the memory usage of JPEG2000 coding buffer. Compared with the line-based wavelet transform, the proposed schemes reduce more than 50% memory usage of the JPEG2000 system and slightly decrease the memory bandwidth. By holding the blocks/stripes in on-chip memory, which do not depend on the image size, the memory bandwidth of the JPEG2000 system can be further significantly reduced. The advantage of the stripe-based wavelet transform over the block-based wavelet transform lies in that it occupies much less internal memory.	blocking (computing);byte;coefficient;computer data storage;discrete wavelet transform;image resolution;jpeg 2000;memory bandwidth;stripes	Linning Ye;Zujun Hou	2015	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2015.2400776	arithmetic;multiresolution analysis;wavelet;constant q transform;transform coding;speech recognition;s transform;harmonic wavelet transform;lapped transform;second-generation wavelet transform;continuous wavelet transform;computer science;theoretical computer science;discrete fourier transform;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;wavelet transform;memory management	EDA	44.124140210367166	-15.594703037661926	107579
64100a837f18f0f117b7d7237b8bf1303aaeb8f2	network-based h.264/avc whole-frame loss visibility model and frame dropping methods	engineering;interpolation;losses;data compression;decoding;packet loss;visibility model;observers;packet dropping policy;video coding;visualization;image reconstruction;decoding videos interpolation visualization observers humans predictive models;network congestion network based h 264 avc whole frame loss visibility model video compression concealment effect frame copy frame interpolation b frame loss video reconstruction visual impact prediction intelligent frame dropping method;predictive models;humans;video coding data compression image reconstruction interpolation losses observers;visibility model packet dropping policy packet loss perceptual video quality;perceptual video quality;videos	We examine the visual effect of whole-frame loss by different decoders. Whole-frame losses are introduced in H.264/AVC compressed videos which are then decoded by two different decoders with different common concealment effects: frame copy and frame interpolation. The videos are seen by human observers who respond to each glitch they spot. We found that about 39% of whole-frame losses of B frames are not observed by any of the subjects, and over 58% of the B frame losses are observed by 20% or fewer of the subjects. Using simple predictive features that can be calculated inside a network node with no access to the original video and no pixel level reconstruction of the frame, we develop models that can predict the visibility of whole B frame losses. The models are then used in a router to predict the visual impact of a frame loss and perform intelligent frame dropping to relieve network congestion. Dropping frames based on their visual scores proves superior to random dropping of B frames.	anatomic node;bannayan-riley-ruvalcaba syndrome;best, worst and average case;business readiness rating;computer performance;decoder device component;dropping;dual;f1 score;ffmpeg;frame (physical object);glitch;group of pictures;h.264/mpeg-4 avc;hearing loss, high-frequency;i-frame delay;independence day: resurgence;interpolation imputation technique;josamycin;jumbo frame;large;largest;morphologic artifacts;motion interpolation;network congestion;network packet;pixel;router (computing);small;video compression picture types;visual effects;hearing impairment;observers;videocassette	Yueh-Lun Chang;Ting-Lan Lin;Pamela C. Cosman	2012	IEEE Transactions on Image Processing	10.1109/TIP.2012.2191567	data compression;iterative reconstruction;reference frame;residual frame;computer vision;visualization;telecommunications;interpolation;computer science;predictive modelling;multimedia;packet loss;statistics	Vision	48.89482862311358	-15.984525242062226	108120
d44ddd53ae0d681d3dbae8626bfc01505e32033b	reversible watermarking based on pmo of triplets	watermarking;reversible watermarking;image coding;psnr;image resolution;pmo of triplets reversible watermarking;watermarking image coding image resolution;watermarking information science pixel law enforcement biomedical imaging data encapsulation data mining educational programs;data mining;embedding capacity;pmo of triplets;piecewise modification operation;pixel;multimedia communication;payloads;triplets reversible watermarking piecewise modification operation image coding embedding capacity;conferences;triplets	A reversible watermarking algorithm based on piecewise modification operation(PMO) of triplets is proposed in this paper. PMO is designed to embed a bit into any two pixels while remaining the intensity sum of pixels unchanged. Two bits are embedded into a triplet by repeatedly using PMO on every two neighboring pixels. Another advantage of using PMO is that the capacity consumed by the additional information can be largely decreased. As a result, the embedding capacity is considerably increased. A series of experiments is conducted to verify effectiveness and advantages of the proposed approach.	algorithm;digital watermarking;embedded system;experiment;pixel;triplet state	ShaoWei Weng;Yao Zhao;Jeng-Shyang Pan;Rongrong Ni	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711789	computer vision;payload;image resolution;peak signal-to-noise ratio;digital watermarking;computer science;theoretical computer science;mathematics;multimedia;pixel	Robotics	40.35190357931418	-12.688671563381755	108381
8c2e742f715c9d508cdd3b5553b8ca656fb7f27c	a high quality/low computational cost technique for block matching motion estimation [video coding applications]	reconstructed video sequence quality video coding quality video coding computational cost block matching motion estimation video encoder rate distortion performance video compression adaptive cost block matching acbm;reconstructed video sequence quality;rate distortion;block matching motion estimation;motion control;full search;decoding;image matching;image reconstruction video coding motion estimation image matching adaptive signal processing;video compression;motion estimation;video sequences;video coding;adaptive signal processing;image reconstruction;image quality;block matching;video coding quality;computational efficiency motion estimation video compression video sequences motion control rate distortion costs image quality decoding microelectronics;microelectronics;video encoder rate distortion performance;video coding computational cost;computational efficiency;high performance;block matching algorithm;acbm;adaptive cost block matching	Motion estimation is the most critical process in video coding systems. First of all, it has a definitive impact on the rate-distortion performance given by the video encoder. Secondly, it is the most computationally intensive process within the encoding loop. For these reasons, the design of high-performance low-cost motion estimators is a crucial task in the video compression field. An adaptive cost block matching (ACBM) motion estimation technique is presented in this paper, featuring an excellent tradeoff between the quality of the reconstructed video sequences and the computational effort. Simulation results demonstrate that the ACBM algorithm achieves a slight better rate-distortion performance than the one given by the well-known full search algorithm block matching algorithm with reductions of up to 95% in the computational load.	algorithmic efficiency;block-matching algorithm;computation;data compression;display resolution;distortion;encoder;motion estimation;search algorithm;simulation;whole earth 'lectronic link	Sebastián López;Gustavo Marrero Callicó;José Francisco López;Roberto Sarmiento	2005	Design, Automation and Test in Europe	10.1109/DATE.2005.17	data compression;iterative reconstruction;image quality;adaptive filter;motion control;computer vision;quarter-pixel motion;computer science;theoretical computer science;motion estimation;block-matching algorithm;rate–distortion optimization;motion compensation;microelectronics;statistics;computer graphics (images)	EDA	46.5972880477804	-17.84037940762996	108402
03b9b73ccb1d32d6bad37fff3c5ab722ec5b3fcd	online compression of video sequences using adaptive vq codebooks	online algorithm;image coding;data compression;video sequences image coding clustering algorithms statistics algorithm design and analysis lattices performance analysis bandwidth pattern matching vector quantization;pattern matching;vector quantisation video signals image sequences data compression image coding;video signals;vector quantisation;coding rate video sequences adaptive vq codebooks online compression space covering properties pattern matching ability online algorithms vq encodings fixed size adaptive codebooks generic baseline algorithm distortion per individual vector realtime implementation statistical assumptions;image sequences	We propose a novel approach that combines the space covering property of high rate lattice VQ with the pattern matching ability of clustering VQ. The proposed scheme encompasses a broad range of online algorithms that use suitable VQ encodings and fixed-size, adaptive codebooks. The generic baseline algorithm for the scheme has the following desirable characteristics. The distortion per individual vector is guaranteed to be less than a user specified threshold. Secondly, the algorithm is amenable to fast realtime implementation and requires minimal statistical assumptions for analysis. Finally, with careful analysis, the coding rate can be bounded with respect to some theoretical benchmark.	baseline (configuration management);benchmark (computing);cluster analysis;codebook;distortion;online algorithm;pattern matching;vector quantization	Xiaomei Wang;Sunil M. Shende;Khalid Sayood	1994		10.1109/DCC.1994.305926	data compression;online algorithm;computer science;theoretical computer science;machine learning;pattern matching;pattern recognition;mathematics;statistics	Vision	49.85029406312703	-18.146062312512946	108600
42c959cde5b2008fe2ae6d50326b62d2f9299173	steganalysis of aac using calibrated markov model of adjacent codebook	steganalysis;markov;scale factor band codebook aac steganalysis calibrated markov model adjacent codebook advanced audio coding audio compression huffman codebook markov transition probability extraction;aac;codebook;huffman;markov aac huffman codebook steganalysis;feature extraction markov processes correlation huffman coding bit rate testing;steganography audio coding huffman codes markov processes probability	AAC(Advanced Audio Coding) is the most popular audio compression standard and used widely in recent years. The steganography schemes of AAC emerged gradually. This paper presents a novel steganalysis method to attack the steganography of Huffman codebook, which hide information by modifying the codebook of each scale factor band(SFB), and have good imperceptivity and security. Based on the correlation of neighboring SFBs' codebook, the paper proposes to extract the Markov transition probability of adjacent SFBs' codebook as steganalysis feature, and adopt calibration to improve the accuracy. Extensive experiments demonstrate the effectiveness of the proposed methods. To the best of our knowledge, this piece of work is the first one to detect AAC steganography of Huffman codebook.	advanced audio coding;codebook;data compression;experiment;huffman coding;markov chain;markov model;steganalysis;steganography	Yanzhen Ren;Qiaochu Xiong;Lina Wang	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472055	markov chain;speech recognition;steganalysis;advanced audio coding;computer science;theoretical computer science;codebook;pattern recognition;mathematics;linde–buzo–gray algorithm;statistics;huffman coding	Vision	40.47176868998197	-12.876101799120452	108675
b43c4b74e40b8517f3cd62e8f9529b2348d803c7	improved adaptive lifting scheme for wavelet-based image compression	improved adaptive lifting scheme;improved adaptive update;prediction scheme;wavelet-based image compression;high-frequency coefficient;domain lower coefficient;adaptive lifting scheme;simulation result;proposed scheme;image compression application;wavelet-based image;neighboring pixel;wavelet transforms;high frequency;image compression;lifting scheme;data compression	This paper investigates an adaptive lifting scheme for wavelet-based image compression, which performs update before prediction, and then we proposes an improved adaptive update and prediction scheme to better exploit the correlation among neighboring pixels. Simulation results show that the proposed scheme generates in the transformed domain lower coefficients' entropy, lower high-frequency coefficients' energy and more zero values, which make it useful for image compression applications.	coefficient;image compression;lifting scheme;pixel;simulation;wavelet	Qiong Li;Guoqiang Li;Yi-Liang He;Xiamu Niu	2007	Third International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP 2007)	10.1109/IIH-MSP.2007.442	data compression;color cell compression;computer vision;data compression ratio;second-generation wavelet transform;image compression;computer science;theoretical computer science;high frequency;pattern recognition;mathematics;lossless compression;context-adaptive binary arithmetic coding;texture compression;lifting scheme;algorithm;wavelet transform	Robotics	44.02341386079653	-15.458210225866194	108788
4ac72d0421d9ab1f55dc43d710e0efc113f2f420	personal video coding for very low bit rate applications	eigenvalues and eigenfunctions;low bit rate application;video sequence;interpolation;multidimensional interpolation algorithm;image motion analysis;codecs;orthogonal component decomposition;singular value decomposition;video coding bit rate streaming media communication channels image reconstruction multidimensional systems interpolation video sequences codecs mpeg 4 standard;personal video coding;motion reconstruction;video sequences;bit rate;limit set;video coding;mpeg 4 standard;streaming media;image reconstruction;principal component analysis;error statistics;video communication eigenvalues and eigenfunctions error statistics image motion analysis image reconstruction image sequences principal component analysis singular value decomposition video coding;orthogonal component decomposition personal video coding video communication low bit rate application residual error motion reconstruction video sequence multidimensional interpolation algorithm principal component analysis singular value decomposition eigen image;communication channels;video communication;eigen image;residual error;multidimensional systems;image sequences	In personal video communications, the postures assumed by the speaker during the connection are very similar. In past works the possible exploitation of inherent voice and video correlation has been proposed for the generation of a likely animation of a speaker, driven by audio information only. In this paper, a novel video coding technique is presented, based on the decomposition of the sequence spanned hyperspace in orthogonal components. Good quality video can be reconstructed at the receiver, particularly for very low bit rate applications, with low residual error and smooth appearance both in motion reconstruction and in the complete absence of crisp coding artifacts such as blocking structures, basing on a few proper selected examples of the speaker and a multidimensional interpolation algorithm. Principal component analysis exploited by singular value decomposition has been used to decompose the video sequence into eigen-images. A limited set of eigen-images is chosen to span the whole video sequence subspace. Only coefficients to be used in the interpolation procedure are transmitted along the communication channel. Preliminary results are presented for Akyio QCIF video sequence.	algorithm;blocking (computing);channel (communications);codebook;coefficient;data compression;eigen (c++ library);encoder;interpolation;key frame;performance evaluation;principal component analysis;ringing artifacts;singular value decomposition;synthetic data	Cataldo Guaragnella	2007	Eighth International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS '07)	10.1109/WIAMIS.2007.68	video compression picture types;iterative reconstruction;limit set;computer vision;codec;speech recognition;multidimensional systems;interpolation;theoretical computer science;video tracking;coding tree unit;residual;mathematics;block-matching algorithm;singular value decomposition;motion compensation;video post-processing;statistics;multiview video coding;channel;principal component analysis	Vision	49.43749715602529	-14.074836164439532	109021
2b0970156bb624d88b61085419217601ad512994	error concealment for video transmission based on watermarking	cuantificacion senal;filigranage numerique;digital watermarking;utilisation information;circuit decodeur;embedding;uso informacion;metodo vectorial;image coding;steganographie;error concealment;transmission error;securite;information use;filigrana;technique video;video quality;intelligence artificielle;error transmision;filigrane;codigo bloque;tecnica video;circuito desciframiento;codage image;video coding;decoding circuit;steganography;esteganografia;signal quantization;senal video;signal video;codage video;motion vector;quantification signal;filigrana digital;vector method;plongement;safety;video transmission;watermark;video signal;video technique;artificial intelligence;methode vectorielle;code bloc;inmersion;inteligencia artificial;seguridad;block code;erreur transmission	We propose an error concealment method for robust video transmission based on watermarking technique. In this method, the motion vector information is used as the watermark and embedded in the transform coefficients before quantization. A proper embedding algorithm is designed with the characteristics of the video coder taken into consideration, so that the embedding watermark has little influence on the video quality and the coding efficiency. At the decoder, the extracted watermark is used to effectively detect errors and restore the corrupted motion vectors. Then the restored motion vectors are utilized in error concealment. Simulation results demonstrate the advantages of the proposed method in error prone environment. The proposed method is applicable to all the block-transform based video coding schemes.	algorithm;algorithmic efficiency;coefficient;cognitive dimensions of notations;data compression;digital watermarking;embedded system;error concealment;simulation	Shuai Wan;Yilin Chang;Fuzheng Yang	2005		10.1007/11596981_88	block code;computer vision;telecommunications;digital watermarking;computer science;video quality;embedding;steganography;watermark;motion compensation;statistics;computer graphics (images)	EDA	46.73163647781152	-13.768092787741994	109043
0a74f5c769e62cccb1416958e7ab571b49d1289c	mesh-based motion estimation and compensation in the wavelet domain using a redundant transform	image sampling;discrete wavelet transforms;wavelet based still image coder;triangle vertices;discrete wavelet transform;spatial domain;data compression;motion compensation;motion compensated residual;correlation methods data compression video coding discrete wavelet transforms motion estimation motion compensation transform coding image sampling;geometry;irregular triangle mesh;video compression;downsampling;motion estimation;transform coding;correlation methods;motion estimation wavelet domain wavelet transforms discrete wavelet transforms motion compensation discrete cosine transforms mpeg 4 standard proposals geometry costs;motion compensated;wavelet transforms;video coding;shift invariant redundant wavelet transform;correlation operator;mpeg 4 standard;wavelet transform;image edge location;redundant transform;discrete cosine transforms;affine transformation;discrete wavelet transform mesh based motion estimation mesh based motion compensation wavelet domain redundant transform irregular triangle mesh wavelet domain shift invariant redundant wavelet transform triangle vertices correlation operator image edge location wavelet subbands affine transformation motion compensated residual downsampling wavelet based still image coder spatial domain video compression;mesh based motion compensation;mesh based motion estimation;wavelet domain;proposals;triangle mesh;wavelet subbands;shift invariant	In this paper, a technique is presented that incorporates an irregular triangle mesh into wavelet-domain motion-estimation and motion-compensation using a shift-invariant redundant wavelet transform. Triangle vertices are identified by a simple correlation operator locating image edges in the wavelet subbands, while motion compensation takes place through an affine transformation mapping triangles from one frame to the next. The motioncompensated residual is downsampled to a non-redundant form which is then coded using any wavelet-based still-image coder. Experimental results indicate that the combined approach outperforms either technique applied separately; in addition, the proposed method outperforms a variety of motion-estimation and motion-compensation approaches operating in both the spatial and	decimation (signal processing);motion compensation;motion estimation;stationary wavelet transform;triangle mesh	Suxia Cui;Yonghui Wang;James E. Fowler	2002		10.1109/ICIP.2002.1038119	data compression;wavelet;computer vision;mathematical optimization;discrete mathematics;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;statistics;wavelet transform	Robotics	45.03647946661644	-16.063759760521542	109171
f0f3ca2305097a947d9d1c11f9b59d926a491449	fuzzy logic-based matching pursuits for lossless predictive coding of still images	fuzzy logic data compression image coding encoding differential pulse code modulation;prediction error;image coding;data compression;linear regression;entropy coding;indexing terms;differential pulse code modulation;spatial prediction;statistical model;context model;fuzzy logic;statistical context modeling fuzzy logic based matching pursuits lossless predictive coding still images reversible compression grayscale images spatial differential pulse code modulation space varying linear regression prediction prototype nonorthogonal predictors entropy coding spatial prediction context based statistical modeling performance trends data compression membership function;membership function;matching pursuit;encoding;fuzzy logic matching pursuit algorithms predictive coding pulse modulation pixel image coding gray scale modulation coding prototypes entropy coding;predictive coding	This paper presents an application of fuzzy-logic techniques to the reversible compression of grayscale images. With reference to a spatial differential pulse code modulation (DPCM) scheme, prediction may be accomplished in a space-varying fashion either as adaptive, i.e., with predictors recalculated at each pixel, or asclassified, in which image blocks or pixels are labeled in a number of classes, for which fitting predictors are calculated. Here, an original tradeoff is proposed; a space-varying linear-regression prediction is obtained through fuzzy-logic techniques as a problem ofmatching pursuit, in which a predictor different for every pixel is obtained as an expansion in series of a finite number of prototype nonorthogonal predictors, that are calculated in a fuzzy fashion as well. To enhance entropy coding, the spatial prediction is followed by context-based statistical modeling of prediction errors. A thorough comparison with the most advanced methods in the literature, as well as an investigation of performance trends and computing times to work parameters, highlight the advantages of the proposed fuzzy approach to data compression.	data compression;entropy encoding;fuzzy logic;grayscale;kerrison predictor;lossless compression;modulation;pixel;prototype;series and parallel circuits;statistical model	Bruno Aiazzi;Luciano Alparone;Stefano Baronti	2002	IEEE Trans. Fuzzy Systems	10.1109/TFUZZ.2002.800691	data compression;fuzzy logic;statistical model;computer vision;index term;membership function;computer science;entropy encoding;linear regression;machine learning;mean squared prediction error;pattern recognition;mathematics;context model;encoding;statistics;matching pursuit	Vision	43.819097419582754	-15.726479007416215	109172
b1648b1ec4387c21918b71142de25e24fd8e3015	exposing fake bit rate videos and estimating original bit rates	video coding discrete cosine transforms frequency domain analysis image resolution image sequences statistical analysis vector quantisation;video compression original bit rate estimation digital video quality digital video forensic fake bit rate mpeg 2 video expose statistical artifact analysis requantization artifact first digit law dct frequency domain query video sequential bit rate down converted version spatial domain 16 d feature vector video sequence video resolution;videos feature extraction forensics transform coding video recording quality assessment visualization	Bit rate is one of the important criterions for digital video quality. With some video tools, however, video bit rate can be easily increased without improving the video quality at all. In such a case, a claimed high bit rate video would actually have poor visual quality if it is up-converted from an original lower bit rate version. Therefore, exposing fake bit rate videos becomes an important issue for digital video forensics. To the best of our knowledge, although some methods have been proposed for exposing fake bit rate MPEG-2 videos, no relative work has been reported to further estimate their original bit rates. In this paper, we first analyze the statistical artifacts of these fake bit rate videos, including the requantization artifacts based on the first-digit law in the DCT frequency domain (12-D) and the changes of the structural similarity indexes between the query video and its sequential bit rate down-converted versions in the spatial domain (4-D), and then we propose a compact yet very effective 16-D feature vector for exposing fake bit rate videos and further estimating their original bit rates. The extensive experiments evaluated on hundreds of video sequences with four different resolutions and two typical compression schemes (i.e., MPEG-2 and H.264/AVC) have shown the effectiveness of the proposed method compared with the existing relative ones.	digital video;discrete cosine transform;experiment;feature vector;h.264/mpeg-4 avc;mpeg-2;structural similarity	Shan Bian;Weiqi Luo;Jiwu Huang	2014	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2014.2334031	video compression picture types;scalable video coding;computer vision;uncompressed video;computer science;video quality;video capture;video tracking;block-matching algorithm;multimedia;video processing;smacker video;rate–distortion optimization;motion compensation;video post-processing;h.261;video denoising;multiview video coding;computer graphics (images)	Networks	43.650032226406026	-18.10353562919831	109235
639066eede2e0d54da204c7fc4e8585e47862ace	adaptive pairing reversible watermarking	embedding bit rate schemes adaptive pairing reversible watermarking pairwise reversible watermarking scheme adaptive pixel pairing prediction errors adaptive approach shifted pixels;2d histogram reversible watermarking pixel pairing;histograms watermarking context indexes decoding sorting distortion;image watermarking	This letter revisits the pairwise reversible watermarking scheme of Ou et al., 2013. An adaptive pixel pairing that considers only pixels with similar prediction errors is introduced. This adaptive approach provides an increased number of pixel pairs where both pixels are embedded and decreases the number of shifted pixels. The adaptive pairwise reversible watermarking outperforms the state-of-the-art low embedding bit-rate schemes proposed so far.	digital watermarking;embedding;pixel	Ioan-Catalin Dragoi;Dinu Coltuc	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2549458	computer vision;discrete mathematics;theoretical computer science;mathematics	Vision	43.69640698064994	-16.524368648768395	109341
7b1309e0ad96eea79c3c66ffccbbf1ba57c702af	steganalysis of lsb matching revisited for consecutive pixels using b-spline functions	steganalysis;b spline smoothing;media security;steganography;lsb matching for consecutive pixels	Least significant bit matching revisited steganography (LSBMR) is a significant improvement of the well-known least significant bit matching algorithm. In this paper, we point out that LSBMR for consecutive pixels and its descendants, including the edge adaptive image steganography based on LSBMR, introduces intrinsic statistical imbalance in secret data embedding process, which results in the imbalance of the power of the additive stegonoise. This intrinsic imbalance can be used to construct a dimensionless discriminator using B-spline smoothing. Experimental results show that the proposed steganalytic method is a reliable detector against LSBMR for consecutive pixels and the edge adaptive image steganography based on LSBMR when block size is 1. An embedding rate estimator based on B-spline functions which can roughly estimate the embedding rate is proposed as well.	b-spline;least significant bit;pixel;steganalysis	Shunquan Tan	2011		10.1007/978-3-642-32205-1_4	steganalysis;theoretical computer science;pattern recognition;mathematics;steganography;statistics	Vision	41.00977811358498	-11.530671087465377	109505
a726d266e6a259e8ca91492835e611b174496c13	a novel image hiding scheme based on vq and hamming distance	data hiding;vector quantization vq;hamming distance;vector quantisation	Data hiding is popularly employed in protecting the copyright, secret information and communication secretly with the convenience of network communication. In order to reduce the amount the transmitted data via network, VQ (vector quantization) is applied to compressing the transmitted data. Since VQ is a low-bit-rate compression scheme, it is difficult to find large redundant hiding space and to get stego-images with high quality for hiding data in VQ-based compressed images. A hiding method for VQ-based compressed images is proposed in this paper according to the Hamming distance between the index values of codewords. Our method makes hiding data more effective and provides stego-images with higher quality than other conventional methods. Experimental results demonstrate the effectiveness and the feasibility of our method.	hamming distance;vector quantization	Chin-Chen Chang;Wei-Liang Tai;Chia-Chen Lin	2007	Fundam. Inform.		hamming distance;computer science;theoretical computer science;pattern recognition;data mining;mathematics;programming language;information hiding;algorithm	Vision	40.13568315148209	-12.183828295153146	109535
2030fd058adf536d8da26f45dd82cdc0de00556f	compressed domain transcoding solutions for mpeg-4 visual simple profile and h.263 baseline videos in 3gpp services and applications	digital video broadcasting;h 263 baseline videos;psnr compressed domain transcoding solutions 3gpp services h 263 baseline videos mpeg 4 visual simple profile video services 3g wireless networks video coding interoperability spatial domain approach bit streams 3rd generation partnership project;spatial domain approach;3gpp services;electronic mail;3g wireless networks;psnr;data compression;decoding;data compression video coding transcoding 3g mobile communication;wireless video;wireless network;quality improvement;transform coding;indexing terms;compressed domain transcoding solutions;mpeg 4 visual simple profile;wireless communication;video coding;video transcoding;bit streams;mpeg 4 standard;3g mobile communication;complexity reduction;streaming media;interoperability;transcoding;video services;3rd generation partnership project;transcoding mpeg 4 standard streaming media transform coding decoding video coding electronic mail wireless communication digital video broadcasting laboratories	To provide basic video services and applications in 3G wireless networks, two video coding formats, H.263 baseline and MPEG-4 visual simple profile, are commonly adopted in different forms for various usage scenarios. To ensure interoperability between these cases, we propose efficient compressed domain algorithms for transcoding between MPEG-4 visual simple profile and H.263 baseline, and among MPEG-4 visual simple profile coding modes. Experimental results show that our suggested algorithms provide significant complexity reduction and quality improvement, in terms of PSNR, compared to the spatial domain approach. Being the first complete work on transcoding between all kinds of MPEG-4 visual simple profile and H.263 baseline bit streams, we believe this work provides valuable solutions for various 3GPP (3rd generation partnership project) compliant video services and applications).	baseline (configuration management)	Yongfang Liang;Fehmi Chebil;Asad Islam	2006	IEEE Trans. Consumer Electronics	10.1109/TCE.2006.1649672	quality management;transcoding;telecommunications;computer science;multimedia;world wide web;statistics;computer network	Visualization	44.825002064636195	-19.73209897356823	109665
3da2c740a5840de171a610091fe1c8be248ac745	3d subband coder for very low bit rates	quadrature mirror filters;baseband;reduced motion search;motion estimation video coding quadrature mirror filters filtering theory image sequences vector quantisation;application software;reference model;low complexity;motion estimation;bit rate;systems engineering and theory;video coding;computational modeling;switched codebook vq;9 6 kbit s very low bit rates 3d subband coder video coding qmf subband analysis input sequence motion detection motion classification reduced motion search vector quantisation codebooks switched codebook vq simulations qcif format subjective evaluation 14 4 kbit s;14 4 kbit s;bit rate motion detection baseband frequency motion estimation systems engineering and theory application software computational modeling computer simulation video coding;input sequence;3d subband coder;qmf subband analysis;9 6 kbit s;vector quantisation;frequency;subjective evaluation;codebooks;computer simulation;motion detection;subband coding;qcif format;filtering theory;very low bit rates;motion classification;image sequences	A video coder based on 3D subband coding system (SBC) targetted for very low bit rate applications is developed and simulated. It employs QMF subband analysis to decompose an input sequence into 4 temporal bands. A low-complexity block-based motion detection scheme is then applied to all bands followed by motion classification and reduced motion search on the baseband which is H.261-like coded and the high temporal bands are vector-quantised (VQ) with a bank of codebooks (switched codebook VQ). Simulations are performed at 9.6 kbit/sand 14.4 kbit/s at 5 frame/s for QCIF format and results showed that it achieved better performance than the ITU-T short-term reference model SlM3 coder at similar bit rates. Subjective evaluation favours the 3D SBC coder because it does not suffer from 'blocking' artifacts. >		Weng Leong Chooi;King Ngi Ngan	1994		10.1109/ICASSP.1994.389402	computer simulation;sub-band coding;computer vision;application software;reference model;speech recognition;computer science;frequency;motion estimation;baseband;mathematics;computational model	Crypto	46.045154373201214	-17.38973343384564	109827
947adc2c3d64139dbfaae45f5ef31eb5def4a067	a robust steganographic wavelet-based system for resistant message hiding under error prone networks	qualified significant wavelet trees;watermarking;propagation losses;image coding;robustness steganography cryptography image color analysis resistance computer errors wavelet coefficients propagation losses image coding computer networks;resistance;transform coding;trees mathematics;computer networks;data encapsulation;cover image;wavelet transforms;watermarks;steganography;transform coding cryptography data encapsulation trees mathematics wavelet transforms watermarking image coding;encryption algorithm;image color analysis;cryptography;error prone networks;compression ratio;robustness;wavelet coefficients;computer errors;watermarks steganography wavelet coefficients message hiding error prone networks cover image encryption algorithm qualified significant wavelet trees;message hiding	In this paper a wavelet-based steganographic method is proposed for robust message hiding. The message is embedded into the most signifcant wavelet coefficients of a cover image to provide invisibility and resistance against lossy transmission, compression or other distortion. The architecture consists of three modules. In the first module the initial message is enciphered by an encryption algorithm. The enciphered message is imprinted onto a white-background image to construct the message-image to be hidden. In the second module the cover image is decomposed into two levels with seven subbands, using the DWT. Next Qualified Significant Wavelet Trees (QSWTs), which are paths of signijkant wavelet coefficients, are estimated for the highest enera pair of subbands. During the third module the messageimage is redundantly embedded to the coefficients of the best QSWT.. and the IDWT is applied to provide the stego-image. The robustness and efficiency of the proposed steganographic system is evaluated under various loss rates, combined with direrent JPEG compression ratios.	algorithm;cipher;coefficient;cognitive dimensions of notations;discrete wavelet transform;distortion;embedded system;encryption;jpeg;lossy compression;stationary wavelet transform;steganography;wavelet tree	Klimis S. Ntalianis;Anastasios D. Doulamis;Nikolaos D. Doulamis;Stefanos D. Kollias	2002		10.1109/ICME.2002.1035678	computer vision;transform coding;speech recognition;computer science;cryptography;theoretical computer science;compression ratio;mathematics;steganography;resistance;statistics;robustness	Visualization	40.53778387927475	-11.828874394581065	110282
cf0d8e76623c85b525f99430b4885ace50fec70f	progressive adaptive correlation estimation(pace) for wzvc	estimation theory;rate distortion;data compression;decoding;video compression;correlation methods;bit rate;video coding;wyner ziv video coding;estimation;transforms;error resilience applications progressive adaptive correlation estimation pace wzvc wyner ziv video coding video compression decoding process trace approach;error resilience;correlation decoding estimation bit rate video coding transforms adaptation models;correlation;adaptation models;video coding correlation methods data compression estimation theory	Wyner-Ziv video coding is a new paradigm for video compression, in which the prediction frames are possibly only available at the decoder. It exploits the redundancy between the source frame and the prediction frame at the decoder by utilizing their correlation information. However, such correlation information is difficult to estimate due to the absence of the prediction frame at the encoder and the lack of the source frame at the decoder. In this paper, we focus on this issue and propose a progressive adaptive correlation estimation (PACE) approach, in which the correlation information is progressively learned during the decoding process. Compared with our previous TRACE approach, PACE has similar performance in estimation accuracy as well as rate-distortion. Furthermore, it can be potentially integrated into more extensive WZVC applications, such as scalable applications and error-resilience applications.	adaptive grammar;data compression;distortion;encoder;programming paradigm;redundancy (engineering);scalability	Xiaopeng Fan;Chang Zhao;Jiayang Gao;Oscar C. Au	2011	2011 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2011.6116050	data compression;computer vision;telecommunications;computer science;theoretical computer science;statistics	Vision	48.8251870212011	-16.554654060772172	110286
566a6c2da152f30b9a13e469e3e3903dbf6089c1	a scheme of digital watermarking for 3-d models using correlation method in polar coordinate system	watermarking;watermarking computational modeling noise smoothing methods solid modeling transforms correlation;computer graphics;correlation method;digital watermark;three dimensional;computer graphic;watermarking computer graphics inverse transforms;random noise;smoothing methods;computational modeling;random noise 3d models digital watermarking scheme correlation method polar coordinate system three dimensional computer graphics inverse transform computer simulation test models;solid modeling;transforms;polar coordinate system;test models;inverse transform;polar coordinate;correlation;3d models;computer simulation;digital watermarking scheme;inverse transforms;three dimensional computer graphics;noise	In this paper, we propose a novel watermarking scheme for three-dimensional computer graphics (3-D CG) using correlation method in polar coordinate system. First, we describe polar coordinate transform and its inverse transform. Next, we explain basic procedures for embedding watermarks into a 3-D model and detecting them from a watermarked model. Finally, we perform the computer simulation in order to prove the availability of proposed scheme. The experimental results for some test models show that the watermark information in each model can be detected by our scheme even when the watermarked models are subjected to attack such as random noise addition.	3d modeling;computer graphics;computer simulation;digital watermarking;noise (electronics);sensor;watermark (data file)	Shouta Sakaino;Hiromu Koda	2010	2010 International Symposium On Information Theory & Its Applications	10.1109/ISITA.2010.5649259	computer simulation;computer vision;polar coordinate system;digital watermarking;computer science;theoretical computer science;mathematics;computer graphics (images)	Arch	41.658381638714644	-11.262503196678743	110410
978460a26bd5798650726cc5113cc45678f0a894	image compression with a vector speck algorithm	image coding;set partitions;chaos;image coding clustering algorithms partitioning algorithms wavelet coefficients streaming media vector quantization shape chaos transform coding costs;transform coding;block based algorithms;vector speck algorithm;energy clustering characteristics;wavelet transforms image coding transform coding vector quantisation;wavelet transforms;performance improvement;scalar quantization;vector quantization;shape;image compression;streaming media;zerotrees;clustering algorithms;spiht;jpeg 2000;vector quantizer;vector quantisation;spiht image compression vector speck algorithm wavelet coefficients block based algorithms energy clustering characteristics vector quantization jpeg 2000 zerotrees;wavelet coefficients;partitioning algorithms	SPIHT is an efficient image compression algorithm based on zerotrees. The significant wavelet coefficients are located by a series of set partitioning operations and then scalar quantized. Block-based algorithms inspired by SPIHT such as AGP and SWEET have good performance, but they are not embedded. Pearlman et al. proposed a block-based SPECK algorithm using set partitioning of embedded blocks to exploit the energy clustering characteristics of the coefficients while the bit stream remains embedded. We here propose a variation on SPECK using vector quantization to code the significant coefficients. Different VQ techniques including TSVQ and ECVQ are also considered. Vector SPECK shows a performance improvement over JPEG 2000 at the cost of added complexity	accelerated graphics port;algorithm;bitstream;cluster analysis;coefficient;embedded system;image compression;jpeg 2000;quantization (signal processing);set partitioning in hierarchical trees;speck (cipher);vector quantization;wavelet	Chih-Chien Chao;Robert M. Gray	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660375	transform coding;speech recognition;shape;image compression;computer science;theoretical computer science;machine learning;pattern recognition;jpeg 2000;mathematics;cluster analysis;set partitioning in hierarchical trees;vector quantization;wavelet transform	EDA	43.62886154850735	-13.972432011450591	110484
0a299b9824c539fd8df5d22cf81f9f19d0417b2a	a modified fractal transformation to improve the quality of fractal coded images	fractal image coding;filtering;fractals;image coding;psnr;domain blocks;compression algorithms;contraction mapping;discrete fractal transformation;filters;psnr modified fractal transformation image quality improvement fractal coded images discrete fractal transformation contraction mapping domain blocks range blocks filtering procedure fractal image coding quadtree based fractal scheme;filtering procedure;noise measurement;shape;discrete transforms;image quality improvement;image quality;quadtree based fractal scheme;filtering theory fractals image coding discrete transforms quadtrees;fractal coded images;fractals image coding filtering extraterrestrial measurements image quality psnr noise measurement filters shape compression algorithms;extraterrestrial measurements;quadtrees;modified fractal transformation;filtering theory;range blocks	Ab stract The Discrete Fractal Transformation has recently e merged as a powerful technique for coding images. The s cheme works by dividing an image into blocks and maki ng use of a c ontraction m apping by which t he dom ain bl ocks ar e m apped i nto r ange blocks. During this p rocess it is u sual to filter the domain blocks. In this w ork, we show how a change in the filtering procedure can improve the image quality of fr actal coding m ethods bas ed on the m entioned t ransformation. A fter a p resentation of the d iscrete fr actal tr ansformation, w e e xplain the change and we apply it to the quadtree-based f ractal scheme obtaining improvements in PSNR that can reach 1 dB when compared with the results obtained by tr aditional filtering.	emoticon;fractal;image quality;ork;peak signal-to-noise ratio;quadtree	José de Oliveira;Gelson V. Mendonça;Roosevelt J. Dias	1998		10.1109/ICIP.1998.723605	data compression;image quality;filter;contraction mapping;computer vision;combinatorics;discrete mathematics;fractal;peak signal-to-noise ratio;fractal analysis;shape;noise measurement;mathematics;fractal transform;fractal compression	Vision	43.756512765686615	-14.483927208687273	111021
c036c2ad6d5f218f07e851eea4227db66b5a131b	image authentication using hierarchical semi-fragile watermarks	image features;image processing;digital watermark;image authentication;wavelet transform	In this paper, a semi-fragile watermarking technique operating in the wavelet domain is proposed. A hierarchy of the image blocks is constructed and the image features are extracted such that relationships among image blocks are established in order to enhance the security and robustness of the system. With such a hierarchy, the image can be authenticated at different levels of resolution, hence providing a good property of tamper localization. In addition, by varying certain parameters, the system is able to control the degree of robustness against non-malicious attacks. The proposed algorithm thus provides a fine trade-off between security and localization, and is also robust to common image processing operations.	algorithm;authentication;brute-force attack;coefficient;embedded system;emoticon;image processing;semiconductor industry;wavelet	Yuan-Liang Tang;Chun-Hung Chen	2004		10.1007/1-4020-4761-4_23	feature detection;color image;binary image;image processing;digital watermarking;computer science;digital image processing;top-hat transform;feature;wavelet transform	Vision	40.78557332352589	-10.781191457105564	111115
6229e554e9e619802d6f8ee9fc612ba7101cdcea	carriage of 3d audio-visual services by t-dmb	digital video broadcasting;built in stereo camera;audio visual systems;service system;three dimensional television;3d depth perception;t dmb;multimedia systems;three dimensional;feasibility;type of service;digital multimedia broadcasting;depth perception;three dimensional television audio visual systems cameras digital audio broadcasting digital video broadcasting hearing three dimensional displays;three dimensional displays;flat panel displays;signal processing;backward compatible system architecture three dimensional audio visual service system 3d av terrestrial digital multimedia broadcasting system t dmb feasibility dmb terminal built in stereo camera 3d depth perception;dmb terminal;multimedia communication;digital multimedia broadcasting digital video broadcasting three dimensional displays multimedia communication flat panel displays digital audio broadcasting multimedia systems tv signal processing hardware;digital audio broadcasting;audio visual;terrestrial digital multimedia broadcasting system;3d av;three dimensional audio visual service system;backward compatible system architecture;tv;flat panel display;system architecture;new media;hearing;cameras;hardware	In this paper, we introduce our experience on the development of a three-dimensional audio-visual (3D AV) service system based on the terrestrial digital multimedia broadcasting (T-DMB) system. 3D AV service is now much more feasible than before with the fast advancement of hardware technologies, especially 3D flat panel display, processors and memory. 3D AV service over DMB system is very attractive due to the facts that (1) glassless 3D viewing with small display is relatively easy to implement and more suitable to the single user environment like DMB, (2) DMB is a new media and thus has more flexibility in adding new services on the existing ones, (3) 3D AV handling capability of 3D DMB terminal has lots of potential to generate new types of services if it is added with other components like built-in stereo camera. In order to provide successful 3D DMB services over existing DMB system, we need to solve several issues like (1) guaranteeing backward compatibility with the T-DMB system, (2) minimizing the overhead on the transmitted bit-rate and the required processing power of the terminal, (3) providing good 3D depth perception without a noticeable eye strain. We propose a very efficient and backward compatible system architecture for the 3D DMB, and show how we can get better depth perception with the limited bit budget of the DMB system	backward compatibility;central processing unit;depth perception;flat panel display;new media;overhead (computing);stereo camera;systems architecture;terrestrial television;user interface	Sukhee Cho;Namho Hur;Jinwoong Kim;Kugjin Yun;Soo In Lee	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262684	embedded system;three-dimensional space;new media;telecommunications;depth perception;computer science;type of service;signal processing;multimedia;digital video broadcasting;computer network;service system;computer graphics (images)	Robotics	42.869356680309146	-21.392776046510853	111121
2806b823cd8ec8fa2fe7c2c2c9d43f12245a5114	predictive compression of animated 3d models by optimized weighted blending of key-frames	3d dynamic mesh;animated mesh compression;dynamic geometry;optimized blending weights;key frame key pose extraction;predictive coding	Efficient compression techniques are required for animated mesh sequences with fixed connectivity and time-varying geometry. In this paper, we propose a key-frame-based technique for three-dimensional dynamic mesh compression. First, key-frames are extracted from the animated sequence. Extracted key-frames are then linearly combined using blending weights to predict the vertex locations of the other frames. These blending weights play a key role in the proposed algorithm because the prediction performance and the required number of key-frames greatly depend on these weights. We present a novel method in order to compute the optimum blending weight that makes it possible to predict location of the vertices of the non-key frames with the minimum number of key-frames. The residual prediction errors are finally quantized and encoded using Huffman coding and another heuristic method. Experimental results on different test sequences with various sizes, topologies, and geometries demonstrate the privileged performance of the proposed method compared with the previous techniques. Copyright © 2015 John Wiley & Sons, Ltd.	3d modeling;anim;algorithm;alpha compositing;andrew glassner;chicken;cluster analysis;distortion;heuristic;huffman coding;john d. wiley;key frame;lossless compression;mathematical optimization;multi-objective optimization;quantization (signal processing);tom	Mohammadali Hajizadeh;Hossein Ebrahimnezhad	2016	Journal of Visualization and Computer Animation	10.1002/cav.1685	computer vision;simulation;computer science;computer graphics (images)	Visualization	41.17116679540055	-18.646876314777185	111177
5d24d4e7463f5fe384e6194a92d99401ee93fd1c	iterative side-information generation in a mixed resolution wyner-ziv framework	baja resolucion;estensibilidad;feedback channel;desciframiento;intermediate nonreference frame;distributed algorithms;circuit decodeur;iterative method;theorie vitesse distorsion;video coding computational complexity correlation methods data compression distributed algorithms image resolution iterative decoding rate distortion theory set theory transform coding video codecs;evaluation performance;rate distortion;wyner ziv;circuit codeur;coding circuit;codecs;performance evaluation;iterative decoding;scalable video coding;image resolution;data compression;decodage;decoding;video signal processing;optimal code;resource allocation;code optimal;implementation;evaluacion prestacion;laplacian;decoding spatial resolution feedback video coding statistics laplace equations automatic voltage control codecs source coding motion estimation;resource management;low resolution;reversed complexity;basse resolution;decoder side;mixed resolution spatial reduction based wyner ziv coding framework;video quality;motion estimation;transform coding;correlation methods;set theory;representation sous forme image;probleme de wyner ziv;circuito desciframiento;quantized transform coefficient;circuito sin memoria;metodo iterativo;rate distortion theory;problema de wyner ziv;video coding;gestion recursos;decoding circuit;laplacien;laplace equations;distributed video coding algorithm iterative side information generation mixed resolution spatial reduction based wyner ziv coding framework intermediate nonreference frame rate distortion performance decoder side encoder rate allocation low resolution encoding complexity laplacian residual frame quantized transform coefficient feedback channel optimal coding parameter selection memoryless coset decoding correlation estimation mechanism parameter choice process video quality h 264 avc codec;laplaciano;feedback;circuit sans memoire;automatic voltage control;retroaccion;codage video;retroaction	We propose a mixed resolution framework based on full resolution key frames and spatial-reduction-based Wyner-Ziv coding of intermediate nonreference frames. Improved rate-distortion performance is achieved by enabling better side-information generation at the decoder side and better rate-allocation at the encoder side. The framework enables reduced encoding complexity by low resolution encoding of the nonreference frames, followed by Wyner-Ziv coding of the Laplacian residue. The quantized transform coefficients of the residual frame are mapped to cosets without the use of a feedback channel. A study to select optimal coding parameters in the creation of the memoryless cosets is made. Furthermore, a correlation estimation mechanism that guides the parameter choice process is proposed. The decoder first decodes the low resolution base layer and then generates a super-resolved side-information frame at full resolution using past and future key frames. Coset decoding is carried using side-information to obtain a higher quality version of the decoded frame. Implementation results are presented for the H.264/AVC codec.	codec;coefficient;distortion;encoder;h.264/mpeg-4 avc;image resolution;key frame;laplacian matrix;residual frame	Bruno Macchiavello;Debargha Mukherjee;Ricardo L. de Queiroz	2009	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2009.2026820	image resolution;telecommunications;computer science;resource management;theoretical computer science;mathematics;statistics	Vision	47.03570680653888	-15.514624757996907	111402
389f02759faea8cf63c6878f7c072022aef2b18c	transform coder classification for digital image forensics	transform coding data compression image classification image coding security of data source coding;compression scheme;image coding;data compression;image and video forensics;histogram analysis;image classification;transform coding;coefficient subbands;indexing terms;digital images forensics image coding image analysis source coding application software histograms information analysis signal analysis watermarking;nonintrusive forensic analysis;pattern classification;digital image forensics;source code;digital image;coefficient subbands transform coder classification digital image forensics nonintrusive forensic analysis source coding digital image compression scheme histogram analysis;pattern classification image and video forensics image coding source coding transform coding;security of data;transform coder classification;source coding	The area of non-intrusive forensic analysis has found many applications in the area of digital imaging. One unexplored area is the identification of source coding in digital images. In other words, given a digital image, can we identify which compression scheme was used, if any? This paper focuses on the aspect of transform coder classification, where we wish to determine which transform was used during compression. This scheme analyzes the histograms of coefficient subbands to determine the nature of the transform method. By obtaining the distance between the obtained histogram and the estimate of the original histogram, we can determine if the image was compressed using the transform tested. Results show that this method can successfully classify compression by transform as well as detect whether any compression has occurred at all in an image.	coefficient;data compression;digital image;digital imaging;transform coding	Steven K. Tjoa;Wan-Yi Sabrina Lin;K. J. Ray Liu	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379532	computer vision;transform coding;speech recognition;computer science;theoretical computer science;fractal transform;distance transform;top-hat transform;statistics;source code	Vision	41.84331572957383	-11.506629058393237	111607
99c10a94975a2eb0d40b7d4323b1c3e8e23ef628	adaptive vector quantization for fixed bit-rate videocoding	metodo adaptativo;videophone;image processing;quantifier;compresion senal;procesamiento imagen;methode adaptative;visiophone;traitement image;compression signal;experimental result;algorithme;algorithm;adaptive quantization;adaptive vector quantization;codificacion;cuantificacion vectorial;videotelefono;vector quantization;senal video;signal video;arbol binario;quantificateur;adaptive method;signal compression;arbre binaire;coding;resultado experimental;video signal;digital video compression;resultat experimental;cuantificador;codage;algoritmo;binary tree;quantification vectorielle	Abstract   In this paper a new algorithm is introduced and discussed for self-adaptive vector quantization of video sequences. The structure of the vector quantizer and the mechanisms for its construction and updating are based on a binary tree whose topology and content adaptively track the varying statistics of the incoming data. Some tree branches are extended more than others depending on the time varying distribution of the input vectors, to guarantee an acceptable reconstruction quality, not far from the target performances of a full search approach. The resulting bit-rate stems from several terms carrying information for both frame reconstruction and codebook updating. An effective policy for codebit allocation has been devised and tested providing very good results within fixed bit-rate applications. Exhaustive simulations have been carried out on standard sequences at a rate of 30 frames/sec with progressive transmission. An average reconstruction quality of 34.5 dB in the peak signal-to-noise ratio has been achieved for the luminance component at a constant bit-rate of 512 kbit/sec corresponding to a compression of 30:1 (0.26 bit/pixel).	vector quantization	Fabio Lavagetto;Sandro Zappatore	1993	Signal Processing	10.1016/0165-1684(93)90024-5	electronic engineering;binary tree;image processing;computer science;theoretical computer science;mathematics;coding;vector quantization;algorithm	ML	46.53705090793881	-14.255601312175289	111648
19b2fb1083d4e57296ac26d4965cf5242d8a6677	fast digital watermarking of uncompressed colored images using bidirectional extreme learning machine		Development of fast watermarking schemes for all multimedia objects is crucial to the present day research in information security. Besides speed of execution minimizing the trade-off between visual quality and robustness is another important requirement of this research domain. In view of this, a newly developed single layer feedforward network (SLFN) commonly known as Bidirectional Extreme Learning Machine (B-ELM) is employed to carry out watermark embedding and extraction from four colored images. The results show that the B-ELM technique outperforms the previously employed ELM technique for this purpose. It is concluded that visual quality and robustness trade-off is minimized and real time targets are achieved. Thus, the proposed scheme is found to be suitable for developing video watermarking applications.	algorithm;digital watermarking;discrete cosine transform;elm;embedded system;feedforward neural network;information security;peak signal-to-noise ratio;real-time computing;structural similarity	Ankit Rajpal;Anurag Mishra;Rajni Bala	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966011	information security;machine learning;robustness (computer science);watermark;artificial intelligence;feed forward;computer vision;computer science;digital watermarking;extreme learning machine;visualization;uncompressed video	EDA	40.36869073688333	-12.873953009783953	111734
efe4e6e15e9ecce12821f9c106ecc053531824ba	asymmetric coding of multi-view video plus depth based 3-d video for view rendering	3 d video;optimal solution;video coding data compression rendering computer graphics signal reconstruction;perceptual visual quality asymmetric coding depth image based rendering virtual view depth map distortion geometry change texture video 3d video system bit allocation 3d video compression multiview video plus depth based 3d video view rendering distortion binocular suppression encoding framework chrominance reconstruction algorithm bitrate constraint;data compression;view rendering;video compression;bit rate;materials;visual quality;journal;three dimensional;computer graphic;video coding;visualization;three dimensional displays;view rendering 3 d video asymmetric coding bit allocation chrominance reconstruction;three dimensional displays encoding rendering computer graphics bit rate visualization materials video coding;chrominance reconstruction;signal reconstruction;bit allocation;depth image based rendering;reconstruction algorithm;depth map;rendering computer graphics;encoding;asymmetric coding	The recent years have witnessed three-dimensional (3-D) video technology to become increasingly popular, as it can provide high-quality and immersive experience to end users, where view rendering with depth-image-based rendering (DIBR) technique is employed to generate the virtual views. Distortions in depth map may induce geometry changes in the virtual views, and distortions in texture video may be propagated to the virtual views. Thus, effective compression of both texture videos and depth maps is important for 3-D video system. From the perspective of bit allocation, asymmetric coding of the texture videos and depth maps is an effective way to get the optimal solution of 3-D video compression and view rendering problems. In this paper, a novel asymmetric coding method of multi-view video plus depth (MVD) based 3-D video is proposed on purpose of providing high-quality view rendering. In the proposed method, two models are proposed to characterize view rendering distortion and binocular suppression in 3-D video. Then, an asymmetric coding method of MVD-based 3-D video is proposed by combining two models in encoding framework. Finally, a chrominance reconstruction algorithm is presented to achieve accurate reconstruction. Experimental results show that compared with other methods, the proposed method can obtain higher performance of view rendering under the total bitrate constraint. Moreover, the perceptual visual quality of 3-D video is almost unaffected with the proposed method.	3d computer graphics;3d film;algorithm;binocular vision;data compression;depth map;distortion;experiment;zero suppression	Feng Shao;Gangyi Jiang;Mei Yu;Ken Chen;Yo-Sung Ho	2012	IEEE Transactions on Multimedia	10.1109/TMM.2011.2169045	video compression picture types;data compression;computer vision;rendering;computer science;video tracking;multimedia;real-time rendering;motion compensation;alternate frame rendering;video post-processing;statistics;multiview video coding;computer graphics (images)	Visualization	43.893659520347555	-19.247370299141323	111896
c72745da7f0eb45995fe1f8b5c5df8862a835afa	realization adaptive strategies and a comparison of fourier bandwidth, shannon bandwidth, and campbell bandwidth	adaptive coefficient selection realization adaptive strategies fourier bandwidth shannon bandwidth campbell bandwidth coefficient rate effective bandwidth adaptive source compression source representations spectral entropy;coefficient rate;sequences;electronic mail;image coding;data compression;helium;speech processing;source representations;adaptive codes;fourier bandwidth;bandwidth entropy random processes electronic mail image coding speech processing discrete cosine transforms energy measurement helium codes;adaptive codes data compression sequences random processes entropy;adaptive coefficient selection;shannon bandwidth;energy measurement;random process;codes;discrete cosine transforms;random processes;adaptive source compression;bandwidth;spectral entropy;entropy;realization adaptive strategies;campbell bandwidth;effective bandwidth	In 1960, Campbell derived a quantity that he defined as the coefficient rate of a random process that involves the process spectral entropy. However, no potential applications of the coefficient rate were identified. In this paper, we present two new derivations of Campbell’s coefficient rate. One derivation solidifies the interpretation of this quantity as a coefficient rate and allows us to define an effective bandwidth for the process. The second derivation implies a new approach for realization adaptive source compression. We demonstrate how coefficient rate can be used for realization adaptive coefficient selection in a sequence of source representations. Furthermore, we designate the effective bandwidth as Campbell bandwidth and contrast it with Fourier bandwidth and Shannon bandwidth. Several specific examples are presented that illustrate the differences among the three quantities.	adaptive compression;coefficient;entropy rate;shannon (unit);stochastic process	Wenye Yang;Tao He;Jerry D. Gibson	2003		10.1109/DCC.2003.1194015	data compression;stochastic process;entropy;mathematical optimization;theoretical computer science;sequence;mathematics;helium;code;bandwidth;statistics	Networks	48.49732470846918	-11.551238146236932	112012
f70dfb40d2479692dab917087d56238467a13856	video coding focusing on block partitioning and occlusion	union bidireccional;moving object;theorie vitesse distorsion;traitement signal;similarity metric;evaluation performance;rate distortion;optimisation;degradation;partition method;low bit rate;debit binaire eleve;estimation mouvement;liaison bidirectionnelle;image segmentation;performance evaluation;image processing;ligne de base;motion compensation;optimizacion;hidden feature removal;occlusion;video signal processing;traitement d arriere plan;redundancia;evaluacion prestacion;estimacion movimiento;pattern based residual encoding;transmision alta caudal;high bit rate;procesamiento imagen;h 264 standard video coding pattern based residual encoding block partitioning background occlusion pattern uncovered background segments interblock temporal redundancy motion compensation rate distortion optimisation lagrangian multiplier;metric;motion estimation;arriere plan;background occlusion;segmentation;uncovered background coding;lagrange multiplier;blanco movil;qualite image;pattern uncovered background segments;traitement image;similitude;block partitioning;motion compensated;velocidad de bit debil;rate distortion theory;compensation mouvement;video coding;video coding hidden feature removal image segmentation motion compensation rate distortion theory;background;velocidad de bit elevada;methode partition;redundancy;shape;codage video;pattern based coding;background processing;signal processing;image quality;bidirectional control;multiplicateur lagrange;similarity;baseline;rate distortion optimisation;h 264;linea de base;traitement signal video;video coding rate distortion shape motion compensation degradation robustness lagrangian functions encoding image quality bidirectional control;high rate transmission;multiplicador lagrange;cible mobile;metrico;robustness;interblock temporal redundancy;optimization;sub blocking;calidad imagen;metodo particion;similitud;bidirectional link;rate distortion optimization;encoding;procesamiento senal;debit binaire faible	Among the existing block partitioning schemes, the pattern-based video coding (PVC) has already established its superiority at low bit-rate. Its innovative segmentation process with regular-shaped pattern templates is very fast as it avoids handling the exact shape of the moving objects. It also judiciously encodes the pattern-uncovered background segments capturing high level of interblock temporal redundancy without any motion compensation, which is favoured by the rate-distortion optimizer at low bit-rates. The existing PVC technique, however, uses a number of content-sensitive thresholds and thus setting them to any predefined values risks ignoring some of the macroblocks that would otherwise be encoded with patterns. Furthermore, occluded background can potentially degrade the performance of this technique. In this paper, a robust PVC scheme is proposed by removing all the content-sensitive thresholds, introducing a new similarity metric, considering multiple top-ranked patterns by the rate-distortion optimizer, and refining the Lagrangian multiplier of the H.264 standard for efficient embedding. A novel pattern-based residual encoding approach is also integrated to address the occlusion issue. Once embedded into the H.264 Baseline profile, the proposed PVC scheme improves the image quality perceptually significantly by at least 0.5 dB in low bit-rate video coding applications. A similar trend is observed for moderate to high bit-rate applications when the proposed scheme replaces the bi-directional predictive mode in the H.264 High profile.	block cipher;clinical use template;computational complexity theory;computer vision;data compression;distortion;embedding;exploit (computer security);h.264/mpeg-4 avc;handling (psychology);high-level programming language;image quality;lagrange multiplier;macroblock;mathematical optimization;motion compensation;obstruction;physical object;server message block;biologic segmentation;format	Manoranjan Paul;M. Manzur Murshed	2010	IEEE Transactions on Image Processing	10.1109/TIP.2009.2033406	computer vision;telecommunications;image processing;computer science;signal processing;mathematics;lagrange multiplier	Visualization	46.72727199917984	-14.9620761910811	112214
ce3f4f0f3eb9fbc6e62893919d1bba5a7eb59a08	a novel robust method to add watermarks to bitmap images by fading technique		www.ijmse.org 43  Abstract—Digital water marking is one of the essential fields in image security and copyright protection. The proposed technique in this paper was based on the principle of protecting images by hide an invisible watermark in the image. The technique starts with merging the cover image and the watermark image with suitable ratios, i.e., 99% from the cover image will be merged with 1% from the watermark image. Technically, the fading process is irreversible but with the proposed technique, the probability to reconstruct the original watermark image is great. There is no perceptible difference between the original and watermarked image by human eye. The experimental results show that the proposed technique proven its ability to hide images that have the same size of the cover image. Three performance measures were implemented to support the proposed techniques which are MSE, PSNR, and SSIM. Fortunately, all the three measures have excellent values.	bitmap;digital watermarking;item unique identification;mean squared error;peak signal-to-noise ratio;structural similarity	Firas A. Jassim	2013	CoRR		image restoration;computer vision;telecommunications;mathematics;computer security	Vision	40.55402220240243	-11.377635698138171	112308
33ed156e248e2350a0cb2628b0e0f9b48a2f9048	communication-aware motion planning for multi-agent systems from signal temporal logic specifications		We propose a mathematical framework for synthesizing motion plans for multi-agent systems that fulfill complex, high-level and formal local specifications in the presence of inter-agent communication. The proposed synthesis framework consists of desired motion specifications in temporal logic (STL) formulas and a local motion controller that ensures the underlying agent not only to accomplish the local specifications but also to avoid collisions with other agents or possible obstacles, while maintaining an optimized communication quality of service (QoS) among the agents. Utilizing a Gaussian fading model for wireless communication channels, the framework synthesizes the desired motion controller by solving a joint optimization problem on motion planning and wireless communication, in which both the STL specifications and the wireless communication conditions are encoded as mixed integer-linear constraints on the variables of the agents' dynamical states and communication channel status. The overall framework is demonstrated by a case study of communication-aware multi-robot motion planning and the effectiveness of the framework is validated by simulation results.	channel (communications);formal system;high- and low-level;logic programming;mathematical optimization;motion controller;motion planning;multi-agent system;optimization problem;quality of service;simulation;temporal logic	Zhiyu Liu;Jin Dai;Bo Wu;Hai Lin	2017	2017 American Control Conference (ACC)	10.23919/ACC.2017.7963331	control engineering;electronic engineering;real-time computing	Robotics	52.72271022973742	-21.11274815031062	112340
b12daccb4bb372e3dad30f3d473511e6ecbf6552	image resizing in the compressed domain using subband dct	estensibilidad;evaluation performance;image storage;image numerique;image coding;performance evaluation;image processing;image resolution;data compression;color;evaluacion prestacion;transformation cosinus discrete;visual communication;performance;procesamiento imagen;image coding discrete cosine transforms image storage digital images displays image resolution color bandwidth transform coding communication channels;display device resolution;transform coding;image resizing;traitement image;discrete cosine transform;algorithme;image resizing color images image halving dugad ahuja algorithm image doubling discrete cosine transform dct jpeg standard performance image compression subband dct digital images image transmission varying communication channels display device resolution;dugad ahuja algorithm;algorithm;image doubling;image halving;color images;image compression;image transmission;subbanda;time varying channels data compression image coding discrete cosine transforms transform coding image colour analysis visual communication;discrete cosine transforms;image colour analysis;subband;displays;imagen numerica;bandwidth;varying communication channels;extensibilite;scalability;digital image;jpeg standard;subband dct;communication channels;digital images;sous bande;dct;time varying channels;color image;algoritmo	Resizing of digital images is needed in various applications, such as transmission of images over communication channels varying widely in their bandwidths, display at different resolutions depending on the resolution of a display device, etc. In this work, we propose a modification of a recently proposed elegant image resizing algorithm by Dugad and Ahuja (2001). We have also extended their approach and our modified versions to color images and studied their performance at different levels of compression for an image. Our proposed modified algorithms, in general, perform better than the earlier method in most cases. Though there is a marginal increase in the computation required in image-halving, the computation overhead of the proposed modification is higher compared to the Dugad-Ahuja algorithm in the case of doubling the images.	discrete cosine transform;image scaling	Debargha Mukherjee;Sanjit K. Mitra	2002	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2002.800509	computer vision;image processing;computer science;discrete cosine transform;mathematics;multimedia;digital image;algorithm;computer graphics (images)	EDA	45.73506165358011	-14.538257339191345	112437
97a3c41217e58d371d309c53af04daf3df9189a0	a key points-based blind watermarking approach for vector geo-spatial data	similarity degree;blind watermarking;spatial data;data format;random noise;least significant bit;similarity transformation;vector data;key points	This paper presents a blind watermarking approach to protecting vector geo-spatial data from illegal use. By taking into account usability, invisibility, robustness, and blindness, the approach firstly determines three feature layers of the geo-spatial data and selects the key points from each layer as watermark embedding positions. Then it shuffles the watermark and embeds it in the least significant bits (LSBs) of the coordinates of the key points. A similar process for selecting the feature layers and the key points in the watermark embedding process is carried out to detect the watermark followed by obtaining the embedded watermark from the LSBs of the coordinates of the key points. Finally, the similarity degrees of three versions of the watermark from three feature layers are calculated to check if the data contains the watermark. Our experiments show that the method is rarely affected by data format change, random noise, similarity transformation of the data, and data editing. 2010 Elsevier Ltd. All rights reserved.		Haowen Yan;Jonathan Li;Hong Wen	2011	Computers, Environment and Urban Systems	10.1016/j.compenvurbsys.2010.10.004	least significant bit;matrix similarity;theoretical computer science;data mining;mathematics;spatial analysis;internet privacy;watermark;statistics	AI	39.637959862757945	-10.87136808023187	112472
ff67be82ef360261a0d34ce21f8afc62b8c317ab	fast rate control algorithm in frame-layer for h.264/avc video coding	complexity theory;h 264 avc;prediction algorithms;buffer management fast rate control algorithm frame layer h 264 avc video coding standard intercoded blocks statistical property complexity updating function quantization parameter function coding gain;bit rate;quantisation signal;rate control;video coding;quantizationparameter;statistical analysis;computational cost;predictive models;complexity theory encoding video coding bit rate predictive models prediction algorithms computational efficiency;computational cost h 264 avc rate control quantizationparameter complexity estimation;computational efficiency;encoding;complexity estimation;video coding quantisation signal statistical analysis	In this paper, we propose a fast rate control algorithm in frame-layer of H.264/AVC video coding standard. We derive a statistical relation between quantization parameter (QP) and encoded bits of inter-coded blocks. In addition, new complexity estimation is defined to determine weight of complexity of current frame. Using the statistical property and the complexity updating function, we determine the amount of bits as a function of quantization parameter, so that computational cost of rate control is significantly reduced with coding gain improvement and stable buffer management. Experimental results demonstrate the capability of the proposed algorithm.	algorithm;coding gain;computational complexity theory;data compression;h.264/mpeg-4 avc;video coding format	Myoung-Jin Kim;Min-Cheol Hong	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6311330	real-time computing;prediction;computer science;theoretical computer science;context-adaptive variable-length coding;machine learning;predictive modelling;context-adaptive binary arithmetic coding;encoding;statistics	Vision	46.95322435583258	-18.09070420206933	112612
5640c3001eca46ef799e69aac10cb5eed2d950a9	motion estimation on decoder side for low-complexity video encoding in wireless sensor networks	complexity theory;decoding;motion estimation;video coding;embedded systems;vectors;vectors delay encoding decoding motion estimation video codecs complexity theory;network delays motion estimation decoder side low complexity video encoding wireless sensor networks encoder motion prediction predicted motion vectors modified h 264 implementation;video codecs;inbaddad systemteknik;wireless sensor networks decoding motion estimation video coding;encoding;wireless sensor networks	In this paper we present an approach to provide efficient low-complexity video encoding for wireless sensor networks. We present a method based on removing the most time-consuming task, that is motion estimation, from the encoder. Instead the decoder will perform motion prediction based on the available decoded frame and send the predicted motion vectors to the encoder. We present results based on a modified H.264 implementation. Our results shows that this approach can provide rather good coding efficiency even for relatively high network delays.	algorithmic efficiency;data compression;encoder;h.264/mpeg-4 avc;motion estimation	Johannes Karlsson	2012	2012 IEEE 8th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)	10.1109/WiMOB.2012.6379154	embedded system;real-time computing;wireless sensor network;quarter-pixel motion;computer science;theoretical computer science;motion estimation;block-matching algorithm;motion compensation;encoding	Robotics	48.397165084911535	-17.409122185027663	112637
a88faf85b9d38cf65e33df492f2f6feff607912a	a novel steganographic algorithm based on the motion vector phase	data hiding;psnr;seganography;simulation;phase of motion vector;transform coding;data encapsulation;data hiding techniques;media;video coding;matrix encoding technique;steganography;motion segmentation;motion vector;steganographic algorithm;matrix encoding technique steganographic algorithm motion vector phase data hiding techniques digital video;digital video;digital video seganography phase of motion vector matrix encoding;encoding;matrix encoding;motion vector phase;steganography videos data encapsulation embedded computing educational institutions motion analysis encoding computer science software engineering software algorithms;algorithm design and analysis;video coding data encapsulation steganography	Most data hiding techniques in digital video utilize I frame to embed the secret information so the capacity of P and B frame is wasted. In this paper, we first analyze a data-hiding algorithm using the phase angle difference of the motion vector, on this base a novel steganographic algorithm based on the phase of the motion vector is proposed. The algorithm utilizes the phase of single motion vector to embed the secret data in P or B frame, in order to improve the embedding efficiency we use the matrix encoding technique to achieve a better tradeoff between the quantities of hidden data and the motion vectors modification rate. The simulation results have validated the feasibility of the algorithm.	algorithm;digital video;simulation;steganography;the matrix;video compression picture types	Xuansen He;Zhun Luo	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.359	algorithm design;computer vision;transform coding;media;peak signal-to-noise ratio;quarter-pixel motion;computer science;theoretical computer science;motion estimation;block-matching algorithm;steganography;information hiding;encoding;statistics;computer graphics (images)	EDA	40.716460515510754	-13.378511168278786	112693
e69cd6df63e21b2ce76da66c78afa855f1f58b62	segmentation-based intra coding of depth maps using texture information	3 d video;image coding;image segmentation;codecs;coding tools;decoding;3d hevc;texture image;three dimensional television;segmentation based intra coding;stereo image processing 3 d video depth map image coding image segmentation;emerging video technologies;video coding decoding image segmentation image texture three dimensional television video codecs;data format;low complexity applications;low complexity applications segmentation based intra coding depth maps texture information emerging video technologies 3dtv coding tools multiview video transmission channels data format depth information segmentation based intra codec color data low complexity color segmentation texture image decoder closed loop property 3d hevc;image texture;transmission channels;video coding;segmentation based intra codec;depth maps;color data;three dimensional displays;image color analysis;stereo image processing;multiview video;depth information;video codecs;3dtv;low complexity color segmentation;texture information;depth map;image segmentation encoding codecs image color analysis decoding three dimensional displays image coding;encoding;closed loop property;decoder	Emerging video technologies, such as 3DTV, increase the demand for high efficiency coding tools. To allow the transmission of a multiview video over the employed transmission channels, a new data format including texture and the corresponding depth information has been proposed. In this paper we present a novel segmentation-based intra codec for depth maps, utilizing the correlation between depth and color data. Our system performs a low-complexity color segmentation on the texture image and encodes the depth values within the segments. The same segmentation is calculated at the decoder, ensuring the closed-loop property of the system. Experimental results show that the proposed system is a viable alternative for 3D-HEVC in low-complexity applications.	3d film;3d television;algorithm;codec;depth map;glossary of computer graphics;high efficiency video coding;image segmentation;intra-frame coding;jpeg 2000	Jan Hanca;Adrian Munteanu;Peter Schelkens	2013	2013 18th International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2013.6622779	image texture;computer vision;range segmentation;multimedia;image segmentation;scale-space segmentation;computer graphics (images)	Robotics	43.76116658683993	-19.46319305878473	112855
7ba6ee0d110b31581df9fe53733a0dffd24dd06e	technique for fractal image compression using genetic algorithm	iterative method;fractals;iterative methods fractals image coding image classification genetic algorithms;iterated function system;image coding;image processing;iterative function system fractal image compression genetic algorithm elitist model self transformation property image self similarities image classification;data compression;search space;compresion senal;procesamiento imagen;image classification;indexing terms;algoritmo genetico;traitement image;compression signal;metodo iterativo;iterative methods;tratamiento numerico;image compression;methode iterative;signal compression;algorithme genetique;fractal;compression ratio;genetic algorithm;genetic algorithms;digital processing;fractals image coding genetic algorithms vector quantization biological information theory machine intelligence image analysis;compresion dato;traitement numerique;compression donnee;fractal image compression	A new method for fractal image compression is proposed using genetic algorithm (GA) with an elitist model. The self transformation property of images is assumed and exploited in the fractal image compression technique. The technique described utilizes the GA, which greatly decreases the search space for finding the self similarities in the given image. This article presents theory, implementation, and an analytical study of the proposed method along with a simple classification scheme. A comparison with other fractal-based image compression methods is also reported.	assumed;cellular automaton;fractal compression;genetic algorithm;image compression;software release life cycle	Suman K. Mitra;C. A. Murthy;Malay Kumar Kundu	1998	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.663505	computer vision;genetic algorithm;fractal;image processing;computer science;theoretical computer science;mathematics;iterative method;fractal transform;fractal compression;algorithm	Visualization	44.78986746451462	-12.890719014462748	112889
27f3cf4921b05bdc1eb129be43024ee00a774a45	compressing encrypted images with auxiliary information	private key cryptography data compression distortion image coding image reconstruction;image coding image reconstruction discrete cosine transforms receivers quantization signal encryption;image encryption compression ratio distortion performance image compression;compressing encrypted images secret key ratio distortion performance principal image content receiver side quantized data encrypted sub image compression ratio distortion criteria optimal parameters quantization method channel provider image reconstruction data compression auxiliary information	This paper proposes a novel scheme of compressing encrypted images with auxiliary information. The content owner encrypts the original uncompressed images and also generates some auxiliary information, which will be used for data compression and image reconstruction. Then, the channel provider who cannot access the original content may compress the encrypted data by a quantization method with optimal parameters that are derived from a part of auxiliary information and a compression ratio-distortion criteria, and transmit the compressed data, which include an encrypted sub-image, the quantized data, the quantization parameters and another part of auxiliary information. At receiver side, the principal image content can be reconstructed using the compressed encrypted data and the secret key. Experimental result shows the ratio-distortion performance of the proposed scheme is better than that of previous techniques.	computational complexity theory;data compression;distortion;encryption;iterative method;iterative reconstruction;key (cryptography);quantization (signal processing);standard streams;stream cipher;triple des;user-generated content	Xinpeng Zhang;Yanli Ren;Liquan Shen;Zhenxing Qian;Guorui Feng	2014	IEEE Transactions on Multimedia	10.1109/TMM.2014.2315974	data compression;image warping;computer vision;image processing;image compression;theoretical computer science;pattern recognition;mathematics;texture compression	Vision	39.89271200049954	-11.75123281840234	113274
689196e9d54e64f98857b32202b6920e9354c1ef	a lossless ecg data compression technique using ascii character encoding	compression algorithm;data compression;reversible logic;reconstruction algorithm	A software based lossless ECG compression algorithm is developed here. The algorithm is written in the C-platform. The algorithm has applied to various ECG data of all the 12 leads taken from PTB diagnostic ECG database (PTB-DB). Here, a difference array has been generated from the corresponding input ECG data and this is multiplied by a large number to convert the number of arrays into integers. Then those integers are grouped in both forward and reverse direction, out of which few are treated differently. Grouping has been done in such a way that every grouped number resides under valid ASCII value. Then all the grouped numbers along with sign bit and other necessary information are converted into their corresponding ASCII characters. The reconstruction algorithm has also been developed in using the reversed logic and it has been observed that data is reconstructed with almost negligible difference as compared with the original (PRD 0.023%). 2011 Elsevier Ltd. All rights reserved.	algorithm;character encoding;code;data compression;decibel;greedy algorithm;lossless compression;online and offline;product requirements document;real-time clock;sign bit;xslt/muenchian grouping	Sourav Kumar Mukhopadhyay;Sucharita Mitra;Madhuchhanda Mitra	2011	Computers & Electrical Engineering	10.1016/j.compeleceng.2011.05.004	data compression;discrete mathematics;computer science;theoretical computer science;mathematics;algorithm;statistics	Robotics	40.48380194538902	-14.100115825280794	113456
35d1c90f925bae33615abd80129f725069a09ea7	bit rate and local quality control for on-board satellite image compression	protocols;local quality;image coding;data compression;bit rate quality control satellites image coding image reconstruction aerospace industry industrial control earth wavelet transforms protocols;earth;ezw encoder;ringing effects;image classification;bit rate control;transform coding;adaptive codes;aerospace industry;bit rate;satisfiability;classification;geophysical signal processing quality control remote sensing image coding data compression adaptive codes wavelet transforms transform coding image classification;wavelet transforms;rate control;constraint;wavelet transform;shapiro dependency protocol;progressive transmission;constraint bit rate control local quality control on board satellite image compression ringing effects adapted encoder wavelet transform classification ezw encoder shapiro dependency protocol energy based threshold;geophysical signal processing;adapted encoder;image reconstruction;remote sensing;satellites;industrial control;satellite image;quality control;energy based threshold;energy based classification;wavelet;on board satellite image compression;local quality control	Delphine Le Guen, St ephane Pateux Gilles Moury Dimitri Lebede Claude Labit IRISA CNES Alcatel Space Industries Campus de Beaulieu 18, av Edouard Belin 100, bld du Midi BP99 35042 RENNES Cedex 31401 TOULOUSE 06156 CANNES LA BOCCA FRANCE FRANCE FRANCE tel:(+33)2 99 84 25 88 tel:(+33)5 61 27 37 90 tel:(+33)4 92 92 64 22 dleguen,spateux,labit@irisa.fr Gilles.Moury@cnes.fr Dimitri.Lebede @space.alcatel.fr	image compression;midi	Delphine Le Guen;Stéphane Pateux;Claude Labit;Gilles Moury;Dimitri Lebedeff	2000		10.1109/DCC.2000.838205	computer vision;computer science;theoretical computer science;mathematics;statistics;wavelet transform	Graphics	44.190770089298	-14.954599055942447	113492
5baaf354febda17fe81e50fc802bc456a2fe8159	computation-aware fast motion estimation for h.264/avc using image indexing	multilevel block indexing;rate distortion;inter mode decision;real time;h 264 avc;reference frame;motion estimation;image indexing;multiple frame referencing;memory access;video coding;fast motion estimation;computational complexity;fast block matching;indexation;block matching;cost effectiveness;computation awareness;mode decision;block matching algorithm;exhaustive search	1047-3203/$ see front matter 2011 Elsevier Inc. A doi:10.1016/j.jvcir.2011.05.003 q This work was supported in part by National Sci Grant NSC 97-2221-E-019-032. ⇑ Corresponding author. Fax: +886 2 24623249. E-mail address: csc@mail.ntou.edu.tw (S.-C. Cheng The key to designing a real-time video coding system is efficient motion estimation, which reduces temporal redundancies. The motion estimation of the H.264/AVC coding standard can use multiple references and multiple block sizes to improve rate-distortion performance. The computational complexity of H.264 is linearly dependent on the number of allowed reference frames and block sizes using a full exhaustive search. Many fast block-matching algorithms reduce the computational complexity of motion estimation by carefully designing search patterns with different shapes or sizes, which have a significant impact on the search speed and distortion performance. However, the search speed and the distortion performance often conflict with each other in these methods, and their high computational complexity incurs a large amount of memory access. This paper presents a novel block-matching scheme with image indexing, which sets a proper priority list of search points, to encode a H.264 video sequence. This study also proposes a computation-aware motion estimation method for the H.264/AVC. Experimental results show that the proposed method achieves good performance and offers a new way to design a cost-effective real-time video coding system. 2011 Elsevier Inc. All rights reserved.	algorithm;brute-force search;computation;computational complexity theory;data compression;display resolution;distortion;encode;fax;h.264/mpeg-4 avc;motion estimation;national supercomputer centre in sweden;romp;random access;real-time clock;real-time computing;real-time transcription;reference frame (video)	Chi-Han Chuang;Bob-Nan Chen;Chin-Chun Chang;Shyi-Chyi Cheng	2011	J. Visual Communication and Image Representation	10.1016/j.jvcir.2011.05.003	reference frame;computer vision;real-time computing;cost-effectiveness analysis;quarter-pixel motion;computer science;theoretical computer science;context-adaptive variable-length coding;motion estimation;brute-force search;block-matching algorithm;computational complexity theory;motion compensation;algorithm	Vision	47.55000720551421	-19.267341473377815	113820
712359a34c346ed3a4e5b61571314b998f29f717	integer-modulated fir filter banks for image compression	simulation ordinateur;circuit codeur;concepcion circuito;transformation cosinus;image coding;coding circuit;ar 1 model;psnr;filter bank;image processing;data compression;filtre reponse impulsion finie;real valued cosine modulated filter bank;longueur mot;banc filtre;wavelet transform coding;filter coefficient word length;bismuth;finite impulse response filter;circuit design;compresion senal;procesamiento imagen;perfect reconstruction conditions;transform coding;indexing terms;satisfiability;traitement image;compression signal;coding gain;cosine modulated filter bank;wavelet transforms;reconstruction image;filtro respuesta impulsion acabada;wavelet transform;image compression;reconstruccion imagen;word length;channel bank filters;image reconstruction;fir filter;banco filtro;circuito codificacion;digital filters;signal compression;longitud palabra;transform coding fir filters digital filters channel bank filters image coding image reconstruction data compression modulation wavelet transforms;transformacion coseno;integer modulated fir filter banks;m channel modulated wavelet filter banks;finite impulse response filter image coding filter bank image reconstruction wavelet transforms channel bank filters bismuth signal synthesis delay design methodology;conception circuit;simulacion computadora;fir filters;signal synthesis;cosine transform;perfect reconstruction;simulation results;computer simulation;design methodology;modulation;wavelet transform coding integer modulated fir filter banks image compression m channel modulated wavelet filter banks filter coefficient word length perfect reconstruction conditions coding gain image coding simulation results psnr ar 1 model real valued cosine modulated filter bank	A method for designing perfect-reconstruction M-channel integer-modulated filter banks (IMFBs) is proposed. Given the filter coefficient word length, many IMFBs satisfying the perfect reconstruction conditions can be obtained. Methods to select the best IMFB for the purpose of image compression are studied, and it is demonstrated that the IMFB obtained by maximizing the coding gain is the most suitable for image coding. Simulation results show that the PSNR of the integer-modulated FIR filter bank designed by maximizing the coding gain with the AR(1) model is close to that of the real-valued cosine-modulated filter bank of the same order.	filter bank;finite impulse response;image compression;modulation	M. Bi;Sim Heng Ong;Yew-Hock Ang	1998	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.736717	computer simulation;speech recognition;kernel adaptive filter;telecommunications;image processing;computer science;finite impulse response;root-raised-cosine filter;filter bank;mathematics;filter design;statistics;m-derived filter;wavelet transform	EDA	47.02321022431939	-12.361789222489021	113894
0e1bcf00f7bae11d060397b2a3b32020c75116c8	measurement of absolute latency for video see through augmented reality	benchmarking;image features;image coding;user interface;standard deviation;absolute latency;delay augmented reality decoding cameras encoding histograms measurement standards image coding feedback visualization;video coding;video see through augmented reality;h 5 2 information interfaces and presentation user interfaces standardization;augmented reality;information interfaces and presentation;video coding augmented reality;h 5 2 information interfaces and presentation user interfaces standardization benchmarking;image coding video see through augmented reality absolute latency camera feedback;camera feedback	Latency is a key property of video see through AR systems since users' performance is strongly related to it. However, there is no standard way of latency measurement of an AR system in the literature. We have created a stable and comparable way of estimating the latency in a video see through AR system. The latency is estimated by encoding the time in the image and decoding the time after camera feedback. We have encoded the time as a translation of a circle in the image. The cross ratio has been used as an image feature that is preserved in a projective transformation. The encoding allows for a simple but accurate way of decoding. We show that this way of encoding has an adequate accuracy for latency measurements. As the method allows for a series of automatic measurements we propose to visualize the measurements in a histogram. This histogram reveals meaningful information about the system other than the mean value and standard deviation of the latency. The method has been tested on four different AR systems that use different camera technology, resolution and frame rates.	ar (unix);augmented reality;autoregressive model;bmc remedy action request system;feature (computer vision);interrupt latency	Tobias Sielhorst;Wu Sa;Ali Kamen;Frank Sauer;Nassir Navab	2007	2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2007.4538850	computer vision;augmented reality;computer science;multimedia;standard deviation;user interface;feature;statistics;benchmarking;computer graphics (images)	Visualization	44.73883128998982	-20.78468283016379	114083
b2c23d6e261878e56a204db3bb04888c1b28940c	hyp-despot: a hybrid parallel algorithm for online planning under uncertainty		Planning under uncertainty is critical for robust robot performance in uncertain, dynamic environments, but it incurs high computational cost. State-of-the-art online search algorithms, such as DESPOT, have vastly improved the computational efficiency of planning under uncertainty and made it a valuable tool for robotics in practice. This work takes one step further by leveraging both CPU and GPU parallelization in order to achieve near real-time online planning performance for complex tasks with large state, action, and observation spaces. Specifically, we propose Hybrid Parallel DESPOT (HyPDESPOT), a massively parallel online planning algorithm that integrates CPU and GPU parallelism in a multi-level scheme. It performs parallel DESPOT tree search by simultaneously traversing multiple independent paths using multi-core CPUs and performs parallel Monte-Carlo simulations at the leaf nodes of the search tree using GPUs. Experimental results show that HyPDESPOT speeds up online planning by up to several hundred times in several challenging robotic tasks in simulation, compared with the original DESPOT algorithm.	algorithmic efficiency;automated planning and scheduling;central processing unit;complex system;computation;graphics processing unit;hyperarithmetical theory;load balancing (computing);monte carlo method;multi-core processor;non-blocking algorithm;online search;overhead (computing);parallel algorithm;parallel computing;real-time clock;real-time computing;robot;robotics;search algorithm;search tree;simulation;speedup;tree (data structure);unbalanced circuit	Panpan Cai;Yuanfu Luo;David Hsu;Wee Sun Lee	2018	CoRR	10.15607/RSS.2018.XIV.004	massively parallel;machine learning;computer science;robot;distributed computing;online search;search tree;parallel algorithm;artificial intelligence;robotics	Robotics	50.56577431767003	-23.82928841784417	114095
0d4c80ebc32f9f4e633145934880f4d3d823f3b9	prediction error context-based lossless compression of medical images	evaluation performance;medical imagery;prediction error;performance evaluation;data compression;learning;edge detection;evaluacion prestacion;matrice diagonale;lossless compression;intelligence artificielle;deteccion contorno;aprendizaje;detection contour;apprentissage;compression image;medical image;matriz diagonal;image compression;relacion compresion;contexto;imagineria medica;imagerie medicale;contexte;compression ratio;artificial intelligence;number;compression sans perte;compresion dato;inteligencia artificial;taux compression;nombre;compresion sin perdida;context;numero;compression donnee;diagonal matrix;compresion imagen	This paper presents a new context formation and lossless compression of medical images in which has huge number of pixels and 2-byte pixel depth. We analyze various prediction techniques and compare their performance. The initial prediction is used for the context to update and correct the prediction error. The results show that diagonal edge detection-based prediction does not perform well in medical images and the proposed scheme outperforms JPEG-LS and DMED in terms of compression ratio up to 2.2%.	lossless compression	Jae-Jeong Hwang;Sang-Gyu Cho;Chi-Gyu Hwang;Jung-sik Lee	2003		10.1007/978-3-540-45080-1_149	data compression;lossy compression;computer vision;data compression ratio;edge detection;numero sign;image compression;computer science;artificial intelligence;mean squared prediction error;compression ratio;mathematics;lossless compression;prediction by partial matching;diagonal matrix;grammatical number;algorithm;statistics	ML	45.673025373475824	-12.94136003104611	114239
39dd582858475f1582502c34ea52aa8e4ac59873	model-based video compression for real world data	data compression;information filtering;video coding data compression image representation image retrieval information filtering rendering computer graphics screens display solid modelling video codecs;video coding;image representation;real world data model based video compression video content display 2d screen depth information redundant encoding process reappearing contents picture borders hybrid video codecs image filtering mpeg iv 2d video representation rendering subjective quality assessment static 3d model;video codecs;rendering computer graphics;data models three dimensional displays solid modeling encoding quality assessment cameras video coding;screens display;solid modelling;image retrieval	This work examines the benefits for displaying video content on a 2D screen that can be gained by incorporating depth information into the encoding process. The creation of a 3D model helps to compensate for redundant encoding of reappearing contents beyond group of picture borders, like needed in hybrid video codecs. Starting with filtering the image data, creating a 3D model and applying a model-based compression via MPEG-IV / Part 25 to the model, finally a 2D video representation is rendered. Subjective quality assessments of first preliminary results show, that the perceived video quality is comparable with H.264/AVC encoded videos for low quality scenarios when working with a static 3D model.	3d modeling;codec;data compression;digital video;h.264/mpeg-4 avc;moving picture experts group	Christian Feller;Jürgen Wünschmann;Raimar Wagner;Albrecht Rothermel	2013	2013 IEEE Third International Conference on Consumer Electronics ¿ Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2013.6697976	video compression picture types;data compression;scalable video coding;computer vision;video;h.263;uncompressed video;computer science;video quality;video capture;video tracking;block-matching algorithm;multimedia;video processing;smacker video;motion compensation;video post-processing;h.261;video denoising;multiview video coding;computer graphics (images)	Vision	44.01146615866709	-19.418178034980656	114248
ff45b4f72a6edc47ea23a07a4123abefe39f861c	unbalanced quantized multiple state video coding	desciframiento;signal image and speech processing;detection erreur;evaluation performance;deteccion error;multiple description;streaming;video streaming;path diversity;performance evaluation;decodage;decoding;video signal processing;constant bit rate;resource allocation;information transmission;correction erreur;coding errors;evaluacion prestacion;canal transmision;resource management;lts4;system performance;erreur codage;video coding;gestion recursos;transmission en continu;quantum information technology spintronics;senal video;signal video;codage video;canal transmission;transmission channel;rate allocation;error correction;image sequence;traitement signal video;video signal;gestion ressources;error resilience;secuencia imagen;rapport signal bruit;asignacion recurso;relacion senal ruido;transmision fluyente;transmision informacion;correccion error;error detection;transmission information;signal to noise ratio;allocation ressource;debit binaire constant;sequence image;heterogeneous network;velocidad de bit constante	Multiple state video coding (MSVC) is a multiple description scheme based on frame-wise splitting of the video sequence into two or more subsequences. Each subsequence is encoded separately to generate descriptions which can be decoded independently. Due to subsequence splitting, the prediction gain decreases. But since reconstruction capabilities improve, error resilience of the system increases. Our focus is on multiple state video coding with unbalanced quantized descriptions, which is particularly interesting for video streaming applications over heterogeneous networks where path diversity is used and transmission channels have varying transmission characteristics. The total bitrate is kept constant, while the subsequences are quantized with different stepsizes depending on the sequence as well as on the transmission conditions. Our goal is to figure out under which transmission conditions unbalanced bitstreams lead to good system performance in terms of the average reconstructed PSNR. Besides, we investigate the effects of intra-coding on the error resilience of the system and show that the sequence characteristics, and in particular the degree of motion in the sequence, have an important impact on the decoding performance. Finally, we propose a distortion model that is the core of an optimized rate allocation strategy, which is dependent on the network characteristics and status as well as on the video sequence characteristics.	data compression;distortion;interpolation;peak signal-to-noise ratio;quantization (signal processing);streaming media;unbalanced circuit	Sila Ekmekci Flierl;Thomas Sikora;Pascal Frossard	2006	EURASIP J. Adv. Sig. Proc.	10.1155/ASP/2006/14694	error detection and correction;telecommunications;computer science;resource management;statistics	ML	47.551210983333206	-14.944600084142014	114361
5cc779112647b8f43ad0a3d86be89e04a9b0742b	robust quantization for image coding and noisy digital transmission	memoryless source encoding;psnr performance;decoding image coding quantisation signal source coding memoryless systems telecommunication channels error correction codes visual communication digital communication noise filtering theory all pass filters;error correction codes;image coding;memoryless gaussian source;robustness quantization image coding filtering bit error rate design optimization encoding protection laplace equations shape;decoding;bit error rate;visual communication;performance;channel bit error rate noisy digital transmission robust quantization memoryless source encoding binary symmetric channel channel optimized scalar quantization all pass filtering binary phase scrambling descrambling method performance gaussian cosq memoryless gaussian source image coding image transmission error protection code side information psnr performance;gaussian cosq;quantisation signal;scalar quantization;digital communication;image transmission;error protection code;channel bit error rate;binary symmetric channel;all pass filtering;noisy digital transmission;telecommunication channels;memoryless systems;side information;channel optimized scalar quantization;robust quantization;filtering theory;binary phase scrambling descrambling method;noise;source coding;all pass filters	A robust quantizer is developed for encoding a variety of memoryless sources and transmission over the binary symmetric channel (BSC). The system combines channel optimized scalar quantization (COSQ) with all-pass filtering, the latter performed using a binary phase-scrambling/descrambling method. Applied to a broad class of sources, the robust quantizer achieves the same performance as the Gaussian COSQ for the memoryless Gaussian source. This quantizer is used for image coding for transmission over a BSC. An explicit error protection code is used only to protect the side information. The PSNR performance degrades gracefully as the channel bit error rate increases.		Thomas R. Fischer	1996		10.1109/DCC.1996.488305	binary symmetric channel;bit error rate;performance;telecommunications;computer science;noise;theoretical computer science;mathematics;statistics;visual communication;source code	Vision	48.25193230536552	-12.473170411477854	114536
564d3a9fae34af2e634e472a9a2c92f98bd8119a	two-level sliding-window vbr control algorithm for video on demand streaming	vbr;hevc;rate control;video coding;video storage;video on demand	A two-level variable bit rate (VBR) control algorithm for hierarchical video coding, specifically tailored for the new High Efficiency Video Coding (HEVC) standard, is presented here. A long-term level monitors the current bit count along a sliding window of a few seconds, comprising several intra periods (IPs) and shifted on an IP basis. This long-term view allows the accommodation of the naturally occurring rate variations at a slow pace, avoiding the annoying sharp quality changes commonly appearing when non-sliding window approaches are used. The bit excesses or defects observed at this level are evenly delivered to a short-term level mechanism that establishes target bit budgets for a narrower sliding window covering a single IP and shifting on a frame basis. At this level, an adequate quantization parameter is estimated to comply with the designated target bit rate. Recommended test conditions as well as two few minutes long video sequences with scene cuts have been used for the assessment of the proposed VBR controller. Comparisons with a state-of-the-art rate control algorithm have produced good results in terms of quality consistency, in exchange for moderate rate-distortion performance losses.		Manuel de-Frutos-López;José Luís González-de-Suso;Sergio Sanz Rodríguez;Carmen Peláez-Moreno;Fernando Díaz-de-María	2015	Sig. Proc.: Image Comm.	10.1016/j.image.2015.05.004	real-time computing;simulation;telecommunications;harmonic vector excitation coding;computer science;machine learning;variable bitrate;multiview video coding	Networks	45.5992575429044	-20.400414686926933	114800
d23e08197d1ec9582c2bf547c88f39fb7d20e34c	one-pass adaptive universal vector quantization	convergence;image coding;information systems;incoming data symbols;data compression;decoding;vector quantization adaptive coding statistics data compression information systems laboratories books decoding compressors scholarships;prior knowledge;adaptive codes;real bounded alphabet stationary sources;books;algorithm;vector quantization;compressors;adaptive coding;codebook;incoming data symbols one pass adaptive universal vector quantization real bounded alphabet stationary sources algorithm statistics encoder decoder codebook;scholarships;statistics;one pass adaptive universal vector quantization;convergence vector quantisation adaptive codes image coding;encoder;vector quantizer;vector quantisation;decoder	We introduce a one-pass adaptive universal quantization technique for real, bounded alphabet, stationary sources. The algorithm is set on line without any prior knowledge of the statistics of the sources which it might encounter and asymptotically achieves ideal performance on all sources that it sees. The system consists of an encoder and a de-coder. At increasing intervals, the encoder refines its code-book using knowledge about incoming data symbols. This codebook is then described to the decoder in the form of updates on the previous codebook. The accuracy t o which the codebook is described increases as the number of symbols seen, and thus the accuracy t o which the codebook is known, grows.	algorithm;codebook;encoder;major stationary source;stationary process;vector quantization	Michelle Effros;Philip A. Chou;Robert M. Gray	1994		10.1109/ICASSP.1994.389437	data compression;encoder;discrete mathematics;speech recognition;convergence;gas compressor;computer science;theoretical computer science;codebook;mathematics;adaptive coding;linde–buzo–gray algorithm;decoder;information system;vector quantization;statistics	Robotics	49.68419516826152	-12.247787907656619	114845
ecfa3bd8e5b4564fe86d901ab1635ec9d414cae7	fast algorithm based on sole- and multi-depth measurements for hevc intra coding	hafnium	In High Efficiency Video Coding (HEVC), intra coding plays an important role, but also involves huge computational complexity due to a flexible coding unit (CU) structure and a large number of prediction modes. This paper presents a fast algorithm based on the sole- and multi-depth measurements to reduce the complexity from CU and prediction mode decisions. For the CU decision, evaluation results with sole and multiple depths are utilized to judge if the CU is a heterogeneous, homogeneous, or depth prominent one, where fast CU decisions are made. For the prediction mode decision, the tendencies for different CU sizes are detected based on multiple depths. The number of searching modes is decreased adaptively for the depth with fewer tendencies. Experimental results show the proposed algorithm reduces 61.49% computational complexity, with 0.75% bit-rate increasing, which is more efficient than state-of-the-arts.	algorithm;computational complexity theory;high efficiency video coding;intra-frame coding;tip (unix utility)	Gang He;Jing Hu;Yunsong Li;Wenxin Yu	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532534	real-time computing;simulation;computer science;theoretical computer science;hafnium	Robotics	47.03093197342799	-19.45492643204162	114937
4bbdc37b56d83f010b3490d72010de1da9673f7d	a bit-plane coding scheme of mpeg-4 fgs with high efficiency based on the distribution of significant coefficients	estensibilidad;codificacion binaria;largeur bande;image coding;canal transmision;technique video;fine granularity scalability;tecnica video;codage image;video coding;binary coding;resilience;codage video;moving picture expert group;canal transmission;transmission channel;anchura banda;video transmission;mpeg 4;bandwidth;video technique;mpeg;extensibilite;scalability;resiliencia;high efficiency;codage binaire	MPEG-4 FGS video coding can perform video transmission with adaptation to channel bandwidth variation on the network. However, binary zero-run-length coding used by FGS decreases its coding efficiency in the lower bit-plane. In this paper, we propose a new coding scheme improving coding efficiency in the lower bit-plane. Based on the distribution of a significant coefficient, each bit is classified into two groups. At the same time, information that raises picture quality higher than any other coefficient is coded first. Simulation results show that this proposed scheme improves the coding efficiency up to more than 1.06%, and achieves 0.2dB gain in terms of average PSNR.	bit plane;coefficient;whole genome sequencing	Kenji Matsuo;Koichi Takagi;Atsushi Koike;Shuichi Matsumoto	2002		10.1007/3-540-36228-2_102	binary code;scalability;simulation;telecommunications;harmonic vector excitation coding;computer science;context-adaptive variable-length coding;coding gain;coding tree unit;multimedia;context-adaptive binary arithmetic coding;mpeg-4;bandwidth;psychological resilience;statistics	DB	47.13073992030905	-14.75281924652927	115001
a5c766bf1b57238cf6bf7f196dc5d15a790e1f65	bio performance complexity trade-off	interpolation;image motion analysis;decoding;computer vision;biomedical optical imaging;encoding;algorithm design and analysis	Bi-directional optical flow (so-called BIO) is part of Joint Exploration Model (JEM) which explores potential coding efficiency improvement over state-of-the-art video codec. BIO allows fine motion compensation on a sample level without additional signaling, since refinement is explicitly calculated using just texture information from both reference frames under assumption the validity of optical flow equation. BIO reduces BD-rate in average by more than 2% (up to 5% for some test video), but computational complexity is rather high. Two simplifications for BIO are studied in this paper. First is redesign chain for MC prediction and gradients calculation scheme. Simplified scheme has slightly high latency but reduces amount of multiplications in bi-predicted blocks by factor 2. Another simplification is clustering samples in order to perform motion refinement in BIO not per sample but for group of samples. This allows reduction of division operation in BIO by factor 9.7 in average (up to 256 times in largest blocks). Both modifications enabled together maintain the same performance for BIO in JEM while reduce encoding and decoding run-time significantly.	4k resolution;algorithmic efficiency;blu-ray;cluster analysis;codec;computational complexity theory;edge detection;gradient;high efficiency video coding;image resolution;level of detail;motion compensation;optical flow;refinement (computing)	Alexander Alshin;Elena Alshina	2016	2016 Picture Coding Symposium (PCS)	10.1109/PCS.2016.7906341	algorithm design;computer vision;simulation;interpolation;computer science;theoretical computer science;mathematics;algorithm;encoding	Vision	47.05506735445474	-19.176806468142697	115011
65df5e8dec325cc43115205ed6a9e31b2ad97de7	block-size adaptive transform domain estimation of end-to-end distortion for error-resilient video coding	decoding;distortion;estimation;discrete cosine transforms;correlation;encoding	The accuracy of end-to-end distortion (EED) estimation is crucial to achieving effective error resilient video coding. An established solution, the recursive optimal per-pixel estimate (ROPE), does so by tracking the first and second moments of decoder-reconstructed pixels. An alternative estimation approach, the spectral coefficient-wise optimal recursive estimate (SCORE), tracks instead moments of decoder-reconstructed transform coefficients, which enables accounting for transform domain operations. However, the SCORE formulation relies on a fixed transform block size, which is incompatible with recent standards. This paper proposes a non-trivial generalization of the SCORE framework which, in particular, accounts for arbitrary block size combinations involving the current and reference block partitions. This seemingly intractable objective is achieved by a two-step approach: i) Given the fixed block size moments of a reference frame, estimate moments of transform coefficients for the codec-selected current block partition; ii) Convert the current results to transform coefficient moments corresponding to a regular fixed block size grid, to facilitate EED estimation for the next frame. Experimental results first demonstrate the accuracy of the proposed estimate in conjunction with transform domain temporal prediction. Then the estimate is leveraged to optimize the coding mode and yields considerable gains in rate-distortion performance.	block size (cryptography);codec;coefficient;data compression;distortion;end-to-end encryption;end-to-end principle;pixel;recursion;reference frame (video)	Bohan Li;Tejaswi Nanjundaswamy;Kenneth Rose	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532727	mathematical optimization;estimation;discrete mathematics;s transform;lapped transform;distortion;mathematics;correlation;encoding;statistics;sum of absolute transformed differences	Vision	45.593479148256236	-17.573786141543287	115343
b825e120ade778e569572599c90538dc31924bd9	on multi-directional context sets	trees mathematics data compression image coding image denoising image representation image sequences;dynamic programming;context bidirectional control heuristic algorithms noise reduction context modeling source coding dynamic programming;image coding;data compression;pruning algorithm multidirectional context set context tree model sequential decision problem data compression tree representation;tree pruning algorithms;context trees;dynamic program;trees mathematics;indexing terms;context model;decision problem;tree pruning algorithms context trees denoising dynamic programming multi directional context sets multi tracked data;image representation;heuristic algorithms;noise reduction;bidirectional control;multi tracked data;source code;image denoising;denoising;multi directional context sets;context modeling;context;heuristic algorithm;source coding;image sequences	The classical framework of context-tree models used in sequential decision problems such as compression and prediction is generalized to a setting in which the observations are multi-tracked, multi-sided, or multi-directional, and for which it may be beneficial to consider contexts comprised of possibly differing numbers of symbols from each track or direction. Tree representations of context sets and pruning algorithms for those trees are extended from the uni-directional setting to two directions. We further show that such tree representations do not extend, in general, to m directions, m >; 2, and that, as a result, determining the best m-directional context set for m >; 2 may be substantially more complex than in the case of m ≤ 2. An application of the proposed pruning algorithm to denoising, where m=2 , is presented.	algorithm;decision problem;dynamic programming;image processing;loss function;mathematical optimization;noise reduction;pixel;pruning (morphology);recurrence relation;recursion;whole earth 'lectronic link	Erik Ordentlich;Marcelo J. Weinberger;Cheng Chang	2011	IEEE Transactions on Information Theory	10.1109/TIT.2011.2165818	computer science;theoretical computer science;machine learning;pattern recognition;noise reduction;context model;source code	Vision	50.204934850325664	-13.554707809029955	115379
52b814e91d27f6afbadf1f92e3aad10be6e91dbe	decoder-side intra mode derivation for block-based video coding	software;standards;decoding;prediction algorithms;video coding;copper;encoding	This paper proposes a decoder-side intra mode derivation algorithm for block-based video coding. Instead of explicitly coding intra mode, the algorithm derives intra mode at both encoder and decoder using a template-based method. Based on rate-distortion optimization, the encoder locally determines whether intra mode derivation or intra mode explicit coding is used. Further, as no intra mode is coded, the proposed algorithm is able to more accurately capture the direction of edges in natural videos by increasing the granularity of directional intra predictions with no signaling overhead increase. To verify the effectiveness, the proposed algorithm is implemented on the Joint Exploration Model 2.0 platform. Experimental results show that an average Bjentegaard delta rate saving of 0.9% is achieved by the proposed algorithm.	algorithm;blu-ray;data compression;distortion;encoder;high efficiency video coding;intra-frame coding;mathematical optimization;overhead (computing);rate–distortion optimization	Xiaoyu Xiu;Yuwen He;Yan Ye	2016	2016 Picture Coding Symposium (PCS)	10.1109/PCS.2016.7906340	real-time computing;simulation;prediction;computer science;theoretical computer science;mathematics;copper;encoding;statistics	AI	47.00727904905407	-18.75374149032833	115454
194d1b3bee0bff3560fd3da5cd5d929f7d730525	an illumination invariant 3d model based tracking algorithm, with application in video compression	image motion analysis illumination invariant tracking algorithm video compression model based coding scheme reconstructed video sequence approximation theory;illumination;data compression;lighting video compression motion estimation tracking image coding decoding video sequences bit rate image reconstruction cameras;constant bit rate;video compression;motion estimation;illumination model based compression;indexing terms;approximation theory;video coding;model based compression;3d model;image reconstruction;video coding approximation theory data compression image reconstruction image sequences motion estimation tracking;illumination invariance;tracking;image sequences	We present an algorithm for illumination invariant 3D model based tracking and video compression. While model-based coding schemes are well developed, the compression rate reduces if the illumination conditions within the sequence change drastically. Our proposed scheme can accurately track under both slow and drastic changes of lighting and achieves a significant reduction in the bit rate compared to a scheme that does not consider any illumination models. The tracking scheme is based on a recently developed framework showing that the joint motion and illumination space of video sequences is approximately bilinear. We show results of the reconstructed video sequences and the reduction in the distortion (for a constant bit rate) for a compression scheme that uses the illumination invariant tracking algorithm.	3d modeling;algorithm;bilinear filtering;data compression;distortion;list of common shading algorithms	Long Nyugen;Yilei Xu;Amit K. Roy-Chowdhury	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312543	data compression;computer vision;computer science;theoretical computer science;mathematics;statistics;computer graphics (images)	Vision	43.651138187099555	-18.54293842850546	115636
4c911a957472820e257ae4394866f28c896b3d76	regularized dequantizers for dct-based transform coding of images	image compression	We have recently proposed a new dequantization scheme for DCT- based transform coding based on regularization principles. The new approach sharply reduced blocking artifacts in decoded images and the performance of the new dequantization scheme has been evaluated against the standard JPEG, MPEG and H.263+ in terms of the peak-signal-to-noise ratio (PSNR) with our own definition of the blockiness measure (BM). Basically, the proposed dequantizer maps the received data to within the range +/- (quantizer spacing/2), so that the final decompressed image is u0027smoothu0027 in the sense of minimizing the cost functional including the stabilizing term weighted by a regularization parameter. In this paper, we focus on several important aspect of this regularized dequantizer, namely the selection of the regularization parameter and the convergence of the dequantization algorithm.	discrete cosine transform;transform coding	Gunho Lee;Sinae Kim;Samuel Moon-Ho Song	2001			mathematical optimization;transform coding;speech recognition;peak signal-to-noise ratio;image compression;theoretical computer science	Vision	44.10041433839468	-16.290728027170097	115819
f905e679fa8a9b7a0160ea8f40aeff9557b31664	no reference video quality assessment based on parametric analysis of hevc bitstream	silicon;psnr;video coding silicon streaming media psnr video recording quality assessment transforms;no reference hevc objective assessment of video quality bitstream;video coding;quality assessment;streaming media;video coding quality of experience video codecs;transforms;objective perceptual video quality measurement video quality assessment parametric analysis hevc bitstream quality of experience qoe consumer video services automatic quality monitoring no reference objective video quality model nr objective video quality model picture quality high efficiency video coding next generation representative codec parametric nr methods avc;video recording	The quality of experience (QoE) on consumer video services must be maintained at a sufficient level. In order to realize automatic quality monitoring with a low burden, a no-reference (NR) objective video-quality-model by using a bitstream is promising because it can estimate a perceptual picture quality by referring to only parameters in the bitstream. High efficiency video coding (HEVC) is expected as the next-generation representative codec. Although parametric NR methods for AVC have been studied, such methods for HEVC are an emerging task in this research field. We propose an objective perceptual video-quality-measurement for HEVC that conforms to the parametric NR scheme.	bitstream;codec;data compression;h.264/mpeg-4 avc;high efficiency video coding;image quality;noise reduction;numerical recipes	Kosuke Izumi;Kei Kawamura;Tomonobu Yoshino;Sei Naito	2014	2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX)	10.1109/QoMEX.2014.6982287	video compression picture types;scalable video coding;subjective video quality;computer vision;simulation;h.263;peak signal-to-noise ratio;computer science;video quality;multimedia;video processing;rate–distortion optimization;silicon;motion compensation;pevq;h.261;multiview video coding	Web+IR	44.537744760225806	-19.77477096112212	115916
84a8d4409505ddbf75527a8f8f089f18c107301b	improvement of h.264 svc by model-based adaptive resolution upconversion	single code stream;rate distortion;video streaming;scalable video coding;adaptive resolution upconversion;video coding;automatic voltage control;inverse problem;estimation;inter layer prediction;pixel;rate distortion performance;pixel static var compensators mathematical model automatic voltage control spatial resolution estimation video coding;mathematical model;static var compensators;video streaming video coding;down sampling filter h 264 svc adaptive resolution upconversion video coding single code stream rate distortion performance;resolution upconversion;down sampling filter;inverse problem scalable video coding inter layer prediction resolution upconversion;h 264 svc;spatial resolution	H.264 SVC extension, as the state of art scalable video coding standard, can offer a single code stream to serve diverse communication bandwidths and display resolutions. However, the rate-distortion performance of H.264 SVC is still inferior to the non-scalable H.264 AVC. To reduce the performance gap between H.264 SVC and H.264 AVC, we propose a model-based adaptive resolution upconversion algorithm to improve the precision of the H.264 SVC inter-layer prediction. The new algorithm treats the up-sampling of video frames as an inverse problem of initial H.264 SVC down-sampling operation, and it significantly improves the performance of current H.264 SVC by optimally reversing the down-sampling filter.	algorithm;data compression;display resolution;distortion;h.264/mpeg-4 avc;inverse filter;minimum phase;nonlinear system;reversing: secrets of reverse engineering;ruby document format;sampling (signal processing);scalability;scalable video coding;video coding format	Xiaolin Wu;Mingkai Shao;Xiangjun Zhang	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5650774	scalable video coding;estimation;real-time computing;image resolution;telecommunications;computer science;inverse problem;mathematical model;mathematics;pixel;statistics	Robotics	45.16331563713983	-17.86290791652438	115992
fdbd78de0dd5fa2a8828aa74fd3ceafb3b8347e1	depth-assisted error concealment for intra frame slices in 3d video	engineering;decoding;image matching;decoding bit rate motion estimation interpolation video sequences computational complexity vectors;video coding;computational complexity;3d video coding error concealment slice loss intra frame;video coding computational complexity decoding image matching image sequences;decoder depth assisted error concealment method intraframe slices 3d video 2d view sequence slice loss 2d depth video sequence boundary matching computational complexity;image sequences	We propose a depth-assisted error concealment method for slice loss in intra frames of 2D+depth video sequence. Intra frames in the 2D view sequence are offset from intra frames in the depth sequence to guarantee the corresponding frame in the other sequence is not also intra mode. Then for a slice loss in an intra frame in the 2D view sequence, the motion information is extracted from the depth sequence to conceal the slice loss using boundary matching. Experimental results show that the proposed method provides improved performance over existing methods both for PSNR results and computational complexity at the decoder.	2d-plus-depth;computational complexity theory;error concealment;intra-frame coding;jumbo frame;peak signal-to-noise ratio	Meng Meng Yang;Yuhong Yang;Pamela C. Cosman	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467101	reference frame;computer vision;computer science;theoretical computer science;block-matching algorithm;multimedia;computational complexity theory;algorithm	Robotics	46.06095319537227	-18.211460274852566	116367
edbb5d5912044d9e9cc9d2323be4a3007702cc37	multiple search centers based fast motion estimation algorithm for h.264/avc	prediction method;rate distortion;prediction algorithms heuristic algorithms bit rate encoding strontium computational complexity motion estimation;video encoder;search range adjustment;motion vector prediction;h 264 avc;multiple search centers;prediction algorithms;motion estimation;bit rate;strontium;video coding;fast motion estimation;computational complexity;heuristic algorithms;motion vector;fast algorithm;search range adjustment motion estimation video encoder multiple search centers h 264 avc prediction method computational complexity;video coding computational complexity motion estimation;prediction accuracy;h 264 avc multiple search centers motion vector prediction fast motion estimation;encoding	Motion estimation is the most computationally intensive part in a typical video encoder. This paper introduces a novel prediction method based on multiple search centers, which can efficiently improve the prediction accuracy. According to the amount and the magnitude of predictive search centers, the search range is dynamically adjusted in order to reduce the computational complexity. Based on multiple search centers prediction and dynamic search range adjustment, a new fast algorithm is proposed. Experimental results show that, on an average, the proposed algorithm gains similar rate-distortion performance to the original FFS, UMHexagonS and EPZS algorithms in H.264 reference software, while reducing about 97.4%, 63.0%, and 42.9% computational complexity respectively.	computational complexity theory;distortion;encoder;h.264/mpeg-4 avc;interpolation search;motion estimation;predictive text;search algorithm	Huitao Gu;Shuming Chen;Shuwei Sun;Shenggang Chen;Xiaowen Chen	2010	2010 International Conference on Parallel and Distributed Computing, Applications and Technologies	10.1109/PDCAT.2010.13	computer vision;encoder;strontium;prediction;computer science;theoretical computer science;machine learning;motion estimation;computational complexity theory;encoding;statistics	Robotics	47.42850612662782	-19.019552215067527	116600
79dfe79124c048b5b4727fde4af574d0cb68e21f	an iterative side information refinement technique for transform domain distributed video coding	silicon;transform domain distributed video coding;iterative refinement;iterative side information refinement technique;rate distortion;codecs;iterative decoding;decoding;video coding iterative decoding discrete cosine transforms discrete transforms codecs costs motion estimation source coding rate distortion video surveillance;dvc codec;video coding discrete cosine transforms iterative decoding rate distortion theory transform coding video codecs;low complexity;motion estimation;transform coding;rate distortion theory;dct iterative side information refinement technique transform domain distributed video coding low complexity video encoder decoder dvc codec rate distortion performance;video coding;low complexity video encoder;side information refinement dvc wyner ziv coding;discrete cosine transforms;rate distortion performance;dvc;distributed video coding;wyner ziv coding;video codecs;side information;encoding;side information refinement;dct;decoder	Distributed Video Coding (DVC), has been an interesting alternative to the conventional video coding for a number of applications because of its flexibility for designing extremely low complexity video encoders. The performance of DVC can be improved by only modifying the decoder to keep the encoder complexity at the same level. In this paper, we propose a novel modified framework for a DVC codec considering an iterative side information refinement technique. Refinement is performed at the decoder, first with the help of the decoded DC frame and then considering partially decoded frame using previously refined side information. By iteratively refining the side information, a significant improvement has been achieved in the rate distortion performance.	codec;data compression;distortion;encoder;iterative method;rate–distortion theory;refinement (computing)	Murat B. Badem;Warnakulasuriya Anil Chandana Fernando;José Luis Martínez;Pedro Cuenca	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202465	computer vision;codec;transform coding;rate–distortion theory;telecommunications;computer science;theoretical computer science;discrete cosine transform;motion estimation;silicon;decoder;encoding;statistics	Vision	48.96753538284761	-17.159273361414545	116650
f9eb75d52ac45a72519c65dbaf8d00540a81dff0	introduction to the special issue on scalable video coding-standardization and beyond	standards;scalable video coding;decoding;special issues and sections;iso standards;video compression;special issues and sections scalability automatic voltage control video coding decoding video compression iso standards iec standards static var compensators streaming media;video coding;iec standards;automatic voltage control;streaming media;static var compensators;scalability	AFEW YEARS ago, the subject of video coding received a large amount of skepticism. The basic argument was that the 1990s generation of international standards was close to the asymptotic limit of compression capability—or at least was good enough that there would be little market interest in something else. Moreover, there was the fear that a new video codec design created by an open committee process would inevitably contain so many compromises and take so long to be completed that it would fail to meet real industry needs or to be state-of-the-art in capability when finished. The Joint Video Team (JVT) then proved those views to be wrong and revitalized the field of video coding with the 2003 creation of the H.264/AVC standard (ITU-T Rec. H.264 and ISO/IEC 14496-10 Advanced Video Coding). The JVT was formed in late 2001 as a partnership between the two preeminent relevant standards bodies—the ITU-T Video Coding Experts Group (VCEG) and the ISO/IEC Moving Picture Experts Group (MPEG). A Special Issue of the IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY (TCSVT) devoted to the H.264/AVC standard was published just a few months after its standardization was completed. Since that time, H.264/AVC has been very strongly embraced by industry and is now being deployed in virtually all new and existing applications of digital video technology. This new Special Issue is devoted to the next major challenge undertaken by the JVT. That challenge was to build upon the strong compression foundation of H.264/AVC to create a Scalable Video Coding (SVC) standard that would extend the capabilities of the H.264/AVC design to address the needs of applications to make video coding more flexible for use in highly heterogeneous and time-varying environments. The fundamental concept of SVC is to enable the creation of a compressed bit stream that can be rapidly and easily adapted to fit with the bit rate of various transmission channels and with the display capabilities and computational resource constraints of various receivers. This is accomplished by structuring the data of a compressed video bit stream into layers. The base layer is decodable by an ordinary H.264/AVC decoder, while one or more enhancement layers provides improved quality for those decoders that are capable of using it. Three fundamental types of scalability were enabled. The first is temporal scalability, in which the enhancement layer provides an increase of the frame rate of the base layer. To a large extent this was already supported in the original H.264/AVC standard—but new supplemental information has been designed to make such uses more powerful. The next is spatial scalability (or resolution scalability), in which the enhancement layer offers increased picture resolution for receivers with greater display capabilities. Among the three types	asymptote;bitstream;codec;computational resource;data compression;digital video;h.264/mpeg-4 avc;image resolution;moving picture experts group;principle of good enough;scalability;scalable video coding	Thomas Wiegand;Gary J. Sullivan;Jens-Rainer Ohm;Ajay Luthra	2007	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2007.905355	data compression;scalable video coding;embedded system;real-time computing;scalability;h.263;computer science;multimedia;mpeg-4;statistics	Visualization	43.221916616558474	-19.85210297832823	116798
b0f3e66d9d21835a167e26274038d8b5a3e5cd89	lossless compression of hdr color filter array image for the digital camera pipeline	lossless compression;digital camera pipeline;color filter array;high dynamic range;weighted template matching prediction	This paper introduces a lossless color filter array (CFA) image compression scheme capable of handling high dynamic range (HDR) representation. The proposed pipeline consists of a series of pre-processing operations followed by a JPEG XR encoding module. A deinterleaving step separates the CFA image to sub-images of a single color channel, and each sub-image is processed by a proposed weighted template matching prediction. The utilized JPEG XR codec allows the compression of HDR data at low computational cost. Extensive experimentation is performed using sample test HDR images to validate performance and the proposed pipeline outperforms existing lossless CFA compression solutions in terms of compression efficiency.	algorithmic efficiency;bayer filter;channel (digital image);codec;color filter array;color space;digital camera;high dynamic range;image compression;jpeg xr;kerrison predictor;lossless compression;preprocessor;scene statistics;spectral efficiency;template matching	Dohyoung Lee;Konstantinos N. Plataniotis	2012	Sig. Proc.: Image Comm.	10.1016/j.image.2012.02.017	data compression;lossy compression;lossless jpeg;computer vision;color filter array;image compression;computer science;jpeg;lossless compression;algorithm;statistics;computer graphics (images)	Vision	43.778176857187184	-16.954145330701287	116846
913f0311888b6da53c97ac33bab2f98d3a808aa7	jbig2 text image compression based on ocr	mismatching;phase measurement;image coding;0705p;data compression;lossy medium;etude experimentale;optical character recognition;circuit sans perte;medio dispersor;4230s;qualite image;algorithme;codage image;desadaptacion;compression image;mesure phase;image compression;pattern matching;image quality;lossless circuit;compression ratio;algorithms;calidad imagen;concordance forme;desadaptation;taux compression;scanning probe microscopy;circuito sin perdida;compression donnee;reconnaissance optique caractere;milieu dissipatif;compresion imagen	The JBIG2 (joint bi-level image group) standard for bi-level image coding is drafted to allow encoder designs by individuals. In JBIG2, text images are compressed by pattern matching techniques. In this paper, we propose a lossy text image compression method based on OCR (optical character recognition) which compresses bi-level images into the JBIG2 format. By processing text images with OCR, we can obtain recognition results of characters and the confidence of these results. A representative symbol image could be generated for similar character image blocks by OCR results, sizes of blocks and mismatches between blocks. This symbol image could replace all the similar image blocks and thus a high compression ratio could be achieved. Experiment results show that our algorithm achieves improvements of 75.86% over lossless SPM and 14.05% over lossy PM&S in Latin Character images, and 37.9% over lossless SPM and 4.97% over lossy PM&S in Chinese character images. Our algorithm leads to much fewer substitution errors than previous lossy PM&S and thus preserves acceptable decoded image quality.	ascii art;algorithm;binary image;black and burst;cluster analysis;encoder;image compression;image quality;jbig2;lossless compression;lossy compression;optical character recognition;pattern matching;super paper mario	Junqing Shang;Changsong Liu;Xiaoqing Ding	2006		10.1117/12.641557	data compression;image quality;lossy compression;computer vision;scanning probe microscopy;telecommunications;image compression;jbig2;computer science;pattern matching;compression ratio;optical character recognition;algorithm	Graphics	45.77886550234751	-13.483509315308856	117031
9819c5c1bbfbc092eff77077886cfb4e22cdfda3	an error-resilient algorithm based on partitioning of the wavelet transform coefficients for a dirac video codec	channel coding;coefficient partitioning;arithmetic coding;data compression;error resilient algorithm;transmission error;variable length code;coefficient;video compression;partitioning;turbo codes;data communication;network simulator;video codec;video streams;wavelet transforms;video coding;protection;arithmetic codes;wavelet transform;partitioning algorithms wavelet transforms video codecs video compression streaming media robustness wireless sensor networks protection predictive coding wavelet packets;channel errors;streaming media;fault tolerance;open source video codec;video transmission;error resilience;bit stream compression;video codecs;robustness;dirac;turbo coding;turbo coding error resilient algorithm wavelet transform coefficient partitioning dirac video codec video transmission channel errors video streams predictive coding variable length coding transmission error bit stream compression open source video codec arithmetic coding;wavelet packets;variable length coding;video communication;dirac video codec;predictive coding;wavelets;wavelet transform coefficient partitioning;wireless sensor networks;dirac error resilient coding coefficient partitioning wavelets;wavelet transforms arithmetic codes channel coding data communication data compression fault tolerance turbo codes video coding video communication;turbo code;compressed video;error resilient coding;partitioning algorithms;open source	Video transmission over the wireless or wired network require protection from channel errors since compressed video streams are very sensitive to transmission errors because of the use of predictive coding and variable length coding. In this paper, we propose a method to achieve robustness to transmission errors to the compressed bit-stream of wavelet based open source video codec, Dirac. By partitioning the wavelet transform coefficients into groups and independently processing each group using arithmetic and turbo coding, we could achieve the robustness to transmission errors of the compressed video stream in the packet erasure wired network. Simulation results show that the proposed technique can achieve up to 5dB PSNR gain over the un-partitioning method	algorithm;bitstream;codec;coefficient;data compression;emoticon;information visualization;network packet;open-source software;peak signal-to-noise ratio;simulation;streaming media;transmitter;turbo code;variable-length code;wavelet transform	Myo Tun;Warnakulasuriya Anil Chandana Fernando	2006	Tenth International Conference on Information Visualisation (IV'06)	10.1109/IV.2006.19	real-time computing;telecommunications;computer science;theoretical computer science	Mobile	48.48030376536674	-15.214010891409705	117081
38a4e5788414eb954e5d1001de99aba18656c5c4	efficient reference frame selector for h.264	traitement signal;evaluation performance;rate distortion;systeme intelligent;estimation mouvement;image coding;multiple reference frames;algoritmo busqueda;performance evaluation;image processing;motion compensation;h 264 video codec;video signal processing;h 264 motion estimation;helium;frame selection;algorithme recherche;evaluacion prestacion;sistema inteligente;estimacion movimiento;search algorithm;procesamiento imagen;reference frame;size measurement;codec video;motion estimation;motion search algorithm;testing;motion estimation motion compensation video codecs encoding testing code standards standards development rate distortion computational complexity;code standards;encoding complexity;indexing terms;traitement image;variable block size;video codec;codage image;video coding;standards development;compression image;image compression;computational complexity;signal processing;variable block size frame selection h264 motion estimation multiple reference frames;intelligent system;variable block size motion estimation;h 264;traitement signal video;video codecs;h 264 video codec h 264 motion estimation reference frame selector variable block size motion estimation encoding complexity motion search algorithm;search problems;mesure dimension;video coding motion estimation search problems video codecs;reference frame selector;encoding;procesamiento senal;compresion imagen	This paper proposes a simple yet effective mechanism to select proper reference frames for H.264 motion estimation. Unlike traditional video codecs, H.264 permits more than one reference frame for increased precision in motion estimation. However, motion estimation is complicated by variable block-size motion estimation, which requires significant encoding complexity to identify the best inter-coding. Our smart selection mechanism selects suitable reference frames by means of a simple test, and only the selected frames will be searched further in the variable block size motion estimation. One major advantage of our mechanism is that it enables working with any existing motion search algorithms developed for the traditional single reference frame. Experimental results demonstrate the effectiveness of our proposed algorithm.	block size (cryptography);codec;encoder;h.264/mpeg-4 avc;motion estimation;reference frame (video);search algorithm	Tien-Ying Kuo;Hsin-Ju Lu	2008	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2008.918111	reference frame;inter frame;residual frame;computer vision;index term;image processing;image compression;quarter-pixel motion;computer science;theoretical computer science;signal processing;motion estimation;block-matching algorithm;software testing;helium;computational complexity theory;motion compensation;algorithm;encoding;search algorithm;computer graphics (images)	Vision	46.55237870559915	-15.369512077475742	117092
513ebb10d44f99d6a2aae1ac8cb2be8f58582fe5	feature based compression of vector quantized codebooks and data for optimal image compression	feature extraction;image coding;vector quantisation;lbg codebook;linde-buzo-gray algorithm;snr;centroids;clustering technique;feature-based clustering;initial codebook;optimal image compression;vector quantized codebooks	The Linde-Buzo-Gray (LBG) algorithm is a procedure in vector quantization. An attempt is made to compress the LBG codebook by simple feature-based clustering. The centroids of the resulting clusters form a reduced LBG codebook. The resulting reduced LBG codebook has almost identical SNR as the equivalent size original LBG codebook. Several other experimental results are presented which show that the above clustering technique may be applied directly to the original images in order to find an initial codebook for LBG codebook training, or to quickly generate an alternative codebook with comparable quality	codebook;image compression	Jack R. Knutson;Chang Y. Choo	1993			speech recognition;computer science;machine learning;pattern recognition;mathematics;linde–buzo–gray algorithm;signal-to-noise ratio	ML	43.89255539550158	-13.087315835789108	117212
d3f977b42801cccdf076cd4c1aa5a65870b2a3f4	efficient binary search space-structured vq encoder applied to a line spectral frequency quantisation in g.729 standard	vector quantisation codebook search algorithm;multiple triangular inequality elimination approach;bss vq algorithm;dynamic rules;data compression;g 729 standard;line spectral frequency quantisation;quasibinary search vq algorithm;itu t g 729 speech codec;intersection rules;lookup tables;input vector;efficient binary search space structured vq encoder;fast locating technique;computational load reduction	This study presents a simple but high performance vector quantisation (VQ) codebook search algorithm for line spectral frequency quantisation in ITU-T G.729 speech codec, designated as the binary search space-structured VQ (BSS-VQ) algorithm. This is done through a combined use of a fast locating technique and lookup tables, such that an input vector, ahead of VQ encoding, is assigned efficiently to a subspace where merely a small number of codeword searches are required to be performed. As a consequence, the computational load is reduced remarkably. A trade-off can be made easy to meet a user's requirement when performing VQ encoding. It is experimentally validated that a search accuracy is well maintained at 99% approximately for a threshold of quantisation accuracy of 0.99. More importantly, with a full search algorithm as a benchmark for search load comparison, this proposal provides a 85% search load reduction, a figure far beyond 43% in multiple triangular inequality elimination approach, 47% in TIE with dynamic and intersection rules and 59% in quasi-binary search VQ algorithm.	binary search algorithm;encoder;g.729;quantization (physics);vector quantization	Tzu-Hung Lin;Cheng-Yu Yeh;Shaw-Hwa Hwang;Shun-Chieh Chang	2016	IET Communications	10.1049/iet-com.2015.0729	data compression;speech recognition;lookup table;computer science;theoretical computer science;machine learning;mathematics;statistics	Vision	46.204857271364034	-10.35123806381891	117357
805a9f64ae16a93de0a56f57a1c210e5c65c3706	perspective transform motion modeling for improved side information creation	signal image and speech processing;quantum information technology spintronics	The distributed video coding (DVC) paradigm is based on two well-known information theory results: the SlepianWolf and Wyner-Ziv theorems. In a DVC codec, the video signal correlation is mostly exploited at the decoder, providing a flexible distribution of the computational complexity between the encoder and the decoder and error robustness to channel errors. To exploit the temporal correlation, an estimate of the original frame to code, wellknown as side information, is typically created at the decoder. One popular approach to side information creation is to perform frame interpolation using a translational motion model derived from already decoded frames. However, this translational model fails to estimate complex camera motions, such as zooms and rotations, and is not accurate enough to estimate the true trajectories of scene objects. In this paper, a new side information creation framework integrating perspective transform motion modeling is proposed. This solution is able to better locally track the trajectories and deformations of each object and increase the accuracy of the overall side information estimation process. Experimental results show peak signal-to-noise ratio gains of up to 1 dB in side information quality and up to 0.5 dB in rate-distortion performance for some video sequences regarding state-of-the-art alternative solutions.	3d projection;algorithm;binocular disparity;codec;coherence (physics);computational complexity theory;data compression;decibel;distortion;encoder;h.264/mpeg-4 avc;information quality;information theory;international symposium on fundamentals of computation theory;motion interpolation;peak signal-to-noise ratio;programming paradigm;ruby document format	Pedro Monteiro;João Ascenso;Fernando da Cruz Pereira	2013	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2013-189	computer vision;simulation;soft-decision decoder;telecommunications;computer science;theoretical computer science	Vision	49.18674303496395	-17.51019433036173	117372
fdd833aebb9fd3879d6fb078a1322befda1060d3	real-time macroblock level bits allocation for depth maps in 3-d video coding	macroblock level;r d optimization;3 d video coding;bits allocation;realtime	In the texture-plus-depth 3-D video format, the texture videos and depth maps will affect the quality of the synthesized views, this makes bits allocation for the depth maps indispensable. The existing bits allocation approaches are either inaccurate or requiring pre-encoding and analyzing in temporal dimension, making them unsuitable for the real-time applications. Motivated by the fact that different regions of the depth maps have different impacts on the synthesized image quality, a real-time macroblock level bits allocation approach is proposed, where different macroblocks of the depth maps are encoded with different quantization parameters and coding modes. As the bits allocation granularity is fine, the R-D performance of the proposed approach outperforms other bits allocation approaches significantly, while no additional pre-encoding delay is caused. Specifically, it can save more than 10% overall bit rate comparing with Morvan's full search approach, while maintaining the same synthesized view quality.		Jimin Xiao;Tammam Tillo;Hui Yuan	2012		10.1007/978-3-642-34778-8_21	computer vision;4b5b;real-time computing;computer science;theoretical computer science;bit field	Theory	46.15700392538488	-20.758865863788902	117396
486cb0398691fb603d2011340dcb24a3e56d152a	speech compression with preservation of speaker identity	speaker identification;data compression;gaussian processes;speech coding;spectrum representation speech compression speaker identity preservation spectral distortion decompressed speech coded speech multistage vector quantization short term filter parameters formant filter parameters text independent speaker identification compressed database speaker model raw speech spectral compression gaussian models spectral vector quantization speaker identification accuracy vq;speaker recognition;signal representation data compression speech coding speaker recognition gaussian processes spectral analysis filtering theory vector quantisation;signal representation;vector quantizer;speech processing databases speech coding phase change materials australia vector quantization filters predictive models laboratories signal processing;spectral analysis;vector quantisation;filtering theory	"""Although much e ort has been directed recently towards speech compression at rates below 4 kb/s, the primary metric for comparison has, understandably, been the amount of spectral distortion in the decompressed speech. However, an aspect which is becoming important in some applications is the ability to identify the original speaker from the coded speech algorithmically. We investigate here the e ect of speech compression using multistage vector quantization of the shortterm (formant) lter parameters on text-independent speaker identi cation. It is demonstrated that in cases where the speech is stored in a compressed database for retrieval, the speaker model should be constructed from the raw speech before spectral compression. Additionally, Gaussian models of su ciently high order are able to reduce the negative e ects of spectral vector quantization upon speaker identi cation accuracy. 1. PROBLEM FORMULATION When attempting to identify speakers from their voice, spectral features (linear transformations or derived from predictor coe cients) have been found to be more effective than prosodic features (pitch, stress and articulation rate) [5]. In considering the evaluation of the e ect of spectrum compression on speaker identi cation, four possible scenarios arise as shown in Table 1. These are :(i) The \benchmark"""" for all cases, using raw speech in the identi cation process. No compression is performed on either the incoming or reference speech data. (ii) The speech database is compressed (for example, on CD-ROM) and the incoming speech is available in uncompressed form. This situation arises in forensic speech processing where the database of suspects has been archived and a new suspect is to be compared. (iii) The incoming speech is compressed, but the reference is not. This problemmay arise in telecommunications applications. Note that in this case the speaker identi cation parametersmay be precomputed and stored (depending on the identi cation algorithm), allowing the speech database to be compressed without substantially compromising the speaker identi cation accuracy. (iv) Both the database and the incoming speech are compressed. We present results for each of these cases in Section 5. Although the e ect of both population size and non-ideal recording conditions has been reported in the literature [2], the availibility of the speech in digital form enables the means of identi cation to be based on the encoded voice model, rather than on an analog reconstruction of the speech. 2. SPECTRUM REPRESENTATION The short-term speech predictor is used for the purposes of both coding and identi cation. This predictor models the spectral envelope of the speech. The shortterm analysis lter is represented as Table 1: Compression and Speaker Identi cation. Condition Speech Database Incoming Speech (i) 16-bit PCM 16-bit PCM (ii) Spectral VQ PCM (iii) PCM Spectral VQ (iv) Spectral VQ Spectral VQ A(z) = 1 + a1z 1 + a2z 2 + + amz m (1) where the m coe cients ai must be coded and transmitted for the coding operation. It should be pointed out that the coding problem requires minimization of the predictor size m, whereas the speaker identi cation problem is not normally constrained in the number of parameters, and the identi cation accuracy increases with the model order. There is a considerable body of theoretical and experimental results to indicate that better performance in compression is obtained with a transformation of the predictor A(z) into the Line Spectrum Frequency (LSF) representation [4]. Given the Linear Predictive Coding (LPC) model with coe cients ai, the LSF representation is found by decomposing A(z) into two polynomials P (z) and Q(z), as follows: P (z) Q(z) = A(z) z A(z ) (2) The resulting LSF's are interleaved on the unit circle, with the roots of P (z) corresponding to the oddnumbered indices and the roots of Q(z) corresponding to the even-numbered indices. The quantization properties of the LSF's have been well documented in recent literature [3] [4]. 3. VECTOR QUANTIZATION The coding method examined in this work involves Vector Quantization (VQ) of the LSF's. This method produces very large compression of the short-term spectral information, at the expense of a far more complex vector coding operation and increased distortion. The coding of the LSF's is examined in more detail in [3]. The vector coding of the LSF's reduces, in the simplest case, to determining the optimal index assignment k at time t subject to a distortion criteria: x̂t(k) = argminfD(xt;yi)g 8 yi 2 C (3) where D ( ) represents the distortion criteria, xt is the vector to be encoded at time t, yi is the i candidate vector and C represents the vector codebook. The codebook design must be su ciently robust against all possible permutations of the input vector to ensure adequate coverage of the vector space. Because of the computation and storage requirements necessary for acceptable distortion, a full-search VQ codebook cannot be used. Some method which reduces the computational complexity and storage requirements is normally employed. This comes at the expense of an increased rate and/or distortion [4]. The VQ method employed in this research is the multistage VQ [3]. Thus the single index k in (3) is replaced by a set of indices, one per sub-codebook. 4. SPEAKER IDENTIFICATION Speaker identi cation involves the identi cation of a speaker from the voice alone, using a distance metric. Text-independent identi cation (the focus of this paper) is more di cult than text-dependent speaker identi cation, but has potentially far greater application. Several measures of distance have been proposed in the literature. In this study, we have utilized the Gaussian speaker model, in which a statistical model is constructed for each speaker in the population. This method has been shown to produce near 100% identication accuracy for speech recorded under ideal conditions [2]. The e ect of telephone conditions (bandlimiting, microphone nonlinearity and channel distortions) has been reported elsewhere for very large populations, and was found to be the major determinant of accuracy in speaker identi cation [2]. It is noted that [2] utilized the cepstral coe cients for the identication algorithm { however since the cepstral coe cients are non-invertible they are unsuitable for speech coding. Thus, we utilize the LSF represenation for our identi cation experiments. A benchmark (unquantized model, unquantized input speech) is therefore presented in the Results section of this paper for comparison. 4.1. Gaussian Mixture Model The Gaussian Mixture Model creates a M -order, Dvariate Gaussian model for each reference speaker [7] = fwi; i; ig i = 1; : : : ;M (4) where x is a D-dimensional random vector, bi(x); i = 1; : : : ;M are the component densities and wi are the mixture weights. The resulting probability density is given by"""	16-bit;algorithm;archive;bandlimiting;benchmark (computing);biconnected component;c date and time functions;cd-rom;cepstrum;codebook;computation;computational complexity theory;data compression;data rate units;distortion;experiment;kerrison predictor;lsf;linear predictive coding;microphone;mixture model;multistage amplifier;nonlinear system;open road tolling;pitch (music);polynomial;population;precomputation;pseudo-spectral method;quantization (signal processing);requirement;single-index model;speaker recognition;speech coding;speech processing;statistical model;vector quantization	John Leis;Mark Phythian;Sridha Sridharan	1997		10.1109/ICASSP.1997.598850	data compression;voice activity detection;speaker recognition;codec2;speaker diarisation;linear predictive coding;speech recognition;computer science;speech coding;pattern recognition;gaussian process;speech processing;vector quantization;statistics	ML	49.318942721514645	-10.100373849471378	117512
e7f7876b4d7a3cc2b0c6ed70a723a8432554489f	improved template-matching-based fruc method in inter-frame video coding		As a new inter prediction mode, Frame Rate Up-Conversion (FRUC) is introduced for exploring the future video coding. In FRUC, for a coding unit, the motion information can be derived by the Template Matching (TM) technique. However, in the TM-based FRUC, the prediction direction, including forward, backward, or bi-directional prediction, is determined only based on the initial motion information without further using pixel information of the current block. This may decrease the efficiency of the FRUC mode. To solve this problem, in this paper, three selection schemes on prediction direction are proposed based on template matching distortion and rate distortion cost, respectively. In the first scheme, the relationship in terms of template matching distortion is obtained among different prediction directions. Accordingly, based on this relationship, the prediction direction can be determined both at encoder and decoder, and no additional bit needs to be transmitted to the decoder. In the second scheme, the prediction direction is determined based on rate-distortion optimization, and two additional bits are transmitted to the decoder for each block. Finally, by combining the above two schemes, the third scheme is proposed to further enhance coding efficiency while reducing coding complexity. Our proposed algorithm is integrated into the Joint Exploration Model 5.0.1 platform. Experimental results show that average Bjontegaard delta rate (BD-rate) savings of 0.23% (Y), 0.47% (Y), 0.51% (Y) for three schemes are achieved by using the proposed three schemes, respectively, with no or a little increase of computational cost.	algorithm;algorithmic efficiency;blu-ray;computation;data compression;distortion;encoder;mathematical optimization;newton's method;overhead (computing);pixel;rate–distortion optimization;rate–distortion theory;template matching;transmitter	S. C. Song;Hongzhou Guo;C. Zhu;Y. B. Gao;Y. B. Lin;J. H. Zheng	2018	2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2018.8436775	encoder;computer science;real-time computing;decoding methods;pixel;algorithmic efficiency;inter frame;distortion;frame rate;template matching;artificial intelligence;pattern recognition	Mobile	47.096549063238704	-18.785196658890484	117610
4caf964ea8aa8f09ade349501fc5832958345e20	distortion measures in mpeg-compressed domain for multidimensional transcoding	minimum decoding process;video streaming;data compression;bitstream;decoding;information extraction;distortion measurement;computational method;multidimensional transcoding;discrete cosine transform;dct domain;video streaming data compression decoding discrete cosine transforms feature extraction multidimensional signal processing transcoding video coding;video coding;video transcoding;discrete cosine transforms;distortion measure;feature extraction;mpeg compressed domain video transcoding distortion measure multidimensional transcoding;multidimensional signal processing;mpeg compressed domain distortion measurement computational method information extraction bitstream minimum decoding process dct domain discrete cosine transform multidimensional transcoding;distortion measurement multidimensional systems transcoding spatial resolution discrete cosine transforms bit rate time measurement video compression data mining decoding;transcoding;mpeg compressed domain	In order to find the optimal combination of spatio-SNR-temporal transcoding operations, we need to measure the distortion due to the transcoding operations. In this paper, we develop computational methods to calculate the distortion by using only the information extracted directly from the input bitstream through a minimum decoding process in the DCT domain. The objective of our distortion modeling is to estimate the multidimensional distortion before the entire transcoding process	bitstream;computational model;discrete cosine transform;distortion;signal-to-noise ratio	Yong Ju Jung;Truong Cong Thang;Yong Man Ro	2005	2005 IEEE 7th Workshop on Multimedia Signal Processing	10.1109/MMSP.2005.248590	computer vision;transcoding;computer science;theoretical computer science;multimedia;information extraction;statistics	Visualization	46.68723954424195	-16.69135892093335	117726
aa3e93e56ffd9c0edd39ca13ef0dc252e57b0de8	pyramid vector quantization applied to a video coding scheme for contribution quality	contribution quality;image coding;motion compensation;video coding scheme;hierarchical systems;testing;discrete cosine transform;dct coefficient coding strategies;22 mbit s contribution quality video coding scheme pyramid vector quantization discrete cosine transform dct coefficient coding strategies computational effort;video coding;laplace equations;computational modeling;scalar quantization;vector quantization;video signals computational complexity discrete cosine transforms hierarchical systems image coding vector quantisation;computational complexity;discrete cosine transforms;chromium;computational effort;pyramid vector quantization;vector quantizer;video signals;tv;vector quantisation;proposals;vector quantization video coding discrete cosine transforms proposals laplace equations computational modeling testing tv chromium motion compensation;22 mbit s	A novel effective video coding scheme for contribution quality is presented. Starting from the CMTT/2 proposal, a modified video coding scheme obtained by introducing vector quantization instead of scalar quantization is presented. In particular, pyramid vector quantization (PVQ), able to exploit the Laplacian distribution of discrete cosine transform (DCT) coefficients, has been chosen as the VQ method. Two DCT coefficient coding strategies have been tested, based on stripe and field base, both using an efficient design of vectors to be quantized. Simulations show that the proposed video coding scheme, without introducing considerable computational effort and working at 22 Mbit/s, gives the same contribution quality as the CMTT/2 proposal at 45 Mbit/s. >		Pier Luigi D'Alessandro;P. Formenti;Rosa Lancini	1993		10.1109/ICASSP.1993.319794	computer vision;chromium;computer science;theoretical computer science;discrete cosine transform;mathematics;motion compensation;vector quantization	Vision	45.97393807533993	-16.450243066251762	118181
b641acc4942a7d374ea2046416e6fe9615d172bc	an adaptive multiple reference frame motion estimation for h.264 video coding	early termination;multiple reference frames;full search;reference frame;motion estimation;azb;video coding;selective multiple reference frames motion estimation smrfme;h 264;region based	Previously, an effective algorithm has been proposed to improve the multiple reference frame motion estimation in H.264, based upon correlation among successive frames and neighboring blocks. To improve coding efficiency, in this paper, we extend the previous work and propose an adaptive mechanism which uses motion characteristic to modify our previous work. The experimental results reveal that the proposed algorithm achieves a significant reduction in computation compared to the multiple reference frame full search algorithm, while maintaining a good coding performance.	h.264/mpeg-4 avc;motion estimation;reference frame (video)	Yu-Ming Lee;Wen-Chuan Hsu;Yinyi Lin	2009		10.1007/978-3-642-10467-1_126	reference frame;inter frame;residual frame;computer vision;real-time computing;quarter-pixel motion;computer science;motion estimation;control theory;mathematics;block-matching algorithm;motion compensation	Vision	46.87308287812745	-18.33838368184348	118190
b7b845d7e3f5426ef9d405f70ced51b7b0236cb8	fast cu splitting in hevc intra coding for screen content coding		The high efficiency video coding (HEVC) standard has significantly improved compression performance for many applications, including remote desktop and desktop sharing. Screen content video coding is widely used in applications with a high demand for real-time performance. HEVC usually introduces great computational complexity, which makes fast algorithms necessary to offset the limited computing power of HEVC encoders. In this study, a statistical analysis of several screen content sequences is first performed to better account for the completely different statistics of natural images and videos. Second, a fast coding unit (CU) splitting method is proposed, which aims to reduce HEVC intra coding computational complexity, especially in screen content coding. In the proposed scheme, CU size decision is made by checking the smoothness of the luminance values in every coding tree unit. Experiments demonstrate that in HEVC range extension standard, the proposed scheme can save an average of 29% computational complexity with 0.9% Bjøntegaard Delta rate (BD-rate) increase compared with HM13.0+RExt6.0 anchor for screen content sequences. For default HEVC, the proposed scheme can reduce encoding time by an average of 38% with negligible loss of coding efficiency. key words: HEVC, screen content, intra prediction, fast CU splitting	algorithm;algorithmic efficiency;blu-ray;coding tree unit;computational complexity theory;data compression;desktop computer;desktop sharing;encoder;high efficiency video coding;huffman coding;intra-frame coding;real-time clock;remote desktop software;time complexity;tip (unix utility)	Mengmeng Zhang;Yonghui Zhang;Huihui Bai	2015	IEICE Transactions		computer vision;telecommunications;multimedia	Web+IR	46.052859769542	-19.746541700209466	118196
d13fa1636db22f45773402e78ff5eff7f74cab01	lenselet image compression scheme based on subaperture images streaming	subaperture images streaming plenoptic camera lenselet image light field image compression;rotation scan mapping lenselet image compression scheme subaperture image streaming plenoptic cameras light field reconstruction image rendering 4d light field information;rendering computer graphics data compression image coding;image coding streaming media cameras standards redundancy video coding spatial resolution	Plenoptic cameras capture the light field in a scene with a single shot and produce lenselet images. From a lenselet image, light field can be reconstructed, with which we can render images with different viewpoints and focal length. Because of large volume data, high efficient image compression scheme for storage and transmission is urgent. Containing 4D light field information, lenselet images have much more redundant information than traditional 2D images. In this paper, we propose a subaperture images streaming scheme to compress lenselet images, in which rotation scan mapping is adopted to further improve compression efficiency. The experiment results show our approach can efficient compress the redundancy in lenselet images and outperform traditional image compression method.	focal (programming language);image compression;light field	Feng Dai;Jun Zhang;Yike Ma;Yongdong Zhang	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351705	computer vision;image-based modeling and rendering;image compression;computer graphics (images)	Robotics	43.66441159986016	-19.04232963178564	118309
d32fc51334c642bda33485160a2b268e603f6af7	stochastic extended lqr for optimization-based motion planning under uncertainty	robot sensing systems;stochastic systems gaussian distribution iterative methods linear quadratic control linear systems linearisation techniques mobile robots optimisation path planning;cost function;uncertainty;trajectory;stochastic processes;nonholonomic motion planning motion planning under uncertainty;medical steerable needle stochastic extended lqr optimization based motion planning linear quadratic regulator selqr linear control policy user defined cost function robotic systems stochastic nonlinear dynamics gaussian distributions backward value iteration forward value iteration cost to come cost to go dynamics linearization cost function quadratization iterative approach car like robot quadrotor;planning;robot sensing systems uncertainty cost function stochastic processes planning trajectory	We introduce a novel optimization-based motion planner, Stochastic Extended LQR (SELQR), which computes a trajectory and associated linear control policy with the objective of minimizing the expected value of a user-defined cost function. SELQR applies to robotic systems that have stochastic non-linear dynamics with motion uncertainty modeled by Gaussian distributions that can be state- and control-dependent. In each iteration, SELQR uses a combination of forward and backward value iteration to estimate the cost-to-come and the cost-to-go for each state along a trajectory. SELQR then locally optimizes each state along the trajectory at each iteration to minimize the expected total cost, which results in smoothed states that are used for dynamics linearization and cost function quadratization. SELQR progressively improves the approximation of the expected total cost, resulting in higher quality plans. For applications with imperfect sensing, we extend SELQR to plan in the robot's belief space. We show that our iterative approach achieves fast and reliable convergence to high-quality plans in multiple simulated scenarios involving a car-like robot, a quadrotor, and a medical steerable needle performing a liver biopsy procedure.		Wen Sun;Jur P. van den Berg;Ron Alterovitz	2016	IEEE transactions on automation science and engineering : a publication of the IEEE Robotics and Automation Society	10.1109/TASE.2016.2517124	planning;control engineering;stochastic process;mathematical optimization;uncertainty;trajectory;control theory;mathematics;statistics	Robotics	51.86212610119623	-22.18130029480362	118620
afe27bd78db980c66f95fb5cd2693abd9031d0a3	fast video transcoding from h.263 to h.264/mpeg-4 avc	full search;search algorithm;low complexity;video transcoding;h 264 mpeg 4 avc;low complexity coding;h 263	In the past 10 years detailed works on different video transcoders have been published. However, the new ITU-T Recommendation H.264—also adapted as ISO/IEC MPEG-4 Part 10 (AVC)—provides many new encoding options for the prediction processes that lead to difficulties for low complexity transcoding. In this work we present very fast transcoding techniques to convert H.263 bitstreams into H.264/AVC bitstreams. We will give reasoning, why the proposed pixel domain approach is advantageous in this scenario instead of using a DCT domain transcoder. Our approach results in less than 9% higher data rate at equivalent PSNR quality compared to a full-search approach. But this rate loss allows the reduction of the search complexity by a factor of over 200 for inter frames and still a reduction of over 70% for intra frames. A comparison to a fast search algorithm is given. We also provide simulation results that our algorithm works for transcoding MPEG-2 to H.264/AVC in the aimed scenario.	bitstream;computational complexity theory;data rate units;discrete cosine transform;encoder;experiment;frame language;framing (world wide web);h.264/mpeg-4 avc;jumbo frame;mpeg-2;megabyte;peak signal-to-noise ratio;pixel;reduction (complexity);refinement (computing);ruby document format;search algorithm;simulation;uncompressed video;unsupervised learning	Jens Bialkowski;Marcus Barkowsky;André Kaup	2007	Multimedia Tools and Applications	10.1007/s11042-007-0126-7	scalable video coding;real-time computing;transcoding;h.263;computer science;theoretical computer science;operating system;multimedia;search algorithm	Vision	46.074623557109184	-18.795637570224283	118642
6e7a5017da8184307f7d59cbba55d6473d7b6ab7	a hybrid bmc/obmc motion compensation scheme	motion compensation;decoding;video coding motion compensation computational complexity search problems image sequences decoding;motion compensation bit rate psnr smoothing methods decoding;overlapped block motion compensation;visual quality;motion compensated;video coding;computational complexity;search problems;decoder hybrid bmc obmc motion compensation scheme block motion compensation overlapped block motion compensation bit rate search complexity encoder block mode visual quality motion bit rate smoothing motion field;image sequences	In this paper, we propose a hybrid scheme which incorporates the block motion compensation (BMC) scheme in the overlapped block motion compensation (OBMC) scheme. The main objective of this hybrid scheme is to improve a pure OBMC system with a lower motion bit rate and a lower search complexity while retaining about the same PSNR level. With this scheme, the encoder can determine the block mode under the tradeoff of complexity and visual quality. Furthermore, the motion bit rate can be reduced by smoothing the motion field of BMC blocks during motion search. Experimental results show that the complexity of both the encoder and the decoder is reduced while the visual quality remains as good as the pure OBMC system.	intelligent platform management interface;motion compensation	Tien-Ying Kuo;C.-C. Jay Kuo	1997		10.1109/ICIP.1997.638616	computer vision;real-time computing;quarter-pixel motion;computer science;motion estimation;computational complexity theory;motion compensation;algorithm	Vision	46.76065012289297	-18.237039333625507	118916
32c75a23f2b8b4778f1bd063deac27a97fdd8325	fast search method for image vector quantization based on equal-average equal-variance and partial sum concept	vector feature estimation;image coding;eeenns search method;computational redundancy;partial sums;search space;search method;euclidean distance;mathematical analysis;search methods vector quantization image coding euclidean distance distortion measurement decoding electronics industry industrial electronics electronic mail mathematical analysis;equal average equal variance;vq;feature extraction;computational redundancy image vector quantization vq equal average equal variance partial sum concept encoding process k dimensional euclidean distance vector feature estimation eeenns search method;image vector quantization;vector quantizer;k dimensional euclidean distance;partial sum concept;vector quantisation;encoding process;vector quantisation feature extraction image coding	The encoding process of image vector quantization (VQ) is very heavy due to it performing a lot of k-dimensional Euclidean distance computations. In order to speed up VQ encoding, it is most important to avoid unnecessary exact Euclidean distance computations as many as possible by using features of a vector to estimate how large it is first so as to reject most of unlikely codewords. The mean, the variance, L 2 norm and partial sum of a vector have been proposed as effective features in previous works for fast VQ encoding. Recently, in the previous work (Z. Lu et al., 2003), three features of the mean, the variance and L2 norm are used together to derive an EEENNS search method, which is very search efficient but still has obvious computational redundancy. This paper aims at modifying the results of EEENNS method further by introducing another feature of partial sum to replace L2 norm feature so as to reduce more search space. Mathematical analysis and experimental results confirmed that the proposed method is more search efficient compared to (Z. Lu et al., 2003)	code word;computation;euclidean distance;lu decomposition;vector quantization	Zhibin Pan;Koji Kotani;Tadahiro Ohmi	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521702	mathematical optimization;discrete mathematics;feature extraction;pattern recognition;euclidean distance;mathematics;series	Robotics	43.61018655509275	-13.207891530032782	119124
17fecf14b34ee00a7d52a81955e75b6937b7610e	comparison between multiple description and single description video coding with forward error correction	channel coding;single description scheme;packet loss probability;video streaming;probability;fec;error concealment capability;channel scenario;packet radio networks;code standards;error concealment capability video streaming multiple description scheme spatial polyphase downsampling single description scheme forward error correcting codes fec channel scenario packet loss probability wireless communication system h 264 avc video coding standard;video coding;forward error correction;video coding forward error correction streaming media packet switching delay scalability fault tolerance performance loss robustness error correction codes;spatial polyphase downsampling;forward error correcting codes;h 264 avc video coding standard;multiple description scheme;wireless communication system;video streaming channel coding code standards forward error correction packet radio networks probability video coding	Video streaming over packet switched best-effort networks is a challenging topic, due to low latency, scalability and fault tolerance requirements. Many techniques can be used to deal with delay, loss and the time-varying nature of best-effort networks. In this paper we compare two techniques to improve the performance of video streaming, i.e., a multiple description (MD) scheme based on spatial polyphase downsampling, and a single description (SD) scheme where robustness to packet loss is increased using forward error correcting (FEC) codes. We consider both a single channel scenario and a multiple channel (or multi-path) scenario. We span a large set of channel conditions, to consider the high packet loss probabilities common in wireless communication systems. A H.264/AVC video coding standard with advanced error concealment capabilities is used. Experimental results show that MD can be competitive in practical scenarios with more flexibility and less complexity than the SD+FEC scheme	best-effort delivery;code;data compression;decimation (signal processing);error concealment;fault tolerance;forward error correction;h.264/mpeg-4 avc;network packet;packet switching;polyphase quadrature filter;requirement;scalability;streaming media;video coding format	Riccardo Bernardini;Marco Durigon;Roberto Rinaldo;Andrea Vitali	2005	2005 IEEE 7th Workshop on Multimedia Signal Processing	10.1109/MMSP.2005.248546	real-time computing;channel code;telecommunications;computer science;probability;forward error correction;statistics;computer network	Mobile	48.36809351367738	-16.12475621613716	119367
5f9e8301430b802f3e7c02af559e02c51869b86d	window-level rate control for smooth picture quality and smooth buffer occupancy	domain model;storage allocation;rate distortion;r qp decision model;psnr;bit rate psnr decoding computers constraint theory streaming media channel capacity jitter size control quadratic programming;rho domain model;buffer storage;window level rate control algorithm;transform coding;bit rate;data mining;rho domain model window level rate control algorithm smooth moving picture quality smooth buffer occupancy bit allocation r qp decision model buffer overflow encoding parameter video coding window level rate distortion model;rate distortion theory;rate control;video coding;adaptation model;smooth buffer occupancy;ρ domain;buffer occupancy rate control smooth picture quality ρ domain window;buffer overflow;discrete cosine transforms;decision theory;video coding buffer storage decision theory rate distortion theory storage allocation;bit allocation;smooth moving picture quality;encoding parameter;encoding;window;smooth picture quality;window level rate distortion model;buffer occupancy	Traditionally, rate control consists of bit allocation and QP decision on R-QP model. In bit allocation, target bits is further confined if buffer overflows. Meanwhile, rate control should also give as smooth as possible picture quality. However, there is no explicit relationship between picture quality and encoding parameters, so the coding result on picture quality is usually unpredictable and uncontrollable. On the condition of smooth picture quality, the smooth buffer occupancy is preferable certainly. In our work, we first proposed a “window model” formulating the size of window and variations of picture quality and buffer occupancy. Thus, given the constraint on picture quality and buffer occupancy, the compliant coding result about them can be expected employing window model. Second, a window-level rate distortion (RD) model inspired by the traditional ρ-domain model is introduced. Lastly, the evaluation of our proposal is presented with elaborate experiments.	buffer overflow;distortion;experiment;image quality;rate–distortion theory;ruby document format;window function	Long Xu;Zhihang Wang;Lei Lei Deng;Xiangyang Ji;Debin Zhao;Wen Gao	2009	2009 Picture Coding Symposium	10.1109/PCS.2009.5167420	real-time computing;transform coding;simulation;rate–distortion theory;decision theory;peak signal-to-noise ratio;telecommunications;buffer overflow;computer science;domain model;mathematics;encoding;statistics	Theory	47.83948055079552	-16.99427988624409	119941
2aa9a131a0ca6c2e81bfbf1eb417ea94116d557e	traffic network control from temporal logic specifications	vehicles vehicle dynamics roads trajectory throughput indexes;temporal logic road traffic;indexes;trajectory;continuous state space traffic network control temporal logic specifications linear temporal logic formal methods correct by construction controller complex specifications finite state abstraction;roads;vehicles;vehicle dynamics;throughput	We propose a framework for generating a signal control policy for a traffic network of signalized intersections to accomplish control objectives expressible using linear temporal logic. By applying techniques from model checking and formal methods, we obtain a correct-by-construction controller that is guaranteed to satisfy complex specifications. To apply these tools, we identify and exploit structural properties particular to traffic networks that allow for efficient computation of a finite-state abstraction. In particular, traffic networks exhibit a componentwise monotonicity property which enables reaching set computations that scale linearly with the dimension of the continuous state space.	computation;control theory;formal methods;linear temporal logic;model checking;sparse matrix;state space;towns	Samuel Coogan;Ebru Aydin Gol;Murat Arcak;Calin Belta	2016	IEEE Transactions on Control of Network Systems	10.1109/TCNS.2015.2428471	control engineering;database index;throughput;real-time computing;vehicle dynamics;simulation;computation tree logic;computer science;trajectory;computer network	Embedded	52.881430001359504	-20.388727384292995	120083
3f1faf668f8dbb4ee6e0c824877d4afb2da2c57d	integrating risk in humanoid robot control for applications in the nuclear industry		This paper discuss the integration of risk into a robot control framework for decommissioning applications in the nuclear industry. Our overall objective is to allow the robot to evaluate a risk associated with several methods of completing the same task by combining a set of action sequences. If the environment is known and in the absence of sensing errors each set of actions would successfully complete the task. In this paper, instead of attempting to model the errors associated with each sensing system in order to compute an exact solution, a set of solutions are obtained along with a prescribed risk index. The risk associated with each set of actions can then be compared to possible payoffs or rewards, for instance task completion time or power consumption. This information is then sent to a high level decision planner, for instance a human teleoperator, who can then make a more informed decision regarding the robots actions. In order to illustrate the concept, we introduce three specific risk measures, namely, the collision risk and the risk of toppling and failure risk associated with grasping an object. We demonstrate the results from this foundational study of risk-aware compositional robot autonomy in simulation using NASA’s Valkyrie humanoid robot, and the grasping simulator HAPTIX.	approximation error;experiment;high-level programming language;humanoid robot;loss function;machine learning;reversing: secrets of reverse engineering;risk assessment;risk measure;riskmetrics;robot control;robotics;simulation;symbolic computation;telerobotics;text simplification;utility;velocity (software development);weight function	Xianchao Long;Philip Long;Aykut Özgün Önol;Taskin Padir	2018	CoRR		humanoid robot;collision;engineering;simulation;robot control;robot;specific risk	Robotics	52.63724724087027	-21.283487059300455	120651
a97382df1cbe49bde24ad8d0f197891af938ac26	highly scalable and perceptually tuned embedded subband/wavelet image coding	wavelets;image compression	Former research on perceptual image coding was mainly developed in the traditional sequential coding framework, where the codestream is neither rate nor resolution scalable. In this paper, our earlier embedded subband/wavelet image coding algorithm EZBC is further developed for highly scalable image coding applications. Special attention is given to perceptual image coding under varying viewing/display conditions --- a common situation in typical scalable coding application environments. Unlike the conventional perceptual image coding approach, all the perceptually coded images (individually targeted at particular viewing conditions) are decoded from a single compressed bitstream file. The experimental results show the bitrate savings by the proposed algorithm are significant, particularly for coding of high-definition (HD) images.	embedded system;scalability;wavelet	Shih-Ta Hsiang;John W. Woods	2002			sub-band coding;computer vision;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;tunstall coding;multimedia;context-adaptive binary arithmetic coding	EDA	44.19444355139826	-17.22001840252272	120724
65ebdeb21fb7d0d4721b45df9e179a7ddd32d6a5	extended color cell compression - a runtime-effient compression scheme for software video	004 informatik	Multimedia applications require a compression and decompression scheme for digital video. The standardized and widely used techniques JPEG and MPEG provide very good compression ratios, but are computationally quite complex and demanding. We propose to use an extension to the much simpler Color Cell Compression scheme as an alternative. Our extension includes the use of variable block sizes, the reuse of color index values from previously encoded blocks, and Huffman encoding of the stream of blocks. We present experimental results showing that our scheme provides much better runtime performance than MPEG, at the cost of a slightly inferior compression ratio. It is thus especially suited for software videos in high-speed networks.		Bernd Lamparter;Wolfgang Effelsberg	1994		10.1007/3-540-58494-3_16	video compression picture types;data compression;s3 texture compression;data compression ratio;real-time computing;block truncation coding;computer science;theoretical computer science;jpeg;lossless compression;texture compression;computer graphics (images)	Vision	44.79546281847866	-19.382273704393025	120779
9ebe41baf6fee479ac66daabc0780fa8be75a586	a new error-mapping scheme for scalable audio coding	mushra error mapping scheme scalable audio coding audio coders mpeg 4 sls quantization errors bit plane coding computational complexity;decoding transform coding audio coding quantization signal histograms conferences;quantisation signal audio coding codecs computational complexity	In scalable audio coders, such as the MPEG-4 SLS, error-mapping is used to map quantization errors in the core coder to an error signal before passing through bit-plane coding. In this paper, we propose a new error-mapping scheme that is derived by observing statistical properties of the error signal. Compared with the error-mapping in SLS, the proposed scheme improves coding efficiency as well as computational complexity of the coder. An average improvement of 9 points in MUSHRA score has been achieved by the proposed scheme in subjective listening tests. The proposed error-mapping adds a useful new tool to the existing toolset for constructing next-generation scalable audio coders.	algorithmic efficiency;bit plane;computational complexity theory;mushra;scalability;standard sea level;structured-light 3d scanner	Haibin Huang;Susanto Rahardja	2014	2014 IEEE 16th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2014.6958815	sub-band coding;adaptive multi-rate audio codec;speech recognition;shannon–fano coding;mpeg-4 part 3;harmonic vector excitation coding;variable-length code;theoretical computer science;audio bit depth;speech coding;coding tree unit;tunstall coding	Mobile	44.35990477941472	-17.424437117622556	120797
7a7737d97cb681d3c7c04b80d8764f60aea9d532	quality evaluation of mpeg-4 and h.26l coded video for mobile multimedia communications	mpeg 4 coded video;perceptual quality;mobile multimedia;edge detection;h 26l coded video;qcif test data quality evaluation moving picture experts group mpeg 4 coded video h 26l coded video mobile multimedia communication visual signal perceptual distortion computing image ringing detection damaged edge detection image blockiness detection cif test data;video sequences;testing;distortion measurement;bit rate;mobile multimedia communication;video coding;image blockiness detection;mpeg 4 standard;mpeg 4 standard multimedia communication mobile communication image edge detection testing bit rate humans distortion measurement bandwidth video sequences;image edge detection;quality evaluation;moving picture experts group;multimedia communication;mobile communication;image ringing detection;cif test data;bandwidth;visual perception;humans;visual signal perceptual distortion computing;qcif test data;edge detection video coding mobile communication multimedia communication visual perception distortion measurement;human perception;damaged edge detection	This paper proposes a new approach for computing perceptual distortion for visual signal in order to provide an objective measure for perceptual quality at low bit rate MPEG-4 and H.26L coding in typically mobile multimedia communications. The regions with three major perceptually disturbing artefacts, namely, damaged edge, blockiness and ringing, are detected as the basis of assessment. The correlation of the metric with human perception has been demonstrated with low bit rate (24~384 Kbps) CIF and QCIF test data.	color vision;data rate units;distortion;experiment;h.264/mpeg-4 avc;neural coding;peak signal-to-noise ratio;ringing (signal);sensor;server message block;test data	Ee Ping Ong;Weisi Lin;Zhongkang Lu;Xiaokang Yang;Susu Yao;Xiao Lin;Fulvio Moschetti	2003		10.1109/ISSPA.2003.1224742	computer vision;speech recognition;edge detection;mobile telephony;visual perception;computer science;multimedia;software testing;perception;bandwidth	Mobile	44.896286660719895	-21.440972207720755	121177
06d9d25dc93b5d333c249904d6d5e623aa8966e9	live capture, rectification, and streaming of stereoscopic internet video for casual users	3d video coding;video streaming;digital tv;usa councils;usa councils abstracts streaming media stereo image processing digital tv;rectification;internet;streaming media;streaming media stereo video h 264 avc sei messages 3d video coding;abstracts;graphics processing units;video streaming graphics processing units internet rectification stereo image processing;stereo image processing;h 264 avc sei messages;stereo video;gpu rendering hardware live capture streaming stereoscopic internet video casual users h 264 avc supplemental enhancement information messages real time rectification show objective quality metrics psnr mssim common test sequences side by side frame compatible video	We investigate stereoscopic 3D (S3D) video using H.264/AVC supplemental enhancement information messages, real-time rectification, and show objective quality metrics PSNR and MSSIM for depth adjusted and rectified common test sequences converted to side-by-side frame compatible video. We use common computer vision techniques to estimate scene disparity and apply texture warping to vertically align images. The S3D system can capture live video from either two commodity webcams or an inexpensive stereo webcam. Two video streams are captured, rectified, encoded as H.264 video and transmitted to a media server. Clients connect to the media server to receive the H.264 encoded S3D video. A client internet browser-plugin decodes the S3D video as either 2D or S3D if the client has GPU rendering hardware.	align (company);binocular disparity;computer vision;graphics processing unit;h.264/mpeg-4 avc;image rectification;media server;peak signal-to-noise ratio;real-time clock;rectifier;server (computing);stereoscopic video game;stereoscopy;streaming media;webcam	M. Scott Bishop;Hyojin Kim;Viswanathan Swaminathan	2012	2012 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2012.6365459	video compression picture types;microsoft video 1;computer vision;video;video production;computer science;video quality;video capture;video tracking;multimedia;video processing;motion compensation;video post-processing;multiview video coding;computer graphics (images)	Vision	43.08848070128307	-21.36039950595725	121253
d33bc55da77812fb80bb97cff7b8e4fdc1b7ccaa	methodologies used for evaluation of video tools and algorithms in mpeg-4	motion estimation;video coding;error resilience;verification model	MPEG-4 issued two calls for proposals requesting submission of algorithms and tools relevant to standardization of MPEG-4. This paper reports on the evaluation of tools submitted for evaluation in November 1995 and January 1996. Complete video coding schemes submitted in January 1996 are also covered. The goal of the evaluation was to cluster the tools according to the technical areas they address, to evaluate them according to the issues relevant to the standardization process, and finally to suggest areas of core experiments to improve a video verification model (VM) as soon as the VM becomes available. Altogether, MPEG evaluated 87 tools and 19 complete coding algorithms, most of them highlighted in this paper. During the evaluation, 19 areas for core experiments were identified. Each core experiment is targeted at different functionalities like compression efficiency, content-based coding, error resilience, scalability. This definition of core experiments caused close collaboration and supported mutual fertilization between organizations working on similar tools, which allowed the VM to progress much faster than expected.	algorithm;data compression;experiment;moving picture experts group;scalability	Jörn Ostermann	1997	Sig. Proc.: Image Comm.	10.1016/S0923-5965(97)00006-4	computer vision;real-time computing;simulation;telecommunications;computer science;motion estimation;data mining	Web+IR	42.32098993427586	-20.365179603145656	121654
19105e4bf19756049d97ba4d342281c4d71293a8	complete-to-overcomplete discrete wavelet transforms: theory and applications	image sampling;estensibilidad;discrete wavelet transforms;traitement signal;multirate system;filtering;filtrage;image coding;discrete wavelet transform;shift invariance complexity reduction overcomplete discrete wavelet transforms scalable image and video coding;image processing;ucl;image resolution;image coding complete to overcomplete discrete wavelet transform complete signal representation lowband shift algorithm signal downsampling prediction filter matrix signal filtering image resolution video codec coding technique complexity reduction video coding;lowband shift algorithm;video signal processing;coding technique;signal entree;filtrado;prediction filter matrix;subband decomposition;procesamiento imagen;invarianza;discovery;overcomplete discrete wavelet transforms;transformation ondelette discrete;theses;conference proceedings;matrix algebra;indexing terms;qualite image;traitement image;video codec;systeme multicadence;signal filtering;algorithme;invariance;algorithm;input signal;codage image;video coding;senal entrada;resolucion imagen;digital web resources;shift invariance;complete signal representation;codage video;complexity reduction;descomposicion subbanda;discrete transforms;computational complexity;ucl discovery;signal downsampling;image representation;image reconstruction;signal processing;image quality;open access;traitement signal video;signal resolution;arithmetic;codec;scalable image and video coding;video codecs;complete to overcomplete discrete wavelet transform;ucl library;calidad imagen;temps retard;extensibilite;scalability;book chapters;delay time;open access repository;matrix algebra video coding discrete wavelet transforms image representation image reconstruction image sampling filtering theory computational complexity;decomposition sous bande;discrete wavelet transforms image reconstruction image resolution delay video coding filtering signal resolution video codecs discrete transforms arithmetic;procesamiento senal;tiempo retardo	A new transform is proposed that derives the overcomplete discrete wavelet transform (ODWT) subbands from the critically sampled DWT subbands (complete representation). This complete-to-overcomplete DWT (CODWT) has certain advantages in comparison to the conventional approach that performs the inverse DWT to reconstruct the input signal, followed by the a/spl grave/-trous or the lowband shift algorithm. Specifically, the computation of the input signal is not required. As a result, the minimum number of downsampling operations is performed and the use of upsampling is avoided. The proposed CODWT computes the ODWT subbands by using a set of prediction-filter matrices and filtering-and-downsampling operators applied to the DWT. This formulation demonstrates a clear separation between the single-rate and multirate components of the transform. This can be especially significant when the CODWT is used in resource-constrained environments, such as resolution-scalable image and video codecs. To illustrate the applicability of the proposed transform in these emerging applications, a new scheme for the transform-calculation is proposed, and existing coding techniques that benefit from its usage are surveyed. The analysis of the proposed CODWT in terms of arithmetic complexity and delay reveals significant gains as compared with the conventional approach.	algorithm;codec;computation;data compression;decimation (signal processing);discrete wavelet transform;emoticon;scalability;upsampling	Yiannis Andreopoulos;Adrian Munteanu;Geert Van Der Auwera;Jan Cornelis;Peter Schelkens	2005	IEEE Transactions on Signal Processing	10.1109/TSP.2005.843707	filter;computer vision;speech recognition;second-generation wavelet transform;image processing;computer science;theoretical computer science;signal processing;mathematics;stationary wavelet transform	Visualization	45.89026123053625	-15.16511752084314	121655
26f05b5163e4fc1f4092d680c9c2bc25a715d736	cielab-driven adaptive quantization scheme for dct-based compression of cmyk images	representation;metodo correlacion;compression algorithm;quantization;cuantificacion;image processing;data compression;color space;redundancia;lossy compression;correlation method;procesamiento imagen;quantification;systeme adaptatif;systeme numerique;traitement image;digital system;redundancy;technology and engineering;adaptive system;estimacion parametro;quantization parameter;sistema numerico;sistema adaptativo;algorithms;compresion dato;parameter estimation;estimation parametre;imagen color;image couleur;compression donnee;redondance;methode correlation;color image;representacion	In todays digital prepress workflow images are most often sorted in the CMYK color representation. In the lossy compression of CMYK color imags, most techniques do not take the tonal correlation between the color channels into account or they are not able to perform a proper color decorrelation in four dimensions. In a first stage a compression method has been developed that takes this type of redundancy into account. The basic idea is to divide the image into blocks. The color information in those blocks is then transformed from the original CMYK color space into a decorrelated color space. In this new color space not all components are of the same magnitude so here the gain for compression purposes becomes clear. After the color transformation step any regular compression scheme meant to reduce the spatial redundancy can be applied. In this paper a more advanced approach for the utilization procedure in the compression algorithm is presented. The proposed scheme tries to control the quantization parameters differently for all blocks and color components. Therefore the influence on the CIELab (Delta) E measure is investigated when making a small shift in the four main directions of the decorrelated color space.© (1999) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	discrete cosine transform;quantization (signal processing)	Peter De Neve;Koen N. Denecker;Ignace Lemahieu	1999		10.1117/12.348433	color cell compression;computer vision;electronic engineering;color quantization;color depth;high color;mathematics;color balance;color space;computer graphics (images)	Vision	45.75299266856282	-13.731233017954152	122445
73ff32ad0caa685243be8ed926d81b766412df74	optimal pyramidal and subband decompositions for hierarchical coding of noisy and quantized images	quantization;scalable image transmission;cuantificacion;image coding;filter bank;image processing;quantized signal;image resolution;progressive image transmission;quantized images;performance optimal subband decompositions optimal pyramidal decompositions hierarchical coding noisy images quantized images progressive image transmission scalable image transmission error difference corruption analysis filters optimal synthesis filters globally optimal structures image decompositions implementation;band pass filters;banc filtre;analysis filters;image communication;implementation;hierarchical coding;visual communication;performance;subband decomposition;filters;optimal synthesis filters;compresion senal;procesamiento imagen;noisy images;senal cuantificada;indexing terms;quantification;analyse multiresolution;image bruitee;traitement image;compression signal;signal quantifie;imagen sonora;globally optimal structures;corruption;image coding filters signal resolution image resolution quantization image reconstruction image communication statistical analysis image analysis image decomposition;statistical analysis;image transmission;descomposicion subbanda;optimal subband decompositions;image representation;image reconstruction;image resolution visual communication noise image coding band pass filters image representation;banco filtro;noisy image;error difference;signal compression;signal resolution;optimal pyramidal decompositions;image analysis;global optimization;decomposition sous bande;image decomposition;multiresolution analysis;analisis multiresolucion;noise;image decompositions	Optimal hierarchical coding is sought, for progressive or scalable image transmission, by minimizing the variance of the error difference between the original image and its lower resolution renditions. The optimal, according to the above criterion, pyramidal and subband image coders are determined for images subject to corruption by quantization or transmission noise. Given arbitrary analysis filters and assuming adequate knowledge of the noise statistics, optimal synthesis filters are found. The optimal analysis filters are subsequently determined, leading to formulas for globally optimal structures for pyramidal and subband image decompositions. Experimental results illustrate the implementation and performance of the optimal coders.	maxima and minima;quantization (signal processing);sample variance;scalability	Michael G. Strintzis	1998	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.660993	iterative reconstruction;multiresolution analysis;computer vision;image analysis;speech recognition;corruption;index term;image resolution;quantization;performance;image processing;computer science;noise;theoretical computer science;filter bank;mathematics;band-pass filter;implementation;visual communication;global optimization	Vision	46.84942911600547	-13.00689177417103	122491
711b5d023c85cdd89c920e9733b3de718ca8ee60	efficient motion re-estimation method based on k-means clustering for spatial resolution reduction transcoding	rate distortion;estimation method;transcoding spatial resolution vectors estimation complexity theory decoding;reference frame;low complexity;motion estimation;video coding;motion vector;video coding motion estimation transcoding;transcoding;video transcoding efficient motion re estimation method spatial resolution reduction transcoding architecture h 264 avc low complexity compensation technique k means clustering algorithm reference frame motion vector refinement technique rate distortion performance motion vector estimation complexity rate distortion performance bd psnr bd rate;k means clustering;spatial resolution	In this paper, we present a spatial resolution reduction transcoding architecture for H.264/AVC with a low-complexity compensation technique. A fast motion vector re-estimation based on a k-means clustering algorithm is proposed for efficiently determining a motion vector and a reference frame. A motion vector refinement technique is used to further improve the rate-distortion performance without additional high complexity. The proposed method significantly reduces the motion vector estimation complexity up to 10 times while the loss of rate distortion performance is negligible with a decrease of 0.1dB in BD-PSNR and 3% increase in the BD-RATE.	algorithm;blu-ray;cluster analysis;distortion;h.264/mpeg-4 avc;k-means clustering;motion estimation;peak signal-to-noise ratio;rate–distortion theory;reference frame (video);refinement (computing)	Kyounghwan Kim;Soongi Hong;Yoonsik Choe	2012	2012 Picture Coding Symposium	10.1109/PCS.2012.6213332	reference frame;computer vision;transcoding;image resolution;quarter-pixel motion;computer science;theoretical computer science;motion estimation;block-matching algorithm;motion compensation;k-means clustering;computer graphics (images)	EDA	46.329778774420106	-18.29761165158471	122692
d26308f13f5ecbdea747cf18063603c497b70f95	a fast inter-frame encoding scheme using the edge information and the spatiotemporal encoding parameters for hevc		High Efficiency Video Coding (HEVC), as a novel video coding standard, has shown a better coding efficiency than all existing standards, such as H.264/AVC. It adopts a lot of new efficient coding tools, the most important one is the new hierarchical structures which include the coding unit (CU), prediction unit (PU) and transform unit (TU). However, the rate-distortion (RD) optimization process for all CUs, PUs, and TUs cause large computational costs. In this paper, a fast Inter-frame encoding scheme using the edge information and the spatiotemporal encoding parameters is proposed to reduce the encoder complexity of HEVC, which consists of a fast all 2 N × 2 N modes decision method, a fast CU depth level decision method and a fast PU mode decision method. This scheme uses edge information to express the structure complexity and uses the difference of edge information between current CU and its spatiotemporal CUs to express the edge similarity (ES) in one frame and the edge movement (EM) between two adjacent frames. And then, utilizes ES and EM as assistant parameters cooperate with CU depth levels and PU mode RD costs of spatiotemporal CUs to accomplish the early termination of CU split and the PU mode selection. The experimental results show that the proposed fast inter-frame encoding method can significantly reduce the computational costs with negligible RD loss. There are 53.7 and 54.9% encoding time savings on average, but only with average 1.5 and 1.8% Bjøntegaard difference bitrate (BDBR) losses for various test sequences under random access and low delay conditions, respectively.	algorithmic efficiency;computation;data compression;decibel;distortion;encoder;gradient;h.264/mpeg-4 avc;high efficiency video coding;ibm systems network architecture;line code;mathematical optimization;one-class classification;polynomial-time approximation scheme;random access;ruby document format;tip (unix utility);traffic collision avoidance system;video coding format	Zhiyao Yang;Shuxu Guo;Qinglong Shao	2016	Multimedia Tools and Applications	10.1007/s11042-016-4165-9	real-time computing;simulation;computer science	AI	46.94647350864661	-19.484783480303392	122697
5a1eef809c5697bdcc463215037480e1933b24b8	video quality evaluation for mobile streaming applications	video;jpeg2000;real time;video quality	ABSTRACT This paper presents the results of a quality evaluation of video sequences encoded for and transmitted over a wireless channel We selected content, codecs, bitrates and bit error patterns representative of mobile applica - tions, focusing on the MPEG - 4 and Motion JPEG2000 coding standards We carried out subjective experiments using the Single Stimulus Continuous Quality Evaluation (SSCQE) method on this test material We analyze the  subjective  data  and  use  them  to  compare  codec  performance  as  well  as  the  e ects  of  transmission  errors on  visual  quality Finally,  we  use  the  subjective  ratings  to  validate  the  prediction  performance  of  a  real - time non - reference  quality  metric		Stefan Winkler;Frédéric Dufaux	2003			subjective video quality;simulation;video;telecommunications;computer science;video quality;machine learning;jpeg 2000;multimedia;pevq	Arch	45.35154892150486	-21.402966966558758	123185
4f93f1e6bd08ebad36bbf548d485d624bf9c048e	adaptive particle-distortion tradeoff control in particle filtering for video tracking	video signal processing;video tracking;tracking filters;indexing terms;rate distortion theory;optimal control;distortion;particle filter;tracking filters distortion optimal control;video signal processing filtering theory rate distortion theory tracking filters;programmable control adaptive control adaptive filters filtering particle tracking particle filters distortion measurement rate distortion theory differential equations constraint theory;video tracking system adaptive particle distortion tradeoff control particle filtering rate distortion theory particle number allocation equation;system simulation;filtering theory	This paper presents a novel approach to particle filtering which minimizes the total tracking distortion by considering dynamic variance of proposal density and optimal number of particles for each frame. Traditionally, particle filters use fixed variance of proposal density and fixed number of particles per frame. We propose a tracking distortion measurement and use rate distortion theory to obtain the optimal memory size and particle number allocation equations under different two constraints. We subsequently propose the dynamic proposal variance and optimal particle number allocation algorithm for video tracking systems. Simulation results show the improved performance of our proposed algorithm in comparison to traditional particle filters. To the best of our knowledge, our approach is the first to consider variant numbers of particles for each frame.	algorithm;distortion;particle filter;rate–distortion theory;simulation;tracking system;video tracking	Pan Pan;Dan Schonfeld	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312398	computer vision;mathematical optimization;index term;optimal control;distortion;rate–distortion theory;particle filter;computer science;video tracking;control theory;mathematics;statistics	Vision	47.91800753588161	-17.322762306399238	123250
047f07a08cd25e2c998c790d2abc823e04a9372d	texture refinement framework for improved video coding	artefacto;texture;circuit codeur;eye;coding circuit;algorithm analysis;video signal processing;debit information;redundancia;information transmission;indice informacion;artefact;algorithme;algorithm;refinement method;video coding;dictionnaire;redundancy;codage video;percepcion visual;circuito codificacion;dictionaries;textura;traitement signal video;perception visuelle;information rate;associative arrays;visual perception;analyse algorithme;transmision informacion;methode raffinement;transmission information;metodo afinamiento;diccionario;analisis algoritmo;redondance;algoritmo	H.264/AVC standard offers an efficient way of reducing the noticeable artefacts of former video coding schemes, but it can be perfectible for the coding of detailed texture areas. This paper presents a conceptual coding framework, utilizing visual perception redundancy, which aims at improving both bit-rate and quality on textured areas. The approach is generic and can be integrated into usual coding scheme. The proposed scheme is divided into three steps: a first algorithm analyses texture regions, with an eye to build a dictionary of the most representative texture sub-regions (RTS). The encoder preserves then them at a higher quality than the rest of the picture, in order to enable a refinement algorithm to finally spread the preserved information over textured areas. In this paper, we present a first solution to validate the framework, detailing then the encoder side in order to define a simple method for dictionary building. The proposed H.264/AVC compliant scheme creates a dictionary of macroblocks	algorithm;codec;cross-correlation;data compression;dictionary;encoder;h.264/mpeg-4 avc;macroblock;pixel;refinement (computing);texture synthesis	Fabien Racapé;Marie Babel;Olivier Déforges;Dominique Thoreau;Jérôme Viéron;Edouard François	2010		10.1117/12.838987	computer vision;visual perception;computer science;artificial intelligence;context-adaptive variable-length coding;texture;redundancy;context-adaptive binary arithmetic coding;algorithm;associative array	Vision	46.400952007581076	-14.215335083296626	123287
253fd1d93543e41c0421eac34b7f7d52df9e5b04	discrete fourier transformation based image authentication technique	discrete fourier transformation dft;s tools authentication data hiding discrete fourier transformation dft frequency domain inverse discrete fourier transform idft;statistical analysis decoding discrete fourier transforms frequency domain analysis image coding message authentication;histograms;data hiding;encoding decoding;image coding;chi square test discrete fourier transformation image authentication technique secret message transmission frequency domain spatial domain message authentication inverse dft encoding decoding histogram analysis;spatial domain;inverse discrete fourier transform;decoding;image converters;frequency domain analysis;authentication;discrete fourier transformation;histogram analysis;s tools;testing;secret message transmission;data mining;inverse discrete fourier transform idft;manganese;steganography;image authentication;statistical analysis;authentication discrete fourier transforms frequency domain analysis image converters encoding decoding steganography histograms image analysis testing;pixel;discrete fourier transform;chi square test;image analysis;inverse dft;message authentication;frequency domain;discrete fourier transforms;encoding;sliding window;image authentication technique	In this paper a novel technique, Discrete Fourier Transformation based Image Authentication (DFTIAT) has been proposed to authenticate an image and with its own application one can also transmit secret message or image over the network. Instead of direct embedding a message or image within the source image, choosing a window of size 2 × 2 of the source image in sliding window manner then convert it from spatial domain to frequency domain using Discrete Fourier Transform (DFT). The bits of the authenticating message or image are then embedded at LSB within the real part of the transformed image. Inverse DFT is performed for the transformation from frequency domain to spatial domain as final step of encoding. Decoding is done through the reverse procedure. The experimental results have been discussed and compared with the existing steganography algorithm S-Tools. Histogram analysis and Chi-Square test of source image with embedded image shows the better results in comparison with the S-Tools.	algorithm;authentication;bit-level parallelism;chi;discrete fourier transform;embedded system;insertion sort;least significant bit;steganography	Debnath Bhattacharyya;Jhuma Dutta;Poulami Das;Rathit Bandyopadhyay;Samir Kumar Bandyopadhyay;Tai-Hoon Kim	2009	2009 8th IEEE International Conference on Cognitive Informatics	10.1109/COGINF.2009.5250756	image warping;feature detection;discrete mathematics;speech recognition;binary image;image processing;theoretical computer science;digital image processing;mathematics	Vision	41.45308475108255	-11.860443094769595	123291
9f92a0ccc8b039a83bd5ba5482facb5829c712aa	maximum resilience of artificial neural networks		The deployment of Artificial Neural Networks (ANNs) in safety-critical applications poses a number of new verification and certification challenges. In particular, for ANN-enabled self-driving vehicles it is important to establish properties about the resilience of ANNs to noisy or even maliciously manipulated sensory input. We are addressing these challenges by defining resilience properties of ANN-based classifiers as the maximum amount of input or sensor perturbation which is still tolerated. This problem of computing maximum perturbation bounds for ANNs is then reduced to solving mixed integer optimization problems (MIP). A number of MIP encoding heuristics are developed for drastically reducing MIP-solver runtimes, and using parallelization of MIP-solvers results in an almost linear speed-up in the number (up to a certain limit) of computing cores in our experiments. We demonstrate the effectiveness and scalability of our approach by means of computing maximum resilience bounds for a number of ANN benchmark sets ranging from typical image recognition scenarios to the autonomous maneuvering of robots.	artificial neural network;autonomous robot;benchmark (computing);computer vision;experiment;heuristic (computer science);mathematical optimization;neural networks;parallel computing;scalability;software deployment;solver	Chih-Hong Cheng;Georg Nührenberg;Harald Ruess	2017		10.1007/978-3-319-68167-2_18	machine learning;mathematical optimization;artificial intelligence;theoretical computer science;psychological resilience;software deployment;artificial neural network;heuristics;scalability;ranging;computer science;optimization problem	AI	49.68341597830649	-22.88327896541145	123386
b8a519b7bec5d742f7f7331fe10b2b27105f4f7b	distributed compressed video sensing based on the optimization of hypothesis set update technique		Compressed Sensing (CS) breakthroughs the limitation of Nyquist sampling rate and realizes the sampling and compression of data simultaneous. Hence, it is widely used in image processing and video compression. However, it remains a challenge to obtain the high quality reconstructed image and video. To this end, we focus on the reconstruction algorithm of CS and the melioration of the existing distributed compressed video sensing (DCVS). In the perspectives of hypothesis set design and reference frames selection, we give detailed analyses for existing schemes and propose hypothesis set updating (HSU) and dynamic reference frames selection (DRS) algorithms to polish up the reconstruction performance. Then the superiorities in performance for these schemes are illustrated. Finally, the simulation results indicate that at a low sampling rate, the block based DCVS with the proposed HSU and DRS (HD-BDCVS) ameliorates the quality of non-key frames and key frames simultaneously without increasing the complexity of the coding.	algorithm;codec;compressed sensing;data compression;decoding methods;display resolution;distributed concurrent versions system;encoder;image processing;key frame;mathematical optimization;nyquist rate;nyquist–shannon sampling theorem;sampling (signal processing);simulation;video	Jian Chen;Ning Wang;Fei Xue;Yatian Gao	2016	Multimedia Tools and Applications	10.1007/s11042-016-3866-4	reference frame;simulation;telecommunications;computer science;theoretical computer science;data mining	EDA	49.3002113329129	-17.815190375479354	123542
dd0eb86432f720c7d8c8a8b7bb2659032343cd8f	improving scanned binary image watermarking based on additive model and sampling	printing;digital watermarking;binary image;printing scanning;additive model;adaptive;期刊论文	The SBWBAMS (Scanned Binary Image Watermarking Based on Additive Model and Sampling) algorithm proposed by Hou et al. owns strong robustness to the process of printing and scanning process. However, because the embedding strength used in the algorithm is set artificially, watermark information may not be correctly embedded into binary image when the embedding strength is low. Firstly, the minimum embedding strength to embed watermark correctly is analyzed in this paper, and then an improved binary image watermarking algorithm based on adaptive embedding strength is proposed. The proposed algorithm adjusts embedding strength adaptively according to image content, ensuring that the embedded watermark information is correct. The experimental results show that the proposed algorithm can not only embed and extract the watermark information correctly, but also still own strong robustness to the process of printing and scanning process. KeyWORDS Adaptive, Additive Model, Binary Image, Digital Watermarking, Printing Scanning	additive model;algorithm;binary image;digital watermarking;embedded system;gibbs sampling;printing	Ping Wang;Xiangyang Luo;Chunfang Yang;Fenlin Liu	2016	IJDCF	10.4018/IJDCF.2016040104	computer vision;binary image;digital watermarking;computer science;theoretical computer science;adaptive behavior;additive model;watermark;computer graphics (images)	EDA	40.58304975573443	-11.459554363771117	123807
d6b3c02a9a583aa842bf7ff553bca95b59158523	efficient video transcoding between h.263 and h.264/avc standards	downsized transcoding;coding efficiency;vector median filter;median filter;iso standards;video compression;filters;video quality;motion estimation;avc standards;intra prediction;video coding;video transcoding;motion estimation transcoding video coding median filters;standards development;format transcoding;iec standards;automatic voltage control;video quality h 263 h 264 transcoding avc standards video transcoding coding efficiency format transcoding downsized transcoding vector median filter motion vector estimation intra prediction mode selection integer transform coefficient coarse edge information;transcoding automatic voltage control video compression standards development video coding filters motion estimation spatial resolution iec standards iso standards;motion vector;integer transform coefficient coarse edge information;motion vector estimation;transcoding;h 263 h 264 transcoding;intra prediction mode selection;median filters;spatial resolution	A new video coding standard H.264/AVC has been recently developed and standardized, which represents a number of advances in standard video coding technology in terms of both coding efficiency enhancement and flexibility, and is expected to replace the existing standards such as H.263 and MPEG-1/2/4. In this paper we investigate and present efficient format transcoding and downsized transcoding methods between H.263 and H.264/AVC standards. Specifically, we use a vector median filter to estimate the new motion vectors required for transcoded videos. In addition, we also propose a fast intra-prediction mode selection based on the coarse edge information obtained from integer-transform coefficients. Experimental results show that the proposed method can gain a significant reduction in the complexity without degrading much the video quality.	algorithmic efficiency;coefficient;computational complexity theory;data compression;h.264/mpeg-4 avc;intra-frame coding;median filter;peak signal-to-noise ratio;video coding format;vii	Viet Anh Nguyen;Yap-Peng Tan	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1464735	scalable video coding;computer vision;real-time computing;transcoding;computer science;context-adaptive variable-length coding;multimedia;context-adaptive binary arithmetic coding;video post-processing	EDA	45.85300181612654	-18.334841473383896	123815
fb4b9af8544950d3d726b643f8acffea55e2519d	improving coding efficiency in grid-based hybrid video coding schemes	motion estimation;coding gain;motion compensated;rate distortion theory;wavelet transforms;video coding;adaptation model;lead;motion vector;adaptation model lead rate distortion theory switches spatial resolution wavelet transforms;switches;image warping;spatial resolution	In the past many grid-based video coding schemes using image warping p17ediction have been proposed. The advantage of grid-based motion compensation is its ability to model non-translational motion. Disadvantages are the interdependency of motion vectors which require more complex motion estimation strategies and its restricted ability to model motion boundaries. In this paper different solutions, to cope with the latter deficiency are presented and compared with respect to coding efficiency. In particular, three adaptive modes for handling motion discontinuities and a block-adaptive loop filtering are presented. It is shown that these are key components for achieving a high coding gain in grid-based hybrid coding. Introducing these features into a wavelet-based video coder leads to a coding gain of up to 2,5 dB compared to ITU H263 + and MPEG-4.	algorithmic efficiency;angular defect;coding gain;computer programming;data compression;decibel;image warping;interdependence;loop invariant;motion compensation;motion estimation;wavelet	Guido Heising	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745351	image warping;computer vision;lead;image resolution;rate–distortion theory;telecommunications;network switch;quarter-pixel motion;computer science;coding gain;motion estimation;mathematics;context-adaptive binary arithmetic coding;motion compensation;statistics;wavelet transform	Robotics	46.00573112310759	-16.763718020387284	123868
1481dede04a362093c31f9533714585cb0e2a604	3d spatial reconstruction and communication from vision field	quantization;iterative methods computer graphics computer vision data compression geometry image coding image reconstruction;image coding;iterative decoding;cascaded quantization 3d spatial reconstruction 3d spatial communication visual information seven dimensional plenoptic function full space vision field reconstruction laplacian iterative geometry 3d mesh coding;data compression;computer graphics;geometry;virtual reality;computer vision;iterative methods;three dimensional displays;image reconstruction;predictive coding three dimensional tv animation virtual reality;solid modeling;animation;three dimensional tv;encoding;predictive coding;encoding quantization three dimensional displays image reconstruction geometry solid modeling iterative decoding	Vision field describes the real world visual information by summarizing the seven-dimensional plenoptic function into three domains: view, light, time, from which a better understanding of previous 3D capture and reconstruction systems can be provided. In this paper, we first show how to reconstruct 3D spatial information from all the three attributes of the vision field, namely full-space vision field reconstruction. Then, based on Laplacian iterative geometry prediction, a 3D mesh coding algorithm with cascaded quantization is presented to facilitate the communication of the reconstructed 3D models from vision field. At last, experimental results of both the 3D spatial reconstruction and the 3D mesh coding are demonstrated.	3d modeling;algorithm;iterative method	Xun Cao;Qifei Wang;Xiangyang Ji;Qionghai Dai	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6289153	3d reconstruction;data compression;iterative reconstruction;anime;computer vision;quantization;computer science;theoretical computer science;virtual reality;iterative method;solid modeling;computer graphics;encoding;computer graphics (images)	Vision	43.01811552386575	-18.789689844747897	123927
f2ffbaae42e63d436035c08cf4019a0c06dd396d	improved variance-based fractal image compression using neural networks	fractals;data compression;qualite image;fidelity;reconstruction image;compression image;image compression;reconstruccion imagen;fidelite;relacion compresion;image reconstruction;peak signal to noise ratio;image quality;fractal;fractale;compression ratio;calidad imagen;rapport signal bruit;relacion senal ruido;compresion dato;fidelidad;taux compression;reseau neuronal;signal to noise ratio;red neuronal;compression donnee;variance;neural network;variancia;compresion imagen;fractal image compression	Although the baseline fractal image encoding algorithm could obtain very high compression ratio in contrast with other compression methods, it needs a great deal of encoding time, which limits it to widely practical applications. In recent years, an accelerating algorithm based on variance is addressed and has shortened the encoding time greatly; however, in the meantime, the image fidelity is obviously diminished. In this paper, a neural network is utilized to modify the variance-based encoding algorithm, which makes the quality of reconstructed images improved remarkably as the encoding time is significantly reduced. Experimental results show that the reconstructed images quality measured by peak-signal-to-noise-ratio is better than conventional variance-based algorithm, while the time consumption for encoding and the compression ratio are almost the same as the conventional variance-based algorithm.	fractal compression;image compression;neural networks	Yiming Zhou;Chao Zhang;Zengke Zhang	2006		10.1007/11760023_85	computer vision;data compression ratio;fractal;telecommunications;computer science;mathematics;artificial neural network;algorithm;statistics	ML	45.0428261837625	-12.597464965483596	123933
64e508a3c752143b841257085ac77c512249f78f	stable watermarking technique based on xnor operation and scale relationship	discrete cosine transform dct;mage watermarking;xnor operation;coordinate system	Digital watermark technique is often used to solve these problems and protect the copyright of multimedia transferred over the Internet. Most watermark methods use pixel values or coefficients as the judgment condition to embed or extract a watermark image. The variation of these values may lead to the inaccurate condition such that an incorrect judgment has been laid out. To avoid this problem, we design a stable judgment mechanism, in which the outcome will not be seriously influenced by the variation. The principle of judgment depends on the scale relationship of two pixels. From the observation of common signal processing operations, we can find that the pixel value of processed image usually keeps stable unless an image has been manipulated by cropping attack or halftone transformation. In the watermark embedding process, we use the XNOR operation to record the outcome to form a secret key instead of modifying pixel values. This can greatly help reduce the modification strength from image processing operations. Experiment results show that the proposed method can resist various attacks and keep the image quality friendly.	2.5d;coefficient;computation;correctness (computer science);data compression;digital watermarking;discrete cosine transform;discrete wavelet transform;image processing;image quality;internet;jpeg;key (cryptography);pixel;signal processing;simulation;xnor gate	Jung-San Lee;Hsiao-Shan Wong;You-Ren Chen;Yi-Hua Wang	2014	Multimedia Tools and Applications	10.1007/s11042-014-1930-5	computer vision;theoretical computer science;coordinate system;computer security	Graphics	39.65306231354565	-11.737747617683523	124018
29c36c9a6c7bad1f467f691e64c453ea5e463528	fast: a framework to accelerate super-resolution processing on compressed videos		State-of-the-art super-resolution (SR) algorithms require significant computational resources to achieve real-time throughput (e.g., 60Mpixels/s for HD video). This paper introduces FAST (Free Adaptive Super-resolution via Transfer), a framework to accelerate any SR algorithm applied to compressed videos. FAST exploits the temporal correlation between adjacent frames such that SR is only applied to a subset of frames; SR pixels are then transferred to the other frames. The transferring process has negligible computation cost as it uses information already embedded in the compressed video (e.g., motion vectors and residual). Adaptive processing is used to retain accuracy when the temporal correlation is not present (e.g., occlusions). FAST accelerates state-of-the-art SR algorithms by up to 15× with a visual quality loss of 0.2dB. FAST is an important step towards real-time SR algorithms for ultra- HD displays and energy constrained devices (e.g., phones and tablets).	algorithm;computation;computational resource;data compression;embedded system;pixel;real-time clock;super-resolution imaging;tablet computer;throughput	Zhengdong Zhang;Vivienne Sze	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.138	artificial intelligence;pixel;residual;throughput;computer vision;superresolution;computer science;motion compensation;image resolution	Vision	45.62503871077481	-20.39562428731296	124074
56616eb1ad42d8613dce8d3a20f4cf7cbe87e6da	block-based colour image steganography using smart pixel-adjustment		By adjusting the pixel-value of a host block, we design an effective steganographic method for color images. Specifically, based on a smart pixel- adjustment policy with two averages of the block, a secret message can be embedded in a host image without arising visual distortion. Experiments indicate that the perceived quality generated by the proposed method is good while the payload is larger than existing techniques. Moreover, the proposed method has a merit of maintaining a certain degree of robustness. Namely, the marked images generated by our method are tolerant of manipulations such as color quantization, equalized, edge sharpening, inversion, JPEG, JPEG2000, noise additions, pixel-truncation, winding, and zigzagging. This robustness is rarely found in the traditional techniques for color image steganography.	color image;pixel;steganography	Ching-Yu Yang;Wen-Fong Wang	2014		10.1007/978-3-319-12286-1_15	computer vision;theoretical computer science;mathematics	Vision	40.615027719573696	-11.684486788496073	124159
f45d87ff077ff43c1d4b2d16d28f0d7433f72e9d	constrained quantization in the transform domain with applications in arbitrarily-shaped object coding	cuantificacion senal;iterative method;quantization;quantum effect;mean square error distortion;reversed iterative algorithm;estudio comparativo;pixel domain;erreur quadratique moyenne;normal quantization;compresion senal;image block;transform coding;matrix algebra;codigo bloque;effet quantique;iterative algorithm;intelligent quantization;compression signal;transform domain;coding quality;quantization arbitrarily shaped object coding coding quality mpeg 4;metodo iterativo;algorithme;etude comparative;iterative methods;algorithm;video coding;signal quantization;senal video;shape;signal video;error cuantificuacion;matrix decomposition;methode iterative;discrete cosine transforms;mean square error;arbitrary shape body;quantification signal;pixel;signal compression;comparative study;mpeg 4;mean square error methods;quantization errors;video signal;quantization pixel image coding distortion measurement mpeg 4 standard transform coding image reconstruction permission iterative algorithms video compression;code bloc;block based transform coding;corps forme arbitraire;video signals;pixel domain distortion;error medio cuadratico;cuerpo forma arbitraria;mpeg 4 constrained quantization transform domain arbitrarily shaped object coding block based transform coding image signals video signals mean square error distortion mse distortion pixel domain transform matrix pixel domain distortion quantization errors reversed iterative algorithm coding quality image block normal quantization intelligent quantization;image signals;efecto cuantico;transform matrix;mse distortion;block code;constrained quantization;erreur quantification;codage transformation;quantization error;video coding iterative methods matrix algebra mean square error methods transform coding;algoritmo;arbitrarily shaped object coding	In any block-based transform coding of image/video signals, it is well-known that the mean square error (MSE) distortion measured in the pixel domain is exactly equal to the MSE distortion resulted from quantization in the transform domain if the involved transform matrix is unitary. However, such a property no longer exists if the pixel-domain distortion is measured only on a selected part of pixels within one image block. This provides us an opportunity of dynamically shaping the quantization errors so as to make the selected pixels (much) better than the unselected ones. In this paper, we first develop a reversed iterative algorithm to guide us to perform a highly constrained quantization so that the coding quality of the selected pixels in each image block is significantly higher than what can be achieved by using the normal quantization. Then, we apply this intelligent quantization in one practical scenario-coding of arbitrarily-shaped image blocks in MPEG-4, showing remarkable improvements in comparison with the original MPEG-4.	algorithm;distortion;iterative method;mean squared error;noise shaping;pixel;transform coding;transformation matrix	Shuyuan Zhu;Bing Zeng	2010	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2010.2046051	color cell compression;computer vision;quantization;theoretical computer science;mathematics;iterative method;quantization;vector quantization;algorithm;statistics	Vision	45.85200315663047	-13.796868393664644	124314
eb2c518473257574e971a616543dd5ab1c3b6786	coding efficiency improvement of hevc using asymmetric motion partitioning	encoding shape video coding iso standards iec standards image coding complexity theory;teleconferencing;image coding;complexity theory;high efficiency video coding coding efficiency improvement hevc asymmetric motion partitioning hm 6 0 amp asymmetric shape partition mode prediction unit image patterns symmetric partition encoder speed up motion estimation encoder complexity encoding time videoconference sequences;iso standards;motion estimation;video coding;iec standards;shape;image representation;video coding image representation image sequences motion estimation teleconferencing;asymmetric motion partition amp;high efficiency video coding hevc;encoding;video coding asymmetric motion partition amp high efficiency video coding hevc;image sequences	In this paper, coding efficiency improvement of HEVC using asymmetric motion partitioning (AMP) is provided based on HM-6.0. AMP allows asymmetric shape partition mode of prediction unit (PU) for inter prediction. AMP improves the coding efficiency, since irregular image patterns, which otherwise would be constrained to being represented by a smaller symmetric partition, can now be more efficiently represented without requiring further splitting. For encoder speed up, additional conditions are checked before doing motion estimation for each motion partitions. If the certain conditions are met, additional motion estimation, which is main source of encoder complexity for AMP, can be skipped. Experimental results demonstrate that AMP with encoding speed-up shows 0.8% coding efficiency improvement with 14% encoding time increase. Especially for videoconference sequences, coding efficiency improvement reaches to 1.4%.	algorithmic efficiency;encoder;high efficiency video coding;ibm systems network architecture;motion estimation	Il-Koo Kim;Sunil Lee;Min-Su Cheon;Tammy Lee;Jeong-Hoon Park	2012	IEEE international Symposium on Broadband Multimedia Systems and Broadcasting	10.1109/BMSB.2012.6264283	computer vision;real-time computing;quarter-pixel motion;theoretical computer science;mathematics	Robotics	46.55599921595921	-18.95187251364846	124511
00505d23211d1f6ea5ab61c52cbc5e551ea862af	efficient error resilient algorithm for h.264/avc: mobility management in wireless video streaming	encoding and transmission algorithm;performance evaluation;error resilient;real time;h 264 avc;wireless video;reference frame;visual quality;flexible macroblock ordering;performance improvement;complexity reduction;smoothness of the video;error propagation;comparative study;mobility management;error resilience;packet networks;wireless video streaming;compressed video	The H.264/AVC standard introduces enhanced error robustness capabilities enabling resilient and reliable transmission of compressed video signals over wireless lossy packet networks. Those robustness capabilities are achieved by integrating some new error resilience tools that are essential for a proper delivery of real-time video services. Those tools include the Intra Refreshing (IR), Arbitrary Slice Ordering (ASO), Sequence Picture Parameter Sets (PPS), Redundant Slices (RS) tools and Flexible Macroblock Ordering (FMO). This paper presents an error resilient algorithm in wireless H.264/AVC streaming. The proposed method merges Reference Frame Selection (RFS), Intra Redundancy Slice and Adaptive Intra Refreshment techniques in order to prevent temporal error propagation in error-phone wireless video streaming. The coding standards only specify the decoding process and the bitstream syntax to allow considerable flexibility for the designers to optimize the encoder for coding performance improvement and complexity reduction. Performance evaluations demonstrate that the proposed encoding algorithm outperforms the conventional H.264/AVC standard. Both subjective and objective visual quality comparative study has been also carried out in order to validate the proposed approach. The proposed method can be used and integrated into H264/AVC without violating the standard. K.E. Psannis ( ) Department of Technology Management, University of Macedonia, Thessaloniki, Greece e-mail: mobility2net@gmail.com Y. Ishibashi Department of Scientific and Engineering Simulation, Nagoya Institute of Technology, Nagoya 466-8555, Japan e-mail: ishibasi@nitech.ac.jp	adaptive switching;algorithm;algorithmic efficiency;arbitrary slice ordering;bitstream;cognitive dimensions of notations;data compression;digital video;email;encoder;flexible macroblock ordering;h.264/mpeg-4 avc;information science;intra-frame coding;lossy compression;network packet;propagation of uncertainty;prospective search;real-time transcription;reduction (complexity);reference frame (video);remote file sharing;simulation;software propagation;streaming media;videocassette recorder	Kostas E. Psannis;Yutaka Ishibashi	2009	Telecommunication Systems	10.1007/s11235-009-9151-3	scalable video coding;reference frame;real-time computing;telecommunications;computer science;propagation of uncertainty;context-adaptive variable-length coding;comparative research;context-adaptive binary arithmetic coding;reduction;computer network	EDA	48.181380501584805	-17.009540057647502	124794
4d5c0dc804291cb4a7fb63fd2cf7d12ba3904e45	low complexity independent multi-view video coding	estimation theory;independent;video streaming;multi view video coding mvc;component analysis ica joint multi view video;independent component analysis;correlation methods;video coding;computational complexity;model jmvm;multi view video coding mvc 3d video independent component analysis ica joint multi view video model jmvm;error statistics;streaming media decoding three dimensional displays vectors correlation computational complexity algorithm design and analysis;video streaming computational complexity correlation methods error statistics estimation theory image sequences independent component analysis video coding;3d video;video peak signal to noise ratio low complexity independent multiview video coding 3d multiview video coding disparity estimation view sequences de process i mvc coding complexity encoder side decoder side independent component analysis ica video streams correlated video sequences dependent video sequences uncorrelated sequences independent sequences mixing matrix h 264 avc video coder 3d multiview video coder overall bit rate mvc computational complexity;image sequences	In 3D multi-view video coding (MVC), disparity estimation (DE) are used to exploit the correlation among different view sequences. The DE process greatly increases the computational complexity of the MVC. In this paper, a novel independent low complexity multi-view video coder (I-MVC) is introduced. In the proposed MVC, the coding complexity is shifted from the encoder side to the decoder side. Instead of disparity estimation, the proposed I-MVC deploys independent component analysis (ICA) on the video streams to remove the correlation between the view sequences. The correlated (dependent) video sequences are decomposed into uncorrelated (independent) sequences and a mixing matrix. Each independent sequence is independently encoded by the H.264/AVC video coder. Then the mixing matrix is used at decoder to jointly decode the received independent sequences. Our experimental results show that the proposed I-MVC has better coding efficiency than conventional 3D multi-view video coder. The I-MVC gives more than 21% savings in overall bit rate and reduces the MVC computational complexity by 49% with less than 0.2 dB loss in the video peak signal to noise ratio.	asp.net mvc;algorithmic efficiency;binocular disparity;computational complexity theory;data compression;encoder;h.264/mpeg-4 avc;independent computing architecture;independent component analysis;model–view–controller;multiview video coding;signal-to-noise ratio;streaming media	Hany S. Hussein;Mostafa El-Khamy;Farhad Mehdipour;Mohamed El-Sharkawy	2013	2013 IEEE 10th Consumer Communications and Networking Conference (CCNC)	10.1109/CCNC.2013.6488422	scalable video coding;independent component analysis;computer vision;real-time computing;computer science;theoretical computer science;coding tree unit;estimation theory;computational complexity theory;statistics	Vision	46.00522548417893	-18.135483377298485	124839
63f13aca16f9004ecab0c9f9f34f5fc26afc1a5c	from 8-tap dct to 4-tap integer-transform for mpeg to h.264/avc transcoding	discrete cosine transforms automatic voltage control transcoding bit rate video codecs transform coding laboratories milling machines video coding video compression;code standards;video coding;discrete cosine transforms transcoding code standards video codecs source coding video coding;discrete cosine transforms;approximation 8 tap dct 4 tap integer transform mpeg to h 264 avc transcoding video coding standards integer transcoding algorithms bit rate reduction;video codecs;transcoding;quantitative evaluation;source coding	H.264 uses a 4-tap integer transform (DCT-like) to produce transform coefficients. This is different from the 8-tap DCT that is used in most prior video coding standards such as MPEG-x. The paper propose two integer algorithms for transcoding from 8-tap DCT data to 4-tap DCT-like data. The approximate nature of the integer transcoding algorithms is quantitively evaluated. The results show that the approximation is hidden in most cases when bit rate reduction is considered.	approximation algorithm;coefficient;data compression;discrete cosine transform;h.264/mpeg-4 avc;mpeg transport stream;moving picture experts group;video coding format	Bo Shen	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1418703	real-time computing;transcoding;computer science;theoretical computer science;coding tree unit;multimedia;context-adaptive binary arithmetic coding;video post-processing;h.261;multiview video coding;source code	Robotics	46.27249424624752	-17.10989391070282	125012
51a09e5861b1cddde3a2270225653ade7af7aa40	level compression-based image representation and its applications	representation;base donnee;algoritmo busqueda;algorithm performance;image processing;binary image;etude experimentale;algorithme recherche;search algorithm;interrogation base donnee;database;procesamiento imagen;interrogacion base datos;base dato;spatial data structure;traitement image;image compression;arbol binario;resultado algoritmo;image representation;compresion;arbre binaire;image binaire;performance algorithme;imagen binaria;compression;estudio experimental;database query;representacion;binary tree	Abstract   In this paper, a level compression-based image representation (LCBIR) is presented. This new image representation method improves the bintree representation for compressing digital binary images. Then we present a fast search algorithm on the LCBIR, which can support fast search and query in pictorial database. Experimental results show that our search algorithm on the LCBIR is faster than the one on the bintree representation.		Kuo-Liang Chung;Kuo-Bao Hong	1998	Pattern Recognition	10.1016/S0031-3203(97)00051-4	computer vision;pyramid;binary image;binary tree;image processing;image compression;computer science;theoretical computer science;mathematics;representation;compression;algorithm;search algorithm	Vision	39.525540759319014	-18.880819035941347	125035
2622cd23d4f7467ce4cc965a74ff3fcac94e2487	context modeling and accessibility for 3d scalable compression	feed forward;transform coding video coding data compression adaptive codes;scalable video;data compression;motion adaptation;transform coding;adaptive codes;context modeling image coding costs video compression wavelet coefficients australia arithmetic predictive coding predictive models propulsion;three dimensional;interactive multimedia;context model;video coding;three dimensional context coding highly scalable video compression invertible motion adaptive lifting transforms image processing research interactive multimedia technology feed forward framework;information theoretic;random access	Highly scalable video compression based on invertible motion adaptive lifting transforms has emerged as a promising area in image processing research and an important component in interactive multimedia technology. However, within this feed-forward framework, the potential for coding efficiency improvement and its impact on random accessibility still has not been carefully assessed. In this paper, we compare the merits of several three-dimensional context coding strategies from an information-theoretic perspective. The variation in random access cost in response to coding parameter adjustments is analyzed, for a variety of spatial and temporal configurations.	accessibility;algorithmic efficiency;data compression;image processing;information theory;lambda lifting;lossy compression;random access;scalability	Raymond Leung;David S. Taubman	2003		10.1109/ICIP.2003.1246617	video compression picture types;data compression;scalable video coding;three-dimensional space;computer vision;transform coding;computer science;theoretical computer science;context-adaptive variable-length coding;speech coding;mathematics;tunstall coding;multimedia;context model;interactive media;context-adaptive binary arithmetic coding;motion compensation;h.261;feed forward;random access;statistics;multiview video coding	Vision	43.962232350827676	-18.669023940269028	125042
743f6b0055829a1a2e7d3ee0596b2efb18552587	adaptive rate control for mpeg transcoder	adaptive rate control;temporal logic;prior knowledge;temporal logic video coding;rate control;video coding;a priori knowledge;programmable control adaptive control instruments bit rate video coding transcoding data mining delay;temporal processing;bit allocation;a priori knowledge adaptive rate control mpeg transcoder temporal processing intelligent bit allocation picture organization bit allocation	In MPEG [l], the input pictures can he coded in three different types, I, P and B. The three pictures require quite different numhcrs of hits because of different natures of their temporal processing. Hcnce, an intelligent hit allocation strategy should assign a target rate for a picture according to the picture's type. Furthermore, for a given hit budget, the types of other pictures should also he taken into consideration. This implies a requirement of a prior knowledge of the picture organization (COP). This requirement is not a problem for encoder as encoder can plan ahead the types of the pictures. A transcoder [3-5] however has no such a priori knowledge ahout a picture's type before actually processing the picture. This creates a difficulty in bit allocation for transcoder. This paper proposes a novel rate control scheme for MPEG transcoder that requires no a priori knowledge of the picture types. The experiments indicate that thc picture target rates determined by the proposed rate control with and withoul a priori knowledge of the picture types are very closc.	encoder;experiment;moving picture experts group	Limin Wang;Ajay Luthra;Robert O. Eifrig	1999		10.1109/ICIP.1999.819592	computer vision;real-time computing;a priori and a posteriori;temporal logic;computer science;theoretical computer science	AI	47.60573503393804	-21.539686299378893	125184
32c7e78d55868e7921015e1b9e43085ea8e73cfe	steganalysis of data hiding in binary text images	data hiding;data encapsulation;steganography steganalysis data hiding electronic binary text images similarity soft pattern matching message length estimation embedding algorithm digital document image processing;pattern matching;security of data data encapsulation document image processing pattern matching parameter estimation;document image processing;data encapsulation polynomials steganography pixel pattern matching document image processing object detection humans shape testing;parameter estimation;security of data	We present a technique for the steganalysis of electronic binary text images. The proposed method makes use of the similarity between the same characters or symbols for the purpose of detection. The idea of soft pattern matching is used to pair similar characters or symbols together for comparison. The proposed method can estimate the length of the message embedded by the targeted embedding algorithm.	algorithm;embedded system;pattern matching;steganalysis;symbol (formal)	Jun Cheng;Alex ChiChung Kot;Jun Liu;Hong Cao	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465608	computer science;theoretical computer science;pattern matching;pattern recognition;data mining;estimation theory;information hiding	Embedded	42.0496446572588	-11.415183640202553	125233
3e1cc1b16433d2faacf85b187f80e323e39b14e3	a new histogram modification based reversible data hiding algorithm considering the human visual system	electronics and computer engineering;histograms;watermarking;data hiding;hvs characteristics;reversible data hiding algorithm;image coding;image coding computer vision data encapsulation embedded systems;pixel level adjustment;isi scopus indexed journals;data embedding level;prediction algorithms;data encapsulation;computer vision;lossless watermarking;pixel level adjustment histogram modification reversible data hiding algorithm human visual system data embedding level hvs characteristics just noticeable difference values;embedded systems;visualization;just noticeable difference values;image edge detection;human visual system;pixel;histogram modification;pixel signal processing algorithms image edge detection histograms watermarking prediction algorithms visualization;just noticeable difference;signal processing algorithms;lossless watermarking data hiding human visual system just noticeable difference;reversible data hiding	In this letter, we propose an improved histogram modification based reversible data hiding technique. In the proposed algorithm, unlike the conventional reversible techniques, a data embedding level is adaptively adjusted for each pixel with a consideration of the human visual system (HVS) characteristics. To this end, an edge and the just noticeable difference (JND) values are estimated for every pixel, and the estimated values are used to determine the embedding level. This pixel level adjustment can effectively reduce the distortion caused by data embedding. The experimental results and performance comparison with other reversible data hiding algorithms are presented to demonstrate the validity of the proposed algorithm.	algorithm;distortion;human visual system model;pixel	Seung-Won Jung;Le Thanh Ha;Sung-Jea Ko	2011	IEEE Signal Processing Letters	10.1109/LSP.2010.2095498	just-noticeable difference;computer vision;visualization;prediction;digital watermarking;computer science;theoretical computer science;histogram;human visual system model;information hiding;pixel;statistics;computer graphics (images)	ML	41.59719087850924	-12.433676761793173	125253
3f5e28dcc434b3c3dc77ee7541225a4f341fc12b	watermarking in binary document images using fractal codes	fractal image coding;document analysis;stimark tool;watermarking and information hiding in documents;binary document image processing	This paper presents a novel watermarking method based on fractal theory. In the proposed method, information is embedded into binary document images. First, host image is coded by the proposed fractal coding method which is designed particularly for binary images. To insert the watermark uniformly over the entire host image, specific Range segments with predefined conditions are selected. Then, the watermark is added to the number of ones in the fractal code of the selected Range segments. Finally, the watermarked image is obtained by the fractal decoding procedure. Experimental results show that the output image quality of the proposed methods is acceptable to human eyes. Furthermore, empirical results show that the proposed fractal based watermarking is robust to the common attacks.	binary image;code;digital watermarking;embedded system;fractal;image quality	Fatemeh Daraee;Saeed Mozaffari	2014	Pattern Recognition Letters	10.1016/j.patrec.2013.04.022	computer vision;theoretical computer science;mathematics;multimedia;fractal transform;fractal compression	Vision	41.102002467079934	-12.117119659351793	125360
fdac6e8054377cbf1a5b9d5e895214d03e0848c4	fast retrieval of hidden data using enhanced hidden markov model in video steganography	data hiding;fast data retrieval;enhanced hidden markov model;video steganography;conditional states	The hidden Markov model is enhanced in the proposed system for fast retrieval of hidden data from video files.Data embedding and retrieval processes are performed using the conditional states and state transition dynamics between the video frames.It enhances the retrieval data rate with minimized computation cost. In the digital world, secure data communication has an important role in mass media and Internet technology. With the increase in modern malicious technologies, confidential data are exposed at a greater risk during data communication. For secured communication, recent technologies and the Internet have introduced steganography, a new way to hide data. Steganography is the growing practice of concealing data in multimedia files for secure data transfer. Nowadays, videos are more commonly chosen as cover media than other multimedia files because of the moving sequence of images and audio files. Despite its popularity, video steganography faces a significant challenge, which is a lack of a fast retrieval system of the hidden data. This study proposes a novel video steganography technique in which an enhanced hidden Markov model (EHMM) is employed to improve the speed of retrieving hidden data. EHMM mathematical formulations are used to enhance the speed of embedding and extracting secret data. The data embedding and retrieving operations were performed using the conditional states and the state transition dynamics between the video frames. The proposed EHMM is extensively evaluated using three benchmark functions, and experimental evaluations are conducted to test the speed of data retrieval using differently sized cover-videos. Results indicate that the proposed EHMM yields better results by reducing the data hiding time by 3-50%, improving the data retrieval rate by 22-77% with a minimum computational cost of 20-91%, and improving the security by 4-77% compared with state-of-the-art methods.	hidden markov model;markov chain;steganography	Mritha Ramalingam;Nor Ashidi Mat Isa	2015	Appl. Soft Comput.	10.1016/j.asoc.2015.05.040	steganography tools;computer science;machine learning;data mining;internet privacy;information hiding;world wide web	AI	39.715469839547104	-13.155642085163318	125429
c795f7423a01238d2ad1c5f52310fcf974a38a68	efficient intra prediction mode decision for h.264 video	theorie vitesse distorsion;filtering;filtrage;image coding;multimedia;filtrado;intra prediction;preparacion serie fabricacion;rate distortion theory;codage image;video coding;senal video;signal video;codage video;complexity reduction;filter;filtre;video signal;process planning;rate distortion optimization;preparation gamme fabrication;mode decision;filtro	In this paper, we propose an efficient 4×4 intra prediction mode decision method for the complexity reduction of H.264 video coding. Based on the observation that a good prediction could be achieved when local edge direction of a block is in the same direction as the intra prediction mode, local edge direction of block is obtained in transform domain to filter out majority of intra prediction modes. By filtering out majority of intra prediction modes, we only have to consider a candidate set of most highly probable 4×4 intra prediction modes for which the rate distortion optimization process should be performed. Experimental results show that the proposed method can achieve a considerable reduction of computation while maintaining similar PSNR and bit rate.	h.264/mpeg-4 avc	Seong Soo Chun;Ja-Cheon Yoon;Sanghoon Sull	2005		10.1007/11581772_15	filter;computer vision;simulation;rate–distortion theory;filter;computer science;theoretical computer science;rate–distortion optimization;reduction	Vision	46.49573828574032	-15.338638821909951	125546
3882aa55109fd11e08e02506c153d768d7c627ef	object-oriented coding using successive motion field segmentation and estimation	translational motion field;two stage algorithm;object oriented video compression algorithms;object oriented coder;block based motion compensation;probability;complex motion models;bit rate reduction;image segmentation;data compression;motion compensation;image matching;video compression;motion estimation;video sequences;bit rate;boundary object;maximum likelihood estimation;motion compensated;video coding;successive motion field segmentation;computational modeling;maximum a posteriori probability estimate;object oriented;computational complexity;pixels;image regions;video coding object oriented coding successive motion field segmentation block based motion compensation pixels translational motion moving object boundaries object oriented video compression algorithms image regions motion compensated images two stage algorithm successive motion field estimation object oriented coder maximum a posteriori probability estimate translational motion field complex motion models computational complexity reduction simulation results bit rate reduction video sequences;successive motion field estimation;computational complexity video coding image sequences data compression image segmentation motion estimation motion compensation image matching maximum likelihood estimation probability;parameter estimation;motion compensated images;computational complexity reduction;simulation results;translational motion;object oriented coding;object oriented modeling;moving object boundaries;block matching algorithm;image segmentation motion estimation object oriented modeling motion compensation video compression parameter estimation computational complexity computational modeling bit rate video sequences;image sequences	Block-based motion compensation assumes that all pixels within a block have the same translational motion. That hypothesis, however, results in inaccurate compensation of moving objects’ boundaries. Object-oriented video compression algorithms typically segment each image in regions of uniform motion and estimates the motion of these regions to generate more accurate motion compensated images. In this paper, we present a two-stage algorithm for motion field segmentation and estimation in an object-oriented coder. In the algorithm’s first stage, a standard block-matching algorithm and a maximum a posteriori probability estimate are used to compute a translational motion field and its segmentation. This segmentation is then utilized in the second stage to estimate the parameters of complex motion models. Parameters of complex motion models are only estimated in the algorithm’s second stage which reduces the computational complexity of the proposed algorithm. Simulation results show that the proposed algorithm significantly reduces the bit rate needed to encode video sequences when compared to standard block-based algorithms.	block-matching algorithm;computational complexity theory;data compression;encode;estimation theory;motion compensation;motion field;pixel;simulation	Dam LeQuang;André Zaccarin;S. Caron	1995		10.1109/ICIP.1995.529582	data compression;computer vision;structure from motion;quarter-pixel motion;computer science;theoretical computer science;pattern recognition;motion estimation;mathematics;block-matching algorithm;motion field;motion compensation;statistics	Vision	48.29760834673079	-18.197631974033996	125623
a9ff97b2aced895e46a3f8a9aaedc0901b0cc310	quantization- and prediction-based image authentication and recovery	watermarking;image coding;data compression;information extraction;authentication;image restoration;indexes;image authentication;vq;side match prediction vq torus automorhism;indexation;watermarking authentication image restoration indexes robustness correlation;torus automorhism;verification codes quantization based image authentication prediction based image authentication image recovery data compression digital image image blocks;robustness;image watermarking;digital image;correlation;image watermarking data compression image coding;side match prediction	This paper proposed an image authentication technique based on a data compression and prediction mechanism in order to verify if a digital image was tampered with over the network. For image blocks, first is to produce special verification codes, and next, these codes and other information related to data recovery will be embedded into the image. When someone receives a suspicious image, he/she can check the integrity of embedded verification codes to judge whether the image was tampered with or not. Further, the recovery information extracted from images can be used to restore the tampered area. Experimental results showed the proposed scheme not only marks those tampered area effectively, but is able to recover them back to initial as similar as possible in order to reduce retransmitting probability and save network bandwidth.	authentication;code;data compression;data recovery;digital image;embedded system	Wen-Chuan Wu;Yi-Pei Hsieh	2011	2011 Fifth International Conference on Genetic and Evolutionary Computing	10.1109/ICGEC.2011.52	data compression;database index;image restoration;computer vision;digital watermarking;computer science;theoretical computer science;authentication;computer security;correlation;information extraction;digital image;robustness	EDA	39.73483373845925	-12.139875524531115	125833
7b261ed9a63927dc4f0ba3f3b007a0f6d233633e	an adaptive initialization technique for color quantization by self organizing feature map	unsupervised learning;image coding;image reconstruction image colour analysis vector quantisation adaptive signal processing self organising feature maps unsupervised learning image classification data compression image coding pattern recognition;data compression;image quality adaptive initialization technique color quantization self organizing feature map unsupervised learning network color classification image compression pattern recognition vector quantization image reconstruction neuron initialization gray scale initialization subsampling butterfly jumping sequences code vectors;image classification;adaptive signal processing;image compression;self organising feature maps;spatial distribution;image colour analysis;self organized feature map;image reconstruction;image quality;pattern recognition;vector quantizer;vector quantisation;organizing neurons image reconstruction unsupervised learning image coding pattern recognition vector quantization lattices acceleration convergence;color quantization	An unsupervised learning network, such as the self organizing feature map (SOFM), has been applied successfully to color classification for image compression and pattern recognition. Like other vector quantization algorithms, the reconstruction quality and adaptation rate of the SOFM are sensitive to the neuron initialization. We propose an efficient new initialization method, whereby an excess number of neurons is defined and the neurons are adaptively pruned, merged and split within their lattice according to the spatial distribution of the input color pixels. Comparisons with conventional gray scale initialization using subsampling and butterfly jumping sequences show that the proposed method obtains good initial code vectors that can accelerate the convergence of the SOFM and improve the reconstructed image quality significantly.	color quantization;organizing (structure);self-organization	Chip Hong Chang;Rui Xiao;Thambipillai Srikanthan	2003		10.1109/ICASSP.2003.1199515	data compression;iterative reconstruction;image quality;adaptive filter;unsupervised learning;computer vision;contextual image classification;color quantization;image compression;computer science;machine learning;pattern recognition;mathematics	Robotics	44.14962639009921	-13.74212964110351	125918
4127ae9b7e2875d739f3c4d6842cb6bcdee981d3	adaptive block-based image coding with pre-/post-filtering	discrete wavelet transforms;filtering;homogenous regions;rate distortion;nonstationary regions;coding efficiency;image coding;image segmentation;data compression;adaptive filtering;data compression image coding adaptive codes adaptive filters discrete cosine transforms transform coding rate distortion theory image reconstruction;transform coding;adaptive codes;post filtering;visual quality;variable block size;rate distortion theory;video coding;adaptive filters;automatic voltage control;discrete cosine transforms;image reconstruction;pre filtering;variable block size transform;block based image coding;reconstruction visual quality;ringing artifacts;switches;rate distortion sense;post filtering adaptive image coding variable block size transform adaptive filtering rate distortion sense adaptive block decomposition ringing artifacts nonstationary regions coding efficiency homogenous regions reconstruction visual quality dct block based image coding pre filtering;blocking artifact;dct;adaptive image coding;adaptive block decomposition;image coding discrete wavelet transforms automatic voltage control video coding image segmentation switches rate distortion adaptive filters filtering image reconstruction	This paper presents an adaptive block-based image coding method, which combines the advantages of variable block size transform and adaptive pre-/post-filtering scheme. Our approach partitions an image into blocks with different sizes, which are best suitable for the characteristics of the underlying data in the rate-distortion (RD) sense. The adaptive block decomposition mitigates the ringing artifacts by adopting a small block size transform in nonstationary regions, and improves the coding efficiency by using a large block size transform in homogenous regions. Moreover, pre-/post-filtering is adaptively applied along the block boundaries to improve coding efficiency and minimize blocking artifacts. Simulation results show that the proposed coder can achieve competitive objective performance as well as yield superior reconstruction visual quality, compared with the RD-optimized JPEG2000 and H.264/AVC I-frame coder.	adaptive filter;algorithm;algorithmic efficiency;block size (cryptography);blocking (computing);codec;distortion;edge detection;experiment;h.264/mpeg-4 avc;intra-frame coding;jpeg 2000;mathematical optimization;on the fly;performance;rate–distortion theory;ringing artifacts;ruby document format;simulation;stationary process	Wei Dai;Lijie Liu;Trac D. Tran	2005	Data Compression Conference	10.1109/DCC.2005.11	adaptive filter;computer vision;mathematical optimization;lapped transform;computer science;theoretical computer science;mathematics;statistics	Vision	44.66213023622921	-16.210006218134744	126270
7caf69efbca6f4901ac4783ec0d5a833aa50fecb	joint weighted sparse representation based median filter for depth video coding	image coding;data compression;sparse prior characteristic joint weighted sparse representation median filter depth video coding auto stereoscopic display mpeg multiview plus depth format video coding standards h 264 avc depth boundaries in loop depth filters;会议论文;joints;video coding image representation median filters stereo image processing;video coding;adaptive filters;image color analysis;joints image color analysis image coding video coding encoding adaptive filters data compression;encoding	In order to promote the development of auto-stereoscopic display, MPEG has proposed multi-view plus depth (MVD) format. The depth video is encoded and transmitted with color video to synthesize virtual views at the receiver side. The existing video coding standards such as H.264/AVC introduces coding artifacts along the depth boundaries, which may seriously affects the synthesized view quality and coding efficiency. Many in-loop depth filters such as joint depth filter have been proposed to remove the artifacts in compressed depth video. However, their performance is unstable and affected by the outliers due to the weighted summation. In this paper, based on the sparse prior characteristic in local region of depth map, we propose a joint weighted sparse representation based median filter to select the most relevant neighboring depth pixel as the output during the filter process. Experimental results show the proposed method is more effective in improving the depth video coding efficiency.	algorithmic efficiency;control theory;data compression;depth map;h.264/mpeg-4 avc;median filter;moving picture experts group;pixel;sparse approximation;sparse matrix;stereoscopy;video coding format	Jinhui Hu;Ruimin Hu;Yu Chen;Liang Liao;Jing Xiao;Ruolin Ruan	2015	2015 Data Compression Conference	10.1109/DCC.2015.38	data compression;scalable video coding;adaptive filter;sub-band coding;computer vision;computer science;context-adaptive variable-length coding;coding tree unit;mathematics;multimedia;context-adaptive binary arithmetic coding;motion compensation;h.261;encoding;statistics;multiview video coding;computer graphics (images)	Vision	45.13292300971544	-17.63330007990302	126341
151b6c7306cfc2fffb2bc82042bb18ced97bd5fe	image coding-from waveforms in animation	image coding;picture processing;transform coding;reviews encoding picture processing;vector quantizer;reviews;encoding;visual system;predictive coding;subband coding;object model;object modeling picture processing animation coding grayscale images scene visual system delta modulation predictive coding transform coding vector quantization hybrid coding subband coding semantic image coding waveform perceptual coding;animation image coding gray scale layout visual system delta modulation predictive coding transform coding vector quantization bridges	The main coding techniques for efficient electronic representation of grayscale images are reviewed. The emphasis is on understanding some of the reasons behind the development of different coding structures. It is shown that various (conscious or unconscious) models of the scene, the image, and the visual system have been and are still being used as the driving source for the construction of coding algorithms. Methods covered in this include delta modulation, predictive coding, transform coding, vector quantization, hybrid coding, subband coding, and semantic image coding. The last two schemes are covered in greater detail. Subband coding forms a bridge between traditional (waveform) concepts and perceptual coding. Semantic coding is an example of a class of new coding techniques based on (3-D) object modeling. >		Robert Forchheimer;Torbjörn Kronander	1989	IEEE Trans. Acoustics, Speech, and Signal Processing	10.1109/29.45550	sub-band coding;computer vision;transform coding;speech recognition;shannon–fano coding;object model;visual system;variable-length code;computer science;context-adaptive variable-length coding;speech coding;coding tree unit;tunstall coding;context-adaptive binary arithmetic coding;encoding;code-excited linear prediction	Visualization	42.74104012648749	-18.812097879734516	126574
7e61ed7e22f734c1626d2cccd93af092f45f2c6c	layered coding schemes for video transmission on atm networks	atm networks;video transmission;layered coding	Abstract   Video services are expected to be a major component of the emerging demand for broadband services that will be supported by the broadband integrated services digital network (B-ISDN). Within the class of video services, the need for interworking between different grades of services and evolution to higher grades of services makes it desirable to provide a video coding technique that covers a wide range of services. Toward this purpose, we have designed layered coding schemes based on interframe multi-layered variable bit rate (VBR) coding that are suitable for transport in the asynchronous transfer mode (ATM) environment. In this paper we describe schemes that use subband and pyramidal multiresolution representation, combined with motion-compensated interframe coding along with a motion estimation procedure that exploits the correlation between the subbands. We designed two interframe two-layered hierarchical coding schemes where we examined the case of both constant bit rate (CBR) and VBR coding of the low-resolution signal, while the high-frequency information was VBR coded. These hierarchical schemes were used to code sequences in progressive scan formats. Coding efficiency and other features of these schemes are compared with those of a single-layer VBR hybrid DCT/DPCM coder.	atm turbo	Christine Guillemot;Rashid Ansari	1994	J. Visual Communication and Image Representation	10.1006/jvci.1994.1006	real-time computing;telecommunications;computer science;coding tree unit;computer network	Vision	44.544487304422326	-20.772620053492528	126612
9bda3e80988bdd20443ef0c4ea90a563e43d549b	rand - steg: an integer wavelet transform domain digital image random steganography using knight's tour	adaptive embedding;knight s tour;steganography;integer wavelet transform iwt;random traversing	In recent times, the use of Internet for data exchange has increased to a great extent. Steganography, cryptography and watermarking are the methods to transfer the data with high security. In this paper, a steganography method with constant bit embedding and adaptive bit embedding in Haar Integer Wavelet Transform domain has been proposed. The adaptive bit embedding provides more security than the constant bit embedding. Here, a mathematical model for Knight's Tour is developed, which adapts random traversing inside the wavelet coefficients to increase the security further. In this proposed work, multiple security is ensured by formulating Knight's Tour algorithm for random traversing and selecting the order of sub-bands to provide high capacity, security and robustness. Copyright © 2015 John Wiley & Sons, Ltd.	digital image;knight's tour;steganography;wavelet transform	V. Thanikaiselvan;P. Arulmozhivarman	2015	Security and Communication Networks	10.1002/sec.1185	speech recognition;knight's tour;theoretical computer science;steganography;algorithm	Crypto	39.600668610544595	-10.268790523525569	126614
525925996d2370cfd87142e7a63943ba21a0cfd5	video steganography with perturbed motion estimation	embedding impact;video compression;raw video;normal estimation deviation;adequate payload;motion estimation;internal dynamic;certain existing steganalyzers;adaptive video steganography	In this paper, we propose an adaptive video steganography tightly bound to video compression. Unlike traditional approaches utilizing spatial/transformed domain of images or raw videos which are vulnerable to certain existing steganalyzers, our approach targets the internal dynamics of video compression. Inspired by Fridrich et alu0027s perturbed quantization (PQ) steganography, a technique called perturbed motion estimation (PME) is introduced to perform motion estimation and message hiding in one step. Intending to minimize the embedding impacts, the perturbations are optimized with the hope that these perturbations will be confused with normal estimation deviations. Experimental results show that, satisfactory levels of visual quality and security are achieved with adequate payloads.	motion estimation;steganography	Xianfeng Zhao;Dengguo Feng;Rennong Sheng	2011		10.1007/978-3-642-24178-9_14	computer vision;simulation;quarter-pixel motion;theoretical computer science;mathematics	Vision	40.01901677609965	-12.755036661768372	126892
13fbf1669923eee24484dee4877304a0baff773b	an image watermarking based on the pdf modeling and quantization effects in the wavelet domain	jpeg2000 quantization;wavelet transform;image watermarking;optimal detection	An image watermarking technique based on the concept of JPEG2000 algorithm is proposed. Biorthogonal wavelet 9/7 transform is used to provide a set of coefficients suitable for watermark embedding. The statistical properties of different subbands are analyzed in order to choose the number of decomposition levels and position of subbands, which will assure the best compromise between the watermark transparency and robustness. The JPEG2000 quantization is applied to avoid insignificant wavelet coefficients, while the remaining ones are used for watermarking. The optimal and blind watermark detection is based on the nonlinear score function and appropriate model of coefficients distribution. The performance of the proposed procedure is tested on examples with various images, showing robustness under different attacks, while maintaining high image quality.	algorithm;biorthogonal wavelet;coefficient;digital watermarking;image quality;jpeg 2000;nonlinear system;portable document format;robustness (computer science)	Irena Orovic;Milica Orlandic;Srdjan Stankovic	2012	Multimedia Tools and Applications	10.1007/s11042-012-1182-1	computer vision;theoretical computer science;pattern recognition;wavelet packet decomposition;stationary wavelet transform;wavelet transform	EDA	41.18060365946923	-10.231500879225573	127100
1525c0e2bc825daa4ae3ef1a63840da73aabab80	improved motion classification techniques for adaptive multi-pattern fast block-matching algorithm	multi pattern algorithms;motion estimation robustness video coding video sequences computational efficiency motion measurement switches communication standards adaptive algorithm classification algorithms;image matching;video coding code standards image classification image matching image sequences motion estimation;image classification;motion estimation;code standards;video coding;adaptive algorithm;quality requirement;binary linear classifier;range of motion;binary linear classifier block matching motion estimation multi pattern algorithms motion classification;block matching;video sequences motion classification adaptive multipattern fast block matching algorithm video coding h 264 standard motion estimation;block matching algorithm;motion classification;image sequences	In several video coding standards, such as H.264, motion estimation becomes the most-time consuming subsystem. Therefore, recently research on video coding focuses on the development of novel algorithms able to save computations with minimal effects over the coding distortion. Due to the fact that real video sequences usually exhibit a wide-range of motion content, from uniform to random, and to the vast amount of coding applications demanding different degrees of coding quality, adaptive algorithms have revealed as the most robust general purpose solutions. In particular, multi-pattern algorithms can adapt to video contents as well as to required coding quality by means of the use of a set of heterogeneus search patterns, each one adapting better to particular motion and quality requirements. This paper applies some improvements to the Motion Classification based Search, an adaptive multi-pattern algorithm based on motion classification techniques. Our experimental results show that MCS notably reduces the computational cost with respect to some well-known algorithms while maintaining the quality.	adaptive algorithm;algorithmic efficiency;approximation algorithm;block-matching algorithm;comparison and contrast of classification schemes in linguistics and metadata;computation;computational complexity theory;data compression;distortion;h.264/mpeg-4 avc;motion estimation;multi categories security;real-time clock;requirement;video coding format	Iván González-Díaz;Fernando Díaz-de-María	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379198	computer vision;contextual image classification;range of motion;quarter-pixel motion;computer science;theoretical computer science;pattern recognition;motion estimation;block-matching algorithm;context-adaptive binary arithmetic coding;motion compensation	Vision	45.412787177637725	-19.34665252974992	127209
25c44b6687385577a17729f9dfd988198a8af544	a hybrid algorithm for image watermarking against signal processing attacks	discrete wavelet transforms;singular value decomposition;mage watermarking;steganography;discrete cosine transforms	In this paper, we have presented a hybrid image watermarking technique and developed an algorithm based on the three most popular trans form techniques which are discrete wavelet transforms DWT, discrete cosine transforms DCT, and singular value decomposition SVD against signal processing attacks. However, the experimental results demonstrate that this algorithm combines the advantages and remove the disadvantages of these three transform. This proposed hybrid algorithm provides better imperceptibility and robustness against various attacks such as Gaussian noise, salt and pepper noise, motion blur, speckle noise, and Poisson noise etc.	digital watermarking;hybrid algorithm;signal processing	Amit Kumar Singh;Mayank Dave;Anand Mohan	2013		10.1007/978-3-642-44949-9_22	computer vision;speech recognition;theoretical computer science;mathematics	Vision	41.2183372181347	-10.659656175729571	127482
164d3bf82004b7182110c1785916ca4d78bb83fe	joint reversible watermarking and progressive compression of 3d meshes	reversible watermarking;mesh compression;copyright protection;3d model;level of detail;compression ratio;compression;3d mesh	A new reversible 3D mesh watermarking scheme is proposed in conjunction with progressive compression. Progressive 3D mesh compression permits a progressive refinement of the model from a coarse to a fine representation by using different levels of detail (LoDs). A reversible watermark is embedded into all refinement levels such that (1) the refinement levels are copyright protected, and (2) an authorized user is able to reconstruct the original 3D model after watermark extraction, hence reversible. The progressive compression considers a connectivity-driven algorithm to choose the vertices that are to be refined for each LoD. The proposed watermarking algorithm modifies the geometry information of these vertices based on histogram bin shifting technique. An authorized user can extract the watermark in each LoD and recover the original 3D mesh, while an unauthorized user which has access to the decompression algorithm can only reconstruct a distorted version of the 3D model. Experimental results show that the proposed method is robust to several attack scenarios while maintaining a good compression ratio.	3d computer graphics;3d modeling;algorithm;authorization;channel (communications);data compression;digital watermarking;embedded system;level of detail;lossless compression;polygonal modeling;progressive refinement;refinement (computing);vertex (geometry);watermark (data file)	Ho Lee;Çagatay Dikici;Guillaume Lavoué;Florent Dupont	2011	The Visual Computer	10.1007/s00371-011-0586-7	polygon mesh;computer science;theoretical computer science;level of detail;compression ratio;compression;computer graphics (images)	Graphics	39.24350907039827	-10.967032936511208	127500
7d93f866a012b0581611a9051b082173c1472e34	internet still image and video formats		Nowadays it is hard to find a website without images. Even buttons and layout elements of a website consist of small bitmaps. Using images in the internet has become possible as there are sophisticated compression algorithms to keep the sizes of the images as small as possible. Video clips are still rare to be found on typical websites, but the usage of video is increasing. However – opposed to still images – up to now there is no commonly accepted video coding standard that has been incorporated into web-browsing applications. In addition the bandwidth that is required for the transmission of high quality videos – larger than the size of a matchbox – is still too high for most users having access to the internet with moderate bandwidth only (e.g. a modem). This chapter introduces some basic facts about images and videos. It explains the essential ideas of image and video compression techniques. Standards and quasi-standards are described and their features will be discussed.	algorithm;bitmap;data compression;display resolution;internet;matchbox;modem;video clip;video coding format;web navigation;web standards	Guido Heising;Kai Uwe Barthel	2005			multimedia;the internet;video processing;computer science	Networks	43.10564841605436	-20.46606618458856	127721
21a130b217b3bf7be32049ba9f7b4931e2138632	low-complexity depth map compression in hevc-based 3d video coding	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	In this paper, a low-complexity algorithm is proposed to reduce the complexity of depth map compression in the high-efficiency video coding (HEVC)-based 3D video coding (3D-HEVC). Since the depth map and the corresponding texture video represent the same scene in a 3D video, there is a high correlation among the coding information from depth map and texture video. An experimental analysis is performed to study depth map and texture video correlation in the coding information such as the motion vector and prediction mode. Based on the correlation, we propose three efficient low-complexity approaches, including early termination mode decision, adaptive search range motion estimation (ME), and fast disparity estimation (DE). Experimental results show that the proposed algorithm can reduce about 66% computational complexity with negligible rate-distortion (RD) performance loss in comparison with the original 3D-HEVC encoder.	3d computer graphics;algorithm;binocular disparity;computational complexity theory;data compression;depth map;distortion;encoder;fast fourier transform;high efficiency video coding;motion estimation;rate–distortion theory;ruby document format;stereoscopic video coding	Qiuwen Zhang;Ming Chen;Xinpeng Huang;Nana Li;Yong Gan	2015	EURASIP J. Image and Video Processing	10.1186/s13640-015-0058-5	video compression picture types;scalable video coding;computer vision;quarter-pixel motion;computer science;video quality;archaeology;video tracking;coding tree unit;pattern recognition;block-matching algorithm;multimedia;context-adaptive binary arithmetic coding;motion compensation;h.261;video denoising;biometrics;multiview video coding;computer graphics (images)	Vision	45.03167181101487	-18.00871749114389	127788
5e50544ebe6f9fb361e8dd0143e52dcb4f077c96	soft-input source decoding for robust transmission of compressed images using two-dimensional optimal estimation	joint source channel decoding;circuits and systems;quantization;channel coding;estimation theory;decoding robustness image coding redundancy protection quantization circuits and systems resilience statistics bit rate;image coding;channel codes;error protection;data compression;decoding;estimation method;combined source channel coding;2d optimal estimation;optimal estimation;quantized subband images;bit rate;negligible explicit redundancy;protection;joint source channel coding;awgn channels;redundancy;resilience;soft input source decoding;two dimensional optimal estimation;compressed images;statistics;estimation theory decoding redundancy data compression image coding parameter estimation combined source channel coding awgn channels;robustness;parameter estimation;implicit residual source redundancy;source correlations;joint source channel decoding soft input source decoding compressed images two dimensional optimal estimation 2d optimal estimation awgn channels negligible explicit redundancy channel codes implicit residual source redundancy quantized subband images error protection source correlations joint source channel coding	In this paperwe addressthe transmissionof compressedimages over highly corruptedAWGN-channelsusingan optimal estimation approachat thedecoder . In contrastto othermethodsweonly usea negligible amountof explicit redundanc y basedon channel codes.Mainly, theimplicit residualsourceredundanc y inherentin thequantizedsubbandimagesandthebit-reliability informationat the channeloutputareutilized for error protection.As a novelty we extendthe optimal estimationtechniquefrom the one-to the two-dimensional case,wherebothhorizontalandverticalcorrelationsareexploitedin thesubbandimages.Basedon this approach the performancesfor several estimationmethodsare compared, wherealsoapproachesfor approximatingthe sourcecorrelations at thedecoderarediscussed.	code	Joerg Kliewer;Norbert Goertz	2001		10.1109/ICASSP.2001.940525	speech recognition;computer science;theoretical computer science;mathematics;estimation theory;psychological resilience;statistics	Vision	48.48982131769764	-12.477733531608683	127988
8627777c342d1c90feab2640aaa760093529c320	scalable subband image coding with segmented orthogonal matching pursuit	matching pursuit algorithms;iterative methods image coding data compression image representation image segmentation;orthogonal matching pursuit;pursuit algorithms;atomic measurements;image coding;image segmentation;psnr;data compression;subjective image quality;video compression;image representation algorithm;bit rate;segmented orthogonal matching pursuit;iterative methods;peak signal to noise ratio scalable subband image coding segmented orthogonal matching pursuit low bit rate image compression image representation algorithm somp subbands subjective image quality;image compression;subbands;image representation;image coding image segmentation matching pursuit algorithms pursuit algorithms bit rate psnr atomic measurements image representation image quality video compression;somp;peak signal to noise ratio;image quality;matching pursuit;low bit rate image compression;scalable subband image coding	In this paper, a novel algorithm for low bit-rate image compression is presented. In this technique, we use a new image representation algorithm called segmented orthogonal matching pursuit (SOMP) (Rabiee and Kashyap, 1998) to encode the subbands of an image. Our preliminary results show that our algorithm performs better than the segmentation based matching pursuit (QTMP) (Rabiee et al. 1996) and EZW (Shapiro 1993) encoders at lower bit rates, based on subjective image quality and peak signal-to-noise ratio (PSNR).	matching pursuit	Hamid R. Rabiee;S. Rasoul Safavian;Rangasami L. Kashyap;Mohammed Saeed	1998		10.1109/ICIP.1998.723615	data compression;computer vision;peak signal-to-noise ratio;computer science;theoretical computer science;pattern recognition;mathematics;algorithm;statistics;matching pursuit	Vision	45.65404378619624	-16.5875926757424	128139
0d1f3975a26e267e1a7463eb8cc717221e4662ed	the technique of information hiding based on modification function and lsb matching	effective hiding scheme;lsb matching information hiding run length coding modification function;image distortion;image coding;data compression;image matching;information hiding;stego image;lsb matching;data mining;steganography;internet;run length coding;lead;image distortion information hiding modification function lsb matching effective hiding scheme stego image run length coding;pixel;steganography image coding image matching;modification function;encoding;image coding information security internet data compression intelligent systems conference management information management technology management control systems data mining	This paper shall propose an effective hiding scheme to embed large number of information and further to control the distortion of the stego image. The proposed scheme compresses the secret information by using run length coding to increase the total number of hiding capacity. Then, the scheme embeds the compressed results into the digital medium in order to create the stego medium. The purpose is to transmit the secret information successfully without detecting by the illegal party. According to the experiment results, the proposed scheme indeed effectively increases the hiding capacity and greatly decreases the image distortion.	distortion;image quality;least significant bit;run-length encoding;sensor;steganography	Tzu-Chuen Lu;Li-Ling Hsu	2008	2008 Eighth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2008.65	data compression;computer vision;lead;the internet;computer science;theoretical computer science;mathematics;steganography;internet privacy;information hiding;pixel;encoding;statistics	Robotics	39.54468512471373	-12.234411360488918	128427
3c0c43d5007a87807e0adacfc33996905936b105	towards practical wyner-ziv coding of video	wyner ziv;data compression;decoding;video compression;decoding video coding data compression video codecs image reconstruction;video codec;intraframe encoding wyner ziv coding interframe video compression systems predictive coding source coding asymmetric video codec motion video;decoding video compression source coding encoding predictive coding cameras information systems laboratories video codecs motion estimation;video coding;image reconstruction;wyner ziv coding;video codecs;source code;side information;predictive coding	In current interframe video compression systems, the encoder performs predictive coding to exploit the similarities of successive frames. The Wyner-Ziv Theorem on source coding with side information available only at the decoder suggests that an asymmetric video codec, where individual frames are encoded separately, but decoded conditionally (given temporally adjacent frames) could achieve similar efficiency. We report results on a Wyner-Ziv coding scheme for motion video that uses intraframe encoding, but interframe decoding. In the proposed system, key frames are compressed by a conventional intraframe codec and in-between frames are encoded using a Wyner-Ziv intraframe coder. The decoder uses previously reconstructed frames to generate side information for interframe decoding of the Wyner-Ziv frames.	codec;data compression;distributed source coding;encoder;intra-frame coding;jumbo frame;key frame;video	Anne Aaron;Eric Setton;Bernd Girod	2003		10.1109/ICIP.2003.1247383	video compression picture types;data compression;scalable video coding;reference frame;intra-frame;sub-band coding;computer vision;distributed source coding;telecommunications;computer science;deblocking filter;coding tree unit;tunstall coding;block-matching algorithm;multimedia;smacker video;context-adaptive binary arithmetic coding;motion compensation;h.261;statistics;multiview video coding	Vision	45.34268832523381	-17.923113557999184	128585
b40644def85d1555298ff7ef173ee0f8dd7c6cfc	a fast intra prediction algorithm for 360-degree equirectangular panoramic video		360-degree video has been gaining its popularity as virtual reality (VR) technology develops in recent years. To compress these videos using standard video encoders, most of them are converted to a 2D image planar format with equirectangular projection (ERP). Compared with other encoders, High Efficiency Video Coding (HEVC) achieves significant improvements in coding efficiency. However, exhaustive computational complexity of HEVC makes it too time-consuming to compress 360-degree videos of high resolution and high frame rate for VR application. In this paper, a fast intra prediction algorithm is proposed according to the characteristics of ERP format video. Compared with original reference software HM 16.15, experimental results show that the proposed algorithm can achieve about 24.5% encoder time saving on average in All-Intra configuration with negligible quality loss.	360-degree video;algorithm;algorithmic efficiency;computational complexity theory;data compression;erp;encoder;fast fourier transform;high efficiency video coding;ibm systems network architecture;image resolution;intra-frame coding;material point method;raster document object;virtual reality	Yingbin Wang;Yiming Li;Daiqin Yang;Zhenzhong Chen	2017	2017 IEEE Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2017.8305110	theoretical computer science;reference software;encoder;computer vision;artificial intelligence;frame rate;algorithmic efficiency;computer science;computational complexity theory;virtual reality;algorithm;planar;equirectangular projection	Vision	45.378060055705376	-19.752014593481604	128767
2c9d791113e8a5028968847ce2f747c83243d95e	joint image coding and image authentication based on absolute moment block truncation coding	quantization;data storage;image compression;image quality	A joint image coding and image authentication scheme based on absolute moment block truncation coding (AMBTC) is proposed. In the proposed scheme, the authentication data is generated by using the pseudo random sequence. Then, the authentication codes are embedded into the bit maps of AMBTC-compressed image blocks. The embedded bit maps and these quantization levels are further losslessly compressed to cut down the required storage cost. Experimental results demonstrate that the proposed scheme achieves good image quality of the embedded image while keeping good detecting accuracy. © 2013 SPIE and IS&T [DOI: 10.1117/1.JEI	authentication;block truncation coding;code;elegant degradation;embedded system;image processing;image quality;lossless compression;map;pixel;pseudorandomness;random number generation;random seed;sensor;twisted nematic field effect	Yu-Chen Hu;Chun-Chi Lo;Wu-Lin Chen;Chia-Hsien Wen	2013	J. Electronic Imaging	10.1117/1.JEI.22.1.013012	image quality;computer vision;discrete mathematics;quantization;image compression;computer science;theoretical computer science;computer data storage;mathematics;algorithm	Mobile	40.78280244738074	-12.003888159432362	128851
002ce48e918e956037ffe57d5103bf19c0a64c79	matching pursuits video coding: dictionaries and fast implementation	matching pursuit algorithms;traitement signal;evaluation performance;fast implementation;computational load;full search;algoritmo busqueda;decomposition;codecs;performance evaluation;psnr;image processing;implementation cost;data compression;cost function;separable gabor functions;reconstruction quality;algorithme recherche;evaluacion prestacion;search algorithm;video compression;computational complexity data compression video coding search problems;procesamiento imagen;pruned full search algorithm;low complexity;matching pursuits codecs;testing;indexing terms;traitement image;video coding;codificacion;senal video;signal video;computational complexity;discrete cosine transforms;heuristic algorithms;signal processing;dictionaries;coding;low complexity algorithm;video signal;matching pursuit;matching pursuits video coding;dictionary factorization;matching pursuit algorithms video coding dictionaries discrete cosine transforms video compression psnr codecs cost function testing heuristic algorithms;search problems;compresion dato;descomposicion;pruned full search;displaced frame difference;procesamiento senal;pruned full search algorithm matching pursuits video coding dictionaries fast implementation separable gabor functions computational load matching pursuits codecs implementation cost dictionary factorization reconstruction quality;compression donnee;codage	Matching pursuits over a basis of separable Gabor functions has been demonstrated to outperform DCT methods for displaced frame difference coding for video compression. Unfortunately, apart from very low bit-rate applications, the algorithm involves an extremely high computational load. This paper contains original contribution to the issues of dictionary selection and fast implementation for matching pursuits video coding. First, it is shown that the PSNR performance of existing matching pursuits codecs can be improved and the implementation cost reduced by a better selection of dictionary functions. Secondly, dictionary factorization is put forward to further reduce implementation costs. A reduction of the computational load by a factor of 20 is achieved compared to implementations reported to date. For a majority of test conditions, this reduction is supplemented by an improvement in reconstruction quality. Finally, a pruned full-search algorithm is introduced, which offers significant quality gains compared to the better-known heuristic fast-search algorithm, while keeping the computational cost low.	algorithmic efficiency;codec;computation;data compression;data dictionary;discrete cosine transform;gabor atom;heuristic;peak signal-to-noise ratio;search algorithm	Przemyslaw Czerepinski;Colin Davies;Cedric Nishan Canagarajah;David R. Bull	2000	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.875515	data compression;computer vision;image processing;computer science;theoretical computer science;signal processing;pattern recognition;algorithm;statistics	Vision	46.3435056260803	-15.673986082369941	129001
1b4fa4caa4609c291e5705b6e05a166cce576752	information preserving image compression for archiving nmr images	radiology;image storage;gray code;prediction error;image coding;computed tomography;image resolution;huffman encoding biomedical nmr encoding computerised picture processing image compression information preserving compression lynch davisson coding linear predictive coding prediction error sequences gray code bit planes;biomedical nmr;nuclear magnetic resonance;reflective binary codes;image coding nuclear magnetic resonance spatial resolution image resolution radiology computed tomography image reconstruction image storage linear predictive coding reflective binary codes;linear prediction coding;linear predictive coding;image compression;gray code bit planes;image reconstruction;lynch davisson coding;information preserving compression;prediction error sequences;computerised picture processing;compression ratio;encoding;predictive coding;huffman encoding;encoding biomedical nmr computerised picture processing;spatial resolution	This paper presents a result on information preserving compression of NMR images for the archiving purpose. Both Lynch-Davisson coding and linear predictive coding have been studied. For NMR images of 256 x 256 x 12 resolution, the Lynch-Davisson coding with a block size of 64 as applied to prediction error sequences in the Gray code bit planes of each image gave an average compression ratio of 2.3:1 for 14 testing images. The predictive coding with a third order linear predictor and the Huffman encoding of the prediction error gave an average compression ratio of 3.1:1 for 54 images under test, while the maximum compression ratio achieved was 3.8:1. This result is one step further toward the improvement, albeit small, of the information preserving image compression for medical applications.	archive;block size (cryptography);huffman coding;image compression;kerrison predictor;linear predictive coding	Ching-Chung Li;Muhittin Gokmen;A. D. Hirschman;Yue Wang	1988	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1109/ICPR.1988.28493	data compression;color cell compression;computer vision;data compression ratio;speech recognition;image resolution;block truncation coding;computer science;theoretical computer science;lossless compression;computed tomography;texture compression	ML	43.577140383198724	-16.442104948819754	129080
70519c21420a5aae2acf6fca34a13f326819a159	transformed-domain mode selection for h.264 intra-prediction improvement	algorithme rapide;circuit codeur;degradation;tecnologia electronica telecomunicaciones;detection forme;coding circuit;selection mode;complexite calcul;edge detection;h 264 avc;degradacion;endommagement;shape detection;estimation algorithm;deterioracion;intra prediction;mode selection;deteccion contorno;avc;detection contour;deteccion forma;codificacion;complejidad computacion;computational complexity;fast algorithm;circuito codificacion;coding;h 264;rapport signal bruit;relacion senal ruido;tecnologias;damaging;signal to noise ratio;grupo a;mode decision;algoritmo rapido;codage	In this paper, a fast mode decision method for intra-prediction is proposed to reduce the computational complexity of H.264/AVC encoders. With edge information, we propose a novel fast estimation algorithm to reduce the computation overhead of H.264/AVC for mode selection, where the edge direction of each coding block is detected from only part of the transformed coefficients. Hence, the computation complexity is greatly reduced. Experimental results show that the proposed fast mode decision method can eliminate about 81.34% encoding time for all intra-frame sequences with acceptable degradation of averaged PSNR and bitrates.	h.264/mpeg-4 avc	Yung-Chiang Wei;Jar-Ferr Yang	2008	IEICE Transactions	10.1093/ietisy/e91-d.3.825	computer vision;degradation;edge detection;telecommunications;computer science;mathematics;coding;computational complexity theory;signal-to-noise ratio;algorithm	DB	46.710198268181855	-15.247668800479259	129371
f23c5cf64b1bfed1e6f5666649ed60bdf95918c7	a high payload histogram-based reversible wartermarking using linear prediction	histograms;watermarking;watermarking histograms prediction algorithms pixel payloads signal processing algorithms economic indicators;prediction error;image coding;watermarking copyright image coding prediction theory;histogram shifting;prediction image;data embedding;prediction errror histogram;prediction algorithms;copyright;secret information;adjacent pixels linear prediction secret information digital work payload reversible watermarking algorithm prediction error histogram shifting data embedding prediction image;histogram shifting watermarking optimal linear prediction prediction errror histogram;linear predictive;payload reversible watermarking algorithm;copyright protection;prediction theory;pixel;payloads;linear prediction;optimal linear prediction;digital image;adjacent pixels;signal processing algorithms;digital work;economic indicators	Reversible watermarking is an efficient method of embedding secret information into digital work for the purpose of copyright protection, certification, tracking, etc. This paper presents a high payload reversible watermarking algorithm for digital images using linear prediction. Prediction error and histogram shifting are used for embedding data. Different from previous methods, we utilize linear prediction to obtain the initial prediction, which is then modified by calculating the variance of adjacent pixels to get a more accurate prediction image. Since the prediction-error histogram is more concentrated, the proposed method can achieve larger capacity and less distortion by using histogram shifting. The experimental results indicate the validity of the proposed method compared with other schemes.	algorithm;digital image;digital watermarking;distortion;ibm notes;image histogram;numerical weather prediction;peak signal-to-noise ratio;pitch shift;pixel	Bo Ou;Yao Zhao;Rongrong Ni;Gang Cao	2010	2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIHMSP.2010.114	computer vision;payload;prediction;linear prediction;digital watermarking;computer science;histogram matching;theoretical computer science;economic indicator;mean squared prediction error;data mining;histogram;digital image;pixel;statistics	Robotics	40.8700603650818	-12.543001789205432	129467
2b8023b1c6b9d52e6cb8e23ef154676279fb85ec	regularity scalable image coding based on wavelet singularity detection	g740 computer vision;image coding;regularity;feature based scalable coding;separable wavelet transform	In this paper, we propose an adaptive algorithm for scalable wavelet image coding, which is based on the general feature, the regularity, of images. In pattern recognition or computer vision, regularity of images is estimated from the oriented wavelet coefficients and quantified by the Lipschitz exponents. To estimate the Lipschitz exponents, evaluating the interscale evolution of the wavelet transform modulus sum (WTMS) over the directional cone of influence was proven to be a better approach than tracing the wavelet transform modulus maxima (WTMM). This is because the irregular sampling nature of the WTMM complicates the reconstruction process. Moreover, examples were found to show that the WTMM representation cannot uniquely 2 C. characterize a signal. It implies that the reconstruction of signal from its WTMM may not be consistently stable. Furthermore, the WTMM approach requires much more computational effort. Therefore, we use the WTMS approach to estimate the regularity of images from the separable wavelet transformed coefficients. Since we do not concern about the localization issue, we allow the decimation to occur when we evaluate the interscale evolution. After the regularity is estimated, this information is utilized in our proposed adaptive regularity scalable wavelet image coding algorithm. This algorithm can be simply embedded into any wavelet image coders, so it is compatible with the existing scalable coding techniques, such as the resolution scalable and signal-to-noise ratio (SNR) scalable coding techniques, without changing the bitstream format, but provides more scalable levels with higher peak signal-to-noise ratios (PSNRs) and lower bit rates. In comparison to the other feature-based wavelet scalable coding algorithms, the proposed algorithm outperforms them in terms of visual perception, computational complexity and coding efficiency.	adaptive algorithm;algorithmic efficiency;bitstream format;coefficient;computation;computational complexity theory;computer vision;data compression;decimation (signal processing);embedded system;maxima;modulus of continuity;pattern recognition;sampling (signal processing);scalability;signal-to-noise ratio;wavelet transform	Charlotte Yuk-Fan Ho;Tai-Chiu Hsung;Daniel Pak-Kong Lun;Bingo Wing-Kuen Ling;Peter Kwong-Shun Tam;Wan-Chi Siu	2008	Int. J. Image Graphics	10.1142/S0219467808003003	mathematical optimization;discrete mathematics;theoretical computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;wavelet transform	Vision	43.90398536013835	-15.79336009751032	129477
5a80a08fd4225c7e8321e0faed4ae8330897c84f	a locally temporal adaptive transform scheme for sub-band video coding	3d subband;distortion spreading adaptive transform scheme sub band video coding 3d wavelet transform 2d wavelet spatial domain temporal dimension;distortion video coding wavelet transforms transform coding;lapped transform;video coding wavelet transforms humans image reconstruction adaptive signal processing video signal processing wavelet domain streaming media scalability robustness;transform coding;local adaptation;wavelet transforms;video coding;distortion;adaptivity;wavelet transform;video representation;lts2	The work presented in this paper extends the concept of sub-band video coding based on a 3D wavelet transform to a more adaptive approach. A formal comparison is presented between the performances inferred by the use of the 3D wavelet transform and the use of a 2D wavelet in the spatial domain extended by a locally adaptive transform in the temporal dimension. Some advantages are foreseen for the new scheme since it is able to better deal with certain signal models like appearing and moving edges. An increased control of the distortion spreading is expected and consequently a lower visual impact relevance.	data compression;distortion;performance;relevance;wavelet transform	Òscar Divorra Escoda;Pierre Vandergheynst	2003		10.1109/ICASSP.2003.1199557	wavelet;sub-band coding;computer vision;transform coding;speech recognition;s transform;harmonic wavelet transform;lapped transform;second-generation wavelet transform;continuous wavelet transform;theoretical computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;wavelet transform	Vision	44.44430689101267	-15.76787253407998	129553
12f2192b1f243b100197ea0775a4f8cf654c391a	fast channel switching based on svc in iptv environments	internet protocol;access network;video streaming;scalable video coding;data compression;decoding;svc;video compression;streaming media switches static var compensators decoding iptv bandwidth delay;channel switching time;video streaming data compression iptv video coding;algorithm;video coding;enhancement layer;technology and engineering;streaming media;zapping time;bandwidth;static var compensators;channel switching speed fast channel switching svc configuration iptv environments internet protocol television video compression random access consumption bandwidth consumption channel switching performance scalable video coding enhancement layer video stream base layer h 264 avc bandwidth reduction access network;switches;static var compensator;random access;zapping speed;transition period;iptv;core network;base layer;steady state;zapping speed channel switching time iptv svc	In Internet Protocol TeleVision (IPTV) systems, the use of video compression results in a trade-off between random access and bandwidth consumption. Most of the time, priority is given to compression efficiency, resulting in low channel switching performance. In this paper, a Scalable Video Coding (SVC) configuration is proposed, wherein base and enhancement layer are coded in such a way that fast channel switching can be offered without impacting the access network bandwidth consumption. This is achieved by encoding a full quality slow channel switching version of the video stream in the base layer extended with a reduced quality fast switching enhancement layer. Additionally, this configuration offers backward compatibility with H.264/AVC, enabling a gradual upgrade of the IPTV system from H.264/AVC to SVC. When comparing this technique to single layer H.264/AVC, 25.0% bandwidth reduction on the access network is obtained without impacting channel switching speed, but only at the cost of a slightly reduced quality ( -1 dB PSNR) during a short transition period (maximum 48 frames). If a comparison is made with a regular SVC configuration, 39.4% bandwidth reduction on the access network during steady state is obtained. Compared to simulcast H.264/AVC, 32.0% bandwidth reduction on the core network is observed.	access network;backward compatibility;data compression;h.264/mpeg-4 avc;iptv;peak signal-to-noise ratio;random access;scalable video coding;simulcast;steady state;streaming media;thyristor	Glenn Van Wallendael;Wim Van Lancker;Jan De Cock;Peter Lambert;Jean-François Macq;Rik Van de Walle	2012	IEEE Transactions on Broadcasting	10.1109/TBC.2011.2170610	data compression;scalable video coding;real-time computing;telecommunications;computer science;label switching;statistics;computer network	Mobile	45.4227100774911	-20.455555202124508	129678
a4c7f98d5c502dba07b6df7116971c713f317c55	adaptive steganography method based on two tiers pixel value differencing		The pixel value differencing (PVD) scheme provided high embedding payload with imperceptibility in the stego images. In their approach, they used two pixels differencing to represent the complexity of pixels, and applied it to estimate how many bit will be hidden into. As the difference with small value, it means that two pixels can not tolerated with larger change, therefore, few secret bit should be embedded into these pixels. PVD scheme did not completely take pixel tolerance into consideration because of only applying one criterion, pixel differencing. In this paper, a new data hiding scheme using PVD operation and incorporating with pixel tolerance into a cover image is proposed. The pixel tolerance indicates that a greater pixel-value is more change of gray-value could be tolerated. Following up this idea, our proposed scheme applies a threshold (TH) and two quantization tables to hide secret data into a block with two pixels using modified k-bits LSB. The number of k-bits is adaptive and depends on the quantization tables setting. The adjustment strategy is to maintain the differencing value in the same range. The experimental results show that our scheme is superior to those in the previous literature.	autoregressive integrated moving average;pixel;steganography	Chi-Yao Weng;Yen-Chia Huang;Chin-Feng Lee;Dong-Peng Lin	2017		10.1007/978-981-10-6487-6_14	steganography;pixel;mathematical optimization;information hiding;quantization (signal processing);computer science;embedding;least significant bit	Vision	40.631090495770486	-12.156044915341871	129884
3a57a1db1d5e2538c95328fa421e3a4385bb45ac	a new way to reduce candidate blocks for block matching motion estimation	image sequences image matching video coding data compression motion estimation search problems iterative methods computational complexity;motion estimation signal processing algorithms iterative algorithms sampling methods video signal processing australia computer science computational efficiency;data compression;search space;image matching;motion estimation;iterative algorithm;iterative methods;video coding;computational complexity;block matching;search problems;image sequences candidate blocks reduction block matching motion estimation block matching algorithm average luminance fast iterative algorithm absolute error search window search space reduction complexity simulation results video encoders;block matching algorithm;image sequences	A new way to reduce candidate blocks for the block matching algorithm is proposed. It consists of three steps: (1) compute the average luminance of all the concerned blocks by a fast iterative algorithm; (2) calculate the absolute error between the average luminance of the currently concerned block and that of the candidate block within the search window in the previous frame; and (3) remove those candidate blocks with large difference. Then the most similar block will be found by searching the remaining candidate blocks. The complexity of reducing the search space is very small, but it can remove candidate blocks from the search window efficiently and effectively. The simulation results are given to demonstrate the effectiveness of this new algorithm.	motion estimation	Xiangyang Xue;Hangzai Luo;Xueqing Chen;Lide Wu	1999		10.1109/ISSPA.1999.818166	computer vision;mathematical optimization;computer science;theoretical computer science;mathematics;iterative deepening depth-first search;iterative method;algorithm;statistics	Robotics	48.68913544894883	-19.278831559933543	129931
c2535a98f0bcbf0dfefb775f7b1ef4b3a93866eb	a segmentation-based chroma intra prediction coding scheme for h.264/avc		In this paper, we propose a novel segmentation-based intra prediction coding scheme for low-bitrate video coding. Different coding schemes are separately designed for the luma and chroma components in our proposed method. The traditional block-based coding scheme is still used for the luma components, and the segmentation-based coding scheme is developed for the chroma components. The segmentation operation is used for the reconstructed luma components, which groups similar pixels together and produces a set of homogenous regions. Here, these local and homogenous regions are referred to superpixels. By utilizing the spatial correlation between the luma and chroma planes, we transfer the segmentation result of the luma components to the chroma components, which will not induce any side information in the chroma intra prediction coding. Instead of using the macroblock (MB) as the coding unit, the proposed method implements the chroma intra prediction in each superpixel, and the original pixels in each superpixel are employed to substitute the neighboring reconstructed samples in the prediction process. The experimental results show that the proposed method can achieve an average 0.20 dB and up to 0.63 dB coding gains in comparison to the directional intra prediction scheme for H.264/AVC low-bitrate video coding.	h.264/mpeg-4 avc	Qingbo Wu;Jian Xiong;Bing Luo;Zhengning Wang	2014	CSSP	10.1007/s00034-013-9675-3	luma;computer vision;telecommunications;coding tree unit;mathematics;multimedia	NLP	44.541403353581146	-17.198391705506115	130045
9bc21d53d02cb8c6f247fffd432f6644ba9d0506	a backward compatible hdr encoding scheme	tone mapping;social networking;enhanced museum experience;edutainment;new genres of entertainment technology	Two-layer encoding schemes for HDR images (and video) can not only reduce the storage requirements, but more importantly they can also ensure backward compatibility during transition from LDR to HDR age. The first layer is a tone-mapped LDR image, which can be shown on existing displays. The second layer is another LDR image and contains the residual information lost in tone-mapping, which can be used by HDR applications.	backward compatibility;high-dynamic-range imaging;ldraw;line code;requirement;tone mapping	Ishtiaq Rasool Khan	2010		10.1145/1836845.1836915	tone mapping;simulation;computer science;multimedia;social network;computer graphics (images)	Security	43.43263999051105	-20.23299709763711	130088
7829ac4b9cd8e91d18419cfdd5bb583eedd6ea1c	joint estimation and optimum encoding of depth field for 3-d object-based video coding	moving object;encoding video coding motion estimation image coding video compression noise robustness parameter estimation cost function vectors electronic mail;data compression;motion compensation;video compression motion estimation optimum encoding dense depth fields 3d object based video coding 3d motion models image frames 2d motion compensation rate distortion theory moving objects intensity frames;video compression;motion estimation;object based video coding;image sequences data compression video coding motion compensation motion estimation rate distortion theory parameter estimation;motion compensated;rate distortion theory;video coding;parameter estimation;image sequences	3-D motion models can be used to remove temporal redundancy between image frames. For efficient encoding using 3-D motion information, apart from the 3-D motion parameters, a dense depth field must also be encoded to achieve 2-D motion compensation on the image plane. Inspired by rate-distortion theory, a novel method is proposed to optimally encode the dense depth fields of the moving objects in the scene. Using two intensity frames and 3-D motion parameters as inputs, an encoded depth field can be obtained by jointly minimizing a distortion criteria and a bit-rate measure. Since the method gives directly an encoded field as an output, it does not require an estimate of the field to be encoded. By efficiently encoding the depth field during the experiments, it is shown that the 3-D motion models can be used in object based video compression algorithms.	data compression;object-based language	A. Aydin Alatan;Levent Onural	1996		10.1109/ICIP.1996.561043	video compression picture types;data compression;reference frame;inter frame;computer vision;structure from motion;quarter-pixel motion;computer science;motion interpolation;theoretical computer science;video tracking;motion estimation;mathematics;block-matching algorithm;multimedia;rate–distortion optimization;context-adaptive binary arithmetic coding;motion field;motion compensation;statistics;multiview video coding	Vision	49.104985459527434	-17.540257410483267	130113
af2e1d2c57cf3db703809f443bd120a6260ee1f9	introducing a watermarking with a multi-objective genetic algorithm	watermarking;multi objective;discrete cosine transform;multi objective genetic algorithm;genetic algorithm;evolutionary algorithm;multi objective optimization problem	We propose an evolutionary algorithm for the enhancement of digital semi-fragile watermaking based on the manipulation of the image discrete cosine transform (DCT). The algorithm searches for the optimal localization of the DCT of an image to place the mark image DCT coefficients. The problem is stated as a multi-objective optimization problem (MOP), that involves the simultaneous minimization of distortion and robustness criteria.	coefficient;discrete cosine transform;distortion;evolutionary algorithm;genetic algorithm;mathematical optimization;multi-objective optimization;optimization problem;robustness (computer science);semiconductor industry;watermark (data file)	Diego Sal Díaz;Manuel Graña	2005		10.1145/1068009.1068383	mathematical optimization;genetic algorithm;digital watermarking;computer science;artificial intelligence;theoretical computer science;machine learning;evolutionary algorithm;discrete cosine transform;mathematics	AI	41.121822111990326	-9.982885179202349	130158
25db301d19da9456e351562c6359f9f105082621	block adaptive interpolation filter using trained dictionary for sub-pixel motion compensation	adaptive interpolation filter;interpolation;image resolution;motion compensation;super resolution adaptive interpolation filter sub pel motion compensation;optimal filtering;up scaled pixels block adaptive interpolation filter sub pixel motion compensation sub pel motion compensation key techniques itu t key technology area kta codec coding efficiency frame based update strategy switched interpolation filter block adaptive filtering advanced block adaptive interpolation filtering well trained dictionary optimized filter coefficients learning based super resolution quarter pel motion compensation half pel accuracy quarter pel accuracy;training;sub pel motion compensation;motion compensated;video coding adaptive filters image resolution interpolation learning artificial intelligence motion compensation;video coding;adaptive filters;pixel;dictionaries;interpolation dictionaries wiener filter motion compensation pixel encoding training;super resolution;wiener filter;learning artificial intelligence;encoding;adaptive filter	Adaptive interpolation filtering for sub-pel motion compensation is one of key techniques of ITU-T key technology area (KTA) codec. However, the adaptive interpolation filtering has a limitation in coding efficiency because of its frame-based update strategy of filter coefficients. Although switched interpolation filter with offset is presented as a sort of block-adaptive filtering for KTA codec, its coding efficiency is generally lower than that of the best adaptive interpolation filter. In order to overcome such a problem, this paper presents an advanced block-adaptive interpolation filtering using well-trained dictionaries which store optimized filter coefficients. We derive those filter coefficients by using learning-based super-resolution. The proposed block-adaptive interpolation filtering for quarter-pel motion compensation consists of two steps: up-scaling of half-pel accuracy and subsequent up-scaling of quarter-pel accuracy. The dictionary optimized for each step is employed to produce the precise up-scaled pixels. Simulation results show that the proposed algorithm improves higher coding efficiency than the previous adaptive interpolation filters for KTA.	adaptive filter;algorithm;algorithmic efficiency;blu-ray;codec;coefficient;computational complexity theory;dictionary;image scaling;motion compensation;pixel;ruby document format;simulation;super-resolution imaging;whittaker–shannon interpolation formula;whole earth 'lectronic link	Jaehyun Cho;Shin-Cheol Jeong;Dong-Bok Lee;Byung Cheol Song	2012	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2011.2160601	demosaicing;adaptive filter;computer vision;bilinear interpolation;kernel adaptive filter;computer science;stairstep interpolation;root-raised-cosine filter;control theory;statistics;image scaling	Vision	45.37083055079796	-17.472777245902265	130220
692acb0060a24e83e8d6a23161ec0fe7d25c4c76	joint jpeg compression and detection resistant performance enhancement for adaptive steganography using feature regions selection		Since it is difficult to acquire a strong JPEG compression resistant ability while achieving a good detection resistant performance for current information hiding algorithms, a JPEG compression and detection resistant adaptive steganography algorithm using feature regions is proposed. Based on the proposed feature region extraction and selection algorithms, the embedding domain robust to JPEG compression and containing less embedding distortion can be obtained. Utilizing the current distortion functions, the distortion value of DCT coefficients in the embedding domain can be calculated. Combined with error correct coding and STCs, the messages are embedded into the cover images with minimum embedding distortion, and can be extracted with high accuracy after JPEG compression, hence, the JPEG compression and detection resistant performance are enhanced at the same time. The experimental results demonstrate that comparing with current J-UNIWARD steganography under quality factor 85 of JPEG compression, the extraction error rates decrease from above 20 % to nearly 0, while the stego images remain a better detection resistant performance comparing with the current JPEG compression and detection resistant adaptive steganography algorithm.	coefficient;discrete cosine transform;distortion;embedded system;error message;jpeg;selection algorithm;steganography	Yi Zhang;Xiangyang Luo;Chunfang Yang;Fenlin Liu	2016	Multimedia Tools and Applications	10.1007/s11042-016-3914-0	data compression;lossless jpeg;computer vision;computer science;theoretical computer science;pattern recognition;jpeg;quantization	ML	40.20962735705167	-11.681139022479355	130606
5c0053f4367e446e061fafd1b4778339946ac147	selective encryption for hierarchical mpeg	estensibilidad;distributed system;streaming;systeme reparti;multimedia;encryption;transmision continua;mpeg video;securite informatique;cifrado;computer security;transmission en continu;compression image;sistema repartido;senal video;signal video;image compression;cryptage;criptografia;cryptography;seguridad informatica;video signal;codec;cryptographie;extensibilite;scalability;selective encryption;compresion imagen	Selective encryption of visual data and especially MPEG has attracted a considerable number of researchers in recent years. Scalable visual formats are offering additional functionality, which is of great benefit for streaming and networking applications. The MPEG-2 and MPEG-4 standards provide a scalability profile in which a resolution scalable mode is specified. In this paper we evaluate a selective encryption approach on the basis of our hierarchical MPEG video codec.	codec;confidentiality;distortion;encryption;framing (world wide web);group of pictures;jpeg;mpeg-2;moving picture experts group;scalability;streaming media;video compression picture types	Heinz Hofbauer;Thomas Stütz;Andreas Uhl	2006		10.1007/11909033_14	embedded system;codec;real-time computing;scalability;telecommunications;image compression;computer science;cryptography;computer security;encryption	Graphics	47.325645979269325	-14.325857027752699	130935
a82170f0923d2d774a4a65096a0f66af37d59c86	a watermarking scheme based on the characteristic of addition among dct coefficients	transformation cosinus;transformacion discreta;systeme protection;geometric transformation;etude theorique;low frequency;correction erreur;filaments;effet frequence;probabilistic approach;frequency effect;transformation orthogonale;filament;methode domaine frequence;frequency domain method;enfoque probabilista;approche probabiliste;error correction;robustesse;transformacion geometrica;transformacion coseno;discrete transformation;basse frequence;transformation geometrique;protection system;robustness;baja frecuencia;orthogonal transformation;metodo dominio frecuencia;theoretical study;cosine transform;sistema proteccion;orthogonal transformations;transformation discrete;efecto frecuencia;robustez	Generally, low frequency domain may be useful to embed a watermark in an image. However, if a watermark is embedded into low frequency components, blocking effects may occur in the image. Then considering blocking effects, we study some characteristics among DCT coefficients and find some interesting mutual relations. Here, the robustness of many schemes based on the orthogonal transformations such as DCT may be doubtful against geometric transformations. For the distortions produced by some geometric transformations, we propose a searching protocol to find the watermarked block which is rotated and shifted. In the proposed scheme, a watermark can remain in the attacked image with very high probability in our scheme. Further, the watermark becomes more robust than the above scheme using error-correction.	blocking (computing);coefficient;digital watermarking;discrete cosine transform;distortion;embedded system;error detection and correction;simulation;watermark (data file)	Minoru Kuribayashi;Hatsukazu Tanaka	2000		10.1007/3-540-44456-4_1	error detection and correction;geometric transformation;computer science;calculus;discrete cosine transform;mathematics;geometry;low frequency;programming language;galaxy filament;robustness;orthogonal transformation	ML	43.90435153320283	-11.196568487835446	130959
ec5753926e12dc40255fe502c0d45c2d5198e5d7	computer model of steganographic system based on contraction mapping with stream audio container		In this paper digital steganographic system based on contraction mapping is considered. The proposed steganographic algorithm uses two channels to achieve informational redundancy, which allow decoding by using an algorithm invariant to container signal. This peculiarity is excellent in case of information hiding in chaotic signal. This paper is devoted to the illustration of algorithm performance with the stream audio container.	algorithm;computer simulation;contraction mapping;numerical weather prediction;steganography	Maxim V. Shakurskiy;Victor K. Shakurskiy;Vladimir I. Volovach	2016	2016 IEEE East-West Design & Test Symposium (EWDTS)	10.1109/EWDTS.2016.7807709	simulation;computer science;theoretical computer science;computer graphics (images)	Embedded	42.058540852476014	-15.557502889661777	131718
d15256d201533c3cb94bf9709d0e9f02b1e8ad89	quality-efficient upsampling method for asymmetric resolution stereoscopic video coding with interview motion compensation and error compensation	image resolution;decoding stereo image processing bit rate video sequences error compensation video coding encoding;wiener filters;irregular textures quality efficient upsampling method asymmetric resolution stereoscopic video coding interview motion compensation error compensation arsvc bandwidth limited channels wiener filter based method;image texture;video coding;stereo image processing;wiener filters image resolution image texture stereo image processing video coding visual perception;wiener filter 6 tap filter asymmetric resolution stereoscopic video coding arsvc bitrate reduction error compensation interview prediction three dimensional television 3 d tv upsampling;visual perception	In asymmetric resolution stereoscopic video coding (ARSVC), to reduce the bitrate required for bandwidth-limited channels, each downsampled right-view frame is a quarter the size of the corresponding left-view frame and will be upsampled to the original size by the decoder. In this paper, two upsampling methods for ARSVC are proposed. The first proposed method integrates the traditional Wiener filter-based method and the interview prediction scheme by incorporating the information from the similarity between the left-view and right-view frames. By compensating for the prediction errors, the second proposed method further improves the quality of the upsampled images obtained by the first proposed method, especially in the sequences with heavy irregular textures.	data compression;decimation (signal processing);motion compensation;overhead (computing);peak signal-to-noise ratio;ruby document format;stereoscopic video coding;upsampling;wiener filter	Kuo-Liang Chung;Yong-Huai Huang;Wen-Chang Liu	2014	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2013.2276877	image texture;computer vision;image resolution;visual perception;computer science;multimedia;motion compensation;multiview video coding;computer graphics (images)	Vision	44.98284482441982	-17.95648407329537	131754
c5b4df955065629dc54d5dc4daae8eec11ad6225	rate-complexity tradeoff for client-side free viewpoint image rendering	dynamic programming;video signal processing;video signal processing dynamic programming image texture rendering computer graphics;optimizations rate complexity tradeoff client side free viewpoint image rendering free viewpoint video image synthesis depth image based rendering dibr image texture depth maps auxiliary information ai dynamic programming computation expensive exemplar block search;image texture;computation complexity interactive multiview video depth image based rendering;rendering computer graphics	Free viewpoint video enables a client to interactively choose a viewpoint from which to synthesize an image via depth-image-based rendering (DIBR). However, synthesizing a novel viewpoint image using texture and depth maps from two nearby views entails a sizable computation overhead. Further, to reduce transmission rate, recent proposals synthesize the second reference view itself using texture and depth maps of the first reference view via a complex inpainting algorithm to complete large disocclusion holes in the second reference image-a small amount of auxiliary information (AI) is transmitted by sender to aid the inpainting process-resulting in an even higher computation cost. In this paper, we study the optimal tradeoff between transmission rate and client-side complexity, so that in the event that a client device is computation-constrained, complexity of DIBR-based view synthesis can be scalably reduced at the expense of a controlled increase in transmission rate. Specifically, for standard view synthesis paradigm that requires texture and depth maps of two neighboring reference views, we design a dynamic programming algorithm to select the optimal subset of intermediate virtual views for rendering and encoding at server, so that a client performs only video decoding of these views, reducing overall view synthesis complexity. For new view synthesis paradigm that synthesizes the second reference view itself from the first, we optimize the transmission of AI used to assist inpainting of large disocclusion holes, so that some computation-expensive exemplar block search operations are avoided, reducing inpainting complexity. Experimental results show that the proposed schemes can scalably and gracefully reduce client-side complexity, and the proposed optimizations achieve better rate-complexity tradeoff than competing schemes.	algorithm;client-side;computation;dynamic programming;inpainting;interactive media;jump search;map;overhead (computing);programming paradigm;server (computing);video decoder;view synthesis	Yu Gao;Gene Cheung;Jie Liang	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738389	image texture;computer vision;rendering;computer science;theoretical computer science;dynamic programming;texture memory;computer graphics (images)	Vision	46.59386186379079	-22.599683448100393	131969
6dc99931b08f04aa7004b1c0fa5ac133194b3fcf	watermarking technique for color halftone images	watermarking;quadratic programming;color halftone images;image coding;image colour analysis data encapsulation watermarking image coding;data mining;visual quality;gray scale;visual quality watermarking color halftone images error diffused halftone images pattern embedding color components hidden patterns boolean operation overlaid images;data encapsulation;hidden patterns;steganography;boolean operation;gold;stochastic processes;watermarking data encapsulation pixel sun gold data mining steganography gray scale stochastic processes quadratic programming;image colour analysis;pixel;pattern embedding;sun;color components;error diffused halftone images;overlaid images	In this paper, we propose a method to hide invisible patterns in color error diffused halftone images. The hidden pattern is embedded in different color components. The hidden patterns would be revealed when the watermarked color halftone images are under Boolean operation or overlaid. Simulation results show that the watermarked color halftone images have good visual quality and the hidden pattern is visible clearly.	digital watermarking;embedded system;simulation;watermark (data file)	Ming Sun Fu;Oscar C. Au	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326561	gold;computer vision;digital watermarking;computer science;pattern recognition;mathematics;steganography;quadratic programming;pixel;grayscale;statistics;computer graphics (images)	Robotics	41.46594384387337	-12.018443529926131	132090
e201c4252518bebfcd61df38b67d20487e39563b	adaptive stopping strategies for fast intra mode decision in hevc		Abstract Fast intra mode decision strategies are proposed to overcome the brute force mode decision for the coding unit (CU) in High Efficiency Video Coding (HEVC). The proposed work improves the rough-mode-decision (RMD) by initializing the candidate intra mode list using the fusion of the Hadamard-cost and the statistical-inference formed using spatial/ temporal correlations. Then an early termination is predicted using optimal stopping theory that addresses early decision for a generic class of decision problems. Subsequently, a novel RD-cost prediction model is developed for early termination that is based on the RD-cost variation in the neighboring CUs with-respect-to their co-located CUs. Experimental results demonstrate that the RMD module of HEVC and the state-of-the-art fast intra mode prediction published method are outperformed by saving up to 0.61% and 0.91% Bjontegaard delta bit rate (BDBR) on average, respectively.	high efficiency video coding	Junaid Tariq;Sam Kwong	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2017.12.008	pattern recognition;mathematics;artificial intelligence;initialization;decision problem;optimal stopping	Vision	47.18091014927046	-19.75433360593147	132099
b7f1fc5aa14be4dc6e57952d0ef314e372165323	a robust information hiding methodology in wavelet domain	information hiding;jpeg2000;robustness;capacity;wavelet	With the proliferation of digital multimedia, information hiding techniques have become more attractive to the researchers. High bitrate information hiding is different from digital watermarking: the former one tries to hide relatively large amount of auxiliary information instead of just one or a few verification bits. This paper presents a novel Discrete Wavelet Transform (DWT) domain high bitrate information hiding algorithm. In the proposed algorithm, the coefficients within the approximation subband of the one-level wavelet decomposition are grouped into vectors to embed information bits. Low-frequency coefficients have been chosen for information hiding due to their relatively large amplitudes and the corresponding smaller step size in JPEG2000 quantization. A mathematical model is proposed to predict the Bit-Error-Rate (BER) of the algorithm under JPEG2000 compression. The proposed algorithm has reached an initial channel capacity of 1/64 bits per pixel (bpp). With performance improvements, the channel capacity has been increased to 1/32bpp and 1/16bpp, at the price of slightly higher BER. Preliminary experiments indicate that the algorithm is robust and most of the hidden information survives JPEG2000 compression.	wavelet	Ming Yang;Monica Trifas;Nikolaos G. Bourbakis;C. Cushing	2007			speech recognition;theoretical computer science;data mining;mathematics	Vision	42.42103608519727	-13.558655746771846	132157
5c239c28f9afafc420dcda5f3bc0b76be0eff165	paper summary: time-bounded adaptive a		This paper summarizes our AAMAS 2012 paper on ”TimeBounded Adaptive A*,” which introduces the game time model to evaluate search algorithms in real-time settings, such as video games. It then extends the existing real-time search algorithm TBA* to path planning with the freespace assumption in initially partially or completely unknown terrain, resulting in Time-Bounded Adaptive A* (TBAA*). TBAA* needs fewer time intervals in the game time model than several state-of-the-art complete and real-time search algorithms and about the same number of time intervals as the best compared complete search algorithm, even though it has the advantage over complete search algorithms that the agent starts to move right away.	a* search algorithm;international conference on autonomous agents and multiagent systems;motion planning;real-time search;real-time clock;real-time locating system;real-time web	Carlos Hernández;Jorge A. Baier;Tansel Uras;Sven Koenig	2012				AI	53.2391849641857	-23.18678770339337	132178
965203adab092664ebe08da136c07a768a9c2b64	commutative reversible data hiding and encryption	image encryption;image recovery;reversible data hiding	This work proposes a novel scheme of commutative reversible data hiding and encryption. In encryption part, the gray values of two neighboring pixels are masked by same pseudo-random bits. In data-hiding part, the additional data are embedded into various bit planes with a reversible manner, and a parameter optimization method based on a capacity–distortion criterion is used to ensure a good performance. Because the data space used for accommodating the additional data is not affected by the encryption operation, the data embedded in plain/encrypted domain can be extracted from encrypted/plain domain, and the way of insertion/extraction of additional data in plain domain is same as that in encrypted domain. Furthermore, the original image can be recoveredwithout any error from an image containing additional data. Copyright © 2013 JohnWiley & Sons, Ltd.	dataspaces;distortion;embedded system;encryption;hidden surface determination;mathematical optimization;pixel;pseudorandomness;the sons of heaven	Xinpeng Zhang	2013	Security and Communication Networks	10.1002/sec.742	multiple encryption;40-bit encryption;theoretical computer science;link encryption;filesystem-level encryption;on-the-fly encryption;internet privacy;computer security;encryption;probabilistic encryption	Graphics	39.1937349377755	-10.783601995126185	132335
9fcdb1a7f00fcb2e9e676f3cb726d8bdc67dcf79	fpga implementation of a predictive vector quantization image compression algorithm for image sensor applications	fpga implementation predictive vector quantization image compression algorithm for image sensor applications;compression algorithm;quantization;image coding;data compression;bepress selected works;implementation;predictive;image;fpga;image sensors;differential pulse code modulation;vector quantisation differential pulse code modulation field programmable gate arrays image coding image sensors;chip;algorithm;fpga implementation;vector quantization;image compression;sensor;image quality;predictive vector quantization;field programmable gate arrays vector quantization image coding image sensors pulse modulation image quality compression algorithms pulse compression methods modulation coding robustness;vector;vector quantizer;integrated data compression processor fpga implementation predictive vector quantization image compression algorithm image sensor block based compression algorithm differential pulse code modulation image quality;field programmable gate arrays;fpga vector quantization predictive vector quantization;compression;vector quantisation;for;image sensor;applications	This paper presents a hybrid image compression scheme based on a block based compression algorithm referred to as Vector Quantization (VQ) combined with the Differential Pulse Code Modulation (DPCM) technique. The proposed image compression technique called the PVQ scheme results in enhanced image quality as compared to the standalone VQ. The generated codebooks for the PVQ scheme are more robust for image coding than that of the VQ. This made our system a suitable candidate for developing on chip image sensor with integrated data compression processor. The proposed system was validated through FPGA implementation. The resulting implementation achieved good compression and image quality with moderate system complexity.	algorithm;ccir system a;cmos;codebook;complexity;data compression;field-programmable gate array;image compression;image quality;image sensor;matlab;modulation;simulation;vector quantization	Yan Wang;Amine Bermak;Abdesselam Bouzerdoum;Brian W. Ng	2008	4th IEEE International Symposium on Electronic Design, Test and Applications (delta 2008)	10.1109/DELTA.2008.69	data compression;computer vision;data compression ratio;electronic engineering;image compression;computer science;theoretical computer science;image sensor;fractal transform;texture compression;information technology;algorithm;field-programmable gate array	EDA	43.40439629087585	-16.458979186728158	132664
45a5f18c33205fb88d32718e1322d94f592b2b9b	joint distributed source-channel coding for 3d videos	channel coding;perceived quality;low complexity;visual quality;joint source channel;computer programming;quality of experience;video coding;joint source channel coding;forward error correction;channel capacity;computational complexity;error correction;distributed video coding;numerical experiment;video;video communication;3d video;turbo code	This paper presents a distributed joint source-channel 3D video coding system. Our aim is the design of an efficient coding scheme for stereoscopic video communication over noisy channels that preserves the perceived visual quality while guaranteeing a low computational complexity. The drawback in using stereo sequences is the increased amount of data to be transmitted. Several methods are being used in the literature for encoding stereoscopic video. A significantly different approach respect to traditional video coding has been represented by Distributed Video Coding (DVC), which introduces a flexible architecture with the design of low complex video encoders. In this paper we propose a novel method for joint source-channel coding in a distributed approach. We choose turbo code for our application and study the new setting of distributed joint source channel coding of a video. Turbo code allows to send the minimum amount of data while guaranteeing near channel capacity error correcting performance. In this contribution, the mathematical framework will be fully detailed and tradeoff among redundancy and perceived quality and quality of experience will be analyzed with the aid of numerical experiments.	channel capacity;computational complexity theory;data compression;encoder;experiment;h.264/mpeg-4 avc;numerical analysis;redundancy (engineering);stereoscopic video coding;stereoscopy;turbo code	Veronica Palma;Michela Cancellaro;Alessandro Neri	2011		10.1117/12.872878	turbo code;real-time computing;video;telecommunications;computer science;theoretical computer science;coding tree unit;computer programming;forward error correction;channel capacity;multiview video coding		48.20997886529048	-16.67184003690173	132806
433c06e2dc3e9a3fb5ab890b4b18434ee84a3a9e	extended reliable robust motion planners		A new method to plan guaranteed to be safe paths in an uncertain environment, with an uncertain initial and final configuration space, while avoiding static obstacles is presented. First, two improved versions of the previously proposed BoxRRT algorithm are presented: both with a better integration scheme and one of them with the control input selected according to a desired objective, and not randomly, as in the original formulation. Second, a new motion planner, called towards BoxRRT?, based on optimal Rapidly-exploring Random Trees algorithm and using interval analysis is introduced. Finally, each of the described algorithms are evaluated on a numerical example. Results show that our algorithms make it possible to find shorter reliable paths with less iterations.		Adina M. Panchea;Alexandre Chapoutot;David Filliat	2017	2017 IEEE 56th Annual Conference on Decision and Control (CDC)	10.1109/CDC.2017.8263805	mathematical optimization;robustness (computer science);configuration space;approximation algorithm;control theory;mobile robot;interval arithmetic;computer science;algorithm design	Robotics	53.365036488520275	-23.78338992847554	132829
af37ee4893b8eaa7d088afd4e289998b58829165	adaptive bit allocation for image compression	image compression	The key to producing data-compressed images of improved fidelity (at a given compression ratio) using the adaptive transform approach is to improve subimage classification. Three simple measures are introduced to minimize inner-class differences based on image energy, directionality, and fineness of local detail. A fast compression scheme incorporating these measures is illustrated by a range of examples.	image compression	J. K. Wu;R. E. Burge	1982	Computer Graphics and Image Processing	10.1016/0146-664X(82)90157-5	data compression;computer vision;data compression ratio;image compression;computer science;theoretical computer science;lossless compression;texture compression	Graphics	43.20225027673666	-14.856608458460643	132837
b934b54646023a0a7bb586bf48f73641153eddbf	switching wavelet transform for roi image coding	tecnologia electronica telecomunicaciones;image coding;lifting scheme;wavelet transform;image compression;roi coding;tecnologias;grupo a	In region-of-interest (ROI) image coding based on wavelet transforms, the tap length of the wavelet filter as well as energy compaction characteristics affect the quality of the restored image. This paper presents a wavelet transform comprised of two wavelet filter sets with different tap lengths. The wavelet filter is switched to the shorter-length set to code a ROI of an image and to the longer-length one for the remaining region, the region of non-interest (RONI). ROI coding examples demonstrate that this switching wavelet transform provides better quality levels than fixed transforms under the same total bits; the quality of the recovered ROI is improved in the lossy coding of both regions while that of the full image is improved in the lossless coding of the ROI.	region of interest;wavelet transform	Shinji Fukuma;Toshihiko Tanaka;Masahiko Nawate	2005	IEICE Transactions	10.1093/ietfec/e88-a.7.1995	wavelet;computer vision;speech recognition;second-generation wavelet transform;image compression;computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;algorithm;wavelet transform;computer graphics (images)	Vision	43.339034098080724	-14.528061240885886	132914
0f914585cf59f70cdc096e7628f89761fd5c690f	a new iitnam representation method of gray images	image segmentation;image segmentation educational institutions image representation memory transforms conferences ieee press;data storage iitnam representation method gray images square packing problems triangle packing problems square subpatterns triangle subpatterns antipacking model linear quadtree;computational geometry;data storage;square gray image representation non symmetry and anti packing model linear quadtree triangle;image representation;quadtrees computational geometry gray codes image representation;quadtrees;gray codes	In this paper, inspired by the idea of the triangle and square packing problems, by using the combination of triangle subpatterns and square subpatterns, we propose a new representation method of gray images based on the triangular and square non-symmetry and anti-packing model (TSNAM). Also, we present a TSNAM representation algorithm of gray images and analyze the storage structures and the total data amount of the algorithm. By comparing the representation algorithm of the TSNAM with those of the latest improved indirect triangle non-symmetry and anti-packing model (IITNAM) and the conventional linear quadtree (LQT), the theoretical and experimental results in this paper show that the former can greatly reduce the numbers of subpatterns or nodes and simultaneously save the data storage much more effectively than the latter, and therefore it is a better method to represent gray images.	algorithm;computer data storage;quadtree;set packing	Yunping Zheng;Zujia Li;Mudar Sarem;Guang Lin;Liqiang Hu	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019906	gray code;combinatorics;discrete mathematics;computational geometry;computer science;computer data storage;mathematics;geometry;image segmentation	Robotics	43.220664190177885	-13.192501175208765	132947
60118e226acad38d36c62032949aa18f49639536	image coding using a complex dual-tree wavelet transform	discrete wavelet transforms;rate distortion;image coding;set partitions;spatial coherence;k d tree;dis crete wavelet transform;transform coding;wavelet transform;noise shaping;direction selectivity;encoding	An embedded wavelet-based coder is deployed to exploit the directional selectivity of a 2D complex dual-tree discrete wavelet transform. Although the dual-tree transform is redundant, a noise-shaping process increases the spar-sity of the transform coefficients, resulting in a high degree of spatially coherent regions of insignificant coefficients. The transform coefficients are coded with binary set-partitioning using k-d trees, and experimental results reveal rate-distortion results superior to the state-of-the-art JPEG2000 standard at low bitrates, particularly for images with strong directional features.	coefficient;coherence (physics);complex wavelet transform;discrete wavelet transform;distortion;embedded system;jpeg 2000;noise shaping;qr decomposition;selectivity (electronic);sourceforge	James E. Fowler;Joseph B. Boettcher;Béatrice Pesquet-Popescu	2007	2007 15th European Signal Processing Conference		wavelet;computer vision;constant q transform;discrete mathematics;transform coding;speech recognition;s transform;harmonic wavelet transform;lapped transform;second-generation wavelet transform;short-time fourier transform;continuous wavelet transform;fractional fourier transform;discrete sine transform;discrete fourier transform;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete fourier transform;discrete wavelet transform;fast wavelet transform;lifting scheme;wavelet transform	Vision	42.92473502170759	-15.038816110494066	133138
f13a40ce3fcca706911880ded894965b4201fdb3	scalable and compact representation for motion capture data using tensor decomposition	tensile stress joints correlation trajectory educational institutions matrix decomposition wavelet transforms;correlation methods;multilayer structure scalable representation compact representation motion capture data tensor decomposition progressive representation 3rd order tensor correlation;signal representation;tensor compression decomposition motion capture;signal representation correlation methods	Motion capture (mocap) technology is widely used in movie and game industries. Compact representation of the mocap data is critical to efficient storage and transmission. In this letter, we propose a novel tensor decomposition based scheme for compact and progressive representation of the mocap data. Our method segments and stacks the mocap sequence locally, and generates a 3rd-order tensor, which has strong correlation within and across slices of the tensor. Then, our method iteratively applies tensor decomposition in a multi-layer structure to explore the correlation characteristic. Experimental results demonstrate that the proposed scheme significantly outperforms existing algorithms in terms of scalability and storage requirement.	algorithm;basis (linear algebra);coherence (physics);entropy encoding;layer (electronics);motion capture;scalability	Junhui Hou;Lap-Pui Chau;Nadia Magnenat-Thalmann;Ying He	2014	IEEE Signal Processing Letters	10.1109/LSP.2014.2299284	mathematical optimization;topology;mathematics;geometry	AI	42.13090057548957	-18.321019169973706	133231
7f304ab3a81efca03056e8dd5328f7a6831f9bb6	fast hevc screen content coding by skipping unnecessary checking of intra block copy mode based on cu activity and gradient	two dimensional displays;conference paper;high efficiency video coding;computational complexity;image color analysis;copper;encoding	The Intra Block Copy (IntraBC) mode is a very efficient coding tool for the screen content coding (SCC) extension in High Efficiency Video Coding (HEVC) by finding the repeating patterns within the same frame. Yet, it also brings along impractically high computational complexity for SCC, which can be a double of the conventional HEVC, as exhaustive block matching is done within the same frame even though there are already some constraints applied to the IntraBC mode. To reduce the complexity, we propose to skip the unnecessary IntraBC mode checking based on the activity and gradient within the coding unit (CU). With our proposed methods, the increased encoding time compared with the conventional HEVC is reduced from 90.0% to 62.2% on average while the coding efficiency can still be maintained with only negligible bitrate increased.	algorithmic efficiency;blu-ray;computational complexity theory;gradient;high efficiency video coding;tip (unix utility)	Sik-Ho Tsang;Wei Kuang;Yui-Lam Chan;Wan-Chi Siu	2016	2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2016.7820900	real-time computing;computer science;theoretical computer science;coding tree unit;multimedia	AI	46.83131432998155	-19.39844391122732	133271
2bd58bace39b0d9bb285523858fa8026ccf91aa0	temporal-scalable coding based on image content	compensacion;estimation mouvement;image segmentation;image processing;motion pictures;motion compensation;normalisation;estimacion movimiento;simulation;compresion senal;procesamiento imagen;simulacion;motion estimation;image coding discrete cosine transforms scalability streaming media shape motion pictures codecs motion estimation motion compensation research and development;indexing terms;traitement image;compression signal;video coding;codificacion;compensation;object oriented;image representation;image quality;signal compression;coding;normalizacion;oriente objet;video codecs;image sequences temporal scalable coding image content object based temporal scalability codec shape coding motion estimation compensation method weighting techniques background composition frame rate motion picture computer simulation image quality motion representation image region low bit rate video coding;orientado objeto;computer simulation;standardization;scalable coding;codage;image segmentation video codecs motion estimation motion compensation video coding image representation	An object-based temporal scalability codec is proposed by introducing shape coding, a new motion estimation/compensation method, weighting techniques, and background composition. The major feature of this technique is determining the frame rate of the selected objects in the motion picture individually so that the motion of the selected region is smoother than that of the other area. The observation of the computer simulation proves that the proposed method achieves the better image quality and it enables us to represents the motion of the selected objects hierarchically.	scalability	Hiroyuki Katata;Norio Ito;Hiroshi Kusao	1997	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.554417	computer simulation;image quality;computer vision;index term;image processing;quarter-pixel motion;computer science;motion estimation;multimedia;image segmentation;coding;motion field;object-oriented programming;motion compensation;standardization;computer graphics (images)	Vision	45.946010692432125	-15.516739128389862	133463
8940b858eacc02651c2a63e5b92a4895855b1a8b	image compression by orthogonal decomposition using cellular neural network chips	coding process;cnn-based coding;new hardware architecture;hardware architecture;low bit-rate image;transformation coefficient;cellular neural network chips;image compression;basis function;orthogonal transformation;orthogonal decomposition;measured coefficient;inverse transformation;chip;error rate;cellular neural network	In the paper a new hardware architecture for the implementation of a high-speed, low bit-rate image coding system is outlined. Our proposed algorithm is based on the Cellular Neural/Nonlinear Network (CNN) chip-set. A simple and fast method is introduced to generate basis functions of 2 dimensional (2D) orthogonal transformations. Using these 2D basis functions of the Hadamard or Cosine functions, the transformation coefficients of the basic blocks of the image are measured by the CNN. Meanwhile, the CNN can produce the inverse transformation of the measured coefficients and the actual distortion-rate can be computed. If a required distortion-rate is reached, the coding process could be stopped (the use of even more coefficients would increase bit-rate needlessly). Effects of noise and VLSI computing accuracy are also considered to optimise the architecture. Hardware architecture and operational scheme of the CNN-based coding/decoding system. The CNN is the basic processor to measure the coefficients of the orthogonal transformation, while it calculates the inverse transformation as well. Error-rate and bit-rate are measured in-flight, as the coefficients of the spatial frequencies are estimated sequentially.	artificial neural network;cellular neural network;image compression	Tamás Szirányi;László Czúni	1999	I. J. Circuit Theory and Applications	10.1002/(SICI)1097-007X(199901/02)27:1%3C117::AID-CTA44%3E3.0.CO;2-H	data compression;iterative reconstruction;chip;electronic engineering;cellular neural network;systems modeling;image processing;image compression;word error rate;computer science;theoretical computer science;mathematics;mean squared error;coding;artificial neural network;algorithm;statistics;orthogonal transformation	ML	46.07900669080826	-13.502765294104126	133588
3d001aecbfd2bc83af5420bf74b4aa10b773f0f2	self embedding watermarking scheme using halftone image	tamper detection;filigranage numerique;protection information;digital watermarking;tecnologia electronica telecomunicaciones;image coding;restauration image;image processing;imagen medio tinte;reduccion de colores;localization;self embedding watermarking;image demi teinte;half tone image;procesamiento imagen;image restoration;localizacion;traitement image;restauracion imagen;codage image;compression image;localisation;image compression;proteccion informacion;digital halftoning;information protection;tratamiento digital;filigrana digital;dithering;image recovery;digital processing;tramage;tecnologias;grupo a;traitement numerique;compresion imagen	Self embedding watermarking is a technique used for tamper detection, localization and recovery. This letter proposes a novel self embedding scheme, in which the halftone version of the host image is exploited as a watermark, instead of a JPEG-compressed version used in most existing methods. Our scheme employs a pixel-wise permuted and embedded mechanism and thus overcomes some common drawbacks of the previous methods. Experimental results demonstrate our technique is effective and practical.		Hao Luo;Zhe-Ming Lu;Shu-Chuan Chu;Jeng-Shyang Pan	2008	IEICE Transactions	10.1093/ietisy/e91-d.1.148	image restoration;computer vision;internationalization and localization;telecommunications;image processing;digital watermarking;image compression;computer science;mathematics;information protection policy;dither	Vision	43.09002914391477	-10.960335841625833	133783
b0e5d4e000d363ea16721ed1565a63c9e11804b0	prioritized side information correction for distributed video coding	pixel domain dvc codecs;silicon;distributed algorithms;channel coding;video encoder;codecs;decoding;coding block;w z frame macroblock classification;video coding block codes channel coding decoding distributed algorithms error statistics image classification transforms video codecs;image classification;prioritized side information correction;video decoder;error analysis;video coding;motion compensated prediction distributed video coding wyner ziv coding side information;r d performance;pixel;information quality;proceedings paper;transforms;distributed video coding;error statistics;wyner ziv coding;video codecs;prioritized channel coding;parity bit;motion compensated prediction;r d performance prioritized side information correction distributed video coding algorithm prioritized channel coding w z frame macroblock classification video encoder error statistics coding block video decoder information quality parity bit pixel domain dvc codecs transform domain dvc codecs;transform domain dvc codecs;side information;distributed video coding algorithm;encoding;block codes;video coding decoding error correction error analysis codecs channel coding error correction codes sun computer science mobile computing	In this paper, a distributed video coding technique with prioritized channel coding of W-Z frames is presented. In the proposed framework, W-Z frame macroblocks are classified into different groups based on the estimated quality of the side information. The information is transmitted via uplink channel back to the encoder so that macroblocks with similar error statistics can be grouped tighter in same coding blocks for channel coding. With this approach, decoder can request more parity bits to correct macroblocks whose side information quality is worse and request less parity bits for macroblocks with smaller side information errors. Initial experimental results show that the proposed technique can improve the R-D performance of both pixel-domain and transform-domain DVC codecs considerably.	algorithm;algorithmic efficiency;baseline (configuration management);channel capacity;codec;data compression;encoder;forward error correction;information quality;macroblock;national supercomputer centre in sweden;parity bit;pixel;refinement (computing);source data;telecommunications link;winzip	Yu-Chen Sun;Shiau-Yu Lian;Chun-Jen Tsai	2009	2009 Picture Coding Symposium	10.1109/PCS.2009.5167347	block code;distributed algorithm;contextual image classification;encoder;codec;real-time computing;channel code;telecommunications;computer science;deblocking filter;theoretical computer science;parity bit;coding tree unit;mathematics;information quality;silicon;pixel;encoding;statistics	EDA	49.30520999208227	-16.938476094877686	133959
3e7640cd76a79f0881bfe2b4a6b52173792d5589	low bit-rate image coding using adaptive geometric piecewise polynomial approximation	transformation ondelette;image coding polynomials image segmentation partitioning algorithms psnr approximation algorithms length measurement graphics bit rate pixel;traitement signal;mumford shah functional;metodo polinomial;evaluation performance;tree structured segmentation;low bit rate;image coding;performance evaluation;image processing;lossy medium;debit information;tree structured segmentation adaptive nonlinear approximation image coding piecewise polynomial approximation;tree structured segmentation low bit rate image coding adaptive geometric piecewise polynomial approximation wavelet coding adaptive nonlinear approximation;information transmission;structure arborescente;polynomial approximation image coding;evaluacion prestacion;medio dispersor;procesamiento imagen;segmentation;indexing terms;qualite image;indice informacion;visual quality;traitement image;piecewise polynomial approximation;velocidad de bit debil;algorithme;algorithms computer communication networks computer graphics data compression image enhancement image interpretation computer assisted numerical analysis computer assisted reproducibility of results sensitivity and specificity signal processing computer assisted;non linear approximation;etat actuel;algorithm;codage image;aproximacion polinomial;compression image;image compression;estructura arborescente;polynomial method;signal processing;image quality;state of the art;tree structure;approximation polynomiale;information rate;estado actual;calidad imagen;rapport signal bruit;relacion senal ruido;transformacion ondita;transmision informacion;transmission information;signal to noise ratio;methode polynomiale;procesamiento senal;debit binaire faible;geometric structure;segmentacion;wavelet transformation;bits per pixel;polynomial approximation;milieu dissipatif;algoritmo;compresion imagen;adaptive nonlinear approximation	We present a new image coding algorithm, the geometric piecewise polynomials (GPP) method, that draws on recent developments in the theory of adaptive multivariate piecewise polynomials approximation. The algorithm relies on a segmentation stage whose goal is to minimize a functional that is conceptually similar to the Mumford-Shah functional except that it measures the smoothness of the segmentation instead of the length. The initial segmentation is ldquoprunedrdquo and the remaining curve portions are lossy encoded. The image is then further partitioned and approximated by low order polynomials on the subdomains. We show examples where our algorithm outperforms state-of-the-art wavelet coding in the low bit-rate range. The GPP algorithm significantly outperforms wavelet based coding methods on graphic and cartoon images. Also, at the bit rate 0.05 bits per pixel, the GPP algorithm achieves on the test image Cameraman, which has a geometric structure, a PSNR of 21.5 dB, while the JPEG2000 Kakadu software obtains PSNR of 20 dB. For the test image Lena, the GPP algorithm obtains the same PSNR as JPEG2000, but with better visual quality at 0.03 bpp.	approximation algorithm;color depth;generalized pustular psoriasis;graph partition;jpeg 2000;kakadu;lenna;lossy compression;mumford–shah functional;peak signal-to-noise ratio;pixel;polynomial;segmentation action;standard test image;wavelet transform;biologic segmentation	Roman Kazinnik;Shai Dekel;Nira Dyn	2007	IEEE Transactions on Image Processing	10.1109/TIP.2007.903250	image quality;computer vision;mathematical optimization;discrete mathematics;index term;color depth;image processing;image compression;computer science;theoretical computer science;signal processing;mathematics;tree structure;signal-to-noise ratio;segmentation;algorithm	Vision	46.57847796535522	-13.435648226720147	134111
1a4cce128559088361ff67a4216b2d1e2acce3db	fast protection of h.264/avc by reduced selective encryption of cavlc	erbium;macro blocks h 264 avc codec reduced selective encryption cavlc mode video sequences selective encryption se encryption ratio er;standards;psnr;encryption;video coding;mobile communication;video coding cryptography image sequences;encryption video coding psnr erbium mobile communication standards	In this paper we propose a new approach to protect video sequences while using selective encryption (SE) and reducing the encryption ratio (ER). Several methods of SE have been applied to video codec H.264/AVC in CAVLC mode in order to perform confidentiality, bit-rate, and data-size of protected video-sequences. In our scheme, SE-CAVLC is used but ER is decreased while preserving the confidentiality of the videos. Prediction error of H.264/AVC is used to spread a selective encryption through each frame, which allows the selection of just a part of the macro-blocks to encrypt.	codec;confidentiality;context-adaptive variable-length coding;encryption;h.264/mpeg-4 avc	Loic Dubois;William Puech;Jacques Blanc-Talon	2011	2011 19th European Signal Processing Conference		multiple encryption;real-time computing;telecommunications;computer science;context-adaptive variable-length coding;context-adaptive binary arithmetic coding;computer network	Vision	45.940298177349995	-17.65032472894318	134516
59d9da05113d037c0f88f0ee1b03f716161b7547	what's your sign? efficient sign coding for embedded wavelet image coding	image coding;data compression;entropy coding;extrapolation;transform coding;wavelet transforms;wavelet transform;entropy codes;context model coding sign coding embedded wavelet coding image coding wavelet transform coefficients transform coding compression gain entropy coding psnr improvements extrapolation zero tree coding;image coding wavelet transforms entropy coding extrapolation context modeling compaction psnr codecs frequency wavelet analysis;sign coding;extrapolation image coding wavelet transforms transform coding data compression entropy codes;wavelet	Wavelet transform coe cients are de ned by both a magnitude and a sign. While promising algorithms exist for e ciently coding the transform coe cient magnitudes [7, 13, 9, 8, 11], current wavelet image coding algorithms are not e cient at coding the sign of the transform coe cients. It is generally assumed that there is no compression gain to be obtained from entropy coding of the sign. Only recently have some authors begun to investigate this component of wavelet image coding [11, 10, 2]. In this paper, sign coding is examined in detail in the context of an embedded wavelet image coder. It is shown that PSNR improvements up to .7 dB are possible from an e cient modeling and entropy coding of the coe cient signs, combined with a new extrapolation technique, which is used to improve the nal estimate of insigni cant coe cients. These sign coding techniques are applicable to any genre (e.g. zero-tree, contextmodel) of embedded wavelet image coder.	algorithm;data compression;embedded system;entropy encoding;extrapolation;peak signal-to-noise ratio;wavelet transform	Aaron Deever;Sheila S. Hemami	2000		10.1109/DCC.2000.838167	arithmetic coding;wavelet;sub-band coding;speech recognition;shannon–fano coding;second-generation wavelet transform;continuous wavelet transform;harmonic vector excitation coding;variable-length code;entropy encoding;theoretical computer science;context-adaptive variable-length coding;coding tree unit;pattern recognition;mathematics;tunstall coding;wavelet packet decomposition;stationary wavelet transform;context-adaptive binary arithmetic coding;discrete wavelet transform;statistics;huffman coding;wavelet transform	Vision	47.351205834237874	-10.572752634260135	134789
203b014238f63c8c07e31f8fad4758c53b409695	approximate coding of digital contours	image;encoding computerised picture processing;codificacion;imagen;coding;fuzzy sets fuzzy reasoning fuzzy logic production hybrid intelligent systems fuzzy systems employment temperature control chromium aggregates;compression ratios approximate coding computerised picture processing digital contours binary images pixels quadratic bezier approximation shape compactness;computerised picture processing;encoding;codage	Two methods are proposed for coding the discrete contour of binary images. A set of key pixels (guiding pixels) on the contour is defined for this purpose. The decoding schemes approximate the contour through the key pixels using the quadratic Bezier approximation technique. The amount of deviation of the decoded image from the original image is studied using the objective measures of percentage error and shape compactness. The decoded images are found to be faithful reproductions of the original image. A set of cleaning operations is also introduced as an intermediate step before final reproduction. It was found that the bit requirements and the compression ratios are also improved significantly as compared to the contour run length coding and discrete line segment coding techniques. >		Sambhunath Biswas;Sankar K. Pal	1988	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.23108	computer vision;mathematical optimization;discrete mathematics;theoretical computer science;machine learning;image;mathematics;coding;encoding;statistics	EDA	45.02706928355566	-12.797565281730499	135384
04eab8ac54b1f2707d2d7a7397545b3feda2708a	blind and robust mesh watermarking using manifold harmonics	embedded watermark blind mesh watermarking robust mesh watermarking 3d mesh watermarking scheme manifold harmonics analysis mesh spectrum coefficient amplitudes iterative scalar costa quantization human visual system mesh low frequency components modification;watermarking;manifolds;low frequency;geometry;blind;mesh watermarking;spectrum;watermarking embedded systems harmonic analysis iterative methods mesh generation;iterative methods;embedded systems;three dimensional displays;human visual system;manifold harmonics mesh watermarking blind;manifold harmonics;robustness;mesh generation;robustness watermarking frequency harmonic analysis quantization humans visual system blindness laplace equations robust stability;graphics;harmonic analysis	In this paper, we present a new blind and robust 3-D mesh watermarking scheme that makes use of the recently proposed manifold harmonics analysis. The mesh spectrum coefficient amplitudes obtained by using this analysis are quite robust against various attacks, including connectivity changes. A blind 16-bit watermark is embedded through an iterative scalar Costa quantization of the low frequency coefficient amplitudes. The imperceptibility of the watermark is ensured since the human visual system has been proved insensitive to the mesh low frequency components modification. The embedded watermark is experimentally robust against both geometry and connectivity attacks. Comparison results with two state-of-the-art methods are provided.	16-bit;coefficient;digital watermarking;embedded system;experiment;iteration	Kai Wang;Ming Luo;Adrian G. Bors;Florence Denis	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414248	spectrum;mesh generation;computer vision;manifold;digital watermarking;computer science;graphics;theoretical computer science;harmonic analysis;mathematics;iterative method;low frequency;human visual system model;robustness	Robotics	41.86336086276499	-10.118987321840233	135635
9b40abb0887277f83cd35dd578aeacd132c9dcdb	layered user dependent multi-view video streaming	video streaming;performance evaluation;performance evaluation layered user dependent video streaming multiview video streaming multiple video sequences multiple closely spaced cameras live encoding scheme udmvt multiview video transmission lums b frame;video coding;switches streaming media bit rate servers encoding delay video sequences;video transmission;user dependent bandwidth efficient encoding and transmission layered multi view video;video streaming image sequences performance evaluation video coding;image sequences	Multi-view video consists of multiple video sequences captured simultaneously by multiple closely spaced cameras from different angles. It allows the users to change their viewpoints by playing different video sequence. However, the transmission of multi-view video is much larger than conventional media and increases with increasing number of views, which brings much more increment in the bandwidth requirement. In our previous works, we have proposed a live-encoding scheme call UDMVT [1][2] to reduce the transmission bitrate for multi-view video transmission. In UDMVT, the periodic feedback containing the position and switching speed of user is used to predict a triangle area. Frames in this triangle area are possible to be displayed in next period of time. Only these specific frames are transmitted instead of all the frames to reduce the transmission bitrate. Even though, UDMVT hasn't effectively solve the “out of triangle” problem in which the user may switch to the outside of the triangle during the playback in the triangle area. In this paper, we propose a layered scheme called Layered User dependent Multi-view video Streaming (LUMS) to solve the “out of triangle” problem for UDMVT. In LUMS each triangle is layered into several layers according to the minimum and maximum switching speed. The corresponding layers are transmitted according to the current switching speed each time received from feedback. If the “out of triangle” occurs, the higher layers will be retransmitted. All the frames in high layers are encoded into B-frame predicted from the basic and lower layers, so the retransmission bitrate is low. Performance evaluation proves that LUMS reduce transmission bitrate by 44.9% compared to UDMVT.	feedback;line code;performance evaluation;retransmission (data networks);streaming media;video compression picture types	Ziyuan Pan;Masaki Bandai;Takashi Watanabe	2012	2012 Picture Coding Symposium	10.1109/PCS.2012.6213293	video compression picture types;scalable video coding;real-time computing;telecommunications;computer science;video capture;video tracking;multimedia;video processing;smacker video;motion compensation;s-video;multiview video coding	AI	44.11210927456378	-21.113139097046062	135949
cbb1486aced97d1efc2edb32af0c77954fcaeadd	contrast sensitive epsilon-svr and its application in image compression	discrete wavelet transforms;kernel;image coding;high dimensionality;psnr;support vector regression svr;support vector machines;kernel function;support vector regression;wavelet transforms image coding regression analysis support vector machines;inner product;contrast sensitive epsilon svr;contrast sensitivity;wavelet decomposition;feature space;wavelet transforms;peak signal to noise ratio contrast sensitive epsilon svr image compression wavelet decomposition support vector regression kernel function image quality;image coding kernel discrete cosine transforms support vector machines discrete wavelet transforms image quality transform coding image storage compression algorithms image reconstruction;kernel machine;image compression;discrete cosine transforms;image reconstruction;peak signal to noise ratio;image quality;compression ratio;wavelet kernels;regression analysis;kernel machines;kernel machines image compression support vector regression svr wavelet kernels	This paper presents a practical and effective image compression system based on wavelet decomposition and contrast sensitive-SVR (support vector regression) for compressing still images. The kernel function in an SVR plays the central role of implicitly mapping the input vector (through an inner product) into a high-dimensional feature space. We study the different wavelet kernel for image compression application. Image quality is measured objectively, using peak signal-to-noise ratio, and subjectively, using perceived image quality. The effects of different wavelet kernels, image contents and compression ratios are assessed. A comparison with JPEG, SPIHT compression system is given. Our results provide a good reference to choose a suitable kernel for image compression application.	algorithm;coefficient;feature vector;image compression;image quality;jpeg;kernel (operating system);lateral thinking;mexican hat wavelet;peak signal-to-noise ratio;set partitioning in hierarchical trees;support vector machine;transmitter	Arvind Tolambiya;Prem Kumar Kalra	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811302	data compression;support vector machine;computer vision;data compression ratio;peak signal-to-noise ratio;transparency;image compression;computer science;machine learning;pattern recognition;mathematics;lossless compression;fractal transform;texture compression;set partitioning in hierarchical trees	Robotics	42.96384513493051	-15.390542684046828	136052
7b530bb732efa703f1cb7565b4aaea0b7b54d94d	a comparison of methods for improving the lossless compression of images with sparse histograms	telecommunication standards data compression image coding prediction theory code standards;image coding;data compression;image coding histograms telecommunications testing degradation image analysis;lossless compression;code standards;prediction theory;image compression;telecommunication standards;sparse histograms lossless image compression extended prediction mode jpeg ls standard compression efficiency general purpose image compression techniques online preprocessing	The importance of efficiently compressing images that are characterized by having sparse histograms is been increasing. A good indicator of this fact is the recent introduction of an extended prediction mode in Part 2 of the JPEG-LS standard, with the aim of improving the compression of this class of images. In this paper, we describe and compare several methods that share the same objective: to overcome the lack of compression efficiency that most of the general purpose image compression techniques usually show when handling images that have sparse histograms. This comparison allow us to conclude that a recently proposed online preprocessing technique, albeit quite simple, is also the most effective and versatile.	image compression;jpeg;lossless compression;preprocessor;sparse matrix	Armando J. Pinho	2002		10.1109/ICIP.2002.1040040	data compression;lossy compression;color cell compression;lossless jpeg;computer vision;data compression ratio;block truncation coding;transparency;image compression;jbig2;computer science;theoretical computer science;pattern recognition;jpeg;mathematics;lossless compression;context-adaptive binary arithmetic coding;texture compression;algorithm;golomb coding	Vision	43.31492204235787	-17.273061504652553	136133
feda72b0e28505a90096e2bca249175298f50b34	spectral decomposition and progressive reconstruction of scalar volumes	hmm;sun salutation;grace;stip;consistency	Modern 3D imaging technologies often generate large scale volume datasets that may be represented as 3-way tensors. These volume datasets are usually compressed for compact storage, and interactive visual analysis of the data warrants efficient decompression techniques at real time. Using well known tensor decomposition techniques like CP or Tucker decomposition the volume data can be represented by a few basis vectors, the number of such vectors, called the rank of the tensor, determining the visual quality. However, in such methods, the basis vectors used between successive ranks are completely different, thereby requiring a complete recomputation of basis vectors whenever the visual quality needs to be altered. In this work, a new progressive decomposition technique is introduced for scalar volumes wherein new basis vectors are added to the already existing lower rank basis vectors. Large scale datasets are usually divided into bricks of smaller size and each such brick is represented in a compressed form. The bases used for the different bricks are data dependent and are completely different from one another. The decomposition method introduced here uses the same basis vectors for all the bricks at all hierarchical levels of detail. The basis vectors are data independent thereby minimizing storage and allowing fast data reconstruction.	3d reconstruction;basis (linear algebra);data compression;imaging technology;interactive visual analysis;tucker decomposition	Uddipan Mukherjee	2016		10.1145/3009977.3010017	mathematical optimization;combinatorics;mathematics;geometry	Vision	41.50238255561326	-18.209711583527593	136147
483cdfbfb800b79a3888cbc9e52d3205583654ad	intra-mode dependent coding method for image compression	intra mode dependent coding;image coding;data compression;data compression intra mode dependent coding h 264 avc lossless image compression;image coding automatic voltage control entropy coding video coding standardization data security information security data compression quantization biomedical imaging;h 264 avc;lossless image compression;entropy coding;transform coding;intra prediction;video coding;automatic voltage control;image compression;image coding data compression;transforms;compression ratio;dpcm;intra mode dependent coding h 264 avc intra prediction dpcm	H.264/AVC based lossless image compression is a latest and extent technique for data compression. Block based intra-prediction is originally exploited in this extension. DPCM based intra-prediction is an outstanding technique to improve the compression ratio for this research area. In this paper, we propose an intra-mode dependent coding scheme to further improve the performance of DPCM-based compression technique. Different scan orders prior to entropy coding is considered to fit the various data attributes after predictions for different intra-modes.	data compression;entropy encoding;h.264/mpeg-4 avc;image compression;lossless compression	Yung-Chiang Wei;Jui-Che Teng;Chien-Wen Chung	2009	2009 Fifth International Conference on Information Assurance and Security	10.1109/IAS.2009.346	data compression;lossy compression;color cell compression;computer vision;data compression ratio;electronic engineering;block truncation coding;transparency;image compression;entropy encoding;theoretical computer science;context-adaptive variable-length coding;lossless compression;tunstall coding;adaptive coding;context-adaptive binary arithmetic coding;texture compression	EDA	43.60626734745258	-16.876796006418562	136302
f2bbad61e79db6ba2f29b895980d343d7fdae2eb	blind robust watermarking mechanism based on maxima curvature of 3d motion data	motion data;mechanism segments motion data;proposed watermarking scheme;noise addition;triangle orthocenter;possible attack;encoding approach;copyright protection;maxima curvature;blind robust watermarking mechanism	motion data;mechanism segments motion data;proposed watermarking scheme;noise addition;triangle orthocenter;possible attack;encoding approach;copyright protection;maxima curvature;blind robust watermarking mechanism	maxima	Ling Du;Xiaochun Cao;Muhua Zhang;Huazhu Fu	2012		10.1007/978-3-642-36373-3_8	computer vision;discrete mathematics;theoretical computer science;mathematics	Vision	41.75853540926375	-10.63079524073074	136500
fd488e9ab794a60f9921e1984b8851c17d59b967	adaptive weighted averaged template matching prediction for intra coding		Template matching (TM) method has been used in video coding to improve the performance of intra prediction by exploiting non-local correlations. One or more blocks, which are selected by matching the block templates, are used to create prediction samples for the target block. Each selected block is a degraded target block superimposed with the noise caused by compression and inner dissimilarities between the blocks. In this paper, an adaptive weighted averaged template matching (AWTM) method is proposed to minimize the noise variance in prediction samples. The prediction samples are generated by weighted average of several selected blocks. The weights are determined according to the correlations adaptively. Furthermore, a novel approach is proposed to integrate the AWTM method into HEVC test model, including a new derivation algorithm of most probable mode (MPM) list and modifications in coding related syntax elements. Our method can achieve 1.05% coding gain on average and up to 4.24% for luminance component under all-intra configuration compared to HEVC test model HM16.0.	algorithm;blu-ray;coding gain;data compression;high efficiency video coding;intra-frame coding;mad;material point method;template matching	Yi Zhang;Yucheng Sun;Qing Zhang;Lu Yu	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8350997	coding gain;coding (social sciences);control theory;template;template matching;computer science;luminance;artificial intelligence;weighted arithmetic mean;pattern recognition	Arch	46.303026713318076	-19.157648678905588	136570
2e8a3074e1a960ea89327779f6c24df65c6c4fad	impact of hyperspectral image coding on subpixel detection	image coding;standards;transform coding;principal component analysis;hyperspectral imaging;encoding	Four lossy hyperspectral image data coding schemes are compared with regard to their aptitude for subpixel detection use. The coding standards H.265/HEVC and JPEG2000 are investigated with and without a PCA preprocessing. As evaluation criteria, both the ‘Area under Receiver Operation Curve’ as well as the ‘Peak Signal to Noise Ratio’ are calculated. The ‘Area under Reiceiver Operation Curve’ is based on the ‘Spectral Angle Mapper’. Under both criteria, the two coding schemes with PCA preprocessing are the best while the JPEG2000 coding scheme works significantly less efficient. Furthermore, it was shown why the classification is not monotonically improving over increasing data rate. The PCA&HEVC and PCA&JPEG2000 schemes are stable at data rates of 0.1 bit per pixel per band [bpppb] and above while achieving an ‘Area under ROC’ of at least 0.99. If a data link of 0.3 bpppb is available, even the HEVC coding scheme reaches an ‘Area under ROC’ of 0.99 or more. Thus, it depends on the available data link, whether the HEVC coding scheme can be applied or if one of the more complex coding schemes with PCA preprocessing is required.	codec;high efficiency video coding;jpeg 2000;lossy compression;peak signal-to-noise ratio;pixel;preprocessor;principal component analysis;uncompressed video;aptitude	Ulrike Pestel-Schiller;Karsten Vogt;Jörn Ostermann;Wolfgang Gros	2016	2016 Picture Coding Symposium (PCS)	10.1109/PCS.2016.7906396	computer vision;transform coding;theoretical computer science;hyperspectral imaging;coding tree unit;pattern recognition;mathematics;encoding;statistics;principal component analysis	Security	43.28000768912305	-16.02645204344752	136612
22679488bea284ad287fb5ced3b3ec1b5d96e703	a subjectively adapted image communication system	thesaurus;research resource;researcher;organization;search engine;human interaction;classification algorithm;science and technology;paper;integrated search;information compression;idea;progressive image transmission;data compression;image communication;jst;technical term;institute;japan science and technology agency;image communication humans image databases data compression discrete cosine transforms transform coding classification algorithms phase change materials data security postal services;sparkingarticle;expanding;database;magazine;a subjectively adapted image communication system;patent;image;technical trend;chemical substance;journal;professional;linking;j global;search;research and development;human factors;bobliography;image transmission;ｊｇｌｏｂａｌ;comprehensive search;material;funding;image communication human factors;facility;r d;compression information;jglobal;jdream;gene;ｊ ｇｌｏｂａｌ;imagination;research project;related search;transmission image;ｊｓｔ;article;linkcenter	An interactive image communication system is described transmitting image information stored in a central database over low-bitrate channels. To shorten the transmission time, a data compression technique is applied in combination with a progressive image transmission procedure. Furthermore, human interaction is implemented in order to select specific areas for picture buildup or to reject additional image sharpening completely. The mean transmission time for each picture is essentially reduced if the transmission parameters of the investigated system are adapted to the visual threshold performance of the human eye. The adaptation is realized by means of the given classification algorithm. For a set of portrait pictures, more than 95 percent of the transmission time can be gained by this adaptation in comparison with PCM transmission.		Herbert Lohscheller	1984	IEEE Trans. Communications	10.1109/TCOM.1984.1096017	simulation;telecommunications;computer science;engineering;human factors and ergonomics;data mining;mathematics;multimedia;statistics;research	Mobile	40.17226873147678	-14.290920485596613	136685
f5d71753f9e844a1dd547b4bae2cc467a3d63299	perceptually adaptive rate-distortion optimization for variable block size motion alignment in 3d wavelet coding	reference codec;rate distortion;optimisation;3d wavelet coding;image motion analysis;rate distortion theory video coding transform coding wavelet transforms optimisation image motion analysis image texture visual perception distortion;codecs;psnr;scalable video coding;iso;texture regions;rate distortion humans visual system lagrangian functions testing video coding static var compensators codecs asia iso;testing;transform coding;edge regions;variable block size;psnr perceptually adaptive rate distortion optimization variable block size motion alignment 3d wavelet coding texture regions edge regions flat regions directional field technique human visual system distortion reduction lagrangian multiplier scalable video coding reference codec;variable block size motion alignment;image texture;rate distortion theory;wavelet transforms;distortion reduction;video coding;distortion;human visual system;content adaptation;static var compensators;visual perception;flat regions;humans;rate distortion optimization;visual system;perceptually adaptive rate distortion optimization;lagrangian functions;directional field technique;lagrangian multiplier;asia	A novel content adaptive rate-distortion optimization scheme has been proposed. The scheme can effectively distinguish texture regions, edge regions and flat regions using a directional field technique. Since the human visual system (HVS) perceives distortions more easily near edges and in flat regions, distortion reduction is more important in those regions than the bits it consumes to code the motion information. Adaptive rate-distortion optimization is carried out by adjusting the Lagrangian multiplier so that small values are assigned to edge and flat regions and large values to the random texture region. The proposed scheme has been tested in the scalable video coding (SVC) reference codec by Microsoft Research Asia (MSRA) (Xu, J. et al., ISO/EEC JTC/WG11 M10569, S05, 2004). Experimental results show that the accuracy of motion alignment in visually important regions is greatly improved in the temporal transform step of 3D wavelet coding and the scheme effectively preserves details in the most perceptually prominent regions for all bitstream layers, with no loss in PSNR.	bitstream;block size (cryptography);codec;data compression;distortion;human visual system model;lagrange multiplier;mathematical optimization;microsoft research;peak signal-to-noise ratio;rate–distortion optimization;scalability;scalable video coding;wavelet transform	Yin Sun;Feng Pan;Ashraf A. Kassim	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415558	scalable video coding;image texture;computer vision;mathematical optimization;codec;transform coding;iso image;visual system;distortion;rate–distortion theory;peak signal-to-noise ratio;visual perception;computer science;mathematics;software testing;rate–distortion optimization;lagrange multiplier;human visual system model;statistics;wavelet transform	Vision	45.83462239797255	-17.25254827217872	136707
4d0a71d551374307fc78de3b62ed851e02b879d5	motion optimization of ordered blocks for overlapped block motion compensation	motion analysis;videoconferencing;iterative methods motion compensation motion estimation video coding optimisation image matching image sequences;iterative process;compensacion;optimisation;teleconferencing;estimation accuracy;ordering algorithms;coding systems;estimation mouvement;motion optimization;image processing;motion compensation;optimizacion;iterative algorithms;image matching;estimacion movimiento;performance;procesamiento imagen;ordered blocks;motion estimation;video sequences;overlapped block motion compensation;code standards;yield estimation;indexing terms;traitement image;compensation error;algorithme;compensation errors;iterative methods;algorithm;video coding;computational modeling;compensation;low cost system;vectors;motion vector;correspondencia bloque;block matching;motion compensation motion estimation vectors video sequences yield estimation computational modeling teleconferencing motion analysis code standards iterative algorithms;optimization;intuitive algorithms;correspondance bloc;motion vector optimization;block matching motion vectors;simulation results;block codes;video sequences motion optimization ordered blocks overlapped block motion compensation block matching motion vectors estimation accuracy motion vector optimization iterative process low cost system videoconferencing compensation errors ordering algorithms coding systems performance intuitive algorithms compensation error simulation results;image sequences;algoritmo	While overlapped block motion compensation (OBMC) with block matching motion vectors yields better estimation accuracy than standard block matching, these estimates may be significantly improved by optimizing the motion vectors. Optimal motion vectors may be determined by an iterative and computationally intensive process. However, for a low-cost system (e.g., videoconferencing), such an approach is not feasible. An analysis of the compensation errors after motion optimization reveals that most gains in estimation accuracy result from the optimization of a fraction of the total number of blocks in a frame. It is thus conceivable that, by defining suitable ordering algorithms for blocks, coding systems could see improved performance by optimizing some number of blocks based on the ordering depending on available computational resources. With the aid of simulations we first show that most improvements by optimizing motion are limited to a few motion vectors. Then we present simple and intuitive algorithms based on compensation error after OBMC with block matching vectors to order blocks. Simulation results using these algorithms for ordering and optimizing motion are presented for two video sequences. The results reveal improvements obtained by optimizing the motion of the blocks from the ordering are reasonable; however, the improvements are not limited to the first fraction of blocks from the ordering, suggesting that better ordering algorithms be investigated in the future.	mathematical optimization;motion compensation	Rajesh Rajagopalan;Ephraim Feig;Michael T. Orchard	1998	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.664094	block code;computer vision;mathematical optimization;teleconference;index term;performance;image processing;quarter-pixel motion;computer science;theoretical computer science;iterative and incremental development;motion estimation;iterative method;videoconferencing;motion compensation;computational model;algorithm	EDA	48.49780407091391	-18.63253517453144	136757
7b01e13ed556ae7703acabc2cd35b8d33e4830c1	data hiding in color halftone images based on new conjugate property		This paper proposes two data hiding methods in color halftone images based on new conjugate property. Firstly, implementing and extending previous methods, and analyzing the performance of various methods. Secondly, this paper proposes new conjugate property and applies this property to hide a secret pattern in color halftone images, and two novel methods called data hiding by new color conjugate error diffusion (NCCED) and data hiding by new color conjugate dot diffusion (NCCDD) can be proposed. Using new conjugate property fewer components need to be toggled, as a result the correct decoding rate (CDR) of the revealed pattern will be increased, and the visual effect of halftone images can be improved. Finally, this paper proposes average distortion per pixel (ADPP) to measure toggling distortion in different methods accurately. Experimental results show that NCCED and NCCDD not only have higher CDR than the previous methods, but also provide better visual effect of halftone images.		Hai-yang Ding;Yixian Yang	2018	Computers & Electrical Engineering	10.1016/j.compeleceng.2016.06.007	computer vision;mathematics;algorithm;computer graphics (images)	Robotics	40.93565957632363	-12.871130259433311	136759
6e3cd06f51d1a6833e3edcb675c87dac284dfb19	on realization of modified encoding/decoding for high capacity panoramic video	random access functions;high volume panoramic data;encoding decoding;video streaming;data compression;decoding;random access functionality;satisfiability;data mining;video codec;video coding;internet;wide screen panoramic images;modified encoding realization;video streaming modified encoding realization modified decoding realization high capacity panoramic video internet mobile areas broadcasting areas high compression efficiency random access functionality high volume panoramic data random access functions high speed algorithm wide screen panoramic images video codec;high compression efficiency;decision support systems;modified decoding realization;video codecs;video streaming data compression decoding video codecs video coding;panoramic image;high speed algorithm;high capacity panoramic video;broadcasting areas;random access;high speed;mobile areas	Providing high quality panoramic video over Internet, mobile and broadcasting areas, requires a suitable video codec that satisfies both highcompression efficiency and random access functionality. On the user side, high compression efficiency is necessary to enable video streaming of high-volume panoramic data; the random access functions allow the user to move the viewpoint and direction freely. In this research, we introduce the cell concept for compression efficiency and random access functionality and propose a high-speed algorithm for the encoder and decoder of wide screen panoramic images.	algorithm;codec;display resolution;encoder;internet;random access;streaming media	Jaejoon Kim;Taeho Kim;Daegyu Lee;Youngback Kim	2009	2009 Fourth International Conference on Digital Information Management	10.1109/ICDIM.2009.5356788	data compression;computer vision;the internet;decision support system;computer science;data mining;multimedia;random access;satisfiability;computer graphics (images)	EDA	42.99367826463812	-21.06682961630804	136812
ffa9e999242e130bc4a2582225a27318c6c86262	a new class-based early termination method for fast motion estimation in video coding	motion estimation video coding costs degradation switches sun statistical analysis video compression standards development prediction methods;complexity theory;image matching;prediction algorithms;class based early termination method;motion estimation;video coding image matching motion estimation;video coding;image matching class based early termination method motion estimation video coding;fast motion estimation;classification algorithms;mobile communication;algorithm design and analysis	Motion Estimation (ME) is one of the most time-consuming parts in video coding. It is always desirable to develop fast ME algorithms to reduce the ME complexity. In this paper, a new early termination method is proposed for fast motion estimation. The proposed method first classifies each Macroblock into one of three classes based on the estimation of the possible matching cost improvement from future search points. Different early termination strategies are then applied to different classes. Experimental results show that the proposed method can significantly reduce the search points with little quality degradation.	algorithm;data compression;elegant degradation;future search;macroblock;motion estimation	Weiyao Lin;Krit Panusopone;David M. Baylon;Ming-Ting Sun	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5117826	statistical classification;algorithm design;computer vision;real-time computing;simulation;mobile telephony;prediction;quarter-pixel motion;computer science;coding tree unit;motion estimation;block-matching algorithm;rate–distortion optimization;motion compensation;multiview video coding	EDA	47.534712656079996	-19.06373341549002	136865
ef6248c4983f488e3ad0800d9b599206995a8579	on block motion estimation using a novel search strategy for an improved adaptive pixel decimation	prediction error;full search;search strategy;motion estimation;motion compensated;fast motion estimation;computational complexity;fast algorithm	Block motion estimation using the exhaustive full search is computationally intensive. Previous fast algorithms tend to reduce the computation by limiting the number of locations to be searched and by reducing the number of pixels for the matching criterion. However, a large prediction error results when these two types of techniques are combined to form a fast algorithm. In this paper, first, we present a new adaptive pixel decimation algorithm with an appropriate pixel selection control. The proposed algorithm selects a targeted number of most representative pixels, which avoids the fluctuation of the computational complexity among different frames. Second, a smart search strategy is proposed which is an excellent complement of the adaptive pixel decimation to form a very efficient block motion estimation algorithm. The new search strategy uses a newly defined matching criterion to adaptively select search locations in the second step of fast motion estimation. It is found that this proposed combination is more robust than previous fast algorithms. Experimental results show that, as compared to the conventional approach, this novel algorithm is more robust, produces smaller motion compensation errors, and has simplified computational complexity.	decimation (signal processing);motion estimation;pixel	Yui-Lam Chan;Wan-Chi Siu	1998	J. Visual Communication and Image Representation	10.1006/jvci.1998.0388	computer vision;mathematical optimization;quarter-pixel motion;computer science;theoretical computer science;mean squared prediction error;motion estimation;mathematics;computational complexity theory	Vision	48.63043268601401	-19.30172590541572	136884
17e65247506a5b099c8d5be728e216840035251a	synchronous and asynchronous multiple object rate control for mpeg-4 video coding	video object;telecommunication control video coding;telecommunication control;visual quality;rate control;video coding;mpeg 4 standard video coding encoding layout bit rate synthetic aperture sonar video compression radio control psnr sun;multiple objectives;temporal resolution;bit allocation;moving picture experts group mpeg 4 video coding asynchronous multiple object rate control synchronous multiple object rate control multiple video object encoding bit allocation strategy	Video scenes containing multiple objects can potentially achieve higher degree of compression and better visual quality with individual coding for each object. Not always are video objects synchronous, implying each object may have a separate temporal resolution. This paper proposes a rate control algorithm for multiple video object encoding. Using a novel bit allocation strategy, the algorithm achieves accurate target hit rate, provides good visual quality, and decreases buffer overflowlunderflow. Experimental results for both synchronous and asynchronous multiple video object encoding demonstrate that, when compared with the existing rate control scheme recommended by the MPEG-4 standard, the proposed algorithm provide bener temporal-spatial tradeoff with more accurate rate regulation.	algorithm;data compression	Yu Sun;Ishfaq Ahmad;Jiancong Luo;Xiaohui Wei	2003		10.1109/ICIP.2003.1247366	video compression picture types;scalable video coding;computer vision;real-time computing;computer science;temporal resolution;video tracking;coding tree unit;block-matching algorithm;multimedia;video processing;smacker video;context-adaptive binary arithmetic coding;motion compensation;h.261;s-video;multiview video coding	Graphics	45.379730606373755	-19.520490541769984	136903
c66fa6004cebd999a48d3c769074d40affe6e974	realistic mesh compression based on geometry image	geometry image based 3d mesh compression;residual image;code geometry image;coding efficiency;geometry images;image coding;mesh compression;data compression;residual images;geometry;geometry image coding three dimensional displays image reconstruction correlation encoding vectors;会议论文;3 d mesh compression;normal map image;coding framework;coding geometry image;original meshes;spatial correlation;vectors;realistic mesh compression;three dimensional displays;image reconstruction;image coding data compression;3d mesh realistic effect realistic mesh compression geometry image based 3d mesh compression coding geometry image normal map image spatial correlation coding framework normal map image correlation code geometry image residual image jpeg2000 coding efficiency;prediction accuracy;jpeg2000;jpeg 2000;strong correlation;correlation;image based;spatial correlations;encoding;3d mesh realistic effect;three component;3d meshes	In order to show the realistic 3D mesh in geometry image-based 3D mesh compression, in addition to coding geometry image, normal-map image is usually required to code. But normal-map image are difficult to compress because it captures more details of the original mesh, and it has less spatial correlation between pixels than geometry image. This paper proposes a novel coding framework to solve this problem, we effectively predict the normal-map image based on the correlation between geometry image and normal-map image, and we also utilize the strong correlation among three components of normal-map image to improve the predicting accuracy. In this framework we only need to code geometry image and residual image which generated from normal-map image and its prediction. Experimental results show that comparing with the method which coding geometry image and normal-map image using JPEG2000 directly, our coding framework not only improves the coding efficiency of geometry images and normal-map images, but also enhances the realistic effect of 3D mesh significantly.	algorithmic efficiency;jpeg 2000;lifting scheme;lossless compression;lossy compression;normal mapping;pixel;wavelet transform	Yunhui Shi;Bo Wen;Wenpeng Ding;Na Qi;Baocai Yin	2012	2012 Picture Coding Symposium	10.1109/PCS.2012.6213304	image warping;image texture;computer vision;feature detection;binary image;image compression;theoretical computer science;free boundary condition;jpeg 2000;mathematics;geometry;anisotropic diffusion;statistics	Vision	43.3463224390631	-18.61082517269392	136936
050f3ff6f73a6d51df1f4d21326550353e3e1ec3	inter-layer prediction scheme for scalable 3-d holoscopic video coding	hevc interlayer prediction scheme scalable 3d holoscopic video coding holoscopic imaging prospective glassless 3d technology depth illusion approach legacy display consumer market scalable coding solution high efficiency video coding;video coding;three dimensional displays encoding image resolution signal processing algorithms video coding rendering computer graphics scalability;3 d holoscopic video coding display scalability hevc integral imaging	Holoscopic imaging has recently become a prospective glassless 3-D technology conquering the attention of researchers seeking more realistic depth-illusion approaches. However, backward compatibility with legacy displays is crucial to progressively introduce this technology into the consumer market and to efficiently deliver 3-D holoscopic content to end-users. Therefore, this letter proposes a new display scalable coding solution for 3-D holoscopic based on an inter-layer prediction scheme that exploits the redundancy between multiview and 3-D holoscopic content representations. Experimental results show that this inter-layer prediction scheme integrated into the High Efficiency Video Coding (HEVC) is advantageous, always outperforming the simulcast approach.	backward compatibility;high efficiency video coding;multitier architecture;prospective search;scalability;simulcast	Caroline Conti;Paulo J. L. Nunes;Luís Ducla Soares	2013	IEEE Signal Processing Letters	10.1109/LSP.2013.2267234	computer vision;computer science;multimedia;computer graphics (images)	HPC	43.99531013767853	-19.647665072311504	136957
90ca9288f01e09da505e879c5265d4c2d2f87bc9	effective rate control algorithm for h.264 based on scene change detection		The standard rate control algorithm works with all the levels such as the GOP, frame, and unit levels. However, it cannot allocate bits reasonably under the condition of scene change. In this paper, an efficient rate control algorithm to detect scene change and reallocate the channel target bits of H.264 is proposed. The proposed method can detect scene change effectively and change the length of GOP adaptively, so the target bits can be allocated more reasonably than standard rate control algorithm.	algorithm;h.264/mpeg-4 avc	Yaoyao Guo;Songlin Sun;Xiaojun Jing;Hai Huang;Yueming Lu;Na Chen	2013		10.1007/978-3-662-43908-1_12	algorithm;change detection;computer science;communication channel	Robotics	47.826637635836356	-17.646326207971992	137081
621eac01ae2221eb723cb5904405c9e945bba83b	image content analysis for sector-wise jpeg fragment classification	image content analysis;dct coefficient analysis;image processing;jpeg directional analysis;jpeg carving;digital image forensics;horizontal versus vertical;erroneous fragment classification	In this paper, we propose a sector-wise JPEG fragment classification approach to classify normal and erroneous JPEG data fragments with the minimum size of 512 bytes per fragment. Our method is based on processing each read-in sector of 512 bytes with using the DCT coefficient analysis methods for extracting the features of visual inconsistencies. The classification is conducted before the inverse DCT and can be performed simultaneously with JPEG decoding. The contributions of this work are two-folds: (1) a sector-wise JPEG erroneous fragment classification approach is proposed (2) new DCT coefficient analysis methods are introduced for image content analysis. Testing results on a variety of erroneous fragmented and normal JPEG files prove the strength of this operator for the purpose of forensics analysis, data recovery and abnormal fragment inconsistencies classification and detection. Furthermore, the results also show that the proposed DCT coefficient analysis methods are efficient and practical in terms of classification accuracy. In our experiment, the proposed approach yields a false positive rate of 0.32% and a true positive rate of 96.1% in terms of erroneous JPEG fragment classification.	byte;coefficient;discrete cosine transform;disk sector;jpeg;performance;progressive scan;test case	Yu Chen;Vrizlynn L. L. Thing	2013	J. Visual Communication and Image Representation	10.1016/j.jvcir.2013.06.005	horizontal and vertical;lossless jpeg;computer vision;image processing;computer science;theoretical computer science;data mining;jpeg 2000;quantization	Vision	41.42461711588427	-14.058638271355491	137157
5d35da79c6f051e18b92d5f466680c8a09ffe3a7	image compression using feedforward neural networks - hierarchical approach	feedforward neural network;image compression	The paper presents the hierarchical approach to the problem of image compression using feedforward neural networks. In this approach smaller frames are used in the regions containing more details and larger, when the grey level is uniform. Thanks to this the number of data is reduced, learning speed accelerated and the quality of compression improved. The numerical results, confirming the efficiency of the proposed approach and good generalization properties are presented and discussed.	feedforward neural network;image compression	Stanislaw Osowski;Robert Waszczuk;Piotr Bojarczak	1995		10.1007/3-540-59497-3_280	feedforward neural network;probabilistic neural network;image compression;computer science;machine learning;time delay neural network	ML	42.69272741066017	-14.34957609134557	137509
4d966e2fe84e7870631b5dfd3ea1ec0c64c7f89e	semi-fixed-length motion vector coding for h.263-based low bit rate video compression	detection erreur;evaluation performance;rate distortion;deteccion error;cellular radio code standards telecommunication standards data compression video coding rate distortion theory noise fading channels;performance evaluation;image processing;data compression;bit rate video compression image coding working environment noise resilience video coding noise level telephony entropy coding decoding;correction erreur;cellular radio;evaluacion prestacion;video compression;procesamiento imagen;code standards;indexing terms;traitement image;rate distortion theory;video coding;tratamiento numerico;codificacion;senal video;signal video;motion vector;error correction;telecommunication standards;coding;binary symmetric channel semi fixed length motion vector coding h 263 based low bit rate video compression structural constraints motion field semi fixed length codes rate distortion performance subjective quality huffman based variable length codes noiseless environment error resilience noisy environment gsm fading channel model bsc model;video signal;error resilience;digital processing;compresion dato;correccion error;error detection;fading channels;traitement numerique;compression donnee;codage;noise	We present a semi-fixed-length motion vector coding method for H.263-based low bit rate video compression. The method exploits structural constraints within the motion field. The motion vectors are encoded using semi-fixed-length codes, yielding essentially the same levels of rate-distortion performance and subjective quality achieved by H.263's Huffman-based variable length codes in a noiseless environment. However, such codes provide substantially higher error resilience in a noisy environment.	binary symmetric channel;channel (communications);coder device component;compliance behavior;data compression;distortion;huffman coding;motion field;peak signal-to-noise ratio;pixel;run-length encoding;slc12a2 wt allele;slc27a2 gene;semiconductor industry;variable-length code	Guy Côté;Michael Gallant;Faouzi Kossentini	1999	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.791971	data compression;error detection and correction;shannon–fano coding;telecommunications;image processing;harmonic vector excitation coding;quarter-pixel motion;computer science;theoretical computer science;motion estimation;mathematics;block-matching algorithm;motion compensation;statistics	Vision	48.4456165868263	-14.00448543739953	137602
453e6d9d8bc420608a73dbe230be740572919ab5	new set-top box for interactive visual communication of home entertainment using mpeg-2 full-duplex codec lsi	home computing;audio visual systems;high definition television visual communication audio visual systems ip networks broadband networks video coding data compression video codecs home computing;visual communication codecs large scale integration transform coding robustness video compression tv bidirectional control broadband communication hdtv;broadband networks;codecs;data compression;interactive visual communication;broadband network;visual communication;interactive visualization;video compression;home entertainment;ftth;ip broadband network;bi directional video transmission;transform coding;mpeg 2 full duplex;chip;video coding;audio coding;visual communication codecs large scale integration transform coding robustness tv video compression broadband communication bidirectional control hdtv;large scale integration;broadband network set top box interactive visual communication home entertainment mpeg 2 full duplex codec lsi single chip mpeg 2 codec lsi audio visual compression technologies home television set bidirectional video transmission ftth network ip network;standard tv quality;bidirectional control;hdtv;video transmission;optical fibre subscriber loops;video codecs;robustness;audio visual;ip networks;audio visual compression technologies;tv;single chip mpeg 2 codec lsi;hdtv mpeg 2 full duplex home entertainment interactive visual communication set top box single chip mpeg 2 codec lsi audio visual compression technologies home television set standard tv quality bi directional video transmission ftth ip broadband network;interactive television;broadband communication;entertainment;broadband networks large scale integration video codecs video coding interactive television entertainment audio coding delays visual communication optical fibre subscriber loops ip networks;set top box;high definition television;delays;home television set	This paper describes a new set-top box of interactive visual communication for home entertainment exploiting single-chip MPEG-2 CODEC LSI. The set-top box has been designed to achieve recent audio-visual compression technologies for consumer applications, especially very-low-delay and robust-error-tolerant dedicated to interactive visual communication. This set-top box with home television set provides standard-TV-quality bi-directional video transmission via commercially available FTTH-based IP broadband network.	codec;duplex (telecommunications);error-tolerant design;fiber to the x;mpeg-2;set-top box;television set	Minoru Inamori;Hiroe Iwasaki;Takayuki Onishi;Mitsuo Ikeda;Jiro Naganuma;Yoshiyuki Yashima	2005	2005 Digest of Technical Papers. International Conference on Consumer Electronics, 2005. ICCE.	10.1109/TCE.2005.1468012	data compression;embedded system;interactive visualization;telecommunications;computer science;multimedia;broadband networks	Visualization	42.9655623029243	-21.188479064152936	137758
9d6edf9770070d3dfd83a4cc58c9a2b1fc4a5477	a motion aided merge mode for hevc		Merge prediction is a practical inter-technique in HEVC, which can significantly improve the coding efficiency, especially for homogeneous regions in video sequences. In this paper, a motion aided merge mode (MAMM) is proposed to achieve a better trade-off between the prediction accuracy and bit rate. Different from the traditional merge mode in HEVC, MAMM is accomplished by a small motion obtained by searching in a specific search region. The search range is comprised of a number of points with high occurrence possibilities. The motion vector difference (MVD) is coded by Huffman coding in MAMM and the Huffman coding table is generated according to the statistical frequency of each possible MVD value. The proposed method is implemented on top of the HEVC reference software (HM −16.15), and experimental results show that 0.6% BD-rate reduction is achieved under Random Access (RA) configuration.	algorithmic efficiency;blu-ray;high efficiency video coding;huffman coding;random access;traffic collision avoidance system	Hao Li;Kui Fan;Ronggang Wang;Ge Li;Wenmin Wang	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461506	huffman coding;reference software;decoding methods;merge (version control);motion vector;frequency;pattern recognition;artificial intelligence;algorithmic efficiency;computer science;random access	Robotics	47.179903306566054	-19.66211570895471	137975
76210edb4782886f1a23323c6fab81870aff60ee	codebook generation and search algorithm for vector quantization using arbitrary hyperplanes	search algorithm;vector quantization;clustering algorithms;algorithm design and analysis;encoding;sum of squares;data compression;tree structure;sum of squared errors;speech coding	An arbitrary hyperplane approach for both codebook generation and search for vector quantization (VQ) is introduced. It is a divisive algorithm and aims at minimizing the sum-of-squared-errors. The relation to tree-structured vector quantization is discussed, and the superiority of the proposed algorithm is demonstrated through a comparison with the LBG (generalized Lloyd) algorithm for a 16-dimension VQ compression of still pictures. u003e	codebook;search algorithm;vector quantization	Shing-Chow Chan;Chi-Wah Kok;S. W. Chau	1993			data compression;algorithm design;learning vector quantization;computer science;theoretical computer science;machine learning;speech coding;pattern recognition;mathematics;explained sum of squares;tree structure;cluster analysis;linde–buzo–gray algorithm;vector quantization;encoding;search algorithm	Vision	44.15307114481726	-13.030269873227887	138091
469ac11f4169c4a3f11c348c168ff26ceae006ad	on the use of ssim in hevc	standards;psnr;video coding;computational modeling;video codecs;optimization;encoding	The Structural SIMilarity (SSIM) index has been attracting an increasing amount of attention recently in the video coding community as a perceptual criterion for testing and optimizing video codecs. Meanwhile, the arrival of the new MPEG-H/H.265 High Efficiency Video Coding (HEVC) standard creates new opportunities and challenges in perceptual video coding. In this paper, we first elaborate what are the attributes that make SSIM a good candidate for perception-based development of HEVC and future video coding standards for both testing and optimization purposes. We then address the computational issues in practical applications of SSIM in HEVC, in particular the trade-off between efficient computation and accurate estimation of SSIM when working with video codecs that have sophisticated block partitioning structures and aim for encoding videos with a wide range of spatial resolutions.	codec;coding gain;computation;data compression;high efficiency video coding;mpeg-h;mathematical optimization;moving picture experts group;peak signal-to-noise ratio;structural similarity;video coding format	Tiesong Zhao;Kai Zeng;Abdul Rehman;Zhou Wang	2013	2013 Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2013.6810465	computer vision;speech recognition;computer science;coding tree unit;multimedia;h.261;multiview video coding	EDA	44.51684061074018	-19.330611943088922	138424
08bb46f65bcf59b81fc85d56929c5763ea8eeaed	compressive sensing based video scrambling for privacy protection	video surveillance;compressed sensing;video surveillance data compression data privacy matrix algebra security of data video coding;data compression;decoding;chaos;matrix algebra;privacy protection;video coding;data privacy;image reconstruction;privacy encoding security chaos decoding compressed sensing image reconstruction;cs measurement matrix compressive sensing video scrambling surveillance video privacy protection video compression security key controlled chaotic sequence drift error;security;encoding;security of data;privacy	Surveillance video privacy protection has drawn significant attention recently. In this paper, we describe a privacy protected video surveillance system which utilizes the emerging compressive sensing (CS) theory. Privacy regions are scrambled through block based CS sampling on quantized coefficients during compression. Security is ensured by key controlled chaotic sequence which is used to construct CS measurement matrix. To prevent drift error caused by scrambling, a coding restricted scheme is exploited. Experimental results show that the proposed system effectively protects privacy with the scene intelligible. Compared with the existing ones, this system has high security and dramatic coding efficiency improvement.	algorithmic efficiency;approximation algorithm;closed-circuit television;coefficient;compressed sensing;privacy;quantization (signal processing);sampling (signal processing)	Lingling Tong;Feng Dai;Yongdong Zhang;Jintao Li;Dongming Zhang	2011	2011 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2011.6115917	data compression;iterative reconstruction;information privacy;telecommunications;computer science;information security;internet privacy;compressed sensing;privacy;computer security;encoding	Security	39.70121062811131	-12.906336963678056	138665
7b4698d70af64741e232db2d46917d4d81780ca0	laplacian-based h.264 intra-prediction mode decision	video coding computational complexity edge detection motion compensation;intra mode;standards;motion compensation;edge detection;laplacian;prediction algorithms;bit rate;video coding;image edge detection;computational complexity;video coding image edge detection encoding prediction algorithms bit rate standards signal to noise ratio;h 264;signal to noise ratio;edge detection laplacian based h 264 intraprediction mode decision video coding multipicture interpicture prediction variable block size quarter pixel precision motion compensation context adaptive variable length coding context adaptive binary arithmetic coding intracoding encoding complexity intramode decision problem;encoding;video coding h 264 laplacian intra mode	Compared with the conventional video coding standards, H.264 provide new coding techniques, such as Multi-picture inter-picture prediction, Variable block-size, Quarter-pixel precision for motion compensation, intra prediction, Context-adaptive variable-length coding and Context-adaptive binary arithmetic coding. Intra coding is used for reducing the spatial redundancy in video coding. H.264 supports several MB (Macroblock) predictions for Intra coding such as luma block four 16×16 modes, nine 4×4 modes and chroma block four modes, which significantly improve Intra coding efficiency, but increase the encoding complexity. This paper presents an approach to the intra mode decision problem. Based on Laplacian, this approach detects edges and then selects the best mode for the block. This method can shorten the time to choose a mode and reduce the encoding time as well.	algorithmic efficiency;binary number;context-adaptive binary arithmetic coding;context-adaptive variable-length coding;data compression;decision problem;h.264/mpeg-4 avc;intra-frame coding;macroblock;motion compensation;pdf/a;pixel;variable-length code;video coding format	Chi-Chou Kao;Yen-Tai Lai;Chao-Feng Tseng	2012	7th International Conference on Communications and Networking in China	10.1109/ChinaCom.2012.6417561	sub-band coding;computer vision;laplace operator;real-time computing;edge detection;prediction;harmonic vector excitation coding;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;context-adaptive binary arithmetic coding;computational complexity theory;signal-to-noise ratio;motion compensation;encoding	Robotics	46.726096283194956	-18.959174957199846	138718
0ea33e322fc034fc4d398b78444841d78369ac39	high-quality symmetric wyner-ziv coding scheme for low-motion videos	computer programming;video coding;期刊论文;video	Traditional Wyner–Ziv video coding (WZVC) structures require either intra (Key) or Wyner–Ziv (WZ) coding of frames. Unfortunately, keeping the video quality approximately constant implies drastic bit-rate fluctuations because consecutive frames of different types (Key or WZ) present significantly different compression performances. Moreover, certain scenarios severely limit rate fluctuation. This work proposes a WZVC scheme with low bit-rate fluctuations based on a symmetric coding structure. First, this work investigates the performance of a generic nonasymmetric distributed source coding structure, showing that the low-density parity-check accumulate channel decoding method is best suited. This is used as a basis to design a symmetric WZVC scheme in which every input video frame is divided into four parallel subframes through subsampling, and then the subframes are encoded by using a symmetric method. Compared with the traditional asymmetric WZVC scheme, the proposed scheme can achieve higher bit-rate stability over time, which is a great advantage to guarantee a reliable transmission in many wireless communication application environments in which bit-rate fluctuations are strongly constrained. Simulation results show the effectiveness of the proposed symmetric WZVC scheme in maintaining a steady bit rate and quality, as well as a quality comparison with the traditional WZVC scheme. © 2014 SPIE and IS&T [DOI: 10.1117/1.JEI.23.6.061112]	algorithm;chroma subsampling;computation;data compression;decoding methods;distributed source coding;intra-frame coding;lz77 and lz78;performance;pixel;quantum fluctuation;simulation;winzip	Xianfeng Ou;Enrico Masala;Linbo Qing;Xiaohai He	2014	J. Electronic Imaging	10.1117/1.JEI.23.6.061112	real-time computing;video;telecommunications;computer science;theoretical computer science;computer programming;mathematics	Web+IR	48.00826308452523	-17.56633551368222	138762
315942dbbf8ffc47143bc64ecd133ae373ef2a95	a perturbative resampling approach to image steganalysis	image coding;steganalysis;image processing;steganography cryptography image processing availability web sites pixel computer science automation color art;support vector machines;jpeg format;image matching;information hiding;transform coding;data mining;media;steganography;perturbative resampling approach;image color analysis;pixel;information hiding steganography steganalysis;image steganalysis;steganography image matching;lsb matching steganography;point of view;image processing operations;jpeg format perturbative resampling approach image steganalysis image processing operations lsb matching steganography	We study the effect of perturbative operations on images from a steganalytic point of view. It is observed that the rates at which unique colors vary in response to image processing operations performed in a perturbative sense - by adding mild disturbances to the image - provides a mechanism to calibrate the size of the hidden content due to LSB matching steganography in images that were earlier stored in the JPEG format. We also explore possible extensions of this approach to raw images and those that were stored as JPEG and subsequently subjected to image processing operations.	bitmap;color;domain-driven design;image processing;jpeg;least significant bit;nonlinear system;perturbation theory (quantum mechanics);pixel;raw image format;resampling (statistics);steganalysis;steganography	V. Suresh;H. V. Shashidhara;C. E. Veni Madhavan	2009	2009 International Conference on Ultra Modern Telecommunications & Workshops	10.1109/ICUMT.2009.5345478	support vector machine;computer vision;transform coding;media;steganalysis;image processing;computer science;theoretical computer science;pattern recognition;mathematics;steganography;information hiding;pixel;statistics	Robotics	42.01568389877717	-11.308018039714112	138764
a45d18e81af6dd8edd9e20a96ac6b45bfe1f5daa	a generalized solution to the quadtree expected complexity problem	expected complexity;node condensation;search problem;arbre recherche;representation;link strength;soft linked quadtree;algoritmo busqueda;image processing;complexite calcul;algorithme recherche;search algorithm;procesamiento imagen;imagerie;problema investigacion;traitement image;hierarchical representation;codificacion;complejidad computacion;general solution;imagery;a priori knowledge;arbol investigacion;computational complexity;almacenamiento;tree structure;coding;stockage;imagineria;weight function;search tree;probleme recherche;quadtree;storage;codage;representacion	Complexity in tree-structured hierarchical representation of images is defined as the total number of nodes in the tree. An a priori knowledge of this quantity is of considerable interest in problems involving tree traversal-based search algorithms and storage and transmission of imagery using hierarchical compression coding. This paper derives a closed-form expression for expected complexity by developing a generalized weighting function defined over a primitive element of a soft-linked quadtree.	quadtree	Bijan G. Mobasseri	1995	Pattern Recognition Letters	10.1016/0167-8655(95)00126-2	combinatorics;a priori and a posteriori;weight function;search problem;image processing;computer science;artificial intelligence;quadtree;mathematics;tree structure;coding;search tree;computational complexity theory;representation;algorithm;search algorithm	Vision	46.43890279500605	-12.518842123812513	138952
d1bce47f6fb0ebfe1e193d75a6446bffce5ea30d	detecting modifications in paper documents: a coding approach	artefacto;software;document imprime;0130c;correction erreur;4230s;artefact;algorithme;visualization;error correcting codes;recherche documentaire;printed document;documento impreso;error correction code;busqueda documental;error correction;pixel;pattern recognition;photocopying;0367p;algorithms;photocopie;document retrieval;reconnaissance forme;inks;code correcteur erreur;encoding;encre;codage	This paper presents an algorithm called CIPDEC (Content Integrity of Printed Documents using Error Correction), which identifies any modifications made to a printed document. CIPDEC uses an error correcting code for accurate detection of addition/deletion of even a few pixels. A unique advantage of CIPDEC is that it works blind – it does not require the original document for such detection. Instead, it uses fiducial marks and error correcting code parities. CIPDEC is also robust to paper-world artifacts like photocopying, annotations, stains, folds, tears and staples. Furthermore, by working at a pixel level, CIPDEC is independent of language, font, software, and graphics that are used to create paper documents. As a result, any changes made to a printed document can be detected long after the software, font, and graphics have fallen out of use. The utility of CIPDEC is illustrated in the context of tamper-proofing of printed documents and ink extraction for form-filling applications.	algorithm;backward compatibility;barcode;computer programming;fiducial marker;forward error correction;graphics;photocopier;pixel;printing;qr code;requirement;sensor	Yogesh Sankarasubramaniam;Badri Narayanan;Kapali Viswanathan;Anjaneyulu Kuchibhotla	2010		10.1117/12.838122	document retrieval;error detection and correction;speech recognition;computer science;pattern recognition;statistics;computer graphics (images)	HCI	43.1956822043987	-11.534752044975905	139221
a3c0fa259fe1ad73eef5474fd99368188bce8dd7	reversible video watermarking using motion estimation and prediction error expansion	filigranage numerique;digital watermarking;prediction error;estimation mouvement;reversible watermarking;steganographie;estimacion movimiento;motion estimation;qualite image;steganography;histogram;esteganografia;estimation erreur;senal video;histogramme;signal video;error estimation;erreur estimation;image quality;filigrana digital;estimacion error;histogram modification;error estimacion;video signal;calidad imagen;estimation error;video;histograma;video watermarking	In this paper, a novel reversible video watermarking scheme using motion estimation and prediction error expansion is presented, which can embed a large amount of secret data into videos with imperceptible modification. Different with other reversible watermarking schemes based on prediction error expansion, motion estimation is employed to explore the relationship of neighboring frames and work out prediction errors, which significantly sharpen the distribution of prediction errors. Then histogram modification is used to expand the prediction errors, wherein a prediction error is changed by 1 or left unchanged for embedding one secret bit. Due to the slight modification of pixels, high video quality is preserved. In the phases of motion estimation and histogram modification, a little side information used for extracting and recovering are generated, which will be combined with the secret bits and embedded into the video. Experimental results demonstrate that the proposed scheme provides a much larger watermark capacity and a better quality of watermarked video than those of as reported in other state-of-the-art literature.	algorithmic efficiency;digital watermarking;distortion;embedded system;motion estimation;pixel;watermark (data file)	Xiao Zeng;Zhenyong Chen;Ming Chen;Zhang Xiong	2011	J. Inf. Sci. Eng.		image quality;computer vision;speech recognition;video;digital watermarking;quarter-pixel motion;mean squared prediction error;motion estimation;histogram;mathematics;steganography;statistics;computer graphics (images)	Vision	44.544320519321204	-12.367146825060496	139326
80047a37ed07bdbdb23e76544dc98332ae2b0d60	gpu-based mpeg-2 to secure scalable video transcoding	parallel transcoding;mpeg 2;graphic processing unit;multimedia security;h 264 svc	Most of the high definition video content are still produced in a single-layer MPEG-2 format. Multiple-layers Scalable Video Coding (SVC) offers a minor penalty in rate-distortion efficiency when compared to single-layer coding MPEG-2. A scaled version of the original SVC bitstream can easily be extracted by dropping layers from the bitstream. This paper proposes a parallel transcoder from MPEG-2 to SVC video with Graphics Processing Unit (GPU), named PTSVC. The objective of the transcoder is to migrate MPEG-2 format video to SVC format video such that clients with different network bandwidth and terminal devices can seamlessly access video content. Meanwhile, the transcoded SVC videos are encrypted such that only authorized users can access corresponding SVC layers. Using various scalabilities SVC test sequences, experimental results on TM5 and JSVM indicate that PTSVC is a higher efficient transcoding system compared with previous systems and only causes little quality loss.	graphics processing unit;mpeg-2	Yueyun Shang;Dengpan Ye;Zhuo Wei;Yajuan Xie	2014	IJDCF	10.4018/ijdcf.2014040104	scalable video coding;real-time computing;computer science;multimedia;mpeg-2;computer network	Crypto	45.226057408372505	-20.030028542972047	139390
75875654502b864bccbc25a721e51e964907e9ac	a multiresolutional coding method based on spiht	image resolution;analyse multiresolution;codificacion;region of interest;coding;multiresolution analysis;resolution image;analisis multiresolucion;codage	In this work, we incorporate a multiresolutional coding functionality into the SPIHT algorithm [1]. The multiresolutional coding can be considered as a kind of the region of interest (ROI) coding with multiple regions of interest. Therefore, the ROI coding proposed by authors can be extended for the multiresolutional functionality without any cost in performance. The parent of ROI (PROI) and the multiple lists for insignificant sets and pixels, which were proposed for the ROI coding, are also used for the multiresolution coding.	multiresolution analysis;set partitioning in hierarchical trees	Keun-hyeong Park;Chul-Soo Lee;Hyun Wook Park	2001		10.1007/3-540-45453-5_127	multiresolution analysis;computer vision;image resolution;computer science;pattern recognition;coding;algorithm;computer graphics (images);region of interest	NLP	46.04619417693126	-13.658209854938818	139504
681d28187dd3ee212572f53e54f37440e58d65a2	a novel scheme to improve lossless image coders by explicit description of generative model classes		In this study, we propose a novel scheme for systematic improvement of lossless image compression coders from the point of view of the universal codes in information theory. In the proposed scheme, we describe a generative model class of images as a stochastic model. Using the Bayes codes, we are able to construct a lossless image compression coder which is optimal under the Bayes criterion for a model class described appropriately. Since the compression coder is optimal for the assumed model class, we are able to focus on the expansion of the model class. To validate the efficiency of the proposed scheme, we construct a lossless image compression coder which achieves approximately 19.7% reduction of average coding rates of previous coders.	generative model;image compression;information theory;lossless compression;norm (social);universal code (data compression)	Yuta Nakahara;Toshiyasu Matsushima	2018	CoRR			ML	49.08797657153747	-14.357907155844023	139509
b5908d8dc9b4f78d71edb640170d900f14a45532	cube-based perceptual weighted kronecker compressive sensing: can we avoid non-visible redundancies acquisition?		Abstract Compressive sensing approach directly avoids the acquisition of statistical redundancies of a signal. However, perceptual redundancies of images and videos due to the human eye sensitivity are not considered so far. Besides, an effective sampling scheme is needed to multidimensional signal reconstruction using a low number of measurements to avoid all redundancies. In this paper, along with the Kronecker structure of the sampling matrix we design various weighting matrices based on the spatio-temporal contrast sensitivity function to avoid acquisition of non-visible redundancies. Moreover, inspired by the block-based compressive sensing, we divide a group of pictures in a video sequence into cubes. Hence, the size of measurement and sparsifying basis matrices are reduced and the reconstruction algorithm can be implemented in parallel. We further show that our simple linear sampling approach can be competitive with motion compensation method. Simulation results verify that our proposed method notably outperforms the other state-of-the-art methods.	compressed sensing;cube	Seyed Hamid Safavi;Farah Torkamani-Azar	2017	J. Visual Communication and Image Representation	10.1016/j.jvcir.2017.08.010	artificial intelligence;compressed sensing;reconstruction algorithm;mathematics;kronecker delta;pattern recognition;computer vision;motion compensation;sampling (statistics);group of pictures;signal reconstruction;matrix (mathematics)	Vision	44.81250717462579	-17.273579990751003	139648
021a28c9c3b4b3a4b6fcf50fcdcece7188bf444c	inevitable collision states - a step towards safer robots?	navigation;sensing constraints;safety;motion planning;collision avoidance	An inevitable collision state for a robotic system can be defined as a state for which, no matter what the future trajectory followed by the system is, a collision with an obstacle eventually occurs. An inevitable collision state takes into account the dynamics of both the system and the obstacles, fixed or moving. The main contribution of this paper is to lay down and explore this novel concept (and the companion concept of inevitable collision obstacle). Formal definitions of the inevitable collision states and obstacles are given. Properties fundamental for their characterisation are established. This concept is very general and can be useful both for navigation and motion planning purposes (for its own safety, a robotic system should never find itself in an inevitable collision state). To illustrate the interest of this concept, it is applied to a problem of safe motion planning for a robotic system subject to sensing constraints in a partially known environment (ie that may contain unexpected obstacles). In safe motion planning, the issue is to compute motions for which it is guaranteed that, no matter what happens at execution time, the robotic system never finds itself in a situation where there is no way for it to avoid collision with an unexpected obstacle.	motion planning;robot;run time (program lifecycle phase)	Thierry Fraichard;Hajime Asama	2004	Advanced Robotics	10.1163/1568553042674662	computer vision;navigation;simulation;computer science;artificial intelligence;motion planning	Robotics	53.15342119969044	-21.885378272309534	139689
62bf53d69cb4a568d5cd90f0a8cc5b54896c824a	an mcmc based efficient parameter selection model for x265 encoder		As an open-source and computationally efficient High Efficiency Video Coding (HEVC) encoder, x265 has been gaining increasing popularity in video applications. x265 provides numerous encoding parameters in view of flexibility. However, proper and efficient setting of parameters often becomes a great challenge in practice. In this paper, we deeply investigate the influence of x265 parameters based on the Slow preset and pick out important parameters in terms of efficiency and complexity. Then a Markov Chain Monte Carlo (MCMC) based algorithm is proposed for efficient parameter adaptation at the target encoding time. This paper shows that carefully selected low-complexity encoding configurations can achieve the coding efficiency comparable to that of high-complexity ones. Specifically, average 26.72% encoding time reduction can be achieved while maintaining similar Rate Distortion (RD) performance to x265 presets using the proposed algorithm.	algorithm;algorithmic efficiency;approximation algorithm;brute-force search;complexity;distortion;encoder;high efficiency video coding;markov chain monte carlo;monte carlo method;open-source software;ruby document format;universal turing machine;x265	Yan Huang;Li Song;Rong Xie;Zhengyi Luo;Xiangwen Wang	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351034	encoder;control theory;theoretical computer science;computer science;algorithmic efficiency;markov chain monte carlo;distortion;parallel processing	EDA	46.27026984594806	-19.617442342131167	139844
506c77038c745c44e1da4cf98d849d5c47b7aa0b	mdvqm: a novel multidimensional no-reference video quality metric for video transcoding	frame dropping;subjective video quality;spatial down sampling;quality of experience;video transcoding;perceived video quality metric;video quality assessment;no reference	In this paper, we study the impact of quantization, frame dropping and spatial down-sampling on the perceived quality of compressed video streams. Based on the analysis of quality ratings obtained from extensive subjective tests, we propose a no-reference metric (named MDVQM) for video quality estimation in the presence of both spatial and temporal quality impairments. The proposed metric is based on the per-pixel bitrate of the encoded stream and selected spatial and temporal activity measures extracted from the video content. All the values required to compute the proposed video quality metric can be obtained without using the original reference video which makes the metric for instance useful for making transcoding decisions in a wireless video transmission scenario. Different from comparable metrics in the literature, we have also considered the case when both frame rate and frame size are changed simultaneously. The validation results show that the proposed metric provides more accurate estimation of the video quality than the state of the art metrics.		Fan Zhang;Eckehard G. Steinbach;Peng Zhang	2014	J. Visual Communication and Image Representation	10.1016/j.jvcir.2013.11.011	video compression picture types;scalable video coding;reference frame;subjective video quality;computer vision;simulation;transcoding;computer science;video quality;video tracking;block-matching algorithm;multimedia;rate–distortion optimization;motion compensation;video post-processing;pevq;multiview video coding	Vision	45.40774398463045	-21.42389232552117	139882
80878d327c3c64d5e026d6585d5a096837cfb723	traffic and video quality with adaptive neural compression	decompression;descompresion;red numerica integracion servicios;estimation mouvement;neural networks;compression decompression neural networks;real time;pressure relief;estimacion movimiento;video compression;video quality;motion estimation;compression image;senal video;signal video;image compression;integrated services digital network;atm traffic;video signal;transmission asynchrone;reseau numerique integration services;reseau neuronal;motion detection;asynchronous transfer mode;neural network	Video sequences are major sources of traffic for broadband ISDN networks, and video compression is fundamental to the efficient use of such networks. We present a novel neural method to achieve real-time adaptive compression of video. This tends to maintain a target quality of the decompressed image specified by the user. The method uses a set of compression/decompression neural networks of different levels of compression, as well as a simple motion-detection procedure. We describe the method and present experimental data concerning its performance and traffic characteristics with real video sequences. The impact of this compression method on ATM-cell traffic is also investigated and measurement data are provided.	atm turbo;adaptive compression;artificial neural network;cell (microprocessor);data compression;integrated services digital network;real-time clock;video	Erol Gelenbe;Mert Sungur;Christopher Cramer;Pamir Gelenbe	1996	Multimedia Systems	10.1007/s005300050037	video compression picture types;data compression;embedded system;simulation;telecommunications;image compression;computer science;video quality;machine learning;asynchronous transfer mode;video tracking;motion estimation;integrated services digital network;block-matching algorithm;context-adaptive binary arithmetic coding;motion compensation;artificial neural network	ML	43.15903763309648	-23.71373386495395	140021
325b49aac76971c2fb8fc4d3a16f902d4648519f	lossless coding of multichannel signals using optimal vector hierarchical decomposition	image coding propagation losses optimization methods signal resolution pixel interpolation signal synthesis filters biomedical imaging image storage;traitement signal;minimum variance;interpolation;image coding;image processing;transmision multicanal;variance minimale;data compression;medical signal processing interpolation optimization image coding data compression digital filters image color analysis electrocardiography;variancia minima;optimal filter;subband decomposition;compresion senal;procesamiento imagen;lossless compression;indexing terms;traitement image;compression signal;codificacion;filtro optimal;electrocardiography;multichannel transmission;descomposicion subbanda;transmission multicanal;image color analysis;signal processing;digital filters;signal compression;coding;compression lossless coding multichannel signals optimal vector hierarchical decomposition multichannel reduced pyramids interpolation synthesis postfilters error variance optimization methodology electrocardiographic signals rgb colored images;minimal variance;optimization;decomposition sous bande;filtre optimal;procesamiento senal;medical signal processing;codage;color image	A methodology is presented for the optimal construction of multichannel reduced pyramids by selecting the interpolation synthesis postfilters so as to minimize the error variance at each level of the pyramid. The general optimization methodology is applied for the optimization of pyramids for the compression of electrocardiographic signals and RGB colored images.	interpolation imputation technique;lossless compression;lossy compression;mathematical optimization;pyramid (geometry);sample variance	Dimitrios Tzovaras;Michael G. Strintzis	2000	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.869191	data compression;computer vision;minimum-variance unbiased estimator;speech recognition;digital filter;index term;color image;image processing;interpolation;computer science;signal processing;mathematics;lossless compression;coding;statistics	Visualization	46.99440009451723	-12.84620018070573	140087
f72a866bd96fdcba06b5f384e22c9bf9f9babf8d	a high capacity data hiding scheme based on re-adjusted gemd		Steganography is a useful technology to protect secret data traveling through the Internet. Recently, Kuo and Wang proposed a useful data hiding scheme based on GEMD(Generalized Exploiting Modification Direction). They claim that the embedding capacity of their scheme is more than 1 bpp(bits per pixel) and keeps good stego-image quality. In addition, the GEMD scheme can prevent RS detection. However, the embedding capacity of GEMD decreases when pixel numbers in the group becomes large. To alleviate this shortcoming, we will propose a data hiding scheme which can embed extra secret data after the GEMD embedding procedure. The major contribution of the proposed scheme is the embedding capacity always maintains 2 bpp which is independent of the pixel numbers in the group. Finally, according to our experiments, our proposed scheme maintains good image quality and also prevents RS detection.	experiment;image quality;pixel;reed–solomon error correction;steganography	Chun-Cheng Wang;Wen-Chung Kuo;Yu-Chih Huang;Lih-Chyau Wuu	2017	Multimedia Tools and Applications	10.1007/s11042-017-4541-0	pixel;information hiding;the internet;computer science;steganography;image quality;theoretical computer science;color depth;embedding	Vision	40.01290376688996	-12.267971620789893	140256
2af976d7c2b5f64f162f0a7b4f852fc353138a6b	protection-enhanced scalable video coding with multiple description coding	packet loss;video coding;redundancy;static var compensators;encoding;spatial resolution	Compressed image and video transmission needs to be protected because a few transmission errors can make the transmitted bitstream undecodable. Among others, multiple description coding techniques were proposed to protect encoded sequences by dividing the original signal into several chunks of data that are transmitted to the receiver over distinct channels. Several multiple description techniques were proposed for scalable and non-scalable video algorithms. However error protection was never considered as a part of scalability nor it was taken into account while designing the coding algorithm. As a consequence, most of the proposed multiple description coding schemes cannot be fully exploited to preserve the quality of the transmitted sequences over unreliable networks, i.e., some of them require too much resources, while others cannot be used because they cannot be executed in real time. In this paper we present the first development stage of a new scalable video codec designed to add protection scalability to coded sequences in order to adapt to the network status and to provide finer scalability.	algorithm;bitstream;codec;data compression;multiple description coding;scalability;scalable video coding;software release life cycle	O. Campana;T. Q. Nguyen	2007	2007 15th European Signal Processing Conference		scalable video coding;real-time computing;telecommunications;computer science;theoretical computer science;multiple description coding;coding tree unit	Visualization	48.208724720594574	-15.89840436600756	140276
f8a618896d90c5e60c3f141b1a2126192e356b34	intra block copy hash reduction for hevc screen content coding	complexity theory;memory management;standards;search methods;high efficiency video coding;proceedings paper;encoding;graphics	To meet a wide range of needs for video applications such as remote desktop, video conference, distance education, and cloud gaming, the ISO/ITU Joint Collaborative Team on Video Coding (JCT-VC) committee is recently specifying the Screen Content Coding (SCC) standard, as one of the extensions of High Efficiency Video Coding (HEVC). In this paper, the hash search method of the standard adopted Intra Block Copy (IBC) coding tool for SCC is investigated. We collect the coded data using the current hash table and examine their efficiency and explore possible ways for further improvement. A low complexity scheme of selecting effective hash nodes and a modified hash key generation method are presented. Experimental results show that the proposed method reduces on the average 37% or at most 70% hash table memory usage but it preserves the similar BD-rate savings and encoding complexity when integrated into the SCM-3.0 test model.	blu-ray;cloud gaming;codec;desktop computer;encoder;hash table;high efficiency video coding;information-based complexity;key generation;remote desktop software;tip (unix utility);vc dimension	Che-Wei Kuo;Hsueh-Ming Hang;Chun-Liang Chien	2016	2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2016.7820766	hash function;computer science;theoretical computer science;coding tree unit;multimedia;context-adaptive binary arithmetic coding;world wide web;multiview video coding	EDA	45.20138182003356	-20.00712241954156	141078
51ca76f4b824c2af160f864eee28f277e61e11a6	no-reference video quality assessment by hevc codec analysis	hevc analysis;elastic net;transform coefficients no reference video quality assessment hevc codec analysis high efficiency video coding bitstream based method pixel based method;machine learning;video coding video codecs;elastic net hevc analysis no reference video quality assessment machine learning;video quality assessment;no reference	This paper proposes a No-Reference (NR) Video Quality Assessment (VQA) method for videos subject to the distortion given by High Efficiency Video Coding (HEVC). The proposed assessment can be performed either as a Bitstream-Based (BB) method or as a Pixel-Based (PB). It extracts or estimates the transform coefficients, estimates the distortion, and assesses the video quality. The proposed scheme generates VQA features based on Intra coded frames, and then maps features using an Elastic Net to predict subjective video quality. A set of HEVC coded 4K UHD sequences are tested. Results show that the quality scores computed by the proposed method are highly correlated with the subjective assessment.	bitstream;codec;coefficient;distortion;elastic map;elastic net regularization;high efficiency video coding;noise reduction;pixel;raster graphics	Xin Huang;Jacob Søgaard;Søren Forchhammer	2015	2015 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2015.7457853	video compression picture types;scalable video coding;computer vision;speech recognition;computer science;video quality;machine learning;multimedia;motion compensation;h.261;elastic net regularization;multiview video coding	Vision	44.92398950675548	-19.57661281084144	141327
485b07efc26dde5293cc5d16b184b1121707b599	low complexity multiple description coding method for wireless video	rate distortion;multiple description;low quality macroblock update;low quality macroblock update multiple description gob alternation;data compression;decoding;packet loss;video coding decoding discrete cosine transforms performance loss code standards video compression rate distortion propagation losses robustness internet;wireless video;low complexity;video coding;gob alternation;single channel;computational complexity video coding decoding data compression;standard video decoder gob alternation low quality macroblock update low complexity multiple description coding wireless video multiple description video coding single channel reconstruction mdtc video coder redundancy rate distortion packet loss scenario;computational complexity;multiple description coding	In this paper, we present a new multiple description video coding method. It starts from GOB alternation to create two descriptions for each frame encoded, carriage of motion information in both descriptions helps to improve the quality of single channel reconstruction, and a process called low quality macroblock update helps to not only further improve the quality of single channel reconstruction but also reduce the mismatch between the encoder and decoder. The advantages of the proposed approach include: effectiveness, which has better performance than the well known MDTC video coder both in terms of redundancy rate distortion, and in the packet loss scenario; low complexity, and standard compliant to the extent that each description can be decoded by a standard video decoder.	data compression;distortion;encoder;macroblock;multiple description coding;network packet;rate–distortion theory;video decoder;whole earth 'lectronic link	Yangli Wang;Chengke Wu	2005	19th International Conference on Advanced Information Networking and Applications (AINA'05) Volume 1 (AINA papers)	10.1109/AINA.2005.237	data compression;real-time computing;telecommunications;computer science;theoretical computer science;multiple description coding;packet loss;computational complexity theory;computer network	Vision	48.175316216655474	-16.372079379180054	141334
a66ed47de9f2decb42a2d8e5700916087364271e	a fast coding unit division and mode selection method for hevc intra prediction		The high efficiency video coding standard (HEVC) has come into view in recent years. In which some specific process and details were introduced to achieve higher coding efficiency, but lead to a very high computational complexity. In this paper, we focus on the need of lower complexity in computing recourse limited environment, and present a fast CU division algorithm and mode selection method for HEVC intra prediction. Experimental results show that the rate distortion cost (RD-Cost) values of no-split CU are clearly lower than that of splitting. So we formulate threshold equations of different depth level in the coding tree, by statistical analyzing of the RD-Cost values under different quantization parameter (QP) probabilities distribution. Division of code unit could be terminated earlier using threshold, which shall reduce the computing complexity. The experimental results show that, compared with HEVC testing model (HM), the improved algorithm can save an average 27.6% of encoding time with negligible loss of coding efficiency (only 0.49% bitrate increasing, and 0.013dB Peak Signal-to-Noise Ratio (Y-PSNR) loss). Meanwhile, by fully exploiting the correlation between the first rank prediction mode of the candidate mode set and the optimal prediction mode, some prediction modes can be skipped. It also leads to the decrease of computing complexity and the experimental results show that the proposed algorithm saves 42.1% of encoding time for the premise of ensuring the video quality.	algorithmic efficiency;character encoding;computational complexity theory;data compression;distortion;division algorithm;encoder;high efficiency video coding;huffman coding;intra-frame coding;lookahead carry unit;mobile app;peak signal-to-noise ratio;rate–distortion theory;relevance;ruby document format;splitting circle method;tip (unix utility);video coding format	Xiaoyan Xie;Xiaofei Xin;Huan Wang	2017	2017 4th International Conference on Systems and Informatics (ICSAI)	10.1109/ICSAI.2017.8248487	coding (social sciences);control theory;division algorithm;computer science;algorithmic efficiency;quantization (signal processing);mathematical optimization;distortion;video quality;computational complexity theory	AI	46.814487086849155	-19.31146520116744	141480
90c170bfe9f1d96a1107aac115ffa8cdef4de370	real time implementation of rate-distortion optimized coding mode selection for263 video coders	rate distortion quantization optimization methods video codecs predictive models rate distortion theory encoding discrete cosine transforms lagrangian functions bit rate;quantization;motion compensated prediction error signals;rate distortion;optimisation;motion compensation;h 263 video coders;trellis search;picture quality;video sequences;code standards;bit rate;video codec;rate distortion theory;quantisation signal;rate control;video coding;very low bit rate;coding parameters;prediction theory;rate distortion optimized coding mode selection;video sequences real time implementation rate distortion optimized coding mode selection h 263 video coders video codecs normalized rate distortion model motion compensated prediction error signals fast algorithm coding parameters quantization parameter lagrangian multiplier trellis search experiments rate control picture quality very low bit rate;discrete cosine transforms;telecommunication standards;fast algorithm;experiments;normalized rate distortion model;quantization parameter;video codecs;predictive models;real time implementation;search problems;motion compensated prediction;rate distortion optimization;encoding;lagrangian functions;lagrangian multiplier;search problems code standards telecommunication standards video codecs video coding quantisation signal rate distortion theory motion compensation prediction theory image sequences optimisation;optimization methods;image sequences	This paper proposes a new method for real time implementation of rate-distortion optimized coding mode selection which can be efficiently applied to H.263-compatible video codecs and other codecs of similar type. We use our previously proposed normalized rate-distortion model to efficiently compute the rate and the distortion when encoding motion-compensated prediction error signals, instead of actually performing DCT, quantization and entropy-encoding. We also propose a fast algorithm to find sub-optimal values of coding parameters such as the quantization parameter and the Lagrangian multiplier, /spl lambda/, for the trellis search. Experiments show that the proposed scheme provides very good rate control as well as good picture quality, especially when applied to very low bit rate video coding.	distortion;real-time business intelligence	Kyeong Ho Yang;Arnaud E. Jacquin	1998		10.1109/ICIP.1998.723387	image quality;mathematical optimization;rate–distortion theory;quantization;harmonic vector excitation coding;computer science;theoretical computer science;coding tree unit;mathematics;predictive modelling;rate–distortion optimization;lagrange multiplier;motion compensation;algorithm;encoding	AI	46.848717965079004	-17.041514154711	141553
0a59deeb4af4aa86efeb4f2eae0864083511c3df	frames for exact inversion of the rank order coder	scalability bio inspired image coding frames theory out of core rank order code;out of core;image coding;filter bank;decoding;vector space;rank order code;reverse transform exact inversion rank order coder original exact decoding retina cells visual stimulus standard size images frames theory filter bank image reconstruction;vectors;channel bank filters;retina;image reconstruction;peak signal to noise ratio;transforms;mathematical model;frames theory;bio inspired image coding;transforms channel bank filters image reconstruction;scalability;encoding;retina transforms image reconstruction decoding vectors encoding mathematical model	Our goal is to revisit rank order coding by proposing an original exact decoding procedure for it. Rank order coding was proposed by Thorpe . who stated that the order in which the retina cells are activated encodes for the visual stimulus. Based on this idea, the authors proposed in a rank order coder/decoder associated to a retinal model. Though, it appeared that the decoding procedure employed yields reconstruction errors that limit the model bit-cost/quality performances when used as an image codec. The attempts made in the literature to overcome this issue are time consuming and alter the coding procedure, or are lacking mathematical support and feasibility for standard size images. Here we solve this problem in an original fashion by using the frames theory, where a frame of a vector space designates an extension for the notion of basis. Our contribution is twofold. First, we prove that the analyzing filter bank considered is a frame, and then we define the corresponding dual frame that is necessary for the exact image reconstruction. Second, to deal with the problem of memory overhead, we design a recursive out-of-core blockwise algorithm for the computation of this dual frame. Our work provides a mathematical formalism for the retinal model under study and defines a simple and exact reverse transform for it with over than 265 dB of increase in the peak signal-to-noise ratio quality compared to . Furthermore, the framework presented here can be extended to several models of the visual cortical areas using redundant representations.	area striata structure;basis (linear algebra);codec;coder device component;computation;decoder device component;dual;filter bank;formal system;frame (linear algebra);frame (physical object);iterative reconstruction;mathematics;out-of-core algorithm;overhead (computing);peak signal-to-noise ratio;performance;recursion;retina;technical standard	Khaled Masmoudi;Marc Antonini;Pierre Kornprobst	2012	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2011.2179557	combinatorics;discrete mathematics;vector space;theoretical computer science;mathematics;statistics	Vision	45.32058654970121	-16.944524808881454	141610
add13467280ee02556d673bbc9d47da4b1e8be51	a deep convolutional neural network approach for complexity reduction on intra-mode hevc		The High Efficiency Video Coding (HEVC) standard significantly saves coding bit-rate over the proceeding H.264 standard, but at the expense of extremely high encoding complexity. In fact, the coding tree unit (CTU) partition consumes a large proportion of HEVC encoding complexity, due to the brute-force search for rate-distortion optimization (RDO). Therefore, we propose in this paper a complexity reduction approach for intra-mode HEVC, which learns a deep convolutional neural network (CNN) model to predict CTU partition instead of RDO. Firstly, we establish a large-scale database with diversiform patterns of CTU partition. Secondly, we model the partition as a three-level classification problem. Then, for solving the classification problem, we develop a deep CNN structure with various sizes of convolutional kernels and extensive trainable parameters, which can be learnt from the established database. Finally, experimental results show that our approach reduces intramode encoding time by 62.25% and 69.06% with negligible Bj⊘ntegaard delta bit-rate of 2.12% and 1.38%, over the test sequences and images respectively, superior to other state-of-the-art approaches.	artificial neural network;brute-force search;coding tree unit;convolutional neural network;data compression;distortion;guid partition table;h.264/mpeg-4 avc;high efficiency video coding;huffman coding;mathematical optimization;rate–distortion optimization;reduction (complexity);remote data objects	Tianyi Li;Mai Xu;Xin Deng	2017	2017 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2017.8019316	pattern recognition;artificial intelligence;kernel (linear algebra);computer science;convolutional neural network;reduction (complexity);machine learning;coding tree unit	DB	46.66660944011337	-19.547175916700862	141637
cc0b7f931be6b64823fc2323bf5d4a5142556cfb	research on a novel model for intra-skip in inter coding	mathematics;encoding efficiency;h 264 encoder;search algorithm;inter coding;motion search algorithm;code standards;video coding code standards;bit rate;intra skip;interframe coding;video coding;automatic voltage control;inter partition mode decision;h 264 avc standard;motion search algorithm intra skip inter coding h 264 avc standard encoding efficiency video coding standard interframe coding h 264 encoder inter partition mode decision;mathematical model;road transportation;encoding;video coding standard;mode decision;quadratic programming encoding bit rate automatic voltage control mathematical model image coding acceleration educational institutions maintenance engineering video coding;partitioning algorithms	The new H.264/AVC standard can achieve considerably higher encoding efficiency compared to prior video coding standards. In inter-frame coding, not only seven inter modes but also two intra modes are taken into account in order to maintain high encoding efficiency. This paper proposed a novel mathematical model for intra-skip in inter-frame coding, which provides remarkable performance exaltation by cutting down encoding time while accompanying very minor bitrate augment. In addition, this flexible model can be adjusted to various environments on account of adopting different tradeoffs between encoding speed and bitrate. The critical advantage of this novel model is that it can optimize H.264 encoder together with any fast inter methods including inter partition mode decision and motion search algorithms. Also, this model does have potential practical value in the transplantation on DSP if fully developed.	data compression;encoder;h.264/mpeg-4 avc;mathematical model;search algorithm;video coding format	Ning Cao;Hui Su	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555364	real-time computing;simulation;computer science;theoretical computer science;mathematical model;mathematics;context-adaptive binary arithmetic coding;encoding;statistics;search algorithm	SE	46.99318172013725	-18.509433414024212	142106
d7f384ff911b75f5af4e6179d4922913f25ab0a6	an efficient optimal rate control scheme for jpeg2000 image coding	rate distortion;optimisation;image coding;entropy coding;rate distortion theory;optimal control;rate control;video coding;optimal control image coding discrete wavelet transforms bit rate rate distortion delay spatial resolution image resolution bandwidth quantization;rate allocation;entropy coding process optimal rate control scheme jpeg2000 image coding post compression rate distortion optimization scheme priority scanning working memory size;entropy codes;working memory;optimisation optimal control image coding entropy codes rate distortion theory;end to end delay	Most of the computation and memory usage of the post-compression rate-distortion (PCRD) optimization scheme in JPEG2000 are redundant. In this paper, an efficient rate PCRD scheme based on priority scanning (PS) is proposed to alleviate the problem. By encoding the truncation points in a different order based on the priority information, the proposed method can efficiently reduce the redundancy while keeping the same quality as the conventional PCRD scheme. The proposed scheme can efficiently reduce both the computational cost and working memory size of the entropy coding process by up to 52% and 71%, in the case of 0.25bpp (1/32) compression, respectively.	computation;computational complexity theory;distortion;entropy encoding;jpeg 2000;mathematical optimization;peak signal-to-noise ratio;truncation	Yick Ming Yeung;Oscar C. Au;Andy R. K. Chang	2003		10.1109/ICIP.2003.1247356	mathematical optimization;discrete mathematics;optimal control;rate–distortion theory;harmonic vector excitation coding;computer science;entropy encoding;theoretical computer science;end-to-end delay;working memory;mathematics;context-adaptive binary arithmetic coding;statistics	Mobile	46.92647502057189	-16.66984037274114	142198
579bce5ca4f21b1151dce7a25aecac9c32d97998	an enhanced generalized lloyd algorithm	image coding;data compression;indexing terms;image coding vector quantisation computational complexity;signal compression letter enhanced generalized lloyd algorithm vector quantization data compression modified k means algorithm coding performance computational complexity codebook design image coding;generalized lloyd algorithm;computational complexity;k means algorithm;vector quantizer;vector quantisation;algorithm design and analysis euclidean distance computational complexity image coding vector quantization costs phase distortion data compression signal design nearest neighbor searches	Vector quantization is an effective means of data compression for it maps a set of real numbers into a single integer. In this letter an enhanced generalized Lloyd algorithm (GLA) for vector quantizer design is presented. Experiment results show that the proposed scheme outperforms the conventional GLA and recently reported modification of the K-means algorithm. Moreover, the achievement in coding performance is not accompanied by significant increase in computational complexity.	codebook;computation;computational complexity theory;data compression;experiment;k-means clustering;linde–buzo–gray algorithm;local optimum;quantization (signal processing);vector quantization	Chang-Qian Chen	2004	IEEE Signal Processing Letters	10.1109/LSP.2003.819869	data compression;index term;computer science;theoretical computer science;lloyd's algorithm;machine learning;pattern recognition;mathematics;linde–buzo–gray algorithm;computational complexity theory;vector quantization;k-means clustering	ML	44.174770535	-13.065043056166655	142389
2434f4b9691ae00b49c8342f52e46ff232c98f48	hybrid no-reference video quality assessment focusing on codec effects	codec effects;feature vector;video quality assessment;no reference	Currently, the development of multimedia communication has progressed so rapidly that the video program service has become a requirement for ordinary customers. The quality of experience (QoE) for the visual signal is of the fundamental importance for numerous image and video processing applications, where the goal of video quality assessment (VQA) is to automatically measure the quality of the visual signal in agreement with the human judgment of the video quality. Considering the codec effect to the video quality, in this paper an efficient non-reference (NR) VQA algorithm is proposed which estimates the video quality (VQ) only by utilizing the distorted video signal at the destination. The VQA feature vectors (FVs) which have high relationships with the subjective quality of the distorted video are investigated, and a hybrid NR VQA (HNRVQA) function is established by considering the multiple FVs. The simulation results, testing on the SDTV programming provided by VCEG Phase I, show that the proposed algorithm can represent the VQ accurately, and it can be used to replace the subjective VQA to measure the quality of the video signal automatically at the destinations.	algorithm;codec;data compression;feature vector;noise reduction;simulation;standard-definition television;vector quantization;video coding format;video processing	Xingang Liu;Min Chen;Tang Wan;Chen Yu	2011	TIIS	10.3837/tiis.2011.03.008	subjective video quality;computer vision;simulation;feature vector;computer science;video quality;machine learning;video tracking;multimedia;pevq;multiview video coding	Visualization	45.08286089733469	-21.4408188845497	142601
7a9e02dfab356081209b5f479ce4cbc095dd5f67	distributed compressed video sensing in camera sensor networks	期刊论文	With the booming of video devices ranging from low-power visual sensors to mobile phones, the video sequences captured by these simple devices must be compressed easily and reconstructed by relatively more powerful servers. In such scenarios, distributed compressed video sensing (DCVS), combining distributed video coding (DVC) and compressed sensing (CS), is developed as a novel and powerful signal-sensing and compression algorithm for video signals. In DCVS, video frames can be compressed to a few measurements in a separate manner, while the interframe correlation is explored by the joint recovery algorithm. In this paper, a new DCVS joint recovery scheme using side-information-based belief propagation (SI-BP) is proposed to exploit both the intraframe and interframe correlations, which is particularly efficient over error-prone channels. The DCVS scheme using SI-BP is designed over two frame signal models, the mixture Gaussian (MG) model and the wavelet hidden Markov tree (WHMT) model. Simulation results evaluated on two video sequences illustrate that the SI-BP-based DCVS scheme is error resilient when the measurements are transmitted through the noisy wireless channels.		Yu Liu;Xuqi Zhu;Zhang Lin;Sung Ho Cho	2012	IJDSN	10.1155/2012/352167	video compression picture types;computer vision;simulation;telecommunications;computer science;video tracking	Mobile	49.91035151665812	-16.899757759711232	142763
5e1119384f295f8a82a2a747ef48f241c46f757d	temporal video compression using mode factor and polynomial fitting on wavelet coefficients	video compression	The core idea of this study is to build an algorithm that functions to compress video sequences. The mode value at every pixel along the temporal direction is calculated. If the frequency of the mode value satisfies a predetermined frequency, then the intensity values for entire entries at that particular pixel position will be changed to the mode value. The wavelet techniques will be applied to the pixels that do not satisfy the predetermined frequency and followed by a polynomial fitting method. For the purpose of compression, only the polynomial coefficients for pixels that do not satisfy the predetermined frequency, the mode values for pixels that satisfy the predetermined frequency and the corresponding pixel positions will be stored. To decompress, wavelet coefficients are estimated by the respective polynomials. The intensity values at the intended pixel position are obtained by inverse wavelet transform for pixels that do not satisfy the predetermined frequency. On the other hand, the stored mode values will be used to represent the intensity values throughout the time interval. This method portrays a prospect to achieve an acceptable decompressed video quality and compression ratio.	algorithm;coefficient;curve fitting;data compression ratio;pixel;polynomial;wavelet transform	T. Nithyaletchumy Devi;W. K. Lim;W. N. Tan;Y. F. Tan;H. T. Teng;Y. F. Chang	2009			data compression;computer vision;data compression ratio;computer science;theoretical computer science;lossless compression;block-matching algorithm;motion compensation;texture compression;computer graphics (images)	Graphics	41.505580097141255	-17.437985239081492	142876
b8c19dcf17204245c67c76acd22ff5828d40a054	adaptive image watermarking scheme using fuzzy entropy and ga-elm hybridization in dct domain for copyright protection	digital watermarking;fuzzy entropy;discrete cosine transform;extreme learning machine;genetic algorithm	A novel semi-blind image watermarking scheme based on fuzzy entropy and genetic algorithm (GA)--extreme learning machine (ELM) hybridization in discrete Cosine transform (DCT) domain for copyright protection is proposed in this paper. The selection of non overlapping blocks to embed the binary watermark is based on fuzzy entropy. As fuzzy entropy is able to discriminate data distribution under noise corrupted and redundant condition, feature extraction is more robust against various attacks. Each selected block is followed by 2-D DCT to transform it from spatial to frequency domain. Low frequency coefficients have good energy compactness and are robust to image processing attacks. As addition of noise corresponds to high frequency coefficients, these are not considered to embed the watermark in the proposed approach. The optimal scaling factor used to control the strength of watermark for each selected block of the image based on its noise sensitivity and tolerance limit is determined using GA optimization process. ELM is used to find an optimal regression function between the input feature vector (low frequency DCT coefficients) and corresponding target vector (in which the watermark bits are embedded) of each selected block. Then watermark embedding and extraction is performed intelligently by the regression function obtained by the trained ELM. The experimental results show that the proposed scheme is highly imperceptible and robust to geometric and non geometric attacks such as JPEG compression, filtering, noise addition, sharpening, gamma correction, scaling and cropping etc. To demonstrate the effectiveness of the proposed scheme, comparison with the state-of-art techniques clearly exhibits its applications for copyright protection.	digital watermarking;discrete cosine transform;elm;software release life cycle	Rajesh Mehta;Navin Rajpal;Virendra P. Vishwakarma	2016	Signal Processing Systems	10.1007/s11265-015-1055-8	computer vision;genetic algorithm;digital watermarking;computer science;theoretical computer science;machine learning;discrete cosine transform;mathematics;algorithm	ML	41.09470341810675	-10.27970994509521	142941
70e04bbf78030da4d0f3f611fe6a3ad3691340b1	multiple oil and gas volumetric data visualization with gpu programming	software;sistema interactivo;0705k;evaluation performance;huile;estacion trabajo;aceite;programmation;performance evaluation;station travail;evaluacion prestacion;gpu programming;oil;voxel;capacidad memoria;systeme conversationnel;programacion;oil and gas;computer programming;data storage;visualization;workstation;capacite memoire;memory capacity;interactive system;graphics processing units;pixel;data visualization;graphic processing unit;volume visualization;computing systems;visualisation donnee;computer hardware;programming	In recent years, multi-volume visualization has become an industry standard for analyzing and interpreting large surveys of seismic data. Advances made in computer hardware and software have moved visualization from large, expensive visualization centers to the desktop. Two of the greatest factors in achieving this have been the rapid performance enhancements to computer processing power and increasing memory capacities. In fact, computer and graphics capabilities have tended to more than double each year. At the same time, the sizes of seismic datasets have grown dramatically. Geoscientists regularly interpret projects that exceed several gigabytes. They need to interpret prospects quickly and efficiently and expect their desktop workstations and software applications to be as performant as possible. Interactive, multi-volume visualization is important to rapid prospect generation. Consequently, the ability to visualize and interpret multiple seismic and attribute volumes enhances and accelerates the interpretation process by allowing geoscientists to gain a better understanding of the structural framework, reservoir characteristics, and subtle details of their data. Therefore, we analyzed seismic volume visualization and defined four levels of intermixing: data, voxel, pixel, and image intermixing. Then, we designed and implemented a framework to accomplish these four levels of intermixing. To take advantage of recent advancements in programmable graphics processing units (GPUs), all levels of intermixing have been moved from the CPU into the GPU, with the exception of data intermixing. We developed a prototype of this framework to prove our concept. This paper describes the four levels intermixing, framework, and prototype; it also presents a summary of our results and comments made by geoscientists and developers who evaluated our endeavor.	data visualization;graphics processing unit	Jim Ching-Rong Lin;Cass Hall	2007		10.1117/12.698706	programming;simulation;visualization;workstation;telecommunications;computer science;theoretical computer science;computer programming;voxel;data visualization;pixel;computer graphics (images)	Visualization	39.7530886766503	-20.779967551877572	142984
21f1b62bc7bbbfc19432ba87276e195331913e72	ftv (free-viewpoint tv)	three dimensional displays tv transform coding standardization cameras visualization video coding;multi view video coding;three dimensional television;3dv ftv free viewpoint tv fdu mvc;data format;video coding;error compensation;video coding error compensation three dimensional television;error compensation 3dtv mpeg multiview video coding 3d video free viewpoint tv data unit;3d video;3d display;internal standard;real time systems	We have developed a new type of television named FTV (Free-viewpoint TV). FTV is the ultimate 3DTV that enables us to view a 3D scene by freely changing our viewpoints. At present, FTV is available on a single PC or a mobile player. The international standardization of FTV has been conducted in MPEG. The first phase of FTV was MVC (Multi-view Video Coding) and the second phase is 3DV (3D Video). FDU (FTV Data Unit) is proposed as a data format for 3DV. FTU can compensate errors of the synthesized views caused by depth error.	3d television;asp.net mvc;first-time user experience;moving picture experts group	Masayuki Tanimoto	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5652084	computer vision;stereo display;computer science;video capture;internal standard;multimedia;multiview video coding;computer graphics (images)	Robotics	43.15167568505659	-21.172593026217296	143572
a357ec1a0c4d6b016b5ca315991c552473a4c34c	authentication of h.264 streams by direct watermarking of cavlc blocks	filigranage numerique;desciframiento;protection information;digital watermarking;syntax;decodage;decoding;securite;video signal processing;codigo longitud variable;authentication;variable length code;syntaxe;authentification;algorithme;computer programming;algorithm;video coding;autenticacion;codage video;proteccion informacion;code longueur variable;information protection;filigrana digital;safety;traitement signal video;video;sintaxis;seguridad;algoritmo	"""In this work we report on the first H.264 authentication watermarker that operates directly in the bitstream with no video decoding or partial decompression. The main contribution of the work is identification of a watermarkable code space in H.264 protocol. The algorithm creates """"exceptions"""" in H.264 code space that only the decoder understands while keeping the bitstream syntax compliant The code space is defined over the Context Adaptive Variable Length Coded(CAVLC) portion of protocol. What makes this algorithm possible is the discovery that most of H.264 code space is in fact unused. The watermarker securely maps eligible CAVLC to unused portions of the code space. Security is achieved through a shared key between embedder and decoder. The watermarked stream retains its file size, remains visually transparent, is secure against forging and is reversible. Since the watermark is placed post compression it remains fragile to re encoding and other tampering attempts."""	authentication;bitstream;coefficient;context-adaptive variable-length coding;data compression;digital watermarking;exception handling;h.264/mpeg-4 avc;map;symmetric-key algorithm;video decoder;watermark (data file)	Bijan G. Mobasseri;Yatish Naik Raikar	2007		10.1117/12.703369	real-time computing;telecommunications;computer science;theoretical computer science;authentication;computer security	Security	41.61032369991471	-13.211584372356146	143588
aeafe6bbec08725cda83bc766d230be5eaffe72f	image coding using pyramid vector quantization of subband coefficients	analytical models;subband data;image coding;information systems;large vector dimensions;lattices;error resiliency;product pyramid vq pyramid vector quantization subband coefficients image coding algorithm large vector dimensions simulations compression performance fixed rate code error resiliency inner pyramid vq statistical analysis subband data;transform coding;compression performance;inner pyramid vq;algorithm;vector quantization;statistical analysis;statistical analysis image coding vector quantisation;discrete cosine transforms;pyramid vector quantization;subband coefficients;error resilience;vector quantizer;image coding vector quantization lattices hardware information systems laboratories transform coding statistical analysis analytical models discrete cosine transforms;vector quantisation;subband coding;product pyramid vq;hardware;fixed rate code	This paper presents an improved algorithm using pyramid vector quantization with subband decomposed images. Specifically, the use of large vector dimensions and different dimensions for each subband yields significant improvement over previously reported results. Simulations reveal compression performance comparable to JPEG using a purely fixed-rate code, which has less hardware complexity and greater error resiliency. A comparison between product and inner pyramid VQ using statistical analysis of subband data and simulations demonstrates that product pyramid VQ is better suited for subband coding. >	coefficient;vector quantization	Ely K. Tsern;Teresa H. Y. Meng	1994		10.1109/ICASSP.1994.389379	sub-band coding;computer vision;transform coding;speech recognition;computer science;theoretical computer science;lattice;mathematics;information system;vector quantization;statistics	ML	47.40578548975401	-11.87547648101983	143755
af8383e1e9d7be17665c3622432072f13820d32f	predictive subband image coding with wavelet transform	transformation ondelette;simulation ordinateur;desviacion tipica;image coding;image processing;edge detection;standard deviation;subband decomposition;compresion senal;procesamiento imagen;valor medio;analyse multiresolution;traitement image;compression signal;deteccion contorno;detection contour;codage predictif;scalar quantization;wavelet transform;descomposicion subbanda;quantification scalaire;signal compression;ecart type;codificacion predictiva;valeur moyenne;compression ratio;mean value;vector quantizer;simulacion computadora;transformacion ondita;decomposition sous bande;image decomposition;multiresolution analysis;predictive coding;computer simulation;wavelet transformation;analisis multiresolucion	Wavelet transform can decompose images into various multiresolution subbands. In these subbands the correlation exists. A novel technique for image coding by taking advantage of the correlation is addressed. It is based on predictive edge detection from the LL band of the lowest resolution level to predict the edge in the LH, HL and HH bands in the higher resolution level. If the coefficient is predicted as an edge it is preserved; otherwise, it is discarded. In the decoder, the location of the preserved coefficients can also be found as in the encoder. Therefore, no overhead is needed. Instead of complex vector quantization, which is commonly used in subband image coding for high compression ratio, simple scalar quantization is used to code the remaining coefficients and achieves very good results. ( 1998 Elsevier Science B.V. All rights reserved.	coefficient;edge detection;encoder;horseland;lh (complexity);ll parser;overhead (computing);quantization (signal processing);vector quantization;wavelet transform	Wen-Chuang Huang;Long-Wen Chang	1998	Sig. Proc.: Image Comm.	10.1016/S0923-5965(97)00053-2	computer simulation;multiresolution analysis;computer vision;speech recognition;edge detection;image processing;computer science;compression ratio;mathematics;standard deviation;statistics;mean;wavelet transform	Vision	45.90342815475942	-13.943528987027056	143842
1e577d6b9654eda9b857d0e49dc48d761de98729	a spiht-like image coder based on the contourlet transform	hierarchical structure;rate distortion;image coding contourlet wavelet;image coding;wavelet transforms image coding anisotropic magnetoresistance bit rate transform coding approximation error filter bank testing telecommunication traffic wireless networks;image resolution;spiht like image coder;indexing terms;image discontinuities;wavelet transforms;wavelet transform;wavelet transform hybrid contourlet transform decomposition spiht like image coder image edges image discontinuities;image edges;contourlet;transforms;wavelet transforms image coding;correlation;encoding;wavelet;hybrid contourlet transform decomposition	The contourlet transform was recently proposed to overcome the limited ability of wavelet to represent image edges and discontinuities. Besides retaining the desirable characteristics of wavelet transform, such as multiresolution and localization, it has two additional important features: directionality and anisotropy. In this work we propose a new image coding technique based on an hybrid contourlet-wavelet decomposition. The encoder builds upon the well-known SPIHT algorithm, which is suitably modified to take into account the new hierarchical structure of the transform coefficients and the correlation between them. Both numerical results and visual quality confirm the potential of this approach, especially for images with high texture content.	algorithm;coefficient;contourlet;encoder;numerical analysis;set partitioning in hierarchical trees;wavelet transform;whole earth 'lectronic link	Sara Parrilli;Luisa Verdoliva;Giovanni Poggi	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712403	wavelet;computer vision;contourlet;speech recognition;s transform;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;pattern recognition;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;top-hat transform;wavelet transform	Robotics	43.674647436452204	-15.434050911652028	143975
1ff14dd4fa709d56ee67d1fad576d53437c4a47d	synthesis error compensated multiview video plus depth for representation of multiview video	video coding data compression image reconstruction image representation;three dimensional displays image coding bit rate transform coding image reconstruction encoding psnr;3d format synthesis error multiview video plus depth remainder residual;compression performance synthesis error compensated multiview video plus depth second mvd multiview video representation residual based representation layered depth video image reconstruction virtual image	SECOND-MVD (Synthesis Error COmpeNsateD Multiview Video plus Depth) is an alternative 3D format that we introduce for representation of multiview video. In this data format, images at some viewpoint remain original, and the others are converted to a novel format. Residual based representation, such as layered depth video and free-viewpoint TV data unit were proposed. We propose hybrid image that not only consists of residual but also remainder pixels. Generation and reconstruction process of a hybrid image uses virtual image synthesized by the images that remained original in SECOND-MVD. In this paper, we investigate the compression performance of SECOND-MVD using hybrid image. Experiments demonstrate reduction in bit rate using hybrid image against residual image in SECOND-MVD framework.	pixel	Mehrdad Panahpour Tehrani;Akio Ishikawa;Makoto Okui;Naomi Inoue;Keita Takahashi;Toshiaki Fujii	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6853725	computer vision;image processing;computer science;multimedia;smacker video;multiview video coding;computer graphics (images)	Vision	44.20360110786634	-19.017986511567724	144064
dcedc6f1dc45ee5c35ac09eb0b1681ca7d36303f	3d video coding using advanced prediction, depth modeling, and encoder control methods	autostereoscopic display;3d video coding;video coding rendering computer graphics stereo image processing;depth modeling;disparity compensated prediction advanced techniques;h 264 avc;interview prediction;distortion measurement;transform coding;bit rate;hevc;bit rate reductions 3d video coding depth modeling multiview video plus depth format video views associated depth maps autostereoscopic display depth image based rendering techniques hevc mvc extension h 264 avc disparity compensated prediction advanced techniques interview prediction intercomponent prediction;depth image based rendering techniques;video coding;intercomponent prediction;mvc extension;vectors;multiview video plus depth format;three dimensional displays;encoding three dimensional displays bit rate video coding transform coding vectors distortion measurement;stereo image processing;video views;depth image based rendering;depth map;rendering computer graphics;encoding;3d video;bit rate reductions;control method;associated depth maps	The presented approach for 3D video coding uses the multiview video plus depth format, in which a small number of video views as well as associated depth maps are coded. Based on the coded signals, additional views required for displaying the 3D video on an autostereoscopic display can be generated by depth image based rendering techniques. The developed coding scheme represents an extension of HEVC, similar to the MVC extension of H.264/AVC. However, in addition to the well-known disparity-compensated prediction advanced techniques for inter-view and inter-component prediction, the representation of depth blocks, and the encoder control for depth signals have been integrated. In comparison to simulcasting the different signals using HEVC, the proposed approach provides about 40% and 50% bit rate savings for the tested configurations with 2 and 3 views, respectively. Bit rate reductions of about 20% have been obtained in comparison to a straightforward multiview extension of HEVC without the newly developed coding tools.	asp.net mvc;autostereoscopy;binocular disparity;data compression;depth map;encoder;h.264/mpeg-4 avc;high efficiency video coding;map;simulcast;stereoscopic video coding;whole earth 'lectronic link	Heiko Schwarz;Christian Bartnik;Sebastian Bosse;Heribert Brust;Tobias Hinz;Haricharan Lakshman;Detlev Marpe;Philipp Merkle;Karsten Müller;Hunn Rhee;Gerhard Tech;Martin Winken;Thomas Wiegand	2012	2012 Picture Coding Symposium	10.1109/PCS.2012.6213271	cognitive psychology;computer vision;transform coding;computer science;mathematics;multimedia;encoding;statistics;multiview video coding;depth map;computer graphics (images)	Graphics	44.41738860907623	-19.1013158962448	144195
71c236ffda3ac4f6808a3c1e8b7e07e1f1ba9b22	multiple description coding for scalable video coding with redundant slice	video coding code standards quantisation signal;encoding static var compensators packet loss cities and towns video coding quantization signal;redundant slice scalable video coding multiple description coding;packet loss channel multiple description coding scalable video coding redundant slice adaptable capability heterogeneous network structures inter layer prediction svc coded bitstream channel error multiple description scheme svc standard primary slice mismatch error propagated error quantization step prediction path channel packet loss rate	Scalable video coding(SVC) is designed to provide adaptable capability for heterogeneous network structures with its scalabilities. Due to the introducing of inter-layer prediction, SVC coded bitstream is much vulnerable to the channel error. To solve this problem, a lot of schemes are proposed. However, they are either not compatible with the standard or just focus on one of scalabilities in SVC. In this paper, we propose a multiple description scheme that is compatible with SVC standard and supports all the scalabilities. The proposed scheme is based on the redundant slice in the SVC standard, in which each layer is composed of primary slice and redundant slice by interleaving. To deal with the mismatch error and propagated error, the quantization step of redundant slice and the mode of each macroblock is tuned by considering the prediction path in the same layer and between different layers, as well as the channel packet loss rate. The experimental results show that the proposed scheme is efficient for the packet loss channel.	bitstream;data compression;forward error correction;macroblock;multiple description coding;network packet;propagation of uncertainty;scalability;scalable video coding;simulation;software propagation	Chunyu Lin;Yao Zhao;Huihui Bai;Kwong Huang Goh	2012	2012 18th IEEE International Conference on Networks (ICON)	10.1109/ICON.2012.6506581	scalable video coding;real-time computing;telecommunications;computer science;coding tree unit;computer network	Robotics	48.25812232300389	-16.4585055710016	144242
4f24b84a8d43e66db4ae97c851e30ef3f1e81431	a new approach to color image secret sharing	private key cryptography image colour analysis image reconstruction;inverse cryptographic processing color image secret sharing secret sharing scheme decryption operations encryption operations decomposed binary domain rgb color domain;cryptography silicon abstracts ice	A new secret sharing scheme for color images is introduced. Using the {k,n}-secret sharing strategy the proposed method encrypts the color image into n color shares. The secret information is recovered only if the k (or more) allowed shares are available for decryption. Both encryption and decryption operations are performed by operating at the bit-levels of the decomposed color image. Modifying the spatial arrangements of the binary components the method produces color shares which vary in both the spectral characteristics among the RGB components and the spatial correlation between the neighboring color vectors. Since encryption is performed in the decomposed binary domain, there is no obvious relationship in the RGB color domain between any two color shares or between the original color image and any of the n shares. This increases protection of the secret information. Inverse cryptographic processing of the shares must be realized in the decomposed binary domain and the procedure reveals the original color image with perfect reconstruction.	bit-level parallelism;color image;encryption;image processing;secret sharing	Rastislav Lukac;Konstantinos N. Plataniotis;Bogdan Smolka;Anastasios N. Venetsanopoulos	2004	2004 12th European Signal Processing Conference		computer vision;color quantization;color depth;binary image;high color;mathematics;homomorphic secret sharing;internet privacy;computer security	Vision	39.503963008179944	-10.150308229250204	144403
294c526eb8e3ee7316a4347248db73304d94b371	unequal error protection and progressive decoding for jpeg2000	transformation ondelette;desciframiento;traitement signal;channel coding;protective device;image coding;image processing;decodage;decoding;imagen fija;taux erreur;correction erreur;quality improvement;procesamiento imagen;qualite image;traitement image;dispositivo proteccion;refinement method;codage image;compression image;fixed image;image compression;image transmission;progressive decoding;error correction;signal processing;image quality;unequal error protection;plotkin construction;jpeg2000;error rate;image fixe;calidad imagen;transformacion ondita;correccion error;methode raffinement;indice error;transmission image;uep;procesamiento senal;metodo afinamiento;wavelet transformation;dispositif protection;transmision imagen;compresion imagen	This paper presents an unequal error protection scheme based on the Plotkin construction for channel (error control) codes. The resulting codes offer the novel ability of using one long channel codeword to protect an entire image, yet still allowing progressive decoding. Progressive quality improvements occur in two ways: the first is the usual progressive refinement, where image quality is improved as more data are received; the second is that residual error rates of earlier received data are reduced as more data are received.	code word;download;error detection and correction;image quality;jpeg 2000;plotkin bound;progressive refinement;refinement (computing)	Lingling Pu;Michael W. Marcellin;Bane V. Vasic;Ali Bilgin	2005	IEEE International Conference on Image Processing 2005	10.1016/j.image.2006.12.007	image quality;computer vision;error detection and correction;channel code;telecommunications;image processing;image compression;word error rate;computer science;signal processing;jpeg 2000;mathematics;algorithm;statistics	Visualization	46.606416872424994	-13.506751288068783	144653
3f7c4af9db52e71aa75dfce1b3a3c16c07c4e3d6	lossless data hiding using integer wavelet transform and threshold embedding technique	histograms;watermarking;art;image coding;psnr;least significant bit plane;integer wavelet transform;data encapsulation wavelet transforms histograms frequency gray scale digital images wavelet coefficients art payloads psnr;high frequency cdf;wavelet transforms data encapsulation image coding watermarking;gray scale;data encapsulation;preprocessing lossless data hiding integer wavelet transform threshold embedding technique digital image least significant bit plane lsb high frequency cdf histogram modification psnr;wavelet transforms;lsb;histogram modification;payloads;digital image;preprocessing;frequency;digital images;wavelet coefficients;lossless data hiding;threshold embedding technique	This paper presents a new lossless data hiding method for digital images using integer wavelet transform and threshold embedding technique. Data are embedded into the least significant bit-plane (LSB) of high frequency CDF (2, 2) integer wavelet coefficients whose magnitudes are smaller than a certain predefined threshold. Histogram modification is applied as a preprocessing to prevent overflow/underflow. Experimental results show that this scheme outperforms the prior arts in terms of a larger payload (at the same PSNR) or a higher PSNR (at the same payload)	arithmetic underflow;bit plane;coefficient;digital image;embedded system;least significant bit;lossless compression;most significant bit;peak signal-to-noise ratio;preprocessor;wavelet transform	Guorong Xuan;Yun Q. Shi;Chengyun Yang;Yizhan Zhen;Dekun Zou;Peiqi Chai	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521722	computer vision;computer science;theoretical computer science;mathematics;digital image;statistics	EDA	40.946576781639244	-11.569076188973224	144825
473877edcdeec647f46e179dd54031442ba5416f	rate-distortion bound for joint compression and classification with application to multiaspect scattering	minimisation;signal reconstruction hidden markov models rate distortion theory minimisation data compression signal classification state estimation;signal reconstruction distortion rate distortion theory joint compression joint classification lagrangian distortion euclidean error alternating minimization procedure blahut arimoto algorithm hmm hidden markov model state estimation;rate distortion;rate distortion hidden markov models distortion measurement lagrangian functions computer errors state estimation signal reconstruction data compression;data compression;hidden markov model;state estimation;rate distortion theory;hidden markov models;signal classification;alternating minimization;signal reconstruction	Rate-distortion analysis is applied to the problem of joint compression and classification. A Lagrangian distortion measure is used to consider both the Euclidean error in reconstructing the original data as well as the classification performance. The bound is calculated based on an alternating-minimization procedure, representing an extension of the Blahut-Arimoto algorithm. This rate-distortion framework is then applied to a joint compression and target-orientation estimation problem, based on a sequence of scattered waveforms measured at multiple target-sensor orientations. A hidden Markov model-Markov model (HMM-MM) is used as the statistical description for the source, here representative of multiaspect scattering data. Target-orientation estimation reduces to assessing the underlying HMM states from a sequence of observations. After deriving the rate-distortion function, we demonstrate that discrete HMM performance based on Lloyd encoding is far from this bound. Performance is improved via block coding, based on Bayes vector quantization. Results are presented for multiaspect acoustic scattering from an underwater elastic target, using measured and synthesized data.	acoustic cryptanalysis;blahut–arimoto algorithm;distortion;estimation theory;hidden markov model;markov chain;rate–distortion theory;vector quantization	Yanting Dong;Lawrence Carin	2003	IEEE Sensors Journal	10.1109/DCC.2003.1194042	data compression;signal reconstruction;minimisation;mathematical optimization;rate–distortion theory;computer science;pattern recognition;mathematics;markov model;hidden markov model;statistics	ML	49.26527325539131	-11.050382608518838	144927
5830b6c28e2c71eb0cdab0e7c2916244c4c78cd7	wyner-ziv coding of multiview images with unsupervised learning of two disparities	unsupervised learning;disparity decoder;unsupervised learning image coding;rate distortion;wyner ziv;image coding;codecs;psnr;data compression;disparity image coding data compression stereo vision;decoding;disparity;indexing terms;image reconstruction;disparity decoder wyner ziv coding multiview images unsupervised learning;stereo vision;transforms;decoding image reconstruction codecs transforms encoding image coding psnr;wyner ziv coding;side information;encoding;multiview images	Wyner-Ziv coding of multiview images is an attractive solution because it avoids communications between individual cameras. To achieve good rate-distortion performance, the Wyner-Ziv decoder must reliably estimate the disparities between the multiview images. For the scenario where two reference images exist at the decoder, we propose a codec that effectively performs unsupervised learning of the two disparities between an image being Wyner-Ziv coded and the two reference images. The proposed two-disparity decoder disparity-compensates the two references images and generates side information more accurately than an existing one-disparity decoder. Experimental results with real multiview images demonstrate that the proposed codec achieves PSNR gains of 1-5 dB over the one-disparity codec.	binocular disparity;codec;distortion;peak signal-to-noise ratio;unsupervised learning	David M. Chen;David P. Varodayan;Markus Flierl;Bernd Girod	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607513	data compression;iterative reconstruction;unsupervised learning;computer vision;codec;index term;peak signal-to-noise ratio;computer science;stereopsis;pattern recognition;multimedia;encoding;statistics	Vision	49.04000183636258	-17.113515818243968	144956
a36dab2451d81a3047af391c563838c8593c1778	a color image digital watermarking scheme based on sofm	robust watermarking;image processing;digital watermark;copy protection;copyright protection;digital image watermarking;vector quantizer;color image	Digital watermarking technique has been presented and widely researched to solve some important issues in the digital world, such as copyright protection, copy protection and content authentication. Several robust watermarking schemes based on vector quantization (VQ) have been presented. In this paper, we present a new digital image watermarking method based on SOFM vector quantizer for color images. This method utilizes the codebook partition technique in which the watermark bit is embedded into the selected VQ encoded block. The main feature of this scheme is that the watermark exists both in VQ compressed image and in the reconstructed image. The watermark extraction can be performed without the original image. The watermark is hidden inside the compressed image, so much transmission time and storage space can be saved when the compressed data are transmitted over the Internet. Simulation results demonstrate that the proposed method has robustness against various image processing operations without sacrificing compression performance and the computational speed.	authentication;codebook;color image;copy protection;data compression;digital image;digital watermarking;embedded system;image processing;internet;quantization (signal processing);self-organizing map;simulation;vector quantization	J. Anitha;S. Immanuel Alex Pandian	2010	CoRR		computer vision;color image;image processing;digital watermarking;computer science;theoretical computer science;digital image processing;watermark;computer graphics (images)	Graphics	40.66885158093158	-11.781138472037295	144988
2f54c660db8a8ed15dd099f7a51ffd3dfec94521	joint optimization coding for level and map information in h.264/avc	image coding;coefficients pruning;video coding;joint optimization coding;scalar quantization;comparative study;h 264;rate distortion optimization	Since the joint optimization coding for the level and the map information of transformed coefficients has been applied to the image coding successfully, this paper addresses the problem of how to optimize the two parts of information of residual coefficients jointly in the state-of-the-art video coding standard-H.264/AVC. In order to solve this problem, this paper presents a pruning method to zero out residual coefficients, which are not valuable for encoding in rate-distortion optimization (RDO) sense. Furthermore, a joint optimization scheme is presented to combine scalar quantization with this pruning method together to further improve the compression performance of the proposed algorithm. Experiments have been conducted based on the reference encoder JM12.4 of H.264/AVC. Comparative studies show that the proposed joint coding method can achieve a PSNR gain more than 0.5dB at a certain bit rate, compared with the coding method in the H.264/AVC. Especially, the PSNR gain can reach up to 0.9dB, or equivalently, 17% bit rate saving can be achieved at some special cases.	h.264/mpeg-4 avc;mathematical optimization	Xingsong Hou;Duan Xue;Baiping Jin;Lijuan Cao	2011	Sig. Proc.: Image Comm.	10.1016/j.image.2011.07.002	scalable video coding;mathematical optimization;linear network coding;shannon–fano coding;harmonic vector excitation coding;variable-length code;theoretical computer science;context-adaptive variable-length coding;comparative research;coding tree unit;mathematics;tunstall coding;rate–distortion optimization;context-adaptive binary arithmetic coding	ML	46.91739249309438	-17.101332333189205	145186
1b866157899602fee5de2203b0f98d061f4c6a5f	global distortion optimal bit allocation scheme for image compression	image compression		distortion;image compression	J. M. Wang;Meng Zu Mao;Y. Y. Xue	2003			image compression;bit (horse);distortion;electronic engineering;data compression;data compression ratio;computer science	Theory	43.07734195441856	-14.5707332157581	145319
0f705f4d027469aa5de1b33ae1a3213d00f06205	a comparative study of multiple description video coding in p2p: normal mdsq versus flexible dead-zones	multiple description scalar quantization;multiple description scalar quantizer;quantization;multiple description;video streaming;image coding;motion compensation;decoding;dead zone;reference frame;p2p;video coding quantization image reconstruction decoding quadratic programming streaming media gold image coding discrete cosine transforms karhunen loeve transforms;joints;dct domain translation;dead zone mdc scalar quantization mdsq;motion compensated;quantisation signal;video coding;mismatching problem multiple description video coding p2p mdsq dead zone image encoding decoding multiple description scalar quantization video streaming drifting error motion compensation dct domain translation;drifting error;scalar quantization;streaming media;discrete cosine transforms;mismatching problem;multiple description coding;mdc;mdsq;peer to peer computing;video streaming decoding discrete cosine transforms motion compensation peer to peer computing quantisation signal video coding;multiple description video coding;image encoding	Multiple-description coding (MDC) encodes a source image or video into several distinct descriptions. Each description can be decoded independently and more than one description can also be decoded jointly so as to deliver a higher quality. One typical design of an MDC encoder is based on the so-called multiple description scalar quantization (MDSQ). A simple MDSQ system is to choose different step-sizes used in all involved quantizers. Another implementation is to apply flexible dead-zones in all quantizers (whereas keeping the same step-size across them). Two issues turn to be particularly important when MDC is used in video streaming applications in the P2P scenario. First, we need to consider more than two descriptions to fit the P2P reality. Second, we have to eliminate any possible drifting errors that are resulted from the use of different reference frames (in different descriptions) at the motion-compensation stage. In this paper, we introduce a DCT-domain translation to solve the mismatching problem in different reference frames. Then, we present some comparative results of the two approaches mentioned above, with detailed pros and cons for each one as well as some practical considerations.	data compression;discrete cosine transform;encoder;motion compensation;multiple description coding;peer-to-peer;quantization (signal processing);spatial reference system;streaming media	Shuyuan Zhu;Jeff Siu-Kei Au-Yeung;Bing Zeng	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202695	reference frame;computer vision;dead zone;quantization;computer science;theoretical computer science;multiple description coding;peer-to-peer;mathematics;motion compensation;algorithm	Vision	48.43993434465076	-17.072833828164722	145569
0fa4ea0aa8217477017e3be3124e50a2179baa5c	a very low bit-rate minimalist video encoder based on matching pursuits	real time;video codec;image quality;matching pursuit	This work proposes and implements a simple and efficient video encoder based on the compression of consecutive frame differences using sparse decomposition through matching pursuits. Despite its minimalist design, the proposed video codec has performance compatible to H.263 video standard and, unlike other encoders based on similar techniques, is capable of encoding videos in real time. Average PSNR and image quality consistency are compared to H.263 using a set of video sequences.	encoder	Vitor de Lima;Hélio Pedrini	2010		10.1007/978-3-642-16687-7_27	video compression picture types;image quality;microsoft video 1;computer vision;computer science;video quality;machine learning;video tracking;block-matching algorithm;multimedia;video processing;rate–distortion optimization;h.261;video denoising;matching pursuit;multiview video coding;computer graphics (images)	Vision	44.97745555472945	-18.902144010720534	145749
13658b3d2e2c277c4c02784e8c1821d7c7922979	improved halftone data hiding scheme using hilbert curve neighborhood toggling	hilbert transforms;data hiding;psnr;image processing;visual quality halftone data hiding scheme hilbert curve neighborhood toggling randomly selected locations halftone images data hiding smart pair toggling dhspt;data encapsulation;indexes;visualization;visualization indexes psnr payloads humans image processing;hilbert curve data hiding halftone;payloads;humans;halftone;image processing data encapsulation hilbert transforms;hilbert curve	In this paper, we examine the effects of embedding in randomly selected locations and present a simple but novel high capacity data hiding method for halftone images. Data hiding smart pair toggling (DHSPT) is a flexible method for hiding data directly into halftone images in high embedded capacity applications. It embedded data into randomly selected locations, having the chance of forced single-pixel toggling especially when the payload increases. The proposed method avoids the chance of forced single-pixel toggling by selecting the best pair of connected neighboring pixels out of a sequence of candidate pairs along Hilbert curve for a to-be-embedded bit. The proposed method can significantly improve the visual quality of marked halftone images especially for high-capacity data hiding.	embedded system;hilbert curve;pixel;randomness	Brian K. Lien;Zhi-Lin Lan	2011	2011 Seventh International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIHMSP.2011.15	database index;computer vision;payload;visualization;peak signal-to-noise ratio;image processing;computer science;theoretical computer science;mathematics;programming language;information hiding;computer graphics (images)	Robotics	40.96661431751029	-12.61323939941662	145757
1c6706776880fd486eed70e4cf6c89bcd41c20d0	a mode mapping and optimized mv conjunction based h.264/svc to h.264/avc transcoder with medium-grain quality scalability for videoconferencing	mgs scalability;mode mapping;mv conjunction;svc-to-avc transcoding;videoconferencing		h.264/mpeg-4 avc;pdf/a;scalability	Lei Sun;Zhenyu Liu;Takeshi Ikenaga	2014	IEICE Transactions		embedded system;real-time computing;telecommunications;computer science;multimedia;videoconferencing	Robotics	43.895754261691195	-21.325714891668788	145771
895e2d98c2aaaf995d22fc96a46a5279ed98f1c9	receding horizon surveillance with temporal logic specifications	robot sensing systems;predictive control;surveillance;path planning;temporal logic;automata robot sensing systems surveillance real time systems trajectory aerospace electronics;real time;receding horizon controller receding horizon surveillance temporal logic specifications robotic vehicle linear temporal logic formulas surveillance mission;mobile robots;satisfiability;receding horizon;automata;trajectory;optimal path;linear temporal logic;aerospace electronics;vehicles mobile robots path planning predictive control surveillance temporal logic;vehicles;cumulant;point of interest;real time systems	In this paper we consider a setting where a robotic vehicle is commissioned to provide surveillance in an area where there are multiple targets, while satisfying a set of high level, rich specifications expressed as Linear Temporal Logic formulas. Each target has an associated reward. The goal of the vehicle is to maximize the cumulative collected reward while satisfying the given high level task specification. By the nature of a surveillance mission, targets points of interest are detected in real time around the current location of the vehicle; hence we employ a receding horizon controller to compute the optimal path of the vehicle inside a subset of the mission space. This paper provides a framework which guarantees that the overall trajectory of the system satisfies the desired linear temporal logic specification, while the control decisions are made based on local information obtained in real time.	control theory;high-level programming language;linear temporal logic;markov chain;model checking;partially observable markov decision process;point of interest;probabilistic ctl;probabilistic automaton;real-time computing;robot;specification language	Xu Chu Ding;Calin Belta;Christos G. Cassandras	2010	49th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2010.5717131	control engineering;mobile robot;linear temporal logic;point of interest;simulation;temporal logic;computer science;engineering;trajectory;control theory;motion planning;automaton;model predictive control;cumulant;satisfiability	Robotics	52.756632132426155	-21.30407289640017	145824
bd12b373dcb6630f4af0553a4f99b5a223d0abc0	a review of dna microarray image compression	dna;loss measurement;standard compressor;image coding;image segmentation;calic;data mining;image coding dna;dna microarray image compression;jpeg2000 microarray images microarray image compression image coding standards;image coding image segmentation loss measurement dna transforms data mining;image coding standards;microarray image compression;image coding standard;transforms;jpeg ls;jpeg2000;prediction based scheme;microarray images;standard compressor dna microarray image compression image coding standard prediction based scheme calic jpeg ls jpeg2000 zero wavelet decomposition levels;zero wavelet decomposition levels	We review the state of the art in DNA micro array image compression. First, we describe the most relevant approaches published in the literature and classify them according to the stage of the typical image compression process where each approach makes its contribution. We then summarize the compression results reported for these specific-specific image compression schemes. In a set of experiments conducted for this paper, we obtain results for several popular image coding techniques, including the most recent coding standards. Prediction-based schemes CALIC and JPEG-LS, and JPEG2000 using zero wavelet decomposition levels are the best performing standard compressors, but are all outperformed by the best micro array-specific technique, Battiato's CNN-based scheme.	bpp (complexity);dna microarray;data compression;experiment;image compression;jbig;jpeg 2000;kakadu;least squares;lempel–ziv–markov chain algorithm;lossless jpeg;lossless compression;netbsd gzip / freebsd gzip;wavelet;bzip2	Miguel Hernández-Cabronero;Ian Blanes;Joan Serra-Sagristà;Michael W. Marcellin	2011	2011 First International Conference on Data Compression, Communications and Processing	10.1109/CCP.2011.21	data compression;computer vision;computer science;bioinformatics;theoretical computer science;lossless compression	Robotics	42.816643729130256	-17.167290802756234	145836
58de7d9f99af1ac6f5cd64d4a4924934e8427fa7	a comparative study of objective video quality assessment metrics		This paper presents a comparison of several video quality metrics, analysing their performance against different types of distortions. Usually, comparisons are made considering a full dataset with few different degradations. We are presenting here a comparison using three very different datasets (VQEG Phase I, LIVE VQA and ReTRIeVED) and a fourth dataset which was generated in a mobile phone network simulator. This was done to check if the video quality metrics can correctly measure the degradations created by variations in the network, very close to real scenarios. The analysis was done with 13 full reference metrics (including Opticom’s PEVQ commercial tool) and two no-reference metrics. We have concluded that NTIA’s VQM achieved the best results, in most of the cases. It is an open source algorithm that outperformed most of the other techniques, including the licensed PEVQ.	algorithm;data rate units;distortion;elegant degradation;experiment;matlab;ms-dos;msu lossless video codec;mobile phone;noise reduction;open-source software;simulation;structural similarity	Carlos A. B. Mello;Marília M. Saraiva;Diego P. A. Menor;Ricardo Nishihara	2017	J. UCS		data mining;computer science;video quality	Vision	45.42109493776515	-21.998534515974164	145945
57f6f83d2bec9e1422da496371ce32b01e40be2d	execution and analysis of high-level tasks with dynamic obstacle anticipation	robot sensing systems;urban environment;probability;autonomous vehicle;temporal logic;mobile robots;statistical analysis;temporal logic collision avoidance data handling mobile robots probability;roads;robot sensing systems vehicles roads vehicle dynamics collision avoidance probabilistic logic;statistical analysis high level tasks dynamic obstacle anticipation high level robot controllers sensor data probabilistic anticipation dynamic obstacles behavior autonomous vehicle urban environment correct by construction controller temporal logic formulas road segments;collision avoidance;vehicles;data handling;probabilistic logic;vehicle dynamics	This paper uniquely embeds high-level robot controllers with sensor data obtained from abstracting probabilistic anticipation of the behavior of dynamic obstacles. An example problem of an autonomous vehicle operating in an urban environment, in the presence of other vehicles and pedestrians, is used as motivation. The correct-by-construction controller is automatically synthesized from a set of high-level tasks, specified as temporal logic formulas. The anticipated behavior of other vehicles is abstracted to a set of propositions describing the safety of road segments at intersections, and used as the output of high-level sensors for the controller. Such an input to the controller is inherently probabilistic, and this paper investigates the types of probabilistic guarantees that can be made about the system using both formal and statistical analysis.	algorithm;autonomous car;autonomous robot;binary-safe;google map maker;high- and low-level;mathematical model;model checking;probabilistic turing machine;sensor;simulation;temporal logic	Benjamin Johnson;Frank Havlak;Mark E. Campbell;Hadas Kress-Gazit	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224980	control engineering;mobile robot;computer vision;vehicle dynamics;simulation;temporal logic;computer science;engineering;artificial intelligence;group method of data handling;probability;probabilistic logic;statistics	Robotics	52.62516313288778	-21.236262127422258	146092
5e9ff578a83dc0568d3cdbfb67f54fdc3a11db08	blind source separation based digital color image-adaptive watermarking	digital watermarking;watermarking;blind source separation;color;digital watermark;watermarking blind source separation digital signatures image colour analysis independent component analysis;digital signatures;digital color image adaptive watermarking;independent component analysis;blind source separation color watermarking independent component analysis discrete fourier transforms data mining internet pixel discrete wavelet transforms fourier transforms;image colour analysis;pixel;attacks;performance analysis;attacks digital watermarking blind source separation independent component analysis;robustness;owner signature;independent component analysis blind source separation digital color image adaptive watermarking owner signature;color image	In a digital watermarking scheme, it is not convenient to carry the original image all the time in order to detect the owner's signature from the watermarked image. Moreover, for those applications that require different watermark for different copies, it is preferred to utilize some kind of watermark-independent algorithm in extraction. This paper presents a digital color image watermarking scheme using Blind Source Separation to embed the watermark by manipulating the least significant levels of the blue channel of the host color image so as not to bring about a perceptible change in the marked image. The novelty of our scheme lies in determining the mixing matrix for BSS model, based on the smooth and textured metric based on energy content of the image. This makes our method image adaptive to embed color image into color images. Meanwhile the performance analysis of two ICA (Independent Component Analysis) algorithms is introduced in extraction procedure, through which the watermark can be derived efficiently. The proposed method, undergoing different experiments, has shown its robustness against many attacks.	algorithm;blind signal separation;channel (digital image);color image;digital watermarking;experiment;independent computing architecture;independent component analysis;source separation;watermark (data file)	Sangeeta Dhananjay Jadhav;Anjali S. Bhalchandra	2009	2009 Fifth International Conference on Image and Graphics	10.1109/ICIG.2009.98	image restoration;computer vision;speech recognition;binary image;digital watermarking;computer science;machine learning;digital image processing;mathematics;internet privacy;watermark	Vision	40.8302244183479	-10.667638172550308	146331
f3033bfbb59016128f6f86c94a99c5a9f97ae5ca	continous time autonomous air traffic control for non-towered airports	airports;ctmdp continuous time autonomous air traffic control nontowered airports general aviation aircraft ga continuous time markov decision process;solid modeling;aerospace electronics;markov processes;aircraft sparse matrices airports atmospheric modeling markov processes solid modeling aerospace electronics;atmospheric modeling;markov processes air traffic control airports autonomous aerial vehicles continuous time systems;sparse matrices;aircraft	The majority of reported near mid-air collisions that involve a general aviation aircraft (GA) occur in the vicinity of non-towered airports. In this paper, we propose a concept for air traffic collision prevention for GA aircraft, which can be modeled as a continuous-time Markov decision process (CTMDP). The problem is equivalent to controlling independent homogeneous multi-agents under communication or resource constraints. We outline a methodology that leverages the structure of the problem to efficiently solve it despite the large state space. Finally, we present simulation results for aircraft operating under the optimized policies in the traffic pattern.	autonomous robot;constraint (mathematics);markov chain;markov decision process;simulation;software release life cycle;state space	Zouhair Mahboubi;Mykel J. Kochenderfer	2015	2015 54th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2015.7402670	atmospheric model;simulation;sparse matrix;aerospace engineering;engineering;aeronautics;mathematics;markov process;solid modeling;statistics	Robotics	53.27447085990881	-21.10939931782245	146456
d5b59db1e2cc53e13d751046b9b42bfb2c3bcbd0	error resilient video coding schemes for real-time and low-bitrate mobile communications	error resilient video coding;circuit codeur;mobile radiocommunication;resilience erreur;coding circuit;error concealment;image processing;transmission error;procesamiento imagen;error transmision;image bruitee;traitement image;radiocommunication service mobile;experimental result;intra refresh;imagen sonora;video coding;reconstruction image;evaluation subjective;senal video;signal video;reconstruccion imagen;image reconstruction;error resilence;noisy image;circuito codificacion;mobile communication;resultado experimental;mpeg 4;video signal;error resilience;resultat experimental;radiocomunicacion servicio movil;subjective evaluation;erreur transmission;prediction avant;evaluacion subjetiva	In this paper, we present self error resilient video coding schemes for low-bitrate mobile communication. They are the dual forward prediction and the adaptive refreshing. If a coded bit stream su!ers from transmission error, the reconstructed image is degraded and the degradation is propagated temporally. The dual forward prediction minimizes the degradation by using two prediction images selectively. Also, the adaptive refreshing quickly eliminates the degradation by concentrating the cyclic intra refreshing on the moving area of image. The simulations are conducted on MPEG-4 test sequences at 24 k and 48 kbps. Experiment results show that the dual forward prediction is e!ective for the bursty error and it improves the coding e$ciency for some sequences. Concerning the adaptive refreshing, it is shown that both PSNR and subjective quality are improved. In conclusion, the proposed schemes are highly error resilient and suitable for low-bitrate mobile communication. ( 1999 Published by Elsevier Science B.V. All rights reserved.	bitstream;data compression;data rate units;elegant degradation;mobile phone;peak signal-to-noise ratio;point of view (computer hardware company);real-time clock;simulation	Koji Imura;Yutaka Machida	1999	Sig. Proc.: Image Comm.	10.1016/S0923-5965(99)00003-X	iterative reconstruction;computer vision;simulation;mobile telephony;telecommunications;image processing;computer science;mpeg-4	HCI	47.37250746034184	-15.034849298086401	146652
3e790cde58a3166365b474ee40b02e68937f7499	fish pond culture via fuzzy and self-adaptive data fusion application		This study builds an automatic monitoring system for the fish pond culture environment. The purpose of this study is to reduce the financial losses in fish culture stock resulting from natural disasters, and facilitate better control of the fish pond environment. The physical water quality signals are therefore extracted using temperature, dissolved oxygen and pH sensing modules. The water heater, submerged motor pump, air pump, feeding trough and LED illuminating lamp are controlled to improve the water quality and reduce labor. This study utilizes the self-adaptive data fusion approach via a wireless sensors network framework to remotely monitor the fish pond automatic control system. The experimental results show that the fish pond culture environment can be accurately and stably monitored.	automatic control;control system;sensor	Wen-Tsai Sung;Jui-Ho Chen;Sung-Jung Hsiao	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8123082	oxygen saturation;wireless sensor network;control theory;air pump;computer science;water quality;automatic control;sensor fusion;fish farming;control engineering	Robotics	53.0540954490253	-10.615726726160068	146706
9bfdf625db82d14863b921f356e8acb73bf59144	a fast algorithm to compress grey level images	statistical approach;algorithme rapide;codificacion binaria;image storage;image processing;data compression;procesamiento imagen;entropy coding;imagen nivel gris;stockage image;traitement image;reconstruction image;codificacion;binary coding;reconstruccion imagen;image reconstruction;fast algorithm;image quality;compresion;image niveau gris;coding;compresion dato;compression;grey level image;almacenamiento imagen;algoritmo rapido;compression donnee;codage;codage binaire	An algorithm to compress and decompress grey level images is described, in which we combined a classical method like D.P.C.M. with an entropy code based on a new statistical approach. The reconstructed image quality is improved by subsequent image postprocessing. The method is low-time consuming and very simple in order to implement it on P.C. machine.	algorithm;grayscale	A. Bozzoli;M. Dell'Erba;Gianluca Tadini	1989		10.1007/3-540-51815-0_36	data compression;iterative reconstruction;image quality;binary code;image processing;computer science;entropy encoding;artificial intelligence;coding;compression;algorithm;statistics	Vision	45.68749295377145	-13.1411991581481	146978
72e7ceb8a72ea319113afbcf23553b051e26b44b	assessment of the compression efficiency of the mpeg-4 avc specification	rate distortion;multimedia;psnr;h 264 avc;mc ezbc;televisions;multimedia application;video codec;xvid;video coding;distortion;data storage;internet;technology and engineering;video conferencing;mpeg 4;digital video;wmv9;compression;video;divx;wavelets	Video coding is used under the hood of a lot of multimedia applications, such as video conferencing, digital storage media, television broadcasting, and internet streaming. Recently, new standards-based and proprietary technologies have emerged. An interesting problem is how to evaluate these different video coding solutions in terms of delivered quality. In this paper, a PSNR-based approach is applied in order to compare the coding potential of H.264/AVC AHM 2.0 with the compression efficiency of XviD 0.9.1, DivX 5.05, Windows Media Video 9, and MC-EZBC. Our results show that MPEG-4-based tools, and in particular H.264/AVC, can keep step with proprietary solutions. The rate-distortion performance of MC-EZBC, a wavelet-based video codec, looks very promising too.	codec;data compression;distortion;h.264/mpeg-4 avc;hood method;microsoft windows;outline of television broadcasting;peak signal-to-noise ratio;streaming media;wavelet;windows media	Wesley De Neve;Peter Lambert;Sam Lerouge;Rik Van de Walle	2004		10.1117/12.526346	video compression picture types;scalable video coding;wavelet;the internet;video;h.263;distortion;peak signal-to-noise ratio;telecommunications;computer science;computer data storage;multimedia;video processing;videoconferencing;global motion compensation;mpeg-4;compression;h.261;multiview video coding;computer graphics (images)	HPC	43.34346022070078	-19.835495072825797	146992
2a3ea92d341655d5213a6566732a0361ddbbad92	demo: uhd live video streaming with a real-time scalable hevc encoder	software;high efficiency video coding;streaming media;high definition video;encoding;real time systems	In this paper we present a real-time streaming demonstration with a multi-layer architecture of a pipelined software High Efficiency Video Coding (HEVC) encoders with inter-layer prediction enabling Scalable HEVC (SHVC) encodings. This SHVC encoder is implemented on an innovative platform performing real-time encodings that already demonstrated promising performance with HDR, HFR and SHVC implementation in previous demonstrations [1], [2]. The transmitted content consists of a spatial SHVC bitstream composed of a High Definition (HD) Base Layer (BL) and an Ultra HD (UHD) Enhancement Layer (EL). The encoder reads an UHD video sequences through Serial Digital Interface (SDI) ports and broadcasts the SHVC bitstream through an Internet Protocol (IP) channel. The bitstream is then decoded using a GPAC player with a real-time decoder.	bl (logic);bitstream;edge enhancement;encoder;gpac;high efficiency video coding;layer (electronics);real-time clock;scalability;serial digital interface;streaming media	Ronan Parois;Wassim Hamidouche;Elie Gabriel Mora;Mickaël Raulet;Olivier Déforges	2016	2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)	10.1109/DASIP.2016.7853830	embedded system;real-time computing;computer science;multimedia	EDA	43.51480399816291	-21.488710799143888	147240
37be370b28f98ee75f9cb0fe3c17d18194d99ef0	efficient compression of 4d-trajectory data in air traffic management	statistical analysis aerospace computing air traffic data compression data handling random processes;air traffic control;standards;data compression;four dimensional 4d trajectories air traffic management atm compression;indexes;trajectory;trajectory standards aircraft encoding indexes europe;traffic data;europe;methodology;atm traffic data handling 4d trajectory data compression air traffic management flight data random walker subtrajectory pointers reference trajectory demand data repository eurocontrol referential compression techniques statistical compression techniques europe;encoding;aircraft	Air traffic management (ATM) is facing a tremendous increase in the amount of available flight data, particularly four-dimensional (4D) trajectories. Computational requirements for analysis and storage of such bulk of data are steeply increasing. Compression is one key technology to address this challenge. In this paper we propose two techniques for compressing air traffic 4D trajectories. Our first technique analyzes a set of samples and computes a prediction for the most likely picked successor coordinate by a random walker. The second technique, i.e., referential compression, compresses a 4D trajectory as a collection of subtrajectory pointers into a reference trajectory. We evaluate our algorithms on trajectory data from the Demand Data Repository provided by EUROCONTROL. We show that a combination of our referential and statistical compression techniques compresses 4D trajectories of all air traffic over Europe in the year 2013 from 60 GB down to 0.78 GB, achieving a compression ratio of more than 75 : 1. The compression ratio for our techniques increases with the number of to-be-compressed flights, whereas standard compression techniques achieve a fixed compressed ratio for any number of flights. Our work contributes toward efficient handling of the increasing amount of traffic data in ATM.	atm turbo;adobe air;amiga walker;computation;data compression;random walker algorithm;requirement	Sebastian Wandelt;Xiaoqian Sun	2015	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2014.2345055	data compression;database index;real-time computing;simulation;telecommunications;computer science;trajectory;air traffic control;methodology;encoding;statistics	Visualization	47.2242570065365	-20.86467417252883	147593
33a01cdf491d3ab6a62c733c53ae02f0096b05ae	lossless and near-lossless image compression scheme utilizing blending-prediction-based approach	compression algorithm;network on chip;lossless image compression;lossless compression;blending predictors;multi path routing;near lossless compression;heuristic algorithm	An approach for lossless and near-lossless compression of still images together with its system-level multi-core hardware model utilizing blending-prediction-based technique is presented in this paper. We provide a mathematical background of the proposed approach and utilize a Network on Chip type of connection in the hardware model which benefits from a new multi-path routing algorithm and heuristic algorithms for core mapping realizing subsequent stages of the compression algorithm. The experimental results confirming the advantages of the proposed approach are provided.	alpha compositing;image compression;lossless compression	Grzegorz Ulacha;Piotr Dziurzanski	2008		10.1007/978-3-642-02345-3_21	data compression;lossy compression;lempel–ziv–stac;lossless jpeg;data compression ratio;mathematical optimization;image compression;computer science;theoretical computer science;mathematics;lossless compression;network on a chip;adaptive coding;context-adaptive binary arithmetic coding;texture compression;statistics	Vision	42.86096563342167	-17.977889173216372	147978
55c5c1c551c9aaa407b73474a65dd3eb3c788212	the value of relative quality in video delivery	electronic imaging video circuits and systems video and tv cameras;image processing pattern recognition tomography;video quality;electronic imaging image quality;quality of experience;maximum likelihood difference scaling;mlds;image reconstruction;qoe	• A submitted manuscript is the author's version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. • The final author version and the galley proof are versions of the publication after peer review. • The final published version features the final layout of the paper including the volume, issue and page numbers.		Vlado Menkovski;Georgios Exarchakos;Antonio Liotta	2011	J. Mobile Multimedia		iterative reconstruction;image quality;subjective video quality;computer vision;simulation;computer science;video quality;machine learning;multimedia	Crypto	41.9791170338111	-23.267024734433342	148034
f5d3535d860d09fb04c7217f4274272aae00f696	dependent random access point pictures in hevc	streaming media switches decoding bit rate servers video coding encoding;tv dependent random access point pictures hevc high efficiency video coding standard inter coded picture intra random access point picture supplementary enhancement indication adaptive video streaming video conferencing channel switching;video streaming broadcasting telecommunication standards teleconferencing video coding video communication;drap hevc video coding random access mpeg dash fast channel switch	This paper describes the concept and a number of system aspects for the Dependent Random Access Point (DRAP) picture, a new feature that was recently introduced in the High-Efficiency Video Coding (HEVC) standard. A DRAP picture is an inter-coded picture that may only reference the previous Intra Random Access Point (IRAP) picture and provides a random access point in the bitstream at the DRAP picture given that the IRAP picture is available. The DRAP picture is indicated in the HEVC bitstream by a supplementary enhancement indication (SEI) message. DRAP pictures could increase coding efficiency for random access configured bitstreams for video services like adaptive video streaming, video conferencing, and may also provide faster channel switching for broadcasted TV for a certain bitrate.	algorithmic efficiency;bitstream;high efficiency video coding;random access;random-access memory;software engineering institute;streaming media;wireless access point	Martin Pettersson;Rickard Sjöberg;Jonatan Samuelsson	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7350923	video compression picture types;scalable video coding;video;h.263;telecommunications;computer science;video capture;coding tree unit;multimedia;video processing;smacker video;motion compensation;h.261;computer network;multiview video coding	Vision	45.05832522068457	-20.477295033900315	148066
59acace70e2f48da74b47e9a3e1d320dbaa30a1b	i/p frame selection using classification based mode decision	computation complexity i p frame selection classification based mode decision video coding bit rate decoded video quality;optimisation;image classification;video coding;computational complexity;video coding bit rate encoding cost function decoding probability density function error probability testing motion analysis layout;decision theory;mode decision;image classification video coding decision theory optimisation computational complexity image sequences;image sequences	There are many mode decisions in the video coding process that are used to optimize the performance in terms of the bit rate, the speed and the quality of the decoded video. We describe a classification based scheme for making mode decisions in the video coding process. We then illustrate the performance of the scheme using the I/P frame selection as an example. The performance of our scheme is measured in terms of both the bit rate as well as the computation complexity, across different kinds of sequences, and the results are very encouraging.	computation;data compression;video compression picture types	Deepak S. Turaga;Tsuhan Chen	2001		10.1109/ICIP.2001.958173	computer vision;contextual image classification;decision theory;harmonic vector excitation coding;computer science;theoretical computer science;machine learning;coding tree unit;mathematics;context-adaptive binary arithmetic coding;computational complexity theory;motion compensation;h.261;statistics	Metrics	47.33702241368444	-17.64000813884303	148152
17884401947aae8b23824498fb08066f1f50e864	2:1 candidate position subsampling technique for fast optimal motion estimation	image sampling;optimal motion vector;psnr performance;metodo caso peor;traitement signal;evaluation performance;candidate position subsampling technique;degradation;sampling rate;estimation mouvement;full search;performance evaluation;psnr;image processing;complexite calcul;motion estimation psnr iec standards iso standards computational complexity avalanche photodiodes sampling methods degradation video coding computer science;image matching;iso standards;evaluacion prestacion;degradacion;estimacion movimiento;optimal estimation;procesamiento imagen;motion estimation;sous echantillonnage;sistema n niveles;endommagement;2 1 cpst;traitement image;deterioracion;razon muestreo;candidate position;avalanche photodiodes;fast full search;video coding;fast motion estimation;complejidad computacion;iec standards;psnr degradation fast optimal motion estimation candidate position subsampling technique search window sampling rate 2 1 cpst optimal motion vector fine search fast optimal block matching algorithm multilevel successive elimination algorithm msea algorithm peak signal to noise ratio psnr performance;fast optimal motion estimation;systeme n niveaux;search problems image matching image sampling motion estimation;computational complexity;multilevel successive elimination algorithm;motion vector;signal processing;peak signal to noise ratio;submuestreo;multilevel system;taux echantillonnage;fast optimal block matching algorithm;methode cas pire;correspondencia bloque;successive elimination block matching candidate position fast full search motion estimation subsampling;block matching;rapport signal bruit;search problems;relacion senal ruido;computer science;msea algorithm;estimation optimale;correspondance bloc;damaging;sampling methods;signal to noise ratio;algoritmo optimo;successive elimination;algorithme optimal;optimal algorithm;procesamiento senal;worst case method;search window;subsampling;psnr degradation;fine search;block matching algorithm;estimacion optima	The candidate position subsampling technique (CPST) basically chooses candidates in a search window at a sampling rate. The 2:1 CPST chooses half the candidates, and then selects one or more candidates that are considered as to be close to the optimal motion vector before conducting a fine search. The fine search is conducted by checking four neighbors of the chosen candidate(s) referred to as winner(s). The CPST can be combined with a fast optimal block-matching algorithm, such as the multilevel successive elimination algorithm (MSEA), in order to reduce the number of computations used in rejecting the nonbest candidate. We propose a new 2:1 CPST fitted to the MSEA. The proposed algorithm adopts a new condition for the winner which helps to find the best candidate efficiently. Moreover, a fast motion estimation step is used to reduce the number of computations of the MSEA, and the peak signal-to-noise ratio (PSNR) compensation step is adopted to guarantee that the PSNR performance of the proposed algorithm is very close to that of the full search. Experimental results show that the proposed algorithm reduces the computational loads of the MSEA by 47.26% on average with only -0.027 dB PSNR degradation in the worst case.	best, worst and average case;block-matching algorithm;british undergraduate degree classification;chroma subsampling;computation;credit bureau;elegant degradation;metabolite set enrichment analysis;motion compensation;motion estimation;peak signal-to-noise ratio;sampling (signal processing);simulation;speedup;window function	Hwal-Suk Lee;Jik-Han Jung;Dong-Jo Park	2010	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2010.2051365	computer vision;mathematical optimization;simulation;peak signal-to-noise ratio;image processing;computer science;signal processing;mathematics;algorithm	EDA	49.124385869920125	-19.061178468506448	148268
81eee3e97eca40f1675ad65a43a7bb08230d19d3	connectivity-guided predictive compression of dynamic 3d meshes	spatiotemporal phenomena data compression decoding image coding prediction theory;angle preserving predictor connectivity guided predictive compression dynamic 3d meshes real time compression vertex locations frame to frame compression decoding predictive coding paradigm local spatio temporal coherence;prediction method;dynamic 3d mesh coding;image coding;mesh compression;spatial dependence;non linear predictive coding;animation compression;data compression;decoding;efficient algorithm;real time;non linear predictive coding mesh compression dynamic 3d mesh coding animation compression prediction methods linear predictive coding;indexing terms;coherence predictive coding decoding animation principal component analysis wavelet transforms compression algorithms quantization prediction methods linear predictive coding;linear prediction coding;linear predictive coding;prediction methods;prediction theory;spatiotemporal phenomena;predictive coding	We introduce an efficient algorithm for real-time compression of temporally consistent dynamic 3D meshes. The algorithm uses mesh connectivity to determine the order of compression of vertex locations within a frame. Compression is performed in a frame to frame fashion using only the last decoded frame and the partly decoded current frame for prediction. Following the predictive coding paradigm, local temporal and local spatial dependencies between vertex locations are exploited. In this framework we present a novel angle preserving predictor and evaluate its performance against other state of the art predictors. It is shown that the proposed algorithm improves up to 25% upon the current state of the art for compression of temporally consistent dynamic 3D meshes.	3d computer graphics;3d printing;algorithm;kerrison predictor;polygon mesh;programming paradigm;real-time clock	Nikolce Stefanoski;Jörn Ostermann	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312961	data compression;inter frame;residual frame;computer vision;linear predictive coding;index term;spatial dependence;computer science;theoretical computer science;machine learning;statistics	Robotics	42.671886531372785	-18.31350751184287	148314
0c2dd43b051402ce6d483655e7d8b2e796998f57	extending svc by content-adaptive spatial scalability	image motion analysis;image coding;scalable video coding;data compression;h 264 avc;finite impulse response filter;non linear image warping h 264 avc scalable video coding spatial scalability content adaptation;static var compensators bit rate image coding video coding scalability streaming media finite impulse response filter;bit rate;image texture;video coding;finite impulse response;prediction theory;streaming media;image sequence;single compressed bit stream svc scalable video coding extension content adaptive spatial scalability cass h 264 avc high quality bit stream image sequence display aspect ratios nonlinear dependencies content adaptive retargeting interlayer prediction tools content adaptive interlayer texture content adaptive interlayer motion residual prediction video content transmission;content adaptation;static var compensators;scalability;non linear image warping;video coding data compression image motion analysis image sequences image texture prediction theory;static var compensator;image warping;aspect ratio;spatial scalability;image sequences	This paper provides details on a complete integration of Content-adaptive Spatial Scalability (CASS) into the scalable video coding extension of H.264/AVC (SVC). CASS enables the efficient encoding of a high-quality bit stream that contains several versions of an original image sequence. Thereby, each such image sequence has been created by content-adaptive and art directed retargeting to different display aspect-ratios and/or resolutions. Non-linear dependencies between spatial layers, which have been introduced through content-adaptive retargeting, are exploited by a generalization of the three inter-layer prediction tools of SVC, i.e. by content-adaptive inter-layer texture, motion and residual prediction. The CASS extended SVC enables the transmission of video content which has been specifically adapted in an art-directed way to multiple display configurations (e.g. to SD and HD displays with 4:3 and 16:9 aspect-ratios, respectively) using a single compressed bit stream. With our extension, video content of higher semantic quality can be transmitted in a scalable way by introducing an average overhead in bit rate of 9.3%.	bitstream;data compression;digital video;h.264/mpeg-4 avc;overhead (computing);retargeting;scalability;scalable video coding	Yongzhe Wang;Nikolce Stefanoski;Manuel Lang;Alexander Sorkine-Hornung;Aljoscha Smolic;Markus H. Gross	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116466	computer vision;real-time computing;computer science;finite impulse response;multimedia	Visualization	44.56714379887922	-18.590435325709468	148356
b9edbfb1cbd0f7ece082211efd99464d7f26e95c	low complexity finite-state scalar quantization of image subbands	prediction error;image coding;image resolution;low frequency;very low bit rate image coding low complexity finite state scalar quantization image subbands subband image coding image compression very low bit rates multiresolution decomposition frequency bands human visual system energy content coding gain image quality entropy coding fssq predictive fssq pfssq scalar quantization interpixel correlation contextual model uniform quantizers experiments;entropy coding;low complexity;spectrum;transform coding;correlation methods;coding gain;quantisation signal;entropy codes image coding image resolution quantisation signal correlation methods;scalar quantization;image compression;human visual system;entropy codes;image quality;quantization image coding frequency bit rate transform coding energy resolution image resolution humans visual system image quality	This paper presents an image coding technique based on the combination of subband decomposition, entropy coding and three different forms of quantization: scalar quantization, finite-state scalar quantization (FSSQ) and predictive FSSQ (PFSSQ). In addition to being fast and requiring no training for quantizer design, the technique has been shown to be competitive with other existing methods. In one of our experiments for example, we encoded the luminance component of the standard image Lena at 0.123 bits per pixel with a PSNR of 30.62 dB.	color depth;entropy encoding;experiment;lenna;peak signal-to-noise ratio;pixel;quantization (signal processing)	Xavier Ginesta;Sharad Sambhwani	1995	Conference Record of The Twenty-Ninth Asilomar Conference on Signals, Systems and Computers	10.1109/DCC.1996.488371	image quality;spectrum;computer vision;transform coding;image resolution;image compression;computer science;entropy encoding;theoretical computer science;mean squared prediction error;coding gain;mathematics;low frequency;human visual system model;statistics	Vision	44.16227530212322	-15.547000998721604	148425
1f5f9ff41f181706cc7296ac12d1b6fc26c5b57b	enhanced jpeg2000 quality scalability through block-wise layer truncation	signal image and speech processing;quantum information technology spintronics	Quality scalability is an important feature of image and video coding systems. In JPEG2000, quality scalability is achieved through the use of quality layers that are formed in the encoder through rate-distortion optimization techniques. Quality layers provide optimal rate-distortion representations of the image when the codestream is transmitted and/or decoded at layer boundaries. Nonetheless, applications such as interactive image transmission, video streaming, or transcoding demand layer fragmentation. The common approach to truncate layers is to keep the initial prefix of the to-be-truncated layer, which may greatly penalize the quality of decoded images, especially when the layer allocation is inadequate. So far, only one method has been proposed in the literature providing enhanced quality scalability for compressed JPEG2000 imagery. However, that method provides quality scalability at the expense of high computational costs, which prevents its application to the aforementioned applications. This paper introduces a Block-Wise Layer Truncation (BWLT) that, requiring negligible computational costs, enhances the quality scalability of compressed JPEG2000 images. The main insight behind BWLT is to dismantle and reassemble the to-be-fragmented layer by selecting the most relevant codestream segments of codeblocks within that layer. The selection process is conceived from a rate-distortion model that finely estimates rate-distortion contributions of codeblocks. Experimental results suggest that BWLT achieves near-optimal performance even when the codestream contains a single quality layer.	bitstream;code::blocks;computation;data compression;distortion;encoder;fragmentation (computing);jpeg 2000;mathematical optimization;network packet;rate–distortion optimization;real-time clock;scalability;streaming media;truncation	Francesc Aulí Llinàs;Joan Serra-Sagristà;Joan Bartrina-Rapesta	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/803542	real-time computing;simulation;telecommunications;computer science;theoretical computer science	Vision	47.79059038297506	-16.27527529374203	148481
6a8675cc2b42e23052f877e54eba1a18027bae34	linear pre/post filters for transform and subband/wavelet coding	receiver;psd;point distribution function linear post filters linear pre filters transform coding wavelet coding linear gain plus additive noise model pdf optimized quantizers psd reconstruction error mean square error frequency domain behavior 1 d gauss markov source dct based coders subband coders a posteriori method jpeg coder transmitter receiver power spectral density;image coding;jpeg coder;nonlinear filters wavelet transforms design methodology power system modeling image reconstruction transform coding mean square error methods frequency domain analysis gaussian processes matched filters;least mean squares methods;linear gain plus additive noise model;markov source;gaussian processes;frequency domain analysis;additive noise;subband coders;linear pre filters;pdf optimized quantizers;transform coding;point distribution function;wavelet transforms;dct based coders;power spectral density;distribution function;wavelet coding;transmitter;discrete cosine transforms;mean square error;image reconstruction;digital filters;1 d gauss markov source;reconstruction error;digital filters transform coding image coding wavelet transforms noise image reconstruction least mean squares methods frequency domain analysis gaussian processes markov processes discrete cosine transforms;markov processes;frequency domain behavior;a posteriori method;frequency domain;linear post filters;noise	We propose a novel method for designing linear pre/post filters for transform and subband/wavelet coders. We use a linear gain-plus-additive noise model to describe the effects of a class of point distribution function-optimized quantizers. Using this quantizer model, we provide an explicit expression for the power spectral density of the reconstruction error in an improved transform coding system that includes linear pre/post filters. This expression allows us to design pre/post filters that minimize the overall mean square error at a given target rate, by improving the frequency domain behavior of the overall system. For a 1-D Gauss-Markov source, we show that pre/post filters help DCT-based coders to match the performance of subband coders. We also describe an a posteriori method of designing postfilters for a JPEG coder, that uses the actual image data and maximizes the PSNR performance of the system. The postfilter coefficients are designed at the transmitter, and are sent to the receiver as part of the coded data.	wavelet transform	Anil M. Murching;John W. Woods	1997		10.1109/ICIP.1997.647377	mathematical optimization;speech recognition;mathematics;frequency domain;statistics	Vision	48.22617794994293	-11.938157046667524	148781
7fd33252342ac61787fac7f11c97d3939e2f8893	frd-fpred: a novel re-decode based error compensation method using fast re-decoding and fast prediction algorithm	feedback channel;degradation;image coding;decoding;h 263 video codec;transmission error;frd fpred;video compression;re decode based error compensation method;error compensation decoding video compression degradation feedback video coding image coding mobile communication transmitters discrete cosine transforms;spatio temporal error propagation;visual quality;transmission errors;video codec;video coding;feedback;computational complexity frd fpred re decode based error compensation method fast re decoding fast prediction algorithm transmission errors low bit rate mobile communications h 263 video codec spatio temporal error propagation feedback channel;error propagation;decoding error compensation mobile communication computational complexity video codecs video coding;computational complexity;discrete cosine transforms;transmitters;error compensation;mobile communication;fast re decoding;reference picture selection;video codecs;low bit rate mobile communications;fast prediction algorithm	This study aims at finding an efficient compensation method of transmission errors in low bitrate mobile communications using the H.263 video codec. For this aim, we suggest a novel error compensation method in the encoder (or transmitter), that can remove visual quality degradation due to spatio-temporal error propagation by utilizing a feedback channel. Most of all, our proposed method can reduce the computational complexity of the re-decoding method and FRD(Fast Re-Decoding) method by using the characteristic for not error-propagated macroblocks. Experimental results show that the proposed error compensation method can give higher coding efficiency compared with the other two non re-decoding based existing ones: the reference picture selection(RPS) method and the error tracking(ET) method. Finally, our proposed method requires only 30% in the amount of operations compare to fast redecoding(FRD) procedure.	algorithm;algorithmic efficiency;codec;computation;computational complexity theory;elegant degradation;encoder;fast fourier transform;macroblock;propagation of uncertainty;software propagation;transmitter	Hyunki Baik;Myong-Soon Park	2001		10.1109/ICPADS.2001.934878	data compression;transmitter;degradation;mobile telephony;telecommunications;computer science;propagation of uncertainty;theoretical computer science;feedback;computational complexity theory;statistics	Robotics	46.79263991802811	-17.7492556151949	148825
0c29a67359b2b657c3a80f80043a4286ac8ab1ff	encoding of line drawings with multiple hexagonal grid chain code	chain code;line drawings	Abstract#R##N##R##N#This paper discusses the encoding of line drawings using a hexagonal quantization grid. When an efficient encoding of line drawings is required in general, one must seek the encoding method which minimizes the code assigned to the unit length of the line-drawing, under the constraint for the quantization error. The multiple square grid chain code, which was already proposed, is one such encoding method. This paper describes the result of a study on the multiple hexagonal quantization grid, which is more efficient than the former method. In those methods, the quantization grid is not fixed. The quantization points are arranged on the edge of the grid, and the quantization is made more efficient by adjusting the point of quantization grid connections according to the previous situation of quantization. The rate-distortion value of the multiple hexagonal grid chain code is shown to be approximately 0.7 times that of the multiple square grid chain code and approximately 0.45 times that of Freeman's chain code. The reason for the differences are discussed in some detail, and the actual algorithm and the processing time are described, considering the real-time property of the code.	chain code;line drawing algorithm	Katsuyuki Shinohara;Toshi Minami	1986	Systems and Computers in Japan	10.1002/scj.4690171201	computer vision;combinatorics;constant-weight code;computer science;alpha-numeric grid;theoretical computer science;chain code;algorithm	HPC	43.17162373511681	-13.520951131271069	149063
95389d40847dcf22c65a8aa236bbf7be83e484b0	analysis of a hybrid fractal-predictive-coding compression scheme	transformation affine;image processing;fractal bit allocation;time complexity;modelo hibrido;procesamiento imagen;modele hybride;traitement image;hybrid model;asignacion bit;allocation bit;linear prediction coding;linear predictive coding;compression image;image compression;affine transformation;hybrid system;fractal;bit allocation;predictive coding;transformacion afin;codage predictif lineaire;compresion imagen;fractal image compression	There has been tremendous progress in fractal compression since the pioneer work of Barnsley and Jacquin in the late 1980s. As the encoding time complexity issues are gradually being solved, there is a steady growth of applications of fractals, especially in hybrid systems. However, such fractal hybrid systems tend to be rather difficult to analyze, and part of that difficulty lies in the quantization of the scaling and luminance offset parameters adopted in most fractal compression schemes. In this paper, we present theoretical and empirical justification for a well-known but underused alternative parametrization for the fractal affine transform. In particular, we shall present a detailed analysis of a hybrid fractal-LPC (linear predictive coding) compression scheme using the aforementioned alternative affine transform parameters. r 2003 Elsevier Science B.V. All rights reserved.	code;data compression;fractal compression;hybrid system;image scaling;kerrison predictor;linear predictive coding;peak signal-to-noise ratio;quantization (signal processing);time complexity	Chong Sze Tong;Minghong Pi	2003	Sig. Proc.: Image Comm.	10.1016/S0923-5965(03)00037-7	time complexity;combinatorics;discrete mathematics;linear predictive coding;fractal;image processing;image compression;computer science;theoretical computer science;affine transformation;mathematics;fractal compression;algorithm;hybrid system	AI	45.162584093246195	-12.645385799102225	149148
1982752316c3815446dc768b35851851f38390cb	polynomial weighted median predictors for image sequences	image coding;data compression;nonlinear prediction;differential pulse code modulation;higher order statistics;higher order statistics polynomial weighted median predictor image compression transmission scheme non gaussian image sequence volterra predictor smoothing effects image acquisition device pwm;smoothing methods;image compression;image acquisition;predictive coding image sequences nonlinear prediction polynomial weighted median;polynomial weighted median;image sequence;smoothing methods data compression higher order statistics image coding image sequences;predictive coding;polynomials image sequences pulse width modulation pulse modulation predictive coding image coding pulse compression methods modulation coding redundancy smoothing methods;quantization error;image sequences	Image sequence prediction is widely used in image compression and transmission schemes such as differential pulse code modulation (DPCM). In traditional predictive coding, linear predictors are usually adopted to exploit the inherent redundancy and correlation between neighboring pixels. However, due to the nonstationary and non-Gaussian nature of image sequences, linear predictors are not often very effective. As an alternative, Volterra predictor is able to compensate for the smoothing effects introduced by linear predictor. However, it suffers from noise that may be attributed to quantization errors or image acquisition devices. In this paper, we propose a novel nonlinear polynomial weighted median (PWM) predictor for image sequence prediction. The proposed PWM predictor is more robust to noise, while still retaining the information of higher-order statistics of pixel values. Experimental results illustrate that the PWM predictor yields better results than other predictors especially in noisy case. The proposed scheme can be incorporated in new predictive coding systems.	emoticon;image compression;integrated circuit layout design protection;kerrison predictor;lotka–volterra equations;nonlinear system;pixel;polynomial;predictor–corrector method;pulse-width modulation;quantization (signal processing);smoothing;video compression picture types	Binwei Weng;Tuncer C. Aysal;Kenneth E. Barner	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312938	data compression;mathematical optimization;quantization;image compression;computer science;pattern recognition;mathematics;algorithm	Vision	44.25008947593542	-16.31600362899498	149860
9a28e695eb62e964c9b6a6a9968384b343d20a06	a fast coding unit depth decision algorithm for hevc inter prediction	video coding computational complexity quadtrees;rd cost;coding unit;hevc;encoding video coding algorithm design and analysis rate distortion prediction algorithms complexity theory standards;depth decision;computational complexity fast coding unit depth decision algorithm hevc inter prediction high efficient video coding quad tree structure;inter prediction;rd cost hevc inter prediction depth decision coding unit	High Efficient Video Coding (HEVC) adopts a quad-tree structure to partition the Coding Unit (CU). Each CU can be split into smaller CUs recursively. It achieves high coding efficiency. But it also dramatically increases the computational complexity. This paper proposes a fast coding unit depth decision algorithm for HEVC inter prediction which can provide better coding efficiency. The proposed algorithm introduces early termination methods based on their motion characteristics. The certain threshold is used to decide whether the coding unit (CU) should be split into smaller CUs (or PUs). By this way, many partition operations and rate distortion (RD) cost computations will be skipped to reduce the complexity. Experimental results show that the proposed technique saves 45.76% encoding time on average with negligible loss of coding efficiency.	algorithm;algorithmic efficiency;computation;computational complexity theory;distortion;high efficiency video coding;rate–distortion theory;recursion;ruby document format;tip (unix utility);tree structure	Nana Shan;Wei Zhou;Zhemin Duan;Henglu Wei	2015	2015 Ninth International Conference on Frontier of Computer Science and Technology	10.1109/FCST.2015.32	real-time computing;simulation;computer science;theoretical computer science;coding tree unit;context-adaptive binary arithmetic coding	Robotics	46.91294170571508	-19.366262991871746	149888
af1aeb5940c91c722d28d06e8f943a466448cbda	a new algorithm for n-dimensional hilbert scanning	software;graph theory;look up table;computer graphics and computer aided design;image processing;data compression;hilbert spaces;electrical and electronic engineering;theoretical computer science;procesamiento imagen;implementation n dimensional hilbert scanning hilbert curve one to one mapping n dimensional space one dimensional space point neighborhoods butz algorithm quinqueton algorithm mapping function addresses bit operations recursive functions nonrecursive algorithm look up tables computation;multidimensional analysis;indexing terms;traitement image;hilbert scanning;analyse n dimensionnelle;image compression;computational theory and mathematics;computational complexity;analisis n dimensional;balayage hilbert;compresion dato;hilbert space hardware table lookup application software image processing image coding multidimensional systems two dimensional displays data analysis intelligent systems;computer vision and pattern recognition;table lookup;compression donnee;hypercube networks;hilbert curve;hypercube networks hilbert spaces table lookup computational complexity graph theory;courbe peano	There have been many applications of the Hilbert curve, such as image processing, image compression, computer hologram, etc. The Hilbert curve is a one-to-one mapping between N-dimensional space and one-dimensional (l-D) space which preserves point neighborhoods as much as possible. There are several algorithms for N-dimensional Hilbert scanning, such as the Butz algorithm and the Quinqueton algorithm. The Butz algorithm is a mapping function using several bit operations such as shifting, exclusive OR, etc. On the other hand, the Quinqueton algorithm computes all addresses of this curve using recursive functions, but takes time to compute a one to-one mapping correspondence. Both algorithms are complex to compute and both are difficult to implement in hardware. In this paper, we propose a new, simple, nonrecursive algorithm for N-dimensional Hilbert scanning using look-up tables. The merit of our algorithm is that the computation is fast and the implementation is much easier than previous ones.		Sei-ichiro Kamata;Richard O. Eason;Yukihiro Bandoh	1999	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.772242	data compression;multidimensional analysis;discrete mathematics;index term;ramer–douglas–peucker algorithm;lookup table;hilbert r-tree;image processing;image compression;computer science;graph theory;theoretical computer science;machine learning;mathematics;computational complexity theory;algorithm;hilbert space	Visualization	39.42613333695749	-18.583169596613768	149906
7e35cb78683621069433c2741a62bb27b82b72f7	efficient pixel-based motion vector recovery in h.264	neighboring mv efficient pixel based motion vector recovery h 264 avc error concealment algorithm neighboring received macroblocks corrupted macroblocks;image motion analysis;data compression;video coding;video coding data compression image motion analysis;video coding vectors signal processing algorithms transform coding psnr artificial intelligence;motion vector recovery error concealment h 264	This paper presents a novel error concealment algorithm for H.264/AVC, which is constructed according to the spatial relationship of motion vectors between corrupted macroblocks and neighboring received macroblocks. We model the motion field such that the lost pixel is a function of all MVs of the neighboring MVs. We compare our method against a state-of-the-art method, and we improve the error concealment performance by up to 1.43 dB.	algorithm;error concealment;h.264/mpeg-4 avc;macroblock;motion field;pixel	Ting-Lan Lin;Chun-Kai Lai;Neng-Chieh Yang	2012	2012 International Symposium on Intelligent Signal Processing and Communications Systems	10.1109/ISPACS.2012.6473510	computer vision;speech recognition;quarter-pixel motion;motion estimation;block-matching algorithm;computer graphics (images)	Robotics	46.02887957328702	-17.183463675162393	150000
b470c219055691bac5e7ba98938652aef8f92321	quality evaluation method considering time transition of coded video quality	genetic algorithms video coding;human memory;video quality;video coding;humans video sequences video coding mean square error methods visual system genetics visual perception logistics equations decoding;quality evaluation;genetic algorithm quality evaluation method time transition coded video quality frame quality optimal weighted function;genetic algorithm;genetic algorithms;weight function	1. I N T R O D U C T I O N Objective mcasures of coded video quality are important in video coding. The mean square error is widely used as an objective measure. Though it's convenient to evaluate the video sequence, the performance is insufficient. On the other hand, some objcctivc measures based on the human visual system give accurate estimation with limited applications [l, 2, 31. We have already proposed the quality estimation method of coded video using objective quality of each frame [4]. It is difficult to estimate the actual dynamic quality of codcd video. So, we suppose the series of the objective picture quality of each frame as thc actual video quality. The estimatcd video quality is provided from the multiple regression of the average and worst score of the quality of each frame. When thc time transition of video quality is stabilizing, the performance of the proposcd method is quite well. Howevcr, when thc time transition changes violcntly, the method doesn't work well. Thcrefore, we need to study the influence of the time transition of video quality with respect to the MOS (Mean Opinion Score). In this paper, we propose the quality cstimation method using weighted function considering the fcatore of human sensation. The influence of the time trausition is considered as thc weighted function, which 0-7803-5467-2/99/$10.00	data compression;human visual system model;image quality;mean squared error;video	Yasuhiro Inazumi;Yuukou Horita;Kazunori Kotani;Tadakuni Murai	1999		10.1109/ICIP.1999.819608	scalable video coding;computer vision;simulation;genetic algorithm;computer science;video quality;machine learning;block-matching algorithm;multimedia;rate–distortion optimization;motion compensation;h.261;multiview video coding	Vision	46.92537313548831	-21.475389553165012	150075
3d5c7b3cdf0c2a560fb61c20eda8503d1d5c86b9	improved global motion estimation based on iterative least-squares with adaptive variable block size	adaptive vari able block size;global motion estimation;iterative least squares;preprocessing;least square	In order to reduce the convergence time in an iterative procedure, some gradient based preliminary processes are employed to eliminate outliers. The adaptive variable block size is also introduced to balance the accuracy and computational complexity. Moreover, the use of Canberra distance instead of Euclidean distance illustrates higher performance in measuring motion similarity.	block size (cryptography);iterative method;least squares;motion estimation	Leiqi Zhu;Dongkai Yang;Qishan Zhang	2011	IEICE Transactions		mathematical optimization;combinatorics;computer science;mathematics;least squares;preprocessor;statistics	Vision	49.375530089955724	-19.496336772036187	150272
0bf072f4f121eda276035e3de9033c0275cfa981	optimal distortion estimation for prediction error expansion based reversible watermarking		Reversible Image Watermarking is a technique to losslessly embed and retrieve information (in the form of a watermark) in a cover image. Prediction Error Expansion based schemes are currently the most efficient and widely used reversible image watermarking techniques. Estimation of the minimum achievable (optimum) distortion for a given payload and a given cover image, is an important problem for reversible watermarking schemes. In this paper, we first show that the bounded capacity distortion minimization problem for prediction error expansion based reversible watermarking schemes is NP-hard, and that the corresponding decision version of the problem is NP-complete. We then propose a low computational overhead heuristic to estimate the minimal distortion. Our estimation technique first estimates (for a given image) the prediction error distribution function without explicit use of any prediction scheme, and then minimizes the distortion for a given payload by solving a convex optimization problem to estimate an embedding threshold parameter. Experimental results for common benchmark images closely match the predictions from our technique, and thus verify the consistency of our approach.	digital watermarking;distortion	Aniket Roy;Rajat Subhra Chakraborty	2016		10.1007/978-3-319-53465-7_20	pattern recognition;statistics	EDA	40.05667011009464	-16.107203465825187	150292
97070a65a322eb6e1b6bf1c4a7405960ab9d0958	adaptive vod architecture for heterogeneous networks based on scalable wavelet video coding	video on demand client server systems motion estimation video coding;client server systems;motion estimation;indexing terms;three dimensional;video on demand adaptive vod architecture heterogeneous networks scalable wavelet video coding client server system motion estimation wavelet domain hierarchical variable size block matching multiple layer motion scalability 3d wavelet coefficients jpeg2000 code streaming;video coding;video on demand;block matching;video coding scalability network servers spatial resolution motion estimation wavelet domain discrete wavelet transforms wavelet coefficients streaming media bandwidth;networked systems;heterogeneous network;spatial scalability;spatial resolution	In this paper, the VoD architecture for heterogeneous networks based on scalable wavelet video coding is proposed, with enhanced spatial scalability. The system consists of the clients and server (C/S). Motion estimation is performed on the resolution of the wavelet domain using hierarchical variable size block matching (DWT-HVSBM). All the vectors of different spatial resolutions are encoded into multiple-layer motion scalability. The sub-pixel accurate MCTF is applied in the temporal frames, and the three-dimensional wavelet coefficients are encoded in 3D EBCOT, which is the extension of JPEG2000 code streaming. The server can adapt to the end users 'requirements to provide the service over a heterogeneous network that has users with different bandwidths. The experiments show the performance of the codec is comparable to MCEZBC, and the video on demand network system can simultaneously support multiple heterogeneous users' demands.	codec;coefficient;data compression;discrete wavelet transform;experiment;jpeg 2000;motion estimation;pixel;requirement;scalability;server (computing)	Xuguang Lan;Nanning Zheng;Jianru Xue;Bin Gao;Xiaoguang Wu	2007	IEEE Transactions on Consumer Electronics	10.1109/TCE.2007.4429230	three-dimensional space;computer vision;real-time computing;heterogeneous network;index term;image resolution;computer science;theoretical computer science;motion estimation;block-matching algorithm;motion compensation;computer network	Visualization	44.115453252756566	-19.240136974250866	150306
19ab3f01c4c0698660893f2d3e50be253998741a	fast selective intra-mode search algorithm based on adaptive thresholding scheme for h.264/avc encoding	sequence type;software;theorie vitesse distorsion;metodo adaptativo;rate distortion;optimisation;inter frame;adaptive thresholding;intra mode;comparative analysis;algoritmo busqueda;threshold detection;optimizacion;logiciel;video signal processing;complexite calcul;algorithme recherche;adaptive thresholding scheme;h 264 avc;rate distortion optimization rdo h 264 avc inter frame intra mode intra skip;search algorithm;jm reference software;methode adaptative;variable block size;ippp sequence type;intra skip;rate distortion theory;detection seuil;video coding;complejidad computacion;deteccion umbral;codage video;computational complexity;adaptive method;neighborhood mac roblocks;h 264 avc encoding;traitement signal video;jm reference software fast selective intramode search algorithm adaptive thresholding scheme h 264 avc encoding rate distortion cost variable block size neighborhood mac roblocks ippp sequence type ibbpbbp sequence type;logicial;optimization;search problems;rate distortion cost;video coding rate distortion theory search problems;rate distortion optimization;ibbpbbp sequence type;rate distortion optimization rdo;fast selective intramode search algorithm;automatic voltage control encoding rate distortion costs video coding histograms high definition video research and development microwave integrated circuits search methods	A fast selective-intra mode search algorithm based on the rate-distortion (RD) cost for an inter-frame is proposed for H.264/AVC video encoding. In addition to the inter-mode search procedure with variable block size, an intra mode search causes a significant increase in the complexity and computational load for an inter-frame. To reduce the computational load of the intra mode search at the inter-frame, the RD costs of the neighborhood mac-roblocks (MBs) for the current MB are used and we propose an adaptive thresholding scheme for skipping intra mode search. For the IPPP sequence type, the overall encoding time can be reduced up to 40% and 42% for the IBBPBBP sequence type through comparative analysis of experimental results with JM reference software.	block size (cryptography);data compression;distortion;encoder;h.264/mpeg-4 avc;image quality;qualitative comparative analysis;rate–distortion theory;ruby document format;scheme;search algorithm;sensor;thresholding (image processing)	Byung-Gyu Kim	2008	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2007.913748	inter frame;qualitative comparative analysis;mathematical optimization;rate–distortion theory;computer science;theoretical computer science;mathematics;thresholding;rate–distortion optimization;computational complexity theory;algorithm;search algorithm	Vision	46.534847627501	-15.210272489187284	150344
66a8cea594be22c4d042ff372054558211c4d181	validation of cognitive models for collaborative hybrid systems with discrete human input	computational modeling;games;aerospace electronics;markov processes;data models;automation	We present a method to validate a cognitive model, based on the cognitive architecture ACT-R, in dynamic human-automation systems with discrete human input. We are inspired by the general problem of K-choice games as a proxy for many decision making applications in dynamical systems. We model the human as a Markovian controller based on gathered experimental data, that is, a non-deterministic control input with known likelihoods of control actions associated with certain configurations of the state-space. We use reachability analysis to predict the outcome of the resulting discrete-time stochastic hybrid system, in which the outcome is defined as a function of the system trajectory. We suggest that the resulting expected outcomes can be used to validate the cognitive model against actual human subject data. We apply our method to a two-choice game in which the human is tasked with maximizing net coverage of a robotic swarm that can operate under rendezvous or deployment dynamics. We validate the corresponding ACTR cognitive model generated with the data from eight human subjects. The novelty of this work is (1) a method to compute expected outcome in a hybrid dynamical system with a Markov chain model of the human's discrete choice, and (2) application of this method to validation of cognitive models with a database of actual human subject data.	act-r;cognitive architecture;cognitive model;discrete choice;dynamical system;hybrid system;markov chain;non-deterministic turing machine;reachability;robot;software deployment;state space;swarm robotics	Abraham P. Vinod;Yuqing Tang;Meeko M. K. Oishi;Katia P. Sycara;Christian Lebiere;Michael Lewis	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759514	cognitive model;games;data modeling;simulation;computer science;artificial intelligence;automation;machine learning;control theory;markov process;computational model;statistics	Robotics	51.99045317652874	-21.142184846769	150606
35f7ee0366d3d5340858fa1feddc1ba500584680	fast fragile watermark embedding and iterative mechanism with high self-restoration performance		Abstract This paper presents a new algorithm to reduce significantly the computational cost of one of the best methods with self-recovery capabilities in the fragile watermarking literature. This is achieved by generating two sequences of reference bits associated to the 5 most significant bit-planes (MSBPs) of the image. The reference bits and some authentication bits are then allocated to the 3 least significant bit-planes (LSBPs) of the image. The receiver uses the authentication bits to localise altered pixel-blocks and then executes an iterative restoration mechanism to calculate the original value of the watermarked pixels. Experimental results demonstrate that the embedding method executes significantly faster compared to the state-of-the-art method while achieving a high restoration performance.	algorithm;algorithmic efficiency;circuit restoration;closed-circuit television;computation;computer engineering;embedded system;iteration;iterative method;l (complexity);most significant bit;pixel;pseudorandomness;time complexity;tornado code	Sergio Bravo-Solorio;Félix Calderón;Chang-Tsun Li;Asoke K. Nandi	2018	Digital Signal Processing	10.1016/j.dsp.2017.11.005	digital watermarking;watermark;pixel;erasure;theoretical computer science;embedding;binary erasure channel;tornado code;authentication;computer science	Security	39.25509587636888	-12.463052822242565	150643
7aeb9317d532f566276b01ee0f76301773f1199d	linfty-constrained high-fidelity image compression via adaptive context modeling	quantization;image coding;psnr;data compression;adaptive codes image coding data compression prediction theory quantisation signal entropy codes minimax techniques rate distortion theory;biomedical imaging;l constrained image coder;transform coding;adaptive codes;calic;bit rate;context model;rate distortion theory;adaptive context modeling;quantisation signal;minimax techniques;high fidelity image compression;standards development;prediction theory;image compression;trellis quantisation;maximum error magnitude;trellis quantisation l sub spl infin constrained image coder high fidelity image compression adaptive context modeling tight bound maximum error magnitude prediction bias correction prediction residues quantisation calic nearly lossless image coders psnr predictive coding;image reconstruction;entropy codes;nearly lossless image coders;prediction bias correction;tight bound;context modeling;predictive coding;l sub spl infin constrained image coder;computer errors;image coding context modeling psnr bit rate quantization image reconstruction biomedical imaging computer errors transform coding standards development;prediction residues quantisation	We study high-fidelity image compression with a given tight bound on the maximum error magnitude. We propose some practical adaptive context modeling techniques to correct prediction biases caused by quantizing prediction residues, a problem common to the current DPCM like predictive nearly-lossless image coders. By incorporating the proposed techniques into the nearly-lossless version of CALIC, we were able to increase its PSNR by 1 dB or more and/or reduce its bit rate by ten per cent or more. More encouragingly, at bit rates around 1.25 bpp our method obtained competitive PSNR results against the best wavelet coders, while obtaining much smaller maximum error magnitude.	image compression	Xiaolin Wu;Wai Kin Choi;Paul Bao	1997		10.1109/DCC.1997.581978	medical imaging;speech recognition;computer science;theoretical computer science;mathematics;context model;algorithm;statistics	Vision	48.69112161568059	-13.21120401496141	150647
2e58f1eb5f1eefa4baeffa13fceb41c529d91e75	aes cryptography in color image steganography by genetic algorithms	encryption;color;matrices;image color analysis;ciphers;genetic algorithms	This work incorporates the AES cryptography algorithm, to improve the hidden data security in two methodologies for steganography: the genetic algorithm and path relinking. It also combines them proposing a new hybrid approach that outperforms the LSB (least significant bits) substitution technique presented in works cited in the literature concerning the quality of a stego image. It improves the possibility of hiding data inside color images significantly, increasing the space available for information by more than three times when compared to the usual steganography approach used by grayscale images. Moreover all types of digital information from text and compressed files to even executable programs can be hidden inside the cover image. This considerably increases the scope of application of the technique for transmitting information inside a typical image, hiding the data from intruders.	color image;cryptography;data security;digital data;executable;genetic algorithm;grayscale;least significant bit;steganography;transmitter	Aura Conci;Andre Luiz Brazil;Simone Bacellar Leal Ferreira;Trueman MacHenry	2015	2015 IEEE/ACS 12th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2015.7507100	computer vision;genetic algorithm;steganography tools;computer science;theoretical computer science;computer security;encryption;algorithm;matrix	Vision	39.5453527021725	-9.972575115161822	150680
3a65861e81f9df371134b4b9354728ec80746b3f	evaluation of quantization error in computer vision	computer vision;computerised picture processing;error statistics;probability;error distribution;error estimation;probability density;quantization error	computer vision;probability density;actual error;quantization error;important role;digitization error;large number;error distribution;careful analysis;mathematical tool;analytic expression;image processing;indexing terms;brightness;algorithm design and analysis;probability;layout;automation;quantization	computer vision;quantization (signal processing)	Behrooz Kamgar-Parsi;Behzad Kamgar-Parsi	1989	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.35496	computer vision;theoretical computer science	Vision	50.1497685287866	-12.377129030230941	150696
3f9c2a9ffecf3dc3d7c80d7683afe21402d6fd5d	rate-distortion-complexity adaptive video compression and streaming	video compression;distortion;computational complexity;transform coding;data compression;motion compensation;decoding;wavelet transforms;rate distortion theory	In this paper, we discuss the benefits of complexity-driven streaming and define a realistic rate-complexity-distortion framework that can assist multimedia streaming systems. We show an illustrative example for the modeling of the decoding complexity of state-of-the-art motion-compensated wavelet coding schemes. We also present a concrete system that uses these complexity metrics, which can help us quantify the performance that can be gained for multimedia streaming applications by considering complexity constraints and architectural features. Finally, we show results obtained using the proposed complexity-driven streaming architecture.	data compression;distortion;server (computing);sockets direct protocol;streaming media;wavelet transform	Mihaela van der Schaar;Deepak S. Turaga;Venkatesh Akella	2004	2004 International Conference on Image Processing, 2004. ICIP '04.		data compression;computer vision;computer science;theoretical computer science;mathematics;multimedia;algorithm;statistics	HPC	47.85040036982388	-16.24987706230867	150782
8492c5d6fb9bffa60202fb72603806a07a2a0b0d	countermeasures for collusion attacks exploiting host signal redundancy	filigranage numerique;digital watermarking;herencia;analisis contenido;song;steganographie;multimedia;chant;contre mesure electronique;securite;redundancia;image successive;heritage;localization;successive frame;localizacion;similitude;motion compensated;steganography;content analysis;esteganografia;localisation;redundancy;senal video;signal video;contra medida electronica;imagen sucesiva;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;filigrana digital;safety;similarity;video signal;similitud;analyse contenu;electronic countermeasure;inheritance;vection;seguridad;veccion;canto;redondance	Multimedia digital data is highly redundant: successive video frames are very similar in a movie clip, most songs contain some repetitive patterns, etc. This property can consequently be exploited to successively replace each part of the signal with a similar one taken from another location in the same signal or with a combination of similar parts. Such an approach is all the more pertinent when video content is considered since such signals exhibit both temporal and spatial selfsimilarities. To counter such attacking strategies, it is necessary to ensure that embedded watermarks are coherent with the redundancy of the host content. To this end, both motion-compensated watermarking and selfsimilarities inheritance will be surveyed.	bra–ket notation;coherence (physics);control system;countermeasure (computer);digital data;digital video;digital watermarking;embedded system;frame (video);relevance;video clip	Gwenaël J. Doërr;Jean-Luc Dugelay	2005		10.1007/11551492_17	similarity;internationalization and localization;canto;content analysis;telecommunications;digital watermarking;computer science;electronic countermeasure;similitude;mathematics;multimedia;steganography;redundancy;computer security;statistics	Graphics	43.52342723385346	-10.66525507263342	150791
cc2899feb4eb1d4039a7c85de3c1a1583ab8c94f	finding acceptable trade-off between rate and distortion in multiple description coding based on zero padding			distortion;multiple description coding	Rong-Hua Chang;Chieh-Wei Su;Chow-Sing Lin	2014		10.3233/978-1-61499-484-8-1184	distortion;computer science;padding;theoretical computer science;multiple description coding;mathematical optimization;trade-off	ML	46.4135313860675	-16.106022573814137	150927
deb3a8c27a633ae8f67d8b966b441e4665fb0056	subband video coding with temporally adaptive motion interpolation	phase change materials;detectors;interpolation;image segmentation;motion compensation;huffman coding;reference frame;motion estimation;reference frames;motion compensation reference frames temporally adaptive motion interpolation subband video coding algorithm motion estimation temporal segmentation spatial subband variable target bit allocation pictures constant output bit rate block wise dpcm pcm run length coding truncated huffman coding quantized data simulation results;truncated huffman coding;layout;bit rate;huffman codes;differential pulse code modulation;motion estimation video coding interpolation adaptive signal processing motion compensation image segmentation differential pulse code modulation runlength codes huffman codes;group of picture;quantized data;video coding interpolation layout bit rate change detection algorithms detectors motion estimation current measurement phase change materials huffman coding;pictures;video coding;subband video coding algorithm;adaptive signal processing;current measurement;spatial subband;temporal segmentation;run length coding;constant output bit rate;runlength codes;temporally adaptive motion interpolation;block wise dpcm;bit allocation;pcm;simulation results;variable target bit allocation;change detection algorithms	We present a new subband video coding algorithm with temporally adaptive motion interpolation. In the approach proposed, the reference frames for motion estimation are adaptively selected using temporal segmentation in the lowest spatial subband. Variable target bit allocation for each picture type in a group of pictures is used to allow variable number of reference frames with the constraint of constant output bit rate. Block-wise DPCM, PCM, and run-length coding combined with truncated Huffman coding are used to encode the quantized data in the subbands. Simulation results of the adaptive scheme compare favorably with those of a non-adaptive scheme. >	data compression;motion interpolation	Jungwoo Lee;Bradley W. Dickinson	1994		10.1109/ICASSP.1994.389462	reference frame;computer vision;computer science;theoretical computer science;motion compensation;huffman coding	Vision	46.663073738950594	-17.7963395557884	151101
de3ad540c11d43a64e04c93d0e56413614a4c8fd	video decoder monitoring using non-linear regression	decoding;test no reference video quality assessment monitoring non linear regression prediction decoding;video coding;prediction theory;measurement video recording quality assessment monitoring decoding psnr video sequences;regression analysis;temporal index video decoder monitoring nonlinear regression based prediction method digital video decoder loop video visual quality monitoring video decoding process video quality metrics video quality monitoring tool vqmt video processing tasks human observers image neatness transmission errors regression model mean opinion score mos nlr method plr packet loss rate psnr peak signal to noise ratio spatial index;video coding decoding prediction theory regression analysis	In this research work, a non-linear regression-based prediction method is incorporated into a digital video decoder loop to monitor the visual quality of videos during the decoding process. Considering well-known video quality metrics, a Video Quality Monitoring Tool (VQMT) has been developed for efficient re-use in a variety of video processing tasks. The idea is based on the fact that when human observers rate video quality, they consider reference aspects such as Noise affecting the video or Neatness of images. In addition, transmission errors such as packet loss rate may impact video quality as well. Therefore, defining a Regression model between each one of these reference aspects and the Mean Opinion Score (MOS) provided by human observers can lead to an automatic way to supervise video decoding quality. Promising results have been achieved using a Non-linear Regression (NLR) method together with fundamental video quality metrics namely PLR (Packet Loss Rate), PSNR (Peak Signal to Noise Ratio), the SI (Spatial Index) and the TI (Temporal Index).	coefficient;digital video;distortion;network packet;nonlinear system;peak signal-to-noise ratio;public lending right;spatial anti-aliasing;spatial database;video decoder;video processing	Brice Ekobo Akoa;Emmanuel Simeu;Fritz Lebowsky	2013	2013 IEEE 19th International On-Line Testing Symposium (IOLTS)	10.1109/IOLTS.2013.6604073	scalable video coding;subjective video quality;computer vision;real-time computing;computer science;video quality;block-matching algorithm;multimedia;rate–distortion optimization;motion compensation;pevq;h.261;regression analysis;statistics;multiview video coding	Embedded	45.4886552772267	-21.29404659647375	151187
27b4866d21c018e962e17c14ebd873328ecb3a4f	near-optimal codes for information embedding in gray-scale signals	gray scale signals;code hamming;protection information;wet paper codes average distortion covering codes embedding efficiency embedding rate information embedding ldgm codes steganography;theorie vitesse distorsion;evaluation performance;signal generators;rate distortion;wet paper codes;art;haute performance;error correction codes;ldgm codes;binary stego codes;steganographie;performance evaluation;hamming codes;average distortion;planification optimale;parity check codes;optimal code;code optimal;stego code families;evaluacion prestacion;code aleatoire;high performance steganography;near optimal codes;imagen nivel gris;gray scale;random coding;rate distortion theory;upper bound;near optimal scheme;steganographic codes;materials science and technology;codigo hamming;steganography;esteganografia;proteccion informacion;computational complexity;information embedding;information protection;treble layered method;image niveau gris;performance analysis;rate distortion bound;alto rendimiento;optimal planning;random codes;embedding rate;code binaire;codigo binario;near optimal scheme near optimal codes information embedding gray scale signals high performance steganography steganographic codes binary stego codes hamming codes wet paper codes stego code families rate distortion bound treble layered method;hamming code;embedding efficiency;steganography hamming codes rate distortion theory;planificacion optima;codigo optimal;covering codes;echelle gris;encoding;grey level image;high performance;digital images;binary code;escala gris;gray scale steganography rate distortion error correction codes materials science and technology rate distortion theory signal generators performance analysis art digital images	High-performance steganography requires large embedding rate and small distortion, i.e., high embedding efficiency. Steganographic codes (stego-codes) derived from covering codes can improve embedding efficiency. In this paper, a new method is proposed to construct binary stego-codes for LSB embedding in gray-scale signals, which shows that not just one but a family of stego-codes can be generated from a covering code by combining Hamming codes and wet paper codes. This method can greatly expand the set of embedding schemes as applied to steganography. Performances of stego-code families (SCF) of structured codes and random codes are analyzed. SCFs of random codes can approach the rate-distortion bound on LSB embedding for any chosen embedding rate. Furthermore, SCFs are modified for applications in ±1 embedding, and a treble layered embedding method for ±2 embedding is obtained. By combining the modified SCFs and the treble layered method, a near-optimal scheme for ±2 embedding is presented.	backward compatibility;binary code;covering code;data compression;digital watermarking;distortion;error detection and correction;forward error correction;grayscale;hamming code;least significant bit;performance;sensor;shannon (unit);steganalysis;steganography	Weiming Zhang;Xinpeng Zhang;Shuozhong Wang	2010	IEEE Transactions on Information Theory	10.1109/TIT.2009.2039087	block code;combinatorics;theoretical computer science;hamming code;mathematics;algorithm;statistics	Theory	46.340089781336154	-12.435056378765116	151271
8b7ec43ad0059421de9a837e54d6ec58b338059c	faster plots by fan data compression	tratamiento datos;concepcion asistida;computer aided design;data compression;computer graphics;traceur courbe;data processing;graphic plotting;traitement donnee;algorithme;algorithm;aparato trazador curva;trace graphique;trazado grafico;conception assistee;curve plotter;fan algorithm bode plots computer graphics fan data compression laser printer speed simulation plots;data compression computer graphics;data compression printers image coding application software data engineering space vehicles computer graphics throughput sampling methods aircraft;algoritmo	A fan data compression method is presented that tripled laser printer speed for Bode and simulation plots with many points. This reduced printer delays without the expense of a faster laser printer, and it saved computer time as well. The authors describe their problem, solution, and conclusions. They give the fan algorithm and present its performance for several applications. They include a pseudocode implementation.<<ETX>>	algorithm;bode plot;data compression;laser printing;printer (computing);pseudocode;simulation	Richard A. Fowell;David D. McNeil	1989	IEEE Computer Graphics and Applications	10.1109/38.19052	data compression;computer vision;simulation;data processing;computer science;artificial intelligence;operating system;computer graphics;algorithm;computer graphics (images)	Graphics	39.442719475350444	-20.150367637087854	151415
86e3a20ca090eaebfa149d38a0bed95f3794311c	dynamic adaptive forward error control for image transmission over lossy networks	interleaving;variable fec;propagation losses;forward error control;network bandwidth;image coding;error concealment;data compression;constant fec;block level triangular interleaving;ber;visual communication;adaptive control;programmable control;fec overhead;transform coding;atm networks;interleaved codes;jpeg compressed image;forward error correction;resilience;selective forward error correction;image transmission;programmable control adaptive control error correction forward error correction transform coding image coding propagation losses interleaved codes bandwidth resilience;jpeg;error correction;telecommunication networks visual communication image coding data compression forward error correction;bandwidth;compression ratio;lossy networks;bursty losses;atm networks dynamic adaptive forward error control image transmission lossy networks jpeg compressed image block level triangular interleaving selective forward error correction variable selective forward error correction network bandwidth bursty losses compression ratio transmitted image transmitted data jpeg variable fec fec overhead constant fec ber;dynamic adaptive forward error control;dynamic adaptation;transmitted image;telecommunication networks;transmitted data;variable selective forward error correction	We address the problem of transmitting JPEG compressed image over a lossy network. We introduce a combination of block level triangular interleaving scheme (TRII), and variable and selective forward error correction scheme (VS-FEC) to make the transmitted data more resilient while using the network bandwidth more efficiently in the presence of bursty losses. Our results show that TRII barely affects the final compression ratio of the transmitted image (on the average just 4% below JPEG), while considerably increases its resilience to errors. Also, in the presence of losses, a new scheme is introduced to dynamically add a variable FEC to avoid retransmission of the lost data. A small amount of FEC overhead was needed (4%, versus 10%-15% in constant FEC) to be at least 90% sure on the average that the protected information will not be damaged during transmission.	error detection and correction;lossy compression	Rogelio Hasimoto-Beltrán;Ashfaq A. Khokhar	2000		10.1109/ITCC.2000.844261	electronic engineering;telecommunications;computer science;computer network	Vision	48.22372338713023	-15.593498898347763	151416
91941b935f47c25c16a0ce6e030c1e2f60d2bfd9	adaptively weighted vector-median filters for motion-fields smoothing	median filter;image matching;adaptive filters smoothing methods motion estimation image restoration optical filters image coding image reconstruction noise reduction optical noise spatial coherence;prediction theory adaptive filters adaptive signal processing video coding median filters motion estimation smoothing methods image matching image restoration correlation methods;motion estimation;image restoration;correlation methods;motion vector field;video coding;smoothing methods;adaptive filters;vector median filters motion fields smoothing adaptively weighted vector median filters video coding backward prediction standards conversion interframe motion estimation motion vector fields restoration image restoration block matching algorithm spatial correlation reliability;adaptive signal processing;spatial correlation;prediction theory;vector field;block matching algorithm;median filters	In the eld of video coding recent issues of backward prediction and standard conversion have focused an increasing attention towards techniques for an eeective estimation of the true interframe motion. In this paper the problem of restoration of motion vector-elds computed by means of a standard Block Matching algorithm is addressed. The restoration must be carried out carefully by exploiting both the spatial correlation of the vector-eld, and the signiicance of the obtained vectors as measures of the reliability of the previous estimation step. In this paper a novel approach matching both the above requirements is presented. Based on the theory of vector-median lters an adaptive scheme is developed and results are discussed.	block-matching algorithm;circuit restoration;data compression;requirement;smoothing	Luciano Alparone;Mauro Barni;Franco Bartolini;Vito Cappellini	1996		10.1109/ICASSP.1996.545874	adaptive filter;computer vision;mathematical optimization;computer science;pattern recognition;mathematics	Vision	45.64483935077185	-16.999043441380852	151510
394c19cd0cef3b2b445be03c73fe63e466cc4a9c	context-based lossless and near-lossless compression of eeg signals	electroencephalography context modeling data compression data mining image coding image reconstruction brain injuries predictive coding law legal factors;data compression;data compression electroencephalography medical signal processing;lossy compression;diagnostic accuracy;entropy coding;lossless compression;indexing terms;context model;clinical conditions context based lossless compression near lossless compression eeg signals compression techniques electroencephalograph signals predictive coding methods dictionary based approaches context modeling techniques compression ratios lossless compression context based bias cancellation activity based conditional coding lossy compression diagnostic accuracy near lossless compression technique quantitative bounds;compression ratio;electroencephalography;condition index;predictive coding;medical signal processing;electroencephalography signal processing computer assisted	We study compression techniques for electroencephalograph (EEG) signals. A variety of lossless compression techniques, including compress, gzip, bzip, shorten, and several predictive coding methods, are investigated and compared. The methods range from simple dictionary based approaches to more sophisticated context modeling techniques. It is seen that compression ratios obtained by lossless compression are limited even with sophisticated context based bias cancellation and activity based conditional coding. Though lossy compression can yield significantly higher compression ratios while potentially preserving diagnostic accuracy, it is not usually employed due to legal concerns. Hence, we investigate a near lossless compression technique that gives quantitative bounds on the errors introduced during compression. It is observed that such a technique gives significantly higher compression ratios (up to 3-bit/sample saving with less than 1% error). Compression results are reported for EEG's recorded under various clinical conditions.	compresses (device);dictionary [publication type];electroencephalograph;electroencephalography;lossless compression;lossy compression;netbsd gzip / freebsd gzip;bzip	Nasir D. Memon;Xuan Kong;J. Cinkler	1999	IEEE Transactions on Information Technology in Biomedicine	10.1109/4233.788586	data compression;lossy compression;lossless jpeg;data compression ratio;speech recognition;index term;block truncation coding;electroencephalography;transparency;image compression;computer science;entropy encoding;theoretical computer science;compression ratio;pattern recognition;mathematics;lossless compression;context model;adaptive coding;context-adaptive binary arithmetic coding;statistics	Visualization	48.809051108724816	-13.156790664701475	151514
60066632be918f93f8db643787450aec119f60f3	textural complexity-based rate control algorithm	cauchy density based bit allocation;complexity theory;psnr;complexity theory encoding algorithm design and analysis psnr streaming media strips estimation;h 264 avc;textural complexity;robust buffer controllability;robust buffer controllability textural complexity rate control algorithm content adaptive initial quantization h 264 avc versatile video scenes bit allocation strategy cauchy density based bit allocation visual quality;visual quality;image texture;rate control;video coding;estimation;streaming media;quantization parameter;content adaptation;rate control algorithm;bit allocation strategy;bit allocation;strips;versatile video scenes;encoding;algorithm design and analysis;video coding image texture;content adaptive initial quantization	This paper presents an efficient rate control algorithm based on our content-adaptive initial quantization parameter setting scheme for H.264/AVC. For versatile video scenes, our algorithm can adaptively set an appropriate initial QP based on the textural complexity estimated from the first picture. In addition, our bit-allocation strategy effectively distributes the bit-rate budget based on the monotonic property to enhance the coding efficiency. Our proposed algorithm surpasses JVT-H014 rate control algorithm and Cauchy-density-based bit-allocation scheme in terms of average PSNR for about 0.47 dB and 0.81 dB respectively. Besides, our algorithm provides more impressive visual quality and more robust buffer controllability.	algorithm;algorithmic efficiency;h.264/mpeg-4 avc;peak signal-to-noise ratio	Gwo Giun Lee;He-Yuan Lin;Ming-Jiun Wang	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607633	image texture;algorithm design;computer vision;estimation;strips;real-time computing;peak signal-to-noise ratio;computer science;theoretical computer science;encoding;statistics	Robotics	46.97237730266266	-17.715103503837756	151657
9085daeff55e776c92519bc4273f09bfe9c1d346	a fast randomized method to find homotopy classes for socially-aware navigation		We introduce and show preliminary results of a fast randomized method that finds a set of K paths lying in distinct homotopy classes. We frame the path planning task as a graph search problem, where the navigation graph is based on a Voronoi diagram. The search is biased by a cost function derived from the social force model that is used to generate and select the paths. We compare our method to Yen’s algorithm, and empirically show that our approach is faster to find a subset of homotopy classes. Furthermore our approach computes a set of more diverse paths with respect to the baseline while obtaining a negligible loss in path quality.	baseline (configuration management);feedback;graph (discrete mathematics);graph traversal;loss function;motion planning;randomized algorithm;sampling (signal processing);search problem;social force model;state space;voronoi diagram;weighted voronoi diagram	Luigi Palmieri;Andrey Rudenko;Kai Oliver Arras	2015	CoRR		mathematical optimization;combinatorics;discrete mathematics;any-angle path planning;mathematics	AI	51.60884164596175	-23.832133416778355	151917
556f24dac2efe52a0996dfec6db24066b2df333a	lossy to lossless image compression based on reversible integer dct	discrete wavelet transforms;matrix factorization;image coding;discrete wavelet transform;codecs;data compression;lossy compression;bit plane encoding;lossless image compression;matrix decomposition codecs data compression discrete cosine transforms discrete wavelet transforms image coding;lossless compression;reversible integer pre filters;reversible integer dct block transform jpeg2000 lossless compression;intdct;transform coding;indexing terms;discrete cosine transform;reversible integer dct;image compression;floating point filter;matrix decomposition;discrete cosine transforms;wavelet image codec;transforms;reversible integer post filters;jpeg2000;block transform;floating point;floating point filter image compression discrete cosine transform reversible integer dct matrix factorization wavelet image codec intdct 121 dct coefficients reorganization bit plane encoding reversible integer pre filters reversible integer post filters jpeg2000;121 dct;encoding;image coding discrete cosine transforms transform coding filters performance loss discrete wavelet transforms codecs image reconstruction laboratories information processing;coefficients reorganization	A progressive image compression scheme is investigated using reversible integer discrete cosine transform (RDCT) which is derived from the matrix factorization theory. Previous techniques based on DCT suffer from bad performance in lossy image compression compared with wavelet image codec. And lossless compression methods such as IntDCT, I2I-DCT and so on could not compare with JPEG-LS or integer discrete wavelet transform (DWT) based codec. In this paper, lossy to lossless image compression can be implemented by our proposed scheme which consists of RDCT, coefficients reorganization, bit plane encoding, and reversible integer pre- and post-filters. Simulation results show that our method is competitive against JPEG-LS and JPEG2000 in lossless compression. Moreover, our method outperforms JPEG2000 (reversible 5/3 filter) for lossy compression, and the performance is even comparable with JPEG2000 which adopted irreversible 9/7 floating-point filter (9/7F filter).	bit plane;codec;coefficient;discrete cosine transform;discrete wavelet transform;image compression;jpeg 2000;least squares;lossless compression;lossy compression;simulation;the matrix	Lei Wang;Jiaji Wu;Licheng Jiao;Li Zhang;Guangming Shi	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711935	data compression;lossy compression;data compression ratio;discrete mathematics;image compression;computer science;theoretical computer science;mathematics;lossless compression;fractal transform;context-adaptive binary arithmetic coding;matrix decomposition;algorithm	Robotics	45.00999129490668	-16.18836515300075	152020
a5398db26eadcf5aff6a5969b5840c7c4f2e5efe	extension of two-stage vector quantization-lattice vector quantization	image storage;rate distortion;transform vq plvq;high resolution;image coding;source coding two stage vector quantization lattice vector quantization high resolution quantization theory spherical boundary vector quantization pyramidal lattice vector quantization simulation memoryless sources rate distortion performance fixed rate quantization transform vq plvq encoding 16 d vectors gauss markov source;lattices;iterative algorithms;markov source;gaussian processes;fixed rate quantization;simulation;indexing terms;two stage vector quantization lattice vector quantization;rate distortion theory;vector quantization lattices image coding source coding signal processing algorithms iterative algorithms image storage constraint theory rate distortion gaussian processes;encoding 16 d vectors;vector quantization;memoryless sources;spherical boundary;high resolution quantization theory;rate distortion performance;constraint theory;markov processes vector quantisation source coding rate distortion theory memoryless systems gaussian processes;vector quantization pyramidal lattice vector quantization;source code;vector quantizer;markov processes;signal processing algorithms;gauss markov source;memoryless systems;vector quantisation;information theory;source coding;lattice vector quantization	This paper is the extension of two-stage vector quantization–(spherical) lattice vector quantization (VQ–(S)LVQ) recently introduced by Pan and Fischer [1]. First, according to high resolution quantization theory, generalized vector quantization–lattice vector quantization (G-VQ–LVQ) is formulated in order to release the constraint of the spherical boundary for the second-stage lattice vector quantization (LVQ), which would provide possibilities of improving this kind of two-stage unstructured/structured quantizer by using more efficient LVQ. Second, among G-VQ–LVQ, vector quantization–pyramidal lattice vector quantization (VQ–PLVQ) is developed which is slightly superior or comparable to VQ–(S)LVQ in performance but has a much lower complexity. Simulation results show that for memoryless sources, VQ–PLVQ achieves a rate-distortion performance that is among the best of the fixed-rate quantization that we found in the literature. Therefore, VQ–PLVQ is an attractive alternative to VQ–(S)LVQ in practice. Third, transform VQ–PLVQ (TVQ–PLVQ) is proposed for sources with memory. For encoding 16-D vectors of the Gauss–Markov source, T-VQ–PLVQ has an advantage of close to 1.0 dB over VQ–PLVQ and is about 0.5 dB better than VQ–(S)LVQ.	distortion;image resolution;learning vector quantization;markov chain;markov information source;michael j. fischer;quantization (signal processing);simulation	Jie Pan	1997	IEEE Trans. Communications	10.1109/26.650231	mathematical optimization;discrete mathematics;learning vector quantization;quantization;information theory;computer science;theoretical computer science;mathematics;linde–buzo–gray algorithm;vector quantization;statistics;source code	Vision	48.49362892748645	-12.532976535816687	152087
ebe2a7ddf95983ba74cfebb11ff4812e19a306e2	options for a new efficient, compatible, flexible 3d standard	3d;compression 3d stereo ldv;video signal processing;consumer electronics;video signal processing consumer electronics stereo image processing;stereo flexible 3d standard 3d video consumer electronics spatial subsampling consumer confusion display types display sizes;monos devices glass cameras laboratories consumer electronics three dimensional displays video compression transform coding standardization fatigue;stereo image processing;stereo;ldv;compression;3d video	Recently the interest in 3D video has significantly increased (as eg. could be seen on the consumer electronics show CES 2009). Because there is a lack of one clear recognized 3D standard, various different (proprietary or non-proprietary) standards or solutions are starting to enter the market already based on stereo where some kind of spatial sub-sampling is being applied. To avoid consumer confusion and to ensure support of many different display types and sizes, a flexible, generic, scalable, compatible 3D video standard would therefore be highly desirable on short notice. This paper presents objectives and some options for realizing such a generic 3D standard.	sampling (signal processing);scalability	W. H. A. Bruls;Rene Klein Gunnewiek	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414060	video compression picture types;stereo cameras;computer vision;computer science;multimedia;video processing;stereophonic sound;compression;3d computer graphics;computer graphics (images)	Vision	43.19030646069775	-20.925536588333852	152394
2d402b0565d5006cac4a60a5ec6bda3b444036ae	low-complexity gmm-based block quantisation of images using the discrete cosine transform	quantization;evaluation performance;mezcla senal;karhunen loeve transformation;gaussian mixture;cuantificacion;signal mixing;image coding;performance evaluation;image processing;280203;gaussian processes;proceso markov;institute for integrated and intelligent systems;probability density function;evaluacion prestacion;transformation cosinus discrete;procesamiento imagen;entropy coding;low complexity;280204;transform coding;quantification;melange signal;markov property;traitement image;journal article;discrete cosine transform;codage image;gaussian mixture model;efficient implementation;computational complexity;faculty of engineering and information technology;discrete cosine transforms;gaussian mixture models;processus markov;markov process;processus gaussien;transformation karhunen loeve;pre2009 image processing;transformacion karhunen loeve;karhunen loeve transform;pre2009 signal processing;block quantisation;bits per pixel	While block transform image coding has not been very popular lately in the presence of current state-of-the-art wavelet-based coders, the Gaussian mixture model (GMM)-based block quantiser, without the use of entropy coding, is still very competitive in the class of fixed rate transform coders. In this paper, a GMM-based block quantiser of low computational complexity is presented which is based on the discrete cosine transform (DCT). It is observed that the assumption of Gaussian mixture components in a GMM having Gauss–Markov properties is a reasonable one with the DCT approaching the optimality of the Karhunen–Loève transform (KLT) as a decorrelator. Performance gains of 6–7 dB are reported over the traditional single Gaussian block quantiser at 1 bit per pixel. The DCT possesses two advantages over the KLT: being fixed and source independent, which means it only needs to be applied once; and the availability of fast and efficient implementations. These advantages, together with bitrate scalability, result in a block quantiser that is considerably faster and less complex while the novelty of using a GMM to model the source probability density function is still preserved. r 2005 Elsevier B.V. All rights reserved.	1-bit architecture;approximation algorithm;block cipher;computational complexity theory;discrete cosine transform;entropy encoding;google map maker;markov chain;mixture model;multimodal interaction;pixel;portable document format;quantization (signal processing);scalability;transform coding;transformation matrix;wavelet	Kuldip K. Paliwal;Stephen So	2005	Sig. Proc.: Image Comm.	10.1016/j.image.2005.03.001	speech recognition;lapped transform;image processing;computer science;pattern recognition;mixture model;mathematics;statistics	Vision	45.62972349476927	-14.652393017596733	152405
cf3a10fb670967b12f02fa87b4407dc6a2b10872	a two-staged multi-level reversible data hiding exploiting lagrange interpolation	histograms psnr interpolation image quality prediction algorithms educational institutions nickel;two staged multilevel reversible data hiding psnr value embedding capacity lena image quality lagrange interpolation;information hiding;histogram shifting modification;histogram shifting modification information hiding reversible data hiding;interpolation cryptography data encapsulation image coding;reversible data hiding	This paper is a two staged multi-layer reversible data hiding scheme based on Lagrange interpolation. Lagrange interpolation will cross applied on odd and even pixels to produce predicted images, and then histogram shifting modification will be applied to embed confidential data into the differences between predicted and original images. The experimental results show the quality of Lena image from this scheme has a PSNR value of 66.88dB and a embedding capacity of 0.1bpp, for one level. For 10 levels, the embedding capacity is 2.17 bpp and the scheme can still maintain a high PSNR value of 45dB, which means this algorithm is more efficient than the algorithms from Ni et al. and Tai et al, for both quality and embedding capacity.	algorithm;confidentiality;image histogram;interpolation;lagrange multiplier;lagrange polynomial;layer (electronics);lenna;peak signal-to-noise ratio;pixel	Chin-Feng Lee;Chin-Chen Chang;Cheng-You Gao	2013	2013 Ninth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2013.126	computer vision;computer science;theoretical computer science;data mining;mathematics;programming language;information hiding	Robotics	40.011833425886216	-11.986699417538125	152430
5954dc362e468649da746963dd2bcd0348b4e5dc	dvts video frame rate adjustment based on motion detection	available bandwidth;temporal difference;image motion analysis;video transfer system;video streaming;video signal processing;video quality;high performance networks;transport system;video quality video frame rate adjustment motion detection digital video transport system video transfer system video stream;digital video;video frame rate adjustment;digital video transport system;video stream;motion detection;video streaming image motion analysis video signal processing;motion detection streaming media bandwidth videoconference degradation video compression computer displays decoding ip networks educational technology	DVTS (digital video transport system) is a high quality video transfer system that is suitable for high performance network environments. In order to transmit a DV stream in a narrow bandwidth environment, the existing approach allows the sender to reduce the frame rate to suit with the available bandwidth. However, one drawback of this method is that the frame rate is fixed for a single DV stream resulting in an inefficient use of network bandwidth. This is because sometimes the motion of objects in the video stream is very small and it is possible to further reduce the frame rate without significantly degrading the video quality. The proposed method is designed to automatically adjust the frame rate by exploiting the motion in the DV stream. The calculation of amount of motion is based on a simple motion detection mechanism that uses temporal differences between corresponding Y (luminance) components of two consecutively sampled DV frames	digital video;display resolution;streaming media	Nawat Kamnoonwatana;Andrey Kuprianov;Poompat Saengudomlert;Teerapat Sanguankotchakorn;Kanchana Kanchanasut	2006	2006 Second International Conference on Automated Production of Cross Media Content for Multi-Channel Distribution (AXMEDIS'06)	10.1109/AXMEDIS.2006.26	inter frame;residual frame;computer vision;computer science;block-matching algorithm;multimedia;motion compensation;micro stuttering;computer graphics (images)	Robotics	43.94483960937607	-20.98664731919584	152461
bebfded987d46177010675ff6b35f05c9e85d247	coding algorithm for grayscale images based on linear prediction and dual mode quantization	pixel value prediction;uniform quantizers;image compression;piecewise uniform quantizers	We propose a new method of grayscale image compression.The algorithm is based on a prediction technique.We use quantizers designed for discrete input samples.Proposed model provides higher PSQNR up to 6.14dB in comparison to other similar models. This paper proposes a novel algorithm for grayscale image compression based on dual mode quantization that is supported by improved linear prediction scheme. The idea of dual mode quantization comes from desire to exploit advantages of the both uniform and piecewise uniform quantizers, designed for discrete input samples. The algorithm performs quantizers with a low and medium number of quantization levels and with a fixed codeword length by using a pixel value prediction in preprocessing. The correlation of adjacent pixels is exploited as the main idea for improving the quality of image compression. The proposed prediction is linear and very simple for practical realization. An analysis of reconstructed image quality is presented considering several parameters and by comparing with few other methods - BTC, DPCM and with methods that use transformation coding. Experiments are done applying the proposed compression model to several standard grayscale test images. Special attention is given to determination of thresholds values that determine whether and which of the two offered quantizers to use. Moreover, method for determining the value of proposed quantizer's variance is explained. Obtained results show that proposed model ensures gain up to 6.14 dB compared to the BTC model that uses fixed piecewise uniform quantization for discrete input without a pixel value prediction as well as gain up to 5.89 dB compared to the DPCM model that applies dual predictor. The proposed algorithm could find application in current grayscale image compression and video standards.	algorithm;grayscale;quantization (signal processing)	Milan S. Savic;Zoran H. Peric;Nikola Simic	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.05.037	mathematical optimization;image compression;computer science;theoretical computer science;mathematics	Vision	43.482772722015305	-15.13492715268688	152612
499a29aff99a207a844e99b2b53d02e9d770b898	a modification-free steganography method based on image information entropy		In order to improve the security and robustness of the Information SteganographyAlgorithmunder strictly controlled environment, a new algorithm of modification-free steganography based on image and big data is introduced in this paper. In the proposed algorithm, a mapping relationship between the hot image entropy and the secret information is constructed and the payload information is expressed by the mapping relation. At the same time, turbo code is introduced in order to improve robustness, the hot image comes from Internet image big data, and the library of hot image is established. The performance of the proposed algorithm is analyzed using simulation experiment. Because of its none-modifying on carrier image, the results of experiment show that the proposed algorithm can achieve good performance in robustness analysis, dimension scaling attack, and rotation attack. In particular, in the test of dimension scaling attack and the rotation attack, the rate of data recovering can be over 95%.The proposed algorithm can be very valuable in the covert communication which requires high security and low volume, for example, the key exchange of symmetric encryption system.		Xia ShuangKui;Jianbin Wu	2018	Security and Communication Networks	10.1155/2018/6256872	computer science;entropy (information theory);computer network;turbo code;steganography;symmetric-key algorithm;robustness (computer science);big data;theoretical computer science;scaling;key exchange	ML	39.906087339121434	-11.77739849749953	152682
6733328d6321166484b9828b133961a700380352	an efficient codec for image compression based on spline wavelet transform and improved spiht algorithm		This paper presents an efficient codec which is based on an optimal spline wavelet transform and an improved Set Partitioning in Hierarchical Trees algorithm. A comparative study of the proposed codec with the existing works using the polynomial spline based transform and the biorthogonal B9/7 which is frequently used in image compression is done. Peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) and encoding time are used for evaluation purpose. The obtained results prove the efficiency and the speed of the proposed codec.	algorithm;b-spline;codec;graphics processing unit;image compression;lifting scheme;peak signal-to-noise ratio;polynomial;set partitioning in hierarchical trees;spline (mathematics);spline wavelet;structural similarity;wavelet transform	Rania Boujelbene;Yousra Ben Jemaa;Mourad Zribi	2017	2017 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCS.2017.124	stationary wavelet transform;wavelet transform;image compression;spline wavelet;codec;algorithm;biorthogonal system;computer science;set partitioning in hierarchical trees;spline (mathematics)	EDA	42.83678073459198	-15.472135603318405	152745
365cd3c98f96dc63ad65a569aa80d916da42d0ca	joint source/channel decoding of scalefactors in mpeg-aac encoded bitstreams	audio coding;combined source-channel coding;error analysis;video coding;mpeg-aac encoded bitstreams;advanced audio coding;bandwidth-efficient method;blind mode;damaged spectral data encoding reconstruction;hard-decoding method;highly noise-sensitive part;informed mode;intraframe error concealment method;joint source-channel decoding improvement;medium snr;moving picture experts group;noisy channel;perceptual signal quality improvement;scale factor reconstruction;soft-decoding method	This paper describes a bandwidth-efficient method for improved decoding of MPEG-AAC bitstreams when the encoded data are transmitted over a noisy channel. Assuming that the critical part (headers) of each frame has been correctly received, we apply a soft-decoding method to reconstruct the scalefactors, which represent a highly noise-sensitive part of the bitstream. The damaged spectral data are reconstructed using an intra-frame error concealment method. Two methods for soft decoding of scalefactors are described: blind mode and informed mode. In the latter, a very small amount of additional data is included in the bitstream. At medium SNR, this method provides a significant improvement in perceptual signal quality compared to the classical hard-decoding method.	advanced audio coding;bit-length;bitstream;coefficient;decoding methods;error concealment;forward error correction;intra-frame coding;markov chain;markov model;moving picture experts group;noisy-channel coding theorem;signal-to-noise ratio;transmitter	Olivier Derrien;Michel Kieffer;Pierre Duhamel	2008	2008 16th European Signal Processing Conference		list decoding;real-time computing;speech recognition;telecommunications;computer science	NLP	48.13664030087358	-10.081517331059588	152872
9cb53b748ae1ad4c1d2c281b70c631e9557cc285	extended application of scalable video coding methods	estensibilidad;analisis imagen;distributed system;red sin hilo;rendimiento elevado;largeur bande;streaming;systeme reparti;video streaming;image coding;scalable video coding;reseau sans fil;transmision continua;wireless network;synchronisation;codage predictif;codage image;video coding;transmission en continu;fine granular scalable;sistema repartido;internet;senal video;signal video;codage video;synchronization;anchura banda;codificacion predictiva;pattern recognition;video signal;rendement eleve;bandwidth;image analysis;sincronizacion;extensibilite;scalability;reconnaissance forme;reconocimiento patron;analyse image;high efficiency;predictive coding	SP(Synchronization-Predictive) frame coding, which enables high efficiency of switching between two video bitstreams with different qualities, is supported by H.264/AVC. And FGS(Fine-Granular-Scalability) coding is supported by MPEG-4 video standard. This paper proposes a solution for combination these two tools with each other so as to adapt to high bandwidth variations of Internet or wireless networks and to low bandwidth variations flexibly for transmitted video streams. Experimental results show that our proposed system outperforms FGS coding by 0.47dB and the H.264/AVC-based video stream switching approach by 0.23dB on average.	scalability;scalable video coding	Zhi-gang Li;Zhaoyang Zhang;Biao Wu	2005		10.1007/11559573_45	video compression picture types;scalable video coding;synchronization;image analysis;telecommunications;computer science;context-adaptive variable-length coding;coding tree unit;multimedia;context-adaptive binary arithmetic coding;motion compensation;h.261;multiview video coding;computer graphics (images)	Vision	47.09449713983519	-14.54367304628575	153280
beb24d3389a29e17bfb3eac505d561306d4bc4c5	zerotree wavelet image compression with weighted sub-block-trees and adaptive coding order	embedded zerotree wavelet;weighted sub block tree;adaptive coding order;image compression;human visual system	Information distortion in regions of image edge is more perceptible for people than those in other regions. To improve the performance on the edge, we present an improved embedded zero-tree wavelet image compression algorithm with weighted sub-block-trees and adaptive coding order. First, we assign bigger weights to the sub-block-trees around image edge. The weights assigned to the sub-blocks in the same spatial location and the same orientation at different scales are equal so that the zero-tree structure of wavelet coefficients is maintained and only a little extra storage is needed. Then we prefer scanning the coefficients on the neighbor of previous significant coefficients and all of them are refined even they are not significant. Adaptive arithmetic coding is applied to the symbols of these coefficients and others respectively. The proposed method pays more attention to the edge and its neighborhood so that the decoded image on the edge is clearer. Compared with similar algorithms, experimental results show that the proposed method can improve the PSNR and SSIM, as well as the subjective visual experience. The proposed method is applicable to any genre of embedded wavelet image codec.	adaptive coding;algorithm;arithmetic coding;codec;coefficient;distortion;embedded system;image compression;nl (complexity);numerical aperture;peak signal-to-noise ratio;set partitioning in hierarchical trees;stationary wavelet transform;structural similarity;tree structure	Hui Liu;Ke-Kun Huang	2016	IJWMIP	10.1142/S0219691316500211	computer vision;image compression;computer science;theoretical computer science;pattern recognition;mathematics;human visual system model;algorithm	Robotics	44.27544584355475	-15.680912798414038	153519
1310d30238dee01a72e04ad88cf7126d8383ff9e	an improved error resilience scheme for transmission of mpeg-4 audio over egprs	bit error rate;signal reconstruction audio coding cellular radio huffman codes;huffman codeword reordering algorithm;cellular radio;real time;packet radio networks;code standards;huffman codes;protection;audio coding;advanced audio coding;egprs;mpeg 4 standard;enhanced gprs;3g mobile communication;resilience;audio signal quality error resilience mpeg 4 audio egprs enhanced general packet radio service enhanced gprs real time mpeg 4 aac advanced audio coding huffman codeword reordering algorithm audio signal reconstruction;audio signal reconstruction;mpeg 4 audio;resilience mpeg 4 standard bit error rate psychoacoustic models audio coding code standards signal reconstruction protection packet radio networks 3g mobile communication;error resilience;signal reconstruction;audio signal quality;general packet radio service;psychoacoustic models;real time mpeg 4 aac;enhanced general packet radio service	An improved pre-sorting and reordering algorithm is proposed for enhancing the quality of transmitting real-time MPEG-4 AAC (advanced audio coding) audio over EGPRS (Enhanced General Packet Radio Service). Compared with the standard Huffman codeword reordering (HCR) algorithm, lower implementation complexity and much better audio signal reconstruction quality may be achieved with the proposed scheme.	mpeg-4 part 3	Lei Miao;Jianhua Lu;Jun Gu	2001		10.1109/VTC.2001.956631	signal reconstruction;real-time computing;edge;bit error rate;telecommunications;advanced audio coding;computer science;operating system;mpeg-4;psychological resilience;huffman coding;computer network;general packet radio service	HCI	47.78696913582119	-10.067387164766561	153656
a325459937467d3d5a263fc855636041fb7f9be5	a fast coding unit size decision algorithm for hevc intra coding based on image complexity	complexity theory;support vector machines;prediction algorithms;video coding;classification algorithms;encoding;algorithm design and analysis	High Efficiency Video Coding (HEVC) achieves the most significant coding efficiency compared with all the existing video coding standards. However, the Intra encoding complexity is increased dramatically since the complex recursive search algorithm for the coding unit (CU) size decisions. In this paper, a fast CU size decision algorithm based on Support Vector Machines (SVM) is proposed to further alleviate the encoder computation load. Firstly, some effective image features are extracted to judge the CU complexity. Then, a three-classifier structure of CU size decisions is established based on SVM. The experimental results show that the proposed algorithm can reduce 53% intra coding time on average with 1.2% BDBR loss on average.	algorithmic efficiency;computation;computational complexity theory;data compression;encoder;high efficiency video coding;intra-frame coding;recursion;search algorithm;support vector machine;tip (unix utility);video coding format	Deyuan Liu;Xingang Liu;Yayong Li	2016	2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)	10.1109/iThings-GreenCom-CPSCom-SmartData.2016.180	statistical classification;support vector machine;algorithm design;prediction;harmonic vector excitation coding;computer science;theoretical computer science;machine learning;coding tree unit;pattern recognition;context-adaptive binary arithmetic coding;encoding	Robotics	46.8585024434267	-19.456240876907554	153657
714bfb1596abf7aafb3c924dd1115ca92afcff2d	provably safe and deadlock-free execution of multi-robot plans under delaying disturbances	system recovery;trajectory;aerospace electronics;planning;collision avoidance;robot kinematics	One of the standing challenges in multi-robot systems is the ability to reliably coordinate motions of multiple robots in environments where the robots are subject to disturbances. We consider disturbances that force the robot to temporarily stop and delay its advancement along its planned trajectory which can be used to model, e.g., passing-by humans for whom the robots have to yield. Although reactive collision-avoidance methods are often used in this context, they may lead to deadlocks between robots. We design a multi-robot control strategy for executing coordinated trajectories computed by a multi-robot trajectory planner and give a proof that the strategy is safe and deadlock-free even when robots are subject to delaying disturbances. Our simulations show that the proposed strategy scales significantly better with the intensity of disturbances than the naive liveness-preserving approach. The empirical results further confirm that the proposed approach is more reliable and also more efficient than state-of-the-art reactive techniques.	autonomous robot;control theory;deadlock;formal proof;format-preserving encryption;liveness;logistics;non-blocking algorithm;robot control;simulation	Michal Cáp;Jean Gregoire;Emilio Frazzoli	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759750	planning;control engineering;simulation;computer science;engineering;artificial intelligence;trajectory;robot kinematics	Robotics	53.40781423237832	-21.883184544280002	153690
f5f619452a93be8387cbba22d8d213d4add37a3f	deinterlacing with motion adaptive vertical temporal filtering	filtering;interpolation;image motion analysis;deinterlacing vertical temporal filter frame;video signal processing;motion adaptation;video quality;low complexity;temporal filtering;correlation low complexity;current frame;intrafield deinterlacing method;adaptive filters;filtering algorithms;deinterlacing;video signal processing adaptive filters image motion analysis;pixel;vertical temporal filter;adjacent frames;correlation low complexity deinterlacing vertical temporal filter frame;mobile communication;motion adaptive vertical temporal filtering;adaptive filters filtering interpolation computational complexity information technology nonlinear filters motion estimation motion compensation image converters image resolution;frame;video quality motion adaptive vertical temporal filtering adjacent frames current frame intrafield deinterlacing method;correlation;containers	In this paper, we propose a deinterlacing method with motion adaptive vertical temporal filtering, which utilizes the correlations between adjacent frames. In this model, we first interpolate the missing lines of the current frame and adjacent frames by using an intrafield deinterlacing method. Then we compute the pixel differences between the current frame and the adjacent frames. Since the differences between the adjacent frames would show similar patterns, we can use these patterns to improve deinterlacing performance. In other words, instead of performing deinterlacing in the frame domain, we perform the operation in the frame difference domain. Since the proposed method produces good performance in stationary regions, we selectively apply the vertical temporal filter. Then we apply the proposed method iteratively in order to enhance video quality. The proposed method shows low complexity and still produces superior performance. Experimental results show that the proposed method provides noticeable improvements over existing methods in terms of both subjective and objective evaluations.	deinterlacing;interpolation;mad;motion estimation;pixel;stationary process	Kwon Lee;Jonghwa Lee;Chulhee Lee	2009	IEEE Transactions on Consumer Electronics	10.1109/TCE.2009.5174433	filter;adaptive filter;frame;computer vision;electronic engineering;mobile telephony;interpolation;computer science;video quality;deinterlacing;mathematics;correlation;pixel;computer graphics (images)	Vision	46.00489592424268	-17.78135276928206	153792
584e3950650b7e1f9e3292c7d9d589ff00f077b1	fast block mode decision algorithm in h.264/avc using a filter bank of kalman filters for high definition encoding	simulation ordinateur;algorithme rapide;kalman filtering;theorie vitesse distorsion;traitement signal;video encoding;evaluation performance;rate distortion;degradation;high resolution;filtrage kalman;performance evaluation;filter bank;filtro kalman;motion compensation;video signal processing;banc filtre;implementation;evaluacion prestacion;h 264 avc;degradacion;filtre kalman;kalman filter;calculateur simd;spatial prediction;motion compensated;rate distortion theory;compensation mouvement;video coding;haute resolution;codage video;simd computer;signal processing;banco filtro;fast algorithm;alta resolucion;traitement signal video;simulacion computadora;implementacion;procesamiento senal;computer simulation;mode decision;high definition;algoritmo rapido;filtrado kalman	In this paper, we propose a fast mode decision algorithm using a filter bank of Kalman filters for H.264/ AVC. For the highest coding efficiency in H.264/AVC, a macroblock can be coded with seven different block sizes for motion compensation in an inter mode and various spatial prediction modes in an intra mode. The conventional encoder employs a complex technique for mode decision based on a rate-distortion (RD) cost of all possible modes. Hence, for the purpose of selecting the best block mode with the minimum RD cost, the conventional procedure requires much computational burden and a very complex encoding structure. In order to reduce the complexity, we propose a fast algorithm for mode decision based on Kalman filtering to estimate RD cost of a specific block mode. Furthermore, we propose an optimized structure of H.264/AVC encoder to implement the proposed algorithm. Without considerable performance degradation, using SIMD technology, the computer simulation shows that the proposed methods are dramatically faster than the original JM 9.6 encoder.	algorithm;algorithmic efficiency;block-oriented terminal;computation;computer simulation;direct mode;discrete cosine transform;distortion;elegant degradation;encoder;fast fourier transform;filter bank;h.264/mpeg-4 avc;kalman filter;macroblock;mathematical optimization;motion compensation;rate–distortion theory;ruby document format;simd	Jinwuk Seok;Jeong-Woo Lee;Chang-Sik Cho	2007	Multimedia Systems	10.1007/s00530-007-0100-2	computer simulation;kalman filter;computer vision;real-time computing;simulation;computer science;machine learning;signal processing	Robotics	46.70473587748974	-15.23786544652904	153907
8e9dc796768de99fbe44bbcbf5ea344aa98ec87e	compression of multispectral images by spectral classification and transform coding	rate distortion;karhunen loeve transformation;image coding;image segmentation;image processing;data compression;spectral response;thematic mapper;procesamiento imagen;image classification;segmentation;transform coding;indexing terms;classification;traitement image;discrete cosine transform;rate distortion theory;codificacion;cuantificacion vectorial;karhunen loeve transforms;vector quantization;geophysical signal processing;teledeteccion multiespectral;entropy codes;remote sensing;multispectral images;coding;multispectral remote sensing;six band thematic mapper image multispectral images spectral classification transform coding compression segmentation land cover linear dependency statistical redundancy rate distortion performance coding strategy vector quantization spectral response vector minimum distortion encoding classification map entropy encode residual vectors karhunen loeve transform spectral domain discrete cosine transform spatial domain;vector quantizer;entropy codes transform coding karhunen loeve transforms remote sensing geophysical signal processing image coding vector quantisation image segmentation image classification rate distortion theory;compresion dato;image coding multispectral imaging pixel transform coding image segmentation statistics redundancy rate distortion encoding entropy;numerical experiment;transformation karhunen loeve;vector quantisation;teledetection multispectrale;side information;transformacion karhunen loeve;karhunen loeve transform;land cover;clasificacion;segmentacion;compression donnee;codage;quantification vectorielle	This paper presents a new technique for the compression of multispectral images, which relies on the segmentation of the image into regions of approximately homogeneous land cover. The rationale behind this approach is that, within regions of the same land cover, the pixels have stationary statistics and are characterized by mostly linear dependency, contrary to what usually happens for unsegmented images. Therefore, by applying conventional transform coding techniques to homogeneous groups of pixels, the proposed algorithm is able to effectively exploit the statistical redundancy of the image, thereby improving the rate distortion performance. The proposed coding strategy consists of three main steps. First, each pixel is classified by vector quantizing its spectral response vector, so that both a reliable classification and a minimum distortion encoding of each vector are obtained. Then, the classification map is entropy encoded and sent as side information, Finally, the residual vectors are grouped according to their classes and undergo Karhunen-Loeve transforming in the spectral domain and discrete cosine transforming in the spatial domain. Numerical experiments on a six-band thematic mapper image show that the proposed technique outperforms the conventional transform coding technique by 1 to 2 dB at all rates of interest.		Giacinto Gelli;Giovanni Poggi	1999	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.753736	data compression;multispectral image;computer vision;contextual image classification;transform coding;speech recognition;index term;rate–distortion theory;image processing;biological classification;computer science;discrete cosine transform;pattern recognition;mathematics;image segmentation;coding;segmentation;karhunen–loève theorem;vector quantization;statistics	Vision	44.21202505822173	-14.75655007687444	154009
c3f646596555690cd935231ac12bcd38962a8f50	a new low-complexity error concealment method for stereo video communication	whole frame loss;error concealment;stereo video	The emerging popularity of three dimensional content has instigated the advances of stereo video coding and transmission techniques. According to the existing video compression standards, highly compressed stereo video bit streams are susceptible to transmission errors. As a consequence of the unavoidable spatial, temporal, and inter-view error propagation, the display quality is severely degraded at the receiver side. In this paper, to deal with the whole-frame loss of the right view in H.264-compressed stereo video bitstream transmission, a highly efficient perceptual whole-frame loss of right view error concealment algorithm is proposed with a fast error concealment strategy. Firstly, a fast error concealment strategy is presented, and an efficient image quality Index, gradient magnitude similarity deviation GMSD is extended to the concepts of temporal GMSD TGMSD and inter-view GMSD VGMSD, respectively, to evaluate the perceptual quality of stereo image. Then, according to the temporal correlation of video sequences, macroblock MB prediction modes of the previous right-view frame are used as the MB prediction mode of the lost frame. Then, MBs in the previous frame of the lost frame are also matched, in both temporal and inter-view domains, to obtain the pixel-based TGMSD and VGMSD maps. Finally, for each MB in the lost right-view frame, its TGMSD and VGMSD values are calculated and compared to obtain the MB prediction mode, after which either motion compensation or disparity compensation can be quickly decided and used for resilience of the lost right-view frame. Experimental results show that compared with traditional error concealment algorithms, the proposed algorithm has superior subjective and objective qualities, and compared with the error concealment algorithms based on structure similarity, its error concealment time is reduced by about 40i¾ź% with almost same perceptual quality.	error concealment	Kesen Yan;Mei Yu;Zongju Peng;Feng Shao;Gangyi Jiang	2015		10.1007/978-3-319-24078-7_60	computer vision;computer science;video quality;multimedia;computer graphics (images)	Vision	45.49190482029383	-17.910551322538424	154329
434e3b6b7b9b7266e61d42174f595d167073e8ef	a watermarking with two signatures	image coding;signal detection;copyright;image coding signatures image watermarking source image watermark extraction methods frequency domain rightful ownership ibm attack robustness copyright invertible watermark noninvertible watermark;source coding image coding copyright security of data signal detection feature extraction;feature extraction;watermarking frequency domain analysis data mining robustness fourier transforms transform coding protection image coding national electric code writing;frequency domain;security of data;extraction method;source coding	A watermarking scheme is presented which embeds two different watermarks to the same frequency and extracts the marks with two different methods in the frequency domain. One of two extraction methods needs source image, whereas the other doesn’t. Each extraction method, of course, uses unique operation in extracting the watermark. Use of two watermark schemes is more effective to claim rightful ownership. Furthermore, this watermarking is non-invertible and is robust against IBM attack as it obtains two different extraction results from the same image.	antivirus software;digital watermarking;watermark (data file)	Ju Han Kim;Won Don Lee;Jin Hyeong Park	1998		10.1109/MMSP.1998.738968	computer vision;feature extraction;computer science;machine learning;internet privacy;watermark;computer security;frequency domain;detection theory;source code	EDA	41.03513604631718	-10.5991519204242	154423
e2acce1b5a028d540e4968c9efe3315ad557f0c9	low-power motion vector estimation using iterative search block-matching methods and a high-speed non-destructive cmos image sensor	iterative method;metodo adaptativo;estimation mouvement;detecteur image;data compression;motion compensation;image sequences cmos image sensors motion estimation data compression video coding computational complexity image matching iterative methods;video signal processing;estimation method;image matching;estimacion movimiento;motion estimation iterative methods cmos image sensors computational complexity cmos technology video compression pixel charge coupled image sensors charge transfer active noise reduction;active pixel sensor;video compression;compresion senal;low power video compression;motion estimation;charge transfer;methode adaptative;indexing terms;metodo no destructivo;bidirectional multiple charge transfer active pixel;compression signal;chip;cmos image sensors;metodo iterativo;adaptive iterative search block matching;iterative methods;compensation mouvement;video coding;non destructive method;high speed nondestructive imaging;cmos image sensor;low power;codage video;computational complexity;methode iterative;motion vector;adaptive method;signal compression;puissance faible;traitement signal video;correspondencia bloque;methode non destructive;block matching;0 35 micron low power motion vector estimation high speed image sensor nondestructive cmos image sensor low power video compression high speed intermediate pictures adaptive iterative search block matching signal accumulation time image sequence active pixel sensor bidirectional multiple charge transfer nondestructive intermediate imaging reduced fixed pattern noise image sensor chip cmos technology computational complexity reduction data loading rate 3 3 v;detector imagen;fixed pattern noise;correspondance bloc;motion vector estimation;full search block matching;high speed;image sensor;image sequences;potencia debil	In this paper, motion vector (MV) estimation methods with high-speed intermediate pictures for low-power video compression are proposed. The intermediate pictures are obtained by a special type of CMOS image sensor. An adaptive iterative-search block matching is proposed to obtain precise MVs of video-rate pictures from high-speed intermediate pictures with the reduced computational complexity. The sensor captures high-speed intermediate pictures without destructing signal charge and video-rate pictures with full signal accumulation time. The proposed active pixel sensor using bidirectional multiple charge transfer is useful for the nondestructive intermediate imaging with a reduced fixed pattern noise. The image sensor chip has been implemented by using 0.35-/spl mu/m CMOS technology. It operates with 3.3 V and captures 480 frames/s high-speed nondestructive intermediate pictures and 30 frame/s fully accumulated video-rate pictures. The proposed adaptive iterative-search block matching has a comparable precision to a full search block matching with reduction of computational complexity by a factor of about 1/13, on average. It also reduces the data-loading rate from the memory by a factor of about 1/4.		Dwi Handoko;Shoji Kawahito;Yoshiaki Tadokoro;Akira Matsuzawa	2002	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2002.806809	data compression;computer vision;computer science;theoretical computer science;image sensor;mathematics;iterative method;algorithm	EDA	46.74685416809227	-15.651020087092233	154686
4c577cc691031162df41d6b702f279410f662223	caption processing for mpeg video in mc-dct compressed domain	mpeg video;mc dct domain;image quality;caption processing;approximation scheme;mpeg editing;compressed video	The (cinema) caption processing that adds descriptive texts on the sequence of frames is an important video manipulation function that video editor should support. This paper proposes an efficient MC-DCT compressed domain approach to insert the caption into the MPEG-compressed video stream. It basically adds the DCT blocks of the caption image to the corresponding DCT blocks of the input frames one by one in MC-DCT domain as in [5]. However, the strength of the caption image is adjusted in the DCT domain to prevent the resulting DCT coefficients from exceeding the maximum value that is allowed in MPEG. In order to adjust the strength of caption image adaptively, we should know the exact pixel values of input image that is a difficult task in DCT domain. We propose an approximation scheme for the pixel values in which the DC value of a block is used as the expected pixel value for all pixels in that block. Although this approximation may lead some errors in the caption area, it still provides a relatively high image quality in the non-caption area, while the processing time is about 4.9 times faster than the decode-captioning-reencode approach.	approximation;cinema 4d;coefficient;data compression;discrete cosine transform;image quality;moving picture experts group;pixel;streaming media	Jongho Nang;Ohyeong Kwon;Seungwook Hong	2000		10.1145/354384.354479	image quality;computer vision;computer science;multimedia;computer graphics (images)	Vision	41.745449074446604	-17.526612221169415	154694
771e3222c8aa9cb6e50ea85d30b5e270ac1aff8f	a scalable depth coding with arc breakpoints based synthesis in 3-d video	edge detection;code standards;depth map edge representation scalable depth coding arc breakpoints based synthesis 3d video depth image based rendering video coding standard depth discontinuity view quality synthesis depth map compression upsampling method depth map reconstruction color video guidance video decoder;video coding;encoding image edge detection decoding video coding image reconstruction image color analysis image coding;image colour analysis;image representation;image reconstruction;arc breakpoints depth coding depth down upsampling curve completion;rendering computer graphics;video coding code standards edge detection image colour analysis image reconstruction image representation rendering computer graphics	Depth map represents three-dimensional information and is used for depth image-based rendering to support 3-D video applications. Conventional video coding standards may cause serious coding artifacts along the depth discontinuities, which ultimately affect the synthesized view quality. This paper proposes an efficient technique to compress the depth map by encoding a reduced resolution of depth map and using special upsampling method to reconstruct the original depth map. Unlike previous upsampling schemes with the guidance of corresponding color video, the proposed method upsamples the depth map using the breakpoints which are generated at the decoder side with only a few breakpoints. Instead of the edge representation of the depth map, the breakpoints would substantially take less bits and low errors with the regularity of the contour. Experimental results show that the proposed technique significantly reduces bit rate while achieving a better quality of the synthesized view in terms of subjective and objective measures.	3d film;breakpoint;data compression;depth map;scalability;upsampling;video coding format	Xiaopeng Zhang;Junni Zou;Xiao Gu;Hongkai Xiong	2013	2013 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2013.6621726	computer vision;computer science;multimedia;video post-processing;multiview video coding;computer graphics (images)	Vision	44.13708942344353	-18.778594192718742	154794
28af17091c09f38dfa5d09cefd50ec6e46b4415a	lossless medical image compression using geometry-adaptive partitioning and least square-based prediction	medical image;lossless compression;geometry-adaptive partitioning;adaptive block-based segmentation;least square-based prediction	To improve the compression rates for lossless compression of medical images, an efficient algorithm, based on irregular segmentation and region-based prediction, is proposed in this paper. Considering that the first step of a region-based compression algorithm is segmentation, this paper proposes a hybrid method by combining geometry-adaptive partitioning and quadtree partitioning to achieve adaptive irregular segmentation for medical images. Then, least square (LS)-based predictors are adaptively designed for each region (regular subblock or irregular subregion). The proposed adaptive algorithm not only exploits spatial correlation between pixels but it utilizes local structure similarity, resulting in efficient compression performance. Experimental results show that the average compression performance of the proposed algorithm is 10.48, 4.86, 3.58, and 0.10% better than that of JPEG 2000, CALIC, EDP, and JPEG-LS, respectively. Graphical abstractᅟ	adaptive algorithm;binary space partitioning;branch predictor;calic;electronic data processing;image compression;image segmentation;jpeg 2000;least squares;least-squares analysis;lossless compression;medical image;natural science disciplines;pixel;quadtree;biologic segmentation	Xiaoying Song;Qijun Huang;Sheng Chang;Jin He;Hao Wang	2017	Medical & Biological Engineering & Computing	10.1007/s11517-017-1741-8	image compression;adaptive algorithm;lossy compression;computer vision;artificial intelligence;data compression;lossless jpeg;mathematics;data compression ratio;jpeg 2000;lossless compression	Vision	44.8595552517728	-14.66330577627861	155085
0f92e79b0e6404fa17b401189001f8295fd222c1	switched scalar quantizers for hidden markov sources	weighted squared error distortion;quantization;optimisation;weighted squared error distortion source coding hidden markov sources switched scalar quantizers nonlinear optimization problem cost function next quantizer map stochastic map gradient based optimization decision rule;cuantificacion;chaine markov;cadena markov;aerospace engineering;modelo markov;cost function;markov source;switched scalar quantizers;stochastic map;quantification;design optimization;system performance;algorithme;algorithm;nonlinear distortion;computational modeling;markov model;optimisation encoding information theory markov processes;scalar quantization;hidden markov models;nonlinear optimization problem;stochastic processes;quantizer;probability distribution;quantificateur;technical report;markov processes;theorie information;modele markov;stochastic systems;switches;cuantificador;nonlinear optimization;encoding;next quantizer map;algorithm design and analysis;hidden markov sources;decision rule;information theory;hidden markov models cost function algorithm design and analysis design optimization steady state stochastic processes nonlinear distortion computational modeling stochastic systems;source coding;gradient based optimization;steady state;algoritmo;markov chain;teoria informacion	An algorithm for designing switched scalar quantizers for hidden Markov sources is described. The design problem is cast as a nonlinear optimization problem. The optimization variables are the thresholds and reproduction levels for each quantizer and the parameters defining the next-quantizer map. The cost function is the average distribution incurred by the system in steady-state. The next-quantizer map is treated as a stochastic map so that all of the optimization variables are continuous-valued, allowing the use of a gradient-based optimization procedure. This approach solves a major problem in the design of switched scalar quantizing systems, namely, that of determining an optimal next-quantizer decision rule. Details are given for computing the cost function and its gradient for weighted-squared-error distortion. Simulation results which compare the new system to current systems show that the present system performs better. It is observed that the optimal system can in fact have a next-quantizer map with stochastic components. >	markov chain	David M. Goblirsch;Nariman Farvardin	1992	IEEE Trans. Information Theory	10.1109/18.149497	mathematical optimization;quantization;information theory;computer science;machine learning;mathematics;statistics	Theory	48.90374623792274	-12.23189999912567	155153
910dc8a516641d12b68f60cb8d3107fd3a563bbf	an efficient lossless image compression algorithm for external memory bandwidth saving	calic algorithm external memory bandwidth saving sdram memory access bandwidth high definition video coding image pixel decompression hardware oriented lossless image compression algorithm block random access patterns line random access patterns block level adaptive prediction pixel level adaptive prediction image spatial correlation rate optimized mode decision horizontal prediction direction vertical prediction direction prediction accuracy overhead cost control multiple range semifixed variable length coding residue coding huffman based vlc tables prediction mode syntax elements standard test images;image coding;data compression;prediction algorithms;image coding algorithm design and analysis prediction algorithms sdram bandwidth high definition video data compression;video coding dram chips;high definition video;bandwidth;algorithm design and analysis;sdram	Summary form only given. Huge SDRAM memory access bandwidth is the performance bottleneck for high definition video coding. Lossless image compression is efficient method to solve this problem. Image pixels are compressed before writing into SDRAM and decompressed after reading out from SDRAM. This work proposes a hardware-oriented lossless image compression algorithm, supporting block and line random access patterns flexibly. First, block or pixel level adaptive prediction is employed to utilize the image spatial correlation. Rate optimized mode decision is used to select the block or pixel prediction mode, and to determine the horizontal or vertical prediction direction, to achieves good tradeoff between prediction accuracy and control overhead cost. Second, multiple range semi-fixed variable length coding is employed for residue coding, and Huffman based VLC tables are designed to code the prediction mode syntax elements, as shown in Fig.1. The proposed algorithm and its competitors are tested with 35 standard test images with size of 960x576. The average bits of the proposed algorithm, algorithm [1], algorithm [2] and CALIC algorithm are respectively 3.79, 3.95, 4.19 and 3.92 bits per pixel. The results justify the effectiveness and its advantage of the proposed algorithm.	algorithm;color depth;data compression;huffman coding;image compression;lossless compression;memory bandwidth;overhead (computing);pixel;random access;semiconductor industry;standard test image;vlc media player;variable-length code	Hai Bing Yin;Hongqi Hu	2014	2014 Data Compression Conference	10.1109/DCC.2014.88	data compression;algorithm design;real-time computing;prediction;computer science;theoretical computer science;mathematics;context-adaptive binary arithmetic coding;memory bandwidth;bandwidth;statistics	ML	46.079585211387794	-20.24251942360527	155182
cc96c34aed1693e3f915c52b34698a0683f0bde3	improved hevc lossless compression using two-stage coding with sub-frame level optimal quantization values	two stage coding;hevc;adaptive quantization;lossless coding	Lossless video coding is used when perfect preservation of video data is required. In HEVC, lossless coding is accomplished by bypassing the transform and quantization stages. Prediction residuals are coded with the entropy coder in the spatial domain. In this paper, two-stage coding with sub-frame adaptive quantization is proposed. The DCT is firstly applied to the prediction residuals and the DCT coefficients are quantized. The quantized DCT coefficients and the quantization error are coded. Adaptive quantization parameters are used for each Coding Unit. Simulation results show that the proposed method significantly outperforms the HEVC lossless coding.	coefficient;data compression;discrete cosine transform;entropy encoding;high efficiency video coding;lossless compression;quantization (signal processing);simulation	Xun Cai;Qunshan Gu	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7026143	data compression;lossless jpeg;speech recognition;shannon–fano coding;quantization;trellis quantization;entropy encoding;theoretical computer science;context-adaptive variable-length coding;coding tree unit;mathematics;tunstall coding;adaptive coding;context-adaptive binary arithmetic coding;vector quantization	Robotics	44.823930984267484	-16.03437124151699	155232
d6507ecc9b34113f4038314162bd11bafbecf38f	a locally adaptive data compression strategy for chinese-english characters	algorithm performance;image processing;data compression;estudio comparativo;heuristic method;reseau ordinateur;teleinformatica;procesamiento imagen;gestion fichier;move to front;file management;chino;traitement image;computer network;algorithme;local adaptation;etude comparative;algorithm;teleinformatique;anglais;resultado algoritmo;manejo archivos;comparative study;performance algorithme;red ordenador;tratamiento caracter;english;compresion dato;chinois;chinese;ingles;remote data processing;traitement caractere;compression donnee;character processing;algoritmo	Abstract   A locally adaptive data compression strategy can be efficiently applied to a text data file when characters are used frequently over short intervals. Some data files, such as program files, image files, or exposition files of application software contain some commonly used sequences of characters which occur consecutively. We can use their common characteristics to develop a simple heuristic method of compression. Among all kinds of compression schemes based upon a locally adaptive data compression strategy, the MTF (Move-To-Front) method is a superior one that can be easily implemented and can obtain a higher compression rate for most data files.  This article uses the idea of a locally adaptive compression strategy and proposes a new data compression mechanism for Chinese and English data files. Some experimental results show that our proposed method can achieve a higher compression rate than using multigroup Huffman encoding, which is a method recently proposed by Chang and Tsai for Chinese data files.	data compression	C. Chin-Chen;W. Chih-Hung	1997	Journal of Systems and Software	10.1016/0164-1212(95)00200-6	data compression;simulation;image processing;image compression;computer science;artificial intelligence;english;lossless compression;chinese;algorithm	OS	44.920801543471484	-12.117249091119891	155533
e5b7e4fc1c9b51c114fc2c153006578176184eaa	ordered statistics decoding of linear block codes for robust h.263 video transmission in awgn channel	channel coding;artifact reduction;ordered statistics decoding;additive white gaussian noise;awgn channel;order statistic;error concealment;psnr;decoding;source decoder;spatial error propagation;visual communication;video compression;linear codes;soft decision decoding;awgn;residual channel coding errors ordered statistics decoding linear block codes robust h 263 video transmission awgn channel soft decision decoding additive white gaussian noise channel effective error concealment source decoder artifact reduction temporal error propagation spatial error propagation error prevention peak signal to noise ratio psnr gain;residual channel coding errors;statistics block codes robustness awgn maximum likelihood decoding psnr streaming media channel coding video compression forward error correction;video coding;error prevention;awgn channels;forward error correction;error propagation;streaming media;robust h 263 video transmission;maximum likelihood decoding;peak signal to noise ratio;video transmission;statistics;robustness;linear block codes;psnr gain;effective error concealment;block codes;forward error correction visual communication awgn channels block codes channel coding linear codes decoding video coding image sequences;additive white gaussian noise channel;temporal error propagation;image sequences	Soft-decision decoding of linear block codes for robust H.263 video transmission in a zero-mean, additive white Gaussian noise (AWGN) channel is investigated. We implement an effective error concealment (EC) scheme at the source decoder to reduce the annoying artifacts caused by decoding a corrupted bit stream. To alleviate the spatial and temporal error propagation, an error prevention (EP) strategy is introduced at the H.263 encoder. Simulation results show that a large portion of the peak signal-to-noise ratio (PSNR) gain is obtained by ordered statistics decoding of the received sequence. Furthermore, the residual channel coding errors are concealed and compensated by realizing the proposed EC and EP schemes.		Wu-Hsiang Jonas Chen;Jenq-Neng Hwang	1998		10.1109/MMSP.1998.739043	list decoding;additive white gaussian noise;speech recognition;peak signal-to-noise ratio;telecommunications;sequential decoding;computer science;statistics	Crypto	48.10038934660701	-15.696797108705622	156275
6ca598d948991774b2effcee5e245381d85830ec	the use of unmanned aerial vehicles and wireless sensor network in agricultural applications	agricultural engineering;control loops unmanned aerial vehicles wireless sensor network agricultural applications pesticides fertilizers crop yields aircrafts spraying crop damage climatic conditions wind direction wind intensity uav wsn waste minimization;crops;agriculture;wireless sensor networks agricultural engineering agriculture autonomous aerial vehicles crops fertilisers wind;wind;autonomous aerial vehicles;wireless sensor networks;chemicals routing protocols wireless sensor networks agriculture sensors spraying routing;fertilisers	The application of pesticides and fertilizers in agricultural areas is of prime importance for crop yields. The use of aircrafts is becoming increasingly common in carrying out this task mainly because of its speed and effectiveness in the spraying operation. However, some factors may reduce the yield, or even cause damage (e.g. crop areas not covered in the spraying process, overlapping spraying of crop areas, applying pesticides on the outer edge of the crop). Climatic conditions, such as the intensity and direction of the wind while spraying add further complexity to the control problem. In this paper, we describe an architecture based on unmanned aerial vehicles (UAVs) which can be employed to implement a control loop for agricultural applications where UAVs are responsible for spraying chemicals on crops. The process of applying the chemicals is controlled by means of the feedback obtained from the wireless sensor network (WSN) deployed on the crop field. The aim of this solution is to support short delays in the control loop so that the spraying UAV can process the information from the sensors. We evaluate an algorithm to adjust the UAV route under changes in wind intensity and direction. Moreover, we evaluate the impact of the number of communication messages between the UAV and the WSN. Results show that the adjustment of the route based on the feedback information from the sensors could minimize the waste of pesticides.	aerial photography;algorithm;control system;sensor web;television antenna;unmanned aerial vehicle	Fausto G. Costa;Jo Ueyama;Torsten Braun;Gustavo Pessin;Fernando Santos Osório;Patrícia Amâncio Vargas	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6352477	agriculture;computer science;physics;remote sensing;wind	Embedded	52.63235921791662	-10.609986125837485	156398
49b51af94c0e7254ce423069667e4d408f3c8170	variation of jpeg-ls to low cost rate control and its application in region-of-interest based coding	region of interest based coding;evaluation performance;rate distortion;image coding;information loss;performance evaluation;image processing;flow rate regulation;evaluacion prestacion;lossless image compression;procesamiento imagen;qualite image;traitement image;rate control;codage image;compression image;image compression;regulation debit;computational complexity;signal processing;region of interest;image quality;jpeg ls;joint photographic expert group;compression ratio;context dependent;lossless;near lossless image compression;calidad imagen;digital image;regulacion caudal;compresion imagen	JPEG-LS is the latest pixel based lossless (LS)/near-lossless (NLS) still image coding standard introduced by the joint photographic experts group (JPEG). Since the introduction of this standard, several improvements, variations and applications to the original algorithms have been proposed in literature (IEE Electron. Lett. 37 (22) (2002) 1327; Proc. of IASTED Int. Conf. on Visualization, Imaging and Image Processing, VCIP 2001, Marbella, Spain, 2001, p. 340; IEE Trans. Cons. Elec. 3 (3) (2001) 466; IEE Proc.: Vision, Image Signal Process. 147 (6) (2000)). While JPEG-LS introduces a constant information loss by using a single pre-determined compression parameter NEAR, previously, attempts have been made to redistribute the information loss by using a context dependent, three-parameter scheme, which is aimed at improving the subjective image quality of the reconstructed images. In this paper we extend this work to rate controlled image compression and region-of-interest (ROI) based coding, thereby extending JPEG-LS standard's use to a wider digital imaging application area. We show that the proposed rate-control scheme outperforms previously published work in applying JPEG-LS to low cost rate control, in terms of algorithmic and computational complexity, accuracy of achieving the target compression ratio, range of performance, subjective image quality and objective rate-distortion performance. Finally we propose the application of the above rate control scheme in ROI based coding.	jpeg;least squares	Eran A. Edirisinghe;Satish Bedi	2003	Sig. Proc.: Image Comm.	10.1016/S0923-5965(02)00140-6	computer vision;simulation;telecommunications;image processing;computer science;signal processing;lossless compression;algorithm	Robotics	46.698933578838606	-14.165992046891853	156542
2a233c90e706b996c040c6f4af8178dd51bb8f6d	frame based redundant-macro-block error resilient in scalable video coding	scalable video coding;base layer error resilient;video quality;frame based;visual quality;packet loss rate;hierarchical b picture;macro block;error resilience;base layer	In this paper, a redundant macro block coding with texturebased selective boundary matching algorithm (RMB-TSBMA) is introduced to improve the video quality. Different from texture-based selective boundary matching algorithm (TSBMA), the algorithm is more suitable for frame based recovery. The RMB coding encodes the information of some essential blocks. When a frame gets lost, we can first decode RMBs, and then neighboring MBs with TSBMA, so that the RMBs can provide the correct position for boundary matching to get better visual quality. The influences of the RMB number, and the reordering method are also discussed. Compared to the other algorithms, the proposed RMBTSBMA performs better than conventional method of temporal direct mode (TDM) by over 3dB PSNR at the packet loss rate 10%.	scalable video coding	Jiung-Liang Lin;Chih-Hung Kuo	2010		10.1007/978-3-642-15696-0_35	scalable video coding;residual frame;real-time computing;telecommunications;computer science;video quality;machine learning;coding tree unit;computer network	ML	45.908821714807	-17.951494076670315	156543
ae5c0cbb60f5e218d49f1ced26b07d9e97dfc2fa	image-adaptive watermarking using jseg segmentation technique	watermarking;image coding;image segmentation;watermarking adaptive codes discrete cosine transforms image coding image colour analysis image segmentation image texture;adaptive codes;image texture;discrete cosine transforms;image colour analysis;watermarking image segmentation image quality signal processing algorithms robustness image processing statistical distributions humans visual system protection;just noticeable distortion;dct image adaptive watermarking jseg segmentation technique spatial masking method color image texture estimation	In this paper a new spatial masking method for watermarking techniques is proposed. The mask is computed using a local and a global reference to image texture. JSEG segmentation algorithm offers an estimation of color texture and some of its parameters are used in our masking method. The proposed masking method is used in a DCT-based watermarking algorithm and results are compared with JND (Just Noticeable Distortion) masking method.	algorithm;automata theory;cellular automaton;digital watermarking;discrete cosine transform;distortion;genetic algorithm;image processing;image texture;pixel;point of view (computer hardware company);the mask;watermark (data file)	Monica Radulescu;Felicia Ionescu	2007	Ninth International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC 2007)	10.1109/SYNASC.2007.7	image texture;image restoration;computer vision;feature detection;binary image;image processing;digital watermarking;computer science;segmentation-based object categorization;digital image processing;pattern recognition;region growing;image segmentation;scale-space segmentation;computer graphics (images)	EDA	41.65208039333652	-11.152807955382547	156875
b60b21355cf1d6d391933d5a74074131777ccdc1	low-complexity block size decision for hevc intra coding using binary image feature descriptors	intra coding hevc h 265 high efficiency video coding coding unit brief binary feature descriptors image descriptors low complexity block size decision;encoding training bit rate complexity theory image coding software standards;offline training approach low complexity block size decision hevc intracoding binary image feature descriptors feature based block size decision coding unit binary robust independent elementary feature descriptor brief descriptor pixel values adaptive training approach;video coding computational complexity video codecs	We present a novel low-complexity algorithm for feature-based block size decision in HEVC intra coding. Our approach evaluates a set of point pairs within a coding unit (CU) in order to determine whether a CU should be further split into smaller sub CUs for coding. We apply a modified version of the Binary Robust Independent Elementary Feature (BRIEF) descriptor, which in its original version is usually applied to describe local image properties in the context of image/video analysis. While the elements of the original BRIEF descriptor describe which pixel of a pixel pair has the higher value, the modified descriptor evaluates whether the difference between the pixel values is above a pre-defined threshold, which is determined by training. If a certain number of pixel pair differences exceed their corresponding threshold the CU is split into its four sub CUs. Furthermore, we restrict the feature point pairs to be located within different potential sub CUs. For an adaptive training approach, we achieved in our experiments an average encoding time reduction of 58% compared to the HEVC reference software HM12.0 with an average rate increase of 3.8% at equal quality and an average encoding time reduction of 65% with an average rate increase of 5.98% at equal quality for an offline training approach.	algorithm;binary image;block size (cryptography);experiment;feature (computer vision);high efficiency video coding;intra-frame coding;online and offline;pixel;tip (unix utility);video content analysis	Walther Geuder;Peter Amon;Eckehard G. Steinbach	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7350796	computer vision;computer science;theoretical computer science;coding tree unit;pattern recognition;context-adaptive binary arithmetic coding	Vision	46.883188996043174	-19.87992349986149	157103
7d8a1fd4952cc8f0044ce51e11bf6ffc8c037e6c	fpga-based hardware cnc interpolator of bezier, splines, b-splines and nurbs curves for industrial applications	fpga;cad cam;tool path interpolator;cnc	Tool path interpolation is an important part of Computerized Numerical Control (CNC) systems because it is related to the machining accuracy, tool-motion smoothness and overall efficiency. The use of parametric curves to generate tool-motion trajectories on a workpiece for high accuracy machining has become a standard data format that is used for CAD/CAM (Computer Aided Design/Computer Aided Manufacturing) and CNC systems. Splines, Bezier, B-splines, and NURBS (Non-Uniform Rational B-splines) curves are the common parametric technique used for tool path design. However, the reported works bring out the high computational load required for this type of interpolation, and then at best only one interpolation algorithm is implemented. The contribution of this paper is the development of a hardware processing unit based on Field Programmable Gate Arrays (FPGA) for industrial CNC machines, which is capable of implementing the four main interpolation techniques. It allows the selection of the required interpolation technique according the application. Two CAD models are designed for test the CNC interpolations; experimental results show the efficiency of the proposed methodology.		J. Jesus de Santiago-Perez;Roque Alfredo Osornio-Rios;René de Jesús Romero-Troncoso;Luis Morales-Velazquez	2013	Computers & Industrial Engineering	10.1016/j.cie.2013.08.024	embedded system;real-time computing;computer science;numerical control;engineering drawing;field-programmable gate array;computer-aided technologies	Robotics	53.62716079203398	-16.38285933282608	157476
26f362c1e2cfc8b99c10d36584a17a9ab91b74aa	mpeg-4 video verification model: status and directions	scalability;mpeg 4;coding efficiency;standard	This article describes the current status and future direcCoding Experts Group. It was established in January 1988. It is tions of the emerging ISO MPEG-4 audiovisual coding standard. The a part of the International Standardization Organization (ISO). article first presents an overview of the different aspects of the stanMPEG’s mandate is to develop standards for coded representation dard and then focuses on the video coding aspects. The current of video and audio data. It operates in the framework of the status of the Video Verification Model (VM) (a completely defined Joint ISO/IEC Technical Committee (JTC 1) on Information encoder and decoder specification) is described in detail and its perTechnology and is formally Working Group 11 (WG11) of Subformance is presented. The new functionalities supported by this committee 29 (SC29). MPEG-4 is the third in line of audiovisual emerging standard and their potential applications are highlighted. coding standards from MPEG. The two prior ones were MPEGq 1997 John Wiley & Sons, Inc. Int J Imaging Syst Technol, 8, 468–479, 1 and MPEG-2. MPEG-1 was mainly targeted at interactive multi1997 media application on CD ROM, and MPEG-2 addressed broad	cd-rom;data compression;encoder;john d. wiley;mpeg-1;mpeg-2;moving picture experts group	Iole Moccagatta;Raj Talluri	1997	Int. J. Imaging Systems and Technology	10.1002/(SICI)1098-1098(1997)8:5%3C468::AID-IMA9%3E3.0.CO;2-8	scalability;telecommunications;computer science;multimedia;algorithmic efficiency;mpeg-4;h.261;computer graphics (images)	HCI	42.725620206914925	-20.046851568125465	157522
8e486ce1b9e6862b6b623dbf5d02424e65e779e0	a novel method on optimal bit allocation at lcu level for rate control in hevc	video coding convergence optimisation;standards;approximation error;bit rate distortion taylor series approximation error encoding standards closed form solutions;high efficiency video coding hevc rate control lcu level optimal bit allocation recursive taylor expansion method rte method r λ rate control scheme convergence speed encoding complexity cost bit rate control error;taylor expansion;bit rate;hevc;closed form solutions;rate control;distortion;encoding;taylor expansion hevc rate control;taylor series	In this paper, we propose a new method, namely recursive Taylor expansion (RTE) method, for optimally allocating bits to each LCU in the R-λ rate control scheme for HEVC. Specifically, we first set up an optimization formulation on optimal bit allocation. Unfortunately, it is intractable to achieve a closed-form solution for this formulation. We therefore propose a RTE solution to iteratively solve the formulation with a fast convergence speed. Then, an approximate closed-form solution can be obtained. This way, the optimal bit allocation can be achieved at little encoding complexity cost. Finally, the experimental results validate the effectiveness of our method in three aspects: compressed distortion, bit-rate control error, and bit fluctuation.	approximation algorithm;distortion;high efficiency video coding;lookahead carry unit;mathematical optimization;quantum fluctuation;recursion	Shengxi Li;Mai Xu;Zulin Wang	2015	2015 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2015.7177445	mathematical optimization;real-time computing;telecommunications;computer science;taylor series;theoretical computer science	Robotics	47.688063721519356	-17.868408277445457	157525
545bdcc53cdba8ec1548b2e8c01fd3fd40a87ad9	dictionary learning-based distributed compressive video sensing	samples extraction;distributed compressive video sensing;compressed sensing;image coding;decoding;sensors;basis based representation;l 1 minimization compressive sensing sparse representation dictionary learning single pixel camera;training;niobium;data embedding;video compression;low complexity;motion estimation;video coding dictionaries feature extraction image reconstruction image representation learning artificial intelligence motion estimation;video coding;dcvs;image representation;compressive sensing;feature extraction;image reconstruction;l 1 minimization;dictionaries;distributed video coding;dictionary learning;video reconstruction;basis based representation dictionary learning distributed video coding video compression motion estimation distributed compressive video sensing camera architecture dcvs video reconstruction samples extraction;learning artificial intelligence;sparse representation;compressed video;single pixel camera;camera architecture;dictionaries image reconstruction sensors decoding training niobium image coding	We address an important issue of fully low-cost and low-complex video compression for use in resource-extremely limited sensors/devices. Conventional motion estimation-based video compression or distributed video coding (DVC) techniques all rely on the high-cost mechanism, namely, sensing/sampling and compression are disjointedly performed, resulting in unnecessary consumption of resources. That is, most acquired raw video data will be discarded in the (possibly) complex compression stage. In this paper, we propose a dictionary learning-based distributed compressive video sensing (DCVS) framework to “directly” acquire compressed video data. Embedded in the compressive sensing (CS)-based single-pixel camera architecture, DCVS can compressively sense each video frame in a distributed manner. At DCVS decoder, video reconstruction can be formulated as an l1-minimization problem via solving the sparse coefficients with respect to some basis functions. We investigate adaptive dictionary/basis learning for each frame based on the training samples extracted from previous reconstructed neighboring frames and argue that much better basis can be obtained to represent the frame, compared to fixed basis-based representation and recent popular “CS-based DVC” approaches without relying on dictionary learning.	basis function;coefficient;compressed sensing;data compression;dictionary;distributed concurrent versions system;machine learning;motion estimation;pixel;sampling (signal processing);sensor;sparse matrix;uncompressed video	Hung-Wei Chen;Li-Wei Kang;Chun-Shien Lu	2010	28th Picture Coding Symposium	10.1109/PCS.2010.5702466	video compression picture types;reference frame;computer vision;computer science;machine learning;video tracking;pattern recognition;block-matching algorithm;compressed sensing;motion compensation;multiview video coding	Vision	49.397334741565125	-17.48255467857577	157602
720cd93fa7e1176ff7b3f1276343ee3fb6f260bd	novel 3d-wpp algorithms for parallel hevc encoding	wavefront parallel processing;video coding parallel processing prediction theory;hevc;dynamic 3d wpp algorithm wavefront parallel processing high efficiency video coding parallel hevc encoding hevc standard spatial prediction temporal prediction rate distortion performance rd performance interframe wpp methods static 3d wpp algorithm bd rate loss;wavefront parallel processing hevc 3d wpp;3d wpp;parallel processing encoding heuristic algorithms delays scalability prediction algorithms decoding	Although wavefront parallel processing (WPP) proposed in the HEVC standard and various inter frame WPP algorithms can achieve comparatively high parallelism, their scalability for its parallelism is still very limited due to various dependencies introduced in spatial and temporal prediction in HEVC. In this paper, we propose three types of 3 Dimensional WPP (3D-WPP) algorithms that can significantly improve the parallelism, while achieving good tradeoffs between implementation complexity, determinism, and rate-distortion (RD) performance. Experimental results show that the proposed algorithms can lead to up to 2.8 × speed up compared with existing inter frame WPP methods. While the Simple 3D-WPP and Static 3D-, WPP algorithm may introduce an BD rate loss between 0 to 4.9% as compared with existing algorithms, the more complex Dynamic 3D-WPP algorithm achieves better parallelism with virtually no coding performance loss.	algorithm;blu-ray;distortion;high efficiency video coding;parallel computing;rate–distortion theory;ruby document format;scalability;web hosting service	Ziyu Wen;Bichuan Quo;Jiashuo Liu;Jisheng Li;Yao Lu;Jiangtao Wen	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7471921	parallel computing;real-time computing;computer science;theoretical computer science	Robotics	47.65856873065201	-18.343671675185913	157615
ac76a80d0a4afb7cddcebc03868522d92622c373	reachability-based synthesis of feedback policies for motion planning under bounded disturbances	uncertain systems;motion control;high level specifications;differential game framework;path planning;moving ground vehicle tracking reachability based synthesis feedback policy bounded disturbances robot motion control model uncertainty environment disturbances robust motion control strategy high level specifications target attainability continuous disturbances motion planning task infinite horizon invariance iterative reachability calculations differential game framework state measurements feedback control policy autonomous helicopter;continuous disturbances;robot motion control;mobile robots;robust control;feedback policy;satisfiability;infinite horizon invariance;target attainability;state measurements;invariance;autonomous helicopter;iterative methods;differential game;feedback;bounded disturbances;trajectory;iterative reachability calculations;model uncertainty;games;robots;uncertain systems differential games feedback infinite horizon invariance iterative methods mobile robots motion control path planning reachability analysis robust control;differential games;land vehicles;robust motion control strategy;motion planning;finite horizon;planning;infinite horizon;environment disturbances;feedback control policy;motion planning task;trajectory land vehicles planning robots games vehicle dynamics;feedback control;reachability analysis;vehicle dynamics;reachability based synthesis;moving ground vehicle tracking	The task of planning and controlling robot motion in practical applications is often complicated by the effects of model uncertainties and environment disturbances. We present in this paper a systematic approach for generating robust motion control strategies to satisfy high level specifications of safety, target attainability, and invariance, under unknown but bounded, continuous disturbances. The motion planning task is decomposed into the two sub-problems of finite horizon reach with avoid and infinite horizon invariance. The set of states for which each of the sub-problems is robustly feasible is computed via iterative reachability calculations under a differential game framework. We discuss how the results of this computation can be used to inform selections of control inputs based upon state measurements at run-time and provide an algorithm for implementing the corresponding feedback control policies. Finally, we demonstrate an experimental application of this method to the control of an autonomous helicopter in tracking a moving ground vehicle.	algorithm;autonomous robot;computation;feedback;high-level programming language;iterative method;motion planning;reachability	Jerry Ding;Eugene Li;Haomiao Huang;Claire J. Tomlin	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5980268	control engineering;simulation;computer science;artificial intelligence;control theory;feedback;mathematics;motion planning	Robotics	52.98132588448589	-21.491758785490497	158025
76a691be031bddb417924cbb699542bea2723bd5	reversible data embedding technique for palette images using de-clustering	data hiding;color palette image;declustering reversible;data embedding;audio video;steganography;industrial application;medical treatment;color image	In this paper, a reversible data embedding method for color images is proposed. The technique of data embedding is used to hide the secret data into multimedia such as text content, audio, video, and images. In some applications, the reversibility of data embedding is an important requirement, as in distance medical treatment and military industrial applications. Many researchers have proposed reversible techniques that work on grayscale images, but these methods cannot be applied directly to color images, which in recent years have increased in popularity and have more redundant space available for embedding the secret data. Moreover, the size of color image can be reduced by being sorted storing in the palette-based format. Some researchers have proposed data embedding techniques for palette-based images. While their methods successfully achieve the purpose of secret data delivery, they do so without reversibility. In this paper, we propose a data embedding method in palette-based images with reversibility. The simulation result shows that the proposed method embeds 255Kb into a palette image sized 512×512 pixels and that the restored image and the original image are the same.	cluster analysis	Chin-Chen Chang;Yi-Hui Chen;Yung-Chen Chou	2007		10.1007/978-3-540-73417-8_20	computer vision;color image;computer science;multimedia;color cycling;steganography;information hiding;statistics;computer graphics (images)	Vision	40.44316527827245	-12.980532601787353	158124
43009d3ccce2c1c755f88b4e48fb56cc3df9f3ac	multistage sdtv/hdtv scanning rate converters	hdtv displays decoding signal processing frequency domain analysis tv receivers visualization video coding nonlinear filters energy resolution;energy resolution;nonlinear filters;convertisseur;tv receivers;multistage;decoding;frequency domain analysis;multistage filters;image frequency;multietage;experimental result;television equipment;filter design;video coding;visualization;codificacion;signal processing;displays;spatio temporal domain;poliescalonado;coding;hdtv;resultado experimental;pyramidal coding;frequence image;television haute resolution;television alta definicion;frequency domain;resultat experimental;television equipment high definition television video coding;frecuencia imagen;convertidor;high definition television;codage;converter;multistage sdtv hdtv scanning rate converters;spatio temporal domain multistage sdtv hdtv scanning rate converters pyramidal coding multistage filters filter design frequency domain	AbstructSDTV/HDTV conversion has several applications of current interest, such as pyramidal coding for simultaneous SDTV and HDTV output, HDTV display of SDTV signals, and SDTV display of HDTV signals. The effectiveness of multistage filters in this application is established. The design of the filters to use in these configurations is a delicate issue, since it has to balance several, and sometimes conflicting, requirements. The design objectives depend on the specific application, and are ultimately targeted to the visual rendition of the system. These objectives do not fit traditional design specs and call for provisions of visual significance in both the frequency domain and the spatio-temporal domain. This work reports on the findings of some systematic research on this field.	holomatix rendition;multistage amplifier;requirement;standard-definition television	Guido M. Cortelazzo;Gian Antonio Mian;Stefano Verri	1995	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.465082	computer vision;telecommunications;computer science;signal processing;multimedia;frequency domain	Visualization	48.44176150193219	-13.933744588241492	158180
a4e349ada415a6b0fd4af33a9cfe0a0ee35327bc	a sequence-based rate control framework for consistent quality real-time video	videocommunication;data transmission;bit rate layout video sequences parameter estimation mpeg 4 standard quantization quadratic programming motion control centralized control code standards;videocomunicacion;stationarity assumption bit allocation model parameter estimation rate control real time encoding smooth quality;flow rate regulation;real time;telecommunication control;look ahead;video quality;indexing terms;stationarity assumption;asignacion bit;allocation bit;quantisation signal;rate control;video coding;codage video;regulation debit;quantisation signal image sequences video coding telecommunication control parameter estimation;peak signal to noise ratio;transmission donnee;temps reel;estimacion parametro;quantization parameter;tiempo real;bit allocation;smooth quality;parameter estimation;estimation parametre;model parameter estimation;requantization sequence based rate control framework real time video video sequence rate complexity model nonlinear model parameter estimation approach quantization parameter video quality;transmision datos;regulacion caudal;nonlinear model;image sequences;real time encoding	Most model-based rate control solutions have the generally questionable assumption that video sequence is stationary. In addition, they often suffer from the fundamental problem of model parameter misestimation. In this paper, we propose a sequence-based frame-level bit allocation framework employing a rate-complexity model that has the capability of tracking the nonstationary characteristics in the video source without look-ahead encoding. In addition, a new nonlinear model parameter estimation approach is proposed to overcome the existing problems in previous model parameter estimation schemes where quantization parameter (QP) is determined to achieve the allocated bits for a frame. Furthermore, a general concept of bit allocation guarantee is discussed and its importance is highlighted. The proposed rate control solution can achieve smoother video quality with less quality flicker and motion jerkiness. Both a complete solution where requantization is employed to guarantee the achievement of the allocated bits, and a simplified solution without requantization are studied. Experimental results show that they both provide significantly better performance, in terms of average peak-signal-to-noise ratio and quality smoothness, than the MPEG-4 Annex L frame-level rate control solution.	coherence (physics);estimation theory;experiment;extensibility;flicker (screen);group of pictures;nonlinear system;peak signal-to-noise ratio;real-time clock;real-time locating system;signal-to-noise ratio;stationary process;video buffering verifier	Bo Xie;Wenjun Zeng	2006	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2005.856911	real-time computing;simulation;index term;peak signal-to-noise ratio;computer science;video quality;estimation theory;statistics;data transmission	Vision	47.655892048877874	-17.484313097425506	158200
866076a2938e9665625fcc197addbe4e6aeffffa	lossless data embedding using generalized statistical quantity histogram	databases;histograms;watermarking;image coding;information transmission generalized statistical quantity histogram histogram based lossless data embedding copyright protection multimedia protection lde method statistical quantity histogram based embedding arithmetic average image diversity image texture gsqh driven lde framework divide and conquer strategy flat histogram distribution jpeg compression;computer science and information systems;data compression;data embedding;copyright;biomedical imaging;video and image watermarking generalized statistical quantity histogram lossless data embedding reversibility;indexing terms;dh hemts;journal article;statistical analysis copyright data compression image coding image texture multimedia communication;image texture;copyright protection;receivers;statistical analysis;reversibility;multimedia communication;video and image watermarking;generalized statistical quantity histogram;image watermarking;side information;divide and conquer;lossless data embedding;histograms biomedical imaging image coding receivers watermarking databases dh hemts	Histogram-based lossless data embedding (LDE) has been recognized as an effective and efficient way for copyright protection of multimedia. Recently, a LDE method using the statistical quantity histogram has achieved good performance, which utilizes the similarity of the arithmetic average of difference histogram (AADH) to reduce the diversity of images and ensure the stable performance of LDE. However, this method is strongly dependent on some assumptions, which limits its applications in practice. In addition, the capacities of the images with the flat AADH, e.g., texture images, are a little bit low. For this purpose, we develop a novel framework for LDE by incorporating the merits from the generalized statistical quantity histogram (GSQH) and the histogram-based embedding. Algorithmically, we design the GSQH driven LDE framework carefully so that it: (1) utilizes the similarity and sparsity of GSQH to construct an efficient embedding carrier, leading to a general and stable framework; (2) is widely adaptable for different kinds of images, due to the usage of the divide-and-conquer strategy; (3) is scalable for different capacity requirements and avoids the capacity problems caused by the flat histogram distribution; (4) is conditionally robust against JPEG compression under a suitable scale factor; and (5) is secure for copyright protection because of the safe storage and transmission of side information. Thorough experiments over three kinds of images demonstrate the effectiveness of the proposed framework.	cluster analysis;experiment;lossless compression;requirement;scalability;sparse matrix	Xinbo Gao;Lingling An;Yuan Yuan;Dacheng Tao;Xuelong Li	2011	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2011.2130410	data compression;medical imaging;image texture;computer vision;divide and conquer algorithms;index term;digital watermarking;computer science;histogram matching;theoretical computer science;data mining;histogram;mathematics;statistics	ML	41.19149208461336	-14.971534624518874	158570
eefb0b65f464e4c29c4373ba40a78e213814015a	global path planning of mobile robot based on particle filter	particle filter algorithm mobile robot path planning;spline;path description;mobile robot;path planning;path planning mobile robots particle filtering numerical methods;mobile robots;particle filter algorithm mobile robot global path planning ferguson splines path description;ferguson splines;particle filter algorithm;particle filter;robots;global path planning;mathematical model;genetic algorithms;particle filters;particle filtering numerical methods;noise;path planning mobile robots particle filters navigation orbital robotics data structures genetic algorithms computer science design engineering sun	An approach for mobile robot path planning based on particle filter is proposed. Ferguson splines have been used as a path description to ensure the smoothness of the path. Supposing the best path as the true state, and the others as states polluted by noise,the problem of searching for the best path has been transferred to a filter problem. Therefore the particle filter algorithm is used to solve the problem.Simulation results show that the proposed algorithm is easy to imply and have a rapid convergence.	mobile robot;motion planning;particle filter;peterson's algorithm;simulation;spline (mathematics)	Yang Gao;Shu-dong Sun;Dong-feng He	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.834	mobile robot;monte carlo localization;computer vision;mathematical optimization;simulation;fast path;particle filter;any-angle path planning;computer science;artificial intelligence;mathematics	Robotics	53.53002785855699	-23.338378474995142	159181
3f89d27b5db205947ec6b80fbb3d604fdc78443e	a quality improving scheme for vq decompressed image based on dct		The better compression rate can be achieved by the traditional vector quantization (VQ) method, and the quality of the recovered image can also be accepted. But the decompressed image quality can not be promoted efficiently, so how to balance the image compression rate and image recovering quality is an important issue. In this paper, an image is transformed by discrete wavelet transform (DWT) to generate its DWT transformed image which can be compressed by the VQ method further. Besides, we compute the values between the DWT transformed image and decompressed DWT transformed image as the difference matrix which is the adjustable basis of the decompressed image quality. By controlling the deviation of the difference matrix, there can be nearly lossless compression for the VQ method. Experimental results show that when the number of compressed bits by our method is equal to the number of those bits compressed by the VQ method, the quality of our recovered image is better. Moreover, the proposed method has more compression capability comparing with the VQ scheme.	discrete cosine transform;vector quantization	Yung-Chen Chou;Shu-Huan Chen;Min-Rui Hou	2014		10.1007/978-3-319-12286-1_20	computer vision;theoretical computer science;pattern recognition;mathematics	EDA	41.76147814014739	-13.500243019220047	159265
134235a7f8139a34844ec941f9ba7fdc897738b3	multiple description perceptual audio coding with correlating transforms	transmision paquete;correlacion;channel coding;circuit codeur;human hearing models;degradation;multiple description;image coding;coding circuit;multiple description perceptual audio coding;channel coding correlating transforms multiple description perceptual audio coding audio communication lossy packet network concealment techniques compressed representation lost information estimation perceptual audio coder human hearing models entropy coded quantized subband samples source coding internet noiseless coding huffman coding;huffman coding;concepcion sistema;redundancia;signal audio;description multiple;packet loss;audio signal;audio communication;concealment techniques;correlating transforms;auditory system;noiseless coding;packet switching;transform coding;correlation methods;indexing terms;bit rate;huffman codes;experimental result;perte paquet;audio coding;codificacion;internet;redundancy;predictability;compressed representation;system design;entropy codes;robustesse;circuito codificacion;coding;transforms;lossy packet network;resultado experimental;packet transmission;robustness;packet networks;humans;predictabilidad;masking threshold;correlation;psychoacoustic models;resultat experimental;perceptual audio coder;transmission paquet;predictabilite;lost information estimation;conception systeme;hearing;senal audio;audio coding psychoacoustic models redundancy robustness humans bit rate image coding auditory system degradation masking threshold;redondance;codage;channel coding audio coding transform coding correlation methods transforms source coding hearing internet entropy codes huffman codes packet switching;entropy coded quantized subband samples;source coding;robustez	In audio communication over a lossy packet network, concealment techniques are used to mitigate the effects of lost packets. This concealment is markedly improved if the compressed representation retains redundancy to aid in the estimation of lost information. A perceptual audio coder employing multiple description correlating transforms demonstrates this phenomenon.	lossy compression;network packet;packet switching;psychoacoustics	Ramon Arean;Jelena Kovacevic;Vivek K. Goyal	2000	IEEE Trans. Speech and Audio Processing	10.1109/89.824698	speech recognition;telecommunications;computer science;statistics;huffman coding	HCI	47.93231159500536	-11.186433946293729	159730
52c5b409e1a6ba6dee6ea1e4998d234a107af160	image compression using a novel edge-based coding algorithm	image coding;image processing;data compression;edge detection;procesamiento imagen;codigo bloque;traitement image;deteccion contorno;receivers;detection contour;codage image;compression image;image compression;pattern classification;code bloc;compresion dato;block code;block codes;compression donnee;compresion imagen;classification forme	In this paper, we present a novel edge-based coding algorithm for image compression. The proposed coding scheme is the predictive version of the original algorithm, which we presented earlier in literature. In the original version, an image is block coded according to the level of visual activity of individual blocks, following a novel edge-oriented classification stage. Each block is then represented by a set of parameters associated with the pattern appearing inside the block. The use of these parameters at the receiver reduces the cost of reconstruction significantly. In the present study, we extend and improve the performance of the existing technique by exploiting the expected spatial redundancy across the neighboring blocks. Satisfactory coded images at competitive bit rate with other block-based coding techniques have been obtained.	algorithm;data compression;image compression	Farhad Keissarian;Mohammad F. Daemi	2001		10.1117/12.438254	block code;color cell compression;computer vision;block truncation coding;image processing;computer science;theoretical computer science;algorithm	Vision	46.01062781903619	-14.42244948751102	159931
3ff32696f0c6094c1a06d40b1f0514ffd68936e7	content adaptive prediction unit size decision algorithm for hevc intra coding	filtering;high resolution;image coding;complexity theory;psnr;encoding complexity theory videos psnr prediction algorithms filtering image coding;texture complexity;high efficiency video coding hevc intra coding content adaptive prediction unit size decision quadtree based picture partition high resolution videos encoding complexity texture complexity down sampled largest coding unit video sequences bit rate;quadtree based picture partition;prediction algorithms;video sequences;video coding encoding error statistics image sequences;encoding complexity;bit rate;hevc;video coding;high efficiency video coding;hevc texture complexity prediction unit intra coding;peak signal to noise ratio;intra coding;content adaptation;error statistics;unit size decision;prediction unit;content adaptive prediction;hevc intra coding;encoding;down sampled largest coding unit;high resolution videos;videos;image sequences	The quadtree based picture partition scheme in HEVC contributes to significant coding efficiency improvement, especially for high resolution videos. But encoding complexity also increases dramatically. This paper brings forward a two-stage prediction unit size decision algorithm to speed up the original intra coding in HEVC. In the pre-stage, texture complexity of down-sampled largest coding unit (LCU) and its four sub-blocks are analyzed according to video content, to filter out unnecessary prediction units for both the LCU and its sub-blocks. Secondly, during intra coding, prediction unit sizes of encoded neighboring blocks are utilized to skip small prediction unit candidates for current block. Experimental results show that proposed algorithm can speed up original HEVC intra coding by a factor of up to 73.7% and by averagely 44.91% for 4k×2k video sequences. Meanwhile, the peak signal-to-noise ratio degradation is less than 0.04dB and bit-rate stays almost the same compared with that of original HEVC intra coding.	algorithm;algorithmic efficiency;coding tree unit;digital video;elegant degradation;high efficiency video coding;image resolution;intra-frame coding;lookahead carry unit;peak signal-to-noise ratio;quadtree;speedup	Guifen Tian;Satoshi Goto	2012	2012 Picture Coding Symposium	10.1109/PCS.2012.6213317	computer vision;peak signal-to-noise ratio;computer science;theoretical computer science;coding tree unit;pattern recognition;mathematics;statistics	AI	46.64786394408499	-19.009509055814895	160105
069fa4b59a8c687d979b277aaedd1c13c48fb119	deringing and deblocking dct compression artifacts with efficient shifted transforms	image quality;color image;computational complexity;transform coding;data compression	A new method, using weighted combinations of shifted transforms, is developed for deringing and deblocking DCT compressed color images. The method shows substantial deringing improvement over prior methods, maintains comparable deblocking and shows comparable PSNR gains. The method automatically adapts to input image quality, and it may be implemented using low-complexity, swath-based processing. Multiplier-less transforms better suited for parallel hardware implementation are developed. Finally, PSNR comparisons are provided for the different methods. The new method using the DCT transform offers good visual results with PSNR comparable to prior work, and the multiplier-less transforms offer good visual results at a slight loss in PSNR.	compression artifact;deblocking filter;discrete cosine transform;image quality;parallel computing;peak signal-to-noise ratio	Ramin Samadani;Arvind Sundararajan;Amir Said	2004	2004 International Conference on Image Processing, 2004. ICIP '04.		data compression;image quality;computer vision;transform coding;color image;image processing;computer science;theoretical computer science;mathematics;parallel algorithm;computational complexity theory;statistics;computer graphics (images)	Robotics	44.35720524500344	-17.23898576664544	160188
3acce584b7623b6778d37599a93cedf90fd574bf	a novel real-time mpeg-2 video watermarking scheme in copyright protection	luminance differential dc coefficient;dither modulation;adaptive quantization step;real time;visual quality;copyright protection;macro block;human visual system;video watermarking	In this paper, an efficient video watermarking scheme is presented through modifying the third decoded luminance differential DC component in each selected macro block. The modification is implemented by binary dither modulation with adaptive quantization step. The proposed scheme is based on the observation that luminance differential DC components inside one macro block are generally space correlated, so the quantization step can be adjusted according to adjacent differential components, to utilize properties of human visual system (HVS). Experimental results show that it can be implemented in real time with better visual quality.	h.262/mpeg-2 part 2;mpeg-2;real-time transcription	Xinghao Jiang;Tanfeng Sun;Jianhua Li;Ye Yun	2008		10.1007/978-3-642-04438-0_4	computer vision;electronic engineering;telecommunications;computer science;mathematics;human visual system model	Embedded	42.07612634104939	-12.778280046185925	160232
401511c809218f104c31e806c9c1470e2f778a3a	unequal loss protection for h.263 compressed video	data compression video coding video streaming motion compensation internet forward error correction;data transmission;video streaming;data compression;motion compensation;protection video compression streaming media internet degradation forward error correction bit rate videoconference delay video coding;video signal processing;packet loss;streaming video;simulation;video compression;simulacion;correction directe erreur;motion compensated;compensation mouvement;video coding;forward error correction;internet;codage video;transmission donnee;graceful degradation;unequal loss protection h 263 motion compensated video;traitement signal video;packet networks;unequal loss protection;h 263;video coding unequal loss protection algorithm h 263 compressed video motion compensated video lossy packet network internet forward error correction frequency based ordering scheme;motion compensated video;transmision datos;compressed video	We study the application of unequal loss protection (ULP) algorithms to motion-compensated video over lossy packet networks. In particular, we focus on streaming video applications over the Internet. The original ULP framework applies unequal amounts of forward error correction to embedded data to provide graceful degradation of quality in the presence of increasing packet loss. In this letter, we apply the ULP framework to baseline H.263, a video compression standard that targets low bit rates, by investigating reorderings of the bitstream to make it embedded. The reordering process allows a receiver to display quality video, even at the loss rates encountered in wireless transmissions and the current Internet.	data compression	Justin Goshi;Alexander E. Mohr;Richard E. Ladner;Eve A. Riskin;Alan F. Lippman	2005	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2004.842613	data compression;telecommunications;computer science;statistics;computer network	Arch	48.23875858100201	-15.348407669434573	160261
6da13f3055a4a14772053794ed1227d5555b9370	an image compression scheme based on parametric haar-like transform	medical x ray image compression;parametrically adaptive transform;image coding;psnr;medical simulation;neural networks;x ray imaging;image coding discrete transforms discrete cosine transforms discrete fourier transforms fast fourier transforms neural networks signal processing algorithms medical simulation biomedical imaging fourier transforms;biomedical imaging;discrete cosine transform;discrete orthogonal transform;medical x ray image compression image compression haar like transform parametrically adaptive transform discrete orthogonal transform dct discrete cosine transform compression ratio psnr;performance improvement;image compression;discrete transforms;discrete cosine transforms;haar like transform;fast algorithm;fourier transforms;fast fourier transforms;compression ratio;orthogonal transformation;discrete cosine transforms image coding haar transforms;signal processing algorithms;haar transforms;discrete fourier transforms;structural similarity;dct	A new image compression scheme is presented, based on a fast orthogonal parametrically adaptive Haar-like transform, which is a discrete orthogonal transform such that it may be computed with a fast algorithm in structure similar to the classical fast Haar transform, and such that its matrix contains one or more predefined row(s) of an arbitrary order. The nature of the proposed image compression scheme is such that its performance (in terms of PSNR versus compression ratio) cannot be worse than that of the classical DCT (discrete cosine transform) based scheme. Simulations show that a significant performance improvement can be achieved for certain types of images such as medical X-ray images.	algorithm;computer simulation;discrete cosine transform;haar wavelet;image compression;peak signal-to-noise ratio;radiography	Susanna Minasyan;Jaakko Astola;David Guevorkian	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465030	medical simulation;discrete hartley transform;computer vision;data compression ratio;mathematical optimization;discrete mathematics;transform coding;s transform;computer science;discrete sine transform;discrete fourier transform;discrete cosine transform;mathematics;move-to-front transform;discrete fourier transform;discrete wavelet transform;fast wavelet transform;top-hat transform;artificial neural network;algorithm	Embedded	43.00546352787334	-14.762527696554189	160390
6eb0e877395b8de73402258412131d972935460f	a new block-based motion estimation algorithm	prediccion;methode recursive;estimacion;movimiento;gradient method;block matching method;metodo recursivo;recursive method;psi_visics;motion estimation;digital television;television numerica;motion;algorithme;methode gradient;algorithm;codificacion;estimation;metodo gradiente;mouvement;coding;television numerique;prediction;codage;algoritmo	The conventional motion estimation algorithms used in digital television coding can roughly be classified into two categories, namely the block-matching method and the recursive method. Each of them has its own strong points. In this paper, a new type of block-based motion estimation algorithm is presented, which is based on the block-recursive (gradient) method and makes use of some of the merits of the block-matching method. For a moderate translational motion, motion estimation with a subpel precision can conveniently be obtained with only a couple of recursive searches, and for a violent or complicated motion which cannot be estimated by any block-based algorithm, the local minimum of prediction errors can always be found. Our experiments show that the proposed algorithm is efficient and reliable, and obviously superior to the conventional block-recursive algorithms and the fast block-matching algorithms. The performance of the proposed algorithm tends almost to the optimum of the full search algorithm with the same estimation precision, but the computational effort is much less than that of the full search algorithm.	computation;experiment;gradient;maxima and minima;motion estimation;recursion;search algorithm	Kan Xie;Luc Van Eycken;André Oosterlinck	1992	Sig. Proc.: Image Comm.	10.1016/0923-5965(92)90035-E	mathematical optimization;estimation;ramer–douglas–peucker algorithm;prediction;digital television;quarter-pixel motion;gradient method;motion;motion estimation;mathematics;geometry;coding;algorithm;statistics;population-based incremental learning	Vision	49.158509539618045	-19.114759526206527	160412
5ee63c1ac4a1455f9722c66320326e1c952d2210	error resilient transcoding of scalable video bitstreams	heterogeneous network link;optimisation;transcoding complexity theory encoding decoding static var compensators bit rate algorithm design and analysis;scalable video;error resilient transcoding;complexity theory;scalable video coding;decoding;target quantization parameter training;wireless network;video coding computational complexity optimisation quantisation signal rate distortion theory transcoding;bit rate;rate distortion theory;quantisation signal;rate control;video coding;enhancement layer;wired network;scalable video bitstream;computational complexity;complexity requirement;fast rate control algorithm;quantization parameter;error resilience;static var compensators;picture level bit information scalable video bitstream error resilient transcoding wireless network wired network heterogeneous network link standard scalable video coding loss aware rate distortion optimization mode decision algorithm fast rate control algorithm target quantization parameter training computational complexity complexity requirement;standard scalable video coding;rate distortion optimization;encoding;transcoding;mode decision;algorithm design and analysis;picture level bit information;loss aware rate distortion optimization mode decision algorithm;heterogeneous network;base layer	We propose in this paper a novel error resilient transcoding scheme that can be placed at the boundary between wired and wireless networks via heterogeneous network links. This error resilient transcoder shall seamlessly complement the standard scalable video coding (SVC) bitstream to offer additional error resilient adaptation capability for receiving devices. The novel error resilient transcoding scheme consists of three different modules; each is designed to meet various levels of complexity need. The three modules are all based on the loss-aware rate-distortion optimization (LA-RDO) mode decision algorithm we have previously developed for SVC. However, each individual module can be tailored to different complexity requirements depending on whether and how the LA-RDO mode decision is implemented. Another innovation of this approach is the design of a fast rate control algorithm in order to maintain consistent bitrates between input and output of the transcoder. This rate control algorithm only needs picture-level bit information for training target quantization parameters. Simulation results demonstrate that, comparing with standard SVC, the proposed approach is able to achieve up to 4 dB gain for the enhancement layer video and up to 1 dB gain for the base layer video.	algorithm;bitstream;data compression;distortion;input/output;mathematical optimization;raster document object;rate–distortion optimization;remote data objects;requirement;scalability;scalable video coding;simulation	Yi Guo;Houqiang Li;Ye-Kui Wang;Chang Wen Chen	2008	2008 IEEE 10th Workshop on Multimedia Signal Processing	10.1109/MMSP.2008.4665094	scalable video coding;algorithm design;real-time computing;transcoding;heterogeneous network;rate–distortion theory;computer science;theoretical computer science;wireless network;rate–distortion optimization;computational complexity theory;algorithm;encoding;computer network	Vision	46.97171488163055	-17.731136218057735	160500
88bbcae09a3048e0e079345a31d579366bb8019a	pyramidal image compression using anisotropic and error-corrected interpolation	coding errors data compression image coding interpolation low pass filters filtering theory differential pulse code modulation error correction codes;interpolation;error correction codes;image coding;image coding pyramidal image compression anisotropic interpolation error corrected interpolation pyramidal coding bit rate reduction interpolation error reduction nonlinear interpolation filters morphological filters median filters isotropic filters anisotropic images anisotropic techniques transmitted error signals anisotropic diffusion low pass filter anisotropic dpcm coder;data compression;coding errors;low pass filter;anisotropic diffusion;differential pulse code modulation;image compression;error correction;low pass filters;image coding anisotropic magnetoresistance interpolation filters image resolution pixel filtering error correction bit rate signal resolution;filtering theory	In pyramidal coding, the key to significant bit rate reduction is the reduction of interpolation error. This is usually accomplished through well-designed nonlinear interpolation filters, such as median and morphological filters. These filters are usually isotropic, hence cannot account for the anisotropic nature of the images. In addition, these filters base their interpolation solely on the image at the immediate higher level (lower resolution), hence a smaller interpolation error can be expected if the already-transmitted error signals are used for the interpolation of the remaining pixels. In view of these two problems, this paper proposes to decrease the interpolation error through the introduction of anisotropic techniques and the use of the transmitted error signals to improve the interpolation of the remaining pixels. This error is further reduced through the usage of anisotropic diffusion as the low-pass filter. Finally, an anisotropic DPCM coder is presented to code the image at the top of the pyramid.	image compression;interpolation	Yu-Li You;Mostafa Kaveh	1996		10.1109/ICASSP.1996.544833	demosaicing;computer vision;bilinear interpolation;low-pass filter;computer science;theoretical computer science;stairstep interpolation;bicubic interpolation;mathematics;anisotropic diffusion;statistics;image scaling	Vision	45.09373675133664	-15.442300910466187	160641
be182ff0a967e08cfc56be2babfb3e5b6560b5d3	perceptual multiview video coding based on foveated just noticeable distortion profile in dct domain	video coding computational complexity discrete cosine transforms;multiview video coding just noticeable distortion foveation;mvc perceptual multiview video coding foveated just noticeable distortion profile dct domain spatio temporal jnd model image signature contrast threshold fjnd complexity;video coding;computational complexity;discrete cosine transforms	Recently just noticeable distortion (JND) has been highly successful in improving the video coding efficiency. Foveated JND (FJND) is an extension of the JND by further exploiting the human vision characteristic. However, there is a challenge to quickly estimate the foveation point and accurately combine the foveation factor with the spatio-temporal JND model of DCT domain for FJND. In this paper, a new FJND model in DCT domain is proposed, which adaptively searches the foveation point by exploiting the property of image signature and builds a foveation model based on the contrast threshold. Experimental results demonstrate that the proposal model remarkably reduces the complexity of FJND in searching foveation. In a number of video coding experiments, we find that, in terms of coding efficiency, the proposed perceptual coding based on FJND for multiview video coding (MVC) significantly outperforms the existing algorithm.	algorithm;algorithmic efficiency;data compression;discrete cosine transform;distortion;experiment;foveated imaging;multiview video coding;psychoacoustics	Xiwu Shang;Yongfang Wang;Lidong Luo;Zhaoyang Zhang	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738394	computer vision;computer science;coding tree unit;mathematics;multimedia;computational complexity theory;algorithm;multiview video coding;computer graphics (images)	Vision	44.30586556610303	-17.7831957966526	160920
5d79e177e4e7a782d6bcf58487c13926bf30d78f	a rate control scheme for three-dimensional wavelet-based video coders	rate distortion;average psnr rate control scheme three dimensional wavelet based video coders motion compensated temporal filtering rate distortion function lagrangian optimization fixed rate allocation;discrete wavelet transform;motion compensation;optimal method;video coding motion compensation;three dimensional;video coding resource management encoding optical fibers psnr filtering discrete wavelet transforms;motion compensated temporal filtering;rate control;video coding;rate distortion function discrete wavelet transform dwt mctf scalable coding rate allocation;rate allocation;parameter optimization;scalable coding	The rate control problem for scalable wavelet video coding system based on motion compensated temporal filtering (MCTF) has been studied in this paper, we first investigate and verify the rate-distortion (RD) function for the temporal frames, and estimate the coding efficiency parameters. Optimal rate allocation among the frames in the group is then examined based on the Lagrangian optimization method, and a rate control strategy related to the variance and coding efficiency parameter of the temporal frames has been presented, the algorithm and its complexity is suitable for video applications. Experiments show that compared with the fixed rate allocation, the proposed rate control strategy can obviously improve the average PSNR.	algorithm;algorithmic efficiency;control theory;data compression;decibel;distortion;mathematical optimization;peak signal-to-noise ratio;rate–distortion theory;ruby document format;scalability;wavelet	Lizhi Zhang;Tao Wang;Huadong Sun;Long Yang;Jinzhao Ma;Lei Zhou	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023589	three-dimensional space;computer vision;mathematical optimization;electronic engineering;computer science;coding tree unit;mathematics;discrete wavelet transform;motion compensation	Robotics	46.88130294884607	-16.824992707304137	161142
d9589cd6fd665eea38369ecf0c470abf1542d5fd	rate-distortion optimal parameter choice in a wavelet image communication system	image coding wavelet image communications system parameter optimization channel induced distortion quantizer step size channel code rate global optimization procedure rate distortion theory source coding channel coding;channel coding;optimisation;image coding;image communication;visual communication;transform coding;optimization methods image analysis performance analysis wavelet analysis image communication rate distortion theory;rate distortion theory;wavelet transforms;visual communication wavelet transforms optimisation source coding channel coding transform coding image coding rate distortion theory;global optimization;rate distortion optimization;parameter optimization;source coding	methodology and results of paramperformed with the aid of analytit h e channel-induced distortion in communications system. R ( D independently for each subband curve representing (in addition an optimal choice of quantizer code rate. A global optimizaperformed to identify the best R ( D ) curve for each subband. Electrical Engineering Department University of California, Los Angeles jgarcia, benyamin, villa@icsl.ucla.edu	code rate;distortion;electrical engineering;quantization (signal processing);wavelet	Javier Garcia-Frias;Dan Benyamin;John D. Villasenor	1997		10.1109/ICIP.1997.638664	sub-band coding;computer vision;mathematical optimization;transform coding;rate–distortion theory;channel code;variable-length code;computer science;theoretical computer science;coding tree unit;mathematics;rate–distortion optimization;statistics;visual communication;global optimization;wavelet transform;source code	Mobile	47.70035976458525	-12.933001104253954	161385
21573c28dbd2e3a9baa1bc1bf5dd0484153a0d7a	interleaving optimization with sampling-based motion planning (ios-mp): combining local optimization with global exploration		Computing globally optimal motion plans for a robot is challenging in part because it requires analyzing a robot’s configuration space simultaneously from both a macroscopic viewpoint (i.e., considering paths in multiple homotopic classes) and a microscopic viewpoint (i.e., locally optimizing path quality). We introduce Interleaved Optimization with Sampling-based Motion Planning (IOS-MP), a new method that effectively combines global exploration and local optimization to quickly compute high quality motion plans. Our approach combines two paradigms: (1) asymptotically-optimal sampling-based motion planning, which is highly effective at global exploration but relatively slow at locally refining paths, and (2) optimization-based motion planning, which locally optimizes paths quickly but lacks a global view of the configuration space. IOS-MP iteratively alternates between global exploration and local optimization, sharing information between the two, to improve motion planning efficiency. We demonstrate the effectiveness of IOS-MP in scenarios of varying complexity and dimensionality.	anytime algorithm;asymptotically optimal algorithm;display resolution;experiment;forward error correction;local search (optimization);mathematical optimization;maxima and minima;motion planning;robot;sampling (signal processing);sequential quadratic programming;statistical relational learning	Alan Kuntz;Chris Bowen;Ron Alterovitz	2016	CoRR		computer vision;mathematical optimization;simulation;mathematics	Robotics	52.08711444805954	-23.892848517865804	161439
3e6130879f05e6bd6e564713732cdf4791bda97c	reversible data hiding based on pairwise prediction-error histogram				Ju-Yuan Hsiao;Zhi-Yang Lin;Po-Yueh Chen	2017	J. Inf. Sci. Eng.		computer science;distributed computing;information hiding;mean squared prediction error;artificial intelligence;pairwise comparison;histogram matching;histogram;pattern recognition	DB	42.09889003765063	-11.980782194676292	161451
a49be7758204f8406bf138a149c7c31281dac788	jasteg2000 - steganography for jpeg2000 coded images			jpeg 2000;steganography	Domenico Introna;Francescomaria Marino	2006			computer security;steganography;theoretical computer science;jpeg 2000;computer science	Vision	41.85146245471323	-12.269887978752422	161766
a4043122e2452f19e39a0c69d88e2f2f567d47b1	an intelligent fragile watermarking scheme based on contourlets for effective detection, localization and recovery of tampered regions in handwritten document images		Handwritten document images are a special class of document images having a lot of variations in font-style, font size, thickness and interspacing of words and lines. Designing an effective fragile watermarking scheme to protect such images needs capturing both structure and information content which is an open research challenge. A novel contourlet based fragile watermarking technique for optimal detection of tampered locations, localization and recovery of handwritten document images is put forth in this paper. The original image is t subjected to Contourlets transform. The first and second level contourlet coefficients are used as watermark. The watermark is embedded intelligently into robust locations based on the significance of the contribution of the contourlet coefficients. A quantization based embedding technique is used for embedding. The tamper detection is performed by comparing the blocks of generated and extracted watermarks and component labelling technique is used to find out different tampered objects. The recovery of tampered objects is carried out using the watermarks embedded at robust locations. The proposed watermarking scheme using contourlets is compared with a popular technique using curvelets. The proposed technique outperforms in detecting tampered locations and recovery of the tampered information. This work also exhibits better efficiency in terms of PSNR values. Further the proposed method computationally is less expensive compared to the existing method based on curvelets.	coefficient;contourlet;curvelet;digital watermarking;embedded system;open research;peak signal-to-noise ratio;quantization (signal processing);scheme;self-information;sensor;thickness (graph theory);watermark (data file)	R K. ChetanK.;S. Nirmala	2017	2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2017.8125874	watermark;robustness (computer science);computer science;control engineering;digital watermarking;contourlet;curvelet;quantization (signal processing);computer vision;embedding;artificial intelligence	Vision	39.586268953429986	-11.322520569516051	161951
d75af74136dff3a5d6b3fe691db78e523d241509	capacity of the watermark channel: how many bits can be hidden within a digital image?	digital watermarking;canal con ruido;image numerique;droit auteur;capacite;noisy channel;copyright;filigrane;canal avec bruit;statistical model;capacidad;transition matrix;input output;protection;matrices;methode domaine frequence;frequency domain method;imagen numerica;modele statistique;watermark;proteccion;modelo estadistico;digital image;metodo dominio frecuencia;capacity;frequency domain;derecho autor	An evaluation of the number of bits that can be hidden within an image by means of frequency-domain watermarking is given. Watermarking is assumed to consist in the modiication of a set of full-frame DCT (DFT) coeecients. The amount of modiication each coeecient undergoes is proportional to the magnitude of the coeecient itself, so that an additive-multiplicative embedding rule results. The watermark-channel is modeled by letting the watermark be the signal and the image coeecients the noise introduced by the channel. To derive the capacity of each coeecient, the input (i.e. the watermark) and the output (i.e. the watermarked coeecients) of the channel are quantized, thus leading to a discrete-input, discrete-output model. Capacity is evaluated by computing the channel transition matrix and by maximizing the mutual input/output information. Though the results we obtained do not take into account attacks, they represent a useful indication about the amount of information that can be hidden within a single image	algorithm;autostereogram;digital image;digital watermarking;discrete cosine transform;full-frame digital slr;input/output;noisy-channel coding theorem;numerical analysis;quantization (signal processing);stochastic matrix;utility functions on indivisible goods;watermark (data file);watermarking attack	Mauro Barni;Franco Bartolini;Alessia De Rosa;Alessandro Piva	1999		10.1117/12.344694	speech recognition;geography;telecommunications;cartography	ML	43.99479282114491	-10.643556316456584	161984
6d1936707a2b62a4f06a3ddcd4da11b339e140ca	binary-image-manipulation algorithms in the image view facility	ひらめく;研究開発;専門;特許;機関;検索;科学技術;affichage;研究者;産学連携;横断検索;algorithm performance;information compression;image processing;binary image;jst;学術;独立行政法人;electronic mailing;echelle;cambio;検索エンジン;科学技術振興機構;procesamiento de imagen;リンクセンター;correo electronico;escala;compresion informacion;scale;関連検索;traitement image;論文;ひろがる;change;courrier electronique;j global;遺伝子;ivf image view facility における画像2値化アルゴリズム;display;データベース;ｊｇｌｏｂａｌ;resultado algoritmo;研究資源;研究課題;exhibicion;burotica;compression information;image binaire;jglobal;国立研究開発法人;jdream;performance algorithme;ｊ ｇｌｏｂａｌ;non coded information;アイディア;資料;imagen binaria;changement;技術動向;information non codee;書誌情報;文献;rotacion;発想;化学物質;統合検索;rotation;ｊｓｔ;科学技術用語;office automation;bureautique;つながる	Most current implementations of electronic mail deal primarily with coded information. A scanned-document-handling system that could scan a document, distribute it, display it on terminals, and print it on host-attached printers would offer a similar convenience for documents in hard-copy rather than coded form. For such a system to be practical, fast software is needed for a number of image-manipulation functions. The required functions are compression, to reduce the size of the data files; decompression, to reconstruct the scanned document; scaling, to match the resolution of the scan to the resolution of the display or printer; and rotation, to reorient documents scanned sideways or upside down. This paper describes a collection of algorithms underlying fast software for manipulating binary images that is used in the Image View Facility, a System/370-based software package that permits the display and printing of binary images at various resolutions.	algorithm;binary image	Karen L. Anderson;Fred Mintzer;Gerald Goertzel;Keith S. Pennington;Joan L. Mitchell;William B. Pennebaker	1987	IBM Journal of Research and Development	10.1147/rd.311.0016	computer vision;scale;simulation;binary image;image processing;rotation;computer science;quantum mechanics	Theory	39.39611014622398	-20.257936699384505	162464
f21839f5a91264ac05a5c1b3ce2040a53dbb9175	a new steganographic algorithm based on multi directional pvd and modified lsb		Steganographic techniques can be utilized to conceal data within digital images with small or invisible changes in the perceived appearance of the image. Generally, five main objectives are used to assess the performance of steganographic algorithms which include embedding capacity, imperceptibility, security, robustness and complexity. However, steganographic algorithms hardly take all of these factors into account. In this paper, a novel steganographic algorithm for digital images is proposed based on the pixel-value differencing (PVD) and modified least-significant bit (LSB) substitution (MDPVD-MLSB) techniques to address most of aforementioned objectives. Although there are many techniques for concealing data within pixels, the restricting factor is always the amount of bits adjusted in every pixel. Therefore, the main contributions of this paper aim to achieve a balance between the amount of embedded data, the level of acceptable distortion, as well as providing high level of security. The performance of this algorithm has been extensively evaluated in terms of embedding capacity, peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). Simulation results and comparisons with six relevant algorithms are presented to demonstrate the effectiveness of this proposed algorithm.	algorithm;autoregressive integrated moving average;digital image;distortion;embedded system;high-level programming language;kad network;least significant bit;modified discrete cosine transform;most significant bit;peak signal-to-noise ratio;physical vapor deposition;pixel;simulation;steganographic file system;steganography;structural similarity;terminal identifier – address resolution protocol	Khalid A. Darabkh	2017	ITC	10.5755/j01.itc.46.1.15253	computer science;theoretical computer science;data mining;world wide web	EDA	39.70419056807572	-11.758029263450348	162537
62d105f5549fd40f2b7d87e8a3b57e46c3317fba	statistical modeling and threshold selection of wavelet coefficients in lossy image coder	image features;rate distortion;adaptive thresholding;image coding;probability;data compression;transform coding;statistical model;data distribution;rate distortion theory;quantisation signal;wavelet transforms;conditional model;wavelet coefficients quantization image coding bit rate image resolution image decomposition discrete wavelet transforms energy resolution spatial resolution signal resolution;statistical analysis;image compression;conditional probability;standard uniform quantization statistical modeling wavelet coefficients lossy image coder wavelet domain data quantization compression efficiency conditional probability model scale spatial magnitudes approximation wavelet coefficient dependencies threshold data selection uniform quantization modification conditional model encoding quantized magnitudes adaptive threshold local image features significance expectation estimation low cost quantization experimental rate distortion curve;statistical analysis image coding quantisation signal wavelet transforms data compression rate distortion theory probability transform coding	An algorithm for using wavelet domain data quantization to improve the compression efficiency is presented. A conditional probability model of adjacent (in scale-spatial sense) magnitudes was applied as a better approximation of the wavelet coefficient dependencies than the marginal data distributions. This model was utilised in the threshold data selection and is proposed as a more effective uniform quantization modification than increasing the dead-zone. The same conditional model was used in quantization and encoding of the quantized magnitudes. Additionally, to fit the adaptive threshold value to local image features, estimation of significance expectation was included in the thresholding procedure. As a result, a more effective low-cost quantization scheme was constructed. It allows a significantly increase in image compression efficiency. An experimental rate-distortion curve shows the same distortion for decreased bit rates even up to 20% in comparison to standard uniform quantization.	coefficient;lossy compression;statistical model;wavelet	Artur Przelaskowski	2000		10.1109/ICASSP.2000.859238	data compression;statistical model;transform coding;conditional probability;rate–distortion theory;quantization;trellis quantization;image compression;theoretical computer science;pattern recognition;probability;mathematics;thresholding;quantization;feature;vector quantization;statistics;wavelet transform	ML	44.083716762105084	-15.473577852244013	162776
15108e12de617b97d9518ac04f59fa2fcedcd5f6	generalized source-channel prediction for error resilient video coding	error resilient video coding;quantization;propagation losses;layered video coding;coding efficiency;single layer video coding;video coding decoding source coding resilience robustness performance gain redundancy quantization propagation losses motion compensation;motion compensation;decoding;error propagation reduction;intra coded macroblocks;combined source channel coding;video coding combined source channel coding image reconstruction;video coding;redundancy;resilience;error propagation;weighted sums;image reconstruction;encoder reconstruction based prediction framework;performance gain;error resilience;robustness;generalized source channel prediction;leaky prediction;source coding;error propagation reduction generalized source channel prediction error resilient video coding encoder reconstruction based prediction framework frame reconstruction leaky prediction coding efficiency single layer video coding intra coded macroblocks;frame reconstruction	This paper proposes an error-resilient modification of conventional (encoder-reconstruction based) prediction framework in video coding. The technique is called generalized source-channel prediction (GSCP) and generates a prediction reference for the next frame as a weighted sum of the current frame reconstruction and the prediction reference of the last frame. Compared to existing leaky prediction, GSCP achieves better coding efficiency in single layer video coding, and specifically better exploits the robustness benefits offered by intra coded macroblocks in past frames so as to reduce error propagation in the future frames. Significant performance gains were observed in simulations and support the effectiveness of GSCP	algorithmic efficiency;data compression;encoder;macroblock;propagation of uncertainty;simulation;software propagation;weight function	Hua Yang;Kenneth Rose	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660397	iterative reconstruction;reference frame;real-time computing;quantization;telecommunications;computer science;propagation of uncertainty;theoretical computer science;algorithmic efficiency;redundancy;motion compensation;psychological resilience;robustness;source code	Robotics	47.83126647534476	-16.605872393047438	162797
b3790829cbb0bda4195122bf3fcd2863d1d38044	robust policies via meshing for metastable rough terrain walking		In this paper, we present and verify methods for developing robust, high-level policies for metastable (i.e., rarely falling) rough-terrain robot walking. We focus on simultaneously addressing the important, real-world challenges of (1) use of a tractable mesh, to avoid the curse of dimensionality and (2) maintaining near-optimal performance that is robust to uncertainties. Toward our first goal, we present an improved meshing technique, which captures the step-to-step dynamics of robot walking as a discrete-time Markov chain with a small number of points. We keep our methods and analysis generic, and illustrate robustness by quantifying the stability of resulting control policies derived through our methods. To demonstrate our approach, we focus on the challenge of optimally switching among a finite set of low-level controllers for underactuated, rough-terrain walking. Via appropriate meshing techniques, we see that even terrain-blind switching between multiple controllers increases the stability of the robot, while lookahead (terrain information) makes this improvement dramatic. We deal with both noise on the lookahead information and on the state of the robot. These two robustness requirements are essential for our methods to be applicable to real high-DOF robots, which is the primary motivation of the authors.	amiga walker;approximation algorithm;cobham's thesis;curse of dimensionality;dynamical system;high- and low-level;iteration;markov chain;markov decision process;mobile robot;on the fly;parsing;requirement;robustness (computer science);rough set;simulation;underactuation	Cenk Oguz Saglam;Katie Byl	2014		10.15607/RSS.2014.X.049	machine learning;artificial intelligence;terrain;robustness (computer science);computer science;underactuation;curse of dimensionality;markov chain;small number;finite set	Robotics	51.450674089158625	-23.888767309406823	162908
e7de97951e4cd02de94b353d84674da15cb7f854	fault-tolerance based block-level bit allocation and adaptive rdo for depth video coding		Depth videos affect the visual quality of virtual view greatly, while conventional encoders are not adapted to encode depth videos. To solve this problem, we propose, in the paper, a fault-tolerance based joint block-level bit allocation scheme for depth video coding. The scheme classifies depth blocks into two classes by the fault-tolerance, and constructs virtual view perceptual quality based rate-distortion (R-D) model for each class. Based on the model, a joint block-level bit allocation scheme is proposed to obtain the optimized quantization parameter (QP) to encode each class of blocks. Further, an adaptive RDO algorithm is designed to determine λMODE, in order to achieve better visual quality of virtual views. Experimental results demonstrate that, compared with 3D-HEVC, the proposed method improves the visual quality of virtual views, especially at preserving details and boundaries of objects.	2d-plus-depth;3d film;algorithm;data compression;distortion;encode;encoder;fault tolerance;high efficiency video coding;raster document object;remote data objects	Shengwei Wang;Li Yu;Sen Xiang;Zixiang Xiong	2017	2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2017.8122250	pattern recognition;computer science;artificial intelligence;encoder;coding (social sciences);quantization (signal processing);fault tolerance;encoding (memory);distortion	Vision	44.74026894943939	-17.735852469926915	163002
71a76feeeaaa80704615add8d21c4fc50edd48c8	fdct-based digital watermarking algorithm for gray images	fast discrete curvelet transforms;bit plane;discrete wavelet transforms;gray image;digital watermarking;watermarking;encrypt;hidden information;image coding;discrete wavelet transform;psnr;image processing;watermarking cryptography curvelet transforms discrete wavelet transforms image coding multimedia computing;digital watermark;arnold transform fdct based digital watermarking algorithm hidden information digital multimedia data signal processing fast discrete curvelet transform discrete wavelet transform image processing encrypted embedded algorithm gray image chaotic map;fdct based digital watermarking algorithm;digital multimedia data;chaotic map;multimedia computing;encrypt fast discrete curvelet transforms digital watermarking bit plane;encrypted embedded algorithm;discrete cosine transforms;cryptography;watermarking discrete wavelet transforms cryptography psnr discrete cosine transforms;signal processing;multimedia data;curvelet transforms;fast discrete curvelet transform;arnold transform	Digital watermarking technology means that the hidden information is embedded in digital multimedia data by way of signal processing. In this paper, we firstly analyze the Fast Discrete Curvelet Transforms (FDCT) and Discrete Wavelet Transforms (DWT) which are usually used in image processing and figure out that DWT can not approximate optimally the image which contains the line singularity. And then a new encrypted embedded algorithm is presented based on FDCT. Based on bit-plane of gray image, the algorithm makes use of the advantage of FDCT and the watermark is encrypted by Chaotic Map and Arnold Transforms (AT) which is imbedded into the proper coefficient of FDCT. The result of our experiment shows that this algorithm is robust and transparent compared with DWT.	approximation algorithm;arnold;bit plane;chaos theory;coefficient;curvelet;digital watermarking;discrete wavelet transform;embedded system;encryption;image processing;signal processing	Jiang Yu;Linsheng Li;Anhong Wang;Qichuan Tian	2010	2010 International Conference on Computational Aspects of Social Networks	10.1109/CASoN.2010.147	computer vision;image processing;digital watermarking;computer science;theoretical computer science;signal processing;mathematics;computer graphics (images)	EDA	41.44435541583975	-11.802495302185516	163133
ce7ee8d32bc44d96b24c4864127eb17e9005c12e	generalizations of pixel-value differencing steganography for data hiding in images	data hiding;steganorgraphy;haar wavelet;steganography;pixel value differencing	Generalizations of the pixel-value differencing (PVD) method for data hiding in graylevel images are proposed. Two extensions of the PVD method are analyzed: the block-based approach and the Haar-based approach. For the block-based PVD, the cover image is divided into non-overlapping horizontal or square blocks of n pixels. In each block, n − 1 differences are calculated between consecutive pixels. These differences are classified to embed the secret message. For the Haar-based PVD, the 2-D integer Haar wavelet is applied to decomposed the cover image. The high-frequency components are used to hide message. Higher level of Haar decomposition allows more bits to be embedded. For both proposed generalizations, the capacity of the embeddedmessage is significantly increased. Moreover, both generalizations are invulnerable to the RS-diagram and histogram steganalysis.	autoregressive integrated moving average;pixel;steganography	Jen-Chang Liu;Ming-Hong Shih	2008	Fundam. Inform.		discrete mathematics;computer science;theoretical computer science;mathematics;steganography;programming language;information hiding;algorithm;statistics	Vision	41.213200398272484	-12.801374264876618	163155
3a66496ce3531a88943a28b9e42e2b6e3cf6e6d7	avoiding explosive search in automatic selection of simplest pattern codes	automatic code reduction;minimization;code generation;structural information;hierarchical representation;coding theory;linear code;shortest route problem;coding theory of pattern perception;computer simulation;hierarchical representation of linear codes	"""-According to the coding theory of pattern perception, the preferred organization of a pattern is reflected by the simplest code that represents this pattern. The number of codes, out of which the simplest one has to be selected, grows exponentially with the complexity of the pattern. So in computer simulation it would not be wise to generate all codes and then select the simplest one. This would take a lot of computing time. The present study proposes a procedure which avoids this explosion of code generation and yet obtains the simplest code. The central part of this procedure consists of translating the search for a simplest code to a shortest route problem. Structural information Coding theory of pattern perception Hierarchical representation of linear codes Minimization Shortest route problem Automatic code reduction I N T R O D U C T I O N Patterns can be interpreted in different ways, but usually one interpretation is preferred. By several scientists this preferred interpretation is regarded as the most regular organization of the pattern."""" 31 In most models of pattern coding this preferred organization is specified as the one that is attained with a minimum of procedural steps34-61 However, there is evidence from perception research that in perception this preference is not based on the process itself, but on the final outcome of this process. Independent of the way in which a description is determined, the visual system tends towards the most probable pattern description,C7 9~ or towards the simplest pattern description/m 13~ The concept of""""simplici ty"""" pertains to the structural information approach ~1.1 and is used here as the starting point. On the one hand, this concept might be perceptually relevant, but on the other hand, it seems to require a very laborious process to find the simplest pattern description. According to a naive process model, all codes of a pattern have to be constructed before the simplest one can be selected. As will be shown later, the amount of all such codes increases dramatically with the pattern complexity. The aim of this paper is to show that this explosion can be largely reduced to a manageable size and that therefore """"'representational simplicity"""" is a realistic criterion for the preferred pattern organization. The structural information approach provides a model which is the starting point of the present study. *This research was supported by the Netherlands Organization for the Advancement of Pure Research IZ.W.O.I. First we will give an overview of this model as far as it is relevant to our present work. The model only concerns the perceptual representation of patterns, not the process that leads to such a representation. This process is considered as a black box in the total perceptual system. In the present work, we want to fill in this black box by proposing a procedure for the selection of a simplest code for a given pattern. This does not imply we think the real process works like this procedure, which only tries to simulate the final outcome of the process. But if the outcomes of the simulation are attained in an acceptable way (by avoiding explosive searches) and if they are in agreement with the outcomes of the real process, the simulation will be a support to the assumption about the perceptual system on which the structural information approach is based. S T R U C T U R A L I N F O R M A T I O N The structural information approach deals with the perceptual organization of patterns. 115~ The organization of a pattern is reflected by a description or code of this pattern. However, a pattern can be described by several codes. Although it ought to be possible to reconstruct the pattern from each code, these codes are not equivalent: some codes express more regularity in the pattern than other codes. In other words, some codes reflect a """"simpler"""" organization than others. The core of the structural information model is that the code which reduces the pattern organization to its """"simplest"""" form, reflects the preferred perceptual organization. This tendency is called the """"'minimum principle"""". """"6 ' 1 ~ This is the general idea. In order to be more specific about the description of patterns we need"""	algorithm;black box;code generation (compiler);coding theory;computer simulation;data structure;design pattern;heuristic (computer science);information model;linear code;pattern language;polynomial;pontryagin's maximum principle;process modeling;time complexity	Peter Arnold van der Helm;E. L. J. Leeuwenberg	1986	Pattern Recognition	10.1016/0031-3203(86)90022-1	computer simulation;block code;systematic code;polynomial code;prefix code;combinatorics;constant-weight code;low-density parity-check code;variable-length code;computer science;self-synchronizing code;theoretical computer science;machine learning;cyclic code;linear code;mathematics;forward error correction;locally testable code;dual code;algorithm;code generation;coding theory	AI	50.335682763790494	-14.627571544842086	163169
7fc9985e7e24a3af913ac0e62a333944700bf4cf	new trends in multimedia standards: mpeg4 and jpeg2000	multimedia application;chip;cable modem	The dramatic increase in both computational power, brought on by the introduction of increasingly powerful chips, and the commu nications bandwidth, unleashed by the introduction of cable modem and ADSL, lays a solid foundation for the take-off of mu ltimedia applications. Standards always play an important role in multimedia pplications due to the need for wide distribution of multimedia contents. St andards have long played pivotal roles in the development of multimedia equipment and contents. MPEG4 and JPEG2000 are two recent mu lti edia st andards under development under the auspice of the International Standards Organization (ISO). These new standards introduce new technology and new featu res that will become enabling technology for many emerging applications. In this paper, we describe the new trends and new developments that sh ape these new standards, and illustrate the potential impact of these new st andards on mu ltimedia applications.	asymmetric digital subscriber line;cable modem;computation;jpeg 2000;noble ape;the sims	Jie Liang	1999	InformingSciJ	10.28945/606	simulation;computer science;multimedia;world wide web	Arch	42.77001740747957	-20.106601300342554	163175
b3cf57276010863641f5fbf8745cc7c3908c33a9	high efficiency 3d video coding using new tools based on view synthesis	data compression;depth maps 3d video coding compression mvd representation hevc;nonlinear depth representation high efficiency 3d video coding hevc view synthesis mvd representation depth maps coding tools disoccluded region coding compression efficiency depth based motion prediction joint high frequency layer coding;video coding;image representation;video coding data compression image representation video codecs;video codecs	We propose a new coding technology for 3D video represented by multiple views and the respective depth maps. The proposed technology is demonstrated as an extension of the recently developed high efficiency video coding (HEVC). One base views are compressed into a standard bitstream (like in HEVC). The remaining views and the depth maps are compressed using new coding tools that mostly rely on view synthesis. In the decoder, those views and the depth maps are derived via synthesis in the 3D space from the decoded baseview and from data corresponding to small disoccluded regions. The shapes and locations of those disoccluded regions can be derived by the decoder without any side information transmitted. To achieve high compression efficiency, we propose several new tools such as depth-based motion prediction, joint high frequency layer coding, consistent depth representation, and nonlinear depth representation. The experiments show high compression efficiency of the proposed technology. The bitrate needed for transmission of two side views with depth maps is mostly less than 50% than that of the bitrate for a single-view video.	bitstream;data compression;decoder device component;depth map;experiment;financial cost;high efficiency video coding;nonlinear system;simulcast;stereoscopic video coding;view synthesis;benefit;disease transmission	Marek Domanski;Olgierd Stankiewicz;Krzysztof Wegner;Maciej Kurc;Jacek Konieczny;Jakub Siast;Jakub Stankowski;Robert Ratajczak;Tomasz Grajek	2013	IEEE Transactions on Image Processing	10.1109/TIP.2013.2266580	video compression picture types;data compression;scalable video coding;computer vision;computer science;theoretical computer science;coding tree unit;mathematics;multimedia;context-adaptive binary arithmetic coding;motion compensation;h.261;statistics;multiview video coding	Graphics	44.04408647685849	-18.790593994292696	163181
aa41be0187c1aea35b2b1ca01539ae94456e826e	design of an edge detection based image steganography with high embedding capacity		In this paper, the authors have proposed an image steganography method for improving the embedding capacity of the gray-scale cover image. In general, the embedding of the secret message into the sharp areas i.e. edge region rather than in the smooth areas i.e. non edge region of the cover image attains relatively better quality stego-image. So in the proposed work, we have also exploited the presence of edges in the cover image to embed a large amount of secret message into the cover image. The proposed method carried out into two phases: in the first phase the cover image is classified into edge region and non-edge region. Subsequently in the second phase, the binary secret message bits are embedded by replacing some least significant bits (LSBs) of each pixel. In the proposed work, x LSBs replacement are preferred for the pixels belongs to edge region and y LSBs replacement are considered for non-edge region pixels where x>y. The proposed method increases the embedding capacity of the cover image compare to some existing standard steganographic methods. The scheme has been tested on several standard grayscale test images and the obtained simulation results depict the feasibility of the proposed scheme.		Arup Kumar Pal;Tarok Pramanik	2013		10.1007/978-3-642-37949-9_69	computer vision;theoretical computer science;mathematics;engineering drawing	Vision	40.77692015172831	-12.86566892880405	163268
01e9246ff81aa2e4fe7ebb2a4047020d35933ae5	a study of jpeg 2000 still image coding versus other standards	discrete wavelet transforms;image coding;complexity theory;iso standards;transform coding;lts1;encoding;transform coding image coding encoding iso standards complexity theory discrete wavelet transforms	JPEG 2000, the new ISO/ITU-T standard for still image coding, is about to be finished. Other new standards have been recently introduced, namely JPEG-LS and MPEG-4 VTC. This paper puts into perspective the performance of these by evaluating JPEG 2000 versus JPEG-LS and MPEG-4 VTC, as well as the older but widely used JPEG. The study concentrates on compression efficiency, although complexity and set of supported functionalities are also evaluated. Lossless compression efficiency as well as the lossy rate-distortion behavior is discussed. The principles behind each algorithm are briefly described and an outlook on the future of image coding is given. The results show that the choice of the “best” standard depends strongly on the application at hand.	algorithm;distortion;jpeg 2000;lossless compression;lossy compression;microsoft outlook for mac	Diego Santa Cruz;Touradj Ebrahimi	2000	2000 10th European Signal Processing Conference		data compression;lossless jpeg;computer vision;shannon–fano coding;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;jpeg;jpeg 2000;tunstall coding;multimedia;adaptive coding;context-adaptive binary arithmetic coding;macroblock;quantization	Vision	43.09305325690089	-18.062022675596754	163353
77b9336dfc16939e630796e280fc6b5f78625a5f	comparison of different wavelet coder	image coding;speck;multimedia application;ezw;visual quality;spiht;wavelet	An images and video are distinct sources of information and play significant role in almost all multimedia applications. The wavelet-based image-coding algorithm considerably improves the compression rate and the visual quality. In this paper a comparative study of the three wavelet coders EZW, SPIHT and SPECK.	algorithm;embedded zerotrees of wavelet transforms;set partitioning in hierarchical trees;speck (cipher)	S. Mishra;S. Sawarkar;P. Saha	2011		10.1145/1980022.1980041	computer vision;speech recognition;computer graphics (images)	Visualization	42.66762025334209	-15.515747183783201	163375
5fdf67ed05fc3f8df6252c87d3d6317d82cb29a0	efficient disparity vector coding for multi-view 3d displays	3d displays;3 d video;image tridimensionnelle;sequences;multi view video coding;image processing;signal estimation;multi view technology;imagen fija;0130c;disparity estimation;disparity;motion estimation;qualite image;traitement image;disparidad;processing time;algorithme;difference scheme;video coding;redundancy;fixed image;technology and engineering;codage video;image quality;image sequence;estimacion senal;tridimensional image;temps traitement;algorithms;image fixe;prediction schemes;calidad imagen;video;4230v;encoding;estimation signal;disparite;tiempo proceso;technologie multi vues;3d display;cameras;sequence image;imagen tridimensional;redondance;codage;image sequences	Disparity estimation can be used for eliminating redundancies between different views of an object or a scene recorded by an array of cameras which are arranged both horizontally and vertically. However, estimation of the disparity vectors is a highly time consuming process which takes most of the operation time of the multi-view video coding. Therefore, either the amount of data that is to be processed or the complexity of the coding method needs to be decreased in order to encode the multi-view video in a reasonable time. It is proven that the disparities of a point in the scene photographed by cameras which are spaced equidistantly are equal. Since there is a strong geometrical correlation of the disparity vectors, the disparity vector of a view can for most blocks be derived from the disparity vector of another view or views. A new algorithm is presented that reduces the amount of processing time needed for calculating the disparity vectors of each neighboring view except the principal ones. Different schemes are proposed for 3x3 views and they are applied to several image sequences taken from a camera-array. The experimental results show that the proposed schemes yield better results than the reference scheme while preserving the image quality and the amount of encoded data.	algorithm;binocular disparity;data compression;encode;feature vector;image quality;operation time	Aykut Avci;Lawrence Bogaert;Roel Beernaert;Jelle De Smet;Youri Meuret;Hugo Thienpont;Herbert De Smet	2010		10.1117/12.838296	computer vision;stereo display;image processing;computer graphics (images)	Vision	46.136668772647205	-15.049210691900377	163385
7ffebe7635d014088e1e430f1ca0ac59664e6197	trellis quantization for l∞-constrained compression with integer wavelets	quantization;image coding;linfin constrained compression;data compression;bit rate;quantisation signal;encoding wavelet transforms quantization wavelet coefficients entropy image coding bit rate;wavelet transforms;near lossless reconstruction trellis quantization linfin constrained compression integer wavelet near lossless signal compression wavelet code stream;near lossless signal compression;integer wavelet;signal reconstruction;trellis quantization;entropy;trellis codes;wavelet code stream;error bound;near lossless reconstruction;encoding;wavelet transforms data compression quantisation signal signal reconstruction trellis codes;wavelet coefficients	Although integer wavelets have been successfully used in lossless signal compression, they have not been generalized to near-lossless (Linfin constrained) coding. This paper proposes a new technique of trellis quantization in the framework of lifting integer wavelet as a promising way of near-lossless signal compression. This new technique achieves continuous scalability of the wavelet code stream from highly lossy (low bit rates) to near-lossless (respecting a tight error bound on each decoded sample) reconstruction.	lifting scheme;lossless compression;lossy compression;scalability;signal compression;trellis quantization;wavelet	Xiaolin Wu;Xiaohan Wang;Guangming Shi	2008	2008 IEEE International Symposium on Information Theory	10.1109/ISIT.2008.4595487	data compression;signal reconstruction;entropy;discrete mathematics;quantization;trellis quantization;computer science;theoretical computer science;mathematics;lossless compression;encoding;statistics;wavelet transform	Arch	45.16503236226309	-15.453466033127626	163473
8720c06d1a9a324dcb798beddd7fef8723e59a9e	motion estimation using cross center-biased distribution and spatio-temporal correlation of motion vector	temporal correlation;diamond search;full search;motion estimation;motion vector field;estimation algorithm;motion vector;image quality;compression ratio;real time application	  Motion estimation (ME) technique is an important part of video encoding, since it could significantly affect the compression  ratio and the output quality. But full search (FS) for block-based motion estimation is computationally intensive and it can  hardly be applied to any real time applications. In this paper, we propose a new adaptive motion vector estimation algorithm  based on the cross center-biased distribution property and the high spatio-temporal correlation of motion vector to find the  motion vector more efficiently. The proposed method identifies a more accurate first search center instead of using the origin  as the initial search center through compensating the search area based on the spatio-temporal correlation of motion vector  and defines a search pattern adaptively. As a result, we reduce the total number of search points used to find the motion  vector of the current block and improve the motion estimation accuracy. Experiments show that the speedup improvement of the  proposed algorithm over Diamond Search algorithm (DS), Motion Vector Field Adaptive Search Technique (MVFAST) and Predictive  Motion Vector Field Adaptive Search Technique (PMVFAST) can be up to 1.3 ~ 2.8 times on average and the image quality improvement  can be better up to 0.1(dB)~ 0.2(dB) compare with PMVFAST.    	motion estimation	Mi-Young Kim;Mi Gyoung Jung	2004		10.1007/978-3-540-30133-2_32	computer vision;mathematical optimization;quarter-pixel motion;motion estimation;block-matching algorithm;motion field;computer graphics (images)	Robotics	47.89321874308795	-19.242876293461823	163613
a2efb4d4d11d265795774952da2fa42410949891	data hiding based quality access control of digital images using adaptive qim and lifting	quantization index modulation;data hiding;discrete wavelet transform;qim;low loss;image simulation;data modulation;minimum distance;information embedding;peak signal to noise ratio;image quality;indexation;access control;digital image;weight function;security;structural similarity;lifting	This paper proposes a joint data-hiding and data modulation scheme to serve the purpose of quality access control of image(s) using quantization index modulation (QIM). The combined effect of external information embedding and data modulation cause visual degradation and may be used in access control through reversible process. The degree of deterioration depends on the amount of external data insertion, amount of data modulation as well as step size used in QIM. A weight function (q) and the quantization step size (Db) of JPEG 2000 are used for defining the step size of QIM. Lifting based discrete wavelet transform (DWT), instead of conventional DWT, is used to decompose the original image in order to achieve advantages, namely low loss in image quality due to QIM, better watermark decoding reliability and high embedding capacity for a given embedding distortion. At the decoder, data are demodulated first and watermark bits are then extracted using minimum distance decoding. Extracted watermark is used to suppress self-noise (SN) that provides better quality of image. Simulation results have shown that an improvement in peak-signal-to-noise-ratio (PSNR), mean structural-similarity-index-measure (MSSIM) and Watson distance by an amount of about 30%, 12% and 77%, respectively, are gained by authorized user when 50% coefficients of high–high (HH) coefficients are modulated. & 2011 Elsevier B.V. All rights reserved.	access control;application-specific integrated circuit;authorization;coefficient;decoding methods;digital image;discrete wavelet transform;distortion;dither;dynamic range;elegant degradation;field-programmable gate array;histogram equalization;human visual system model;image processing;image quality;jpeg 2000;lifting scheme;lossy compression;modulation;peak signal-to-noise ratio;sampling (signal processing);signal processing;simulation;weight function	Amit Phadikar;Santi Prasad Maity	2011	Sig. Proc.: Image Comm.	10.1016/j.image.2011.07.008	image quality;computer vision;weight function;speech recognition;peak signal-to-noise ratio;telecommunications;computer science;information security;access control;theoretical computer science;structural similarity;mathematics;information hiding;discrete wavelet transform;digital image;statistics	Vision	41.122514672273965	-12.422494334665314	163673
1dabc3492933ae009815bf4f84ce12cc8f27b3ca	gamut compression algorithm development on the basis of observer experimental data	compression algorithm		algorithm;data compression	Byoung-Ho Kang;Maeng-Sub Cho;Ján Morovic;M. Ronnier Luo	2000			computer science;computer vision;experimental data;artificial intelligence;data compression;observer (quantum physics);theoretical computer science;texture compression;gamut;lossless compression	ML	41.69097804099771	-16.36737619657002	163731
2a462a4d1bf8f7b6112c488e106e273f6aabbe07	an integrated technique for video watermarking	discrete wavelet transforms;watermarking discrete wavelet transforms spread spectrum communication computer vision brightness robustness motion detection humans visual system resists;watermarking;frame shuttle video watermarking image blockwhich robustness discrete wavelet transform spread spectrum feature blocks nonfeature blocks linear transformations frame reduction;discrete wavelet transform;spread spectrum;feature blocks;nonfeature blocks;frame reduction;resists;computer vision;brightness;video coding;spread spectrum communication;watermarking discrete wavelet transforms video coding;human visual system;image blockwhich;linear transformations;linear transformation;robustness;humans;frame shuttle;digital video;just noticeable difference;video watermarking;visual system;motion detection	The major goal of this study is to embed watermarks into a video file. The proposed scheme embeds watermarks into image block which contains features with high intensity, high texture, and fast motion, and improves the robustness of the embedded watermarks based on spatial domain. Since human visual system can't sense the variations of these feature regions within the video frames, an adaptive watermarking technique was developed to embed watermark into these regions. To this end, a digital video is first divided into various frames consisting of several blocks which are transformed by discrete wavelet transform. Spread spectrum incorporated with just notice difference is utilized to embed the watermark into the feature blocks or nonfeature blocks. The proposed method can resist attacks performed by linear transformations including average, frame reduction, and frame shuttle, and obtain better performance in comparison with traditional schemes.	digital video;digital watermarking;discrete wavelet transform;embedded system;gaussian blur;image processing;image quality;image scaling;jpeg;median filter;moving picture experts group;sensor;watermark (data file)	Chien-Chuan Ko;Bo-Zhi Yang	2007	6th IEEE/ACIS International Conference on Computer and Information Science (ICIS 2007)	10.1109/ICIS.2007.59	computer vision;mathematics;multimedia;computer graphics (images)	Vision	41.880231374596164	-12.534978696479348	163802
0b07c963867603b419d1deb8c68903cdb82a6fcc	distributed source coding: theory and applications	source coding;dsc theory;distributed source coding;practical code designs;decoding;correlation	Distributed source coding (DSC) refers to separate compression and joint decompression of mutually correlated sources. Though theoretical foundations were set more than thirty years ago, driven by applications such as wireless sensor networks, video surveillance, and multiview video, DSC has over the past few years become a very active research area. This paper provides an introduction to DSC theory, practical code designs and applications, and outlines current research trends while identifying challenges and opportunities in both theory and practice of DSC.	closed-circuit television;data compression;distributed source coding	Vladimir Stankovic;Lina Stankovi&#x0107;;Samuel Cheng	2010	2010 18th European Signal Processing Conference		distributed source coding;electronic engineering;telecommunications;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;tunstall coding;context-adaptive binary arithmetic coding;multiview video coding	EDA	42.326776371194654	-20.176743432017517	163858
be737f514b52fd2c3084ff007d79123cfb886f9f	analysis of trellis quantization for near-lossless image coding	optimisation;image coding;convergence of numerical methods;lossless image compression;quantisation signal;prediction theory;image analysis quantization image coding image reconstruction testing pixel predictive coding computational efficiency zinc entropy;optimisation quantisation signal image coding prediction theory convergence of numerical methods;predictive coding;prediction context modelling trellis quantization near lossless image coding algorithm two row joint optmizations computation time coding gains progressive probability update convergence lossy coding context modelling computational complexity near lossless residual coding	In near-lossless image coding the original and the reconstruction image are only allowed to differ by grey levels ineachpixel, where is a pregiven threshold. In a predictive coding scheme the simplest way to guarantee this L1 bound is to perform a uniform quantization of the prediction residues with quantizer bin size matching the required tolerance. The near-lossless version of CALIC[1] uses this mechanism. Since quantization is inside the DPCM loop, changing a pixel value at the current position affects subsequent predictions of forthcoming pixels. The trellis quantization (TQ) scheme proposed in [2] tries to take into account those global implications of quantization. However, despite its high computational cost the TQ scheme is inferior to the near-lossless CALIC. For a one-dimensional signal s 2 Zn one has to find the reconstruction signal ŝ 2 N (s) := ft 2 Znj kt sk1 g that can be coded most efficiently, i.e., whose corresponding sequence of prediction residuals has minimal entropy. The TQ scheme provides a local optimum solution to this problem in an iterative fashion. The method is extended to the 2-dimensional case by using the planar predictor and by doing the optimization for each row seperately. The context modelling used in [2] only involves the 3 pixels that are also used for the prediction. Our study was motivated by the following questions: Is the iterative method unable to find a ' near-optimal' solution? Or is the simplicity of the prediction/context-modelling scheme itself the reason for the inferior performance? In this poster we discuss several variations to the original algorithm. We have extended the TQ scheme by performing two-row joint optimizationsinstead of optimizing row by row. Unfortunately, while increasing the computation time quite a bit, this has lead only to marginal coding gains. Aprogressive probability update scheme has lead to much better convergence and to a 0.3 bpp gain over the original fixed scheme. When using lossy plus near-lossless coding the lossy version can be used for better context modelling without increasing the computational complexity of the near-lossless residual coding. Improvements of 0.1–0.2 bpp were observed. Since it is computationally infeasible to include more pixels to be determined by the TQ process, one has the choice of either using better prediction/context-modelling or doing TQ. Our tests indicate that the preference should be given to sophiticated prediction/modelling. A full version of this paper including coding results for various standard test images and another near-lossless coding technique can be obtained via [3].	algorithm;algorithmic efficiency;computation;computational complexity theory;grayscale;iterative method;kerrison predictor;local optimum;lossless compression;lossy compression;marginal model;mathematical optimization;pixel;quantization (signal processing);standard test image;time complexity;trellis quantization	Hannes Hartenstein;Xiaolin Wu	1998		10.1109/DCC.1998.672288	data compression;mathematical optimization;shannon–fano coding;harmonic vector excitation coding;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;machine learning;coding tree unit;mathematics;lossless compression;tunstall coding;context-adaptive binary arithmetic coding;algorithm;statistics	AI	48.689267366718624	-17.548190817646347	164513
af9869e25555ce37d4e2737a84e36ae609523b99	blocking artifacts reduction in image compression with block boundary discontinuity criterion	minimisation;transform coding blocking artifacts reduction image compression block boundary discontinuity transform coefficients quantization pixel values transform domain dct based compressed images;minimization;discontinuity;discontinuite;transformation cosinus;disturbance rejection;image coding;image processing;data compression;procesamiento imagen;minimizacion;rejet perturbation;transform coding;indexing terms;traitement image;reduccion ruido;quantisation signal;codificacion;image compression;discrete cosine transforms;noise reduction;image coding finite impulse response filter low pass filters quantization transform coding noise reduction humans discrete transforms discrete cosine transforms visual system;transformacion coseno;coding;reduction bruit;recuazamiento pertubacion;discontinuidad;compresion dato;cosine transform;reduction method;discrete cosine transforms image coding data compression quantisation signal transform coding;blocking artifact;compression donnee;codage	This paper proposes a novel blocking artifacts reduction method based on the notion that the blocking artifacts are caused by heavy accuracy loss of transform coefficients in the quantization process. We define the block boundary discontinuity measure as the sum of the squared differences of pixel values along the block boundary. The proposed method compensates for selected transform coefficients so that the resultant image has a minimum block boundary discontinuity. The proposed method does not require a particular transform domain where the compensation should take place; therefore, an appropriate transform domain can be selected at the user's discretion. In the experiments, the scheme is applied to DCT-based compressed images to show its performance.	image compression;reflections of signals on conducting lines	Byeungwoo Jeon;Jechang Jeong	1998	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.678634	data compression;computer vision;minimisation;discrete mathematics;transform coding;index term;image processing;image compression;computer science;discontinuity;discrete cosine transform;noise reduction;mathematics;coding;algorithm;statistics	EDA	45.787541936406605	-13.703935555181936	164678
4eb37fccd998165426917a7e75cad7609046d087	perceptual dft watermarking with improved detection and robustness to geometrical distortions	blind detection method perceptual dft watermarking robustness geometrical distortion exchanged digital content intellectual property rights watermarking techniques objective quality metrics dft watermarking scheme optimal visibility noise like square patch fourier domain amplitude component watermark strength phase component human visual system hvs contrast sensitivity function csf contrast pooling optimal strength;watermarking;watermarking robustness visualization sensitivity correlation gratings computational modeling;perceptual dft watermarking robustness geometrical distortion exchanged digital content intellectual property rights watermarking techniques objective quality metrics dft watermarking scheme optimal visibility noise like square patch fourier domain amplitude component watermark strength phase component human visual system hvs contrast sensitivity function csf contrast pooling optimal strength blind detection method;gratings;sensitivity;visualization;computational modeling;robustness;correlation;watermarking discrete fourier transforms object detection;discrete fourier transforms object detection watermarking	More than ever, the growing amount of exchanged digital content calls for efficient and practical techniques to protect intellectual property rights. During the past two decades, watermarking techniques have been proposed to embed and detect information within these contents, with four key requirements at hand: robustness, security, capacity, and invisibility. So far, researchers mostly focused on the first three, but seldom addressed the invisibility from a perceptual perspective and instead mostly relied on objective quality metrics. In this paper, a novel DFT watermarking scheme featuring perceptually optimal visibility versus robustness is proposed. The watermark, a noise-like square patch of coefficients, is embedded by substitution within the Fourier domain; the amplitude component adjusts the watermark strength, and the phase component holds the information. A perceptual model of the human visual system (HVS) based on the contrast sensitivity function (CSF) and a local contrast pooling is used to determine the optimal strength at which the mark reaches the visibility threshold. A novel blind detection method is proposed to assess the presence of the watermark. The proposed approach exhibits high robustness to various kinds of attacks, including geometrical distortions. Experimental results show that the robustness of the proposed method is globally slightly better than state-of-the-art. A comparative study was conducted at the visibility threshold (from subjective data) and showed that the obtained performances are more stable across various kinds of content.	coefficient;digital recording;digital watermarking;distortion;embedded system;human visual system model;performance;requirement;robustness (computer science)	Matthieu Urvoy;Dalila Goudia;Florent Autrusseau	2014	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2014.2322497	computer vision;visualization;sensitivity;digital watermarking;computer science;theoretical computer science;mathematics;computational model;correlation;robustness	Security	41.74163059476032	-10.278715504416699	164814
b6946c7892070976cf85d6ab83e2712f92bc0ad3	a new bitstream random access scheme using multipicture motion-compensated prediction	video coding standards;coding efficiency;coding efficiency bitstream random access scheme multipicture motion compensated prediction compression performance video coding standards video bitstream predictive pictures recovery picture;video streaming;video coding decoding automatic voltage control video on demand standards development video compression iec standards iso standards streaming media tv broadcasting;error concealment;standards;psnr;data compression;motion compensation;decoding;video bitstream;decoding refresh;compression performance;multipicture motion compensated prediction;predictive pictures;video coding;video streaming data compression motion compensation video coding;automatic voltage control;mobile communication;bitstream random access scheme;decoding refresh random access recovery picture multipicture motion compensated prediction error concealment;motion compensated prediction;encoding;recovery picture;random access	Multipicture motion-compensated prediction is exploited to improve compression performance in the up-to-date video coding standards. However, in order to support random access into video bitstream, a restriction on multipicture motion-compensated prediction is introduced: the number of reference pictures has to be limited so that the pictures prior to random access point (RAP) are not allowed to be referenced by those predictive pictures following the RAP. This restriction leads to a decrease of coding efficiency. In this paper, a new kind of picture for reference, the recovery picture, is proposed to compensate the decrease of coding efficiency by taking full use of multipicture motion-compensated prediction. Furthermore, the proposed scheme supports random access without the assistance of the transmitting end. Some factors affecting the proposed schemeiquests performance are discussed. The experimental results show that the proposed random access scheme achieves better compression performance compared with the existing random access scheme.	algorithmic efficiency;bitstream;data compression;digital video broadcasting;digital video recorder;encoder;error concealment;fast forward;h.264/mpeg-4 avc;image;motion compensation;p (complexity);propagation of uncertainty;random access;rapid refresh;software propagation;transmitter;video coding format;wireless access point	Yongbing Lin;Philipp Zhang	2009	IEEE Transactions on Consumer Electronics	10.1109/TCE.2009.5174438	data compression;real-time computing;mobile telephony;peak signal-to-noise ratio;computer science;theoretical computer science;multimedia;algorithmic efficiency;motion compensation;algorithm;random access;encoding	Vision	46.59870364891338	-18.052489052131307	164878
ed1623e3d1d3bdaf030ff94d045d4425ba2a3865	weighted prediction in the h.264/mpeg avc video coding standard	nonnormative method;h 264 standardization;additive offset;evaluation performance;standards video coding data compression prediction theory motion compensation;fading;weighted prediction tool;standards;performance evaluation;data compression;motion compensation;normalisation;evaluacion prestacion;explicit mode;video compression;slice header;video sequences;evanouissement;bitrate reductions mpeg avc video coding standard weighted prediction tool coding fades main profiles extended profiles multiplicative weighting factor additive offset motion compensated prediction explicit mode slice header reference picture index implicit mode picture order count h 264 standardization nonnormative method coding efficiency improvement fade to black sequences;main profiles;bit rate;compensation mouvement;coding fades;video coding;codificacion;automatic voltage control;prediction theory;fade to black sequences;mpeg avc;coding efficiency improvement;indexation;bidirectional control;extended profiles;coding;normalizacion;bitrate reductions;picture order count;reference picture index;mpeg standards;desvanecimiento;motion compensated prediction;video coding standard;proposals;implicit mode;standardization;automatic voltage control video coding video compression fading standardization bit rate video sequences mpeg standards bidirectional control proposals;multiplicative weighting factor;codage	H.264/MPEG AVC is the first international video coding standard to include a weighted prediction (WP) tool, which is particularly useful for coding fades. In the H.264 WP tool, which is included in its main and extended profiles, a multiplicative weighting factor and an additive offset are applied to the motion compensated prediction. In explicit mode, a weighting factor and offset may be coded in the slice header for each allowable reference picture index. In implicit mode, the weighting factors are not coded but are derived based on the relative picture order count (POC) distances of the two reference pictures. In this paper, H.264's standardization of the WP tool is described, and a non-normative method for calculating weighting factors is proposed. Experimental results are provided which measures the coding efficiency improvement using WP. When coding fade-to-black sequences, bitrate reductions of up to 67% were achieved.	algorithmic efficiency;data compression;h.264/mpeg-4 avc;utility functions on indivisible goods;video coding format	Jill M. Boyce	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)	10.1109/ISCAS.2004.1328865	data compression;electronic engineering;telecommunications;computer science;context-adaptive variable-length coding;mathematics;multimedia;statistics	Embedded	47.539516581514	-17.376027243163428	165230
1b20932f6d71c9053abb74a2145a595c3c7b74dc	directionlets using in-phase lifting for image representation	image coding;wavelet transforms image coding image reconstruction image representation;image segmentation lattices vectors image coding switches wavelet transforms;journal article;drntu engineering computer science and engineering computing methodologies image processing and computer vision;wavelet transforms;adaptive directional wavelet transform in phase lifting image representation multidirectional anisotropic spatial segmentation image segments processing image coding de blocking filter adaptive directional lifting;image representation;image reconstruction;image coding directionlets directional dwt directional lifting image representation	Directionlets allow a construction of perfect reconstruction and critically sampled multidirectional anisotropic basis, yet retaining the separable filtering of standard wavelet transform. However, due to the spatially varying filtering and downsampling direction, it is forced to apply spatial segmentation and process each segment independently. Because of this independent processing of the image segments, directionlets suffer from the following two major limitations when applied to, say, image coding. First, failure to exploit the correlation across block boundaries degrades the coding performance and also induces blocking artifacts, thus making it mandatory to use de-blocking filter at low bit rates. Second, spatial scalability, i.e., minimum segment size or the number of levels of the transform, is limited due to independent processing of segments. We show that, with simple modifications in the block boundaries, we can overcome these limitations by, what we call, in-phase lifting implementation of directionlets. In the context of directionlets using in-phase lifting, we identify different possible groups of downsampling matrices that would allow the construction of a multilevel transform without forcing independent processing of segments both with and without any modifications in the segment boundary. Experimental results in image coding show objective and subjective improvements when compared with the directionlets applied independently on each image segment. As an application, using both the in-phase lifting implementation of directionlets and the adaptive directional lifting, we have constructed an adaptive directional wavelet transform, which has shown improved image coding performance over these adaptive directional wavelet transforms.	blocking (computing);deblocking filter;decimation (signal processing);image segmentation;lambda lifting;lifting scheme;mandatory - hl7definedroseproperty;morphologic artifacts;sampling - surgical action;scalability;wavelet transform	Dakala Jayachandra;Anamitra Makur	2014	IEEE Transactions on Image Processing	10.1109/TIP.2013.2288912	iterative reconstruction;computer vision;second-generation wavelet transform;theoretical computer science;pattern recognition;mathematics;wavelet transform	Vision	44.54333450914751	-15.454715793359947	165257
4784f3570fc21c5f81b89940f502d0f600966bc4	mpeg-2 test stream for static picture in digital television system	digital television		mpeg-2	Soo-Wook Jang;Gwang-Soon Lee;Eun-Su Kim;Chan-Ho Han;Sung-Hak Lee;Man-Sik Park;Kyu-Ik Sohng	2005			internet television;multimedia;mpeg-2;digital television;computer graphics (images);computer science;terrestrial television	EDA	43.15193204521531	-21.38997551629971	165345
561fb5e5d779e002bc172ca3ce5ee63714b94b68	toward enhancing the distributed video coder under a multiview video codec framework	computer programming;video coding;error control coding;video	The advance of video coding technology enables multiview video (MVV) or three-dimensional television (3-D TV) display for users with or without glasses. For mobile devices or wireless applications, a distributed video coder (DVC) can be utilized to shift the encoder complexity to decoder under the MVV coding framework, denoted as multiview distributed video coding (MDVC). We proposed to exploit both inter- and intraview video correlations to enhance side information (SI) and improve the MDVC performance: (1) based on the multiview motion estimation (MVME) framework, a categorized block matching prediction with fidelity weights (COMPETE) was proposed to yield a high quality SI frame for better DVC reconstructed images. (2) The block transform coefficient properties, i.e., DCs and ACs, were exploited to design the priority rate control for the turbo code, such that the DVC decoding can be carried out with fewest parity bits. In comparison, the proposed COMPETE method demonstrated lower time complexity, while presenting better reconstructed video quality. Simulations show that the proposed COMPETE can reduce the time complexity of MVME to 1.29 to 2.56 times smaller, as compared to previous hybrid MVME methods, while the image peak signal to noise ratios (PSNRs) of a decoded video can be improved 0.2 to 3.5 dB, as compared to H.264/AVC intracoding.	codec	Shih-Chieh Lee;Jiann-Jone Chen;Yao-Hong Tsai;Ching-Hua Chen	2016	J. Electronic Imaging	10.1117/1.JEI.25.6.063022	video compression picture types;scalable video coding;computer vision;real-time computing;video;h.263;uncompressed video;computer science;deblocking filter;video tracking;coding tree unit;computer programming;multimedia;video processing;smacker video;motion compensation;h.261;multiview video coding	Vision	45.312882008772306	-18.620537325790362	165539
de241e893a8321b4c47cbfe1decfcd4dbfad1ebc	variable tree size fractal compression for wavelet pyramid image coding	transformation ondelette;hierarchical data structure;transformacion discreta;image coding;discrete wavelet transform;image processing;hybrid coding;compresion senal;procesamiento imagen;wavelet decomposition;qualite image;traitement image;compression signal;codificacion;wavelet transform;image compression;image representation;affine transformation;image quality;tree structure;estructura datos;signal compression;fractal coding;coding;discrete transformation;fractal;compression ratio;structure donnee;calidad imagen;transformacion ondita;data structure;transformation discrete;wavelet transformation;codage	Pyramidal wavelet decomposition provides a hierarchical data structure for image representation which is suitable for further quantization and compression. Through discrete wavelet transform, image signals are decomposed into multiresolution and multi-frequency subbands with a set of tree-structured coefficients. The coefficients which have the same spatial location but with different resolution and different orientation can be organized into wavelet subtree. This efficient representation of image signals has achieved superior coding performance in wavelet-based image compression. In this paper, a novel variable tree partition algorithm is introduced which can efficiently split the wavelet subtrees according to the local details. Then a new variable size wavelet-subtree-based fractal coding algorithm is proposed to obtain a good trade-off between image quality and compression ratio. The self-similarities among wavelet subtrees are successfully exploited in the fractal coding method by predicting the coefficients at finer scales from those at coarser scales through proper affine transformation. Experimental results show a gain over JPEG of 5–6 dB in PSNR with even a slightly higher compression ratio (around 60 : 1 to 70 : 1). A slight gain in terms of coding efficiency is also achieved when compared to a method proposed by Davis, which also applies variable tree size wavelet fractal coding, but the complexity has been significantly reduced by avoiding iterative optimization procedures.	fractal compression;wavelet	Lai-Man Po	1999	Sig. Proc.: Image Comm.	10.1016/S0923-5965(98)00008-3	data compression;image quality;wavelet;computer vision;combinatorics;discrete mathematics;data structure;fractal;second-generation wavelet transform;image processing;image compression;computer science;compression ratio;cascade algorithm;affine transformation;mathematics;wavelet packet decomposition;stationary wavelet transform;fractal transform;tree structure;coding;discrete wavelet transform;fractal compression;set partitioning in hierarchical trees;algorithm;wavelet transform	Vision	45.79590961739868	-13.996085628284767	165565
26bef2ef912e469e68b655d1a96dbe7baba88de0	efficient bit allocation and rate control algorithms for hierarchical video coding	cuantificacion senal;estensibilidad;teletrafic;theorie vitesse distorsion;quantization;hierarchical structure;evaluation performance;rate distortion;quantum effect;circuit codeur;complexity theory;coding circuit;performance evaluation;video signal processing;debit information;hierarchized structure;linear r q model bit allocation algorithm rate control algorithm hierarchical video coding hierarchical based encoder;flow rate regulation;information transmission;evaluacion prestacion;gestion trafic;simulation;modele lineaire;traffic control;video quality;codec video;structure hierarchisee;simulacion;modelo lineal;traffic management;bit rate;effet quantique;qualite image;indice informacion;video codec;algorithme;rate distortion theory;asignacion bit;allocation bit;rate control bit allocation hierarchical video coding r q model;algorithm;rate control;video coding;teletrafico;signal quantization;codage video;regulation debit;linear r q model;image quality;quantification signal;circuito codificacion;linear model;teletraffic;bit rate video coding scalability quadratic programming automatic voltage control video codecs static var compensators permission quantization rate distortion;gestion trafico;quantization parameter;traitement signal video;information rate;rate control algorithm;video codecs;predictive models;calidad imagen;extensibilite;scalability;transmision informacion;bit allocation;regulation trafic;transmission information;hierarchical based encoder;efecto cuantico;hierarchical video coding;encoding;r q model;estructura jerarquizada;regulacion trafico;regulacion caudal;algoritmo;bit allocation algorithm	Hierarchical structure is a useful tool for providing the necessary scalability in adapting to the variety of channel environments. For schemes involving hierarchical picture structures, bit allocation, and rate control algorithms are vital components for improving video codec performance. Since conventional bit allocation schemes do not efficiently consider the hierarchical structure characteristics, it is difficult to optimize the video quality at an arbitrary bitrate. Similarly, conventional quantization parameter decision methods are not appropriate for controlling the bitrate generated by a codec using a hierarchical encoding structure. In this paper, we propose an effective bit allocation scheme that assigns the target number of bits to pictures or macroblocks (MBs) and improves the overall quality of images encoded by a hierarchical-based encoder. A rate control scheme is also proposed to ensure that the generated bitrate is equal to the assigned target bitrate. From the simulation results, the proposed schemes outperformed conventional methods from a rate-distortion perspective, by efficiently controlling the bitrate of the MB unit. The algorithms regulated the generated bits to achieve the target bits by using the proposed linear R-Q model.	algorithm;codec;data compression;distortion;encoder;macroblock;scalability;simulation	Chan-Won Seo;Jung Won Kang;Jong-Ki Han;Truong Q. Nguyen	2010	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2010.2057011	image quality;quantum hall effect;average bitrate;throughput;active traffic management;real-time computing;scalability;rate–distortion theory;quantization;telecommunications;computer science;video quality;theoretical computer science;machine learning;linear model;predictive modelling;variable bitrate;constant bitrate;algorithm;encoding;statistics	Networks	47.4621806274346	-15.350965780333432	165579
aafc55ac2703b4adfee5fb4db03c00e6e555f57b	s*-tree: an improved s+-tree for coloured images	image storage;base donnee;quad tree;binary image;quad arbol;database;base dato;stockage image;spatial database;arbol binario;estructura datos;arbre binaire;quad arbre;structure donnee;imagen color;access method;data structure;spatial access method;almacenamiento imagen;image couleur;color image;binary tree	In this paper we propose and analyze a new spatial access method, namely the S∗-tree, for the efficient secondary memory encoding and manipulation of images containing multiple non-overlapping features (i.e., coloured images). We show that the S∗-tree is more space efficient than its precursor, namely the S-tree, which was explicitly designed for binary images, and whose straightforward extension to coloured images can lead to large space wastage. Moreover, we tested time efficiency of the S∗-tree in answering classical window queries, comparing it against a previous efficient access method, namely the HL-quadtree [7]. Our experiments show that the S∗-tree can reach up to a 30% of time saving.	auxiliary memory;binary image;computer data storage;computer vision;data structure;enrico clementi;experiment;graphics;horseland;image processing;information system;lecture notes in computer science;ocean observatories initiative;paging;pixel;pointer (computer programming);quadtree;spatial database;springer (tank)	Enrico Nardelli;Guido Proietti	1999		10.1007/3-540-48252-0_12	computer vision;color image;data structure;binary image;binary tree;computer science;quadtree;database;mathematics;programming language;access method;spatial database;computer graphics (images)	Vision	39.39237151280973	-19.133256331184647	165623
aaee9d46a4e7190af4e9dff20e810c830bfc31ab	planning for provably reliable navigation using an unreliable, nearly sensorless robot	navigation;planning	This paper addresses a navigation problem for a certain type of simple mobile robot modeled as a point moving in the plane. The only requirement on the robot is that it must be able to translate in a desired direction, with bounded angular error (measured in a global reference frame), until it reaches the nearest obstacle in its motion direction. One straightforward realization of this capability might use a noisy compass and a contact sensor. We present a planning algorithm that enables such a robot to navigate reliably through its environment. The algorithm constructs a directed graph in which each node is labeled with a subset of the environment boundary. Each edge of the graph is labeled with a sequence of actions that can move the robot from any location in one such set to some location in the other set. We use a variety of local planners to generate the edges, including a “corner-finding” technique that allows the robot to travel to and localize itself at a convex vertex of the environment boundary. The algorithm forms complete plans by searching the resulting graph. We have implemented this algorithm and present results from both simulation and a proof-of-concept physical realization.	algorithm;angularjs;automated planning and scheduling;directed graph;mobile robot;motion planning;reference frame (video);simulation	Jeremy S. Lewis;Jason M. O'Kane	2013	I. J. Robotics Res.	10.1177/0278364913488428	planning;computer vision;visibility graph;navigation;simulation;computer science;mathematics;distributed computing;mobile robot navigation	Robotics	53.68388469239241	-23.29645854613086	165971
23bb932c32f37efc81996118f3c439c39e6a8b13	super-resolution of video using key frames and motion estimation	scalable video coding;reversed complexity coding;image resolution;decoding;low pass;reversed complexity video coding;motion estimation;indexing terms;normal resolution;strontium;normal resolution scalable video coding video super resolution key frames motion estimation frame down sampling reversed complexity coding image enhancement layers band pass versions up sampling processes recovering processes;video coding image enhancement image resolution motion estimation;band pass;video coding;image enhancement;enhancement layer;motion estimation spatial resolution decoding video coding energy resolution image quality encoding strontium video compression quantization;image enhancement layers;motion estimation super resolution reversed complexity video coding key frames;video super resolution;super resolution;key frames;frame down sampling;encoding;up sampling processes;high frequency;recovering processes;band pass versions;spatial resolution	Many scalable video coding systems use frame down- sampling in order to reduce complexity and to enable enhancement layers. Super-resolution (SR) can be used to help the up-sampling and recovering processes of those frames. We are interested in reversed-complexity (distributed) coding methods, wherein few key frames are encoded at normal resolution, while the rest are down- sampled and encoded at reduced resolution along with the enhancement layers. We are only interested on the decoder side, wherein we carry motion estimation of the down- sampled frames using the key frames as references. When a match is made, the high-frequency components of the key frames (KF) are used to super-resolve the non-key frames (NKF). The motion estimation process is performed using blocks of band-pass versions of the frames, rather than low- pass ones. Results indicate the improved performance of the proposed super-resolution algorithm.	algorithm;data compression;kalman filter;key frame;motion estimation;network kanji filter;sampling (signal processing);sampling in order;scalability;scalable video coding;super-resolution imaging	Fernanda Brandi;Ricardo L. de Queiroz;Debargha Mukherjee	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711756	reference frame;computer vision;image resolution;computer science;motion estimation;multimedia;motion compensation;computer graphics (images)	Vision	45.22640049691468	-17.23362327413571	166297
68522ea27888b80faec6d5035fad911f88d63c46	image coding by block prediction of multiresolution subimages	transformation ondelette;wavelet analysis;orientation localisation;block prediction;image coding image resolution fractals wavelet transforms signal resolution redundancy image recognition wavelet analysis iterative decoding image reconstruction;block search;image recognition;fractals;image coding;multiresolution image decomposition;iterative decoding;image processing;image resolution;decodage;decoding;fractal images;methode echelle multiple;image matching;subband decomposition;self similarity;image classification;metodo escala multiple;multiresolution representation;fractals image coding image resolution image representation image reconstruction image classification image matching decoding prediction theory;causal similarity;experimental results multiresolution subimages block prediction image coding multiresolution representation redundancy fractal images fractal block coders self similarity image coder causal similarity multiresolution image decomposition pyramid subband decomposition orientation localisation space localisation prediction maps iterative mapping decoding mean squared error automatic block classifier block search block matching;traitement image;experimental result;iterative mapping;wavelet transforms;codificacion;redundancy;prediction theory;prediction maps;image representation;image reconstruction;space localisation;coding;resultado experimental;image coder;fractal;signal resolution;fractal block coders;block matching;code bloc;multiscale method;decomposition sous bande;automatic block classifier;resultat experimental;experimental results;block codes;codage;multiresolution subimages;mean squared error;pyramid subband decomposition	The redundancy of the multiresolution representation has been clearly demonstrated in the case of fractal images, but it has not been fully recognized and exploited for general images. Fractal block coders have exploited the self-similarity among blocks in images. We devise an image coder in which the causal similarity among blocks of different subbands in a multiresolution decomposition of the image is exploited. In a pyramid subband decomposition, the image is decomposed into a set of subbands that are localized in scale, orientation, and space. The proposed coding scheme consists of predicting blocks in one subimage from blocks in lower resolution subbands with the same orientation. Although our prediction maps are of the same kind of those used in fractal block coders, which are based on an iterative mapping scheme, our coding technique does not impose any contractivity constraint on the block maps. This makes the decoding procedure very simple and allows a direct evaluation of the mean squared error (MSE) between the original and the reconstructed image at coding time. More importantly, we show that the subband pyramid acts as an automatic block classifier, thus making the block search simpler and the block matching more effective. These advantages are confirmed by the experimental results, which show that the performance of our scheme is superior for both visual quality and MSE to that obtainable with standard fractal block coders and also to that of other popular image coders such as JPEG.	automatic differentiation;causal filter;coder device component;fractal;iteration;jpeg;jump search;map;mean squared error;multiresolution analysis;self-similarity	Roberto Rinaldo;Giancarlo Calvagno	1995	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.392333	computer vision;discrete mathematics;fractal;image processing;pattern recognition;mathematics;statistics	Vision	45.455178897644835	-14.642274952737807	166298
04b435f50cfc9908a989a52962d45005deb23a08	path planning using adaptive burs of free configuration space		This paper presents an adaptive version of the path planning algorithm based on the recently proposed structure called bur of free configuration space. The original planning algorithm — rapidly exploring bur tree (RBT) is based on the multi-directional extension of tree nodes for efficient exploration of free configuration space. A suitable number of directions for extension (extension degree) was empirically determined and has been kept fixed during the algorithm run. This paper investigates the possibility to adapt the extension degree during the algorithm execution in order to further boost the efficiency of the path planner in terms of number of iterations and runtime. Validation study demonstrates that the proposed adaptive version of RBT algorithm (aRBT) outperforms the original algorithm.	adaptive algorithm;automated planning and scheduling;burs;iteration;motion planning;sampling (signal processing);simulation	Bakir Lacevic;Dinko Osmankovic;Adnan Ademovic	2017	2017 XXVI International Conference on Information, Communication and Automation Technologies (ICAT)	10.1109/ICAT.2017.8171616	configuration space;mathematical optimization;motion planning;computer science	Robotics	52.28605035397621	-23.78117688217169	166321
017b5eba63c5b1309a9a3ec9784b19e610c0c677	scalable coding of variable size blocks motion vectors	decoding scalability algorithm design and analysis lattices encoding bit rate motion estimation continuous wavelet transforms streaming media wavelet coefficients;image resolution;motion compensation;decoding;image matching;motion estimation;decoding image representation motion compensation motion estimation video coding quadtrees image matching image resolution wavelet transforms;wavelet transforms;video coding;image representation;motion vector;quadtree structure scalable representation motion field information decoder full resolution scalable wavelet based video coder motion estimation motion compensation variable size block matching algorithm;full resolution;quadtrees;block matching algorithm;scalable coding	In this paper we discuss an algorithm that is able to provide a scalable (multiresolution) representation of the motion field information. It has been recently demonstrated that, for an open loop wavelet video coder, it is possible to use at the decoder side a scaled version of the original motion information and the residual coefficients computed with the full resolution/quality motion field without incurring into drift. We propose a fully scalable wavelet based video coder that performs motion estimation-compensation in the wavelet domain. In particular the coding scheme is specifically designed for variable size block matching algorithms. In this scenario motion vectors are distributed across an irregular lattice according to a quadtree structure. The developed system allows a scalable representation of the motion field and a flexible allocation of the bit budget between motion and residual data. The simulations that we have carried out show, at low bit-rates, a significant gain of the proposed approach with respect to the case in which the motion information is coded lossless.	algorithm;block-matching algorithm;coefficient;lossless compression;lossy compression;motion estimation;motion field;multiresolution analysis;quadtree;scalability;simulation;wavelet	Davide Maestroni;Augusto Sarti;Marco Tagliasacchi;Stefano Tubaro	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1419745	computer vision;image resolution;quarter-pixel motion;computer science;theoretical computer science;motion estimation;mathematics;block-matching algorithm;motion field;motion compensation;wavelet transform;computer graphics (images)	Robotics	45.42921645939206	-17.117951097859848	166775
3c50ffd6031977315ece505e57e9b884935aae8f	improved quantization in multiple description coding by correlating transforms	quantization shape performance gain image coding image reconstruction distortion measurement sensor phenomena and characterization sensor systems ip networks protocols;probability;transform coding;correlation methods;quantisation signal;transform coding correlation methods quantisation signal probability gaussian distribution;multiple description coding;quantization cell multiple description coding multiple bitstream pairwise correlating transform;gaussian distribution;telekommunikation;telecommunications	The objective with multiple description coding (MDC) is to code one source of data into multiple bitstreams. The coding is done in such a way that multiple levels of quality is achieved. This means that even if one or a few of the bitstreams are lost, the received bits should make it possible to get an approximated version of the original data. One way to do this is to use pairwise correlating transforms which introduce correlation between the bitstreams. This correlation can be used in order to get an estimate of a lost stream. In this paper a new approach for MDC using pairwise correlating transforms is presented. In this approach, contrary to previous work, quantization of the source data is performed after the data has been transformed. This makes it possible to improve the shape of the quantization cells and to tailor these to the employed transform. We demonstrate that this offers a substantial performance gain compared with previous approaches to MDC using pairwise correlating transforms.	approximation algorithm;multiple description coding;quantization (signal processing);source data	Niklas Wernersson;Tomas Skollermo;Mikael Skoglund	2004	IEEE 6th Workshop on Multimedia Signal Processing, 2004.	10.1109/MMSP.2004.1436601	normal distribution;transform coding;speech recognition;theoretical computer science;multiple description coding;probability;mathematics;statistics	Vision	48.61993412047845	-12.883395661464595	166865
099df7362fc71669ddc9f3acaea3013c1a71d117	tree-structured oversampled filterbanks as joint source-channel codes: application to image transmission over erasure channels	image sampling;erasure channel;image coding;filter bank;image communication image coding transport protocols streaming media robustness time domain analysis algorithm design and analysis quantization image reconstruction reconstruction algorithms;banc filtre;quantization noise;combined source channel coding;metodo arborescente;visual communication;frames;oversampling;sobremuestreo;time domain analysis;reconstruction image;joint source channel coding;codificacion;image transmission;surechantillonnage;reconstruccion imagen;channel bank filters;image reconstruction;banco filtro;unequal error protection;tree structure;codage joint source canal;analyse performance;multimedia communication;coding;performance analysis;multiple description coding;tree structured method;time domain;methode arborescente;oversampled filterbanks;reconstruction algorithm;quantization noise tree structured oversampled filterbanks joint source channel codes image transmission erasure channels robust transmission multimedia signals bursty erasure pattern erasure recovery algorithm signal space projection;transmission image;image sampling channel bank filters combined source channel coding visual communication multimedia communication noise time domain analysis image coding image reconstruction;bruit quantification;codage;noise;transmision imagen;ruido cuantificacion;erasure resilient coding;analisis eficacia	This paper studies oversampled filterbanks for robust transmission of multimedia signals over erasure channels. Oversampled filterbanks implement frame expansions of signals in l/sup 2/(Z). The dependencies between the expansion coefficients introduced by the oversampled filterbank are first characterized both in the z-domain and in the time-domain. Conditions for recovery of some typical erasure patterns like bursty erasure patterns are derived. The analysis leads to the design of two erasure recovery algorithms that are first studied without quantization noise. The reconstruction algorithm derived from the time-domain analysis exploits the fact that an oversampled filterbank represents signals with more than one set of basis functions. The erased samples are first reconstructed from the received ones, and then, signal space projection is applied. The effect of quantization noise on the reconstructed signal is studied for both algorithms. Using image signals, the theoretical results are validated for a number of erasure patterns, considering unequal error protection enabled tree-structured decompositions.	algorithm;basis function;binary erasure channel;channel capacity;code;coefficient;data compression;domain analysis;filter bank;forward error correction;oversampled binary image sensor;oversampling;peak signal-to-noise ratio;polynomial;quantization (signal processing);sampling (signal processing);simulation	Ravi Motwani;Christine Guillemot	2004	IEEE Transactions on Signal Processing	10.1109/TSP.2004.832027	iterative reconstruction;erasure code;computer vision;binary erasure channel;speech recognition;oversampling;quantization;telecommunications;time domain;computer science;noise;multiple description coding;filter bank;mathematics;tree structure;coding;statistics;visual communication	Visualization	48.35360321568837	-14.638029200196609	166962
b0662206b83d1271c6825d0f78e2634d00940054	perceptual stereoscopic video coding using disparity just-noticeable-distortion model		In this paper, we propose perceptual stereoscopic video coding using a disparity just-noticeabledistortion (JND) model. We obtain the disparity JND model in stereo videos by disparity masking effects of the human visual system (HVS). The disparity JND model represents the maximum distortion of stereo perception that HVS cannot perceive. Based on the disparity JND model, we adjust prediction residuals to remove the perceptual redundancy of stereo videos. Thus, we achieve significant bit-rate saving while maintaining visual quality. Experimental results demonstrate that the proposed method significantly improves coding efficiency without loss of stereoscopic perceptual quality. 1 Index Terms —3DTV, disparity estimation, human visual system (HVS), just-noticeable-distortion (JND), perceptual redundancy, stereo video coding.	algorithmic efficiency;binocular disparity;data compression;distortion;human visual system model;most significant bit;stereoscopic video coding;stereoscopy;triple modular redundancy	Zhendong Zhang;Qingtao Fu;Fei Xue	2017	J. Visual Communication and Image Representation	10.1016/j.jvcir.2017.06.007		Vision	45.132212843288094	-17.81492935424276	167332
2fe1ddd3a27a84aafc1c4868b46e6070e348a84c	noncausal predictive video codec offering hierarchical qos	temporal domain qos;spatial domain qos;scalable video coding scheme;scalable video coding;gaussian processes;h 263 itu standard;video compression;vector quantisation video coding video codecs quality of service gaussian processes markov processes;video sequences;compression ratio noncausal predictive video codec hierarchical qos scalable video coding scheme 3d gauss markov random process replenishment extended vector quantization temporal domain qos spatial domain qos h 263 itu standard;three dimensional;video codec;noncausal predictive video codec;video coding;hierarchical qos;vector quantization;random process;streaming media;video on demand;random processes;compression ratio;video codecs;vector quantizer;markov processes;quality of service;replenishment extended vector quantization;vector quantisation;3d gauss markov random process;video codecs quality of service video compression gaussian processes random processes streaming media video on demand video coding vector quantization video sequences	We present a scalable video coding scheme based on a three dimensional (3D) Gauss Markov random process (GMrp) and replenishment extended vector quantization (VQ). The proposed video codec is capable of offering different quality of service (QoS) in the temporal and spatial domains. Compression results illustrate the superiority of the proposed scheme over the ITU standard, H.263, at high compression ratios.	codec;data compression;markov chain;quadtree;quality of service;scalability;scalable video coding;stochastic process;vector quantization;video coding format	Maria Kouras;Amir Asif	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326650	data compression;scalable video coding;stochastic process;three-dimensional space;real-time computing;quality of service;telecommunications;computer science;theoretical computer science;compression ratio;gaussian process;mathematics;markov process;vector quantization;statistics	Robotics	45.5946098106301	-18.57358006910943	167433
d40838bfad7ceaac78718686f7345a55c033e59e	discrete cosine transformation based image authentication and secret message transmission scheme	histograms;inverse dct;steganography discrete cosine transforms image coding message authentication statistical testing;encoding decoding;image coding;spatial domain;decoding;steganography discrete cosine transformation image authentication secret message transmission multimedia image ubiquitous secret message mobile network spatial domain frequency domain inverse dct encoding decoding histogram analysis chi square test;image chi square dct lsb transmission;transmission;communication systems;image converters;frequency domain analysis;computational intelligence;authentication;histogram analysis;image;secret message transmission;data mining;discrete cosine transform;multimedia image;ubiquitous secret message;chi square;steganography;image authentication;lsb;image transmission;discrete cosine transforms;pixel;decision support systems;chi square test;image analysis;statistical testing;message authentication;discrete cosine transformation;authentication frequency domain analysis discrete cosine transforms image converters pixel encoding decoding steganography histograms image analysis;cosine transform;frequency domain;encoding;dct;mobile network	In this paper a novel “Discrete Cosine Transformation based Image Authentication & Secret Message Transmission Scheme” (DCTIASMTT) has been proposed to authenticate a multimedia image and at the same time some ubiquitous secret message or image can be transmitted over the mobile network. Instead of direct embedding a message or image within the source image, choose a window of size 2x2 of the source image and then convert it from spatial domain to frequency domain using Discrete Cosine Transformation (DCT). The bits of the authenticating message or image are then embedded at LSB of the transformed image (excluding first pixel). Inverse DCT has been performed for the transformation from frequency domain to spatial domain as final step of encoding. Decoding is done through reverse procedure. The experimental results have been discussed and compared with the existing steganographic algorithm S-Tools. Histogram analysis and Chi-Square test of source image with embedded image shows the better results in comparison with STools.	algorithm;authentication;chi;discrete cosine transform;embedded system;inverse transform sampling;pixel;steganography	Debnath Bhattacharyya;Jhuma Dutta;Poulami Das;Samir Kumar Bandyopadhyay;Tai-Hoon Kim	2009	2009 First International Conference on Computational Intelligence, Communication Systems and Networks	10.1109/CICSYN.2009.11	computer vision;feature detection;image analysis;speech recognition;image processing;computer science;theoretical computer science;computational intelligence;discrete cosine transform;mathematics;statistics	Vision	41.453486873210466	-11.99464592328832	167480
4f4f52f69a0a95ff14239c69e579f5d24c8190e7	quad-tree partitioned compressed sensing for depth map coding	pixel domain total variation minimization;minimization;compressed sensing;image coding;codecs;decoding;depth image;sensors;sparse signals;sub nyquist sampling;image reconstruction codecs compressed sensing image coding;depth map coding;inter frame encoding;quad tree partitioned compressed sensing;high quality depth map reconstruction quad tree partitioned compressed sensing depth map coding quad tree decomposition depth image inter frame encoding decoder pixel domain total variation minimization;total variation quad tree decomposition compressed sensing sub nyquist sampling sparse signals depth map;discrete cosine transforms;image reconstruction;high quality depth map reconstruction;total variation;depth map;encoding;decoder;encoding decoding image reconstruction compressed sensing minimization sensors discrete cosine transforms;quad tree decomposition	We consider a variable block size compressed sensing (CS) framework for high efficiency depth map coding. In this context, quad-tree decomposition is performed on a depth image to differentiate irregular uniform and edge areas prior to CS acquisition. To exploit temporal correlation and enhance coding efficiency, such quad-tree based CS acquisition is further extended to inter-frame encoding, where block partitioning is performed independently on the I frame and each of the subsequent residual frames. At the decoder, pixel domain total-variation minimization is performed for high quality depth map reconstruction. Experiments presented herein illustrate and support these developments.	algorithmic efficiency;block size (cryptography);compressed sensing;depth map;display resolution;pixel;quadtree;tree decomposition	Ying Liu;Krishna Rao Vijayanagar;Joohee Kim	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6853721	iterative reconstruction;computer vision;mathematical optimization;codec;computer science;sensor;theoretical computer science;mathematics;decoder;compressed sensing;total variation;encoding;statistics;depth map	Robotics	44.89667887043038	-16.86284568851471	167833
382b59576826c0e10e6548fb3d887adc70e5be7e	an algorithm for transform coding for lossy packet networks	quantization;channel erasure statistics;image coding;source channel coding;minimum average reconstruction error;combined source channel coding;image restoration;transform coding;correlating block;error analysis;protection;karhunen loeve transforms;vectors;discrete transforms;discrete transforms transform coding image coding combined source channel coding karhunen loeve transforms;image reconstruction;discrete karhunen loeve transform;lossy packet networks;multiple description coding;packet networks;transform coding karhunen loeve transforms image reconstruction vectors image coding quantization protection error analysis discrete transforms image restoration;karhunen loeve transform;source channel coding transform coding lossy packet networks discrete karhunen loeve transform multiple description coding correlating block channel erasure statistics gradient algorithm minimum average reconstruction error;gradient algorithm	We propose an algorithm to compute a modification of the classical discrete Karhunen-Loeve Transform (KLT) useful when some of the coefficients are randomly unavailable for reconstruction. Such a scheme can provide Multiple Description Coding (MDC) for signals and images transported by lossy packet links. The modification of the KLT is based on a “correlating” block that, from knowledge of the channel erasure statistics, is optimized with a gradient algorithm to provide minimum average reconstruction error. A set of simulations show appreciable improvements over standard schemes. 1. TRANSFORM CODING WITH ERASURES	algorithm;coefficient;gradient;lossy compression;multiple description coding;network packet;randomness;simulation;transform coding	Francesco Palmieri;Dario Petriccione	2001		10.1109/ICASSP.2001.940531	iterative reconstruction;image restoration;computer vision;transform coding;speech recognition;quantization;telecommunications;computer science;theoretical computer science;multiple description coding;mathematics;karhunen–loève theorem;statistics	Vision	48.62525552387879	-13.111281937426376	167936
8445bf3070ad5b19751794de733fc3996d1145ec	3d videoconferencing system using spatial scalability	audio streaming;teleconferencing;encoding decoding;video streaming;data compression;personal computer;real time;video stream decoding spatial scalability 3d videoconferencing system side by side spatial compression stereoscopic images standard personal computer 3d compatible tv 2d compatibility mode 3d technology video stream encoding audio stream encoding audio stream decoding;video coding;audio coding;video streaming audio coding audio streaming data compression microcomputers stereo image processing teleconferencing video coding video communication;stereo image processing;three dimensional displays streaming media image coding teleconferencing decoding microcomputers tv;video communication;microcomputers;spatial scalability	An implementation of a real-time 3D videoconferencing system using the currently available technology is presented. This approach is based on the side by side spatial compression of the stereoscopic images. The encoder and the decoder have been implemented in a standard personal computer and a conventional 3D compatible TV has been used to present the frames. Moreover, the users without 3D technology can use the system because 2D compatibility mode has been implemented in the decoder. The performance results show that a conventional computer can be used for encoding/decoding audio and video streams and the delay in the transmission is lower than 200 ms.	compatibility mode;data compression;encoder;personal computer;real-time clock;scalability;stereoscopy;streaming media	M. Chavarrias;Fernando Pescador;Fernando Jaureguizar;E. Juarez;Matías J. Garrido	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6161789	data compression;computer vision;teleconference;computer science;operating system;microcomputer;multimedia;statistics;computer graphics (images)	Robotics	43.233938312387565	-21.039006386359812	168017
bb2e18f33fc54b3e6c7193b01a3187af1f01028f	daala: a perceptually-driven still picture codec	image coding;image coding transforms lapping encoding measurement video codecs;measurement;transforms;video codecs;encoding;lapping	Daala is a new royalty-free video codec based on perceptually-driven coding techniques. We explore using its keyframe format for still picture coding and show how it has improved over the past year. We believe the technology used in Daala could be the basis of an excellent, royalty-free image format.	codec;data compression;image file formats;key frame	Jean-Marc Valin;Nathan E. Egge;Thomas J. Daede;Timothy B. Terriberry;Christopher Montgomery	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532322	adaptive multi-rate audio codec;computer vision;codec;h.263;computer science;coding tree unit;multimedia;smacker video;context-adaptive binary arithmetic coding;h.261;encoding;measurement;statistics;multiview video coding;lapping;computer graphics (images)	Robotics	43.653560920145075	-19.493682623158776	168321
6ecad274fb0783930c7e4adc4f7520f25980acfa	mode selective interpolation for stereoscopic 3d video in frame-compatible top-bottom packing	interpolation;top bottom packing structure;frame compatible packing;3d video format	As a transition stage from a conventional 2D TV to a full stereoscopic 3D TV system, a frame-compatible format of fitting stereoscopic left and right images to a single frame of the existing 2D TV is required to utilize existing codec and transmission infrastructure. To meet this requirement, a frame-compatible top-bottom packing with a horizontal line offset is proposed, where the vertical resolutions of the stereoscopic left and right images are reduced by half. Then, the optimal interpolation mode for each line segment of the sub-sampled horizontal line is determined by exploiting parallax-compensated data as well as undeleted neighboring upper and lower horizontal lines. At the receiver, the discarded horizontal lines for the left and right images are reconstructed by the interpolation modes provided by the sender. Experimental results show that the proposed algorithm improves the PSNR as much as 1.5---3dB comparing to conventional interpolation filters.		Chee Sun Won	2013	Multidim. Syst. Sign. Process.	10.1007/s11045-012-0188-1	computer vision;interpolation;mathematics;statistics;computer graphics (images)	Vision	44.7760912147958	-17.904851272984466	168415
9579d680e63b117a9d85e206452c042100e3f9bf	effective coding unit size decision based on motion homogeneity classification for hevc inter prediction	encoding rate distortion software classification algorithms software algorithms support vector machine classification video coding;inter coding;high efficiency video coding coding unit size decision taking algorithm motion homogeneity classification hevc interprediction coding tree unit ctu hevc encoder brute force search through quad tree hierarchy high definition video hd videos rd cost feature vector encoding time efficiency rate distortion performance hevc hm12 0;hevc;video coding;optimization video coding hevc inter coding cu size;cu size;optimization;video coding high definition video image classification image motion analysis prediction theory tree codes	Determining the best partitioning structure for a given Coding Tree Unit (CTU) is one of the most time consuming operations within the HEVC encoder. The brute force search through quad tree hierarchy has a significant impact on the encoding time especially on high definition (HD) videos. This paper presents a fast coding unit size decision-taking algorithm for inter prediction in HEVC. The proposed algorithm uses a motion homogeneity based classification approach utilizing RD cost as a feature vector. Simulation results show that the proposed algorithm achieves an average of 73.25% encoding time efficiency improvement with similar rate distortion performance compared to HEVC HM12.0 reference software.	algorithm;brute-force search;coding tree unit;distortion;encoder;feature vector;high efficiency video coding;quadtree;rate–distortion theory;ruby document format;simulation	Thanuja Mallikarachchi;Warnakulasuriya Anil Chandana Fernando;Hemantha Kodikara Arachchi	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025749	computer vision;real-time computing;quarter-pixel motion;computer science;theoretical computer science;coding tree unit;mathematics;context-adaptive binary arithmetic coding;h.261;multiview video coding	Robotics	46.79778478595257	-19.042867203719528	168483
499636858cd69ff311c324693d73bdeb6af3027b	a new rate control scheme for video coding based on region of interest	encoding distortion copper computational modeling discrete cosine transforms video coding;machine learning video coding vp9 hevc region of interest rate control radial basis function neuron network	The quality fluctuation of video is significant in human visual system, and thus, many rate control schemes are widely developed in the area of video communication. In recent years, researchers show more interests in region of interest (ROI)-based encoding, and it is widely applied in the latest video codecs, such as HEVC and VP9. This paper presents a new rate control scheme for ROI mode coding based on discrete fourier transform coefficient model and radial basis function neuron network. A new R-D model is proposed by classifying blocks into different depth, ROI groups, and so on. Then, rate and distortion are described based on the Laplacian distribution model using mathematical ways. A machine learning approach is induced to enhance the accuracy of the distortion estimation. By utilizing the new R-D model, a new rate control scheme is designed for ROI mode coding from the group of picture layer to coding unit layer. By comparisons with other rate control approaches, the proposed one has a better result in terms of visual quality, R-D performance, bitrate accuracy, and so on. Hence, it outperforms the conventional schemes especially for sequences with obvious ROI details.	codec;coefficient;discrete fourier transform;distortion;high efficiency video coding;machine learning;neuron;quantum fluctuation;radial (radio);radial basis function;region of interest;vp9	Zhewei Zhang;Tao Jing;Jingning Han;Yaowu Xu;Fan Zhang	2017	IEEE Access	10.1109/ACCESS.2017.2676125	scalable video coding;sub-band coding;computer vision;speech recognition;computer science;theoretical computer science;coding tree unit;context-adaptive binary arithmetic coding;h.261;multiview video coding	AI	43.783186321302786	-16.060368126638828	168543
8f3f0bce4217a78c3794185661468f464596b896	scalable compression of 3d medical datasets using a (2d+t) wavelet video coding scheme	coding efficiency;3d subband decomposition;image coding;data compression;motion compensation;decoding;video bitstream;reconstruction quality;video compression;hospitals;advanced video coding;biomedical imaging;medical data;transform coding;adaptive codes;visualization quality;hierarchical trees;visual quality;video codec;motion compensated;wavelet transforms;video coding;wavelet coding;medical image;temporal scalability;medical image processing;data visualization;wavelet coding medical data data compression data access visualization quality diagnosis medical image sequences advanced video coding decoding video bitstream reconstruction quality video codec 3d subband decomposition motion compensation temporal scalability spatial scalability snr scalability compression ratio coding efficiency;data access;compression ratio;scalability;motion compensation image sequences video coding medical image processing data compression wavelet transforms transform coding adaptive codes decoding;medical image sequences;biomedical imaging medical diagnostic imaging scalability decoding hospitals image coding data visualization image sequences video coding video compression;diagnosis;snr scalability;medical diagnostic imaging;spatial scalability;image sequences	Nowadays the all digital solution in hospitals is becoming widespread. The formidable increase of medical data amount to be processed, transmitted and stored, requires some efficient compression systems and innovative tools to improve data access, while ensuring a sufficient visualization quality for diagnosis. Medical image sequences can benefit from advanced video coding techniques when adapted to their specific constraints. Scalability, or the capability to partly decode a video bitstream and to get a reconstruction quality proportional to the decoded amount of information, is a key functionality. We have developed a video codec based on a 3D Motion-Compensated subband decomposition, which provides a combination of temporal, spatial and SNR scalabilities together with a very competitive compression ratio. We show that, when applied to medical sequences, it outperforms JPEG-2000 on coding efficiency aspects and offers new functionalities.	algorithmic efficiency;bitstream;codec;data access;data compression;h.264/mpeg-4 avc;jpeg 2000;lossy compression;scalability;signal-to-noise ratio;wavelet	Marion Benetiere;Vincent Bottreau;Antoine Collet Billon;Thomas Deschamps	2001		10.1109/ISSPA.2001.950199	video compression picture types;data compression;scalable video coding;computer vision;computer science;theoretical computer science;video tracking;mathematics;multimedia;context-adaptive binary arithmetic coding;motion compensation;data visualization;statistics;multiview video coding	Graphics	43.9757384495624	-17.521713510762574	168586
1976ec8db109ed8c9b82824dced9fba017052c29	an adaptive workload management scheme for hevc encoding	full hd sequences adaptive workload management scheme hevc encoding hevc standard encoding process computational complexity user defined operation frequency execution time rate distortion losses;video coding;computational complexity;video coding computational complexity encoding;encoding;video coding hevc workload management complexity control	Managing the complexity of the emerging HEVC standard is a matter of academic and industrial research since its earlier versions. The sophisticated and computation-intensive tools involved in the encoding process must be leveraged if real-time applications are considered. In this paper, we propose a workload management scheme for dynamically controlling the computational complexity of HEVC, under user-defined operation frequency and target FPS. Our scheme receives these two parameters as input and aims to meet the target FPS by adjusting different encoding parameters during execution time. Experiments demonstrate that our scheme successfully meets the target FPS while introducing negligible rate-distortion losses. A comparison with state-of-the-art shows that our scheme is capable of achieving a time reduction of up to 43% for Full HD sequences, with a maximum loss of 0.03 dB in Y-PSNR and a 3.5% increase in bitrate.	computation;computational complexity theory;discounted maximum loss;distortion;floating point systems;high efficiency video coding;peak signal-to-noise ratio;real-time clock;real-time computing;run time (program lifecycle phase)	Mateus Grellert;Muhammad Shafique;Muhammad Usman Karim Khan;Luciano Volcan Agostini;Júlio C. B. de Mattos;Jörg Henkel	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738381	real-time computing;simulation;computer science;theoretical computer science;computational complexity theory;algorithm;encoding	Robotics	47.48645676559906	-19.310736118177683	168630
4b7d219b0db0ea1c495b3cd3f08d140a6214e7e7	multiplication-free fast codeword search algorithm using haar transform with squared-distance measure	haar wavelet;distance measure;search algorithm;vector quantization;image compression;computational complexity;vector quantizer;fast codeword search;computational efficiency	This letter presents novel multiplication-free fast codeword search algorithms for encoding of vector quantizers (VQs) based on squared-distance measure. The algorithms accomplish fast codeword search by performing the partial distance search (PDS) in the wavelet domain. To eliminate the requirement for multiplication, simple Haar wavelet is used so that the wavelet coefficients of codewords are finite precision numbers. The computation of squared distance for PDS can therefore be effectively realized using additions. To further enhance the computational efficiency of the algorithms, the addition-based squared-distance computation is decomposed into a number of stages. The PDS process is then extended to these stages to reduce the addition complexity of the algorithm. In addition, by performing PDS over smaller number of stages, lower computational complexity can be obtained at the expense of slightly higher average distortion for encoding. Simulation results show that our algorithms are very effective for the encoding of VQs, where both low computational complexity and average distortion are desired.	code word;haar wavelet;search algorithm	Wen-Jyi Hwang;Ray-Shine Lin;Wen-Liang Hwang;Chung-Kun Wu	2000	Pattern Recognition Letters	10.1016/S0167-8655(00)00007-6	mathematical optimization;discrete mathematics;image compression;computer science;theoretical computer science;mathematics;computational complexity theory;vector quantization;algorithm;search algorithm	Vision	43.452273782854675	-13.689045108268276	168709
f0f46def95bf14d4bf5384900170eac0615caf50	content adaptive visible watermarking during ordered dithering	filigranage numerique;protection information;digital watermarking;anotacion;analisis contenido;metodo adaptativo;8 bit processor;evaluation performance;tecnologia electronica telecomunicaciones;performance evaluation;halftone image;image processing;procesador 8 bits;binary image;imagen medio tinte;reduccion de colores;evaluacion prestacion;image demi teinte;half tone image;procesamiento imagen;annotation;methode adaptative;modulacion;imagen nivel gris;traitement image;processeur 8 bits;ordered dithering;content analysis;proteccion informacion;visible watermarking;information protection;filigrana digital;adaptive method;image niveau gris;image binaire;dithering;imagen binaria;content adaptation;content adaptive;tramage;analyse contenu;tecnologias;grupo a;grey level image;modulation	This letter presents an improved visible watermarking scheme for halftone images. It incorporates watermark embedding into ordered dither halftoning by threshold modulation. The input images include a continuous-tone host image (e.g. an 8-bit gray level image) and a binary watermark image, and the output is a halftone image with a visible watermark. Our method is content adaptive because it takes local intensity information of the host image into account. Experimental results demonstrate effectiveness of the proposed technique. It can be used in practical applications for halftone images, such as commercial advertisement, content annotation, copyright announcement, etc.	digital watermarking;dither;ordered dithering	Hao Luo;Jeng-Shyang Pan;Zhe-Ming Lu	2007	IEICE Transactions	10.1093/ietisy/e90-d.7.1113	computer vision;binary image;content analysis;telecommunications;image processing;digital watermarking;computer science;error diffusion;8-bit;information protection policy;dither;computer graphics (images);modulation	Visualization	43.386535867114816	-11.049883700878139	168786
2dcee58126375e1441351fc65ddbadd8255a1e03	design of a new selective video encryption scheme based on h.264	group communication;key agreement protocol;computer security;protocols;civil engineering;computational intelligence;algorithm design and analysis;diffie hellman;bandwidth;key management;broadcasting	With the development and application of H.264 standard, the technology of H.264-based video data security becomes increasingly important. This paper proposes a new selective encryption scheme based on H.264, it combines the AES OFB mode with the sign encryption algorithm, and encrypts DCs and parts of ACs respectively. This method not only keeps advantages of former selective encryption algorithms in computational complexity and error-propagation prevention, but also efficiently make up for the deficiency in security and compression performance. Experimental results show that the proposed method exhibits low complexity and good security, and it has little effect on compression ratio and supports error- propagation prevention. Moreover, it is suitable for secure transmission of mobile multimedia and wireless multimedia network based on H.264.	algorithm;angular defect;block cipher mode of operation;computational complexity theory;context-adaptive binary arithmetic coding;context-adaptive variable-length coding;data loss prevention software;data security;encryption;entropy encoding;h.264/mpeg-4 avc;performance;propagation of uncertainty;secure transmission;software propagation	Yajun Wang;Mian Cai;Feng Tang	2007	2007 International Conference on Computational Intelligence and Security (CIS 2007)	10.1109/CIS.2007.99	multiple encryption;40-bit encryption;client-side encryption;computer science;cryptography;theoretical computer science;frequency;compression ratio;link encryption;data security;disk encryption hardware;computer security;encryption;probabilistic encryption;algorithm;statistics;56-bit encryption;computer network	EDA	39.386736746427374	-12.75547687850166	169234
9a078733ff8024dee765c3b03ca81d4db0536ec8	clock recovery and reconstruction of pal pictures for mpeg coded streams transported over atm networks	desciframiento;traitement signal;switching networks;sistema pal;image processing;phase locked loops video coding image reconstruction asynchronous transfer mode telecommunication networks switching networks clocks jitter synchronisation decoding;decodage;decoding;clocks;audio video;procesamiento imagen;traitement image;atm networks;phase locked loops;reduccion ruido;synchronisation;video coding;reconstruction image;senal video;signal video;reconstruccion imagen;clocks decoding demodulation phase locked loops jitter band pass filters streaming media frequency synchronization stability bandwidth;image reconstruction;signal processing;noise reduction;systeme pal;reduction bruit;video signal;buffer size clock recovery pal pictures reconstruction mpeg audio video decoder mpeg stream atm network decoded signal pal ntsc format atm cell delay variation jitter source reference signals coded streams pal ntsc synchronization signals hue changes clock recovery design consumer quality video display;jitter;procesamiento senal;clock recovery;asynchronous transfer mode;telecommunication networks;pal system	The problem of clock recovery for an MPEG audio/video decoder, when the MPEG stream is transmitted over an ATM network and the decoded signal is to be converted to a PAL/NTSC format, is addressed. The presence of ATM cell delay variation represents a jitter source for the reconstruction of the main presentation reference signals associated with the coded streams. Since the PAL/NTSC synchronization signals are obtained from these signals, care must be taken to avoid visible artifacts in the displayed images, typically hue changes. It is shown that the clock recovery design, based on consumer quality video display requirements, is not more complex than ordinary PLLs used in transmission systems, while the buffer size requirements can be certainly contained within acceptable limits.	atm turbo;clock recovery;moving picture experts group;pal	Giovanni Fausto Andreotti;Giampaolo Michieletto;Luigi Mori;Alberto Profumo	1995	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.475893	iterative reconstruction;synchronization;computer vision;real-time computing;phase-locked loop;jitter;telecommunications;image processing;computer science;signal processing;asynchronous transfer mode;noise reduction	Arch	48.20602061637211	-14.229239205684234	169396
2163b408abcce4e5472ce45db13bc0d54940e918	optimal bit allocation and best-basis selection for wavelet packets and tsvq	rate distortion;optimisation;image coding;image processing;data compression;wavelet base;subband decomposition;base ondita;procesamiento imagen;wavelet packet decomposition optimal bit allocation best basis selection tsvq lossy data compression wavelet subbands quantisation lower convex hull rate distortion curve tree structured vector quantizers image subband coding;transform coding;tree data structures;wavelet packet;traitement image;rate distortion theory;asignacion optima;wavelet transforms;codificacion;cuantificacion vectorial;vector quantization;descomposicion subbanda;tree structure;wavelet packet decomposition;coding;allocation optimale;compresion dato;bit allocation;decomposition sous bande;vector quantisation;convex hull;optimal allocation;base ondelette;rate distortion theory image coding transform coding vector quantisation tree data structures wavelet transforms optimisation;compression donnee;codage;bit rate wavelet packets frequency rate distortion image coding data compression acoustic distortion rate distortion theory vector quantization information analysis;quantification vectorielle	To use wavelet packets for lossy data compression, the following issues must be addressed: quantization of the wavelet subbands, allocation of bits to each subband, and best-basis selection. We present an algorithm for wavelet packets that systematically identifies all bit allocations/best-basis selections on the lower convex hull of the rate-distortion curve. We demonstrate the algorithm on tree-structured vector quantizers used to code image subbands from the wavelet packet decomposition.	algorithm;allocation;convex hull;data compression;distortion;lossy compression;mathematical optimization;wavelet packet decomposition	Jill R. Goldschneider;Eve A. Riskin	1999	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.784444	data compression;mathematical optimization;discrete mathematics;transform coding;rate–distortion theory;second-generation wavelet transform;image processing;computer science;theoretical computer science;convex hull;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;tree structure;coding;tree;set partitioning in hierarchical trees;vector quantization;statistics;wavelet transform	Visualization	46.7380986579168	-12.792521320058658	169445
762adc8a5a4d1bdc0445d7d28151d18c4dfd87aa	color image vector quantization using an enhanced self-organizing neural network	tabla codificacion;image coding;algorithm complexity;data compression;optimal code;computer graphics;code optimal;complejidad algoritmo;competitive learning;codage image;compression image;cuantificacion vectorial;vector quantization;complexite algorithme;image compression;codebook;relacion compresion;table codage;autoorganizacion;compression ratio;self organization;vector quantizer;compresion dato;taux compression;reseau neuronal;codigo optimal;imagen color;grafico computadora;infographie;red neuronal;image couleur;autoorganisation;compression donnee;color image;neural network;compresion imagen;quantification vectorielle	In the compression methods widely used today, the image compression by VQ is the most popular and shows a good data compression ratio. Almost all the methods by VQ use the LBG algorithm that reads the entire image several times and moves code vectors into optimal position in each step. This complexity of algorithm requires considerable amount of time to execute. To overcome this time consuming constraint, we propose an enhanced self-organizing neural network for color images. VQ is an image coding technique that shows high data compression ratio. In this study, we improved the competitive learning method by employing three methods for the generation of codebook. The results demonstrated that compression ratio by the proposed method was improved to a greater degree compared to the SOM in neural networks.	artificial neural network;color image;vector quantization	Kwang-Baek Kim;Abhijit S. Pandya	2004		10.1007/978-3-540-30497-5_172	data compression;computer vision;data compression ratio;self-organization;color image;image compression;computer science;theoretical computer science;machine learning;codebook;compression ratio;mathematics;lossless compression;competitive learning;computer graphics;vector quantization;artificial neural network;algorithm	ML	45.01323673224674	-12.621462580520076	169536
6fcef8db3ca2c1a169ed61da664187d25de63bab	reconstruction of bidirectional predicted residual for stereoscopic video based on compressed sensing	compressed sensing;video processing;video	As an effective method applied in video processing, compressed sensing(CS)[1] has gained wide interests. As we known, in traditional methods, if we want to recover a signal accurately from the samples, then the sampling rate has to be at least twice the maximum frequency present in the signal, which known as the Nyquist sampling rate. However, the sampling rate under the framework of compressed sensing can be much lower than the Nyquist sampling rate. As we will see in the remainder of this paper, CS asserts that we can recover certain signals from far few samples or measurements than traditional ways use. So as to save costs and improve efficiency, based on compressed sensing,we propose a reconstruction method for stereoscopic video codec. In our method, sparse residuals obtained by the block-based stereoscopic video processing and bidirectional prediction, are coded and reconstructed based on compressed sensing. Compared with other methods through simulation experiments, the proposed method reduces the sampling rate, and improves the quality of the reconstructed stereoscopic videos.	algorithm;circuit restoration;codec;compressed sensing;data compression;effective method;experiment;image restoration;motion estimation;nyquist rate;nyquist–shannon sampling theorem;sampling (signal processing);simulation;sparse matrix;stereoscopy;video coding format;video processing	Jueqiong Yu;Shigang Wang;Yuanzhi Lu;Xiaojun Zhang;Xiaojie Lu	2013		10.1117/12.2030625	computer vision;simulation;computer science;multiview video coding;computer graphics (images)	Mobile	44.4411216065507	-17.76248225268596	169538
93dc65e85654591544b924e3458a36a54d012d74	a new dct-domain distortion model for mb-level quality control	quantization;video surveillance;psnr;fluctuations;standard deviation;video quality;quality control dct distortion model;bit rate;video coding;estimation;discrete cosine transforms;quality control quadratic programming quantization psnr bit rate encoding fluctuations distortion streaming media electronic mail;quantization parameter;video surveillance discrete cosine transforms video coding;rmse dct domain distortion model mb level quality control video surveillance digital cinema video content constant video quality constant quantization parameter video encoder psnr dct coefficients;quality control;distortion model;dct	For many applications like video surveillance and digital cinema, it is desirable to encode video content with constant video quality. However, although constant quantization parameter is used in video encoder, quality fluctuation in macroblock level is still very large and standard deviation of PSNR is typically about 1.8dB. Since distortion is introduced in the process of quantization of DCT coefficients, we propose a new DCT-domain distortion model which utilizes the truncated tailing bits in quantization to estimate distortion in macroblock level, and QP of each block is determined by the estimated PSNR. Experimental results show that our proposed model can accurately estimate distortion of each MB before implementing quantization and the corresponding RMSE is only about 0.3dB (less than 1% of the actual value). Compared with previous works, our proposed algorithm can achieve 0.57dB enhancement in quality stabilization.	algorithm;cinema 4d;closed-circuit television;coefficient;digital video;discrete cosine transform;distortion;encode;encoder;h.264/mpeg-4 avc;macroblock;peak signal-to-noise ratio;quantum fluctuation;universal conductance fluctuations	Xun He;Xianmin Chen;Peilin Liu;Satoshi Goto	2009	2009 International Conference on Computer Modeling and Simulation	10.1109/ICCMS.2009.28	computer vision;quality control;estimation;electronic engineering;quantization;trellis quantization;peak signal-to-noise ratio;computer science;video quality;discrete cosine transform;mathematics;multimedia;standard deviation;statistics	Vision	46.76069595662158	-17.338250764223158	169659
51b5fe43394c3f7e132bc4ed56b7837ec98fd674	intensity dependent spatial quantization with application in hevc	luminance masking;quantization;image resolution;data compression;quantization signal;bit rate;hevc;multimedia systems;video coding;bitrate reduction video resolution multimedia applications next generation video compression standard high efficiency video coding itu t iso iec mpeg h 264 avc hevc compression capabilities bitrate saving mechanism intensity dependant spatial quantization perceptual tool idsq perceptual tool intensity masking human visual system video characteristics complexity implementation requirements hevc test model;sensitivity;human visual system;quantization signal encoding bit rate video coding video codecs sensitivity;video codecs;encoding;video coding data compression image resolution multimedia systems;hevc human visual system luminance masking quantization video coding	With the increase of video resolutions used in multimedia applications, solutions for improved compression are sought. The next generation video compression standard, High Efficiency Video Coding (HEVC) is being developed by ITU-T and ISO/IEC MPEG with the goal to provide significant improvements over H.264/AVC. To further increase compression capabilities of HEVC, this paper proposes an additional bitrate saving mechanism. In this context, an Intensity Dependant Spatial Quantization (IDSQ) perceptual tool is proposed which exploits the intensity masking of the human visual system and perceptually adjusts quantization. The proposed IDSQ allows for adaptation to the video characteristics and its design meets low complexity implementation requirements. The proposed IDSQ has been tested in the HEVC test model and its performance is reported under “just noticeable distortion” or “perceptually lossless” conditions. In this setup, bitrate reductions of up to 25% are reported with an average of 3.4% over an exhaustive test setup.	angular momentum operator;data compression;distortion;h.264/mpeg-4 avc;high efficiency video coding;lossless compression;moving picture experts group;next-generation network;requirement;video coding format	Matteo Naccari;Marta Mrak	2013	2013 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2013.6607535	data compression;computer vision;real-time computing;image resolution;quantization;sensitivity;computer science;multimedia;human visual system model;encoding	Robotics	44.39417150057495	-19.620346021240685	169797
f24bfa046ddff4c78ef111018cb8df67dfbcdff7	fast rate-distortion optimization in the h.264/avc standard	full search;h 264 avc;motion estimation;video coding;computational complexity;rate distortion optimization;lagrangian multiplier	Rate-Distortion Optimization (RDO) is a new feature of JVT H.264/AVC video encoder that enables a substantial enhanced efficiency with respect to all previous video coding standards. However, RDO causes a considerable increase in encoding computational complexity, proportional to the square of the search window size when the Full-Search motion estimation algorithm is applied. In this paper, a fast block-based predictive-recursive motion estimation algorithm is illustrated in combination with RDO algorithms. Extensive simulations show that, using the proposed algorithms, the time devoted to RDO is significantly reduced while keeping performance extremely close to JVT reference software JM8.6.	algorithm;data compression;distortion;encoder;mathematical optimization;motion estimation;raster document object;rate–distortion optimization;recursion;remote data objects;simulation;video coding format	Tea Anselmo;Daniele Alfonso	2006		10.1145/1374296.1374338	mathematical optimization;real-time computing;computer science;theoretical computer science;motion estimation;rate–distortion optimization;lagrange multiplier;computational complexity theory	Vision	47.30089760254514	-19.04188709504682	169951
0ad2415a72a02ec426cfd0037c026605d62a7333	an audio steganography technique to maximize data hiding capacity along with least modification of host		Steganography is the concept of concealing information within a cover media. In this paper, we present a technique to hide the text information within an audio file using a higher Least Significant Bit (LSB) layer. To hide text information within the audio file, we propose an approach that uses two LSB layers of the host file. The data hiding is done by minimum modification to the host file, which implies that changes of host audio file do not impact the Human Auditory System (HAS). Hiding of dual bit in the 4th and 1st LSB layer improves the hiding capacity of the host file. The proposed approach is also robust with respect to the errors, occurred during embedding of text in the audio files, as minimum modifications are performed on the original host audio [1].	steganography	Lukman Bin Ab. Rahim;Shiladitya Bhattacharjee;Izzatdin B. A. Aziz	2013		10.1007/978-981-4585-18-7_32	steganography tools;internet privacy;world wide web;computer security	Crypto	39.692780150786206	-12.631003045598137	170159
633279d0c555b31e053686356b9758356b039c54	fast two-stage lempel-ziv lossless numeric telemetry data compression using a neural network predictor	lempel ziv;data compression;lossless compression;lossless data compression;compression ratio;processing speed;neural network	Lempel-Ziv (LZ) is a popular lossless data compression algorithm that produces good compression performance, but suffers from relatively slow processing speed. This paper proposes an enhanced version of the Lempel-Ziv algorithm, through incorporation of a neural pre-processor in the popular predictor-encoder implementation. It is found that in addition to the known dramatic performance increase in compression ratio that multi-stage predictive techniques achieve, the results in this paper show that overall processing speed for the multistage scheme can increase by more than 15 times for lossless LZ compression of numeric telemetry data. The benefits of the proposed scheme may be expanded to other areas and applications.	algorithm;artificial neural network;data compression;encoder;kerrison predictor;lempel–ziv–stac;lempel–ziv–welch;lossless compression;multistage interconnection networks;performance	Rajasvaran Logeswaran	2004	J. UCS	10.3217/jucs-010-09-1199	data compression;lossy compression;lempel–ziv–stac;lossless jpeg;data compression ratio;speech recognition;image compression;computer science;theoretical computer science;lossless compression;adaptive coding;context-adaptive binary arithmetic coding;artificial neural network;statistics	ML	45.605304132289135	-11.658577791737525	170230
b09bd6fcbd513d3bd2e167f629da968b8d74fba9	using bayesian classifiers for low complexity multiview h.264/avc and hevc hybrid architecture	complexity theory;multiview hybrid coding;standards;h 264 avc;ctu splitting h 264 avc hevc multiview hybrid coding;coding efficiency bayesian classifier low complexity multiview compatibility 3d functionality overall bitrate multiview h 264 hevc hybrid architecture multiview hybrid architecture encoding complexity h 264 avc encoder hybrid multiview architecture coding unit naïve bayes probabilistic classifier;hevc;video coding bayes methods computational complexity;video coding;computer architecture;technology and engineering;streaming media;three dimensional displays;ctu splitting;video coding encoding three dimensional displays computer architecture complexity theory standards streaming media;encoding	In order to enable a system which offers compatibility with currently existing H.264/AVC based systems, 3D functionality, and a low overall bitrate, a multiview H.264/HEVC hybrid architecture was proposed in the context of 3D applications and standardization. This paper presents an algorithm to reduce the complexity of this multiview hybrid architecture by reducing the encoding complexity of the HEVC side views. The proposed technique exploits the information gathered in the center view of the H.264/AVC encoder of this hybrid mul-tiview architecture to make decisions on Coding Units splitting in HEVC side views using a Naïve-Bayes probabilistic classifier. Thus, the proposal is quite novel since no similar works has been found in the literature and it exploits a new characteristic of the multiview HEVC streams which was not present in previous standards. Experimental results show that the proposed algorithm can achieve a good tradeoff between coding efficiency and complexity.	algorithm;algorithmic efficiency;bayesian network;encoder;h.264/mpeg-4 avc;high efficiency video coding;multiview video coding;naive bayes classifier	Antonio Jesús Díaz-Honrubia;Johan De Praeter;Sebastiaan Van Leuven;Jan De Cock;José Luis Martínez;Pedro Cuenca	2015	2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2015.7324366	computer vision;real-time computing;computer science;theoretical computer science	AI	46.66434682956545	-19.456267683760277	170258
c0e67a1c7e601d8c394e9b9cf5c84bdf80c9ab81	still image coding using a wavelet-like transform	kernel;image coding;orthogonal polynomial;transform coding;polynomials;wavelet transforms;image coding wavelet transforms transform coding discrete cosine transforms polynomials discrete wavelet transforms quantization discrete transforms frequency image reconstruction;image compression;wavelet transforms image coding polynomials transform coding;discrete cosine transforms;human visual system;bit allocation;jpeg baseline system still image coding wavelet like transform orthogonal polynomial basis bivariate orthogonal polynomial functions 2d nonseparable wavelet functions human visual system quantization bit allocation;transform coding image compression	In this paper, a new image coding scheme based on a wavelet-like transform derived from orthogonal polynomial basis is presented. From a set of bivariate orthogonal polynomial functions, we first obtain the 2D non-separable wavelet functions to propose a wavelet-like transform coding. The motivation behind using orthogonal polynomials is that they exhibit some properties related to the human visual system (HVS) [1]. After applying the proposed transformation, the transform coefficients are threshold coded using quantization and bit allocation as in JPEG baseline system. The performance of the proposed transform coding is reported. The proposed coding scheme is also compared with other transform coding methods such as JPEG, JPEG 2000 and JPEG-XR/HDPHOTO.	baseline (configuration management);bivariate data;coefficient;human visual system model;jpeg 2000;polynomial basis;quantization (signal processing);transform coding;variable-length code;wavelet	Aldo Maalouf;Mohamed-Chaker Larabi;Christine Fernandez-Maloigne	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413676	wavelet;lossless jpeg;computer vision;constant q transform;discrete mathematics;kernel;transform coding;s transform;harmonic wavelet transform;lapped transform;second-generation wavelet transform;continuous wavelet transform;image compression;theoretical computer science;discrete sine transform;discrete fourier transform;discrete cosine transform;mathematics;wavelet packet decomposition;stationary wavelet transform;orthogonal polynomials;human visual system model;discrete wavelet transform;fast wavelet transform;lifting scheme;polynomial;wavelet transform	Vision	43.106671130072385	-15.374529957104034	170450
edd95f193621fb5acdcdad526ed6d5a7687cc9cb	tile format: a novel frame compatible approach for 3d video broadcasting	digital video broadcasting;3d tv;avc h264;video format 3d video broadcasting 2d backward compatibility transmission bandwidth dvb 3d multiplexing standard video sequence tile format frame compatible arrangement image quality 3d tv broadcasting;3d multiplexing;three dimensional television;3d tv broadcasting;2d backward compatibility;television receivers;three dimensional displays psnr tiles digital video broadcasting tv image quality;multiplexing;transmission bandwidth;three dimensional television digital video broadcasting image sequences multiplexing television receivers;user experience;image quality;3d video broadcasting;tile format frame compatible arrangement;dvb;3d video;standard video sequence;video format;avc h264 3d tv dvb;image sequences	The success of the 3D TV broadcasting service largely depends on the technical, economical and user experience aspects. Nevertheless, it is quite clear that one of the keys for the successful deployment of 3D TV is the offer of a service which permits a seamless transition to 3D by exploiting the existing infrastructure and guaranteeing 2D backward compatibility towards legacy receivers, whilst only requiring a moderate increase in the transmission bandwidth. To this end, the DVB has just drafted the first phase of the 3D TV specification and has selected a number of frame compatible format arrangements for the multiplexing of 3D in a standard video sequence. In this paper we propose the novel Tile Format frame compatible arrangement and analyze its advantages over other existing solutions in terms of both image quality and backward compatibility. Furthermore, we detail a real 3D TV broadcasting trial which is currently being conducted in our country and is based upon the proposed video format.	3d television;backward compatibility;digital video broadcasting;image quality;multiplexing;seamless3d;software deployment;user experience;video coding format;while	Giovanni Ballocca;Paolo D'Amato;Marco Grangetto;Maurizio Lucenteforte	2011	2011 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2011.6012053	user experience design;telecommunications;computer science;multimedia;digital video broadcasting	Visualization	43.41200848221439	-20.75725403842745	170474
5e8e3d2a79537a6cd0c138545bce63ddafaa853c	intent-aware long-term prediction of pedestrian motion	stochastic policy intent aware long term prediction pedestrian motion prediction jump markov process markov decision process framework rao blackwellized filter pedestrian state;road traffic control filtering theory image motion analysis markov processes optimal control pedestrians;trajectory predictive models semantics markov processes collision avoidance context	We present a method to predict long-term motion of pedestrians, modeling their behavior as jump-Markov processes with their goal a hidden variable. Assuming approximately rational behavior, and incorporating environmental constraints and biases, including time-varying ones imposed by traffic lights, we model intent as a policy in a Markov decision process framework. We infer pedestrian state using a Rao-Blackwellized filter, and intent by planning according to a stochastic policy, reflecting individual preferences in aiming at the same goal.	hidden variable theory;markov chain;markov decision process;markov property	Vasiliy Karasev;Alper Ayvaci;Bernd Heisele;Stefano Soatto	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487409	markov decision process;computer vision;simulation;partially observable markov decision process;computer science;machine learning;markov process;markov model;variable-order markov model	Robotics	52.053504748598705	-20.885505795967884	170604
aa1edea049bdaac15d6342cea70567c073a9ef45	the gerrymander problem in vector quantization	code vector convergence gerrymander problem vector quantization image coding nonrepresentative code vector replacement population vector representation iterative algorithms gray scale images progressive c means popular code vector duplication;image coding;iterative methods vector quantisation image coding;iterative methods;vector quantizer;vector quantisation;vector quantization image coding iterative algorithms digital images prototypes computer science convergence signal mapping distortion signal processing	In vector quantization applied to image coding the objective is to determine a set of code vectors for the coding of the population vectors of an image. It is clear that each basis vector should represent about the same number of population vectors, and no population vector should be very badly represented. However, various VQ algorithms have a tendency to produce a gerrymander, in which a few code vectors come to be the representative of many or most population vectors, while at the same time some code vectors represent very few of the population. This situation, analogous to a political gerrymander, arises for ill-chosen initial code vectors. We introduce a scheme, to be applied in conjunction with iterative VQ algorithms, that blocks the development of a gerrymander. This new algorithm involves the replacement of non-representative code vectors, by those population vectors that are most poorly represented. Experimental data on the VQ of gray-scale images using progressive (hard) c-means show that the scheme is most effective, and marginally improved when applied in conjunction with the deliberate duplication of the most popular code vector.	algorithm;basis (linear algebra);grayscale;iteration;population vector;vector quantization	Harvey A. Cohen	1996		10.1109/ANZIIS.1996.573968	mathematical optimization;feature vector;theoretical computer science;machine learning;iterative method;vector quantization	ML	44.17915656743294	-12.564649303691283	170689
e36a599fdb22db26fda5e3f6687afe5ff13b66fe	modeling and classifying six-dimensional trajectories for tele-operation under time delay	automatic control;predictions;teleoperators;human behavior;trajectories;stochastic processes;robots;time lag;data acquisition	Within the context of tele-operating the JSC Robonaut humanoid robot under 2-10 second time delays, this paper explores modeling and classifying human motions represented as six-dimensional (position and orientation) trajectories. A dual path research agenda is reviewed exploring both deterministic and stochastic approaches using Hidden Markov Models. Finally, recent results are shown from a new model that integrates these two research paths. In future work it will be possible to automatically generate autonomous actions by reusing these same predictive models of human behavior to be the source of autonomous control. This approach may change the role of tele-operation from being a stand-in for autonomy into the first step of mentoring generative models capable of autonomous robotic control.	autonomous robot;generative model;hidden markov model;humanoid robot;markov chain;predictive modelling;robonaut;television	Vytas SunSpiral;Mark B. Allan;Rodney Martin;Kevin R. Wheeler	2006			robot;computer vision;simulation;prediction;computer science;artificial intelligence;trajectory;machine learning;automatic control;data acquisition;human behavior;statistics	Robotics	51.902160211839	-21.092447008192583	170693
a1b56c7ab6b71fe2ac6ddb8cce3a0ed7e0a3894a	low complexity index-compressed vector quantization for image compression	image coding;index compression;low complexity;computational complexity image coding vector quantisation;image compression;vector quantization image coding bit rate computational efficiency decoding image quality computer science compression algorithms digital images image reconstruction;computational complexity;indexation;image vector quantization low complexity index compressed vq image compression lossless index compression algorithm interblock correlation index domain codebook ordering low bit rate codeword codebook input vector predefined search order bit rate reduction;vector quantizer;vector quantisation	This paper proposes a novel lossless index compression algorithm that explores the interblock correlation in the index domain and the property of the codebook ordering. The goal of this algorithm is to improve the performance of the VQ scheme at low bit rate while keeping low computation complexity. In this algorithm, the closest codeword in the codebook is searched for each input vector. Then, the resultant index is compared with the previously encoded indices in a predefmed search order to see whether the same index value can be found in the neighboring region. Besides, the relative addressing technique is employed to encode the current index if the same index value can not be found in the neighboring region. According to the experimental resuks, the newly proposed algorithm achieves significant reduction of bit rate without introducing extra coding distortion. It is concluded that our algorithm is very efficient and effective for image vector quantization.	algorithm;code word;codebook;complexity index;computation;distortion;encode;image compression;lossless compression;offset (computer science);resultant;vector quantization	Yu-Chen Hu;Chin-Chen Chang	1999	IEEE Trans. Consumer Electronics	10.1109/30.754439	data compression;data compression ratio;discrete mathematics;harmonic vector excitation coding;image compression;computer science;theoretical computer science;pattern recognition;mathematics;fractal transform;linde–buzo–gray algorithm;computational complexity theory;vector quantization;algorithm	ML	43.394343913623025	-13.98847841724276	170819
8741d84b899e4aa34053a2fcb51a2a7ca8ac0099	coding algorithms for 3dtv—a survey	filigranage numerique;protection information;analisis imagen;digital watermarking;3dtv depth coding disparity coding mpeg multiview video coding mvc multiple description coding mdc stereo video coding watermarking 3 d mesh compression;data transmission;watermarking;detection erreur;deteccion error;analisis escena;analyse scene;multiple description coding mdc;3d mesh models;2d video;image coding;mesh compression;depth data;standards;television 3 dimensiones;image processing;modelo 3 dimensiones;static geometry;intellectual property;video compression layout video coding geometry protection watermarking displays standards development power system modeling redundancy;imagen fija;video signal processing;redundancia;modele 3 dimensions;correction erreur;three dimensional television;multiview video coding mvc;compresion senal;procesamiento imagen;image multiple;disparity;three dimensional model;disparity coding coding algorithms 3dtv technology media processing chain 3d scene representations pixel type data compression stereo video coding disparity maps open international standards multiview video coding depth data 3d mesh models static geometry dynamic 3d geometry temporal prediction animated 3d mesh sequences error resilience data transmission error prone channels multiple description coding 2d video intellectual property protection 3d watermarking depth coding;depth coding;imagen multiple;stereo video coding;intellectual property protection;watermarking stereo image processing video coding;traitement image;disparidad;compression signal;multiple image;algorithme;3 d mesh compression;algorithm;codage image;multiview video coding;video coding;animated 3d mesh sequences;redundancy;senal video;fixed image;signal video;coding algorithms;codage video;disparity coding;proteccion informacion;dynamic 3d geometry;streaming media;3d watermarking;error prone channels;error correction;information protection;filigrana digital;signal compression;propiedad intelectual	Research efforts on 3DTV technology have been strengthened worldwide recently, covering the whole media processing chain from capture to display. Different 3DTV systems rely on different 3D scene representations that integrate various types of data. Efficient coding of these data is crucial for the success of 3DTV. Compression of pixel-type data including stereo video, multiview video, and associated depth or disparity maps extends available principles of classical video coding. Powerful algorithms and open international standards for multiview video coding and coding of video plus depth data are available and under development, which will provide the basis for introduction of various 3DTV systems and services in the near future. Compression of 3D mesh models has also reached a high level of maturity. For static geometry, a variety of powerful algorithms are available to efficiently compress vertices and connectivity. Compression of dynamic 3D geometry is currently a more active field of research. Temporal prediction is an important mechanism to remove redundancy from animated 3D mesh sequences. Error resilience is important for transmission of data over error prone channels, and multiple description coding (MDC) is a suitable way to protect data. MDC of still images and 2D video has already been widely studied, whereas multiview video and 3D meshes have been addressed only recently. Intellectual property protection of 3D data by watermarking is a pioneering research area as well. The 3D watermarking methods in the literature are classified into three groups, considering the dimensions of the main components of scene representations and the resulting components after applying the algorithm. In general, 3DTV coding technology is maturating. Systems and services may enter the market in the near future. However, the research area is relatively young compared to coding of other types of media. Therefore, there is still a lot of room for improvement and new development of algorithms.	3d modeling;3d printing;3d television;algorithm;augmented reality;authorization;binocular disparity;capability maturity model;cognitive dimensions of notations;data compression;digital watermarking;high-level programming language;ieee xplore;interoperability;map;mathematical optimization;multiple description coding;multiview video coding;pixel;prospective search;simulation;static mesh;synthetic intelligence;vertex (graph theory);view synthesis	Aljoscha Smolic;Karsten Müller;Nikolce Stefanoski;Jörn Ostermann;Atanas P. Gotchev;Gozde Bozdagi Akar;George A. Triantafyllidis;A. Koz	2007	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2007.909972	video compression picture types;data compression;computer vision;image analysis;error detection and correction;telecommunications;image processing;digital watermarking;computer science;video tracking;multimedia;intellectual property;computer graphics (images)	Graphics	42.869940605348766	-20.08940330004097	170875
16bf60b17a72a825a9a0a248a1d8cb7798eb08b2	multiscale (inter/intra-frame) fractal video coding	compression algorithm;scale function;fractals;data compression;pyramid algorithm;multimedia application;iterative methods;video coding;functional equation;image compression;multimedia communication;functional equations;iterative methods video coding data compression functional equations multimedia communication fractals;fractals video coding image coding equations video compression streaming media compression algorithms bit rate discrete cosine transforms motion compensation;fractal image compression;multimedia applications multiscale fractal video coding inter frame fractal video coding intra frame fractal video coding image compression algorithm iterated transformation theory second kind two scale functional equation pyramid algorithm still image compression itt chain video coder bit streams	In this work we present a fractal image compression algorithm for interrintra-frame video coding. We h a ve shown previously how the Iterated TransformationThe-ory ITT compression algorithm proposed by Jacquin can be modeled as the solution of a second kind two-scale functional equation. This approach allows us to introduce a chain of functional equations which w e use to build a pyramid algorithm for still image compression. In this work we use the prediction property of the ITT-chain to create a inter-frame video coder. We combine the inter-frame video encoder with the still-image ITT-pyramid to generate a hierarchy o f b i t streams that can be used in multimedia applications.	algorithm;data compression;encoder;fractal compression;image compression;iterated function;iteration;whole earth 'lectronic link	Alexandru Bogdan	1994		10.1109/ICIP.1994.413417	video compression picture types;data compression;inter frame;functional equation;computer vision;computer science;theoretical computer science;mathematics;block-matching algorithm;multimedia;context-adaptive binary arithmetic coding;motion compensation;algorithm	Vision	45.344364649175276	-18.36238863765415	170931
a69eef3f3671a5d07786953b9a8a80953548cb80	hierarchical sub-band coding of super high definition image with adaptive block-size multistage vq	metodo adaptativo;multistage;high resolution;image processing;optimal filtering;subband decomposition;procesamiento imagen;methode adaptative;multietage;codigo bloque;traitement image;codificacion;haute resolution;cuantificacion vectorial;vector quantization;poliescalonado;adaptive method;coding;alta resolucion;methode hierarchique;code bloc;decomposition sous bande;block code;high definition;filtrado optimo;codage;filtrage optimal;quantification vectorielle	Abstract   A hierarchical image coding algorithm based on sub-band coding and adaptive block-size multistage vector quantization (VQ) is proposed, and its coding performance is examined for super high definition (SHD) image. First, the concept on SHD image is briefly described. Next, the signal power spectrum is evaluated, and the sub-band analysis pattern is determined from its characteristics. Several quadrature mirror filters are examined from the viewpoints of reconstruction accuracy, coding gain, and low-pass signal quality. Then an optimum filter is selected for the sub-band analysis. The two-stage VQ using the adaptive bit allocation is also introduced to control quantization accuracy and to achieve high-quality image reproduction. Coding performance and hierarchical image reconstruction are demonstrated using SNR and some photographs.	multistage amplifier;sub-band coding;vector quantization	Isao Furukawa;Mitsuru Nomura;Sadayasu Ono	1993	Sig. Proc.: Image Comm.	10.1016/0923-5965(93)90015-L	block code;speech recognition;image resolution;telecommunications;image processing;computer science;mathematics;coding;vector quantization;algorithm;statistics	Vision	46.537262859505226	-13.069644975429137	171057
982d48b46bbf29b3c275c1a1aa53b3e140635707	from image and video compression to computer graphics	teleconferencing;image coding;data compression;computer graphics;video compression;video codecs image coding data compression video coding computer animation rendering computer graphics teleconferencing;video compression computer graphics image coding sprites computer decoding rendering computer graphics layout video codecs motion compensation discrete cosine transforms;video codec;computer graphic;immersive environment;video coding;image compression;video conferencing;face animation;video conferencing platforms;networked immersive environment;video codecs;image analysis;3d objects streaming;video codec computer graphics video compression image compression image based rendering networked immersive environment image analysis face animation 3d objects streaming video conferencing platforms;image based rendering;computer animation;rendering computer graphics	We report previous effort in expanding image and video compression to computer graphics applications, in particular those based on image-based rendering. We then introduce our progress in developing a networked immersive environment that integrates image analysis, face animation, and streaming of 3D objects, to provide a truly immersive environment, with the goal of replacing existing video conferencing platforms.	computer graphics;data compression	Tsuhan Chen	2000		10.1109/ICIP.2000.899211	video compression picture types;data compression;computer vision;image analysis;image-based modeling and rendering;computer science;real-time computer graphics;multimedia;computer graphics;algorithm;computer graphics (images)	Graphics	42.973358362885875	-19.956137424639007	171318
3520b9ca7d7137bd8c3c6d935497ae336f01306c	real, tight frames with maximal robustness to erasures	quantization;polynomials internet noise robustness quantization data compression ip networks mathematical model algebra vehicles;data compression;hilbert spaces;polynomial transforms;orthogonal polynomial;matrix algebra;set theory;noise robustness;polynomials;maximum erasure robustness;equal norm frames;internet;redundancy;robust transmission;algebra;tight frame;fourier transforms;signal representation;orthogonal polynomials;mathematical model;tightness;real tight frames;ip networks;fourier transforms matrix algebra polynomials signal representation redundancy set theory data compression hilbert spaces;vehicles;real tight frames robust transmission internet maximum erasure robustness polynomial transforms tightness orthogonal polynomials equal norm frames	Motivated by the use of frames for robust transmission over the Internet, we present a first systematic construction of real tight frames with maximum robustness to erasures. We approach the problem in steps: we first construct maximally robust frames by using polynomial transforms. We then add tightness as an additional property with the help of orthogonal polynomials. Finally, we impose the last requirement of equal norm and construct, to our best knowledge, the first real, tight, equal-norm frames maximally robust to erasures.	internet;jumbo frame;maximal set;polynomial	Markus Püschel;Jelena Kovacevic	2005	Data Compression Conference	10.1109/DCC.2005.77	combinatorics;mathematical analysis;discrete mathematics;mathematics;orthogonal polynomials	Theory	49.68584732027149	-14.088968346720723	171356
05be7c49c939d6be267054666a7f7c92d1349ca2	satellite image compression technique using noise bit removal and discrete wavelet transform	dwt;satellite image;denoising;compression	A feature of satellite images is the presence of noise, where this noise often requires preprocessing of the images by a denoising method adapted prior to their compression, storage and transmission. In this work to improve the compression scheme, we are interested in image compression by discarding noisy bits from the satellite image and discrete wavelet transform (DWT). This method was tested on various satellite images; this type of compression has allowed us to determine the quality of the reconstructed images (PSNR) and the compression ratio (CR) according to the corresponding of denoising method. A comparative study was conducted to determine the methods leading to the best possible results.	data compression ratio;discrete wavelet transform;image compression;noise reduction;peak signal-to-noise ratio;preprocessor	Xiaodong Wang;Jianhui Yang	2018	JMPT		data compression;computer vision;electronic engineering;image compression;mathematics;texture compression;computer graphics (images)	Graphics	42.31298454577672	-14.393412120784406	171506
0eba2c16daacab36ab62bcb8630da263fbfdb4dc	macroblock classification for complexity management of video encoders	prediccion;rate distortion;video signal processing;low frequency;classification;algorithme;algorithm;video coding;codage video;macrobloc;traitement signal video;prediction;clasificacion;algoritmo	Typically, many macroblocks (MBs) are skipped during encoding of H.263 or MPEG-4 SP video data, particularly at low bit-rates. In this paper, we describe an algorithm that predicts the occurrence of skipped MBs prior to encoding, making it possible to save significant computational effort by not coding these MBs. The algorithm estimates the energy of low-frequency quantized coefficients in order to classify each MB as ‘skipped’ or ‘not skipped’. Results show that the algorithm can deliver substantial computational savings at the expense of a small reduction in rate-distortion performance. r 2003 Elsevier B.V. All rights reserved.	algorithm;approximation algorithm;coefficient;computation;computational complexity theory;discrete cosine transform;distortion;encoder;macroblock;motion estimation;peak signal-to-noise ratio;while	Yafan Zhao;Iain E. Garden Richardson	2003	Sig. Proc.: Image Comm.	10.1016/S0923-5965(03)00072-9	simulation;speech recognition;prediction;telecommunications;biological classification;computer science;mathematics;low frequency;algorithm;statistics	ML	47.160002812593454	-14.972385946873862	171768
329482d93437696d13840754d95ccd6102200535	fractional-bit and value-location lossless encoding in g.711.0 coder	vectors encoding;g 711;g 711 0 coder;speech;lossless compression;speech coding;polynomials;value location code vector;vectors;computational complexity;fractional bit encoding;g 711 0;reference value;encoding signal processing computational complexity instruments digital signal processing research and development speech coding standardization huffman coding predictive coding;g 711 0 speech coding lossless compression itut standardization g 711;value location code vector fractional bit encoding value location lossless encoding g 711 0 coder;encoding;value location lossless encoding;itut standardization;context;modulation	The paper describes two lossless coding tools employed in the new ITU-T G.711.0 Recommendation: fractional-bit and value-location encoding. Instead of encoding each sample individually as done in G.711, the fractional-bit coding tool identifies the total number of signal levels that exist within an input frame and then combines several samples for joint encoding with fractional-bits per sample. The value-location tool encodes positions of all values within an input frame that differ from a reference value. The method efficiently represents an input frame as a sum of value-location code vectors that are sequentially encoded using Rice, binary, or explicit location encoding. Presented results illustrate how the described coding techniques were adopted for usage within the new ITU-T G.711.0 standard.	algorithm;bitstream;color depth;data compression;g.711;lossless compression	Jacek Stachurski;Lorin Netsch	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495202	arithmetic coding;speech recognition;truncated binary encoding;telecommunications;computer science;entropy encoding;speech;theoretical computer science;speech coding;lossless compression;adaptive coding;context-adaptive binary arithmetic coding;computational complexity theory;g.711;encoding;polynomial;range encoding;modulation	Robotics	47.81559837589664	-10.300156318819859	171971
02a755fe6f9b90e632d284bd0e0d6fb05957fae6	an iterative method of palette-based image steganography	data hiding;stego image;palette based image;steganography;iteration method;security	This article presents a novel iterative method of palette-based image steganography that minimizes the RMS error between an original image and its stego-image. The proposed method is based on a palette modification scheme, which can iteratively embed one message bit into each pixel in a palette-based image. In each iteration, both the cost of removing an entry color in a palette and the benefit of generating a new one to replace it are calculated. If the maximal benefit exceeds the minimal cost, a entry color is replaced. Experimental results show that the proposed method remarkably reduces the distortion of the carrier images (stego-images) to other palette-based methods. 2003 Elsevier B.V. All rights reserved.	color;distortion;iteration;iterative method;maximal set;palette (computing);pixel;steganography	Mei-Yi Wu;Yu-Kun Ho;Jia-Hong Lee	2004	Pattern Recognition Letters	10.1016/j.patrec.2003.10.013	computer vision;computer science;information security;theoretical computer science;mathematics;iterative method;steganography;8-bit color;information hiding;histogram equalization;statistics;computer graphics (images)	Vision	40.98428377292135	-13.018634046072888	171972
a4a64cd994582dd422dc47e10b82a677d6ad26d0	face location in wavelet-based video compression for high perceptual quality videoconferencing	high perceptual quality videoconferencing;transformation ondelette;perceptual image quality;perceptual quality;quantization;decomposed subbands;teleconferencing;spatial constraints;image coding;hybrid compression scheme;image processing;data compression;low bit rates;videoconference;localizacion objeto;edge detection;object location;simulation;video compression;wavelet based video compression;compresion senal;procesamiento imagen;transform coding;adaptive codes;human face location technique;bit rate;data mining;traitement image;compression signal;perceptually important information;deteccion contorno;quantisation signal;wavelet transforms;detection contour;adaptive quantization;video coding;reconstruction image;video compression teleconferencing face detection image edge detection humans quantization data mining image coding bit rate feature extraction;semantic information;feature point analysis;image edge detection;reconstruccion imagen;feature extraction;image reconstruction;image quality;contour extraction;signal compression;high fidelity;perceptual image quality contour extraction wavelet based video compression high perceptual quality videoconferencing human face location technique adaptive quantization spatial constraints low bit rates perceptually important information semantic information hybrid compression scheme high fidelity feature point analysis approximated face mask decomposed subbands image coding simulation;videoconferencia;humans;quantisation signal video coding data compression wavelet transforms transform coding teleconferencing feature extraction edge detection adaptive codes;transformacion ondita;face detection;approximated face mask;localisation objet;wavelet transformation	We present a human face location technique based on contour extraction within the framework of a wavelet-based video compression scheme for videoconferencing applications. In addition to an adaptive quantization in which spatial constraints are enforced to preserve perceptually important information at low bit rates, semantic information of the human face is incorporated to design a hybrid compression scheme for videoconferencing since the human face is often the most important portion within a frame and should be coded with high fidelity. The human face is detected based on contour extraction and feature point analysis. An approximated face mask is then used in the quantization of the decomposed subbands. At the same total bit rate, coarser quantization of the background enables the face region to be quantized finer and coded with higher quality. Simulation results have shown that the perceptual image quality can be greatly improved using the proposed scheme.	data compression;wavelet	Jiebo Luo;Chang Wen Chen;Kevin J. Parker	1996	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.510934	data compression;computer vision;speech recognition;image processing;computer science;pattern recognition	EDA	45.52474138011143	-14.530160119199431	172087
4d7b268808c0c3311c3e72a551a82622be29c671	rate-distortion optimized reference picture management for high efficiency video coding	optimisation;decoding;reference picture selection high efficiency video coding hevc rate distortion rd optimization reference picture management;rate distortion theory;encoding video coding optimization rate distortion bit rate decoding;video coding;image reconstruction;video coding decoding image reconstruction optimisation rate distortion theory;hevc software rate distortion optimized reference picture management high efficiency video coding motion compensation multiple reference pictures hevc standard heuristic strategy rate distortion performance decoded picture buffer decoder encoder optimization reference picture selection	Motion compensation with multiple reference pictures has been widely used during the development of the emerging High Efficiency Video Coding (HEVC) standard, which greatly helps to improve the coding efficiency. Usually, a heuristic strategy is exploited to use the nearest reconstructed pictures as references. However, such a strategy may not be efficient on all occasions, especially when different content characteristics and coding settings are considered. In this paper, we investigate how to manage reference pictures so as to achieve better rate-distortion performance under the memory constraint of the decoded picture buffer at the decoder. We formulate the reference picture management as an optimization problem and approximate its optimal solution. Moreover, we explore how to adjust quality for each picture according to the reference structure to further improve coding efficiency. For some coding cases, where a complicated encoder optimization is unaffordable, we also develop fast algorithms to get the most benefit from reference picture selection. Among them, one strategy has been adopted by the HEVC software and common test conditions to generate the anchor. Experimental results show that the proposed full search algorithm and fast search algorithms achieve significant bitrate reduction.	algorithmic efficiency;approximation algorithm;data compression;distortion;encoder;heuristic;high efficiency video coding;iterative method;local optimum;mathematical optimization;motion compensation;optimization problem;reference counting;reference frame (video);search algorithm;time complexity	Houqiang Li;Bin Li;Jizheng Xu	2012	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2012.2223038	iterative reconstruction;computer vision;rate–distortion theory;computer science;theoretical computer science;coding tree unit;mathematics;multimedia;motion compensation;statistics;multiview video coding	DB	47.5018890225973	-18.45645783288368	172245
f05e9ba8180e74f506585cc41fc92997baa774bf	low complexity video encoding and high complexity decoding for uav reconnaissance and surveillance	video surveillance;data compression;decoding;kalman filters;motion estimation;video coding;video surveillance autonomous aerial vehicles computational complexity data compression decoding image sequences kalman filters motion estimation video coding;computational complexity;complexity theory decoding mathematical model transforms memory management kalman filters equations;motion trajectory low complexity video encoding high complexity decoding uav reconnaissance surveillance application video compression scheme h 264 avc high complexity encoder block motion estimation block me low complexity low latency decoder unmanned aerial vehicle reconnaissance high complexity decoders video sequences camera mount uav movement global motion based frame prediction fly over videos h 264 encoder me block size kalman filtering;autonomous aerial vehicles;image sequences	Conventional video compression schemes such as H.264/AVC use a high complexity encoder with block motion estimation (ME) and a low complexity, low latency decoder. However, unmanned aerial vehicle (UAV) reconnaissance and surveillance applications require low complexity encoders but can accommodate high complexity decoders. Moreover, the video sequences in these applications often primarily have global motion due to the known movement of the UAV and camera mounts. Motivated by this scenario, we propose and investigate a low complexity encoder with global motion based frame prediction and no block ME. For fly-over videos, our encoder achieves more than a 40% bit rate savings over a H.264 encoder with ME block size restricted to 8 × 8 and at lower complexity. We also develop a high complexity decoder based on Kalman filtering along motion trajectories and show average PSNR improvements of up to 0.5 dB with respect to a classic low complexity decoder.	aerial photography;bitstream;block size (cryptography);blu-ray;codec;data compression;emoticon;encoder;global motion compensation;h.264/mpeg-4 avc;kalman filter;motion estimation;peak signal-to-noise ratio;unmanned aerial vehicle	Malavika Bhaskaranand;Jerry D. Gibson	2013	2013 IEEE International Symposium on Multimedia	10.1109/ISM.2013.34	data compression;kalman filter;computer vision;simulation;quarter-pixel motion;computer science;theoretical computer science;motion estimation;computational complexity theory;statistics	Vision	47.005486197240316	-18.201931601410834	172465
e8082f99ae614541297036b0f377154518695248	robust source coding for images over very noisy channels	nonlinear filters;quantization;channel coding;source coding noise robustness nonlinear filters working environment noise bit error rate image coding channel coding quantization nonlinear distortion image reconstruction;image coding;bit error rate;coding errors;robust source coding;working environment noise;visual communication;performance;noise mitigation;nonlinear filter;differential pulse code modulation;trellis coded quantization;noise robustness;nonlinear distortion;distortion;prediction theory;image transmission;low ber;image reconstruction;error statistics;noisy environment;source code;ptcq;visual communication source coding telecommunication channels vector quantisation differential pulse code modulation prediction theory nonlinear filters coding errors error statistics image reconstruction;compression;nonlinear prediction filter;high ber;dpcm;telecommunication channels;vector quantisation;noisy channels;image reconstruction noisy channels image transmission robust source coding image coding compression noise mitigation dpcm nonlinear filter predictive trellis coded quantization ptcq nonlinear prediction filter performance noisy environment distortion low ber high ber;source coding;predictive trellis coded quantization	Robust source coding provides both compression and noise mitigation without channel coding. In this paper , two methods of robust source coding are presented. The rst is DPCM incorporating a nonlinear lter; the second is Predictive Trellis-Coded Quantiza-tion (PTCQ), whose prediction lter is also nonlinear. Findings show that the incorporation of the nonlinear lter provides the ability to conceal the eeects of noise and minimize the propagation of error. A comparison is made between the performance of the two methods in a noisy environment. It is demonstrated that the PTCQ scheme provides a method of least distortion at low BER. However, at higher BER the DPCM scheme outperforms PTCQ. Robust source coding of images employing PTCQ or DPCM with the nonlinear prediction lter exhibits low sensitivity to bit errors and yields, visually, a better reconstructed image after transmission over very noisy channels.	data compression;distortion;forward error correction;nonlinear system;propagation of uncertainty;trellis quantization;x.690	Lisa M. Marvel;Ali S. Khayrallah;Charles G. Boncelet	1996		10.1109/ICIP.1996.560806	speech recognition;computer science;theoretical computer science;mathematics;source code	ML	48.32812661153941	-12.550507164487836	172588
dfd4d831c20234214e5faf8e013fe44f4e2ef321	on robustness against jpeg2000: a performance evaluation of wavelet-based watermarking techniques	wavelet based image watermarking;jpeg2000;content adaptation;robustness;watermarking evaluation;scalable coding	With the emergence of new scalable coding standards, such as JPEG2000, multimedia is stored as scalable coded bit streams that may be adapted to cater network, device and usage preferences in multimedia usage chains providing universal multimedia access. These adaptations include quality, resolution, frame rate and region of interest scalability and achieved by discarding least significant parts of the bit stream according to the scalability criteria. Such content adaptations may also affect the content protection data, such as watermarks, hidden in the original content. Many wavelet-based robust watermarking techniques robust to such JPEG2000 compression attacks are proposed in the literature. In this paper, we have categorized and evaluated the robustness of such wavelet-based image watermarking techniques against JPEG2000 compression, in terms of algorithmic choices, wavelet kernel selection, subband selection, or watermark selection using a new modular framework. As most of the algorithms use a different set of parametric combination, this analysis is particularly useful to understand the effect of various parameters on the robustness under a common platform and helpful to design any such new algorithm. The analysis also considers the imperceptibility performance of the watermark embedding, as robustness and imperceptibility are two main watermarking properties, complementary to each other.	algorithm;bitstream;categorization;coefficient;common platform;content adaptation;copy protection;digital watermarking;distortion;emergence;filter design;human visual system model;image resolution;jpeg 2000;orthogonal wavelet;peak signal-to-noise ratio;performance evaluation;region of interest;scalability;user-generated content;wavelet	Deepayan Bhowmik;G. Charith K. Abhayaratne	2013	Multimedia Systems	10.1007/s00530-013-0334-0	computer science;theoretical computer science;data mining;jpeg 2000;multimedia;robustness	Web+IR	42.7761373047493	-16.345514790955768	172835
c8768f03569122c1ed4cfe080cad3907a31ffaa0	fast block mode decision scheme for b-picture coding in h.264/avc	new technology;mode decision h 264 avc b slice differential mode allocation;automatic voltage control motion estimation encoding psnr prediction algorithms resource management complexity theory;complexity theory;psnr;h 264 avc;resource management;prediction algorithms;motion estimation;b slice;video coding;automatic voltage control;computationally complex;b picture coding;computational complexity;image quality;differential mode allocation;block motion estimation;differential mode;video coding motion estimation;encoding;video coding standard;mode decision;image quality fast block mode decision scheme b picture coding video coding standard computationally complex block motion estimation differential mode allocation;fast block mode decision scheme	The recent H.264/AVC video coding standard provides a higher coding efficiency than previous standards. H.264/AVC achieves a bit rate saving of more than 50 % with many new technologies, but it is computationally complex. Most of fast mode decision algorithms have focused on Baseline profile of H.264/AVC which does not consider B-picture coding. In this paper, a fast block mode decision scheme for B-pictures in High profile and Main profile is proposed to reduce the computational complexity for H.264/AVC. To reduce the block mode decision complexity in B-pictures of High profile, we use the SAD value after 16×16 block motion estimation. This SAD value is used for the classification feature to divide all block modes into some proper candidate search block modes. A differential mode allocation method is also used for the list (list 0, list 1) of B-slices based on the SAD value of the 16 × 16 block mode. The proposed algorithm shows the average speed-up factors of 41.9 ∼ 58.57% for IBBPBB sequences with a negligible bit increment and a minimal loss of image quality.	algorithm;algorithmic efficiency;best, worst and average case;bit plane;block-oriented terminal;computational complexity theory;data compression;h.264/mpeg-4 avc;image quality;korea computer center;motion estimation;next-generation network;qualitative comparative analysis;video coding format	Jong-Ho Kim;Hyo-Sung Kim;Byung-Gyu Kim;Hui Yong Kim;Seyoon Jeong;Jin Soo Choi	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5651952	image quality;computer vision;real-time computing;prediction;peak signal-to-noise ratio;computer science;resource management;theoretical computer science;context-adaptive variable-length coding;motion estimation;mathematics;context-adaptive binary arithmetic coding;computational complexity theory;encoding;statistics	Vision	46.81655882796928	-19.28508692220325	172865
677c43062da5bea42cbe4a80f00ecfa94f59ab1d	deep neural network compression for aircraft collision avoidance systems		One approach to designing decision making logic for an aircraft collision avoidance system frames the problem as a Markov decision process and optimizes the system using dynamic programming. The resulting collision avoidance strategy can be represented as a numeric table. This methodology has been used in the development of the Airborne Collision Avoidance System X (ACAS X) family of collision avoidance systems for manned and unmanned aircraft, but the high dimensionality of the state space leads to very large tables. To improve storage efficiency, a deep neural network is used to approximate the table. With the use of an asymmetric loss function and a gradient descent algorithm, the parameters for this network can be trained to provide accurate estimates of table values while preserving the relative preferences of the possible advisories for each state. By training multiple networks to represent subtables, the network also decreases the required runtime for computing the collision avoidance advisory. Simulation studies show that the network improves the safety and efficiency of the collision avoidance system. Because only the network parameters need to be stored, the required storage space is reduced by a factor of 1000, enabling the collision avoidance system to operate using current avionics systems.	airborne ranger;airborne collision avoidance system;approximation algorithm;artificial neural network;avionics;data compression;deep learning;dynamic programming;gradient descent;integer factorization;loss function;markov chain;markov decision process;pivot table;simulation;state space;storage efficiency;system x (computing);unmanned aerial vehicle;x window system	Kyle Julian;Mykel J. Kochenderfer;Michael P. Owen	2018	CoRR		control engineering;collision;storage efficiency;state space;dynamic programming;airborne collision avoidance system;artificial neural network;gradient descent;mathematics;markov decision process	Robotics	49.82114493428854	-22.807881127780565	172914
1bb7c31df27b2729bbb82fa3e6c19a6f37d5e93b	image compression by vector quantization: a review focused on codebook generation	data compression;generation code;generacion codigo;code generation;image;algorithme;algorithm;cuantificacion vectorial;vector quantization;image compression;imagen;decision;vector quantizer;compresion dato;codebook generation;compression donnee;algoritmo;quantification vectorielle	Recently. vector quantization (VQ) has received considerable attention. and has become an effective tool for image t:ompression. It provides a high compression ratio and a simple decoding process. However, studies on the practical implementation of VQ have revealed some major difficulties \uch as edge integrity and codebook design efficiency. After reviewing the stale-of-the-art in the field of vector quantiza: ion. we focus on iterative and non-iterative codebook sreneration algorithms. .-	algorithm;codebook;image compression;iteration;iterative method;vector quantization	N. Akrout;Rémy Prost;Robert Goutte	1994	Image Vision Comput.	10.1016/0262-8856(94)90038-8	data compression;computer vision;speech recognition;image compression;computer science;theoretical computer science;image;mathematics;linde–buzo–gray algorithm;vector quantization;code generation	AI	45.9592365169469	-13.063612502385954	172987
e5aec5daf1b99237dbdd0dbde17fc4aef57573f0	algorithms and architectures for hierarchical compression of video	discrete wavelet transforms;vector quantisation video signals image coding wavelet transforms;compression algorithm;image coding;discrete wavelet transform;standards;decoding;efficient algorithm;compression algorithms;hierarchical vector quantization;collaboration;video compression;video compression discrete wavelet transforms collaboration measurement standards rate distortion theory distortion measurement compression algorithms computer architecture vector quantization decoding;distortion measurement;software implementations hierarchical compression collaborative video heterogeneous networks standards video compression compression algorithms discrete wavelet transform hierarchical vector quantization encoder decoder table lookups hardware implementations;rate distortion theory;wavelet transforms;computer architecture;collaborative video;vector quantization;hierarchical compression;table lookups;encoder;software implementations;hardware implementations;vector quantizer;video signals;measurement standards;vector quantisation;heterogeneous networks;hardware implementation;decoder;heterogeneous network	This paper addresses the problem of collaborative video over “heterogeneous” networks. Current standards for video compression are not designed to deal with this problem. We define an additional set of metrics (:e., in addition to the standad rate versus distortion measure) to evaluate compression algorithms for this application. We also present an eficent algorithm and corresponding architectures for video compression in such an environment. The algorithm is a unique combination of the Discrete Wavelet Transform and Hierarchical Vector Quantization. It is unique in that both the encoder and the decoder are implemented with only table lookups. This makes both the software and hardware implementations very eficient and cheap	algorithm;codec;data compression;discrete wavelet transform;distortion;encoder;vector quantization	Mohan Vishwanath	1994		10.1109/ASAP.1994.331820	video compression picture types;data compression;real-time computing;heterogeneous network;computer science;theoretical computer science;video tracking;statistics	ML	49.08475498786023	-17.366731639598967	173067
848799dde9faedd3fb0fbd990790fdf4f9e8b018	a partial norm based early rejection algorithm for fast motion estimation	tecnologia electronica telecomunicaciones;successive elimination algorithm;fast motion estimation;halfway stop technique;block motion estimation;tecnologias;grupo a;partial norms	Recently, many algorithms have been proposed for fast full search motion estimation. Among them, successive elimination algorithm (SEA) and its modified algorithms significantly speed up the performance of the full search algorithm. By introducing the inequality equation between the norm and the mean absolute difference (MAD) of two matching blocks, the SEA can successively eliminate invalid candidate blocks without any loss in estimation accuracy. In this paper, we propose a partial norm based early rejection algorithm (PNERA) for fast block motion estimation. The proposed algorithm employs the sum of partial norms from several subblocks of the block. Applying the sum of partial norms to the inequality equation, we can significantly reduce the computational complexity of the full search algorithm. In an attempt to reduce the computational load further, the modified algorithms using partial norm distortion elimination (PNDE) and subsampling methods are also proposed. Experimental results show that the proposed algorithm is about 4 to 9 times faster than the original exhaustive full search, and is about 3 to 4 times faster than the SEA.	algorithm;motion estimation;rejection sampling	Won-Gi Hong;Young-Ro Kim;Tae-Myoung Oh;Sung-Jea Ko	2005	IEICE Transactions	10.1093/ietfec/e88-a.3.626	mathematical optimization;combinatorics;mathematics;algorithm	Vision	49.29010938765813	-19.354468525368816	173336
07257cf6121a9e8ad1d47083a4eccfdec442b82d	applying the hamiltonian algorithm to optimize jpeg quantization tables	circuit decodeur;quantization;table quantification;image storage;optimisation;image processing;algorithme hamiltonien;optimizacion;optimal method;quantization table;senal crominancia;procesamiento imagen;stockage image;qualite image;circuito desciframiento;traitement image;experimental result;decoding circuit;image transmission;chrominance signal;jpeg;image quality;resultado experimental;signal chrominance;optimization;calidad imagen;halmitonian algorithm;resultat experimental;transmission image;almacenamiento imagen;transmision imagen	Optimizing JPEG quantization tables is expected to transmit or store JPEG images. The Hamiltonian algorithm is one optimizing method and it can simultaneously optimize numerous parameters. JPEG qunatization tables consists of 128 variables. We propose applying the Hamiltonian algorithm for optimizing JPEG quantization tables. The JPEG quantization tables, which are optimized by the Hamiltonian algorithm, can reduce the rate and improve the quality of a decoded JPEG image. This report describes a configuration of a JPEG quantization table optimizing system and the experimental results of using it.	algorithm;code;hamiltonian (quantum mechanics);jpeg	Kazutaka Hirata;Jun-Ichi Yamada;Kazumasa Shinjo	1999		10.1117/12.334576	lossless jpeg;computer vision;theoretical computer science;jpeg;jpeg 2000;mathematics;quantization;algorithm	Vision	46.28909012835868	-13.218845764353391	173384
2acd9e9d52435e33fe6110708100d5ca9a9541b8	probabilistic collision state checker for crowded environments	collision mitigation;probability;personal communication networks;probability collision avoidance mobile robots;path planning;bismuth;crowded environments;robots robotics and automation personal communication networks vehicle safety vehicle detection usa councils path planning humans state estimation collision mitigation;vehicle detection;mobile robots;usa councils;state estimation;acceleration;trajectory;robots;inevitable collision states;probabilistic collision state checker;humans;collision avoidance;vehicle safety;probabilistic logic;inevitable collision states probabilistic collision state checker crowded environments path planning algorithms robots;robotics and automation;robot kinematics;path planning algorithms	For path planning algorithms of robots it is important that the robot does not reach a state of inevitable collision. In crowded environments with many humans or robots, the set of possible inevitable collision states (ICS) is often unacceptably high, such that the robot has to stop and wait in too many situations. For this reason, the concept of ICS is extended to probabilistic collision states (PCS), which estimates the collision probability for a given state. This allows to efficiently run planning algorithms through crowded environments when accepting a certain collision probability. A further novelty is that the obstacles possibly react to the robot in order to mitigate the risk of a collision. The results show a significant difference in interaction behavior. Thus, this approach is especially suited for active and non-deterministic moving obstacles in the robot workspace.	algorithm;humans;motion planning;robot;workspace	Daniel Althoff;Matthias Althoff;Dirk Wollherr;Martin Buss	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509369	acceleration;robot;mobile robot;computer vision;simulation;computer science;engineering;artificial intelligence;trajectory;bismuth;probability;motion planning;probabilistic logic;computer security;robot kinematics	Robotics	52.73564408002664	-23.26521081490808	173595
f503875911e9662b4c6e42bc9a1d82a9ae671717	a framework of enhancing image steganography with picture quality optimization and anti-steganalysis based on simulated annealing algorithm	filigranage numerique;digital watermarking;bucle cerrado;metodo caso peor;optimisation;steganography simulated annealing computational modeling iterative algorithms cost function testing convergence mean square error methods humans visual system;convergence;steganographie;steganalysis;analisis estadistico;image processing;image resolution;optimizacion;cost function;iterative algorithms;simulated annealing algorithm;cryptanalyse;data embedding;erreur quadratique moyenne;anti steganalysis capability;procesamiento imagen;image steganography enhancement;indice aptitud;testing;simulated annealing;qualite image;traitement image;picture quality optimization;cryptanalysis;indice aptitude;criptoanalisis;steganography;image enhancement;esteganografia;computational modeling;recuit simule;statistical analysis;capability index;human visual system;mean square error;image quality;filigrana digital;analyse statistique;closed loop;methode cas pire;human visual system image steganography enhancement picture quality optimization anti steganalysis capability simulated annealing algorithm statistical undetectability closed loop computing framework anti steganalysis tester cost function mean square error;statistical undetectability;mean square error methods;aparato visual;closed loop computing framework;appareil visuel;recocido simulado;optimization;calidad imagen;humans;boucle fermee;image watermarking;error medio cuadratico;worst case method;anti steganalysis tester;visual system;steganography data embedding human visual system image watermarking steganalysis;steganography image enhancement image resolution mean square error methods simulated annealing statistical analysis	Picture quality and statistical undetectability are two key issues related to steganography techniques. In this paper, we propose a closed-loop computing framework that iteratively searches proper modifications of pixels/coefficients to enhance a base steganographic scheme with optimized picture quality and higher anti-steganalysis capability. To achieve this goal, an anti-steganalysis tester and an embedding controller-based on the simulated annealing (SA) algorithm with a proper cost function-are incorporated into the processing loop to conduct the convergence of searches. The cost function integrates several performance indices, namely, the mean square error, the human visual system (HVS) deviation, and the differences in statistical features, and guides a proper direction of searches during SA optimization. Our proposed framework is suitable for the kind of steganographic schemes that spreads each message information into multiple pixels/coefficients. We have selected two base steganographic schemes for implementation to show the applicability of the proposed framework. Experiment results show that the base schemes can be enhanced with better performances in image PSNR (by more than 5.0 dB), file-size variation, and anti-steganalysis pass-rate (by about 10% ~ 86%, at middle to high embedding capacities).	algorithm;anti-aliasing filter;apple a5;apple a6;apple a7;apple a9;coefficient;computational complexity theory;discrete cosine transform;embedded system;experiment;feasible region;human visual system model;image quality;iteration;loss function;mathematical optimization;mean squared error;multi-objective optimization;peak signal-to-noise ratio;performance;pixel;rewrite (programming);simulated annealing;steganalysis;steganography;whole earth 'lectronic link	Guo-Shiang Lin;Yi-Ting Chang;Wen-Nung Lie	2010	IEEE Transactions on Multimedia	10.1109/TMM.2010.2051243	computer vision;simulated annealing;telecommunications;image processing;computer science;theoretical computer science;machine learning;mathematics;algorithm;statistics	Visualization	42.52530053134713	-10.409568994546609	173900
7e74e7900cbe79e0bd8d659b2ec9ceab611421e5	optimal input sizes for neural network de-interlacing	4230;neural networks;0705m;format video;0130c;imagerie;desentrelacement;imagery;formato video;deinterlacing;imagineria;video;reseau neuronal;desentrelazado;video format;neural network	Interlaced scanning has been used for a long time in analog SDTV systems. With the increasing demand for high resolution video services, a number of countries have started to offer high-definition television broadcasting services. Although recent displays such as LCD and PDP are more suitable for the progressive scan format, the interlaced scan format still constitute a key part of the HDTV standard (e.g., 1920*1080i) and many programs are provided as interlaced signals. Therefore, de-interlacing technique will play an important factor in video quality on flat panel displays.	artificial neural network;flat panel display;hdmi;image resolution;interlaced video;interlacing (bitmaps);liquid-crystal display;outline of television broadcasting;progressive scan;standard-definition television	Hyunsoo Choi;Guiwon Seo;Chulhee Lee	2009		10.1117/12.810569	computer vision;video;telecommunications;deinterlacing;time delay neural network;artificial neural network;computer graphics (images)	HCI	43.167410338723194	-23.739766994662947	174391
c944dfe4525928959a0bca5685dd3c03be34c875	hadamard transform based adaptive visible/invisible watermarking scheme for digital images	sigmoid function;invisible watermarking;visible watermarking;transform domain embedding;hadamard based watermarking;adaptive watermarking	In this work, a novel and adaptive visible/invisible watermarking scheme for embedding and extracting a digital watermark into/from an image is proposed. The proposed method uses an adaptive procedure for calculating scaling factor or scaling strength using sigmoid function in Hadamard transform domain. The value of scaling factor is governed by a control parameter. The control parameter can be adjusted to make the watermarking scheme as either visible or invisible. The proposed methodology facilitates in preserving ownership rights and prevents the piracy of digital data which are considered to be the basic needs of digital watermarking. As proposed watermarking process is carried out in Hadamard transform domain, it is more robust to image/signal processing attacks. The experimental results and performance analysis confirm the efficiency of the proposed scheme. a 2013 Elsevier Ltd. All rights reserved.	digital data;digital image;digital watermarking;hadamard transform;image scaling;profiling (computer programming);sigmoid function;signal processing	V. Santhi;P. Arulmozhivarman	2013	J. Inf. Sec. Appl.	10.1016/j.istr.2013.01.001	computer vision;computer science;theoretical computer science;machine learning;sigmoid function	AI	40.6834871596964	-10.356523812763623	174512
412aefa4cf728c5c5dc60d1d030533ba9e481fb1	countering jpeg anti-forensics	quantization;image coding;jpeg compression;transform coding image coding discrete cosine transforms q factor quantization forensics noise;quality factor;transform coding;discrete cosine transform;jpeg compression digital image forensics anti forensics;discrete cosine transforms;image quality;image coding discrete cosine transforms;digital image forensics;digital image;forensics;image signal countering jpeg antiforensics jpeg coding footprints characteristics knowledgeable attacker dct domain;q factor;noise;anti forensics	JPEG coding leaves characteristic footprints that can be leveraged to reveal doctored images, e.g. providing the evidence for local tampering, copy-move forgery, etc. Recently, it has been shown that a knowledgeable attacker might attempt to remove such footprints by adding a suitable anti-forensic dithering signal to the image in the DCT domain. Such noise-like signal restores the distribution of the DCT coefficients of the original picture, at the cost of affecting image quality. In this paper we show that it is possible to detect this kind of attack by measuring the noisiness of images obtained by re-compressing the forged image at different quality factors. When tested on a large set of images, our method was able to correctly detect forged images in 97% of the cases. In addition, the original quality factor could be accurately estimated.	anti-computer forensics;coefficient;discrete cosine transform;dither;image quality;jpeg	Giuseppe Valenzise;Vitaliano Nobile;Marco Tagliasacchi;Stefano Tubaro	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6115854	computer vision;computer science;theoretical computer science;forensic science;q factor;algorithm;computer graphics (images)	Vision	39.882128600641806	-11.806312676606101	174572
6ee0c6576bd9460fc1ea22325e7ac71779cc26e0	a real-time quadtree searchless fractal image compression device	fractal image compression;real time		fractal compression;image compression;quadtree;real-time transcription	Songpol Ongwattanakul;Xianwei Wu;David Jeff Jackson	2004			computer science;parallel computing;computer vision;fractal transform;quadtree;artificial intelligence;fractal compression	HCI	42.18414498597641	-15.900570972993902	174577
83398b145748c0a472013e33480e29703edeec11	the perceptually transparent coding for image	quantization factor;quantization;image coding;codecs;image processing;data compression;quantisation signal data compression distortion image coding;biomedical imaging;images compression transparent image coding quantization factor just noticeable distortion;bit rate;quantisation signal;image coding redundancy quantization discrete cosine transforms bit rate codecs humans image processing equations biomedical imaging;distortion;redundancy;image compression;discrete cosine transforms;images compression;humans;transparent image coding;just noticeable distortion	In this paper, an improved transparent image coding algorithm is proposed, where the quantization factor can be tuned for each block according to the just-noticeable distortion (JND). The experimental results show that the images compressed by the proposed method are hardly distinguished from the original images. Therefore, the proposed method can achieve perceptually transparent quality for images. Moreover, the bitrate of the proposed algorithm is less than that of the state-of-the-arts lossless and near-lossless codecs.	algorithm;codec;distortion;image compression;lossless compression;redundancy (information theory);triple modular redundancy	Zhenyu Wei;King Ngi Ngan	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5118082	data compression;computer vision;codec;speech recognition;distortion;quantization;image processing;image compression;computer science;theoretical computer science;mathematics;redundancy;algorithm	EDA	44.18485393318536	-16.181325163131223	174593
e4b127ac44adfe4848c6089e23608e40d5131ac0	reversible data-hiding scheme for 2-d vector maps based on difference expansion	map data authentication;watermarking;data hiding;reversible watermarking;watermarking data encapsulation;data embedding;reversible data hiding difference expansion invisibility payload;2d vector maps;data mining;reversible data hiding scheme;data encapsulation;difference expansion;map data authentication reversible data hiding scheme 2d vector maps difference expansion reversible watermarking invertible integer mappings;data encapsulation watermarking data mining robustness authentication discrete cosine transforms payloads information systems geographic information systems copyright protection;invertible integer mappings;transforms;robustness;image analysis;payload;correlation;invisibility;security;reversible data hiding	Reversible watermarking is suitable for hiding data in 2-D vector maps because the distortions induced by data embedding can be removed after extracting the hidden bits. In this paper, two reversible data-hiding schemes are explored based on the idea of difference expansion. The first scheme takes the coordinates of vertices as the cover data and hides data by modifying the differences between the adjacent coordinates. The scheme achieves high capacity in the maps with highly correlated coordinates. Instead of the raw coordinates, the second scheme adopts the manhattan distances between neighbor vertices as the cover data. A set of invertible integer mappings is defined to extract manhattan distances from coordinates and the hidden data are embedded by modifying the differences between the adjacent distances. For those maps where distances exhibit high correlation, this scheme shows better performance than the former one, both in capacity and invisibility. Three different maps with distinct features are used for the experiments. The results indicate that two proposed schemes suit different types of maps, respectively, according to the correlation of the selected cover data. Both schemes are strictly reversible. In addition, they can be slightly robust for low amplitude distortions by selecting higher digits for data hiding. The potential applications of proposed schemes may include map data authentication, secret communication, etc.	computer-aided design;data structure;digital watermarking;distortion;embedded system;experiment;hash function;message authentication;neighbourhood (graph theory);performance;polygon (computer graphics);polygon mesh;reversible computing;robustness (computer science);steganalysis;vector map;vertex (geometry)	Xiao Tong Wang;Cheng Yong Shao;X. G. Xu;Xiamu Niu	2007	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2007.902677	payload;discrete mathematics;image analysis;digital watermarking;computer science;information security;theoretical computer science;data mining;mathematics;information hiding;correlation;robustness;invisibility	Visualization	39.77808783737872	-10.748300970125394	174811
546b030f421ef809f2eaa8827751a9ad706d4684	integral images compression scheme based on view extraction	image coding;image resolution;hevc coding scheme view extraction integral images compression scheme glasses free 3d video technology light field scene representation autostereoscopic techniques;bit rate;video coding data compression image representation stereo image processing;three dimensional displays;image reconstruction;imaging;view extraction integral imaging plenoptic imaging holoscopy image and video coding;image coding image reconstruction encoding imaging three dimensional displays image resolution bit rate;encoding	Integral imaging is a glasses-free 3D video technology that captures a light-field representation of a scene. This representation eliminates many of the limitations of current stereoscopic and autostereoscopic techniques. However, integral images have a large resolution and a structure based on microimages which is challenging to encode. In this paper a compression scheme for integral images based on view extraction is proposed. Average BD-rate gains of 15.7% and up to 31.3% are reported over HEVC. Parameters of the proposed coding scheme can take a large range of values. Results are first provided with an exhaustive search of the best configuration. Then an RD criterion is proposed to avoid exhaustive search methods, saving runtime while preserving the gains. Finally, additional runtime savings are reported by exploring how the different parameters interact.	autostereoscopy;blu-ray;brute-force search;codec;encode;high efficiency video coding;image compression;integral imaging;iteration;iterative method;low-pass filter;raster document object;ruby document format;stereoscopy	Antoine Dricot;Joël Jung;Marco Cagnazzo;Béatrice Pesquet-Popescu;Frédéric Dufaux	2015	2015 23rd European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2015.7362353	computer vision;image processing;mathematics;multimedia;context-adaptive binary arithmetic coding;computer graphics (images)	Vision	44.53246539400438	-18.620380094467112	174820
60550527dce777df7a2d5bb856ba8e0dedfa0615	geometry sequence based progressive mesh compression	video compression;computer graphic;progressive mesh;internal standard	Nowadays computer graphics applications are widely applied in various fields. Due to the limitations of storage, transmission or display constraints, the compression, especially progressive compression of 3D meshes, is becoming a more and more urgent research topic. A lot of schemes have been proposed for compressing progressive meshes. However, there is still no common framework or international standard of compressing mesh, which restricts the application of meshes. In comparison, 2D video compression is a well-studied topic over the past decades and many international standards like MPEG 2, MPEG 4, have already been widely accepted by the industry.	3d printing;computer graphics;data compression;lossless compression;mpeg-2;mesh networking;moving picture experts group;progressive meshes	Jiheng Yang;Baocai Yin;Yanfeng Sun;Dehui Kong	2007		10.1145/1280720.1280738	data compression;computer science;internal standard;multimedia;computer graphics (images)	Visualization	42.946220957340365	-19.904418201775787	174842
3968919cdd1283415c2c4372f57a325a33bf52ee	an automatic scheme for stereoscopic video object-based watermarking using qualified significant wavelet trees	stereoscopic video;discrete wavelet transforms;qualified significant wavelet trees;watermarking;embedded zerotree wavelet;signal distortions;video object segmentation stereoscopic video video object extraction watermarking qualified significant wavelet trees discrete wavelet transform inverse dwt embedded zerotree wavelet signal distortions jpeg lossy compression sharpening blurring;image coding;discrete wavelet transform;image segmentation;image segmentation watermarking trees mathematics stereo image processing video coding discrete wavelet transforms feature extraction parameter estimation distortion;video compression;jpeg lossy compression;transform coding;trees mathematics;sharpening;video coding;distortion;blurring;inverse dwt;video object segmentation;video object extraction;feature extraction;stereo image processing;performance analysis;pattern recognition;system testing;parameter estimation;watermarking discrete wavelet transforms pattern recognition video compression performance analysis performance loss system testing distortion transform coding image coding;performance loss	In this paper a fully automatic system for embedding visually recognizable watermark patterns to video objects is proposed. The architecture consists of 3 main modules. During the first module unsupervised video object extraction is performed, by analyzing stereoscopic pairs of frames. Then each video object is decomposed into three levels with ten subbands, using the Discrete Wavelet Transform (DWT) and three pairs of subbands are formed (HL3, HL2), (LH3, LH2) and (HH3, HH2). Next Qualified Significant Wavelet Trees (QSWTs) are estimated for the specific pair of subbands that contains the highest energy content compared to the other two pairs. QSWTs are derived from the Embedded Zerotree Wavelet (EZW) algorithm and they are high-energy coefficient paths within the selected pair of subbands. Finally during the third module, visually recognizable watermark patterns are redundantly embedded to the coefficients of the highest energy QSWTs and the inverse DWT is applied to provide the watermarked video object. The performance of the proposed video object watermarking system is tested under various signal distortions such as JPEG lossy compression, sharpening, blurring and adding different types of noise. Experimental results on real life stereoscopic images are presented to indicate the efficiency and robustness of the proposed scheme.	algorithm;coefficient;digital watermarking;discrete wavelet transform;distortion;embedded zerotrees of wavelet transforms;embedded system;jpeg;lossy compression;object-based language;real life;stereoscopy;wavelet tree	Nikolaos D. Doulamis;Anastasios D. Doulamis;Stefanos D. Kollias;Klimis S. Ntalianis	2002		10.1109/ICIP.2002.1039017	data compression;computer vision;transform coding;speech recognition;distortion;feature extraction;digital watermarking;computer science;mathematics;image segmentation;estimation theory;discrete wavelet transform;system testing;statistics;computer graphics (images)	Vision	42.273736486030785	-12.658787604478855	175025
16dd516d398eed430afa284e68bf3e319a28a721	fast 2d discrete cosine transform on compressed image in restricted quadtree and shading format	ombrage environnement;transformation cosinus;transformacion discreta;image processing;procesamiento imagen;traitement image;discrete cosine transform;algorithme;algorithm;compression image;image compression;fast algorithm;transformacion coseno;discrete transformation;cosine transform;quadtree;transformation discrete;dct;sombrajo;algoritmo;compresion imagen;shading	Given a compressed image in the restricted quadtree and shading format, this paper presents a fast algorithm for computing 2D discrete cosine transform (DCT) on the compressed grey image directly without the need to decompress the compressed image. The proposed new DCT algorithm takes O (K2 logK + N2) time where the decompressed image is of size N × N and K denotes the number of nodes in the restricted quadtree. Since commonly K < N , the proposed algorithm is faster than the indirect method by decompressing the compressed image first, then applying the conventional DCT algorithm on the decompressed image. The indirect method takes O (N2 logN) time.  2002 Elsevier Science B.V. All rights reserved.	algorithm;discrete cosine transform;quadtree;shading	Kuo-Liang Chung;Wen-Ming Yan	2002	Inf. Process. Lett.	10.1016/S0020-0190(01)00188-0	arithmetic;computer vision;image processing;computer science;discrete cosine transform;mathematics;algorithm;computer graphics (images)	Vision	44.44112298513473	-12.19452262626487	175088
3ebe0098ac72a6d2508224f4f281fd422c5c90f2	spatiotemporal error concealment with optimized mode selection and application to h.264	detection erreur;deteccion error;interpolation;robust video coding;selection mode;error concealment;video signal processing;telecommunication sans fil;edge preserving interpolation;information transmission;correction erreur;texture synthesis;interpolacion;qualite image;visual quality;mode selection;synthese texture;algorithme;algorithm;video coding;texture analysis;senal video;signal video;codage video;telecomunicacion sin hilo;error correction;image quality;gradient boundary matching;video transmission;spatial interpolation;h 264;traitement signal video;video signal;packet networks;calidad imagen;transmision informacion;correccion error;error detection;transmission information;h 26l;algoritmo;wireless telecommunication	In this paper we propose an error concealment algorithm for video transmission over lossy packet networks. The proposed technique is based on temporal and spatial interpolation. A sophisticated mode selection algorithm decides whether to employ the temporal or the spatial part, or a combination thereof, to estimate a missing macroblock; the selection does not rely on knowledge of the original coding modes. The resulting error concealment algorithm is designed so as to optimize both PSNR and visual quality of the restored video sequence, and employs directional interpolation and texture analysis/synthesis. The technique has been applied to H.264 coded video, providing satisfactory results on a number of test sequences. r 2003 Elsevier B.V. All rights reserved.	error concealment;h.264/mpeg-4 avc;lossy compression;macroblock;mesa;multivariate interpolation;network packet;pdf/a;peak signal-to-noise ratio;selection algorithm	Stefano Belfiore;Marco Grangetto;Enrico Magli;Gabriella Olmo	2003	Sig. Proc.: Image Comm.	10.1016/j.image.2003.08.008	computer vision;error detection and correction;speech recognition;telecommunications;interpolation;computer science;mathematics;algorithm;statistics	Vision	46.66409279410509	-14.206428181059874	175453
8d68b2c7fe415aa5b2f8a5a0f216f03befd57f93	a fast exact gla based on code vector activity detection	tabla codificacion;cluster algorithm;iterative algorithms partitioning algorithms computer science vector quantization image generation image coding clustering algorithms application software genetic algorithms iterative methods;image coding;image classification vector quantisation search problems image coding;image processing;data compression;etude theorique;image classification;indexing terms;classification;traitement image;algorithme;vector quantization;image compression;codebook;table codage;partition;algorithms;genetic algorithm;image coding fast exact gla code vector activity detection distance calculations reduction generalized lloyd algorithm codebook vector quantization vq reduced comparison search training vectors classification running time reduction;tabu search;search problems;vector quantizer;theoretical study;vector quantisation;encoding;compression donnee;codage;quantification vectorielle	This paper introduces a new method for reducing the number of distance calculations in the generalized Lloyd algorithm (GLA), which is a widely used method to construct a codebook in vector quantization. Reduced comparison search detects the activity of the code vectors and utilizes it on the classification of the training vectors. For training vectors whose current code vector has not been modified, we calculate distances only to the active code vectors. A large proportion of the distance calculations can be omitted without sacrificing the optimality of the partition. The new method is included in several fast GLA variants reducing their running times over 50% on average.	codebook;distance;familial generalized lipodystrophy;linde–buzo–gray algorithm;vector quantization	Timo Kaukoranta;Pasi Fränti;Olli Nevalainen	2000	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.855429	data compression;partition;contextual image classification;genetic algorithm;index term;image processing;biological classification;tabu search;image compression;computer science;theoretical computer science;machine learning;codebook;pattern recognition;mathematics;vector quantization;algorithm;encoding	Visualization	44.152740928098375	-12.914935175591578	175591
2f10effd7fbb5274fbcce0f6418db8c4ba63583e	near-lossless image compression: minimum-entropy, constrained-error dpcm	image coding;gray level error;data compression;constrained error dpcm;trellis coded modulation;minimum entropy methods;lossless image compression;biomedical imaging;minimum entropy;differential pulse code modulation;algorithm;image coding entropy context modeling predictive models biomedical imaging medical diagnostic imaging ice computer errors image reconstruction;prediction theory;image reconstruction;entropy codes;trellis coded modulation image coding data compression minimum entropy methods differential pulse code modulation prediction theory entropy codes;quantized prediction error sequence;predictive models;near lossless image compression;entropy;trellises;contexts near lossless image compression minimum entropy constrained error dpcm quantized prediction error sequence gray level error trellises conditioning prediction error model algorithm;context modeling;ice;computer errors;conditioning prediction error model;medical diagnostic imaging;contexts	"""A near-lossless image compression scheme is presented. It is essentially a DPCM system with a mechanism incorporated to minimize the entropy of the quantized prediction error sequence. With a \near-lossless"""" criterion of no more than a d gray level error for each pixel, where d is a small non-negetive integer, trellises describing all allowable quantized prediction error sequences are constructed. A set of \contexts"""" is de ned for the conditioning prediction error model and an algorithm that produces minimum entropy conditioned on the contexts is presented. Finally, experimental results are given."""	algorithm;entropy (information theory);grayscale;image compression;lossless compression;pixel	Ligang Ke;Michael W. Marcellin	1995		10.1109/ICIP.1995.529705	data compression;iterative reconstruction;medical imaging;entropy;trellis modulation;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;lossless compression;predictive modelling;context model;algorithm	ML	48.41227848587091	-13.129484721200653	175691
1a78214583e48687fe15f2538a4d0fb0f0c572fc	a robust parallel architecture for adaptive color quantization	simd structure;adaptive color quantization;quantization;image storage;neural net architecture parallel architectures image coding image colour analysis data compression adaptive codes quantisation signal vlsi digital signal processing chips self organising feature maps;propagation losses;3d self organizing map;image coding;colored noise;data compression;lossy compression;color;3d som architecture;vlsi area time efficiency;vlsi area time efficiency robust parallel architecture adaptive color quantization color image compression image storage image display low cost monitors palette based mapping noise lossy compression image transmission 3d self organizing map 3d som architecture color vectors clustering parallel architecture simd structure parallel winner determination;neural net architecture;low cost monitors;adaptive codes;noise robustness;three dimensional;quantisation signal;parallel architectures;image transmission;self organising feature maps;image display;robust parallel architecture;image colour analysis;computer displays;palette based mapping;robust method;vlsi;digital signal processing chips;self organized map;parallel architecture;parallel architectures quantization colored noise image coding image storage noise robustness color costs computer displays propagation losses;color vectors clustering;parallel winner determination;color quantization;noise;color image compression	Adaptive color quantization (ACQ) as described here is one method of color image compression employed for image storage and display in low cost monitors. Due to the nature of the palette based mapping employed in ACQ, it is prone to large errors due to noises introduced in transmission or additional lossy compression. This paper proposes a robust method of ACQ using a three dimensional self organizing map (3D SOM) which is tolerant to noises. The robustness of the 3D SOM is due to the property of such maps to cluster the color vectors in a topological manner. An efficient parallel architecture based on the SIMD structure and parallel winner determination scheme, is presented for the implementation of the 3D SOM. The VLSI area-time efficiency of this parallel architecture over the current implementations is also established.	color quantization	Babu Mailachalam;T. Srikanthan	2000		10.1109/ITCC.2000.844201	computer vision;computer science;theoretical computer science;computer graphics (images)	Vision	44.03666846971207	-13.987185127791824	175833
479dc5b74318508e75c1402076c4bbea229c5d6d	voice activity detection algorithm based on radial basis function network	background noise;nivel ruido;code lineaire;least mean square;tecnologia electronica telecomunicaciones;methode moindre carre moyen;algorithm performance;detection signal;code excited linear prediction;least mean squares methods;algorithme k moyenne;correction erreur;signal detection;fonction base radiale;adaptive threshold;niveau bruit;senal vocal;codage predictif;signal vocal;deteccion senal;noise level;radial basis function;radial basis function network;resultado algoritmo;error correction;linear code;signal classification;prediccion lineal;performance algorithme;codificacion predictiva;classification signal;signal acoustique;ruido fondo;algoritmo k media;k means algorithm;linear prediction;acoustic signal;correccion error;classification automatique;voice activity detection;tecnologias;grupo a;automatic classification;vocal signal;bruit fond;funcion radial base;clasificacion automatica;predictive coding;k means clustering;senal acustica;prediction lineaire;codigo lineal	This paper proposes a Voice Activity Detection (VAD) algorithm using Radial Basis Function (RBF) network. The k-means clustering and Least Mean Square (LMS) algorithm are used to update the RBF network to the underlying speech condition. The inputs for RBF are the three parameters a Code Excited Linear Prediction (CELP) coder, which works stably under various background noise levels. Adaptive hangover threshold applies in RBF-VAD for reducing error, because threshold value has trade off effect in VAD decision. The experimental results show that the proposed VAD algorithm achieves better performance than G.729 Annex B at any noise level.	algorithm;radial (radio);radial basis function network;voice activity detection	Hong-Ik Kim;Sung-Kwon Park	2005	IEICE Transactions	10.1093/ietcom/e88-b.4.1653	speech recognition;computer science;artificial intelligence;mathematics;algorithm;k-means clustering	Vision	45.58979474398701	-11.343514354753388	175900
4cdef5da3c463d531789e94585a751c402e8705e	h.264 video streaming with data-partitioning and growth codes	equal error protection;streaming media error correction codes resilience channel coding protection video codecs psnr forward error correction decoding video compression;channel coding;erasure channel;wireless channels;error correction codes;video streaming;psnr;decoding;data partitioning video streaming uep channel coding h 264;wireless channels channel coding digital subscriber lines video coding video streaming;uep channel coding;reference frame;video quality;video codec;data partitioning;digital subscriber lines;wireless communication;video coding;resilience;streaming media;error protection h 264 video streaming growth codes raptor channel coding incremental protection h 264 video codec data partitioned network adaption layer units video reference frame protection adsl erasure channel wireless channel;h 264;error resilience;encoding	This paper demonstrates that Growth codes, based on Raptor channel coding, allow incremental protection of H.264 video codec data-partitioned Network Adaption Layer units. When combined with increased protection of video reference frames, in an ADSL erasure channel up to 10 dB in video quality (PSNR) can be gained through this scheme compared to equal error protection with rateless codes. Equivalent gains occur in a wireless channel from combining data-partitioning with error protection. The bitrate overhead from data-partitioning is also shown to be less than from other H.264 error resilient tools.	asymmetric digital subscriber line;binary erasure channel;code;codec;forward error correction;h.264/mpeg-4 avc;independence day: resurgence;network abstraction layer;overhead (computing);peak signal-to-noise ratio;streaming media	Rouzbeh Razavi;Martin Fleury;Muhammad Altaf;Hanadi Sammak;Mohammed Ghanbari	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414052	reference frame;binary erasure channel;real-time computing;digital subscriber line;peak signal-to-noise ratio;channel code;telecommunications;computer science;video quality;wireless;psychological resilience;encoding;statistics;computer network	Robotics	48.5459703687664	-15.88666116113744	175968
7374743fcadecc6a3461badbad92ba95cc3afcdb	regularized subband coding scheme	artefacto;regularisation;image coding;restauration image;image processing;quantization noise;optimal bit allocation subband coding scheme regularized image restoration global regularity decompressed image scalar quantization pyramidal lattice vector quantization optimal quantizer allocation block effect vq quantization noise image coding;subband decomposition;procesamiento imagen;image coding image restoration filters image reconstruction vector quantization lattices bit rate noise reduction frequency filtering;image restoration;indexing terms;traitement image;artefact;experimental result;regularization;restauracion imagen;codificacion;cuantificacion vectorial;scalar quantization;vector quantization;descomposicion subbanda;coding;resultado experimental;regularizacion;decomposition sous bande;global regularity;vector quantisation image coding image restoration;resultat experimental;vector quantisation;subband coding;bruit quantification;codage;ruido cuantificacion;lattice vector quantization;quantification vectorielle	Two enhanced subband coding schemes using a regularized image restoration technique are proposed: the first controls the global regularity of the decompressed image; the second extends the first approach at each decomposition level. The quantization scheme incorporates scalar quantization (SQ) and pyramidal lattice vector quantization (VQ) with both optimal bit and quantizer allocation. Experimental results show that both the block effect due to VQ and the quantization noise are significantly reduced.	allocation;blocking (computing);circuit restoration;computation;image restoration;quantization (signal processing);sound quality;sub-band coding;vector quantization	Rémy Prost;Yi Ding;Atilla Baskurt;Hugues Benoit-Cattin	1999	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.753743	sub-band coding;image restoration;regularization;computer vision;mathematical optimization;index term;quantization;image processing;computer science;mathematics;coding;vector quantization	Vision	46.62951059870518	-13.138998484661375	176006
6ffbe713143cf75db9e297433bce5d99f7675a8d	an image coding scheme using smvq and support vector machines	encoding decoding;image coding;discrete wavelet transform;support vector machines;spatial relation;image compression;vector quantizer;support vector machine;side match vector quantization	To efficiently compress images, the spatial relations among image blocks to achieve low bit-rate compression has been widely adopted in these years. An example of using such spatial relations is side-match vector quantization (SMVQ). In this paper, a new technique utilizing support vector machines (SVM) to enhance the quality of images compressed by an algorithm based on SMVQ is proposed. We incorporate SVM as a part of the encoder/decoder to detect the edges across boundaries and, therefore, to further improve the accuracy in the block predicting phase, while the number of codewords available in the state codebooks can be enlarged at the same time without increasing the bit-rate. Our simulation confirms the superiority of the proposed scheme. Reasonable improvements in the resulting images were also obtained. r 2006 Elsevier B.V. All rights reserved.	algorithm;code word;codebook;encoder;simulation;support vector machine;vector quantization	Chin-Chen Chang;Chia-Te Liao	2006	Neurocomputing	10.1016/j.neucom.2005.02.022	support vector machine;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;vector quantization	AI	41.894061666089605	-13.702401341444688	176334
01c9e7cc5d51311951bf9e0738c4706ba7df8cc6	a novel scheme to code object flags for video synopsis	scalable browsing surveillance video object flags video synopsis;video surveillance;video coding;video surveillance redundancy video coding;redundancy;encoding surveillance streaming media video coding electron tubes redundancy nonhomogeneous media;video synopsis browsing object flag coding scheme digital video surveillance fast browsing technique region of interest information coding roi object mapping flags intraframe coding temporal domain redundancy region flag scanning mode coding cost	With extending applications of digital surveillance video, the fast browsing technique for surveillance video has become a hot spot in the domain. As one of the most important supporting technologies, region of interest (ROI) information coding is a necessary part of fast browsing framework for surveillance video. Therefore, a novel coding method of object region flags and successful application of object mapping flags for the scalable browsing of surveillance video are proposed in this paper. This object region flags coding method includes both intra-frame coding and inter-frame coding, wherein, the former eliminates the redundancy in spatial domain by improving the scanning mode of region flags, the latter eliminates the redundancy in temporal domain, which cuts coding cost obviously and provides essential supports for video synopsis browsing of surveillance video.	closed-circuit television;computer and network surveillance;intra-frame coding;object code;region of interest;scalability;video synopsis	Shizheng Wang;Heguang Liu;Danfeng Xie;Binwei Zeng	2012	2012 Visual Communications and Image Processing	10.1109/VCIP.2012.6410771	scalable video coding;computer vision;computer science;video capture;video tracking;multimedia;video processing;smacker video;redundancy;h.261;multiview video coding;computer graphics (images)	Vision	43.24682123317287	-19.163580062597116	176728
2736c37ba0a49c8fdaa562b8aa5438acc69f7750	quad-tree-based image shape coding with block compensation	shape image coding mpeg 4 standard tree data structures image restoration video compression bit rate layout arithmetic decoding;software standards image coding data compression;image coding;data compression;image object shape coding quad tree based coding block compensation mpeg 4;tree structure;software standards	Image shape coding is a core technique in MPEG-4. In the paper, we present a quad-tree-based technique for image object shape coding. Quad tree has been used to represent the image content since long time ago, it describes the hierarchy of various sizes of rectangular blocks that collectively constitute the image object area. Quad tree structure can be quite complicate to describe an image object with unsmooth boundary. We propose a block compensation and a run-length scheme to effectively attack the problem. Experimental results show that the algorithm is quite efficient in comparison with other methods.	algorithm;quadtree;run-length encoding;tree structure	Lih-Yang Wang;Ching-Hui Lai;Kuan-Ren Pan	2005	Third International Conference on Information Technology and Applications (ICITA'05)	10.1109/ICITA.2005.227	color cell compression;computer vision;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;tunstall coding;context-adaptive binary arithmetic coding;computer graphics (images)	Robotics	44.91450050146154	-15.301802898693325	176928
838a5ec216a6d4fd68090e6a0096b41cc6f1d4b7	a new wavelet-based robust watermarking for digital image	watermarking;image coding;robust watermarking;independent component analysis wavelet based robust watermarking digital image embedding process mean removal technique middle frequency sub bands noise visibility function extraction process;noise visibility function;independent component analysis;watermarking feature extraction image coding independent component analysis;mean removal technique;feature extraction;wavelet based robust watermarking;middle frequency sub bands;digital image watermarking;digital image;robustness watermarking digital images wavelet domain data mining automation independent component analysis protection cryptography frequency;embedding process;extraction process	A new digital image watermarking technique in wavelet domain is proposed. In embedding process this method uses scrambling, extension and mean removal technique to original watermark, then watermark is embedded in middle frequency sub-bands of original image. The embedded strength of watermark is determined adaptively by noise visibility function. In extraction process this method adopts independent component analysis and doesn't need any information about original image and original watermark. The experimental results show that this method has a good robustness to wide variety of attacks.	digital image;digital watermarking;embedded system;independent component analysis;scrambler;wavelet	Jun Yu;Jieru Chi;Xiaodong Zhuang	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525436	independent component analysis;computer vision;feature extraction;digital watermarking;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;watermark;digital image	EDA	41.26818432647624	-10.69884157786399	177156
19141972966e3985a0eed92f1419dcfbd7b70a09	intra coding with adaptive partial reconstruction	video coding block based image coding h264 advanced video coding avc intra prediction;video coding image reconstruction redundancy;video coding;redundancy;image reconstruction;encoding image reconstruction discrete cosine transforms image coding interpolation correlation;block based image coding intra coding adaptive partial reconstruction inter pixel redundancy reduction block transform	Intra prediction improves coding performance by reducing inter pixel redundancy. However, to accommodate the use of block transforms, not all pixels can be predicted from reconstructed pixels that are located close to themselves. This causes prediction performance to suffer as pixel values further apart are less correlated. This paper presents additional intra coding modes designed with the goal of improving prediction performance. Experimental results show an average gain of about 2% in the key technical area software when the new modes are incorporated in the current 8×8 prediction modes. Since the new coding modes (8×8) are designed with transform size smaller than coding block size, the modes can also be useful when the source block is larger than the maximum transform size.	block size (cryptography);coding theory;data compaction;intra-frame coding;lossless compression;pixel	Yih Han Tan;Chuohao Yeo;Zhengguo Li;Susanto Rahardja	2013	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2012.2203735	iterative reconstruction;color cell compression;sub-band coding;computer vision;lapped transform;variable-length code;theoretical computer science;context-adaptive variable-length coding;coding tree unit;pattern recognition;mathematics;redundancy;context-adaptive binary arithmetic coding	Visualization	44.462777709602776	-16.73470793248023	177265
63b60c7d328e52ce2c79b62d0e9831bbaee3d0c1	reversible watermark with large capacity using the predictive coding	filigranage numerique;digital watermarking;distributed system;systeme reparti;prediction error;steganographie;image processing;vectorisation;localization;securite informatique;procesamiento imagen;localizacion;traitement image;vectorization;computer security;codage predictif;vectorisacion;steganography;esteganografia;sistema repartido;localisation;seguridad informatica;filigrana digital;codificacion predictiva;predictive coding	A reversible watermarking algorithm with large capacity has been developed by applying the difference expansion of a generalized integer transform. In this algorithm, a watermark signal is inserted in the LSB of the difference values among pixels. In this paper, we apply the prediction errors calculated by a predictor in JPEG-LS for embedding a watermark signal, which contributes to increase the amount of embedded information with less degradation. As one of the drawback discovered in the above conventional method is a large size of the embedded location map introduced to make it reversible, we decrease the large size of the location map by vectorization, and then modify the composition of the map using the local characteristic in order to enhance the performance of JBIG2.	algorithm;automatic vectorization;digital watermarking;distortion;elegant degradation;embedded system;jbig2;jpeg;kerrison predictor;least significant bit;least squares;pixel;simulation;vector graphics	Minoru Kuribayashi;Masakatu Morii;Hatsukazu Tanaka	2005		10.1007/11602897_37	internationalization and localization;telecommunications;image processing;digital watermarking;computer science;mean squared prediction error;vectorization;steganography;algorithm;statistics	ML	44.414171752807995	-11.930532092237975	177296
18a38b133db1a9371276a9ed58d6ce50c8506ac8	a fully scalable motion model for scalable video coding	estensibilidad;desciframiento;motion vector field layered coding;theorie vitesse distorsion;sensitivity and specificity;traitement signal;campo desplazamiento;evaluation performance;rate distortion;optimisation;algorithms computer simulation data compression image enhancement image interpretation computer assisted models statistical motion pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted video recording;low bit rate;scalable video;estimation mouvement;displacement field;champ vectoriel;performance evaluation;scalable video coding;image processing;data compression;optimizacion;decodage;decoding;video signal processing;motion estimation algorithm scalable motion model scalable video coding motion information scalability progressive motion vector precision coding motion vector field layered coding;motion information scalability;rate distortion optimization rdo motion estimation me;scalable video coding svc;evaluacion prestacion;estimacion movimiento;simulation;scalable motion model;procesamiento imagen;codec video;simulacion;motion estimation;motion;bit rate;motion vector field;traitement image;video coding scalability static var compensators decoding video codecs rate distortion bit rate motion estimation displays hdtv;velocidad de bit debil;algorithme;rate distortion theory;signal processing computer assisted;algorithm;campo vectorial;video coding;image enhancement;codage video;image interpretation computer assisted;motion vector;signal processing;displays;progressive motion vector precision coding;video recording;hdtv;reproducibility of results;scalable video coding svc hierarchical variable size block matching hvsbm rate distortion optimization rdo motion estimation me scalable motion;scalable motion;champ deplacement;traitement signal video;correspondencia bloque;models statistical;block matching;static var compensators;algorithms;video codecs;pattern recognition automated;optimization	Motion information scalability is an important requirement for a fully scalable video codec, especially for decoding scenarios of low bit rate or small image size. So far, several scalable coding techniques on motion information have been proposed, including progressive motion vector precision coding and motion vector field layered coding. However, it is still vague on the required functionalities of motion scalability and how it collaborates flawlessly with other scalabilities, such as spatial, temporal, and quality, in a scalable video codec. In this paper, we first define the functionalities required for motion scalability. Based on these requirements, a fully scalable motion model is proposed along with tailored encoding techniques to minimize the coding overhead of scalability. Moreover, the associated rate distortion optimized motion estimation algorithm will be provided to achieve better efficiency throughout various decoding scenarios. Simulation results will be presented to verify the superiorities of proposed scalable motion model over nonscalable ones.	algorithm;codec;data compression;distortion;image resolution;motion estimation;overhead (computing);rate–distortion theory;requirement;scalability;scalable video coding;simulation;vagueness;anatomical layer	Meng-Ping Kao;Truong Q. Nguyen	2007	IEEE Transactions on Image Processing	10.1109/TIP.2008.921307	data compression;scalable video coding;computer simulation;computer vision;vector field;scalability;rate–distortion theory;image processing;quarter-pixel motion;computer science;theoretical computer science;motion;displacement field;signal processing;motion estimation;rate–distortion optimization;motion compensation;algorithm;computer graphics (images)	Vision	46.54702500070877	-15.580069060258753	177508
0533d0f16e32276bb03e569957642fd6af333c06	embedded wavelet region-based coding methods applied to digital mammography	set partitioning in hierarchical trees;image coding;data compression;set partitions;mammography image coding biomedical imaging medical diagnostic imaging breast transform coding discrete wavelet transforms psnr image storage partitioning algorithms;trees mathematics;wavelet transforms;digital mammography;wavelet transform;image compression;medical image processing;peak signal to noise ratio;region of interest;medical image processing wavelet transforms image coding data compression trees mathematics block codes mammography;mammography;block codes;full image methods region based embedded wavelet compression set partitioning in hierarchical trees set partitioning embedded block coding breast region digital mammograms jpeg 2000 peak signal to noise ratio background pixels compression efficiency;bits per pixel	In this paper, we investigate region-based embedded wavelet compression methods and introduce region-based extensions of the Set Partitioning In Hierarchical Trees (SPIHT) and the Set Partitioning Embedded bloCK (SPECK) coding algorithms applied to the breast region of digital mammograms. We have compared these region-based extensions, called OB-SPIHT and OB-SPECK, against the original SPIHT and the new standard JPEG 2000 on five digital mammograms compressed at rates ranging from 0.1 to 1.0 bits per pixel (bpp). Distortion was evaluated for all images and compression rates by the Peak Signal-to-Noise Ratio (PSNR). A comparison applying SPIHT and JPEG 2000 to the same set of images with the background pixels fixed to zero was also carried out. For digital mammography, region-based compression methods represent an improvement in compression efficiency from full-image methods, also providing the possibility of encoding multiple regions of interest.	algorithm;color depth;distortion;embedded system;jpeg 2000;outside broadcasting;peak signal-to-noise ratio;pixel;region of interest;set partitioning in hierarchical trees;wavelet transform	Monica Penedo;William A. Pearlman;Pablo G. Tahoces;Miguel Souto;Juan J. Vidal	2003		10.1109/ICIP.2003.1247215	computer vision;computer science;theoretical computer science;mathematics;algorithm;statistics;wavelet transform	Graphics	43.627607157149065	-14.503828538385804	177628
ef08a0b66ea5c1f1f2c17a9984bdc50a23da0654	a progressive qim to cope with svd-based blind image watermarking in dwt domain	singular value decomposition progressive qim technique blind image watermarking dwt domain svd based methods quantization step sizes human visual characteristics particle swarm optimization method watermark extraction ground level identification level shift attacks;particle swarm optimization blind image watermarking discrete wavelet transform singular value decomposition;watermarking discrete wavelet transforms transform coding robustness quantization signal;singular value decomposition discrete wavelet transforms image watermarking particle swarm optimisation	A novel progressive QIM technique is developed to cooperate with the SVD-based watermarking scheme in the DWT domain. The progressive structure in quantization step sizes not only allows the exploitation of human visual characteristics but provides the clues of the ground level of brightness. A particle swarm optimization method is adopted to identify the ground level prior to performing watermark extraction. The experimental results show that the proposed scheme renders better robustness in comparison with other SVD-based methods. With the capability of the ground level identification, this scheme can also survive the level shift attacks.	digital watermarking;discrete wavelet transform;height above ground level;mathematical optimization;particle swarm optimization;rendering (computer graphics);singular value decomposition	Hwai-Tsu Hu;Yu-Jung Chang;Szu-Hong Chen	2014	2014 IEEE China Summit & International Conference on Signal and Information Processing (ChinaSIP)	10.1109/ChinaSIP.2014.6889277	computer vision;mathematical optimization;pattern recognition;mathematics	Vision	41.46495933042702	-10.41650464671965	178032
012bc6bec42f236eba4c9c3fa8672c1fa560f82e	coding local and global binary visual features extracted from video sequences	video coding visual features binary descriptors brisk bag of words;image coding;training;video sequences;visualization;feature extraction;mobile communication;video coding content based retrieval feature extraction image sequences;feature extraction encoding visualization image coding video sequences training mobile communication;encoding;content based retrieval local binary visual features extraction global binary visual features extraction video sequences intra frame coding scheme inter frame coding scheme analyze then compress paradigm visual analysis compress then analyze paradigm rate efficiency curves homography estimation	Binary local features represent an effective alternative to real-valued descriptors, leading to comparable results for many visual analysis tasks while being characterized by significantly lower computational complexity and memory requirements. When dealing with large collections, a more compact representation based on global features is often preferred, which can be obtained from local features by means of, e.g., the bag-of-visual word model. Several applications, including, for example, visual sensor networks and mobile augmented reality, require visual features to be transmitted over a bandwidth-limited network, thus calling for coding techniques that aim at reducing the required bit budget while attaining a target level of efficiency. In this paper, we investigate a coding scheme tailored to both local and global binary features, which aims at exploiting both spatial and temporal redundancy by means of intra- and inter-frame coding. In this respect, the proposed coding scheme can conveniently be adopted to support the analyze-then-compress (ATC) paradigm. That is, visual features are extracted from the acquired content, encoded at remote nodes, and finally transmitted to a central controller that performs the visual analysis. This is in contrast with the traditional approach, in which visual content is acquired at a node, compressed and then sent to a central unit for further processing, according to the compress-then-analyze (CTA) paradigm. In this paper, we experimentally compare the ATC and the CTA by means of rate-efficiency curves in the context of two different visual analysis tasks: 1) homography estimation and 2) content-based retrieval. Our results show that the novel ATC paradigm based on the proposed coding primitives can be competitive with the CTA, especially in bandwidth limited scenarios.	advanced tactical center;advanced transportation controller;anatomic node;architecture as topic;augmented reality;bag-of-words model in computer vision;cancer/testis antigen;coherence (physics);collections (publication);compresses (device);computational complexity theory;data compression;data rate units;entity name part qualifier - adopted;experiment;extraction;global serializability;homography (computer vision);programming paradigm;requirement;visual word;disease transmission	Luca Baroffio;Antonio Canclini;Matteo Cesana;Alessandro Enrico Cesare Redondi;Marco Tagliasacchi;Stefano Tubaro	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2445294	computer vision;visualization;mobile telephony;feature extraction;computer science;machine learning;coding tree unit;pattern recognition;multimedia;context-adaptive binary arithmetic coding;encoding	Vision	45.76981332559318	-23.16792600512153	178156
dba544f60559077ffe2cbc1b2b3405573ea5c4cd	optimized quantization of wavelet subbands for high quality real-time texture compression	wavelet transforms graphics processing units image texture real time systems rendering computer graphics;real time rendering applications wavelet subbands optimized quantization high quality real time texture compression wavelet based system 3d graphics applications lloyd max quantization quantization techniques texture compression formats gpu texture compression schemes;quantization signal image color analysis image coding codecs real time systems psnr transforms	This paper proposes a new wavelet-based system for fixed-rate texture compression in 3D graphics applications. An analysis of the optimized quantization of the wavelet subbands is carried out, focusing on Lloyd-Max quantization of subband blocks as well as on quantization techniques employed in existing texture compression formats on GPUs. The results demonstrate that the proposed compression technique yields higher performance compared to existing GPU texture compression schemes and previously proposed transformed-based systems. Moreover, compared to conventional schemes, the wide range of quantization schemes results in a much wider range of available bitrates. Additionally, the proposed scheme offers real-time execution, being suitable for real time rendering applications.	3d computer graphics;display resolution;graphics processing unit;quantization (signal processing);real-time clock;texture compression;wavelet	Bob Andries;Jan Lemeire;Adrian Munteanu	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7026136	s3 texture compression;computer vision;adaptive scalable texture compression;color quantization;quantization;computer science;theoretical computer science;texture compression;quantization;texture filtering;vector quantization;computer graphics (images)	EDA	42.5608543079479	-17.02862502620578	178245
a7df7664ade1eca051a80742795df469fb02bdef	an dwt-svd based digital image watermarking using a novel wavelet analysis function	discrete wavelet transforms;watermarking;discrete wavelet transform;image quality dwt svd digital image watermarking wavelet analysis function copyright multimedia objects;singular value decomposition;copyright;wavelet functions;multimedia systems;singular value decomposition copyright discrete wavelet transforms image watermarking multimedia systems;matrix decomposition;multimedia communication;wavelet functions discrete wavelet transform singular value decomposition watermarking;robustness;image watermarking;watermarking discrete wavelet transforms robustness multimedia communication matrix decomposition	Digital watermarking techniques have been developed to protect the copyright of multimedia objects such as text, audio, video, etc. In this paper, we propose SVD-based digital watermarking technique for robust watermarking of digital images for copyright protection. The security of the proposed scheme is increased by applying another wavelet function. We also demonstrate the good correlation between the embedded and the extracted watermark with the help of experimental results. One of the major advantages of the proposed scheme is the robustness of the technique on wide set of attacks. Analysis and experimental results show much improved performance of the proposed method in comparison with the pure SVD-based watermarking and the technique without using some wavelet function. Experimental results confirm that the proposed scheme provides good image quality of watermarked images.	digital image;digital watermarking;discrete wavelet transform;embedded system;image quality;singular value decomposition	Mohsen Kariman Khorasani;Mohammad Mojtaba Sheikholeslami	2012	2012 Fourth International Conference on Computational Intelligence, Communication Systems and Networks	10.1109/CICSyN.2012.54	wavelet;computer vision;second-generation wavelet transform;digital watermarking;computer science;theoretical computer science;cascade algorithm;mathematics;wavelet packet decomposition;multimedia;stationary wavelet transform;matrix decomposition;discrete wavelet transform;singular value decomposition;lifting scheme;robustness;wavelet transform	EDA	40.670595856506125	-10.937999573729552	178250
98448fe03590f09f7d816f507870df5a240ad36a	fast block-matching algorithm using selective integral projections	computational complexity;gradient descent;algorithms;block matching algorithm;spatial correlation;search algorithm;motion estimation	Existing fast block motion estimation algorithms, which reduce the computation by limiting the number of search points, utilize the motion vector (MV) characteristics of high spatial correlation as well as center-biased distribution in predicting an initial MV. Even though they provide good performance for slow motion sequences, they suffer from poor accuracy for fast or complex motion sequences. In this paper, a new fast and efficient block motion estimation algorithm is proposed. To find an initial search point, in addition to the predictors of zero MV and neighboring MVs, the algorithm utilizes another predictor obtained from one-dimensional feature matching using selective integral projections. This low complexity procedure enables the selection of a better initial search point so that a simple gradient descent search near this point may be enough to find the global minimum point. Compared to recent fast search algorithms, the proposed algorithm has lower computational complexity and provides better prediction performance, especially for fast or complex motion sequences.	block-matching algorithm	Jae Hun Lee;Jong Beom Ra	2002			mathematical optimization;quarter-pixel motion;theoretical computer science;machine learning;motion estimation;mathematics	Robotics	49.01173573042257	-19.33849033356435	178295
5789c67fe4842dac8ac18889d4df40c968f6e9e2	a chosen plaintext steganalysis of hide4pgp v 2.0	steganographie;building block;image databank;image bruitee;imagen sonora;steganography;esteganografia;steganalyse;noisy image;banco imagen;extraction message cache;banque image;outil steganographie hide4pgp v 2 0	A chosen plaintext steganalysis algorithm is described to isolate the corrupted bits in an image tampered with Hide4PGP V 2.0. The method is developed from the notion of representation of two dimensional image data in terms of a linear bit stream consisting of a set of basic building blocks. Its performance for message extraction is demonstrated on different 24 bit BMP images.	plaintext;steganalysis	Debasis Mazumdar;Soma Mitra;Sonali Dhali;Sankar K. Pal	2005		10.1007/11590316_71	computer science;theoretical computer science;steganography;computer security;algorithm;statistics	Crypto	42.85098384609378	-11.073811815316924	178300
04ec99c527cbedc9ecffb4496e57f4b463e15103	cell-based two-step scalar deadzone quantization for high bit-depth hyperspectral image coding	image coding;high bit depth hyperspectral image coding remote sensing images constrained computational resources spatial resolution spectral resolution bit depth resolution embedded quantization scheme two step scalar deadzone quantization transmitted image quality jpeg2000 cell based 2sdq wavelet coefficients codeblocks high bit depth hyperspectral images cell based two step scalar deadzone quantization;quantization signal transform coding image coding encoding hyperspectral imaging image reconstruction;quantization signal;transform coding;image reconstruction;wavelet transforms geophysical image processing hyperspectral imaging image coding image resolution remote sensing;hyperspectral imaging;encoding;two step scalar deadzone quantization 2sdq embedded quantization high bit depth images jpeg2000	Remote sensing images often need to be coded and/or transmitted with constrained computational resources. Among other features, such images commonly have high spatial, spectral, and bit-depth resolution, which may render difficult their handling. This letter introduces an embedded quantization scheme based on two-step scalar deadzone quantization (2SDQ) that enhances the quality of transmitted images when coded with a constrained number of bits. The proposed scheme is devised for use in JPEG2000. It is named cell-based 2SDQ since it uses cells, i.e., small sets of wavelet coefficients within the codeblocks defined by JPEG2000. Cells permit a finer discrimination of coefficients in which to apply the proposed quantizer. Experimental results indicate that the proposed scheme is especially beneficial for high bit-depth hyperspectral images.	cell (microprocessor);closure (computer programming);code::blocks;coefficient;color quantization;computational resource;embedded system;jpeg 2000;mathematical optimization;quantization (signal processing);scalar processor;wavelet	Joan Bartrina-Rapesta;Francesc Auli-Llinas	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2015.2436438	iterative reconstruction;computer vision;transform coding;quantization;theoretical computer science;hyperspectral imaging;mathematics;encoding;remote sensing	EDA	43.803787534132915	-15.318653576059758	178345
250e9be90b7e6edb905829e198b541285e62cb52	reactive synthesis for finite tasks under resource constraints		There are many applications where robots have to operate in environments that other agents can change. In such cases, it is desirable for the robot to achieve a given high-level task despite interference. Ideally, the robot must decide its next action as it observes the changes in the world, i.e. act reactively. In this paper, we consider a reactive planning problem for finite robotic tasks with resource constraints. The task is represented using a temporal logic for finite behaviors and the robot must achieve the task using limited resources under all possible finite sequences of moves of other agents. We present a formulation for this problem and an approach based on quantitative games. The efficacy of the approach is demonstrated through a manipulation case study.	algorithm;cups;high- and low-level;interference (communication);motion planning;reactive planning;reduction (complexity);robot;run time (program lifecycle phase);stacking;temporal logic	Keliang He;Morteza Lahijanian;Lydia E. Kavraki;Moshe Y. Vardi	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206426	computer science;real-time computing;control engineering;robot;temporal logic;reactive planning	Robotics	52.84096782001413	-21.443242513499843	178588
4571de5ac1f80652cca460c6bec4fef89f94c867	dynamic stochastic resonance-based grayscale logo extraction in hybrid svd-dct domain		This paper presents a novel dynamic stochastic resonance (DSR)-based technique for robust extraction of a grayscale logo from a tampered watermarked image. The watermark embedding is done on the singular values (SV) of the discrete cosine transform (DCT) coefficients of the cover image. DSR is then strategically applied during the logo extraction process where the SV of DCT coefficients are tuned following a double-well potential model by utilizing the noise introduced during attacks. The resilience of this technique has been tested in the presence of various noises, geometrical distortions, enhancement, compression, filtering and watermarking attacks. The proposed DSR-based technique for logo extraction gives noteworthy robustness without any significant trade-off in perceptual transparency of the watermarked image. A maximization approach has been adopted for the selection of bistable double-well parameters to establish noise-enhanced resonance. When compared with existing watermark extraction techniques based in SVD, DCT, SVD-DCT domains, as well as with their combination with DSR, the results suggest that remarkable improvement of robustness is achieved by using DSR on singular values of DCT.	discrete cosine transform;grayscale;logo;singular value decomposition;stochastic resonance	Rajib Kumar Jha;Rajlaxmi Chouhan	2014	J. Franklin Institute	10.1016/j.jfranklin.2014.01.017	computer vision;speech recognition;telecommunications;engineering	Vision	41.64060409561329	-10.32825974255324	178596
b61cbfa83a228dda30bc32cd7437c6f891aba61a	scalable prediction structure for multiview video coding	complexity theory;video coding iec standards iso standards intserv networks encoding decoding information analysis cameras layout tv;decoding;iso;iso standards;disparity estimation;interview prediction;intserv networks;layout;joints;encoding complexity;iec;random access ability multiview video coding temporal prediction interview prediction temporal hierarchy scalable prediction structure disparity estimation encoding complexity decoded picture buffer size;intra prediction;multiview video coding;video coding;decoded picture buffer size;scalable prediction structure;iec standards;prediction theory;video coding decoding prediction theory;random access ability;tv;correlation;temporal prediction;encoding;information analysis;random access;temporal hierarchy;cameras	Both temporal prediction and inter-view prediction are employed to improve the coding efficiency in multiview video coding. Hierarchical B pictures are usually used as the basic structure for temporal prediction. The inter-view prediction in each temporal hierarchy level brings different improvement to the entire coding efficiency. We propose a scalable prediction structure in which inter-view prediction would be disabled if the picture redundancy can be almost exploited by temporal prediction and intra prediction. In this way, time-consuming computation of disparity estimation can be saved. Experimental results show that lower encoding complexity, smaller decoded picture buffer size and better random access ability can be achieved with a slightly gain loss.	algorithmic efficiency;binocular disparity;computation;data compression;high efficiency video coding;intra-frame coding;model–view–controller;multiview video coding;random access;scalability	Junyan Huo;Yilin Chang;Ming Li;Yanzhuo Ma	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5118332	layout;computer vision;iso image;computer science;theoretical computer science;machine learning;data analysis;prediction by partial matching;correlation;random access;encoding;multiview video coding	Vision	46.45038230400656	-19.46227341013907	178697
3a65574b198a74258e04e898a47a10a6880fda8f	a jpeg-like texture compression with adaptive quantization for 3d graphics application	article;3d graphics;texture compression	DCT-based compression is widely used in video and image compression for a high compression ratio, but it suffers from random-access problem when applied to texture compression. In this paper we present a JPEG-Like DCT-based texture compression technique which is suitable for 3D graphics rendering system. We apply a simple adaptive quantization on an 8×8 blocksize texture, such that the length of the encoded bit stream of one block can be approximated to the target. A pre-defined quantizer scale is encoded with the bit stream with a small overhead. Our technique achieved a high compression ratio, quality control of true color texture, and random access of texture data.	3d computer graphics;3d rendering;4-bit;approximation algorithm;bitstream;block truncation coding;color depth;discrete cosine transform;encode;fxt1;graphics software;image compression;internet;jpeg;national supercomputer centre in sweden;overhead (computing);palette (computing);powervr;quantization (signal processing);random access;real-time transcription;rendering (computer graphics);requirement;s3 texture compression;text retrieval conference;vector quantization	C.-H. Chen;C.-Y. Lee	2002	The Visual Computer	10.1007/s003710100130	s3 texture compression;computer vision;adaptive scalable texture compression;color quantization;computer science;theoretical computer science;texture memory;texture compression;texture filtering;anisotropic filtering;3d computer graphics;computer graphics (images)	Graphics	41.95411459852957	-16.580346453709264	178942
1ccc4cc966d7c5d95a6165af575916833b37c473	digital space transmission of an interference fringe-type computer-generated hologram using irsimple	image coding;psnr;light emitting diodes;transform coding;jpeg2000 digital space transmission interference fringe type computer generated hologram irda technical standard image data sending infrared rays irsimple infrared digital transmission transmitted file size minimization suitable compression method compressed file;interference fringe type computer generated hologram irsimple jpeg2000 space transmittance;image reconstruction;bandwidth;image coding light emitting diodes image reconstruction transform coding psnr encoding bandwidth;encoding;interference signal data compression holography image coding image representation infrared imaging	In this paper, we present a method to perform a digitalspace transmission of an interference fringe-type computergenerated hologram using IrSimple. IrSimple was defined by IrDA technical standard, and it was developed for the purpose of sending image data at high speed using an infrared-rays. We performed infrared digital transmission using IrSimple, and we minimized the size of the transmitted file by using a suitable compression method. The size of the compressed file was very small compared with that of the bitmap file. The transmission preserved the quality of the representation while requiring a short transmission time.	bitmap;computer-generated holography;interference (communication);technical standard	Masataka Tozuka;Kunihiko Takano;Koki Sato;Makoto Ohki	2013	2013 Proceedings of ITU Kaleidoscope: Building Sustainable Communities		computer vision;electronic engineering;computer science;optics	Visualization	41.845139242609015	-19.265442017442695	178999
5bc1a7be3c52f304db749dad8ef021cd4ff2def3	an effective lossless hiding technique based on reversible contrast mapping and histogram embedding methods	histograms;watermarking;data hiding;look up table;image coding;psnr;computed tomography;histogram embedding method;stego image;reversible contrast mapping technique;lossless data hiding technique;image restoration;data mining;image recovery algorithm;reversible contrast mapping;absolute difference lossless data hiding histogram reversible contrast mapping;media;absolute difference;steganography;stego image lossless data hiding technique histogram embedding method reversible contrast mapping technique image recovery algorithm look up table watermarking embedding scheme image restoration feature extraction;histogram;image generation;feature extraction;image quality;pixel;histograms data mining image quality watermarking web and internet services computer hacking information management data encapsulation image generation image restoration;watermarking feature extraction image coding image restoration steganography table lookup;table lookup;lossless data hiding;watermarking embedding scheme	Lossless data hiding is a technique that used to embed secret message into media for generating the stego image. The receiver who obtains the stego image can extract the secret message from it and restore the original media via the extraction and recovery image algorithms. Coltuc and Chassery combined look-up table and reversible contrast mapping technique to generate a very fast watermarking embedding scheme in 2007. Their scheme is simple and the hiding capacity is high. In addition, the image quality of the stego image generated by their scheme is fine. However, in their method, absolute difference is used to control image distortion. If the value of the absolute difference is low, then the hiding capacity will be limited and decreased. Therefore, we apply the histogram technique to improve Coltuc and Chassery’s scheme. The experimental results show that the image quality of the proposed scheme is better than that of Coltuc and Chassery’s scheme.	algorithm;distortion;image quality;lookup table;lossless compression;reliability-centered maintenance;steganography	Tzu-Chuen Lu;Ying-Hsuan Huang	2009	2009 Fifth International Conference on Information Assurance and Security	10.1109/IAS.2009.113	computer vision;theoretical computer science;pattern recognition;mathematics	EDA	39.98189247193905	-12.111951320698836	179059
90508df54f6bacab305f5aa86671a2ccdc66b676	motion compensation and scalability in lifting-based video coding	estensibilidad;prediccion;evaluation performance;temporal lifting;optimisation;scalable video;performance evaluation;scalable video coding;motion compensation;optimizacion;video signal processing;adaptive filtering;filtrado adaptable;evaluacion prestacion;subband decomposition;simulation;compresion senal;simulacion;prediction optimization;compression signal;video codec;motion compensated;motion compensated temporal filtering;compensation mouvement;video coding;descomposicion subbanda;temporal scalability;signal compression;on the fly;traitement signal video;optimization;filtrage adaptatif;extensibilite;scalability;decomposition sous bande;prediction;5 3 motion compensated temporal filtering	Motion-compensated temporal filtering subband video codecs have attracted recently a lot of attention, due to their compression performance comparable with that of state-of-the-art hybrid codecs and due to their additional scalability features. In this paper we present a scalable video codec based on a 5/3 adaptive temporal lifting decomposition. Different adaptation criteria for coping with the occluded areas are discussed and new criteria for optimizing the temporal prediction are introduced. For our simulations, we use a memory-constraint “on-the-fly” implementation. We also evaluate the temporal scalability properties of this video coding structure.	algorithmic efficiency;codec;data compression;distortion;lifting scheme;mathematical optimization;motion compensation;multiresolution analysis;qr decomposition;scalability;scalable video coding;simulation;video coding format;wavelet	Grégoire Pau;Christophe Tillier;Béatrice Pesquet-Popescu;Henk J. A. M. Heijmans	2004	Sig. Proc.: Image Comm.	10.1016/j.image.2004.05.003	scalable video coding;adaptive filter;computer vision;scalability;simulation;prediction;computer science;motion compensation;statistics	Vision	46.40646049750757	-15.213201864368521	179289
0f3547ed3a1973befd9fe521b41c26aa5ac891d9	spatial coefficient partitioning for lossless wavelet image coding	image coding;codecs;image resolution;image coding image resolution codecs;probability distribution;data structure;spatial orientation	A novel coefficient partitioning algorithm is introduced for splitting the coefficients into two sets using spatial orientation tree data structure. By splitting the coefficients, the overall theoretical entropy is reduced due to the different probability distribution for the two coefficient sets. In spatial domain, it is equivalent to identifying smooth regions of the image. A lossless coder based on this spatial coefficient partitioning is described. Experimental results show that the new algorithm has a better coding performance than other wavelet based lossless image coder such as S+P and JPEG-2000.	algorithm;binary space partitioning;data structure;jpeg 2000;lossless compression;matthews correlation coefficient;tree (data structure);wavelet	William Kwok-Wai Cheung;Lai-Man Po	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745411	probability distribution;computer vision;codec;image resolution;spatial disorientation;data structure;computer science;theoretical computer science;pattern recognition;mathematics;statistics	Robotics	43.88094301662746	-14.855553945753257	179415
844f648dc29e1920a258b6154442a2431622ae38	cutting across visual mpeg standards for video scalability	scalable video coding;mpeg a and video transcoding	This paper proposes to replace the Intraframes of MPEG- 2 with JPEG2k coded bitstreams governed by the syntax of MPEG-2 spatial scalability. The paper elaborates upon the proposed coding architectures and compatibility with published MPEG standards. The proposed coding architecture is compared against counterpart video transcoders, scalable and single layer codecs. It is shown that the proposed spatial encoder is either comparable or superior to all the aforementioned codecs whilst enjoying an extensible architecture capable of serving restricted end systems and hosting additional scalable layers such as SNR and temporal scalabilities.	moving picture experts group;scalability	Tamer Shanableh	2006			embedded system;real-time computing;computer science;multimedia;multiview video coding	Vision	44.395966535146734	-20.523906408760745	179472
3fe05dd972e4ca676fc2ecabc5cdefd49a0900b9	fast algorithms for inter-view prediction of multiview video coding	multiview video coding mvc;inter view estimation;hierarchical b picture prediction structure hbps;global disparity vector gdv	The compression efficiency and complexity should be simultaneously taken into consideration in Multiview Video Coding (MVC). In order to reduce the complexity while maintaining the efficiency of MVC, we proposed in this paper two algorithms called Adaptive Search Region Adjustment (ASRA) and Adaptive Selection of Inter-view References (ASIR). In ASRA, we simplified the exiting method to obtain the global disparity vector (GDV ), using which the correlations between the disparity vector (DV ) and predicted vector were exploited. The statistical property of the absolute difference value of DV and GDV was analyzed and the results were applied to adjust the search range dynamically. In ASIR, we first used the prediction information of the lower temporal layer in the hierarchical B picture prediction structure to reduce the number of inter-view reference pictures, and then applied the results of reference selection after inter 16×16 prediction mode decision to revise our method. The experimental results demonstrate that the algorithms suggested in this paper can reduce the computation up to 25% with little decrease of compression efficiency.	asp.net mvc;algorithm;binocular disparity;computation;data compression;fast fourier transform;model–view–controller;multiview video coding;time complexity	Rong Pan;Zheng-Xin Hou;Yu Liu	2011	Journal of Multimedia	10.4304/jmm.6.2.191-201	computer vision;computer science;theoretical computer science;machine learning	AI	47.35529864385793	-19.390920302697893	179500
b3eb1cbb7c62cd619216d586f926323a82d21a38	probabilistic planning for continuous dynamic systems under bounded risk	article	This paper presents a model-based planner called the Probabilistic Sulu Planner or the p-Sulu Planner, which controls stochastic systems in a goal directed manner within user-specified risk bounds. The objective of the p-Sulu Planner is to allow users to command continuous, stochastic systems, such as unmanned aerial and space vehicles, in a manner that is both intuitive and safe. To this end, we first develop a new plan representation called a chance-constrained qualitative state plan (CCQSP), through which users can specify the desired evolution of the plant state as well as the acceptable level of risk. An example of a CCQSP statement is “go to A through B within 30 minutes, with less than 0.001% probability of failure.” We then develop the p-Sulu Planner, which can tractably solve a CCQSP planning problem. In order to enable CCQSP planning, we develop the following two capabilities in this paper: 1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a continuous domain with temporal constraints. The first capability is to ensures that the probability of failure is bounded. The second capability is essential for the planner to solve problems with a continuous state space such as vehicle path planning. We demonstrate the capabilities of the p-Sulu Planner by simulations on two real-world scenarios: the path planning and scheduling of a personal aerial vehicle as well as the space rendezvous of an autonomous cargo spacecraft.	aerial photography;automated planning and scheduling;autonomous robot;dynamical system;goto;motion planning;scheduling (computing);simulation;state space;stochastic process;unmanned aerial vehicle	Masahiro Ono;Brian C. Williams;Lars Blackmore	2013	J. Artif. Intell. Res.	10.1613/jair.3893	simulation;computer science;artificial intelligence	AI	52.65178265326705	-21.444607099063656	179589
201b55add2cd5cba86609b53b1cf2eb92ab57be6	progressive image transmission	simulation ordinateur;intermediate images;teleconferencing;image numerique;image coding;progressive image transmission;data compression;application software;image communication;quantifier;progressif;visual communication;modified kohonen self organizing feature map algorithm;intermediate images pit reconstructed images spatial domain progressive image transmission modified kohonen self organizing feature map algorithm kohonen vector quantizer edge distortion classification technique;bit rate;kohonen vector quantizer;reconstruction image;visual communication image coding image reconstruction self organising feature maps vector quantisation;progressive;cuantificacion vectorial;image generation;postal services;vector quantization;image transmission;reconstruccion imagen;self organising feature maps;reconstructed images;self organized feature map;image reconstruction;progresivo;quantificateur;imagen numerica;classification technique;hdtv;spatial domain progressive image transmission;vector quantizer;simulacion computadora;compresion dato;digital image;vector quantisation;transmission image;cuantificador;computer simulation;image communication image reconstruction bit rate data compression application software teleconferencing postal services hdtv medical diagnosis image generation;medical diagnosis;compression donnee;pit;edge distortion;transmision imagen;quantification vectorielle	Progressive image transmission (PIT) is widely used in many applications, since it generates the successively improved reconstructions of an image. In spatial domain PIT systems, perfectly reconstructed images can be obtained at the final stage. However the intermediate images are not good and there is no data compression. In this paper, the authors present a new simple spatial domain progressive image transmission using modified Kohonen self-organizing feature map algorithm, which is called Kohonen vector quantizer (KVQ). To alleviate edge distortion a classification technique is applied to KVQ. Computer simulation results show that very good intermediate images can be obtained at reasonable bit rates using the PIT scheme introduced. >		Wei Gong;K. R. Rao;Michael T. Manry	1993	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.246089	data compression;iterative reconstruction;computer simulation;computer vision;application software;teleconference;computer science;machine learning;medical diagnosis;digital image;vector quantization;statistics;visual communication;computer graphics (images)	EDA	45.27921430428327	-13.457679864304563	179635
4feb5d9e427ad2d6249acdd62431312ba3282140	hash-based motion modeling in wyner-ziv video coding	motion analysis;rate distortion theory video coding motion analysis;rate distortion characteristics;rate distortion;wyner ziv distributed video coding;encoder side motion estimation;hash based motion estimation;motion compensation;motion estimation;video sequences;indexing terms;motion compensated;rate distortion theory;video coding;wyner ziv video coding;video coding image sequences motion compensation motion estimation rate distortion theory;motion vector;decoder side motion compensation;video coding motion estimation decoding rate distortion costs encoding motion compensation predictive models predictive coding video sequences;distributed video coding;video sequences hash based motion estimation wyner ziv distributed video coding auxiliary hash encoder side motion estimation rate distortion characteristics decoder side motion compensation predictive coding;motion compensated prediction;side information;predictive coding;auxiliary hash;image sequences	Generally, distributed video coding (DVC) schemes perform motion estimation at the decoder side, without the current frame being available. In order to generate the side-information reliably, one solution consists in allocating a limited bit budget to send a hash of the current frame. At the decoder, this auxiliary hash is used to perform motion estimation. This paper studies the accuracy of hash-based motion estimation and compares it to conventional encoder-side motion estimation. We show that, at low rates, the very limited bit-budget of the hash does not ensure a reliable motion estimation, while at medium to high rates the motion accuracy is comparable with the finite precision used to represent motion vectors. Then, we derive the rate-distortion characteristic, which combines the cost of encoding the hash and the prediction residuals after decoder-side motion compensation. We show that, at high rates, hash-based motion modeling can virtually achieve the same coding efficiency as motion-compensated predictive coding. Instead, at medium-to-low rates we observe a significant coding loss. Experimental results on real video sequences validate the results of the proposed model.	algorithmic efficiency;data compression;distortion;encoder;motion compensation;motion estimation	Marco Tagliasacchi;Stefano Tubaro	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366728	computer vision;real-time computing;hash function;index term;rate–distortion theory;quarter-pixel motion;computer science;theoretical computer science;motion estimation;block-matching algorithm;motion compensation	Vision	48.26755965019118	-17.006016946390424	180022
1b97d4f297f20782f8de4174d0f4eba55af070d8	implications for high capacity data hiding in presence of lossy compression	watermarking;quantization;data hiding;channel coding;multimedia signal;data encapsulation statistics signal processing signal design educational institutions read only memory water storage bit rate quantization information retrieval;data compression;security of data data compression data encapsulation channel capacity channel coding quantisation signal;capacity bounds;lossy compression;information retrieval;nongaussian statistics;signal design;binary antipodal channel codes;bit rate;water storage;data encapsulation;quantisation signal;just noticeable difference high capacity data hiding nongaussian statistics capacity bounds watermarking jnd perceptual coding lossy compression distortions binary antipodal channel codes storage efficiency bit rate quantization multimedia signal;channel capacity;signal processing;jnd perceptual coding;statistics;high capacity data hiding;just noticeable difference;lossy compression distortions;read only memory;security of data;storage efficiency bit rate	We derive capacity bounds for watermarking and data hiding in the presence of JND perceptual coding for a class of techniques that do not suffer from host signal interference. By modeling the lossy compression distortions on the hidden data using non-Gaussian statistics, we demonstrate that binary antipodal channel codes achieve capacity. It is shown that the data hiding capacity is at most equal to the loss in storage efficiency bit rate if watermarking and quantization for lossy compression occur in the same domain.	code;digital watermarking;distortion;interference (communication);lossy compression;psychoacoustics;quantization (signal processing);storage efficiency	Deepa Kundur	2000		10.1109/ITCC.2000.844177	data compression;lossy compression;data compression ratio;electronic engineering;telecommunications;transparency;computer science;theoretical computer science	Networks	47.84469563090827	-11.211424403437215	180448
815b47ddd2e8b49d002f28f7dc2ca89945d8099c	an improvement of diamond encoding using characteristic value positioning and modulus function	diamond encoding;diamond characteristics;exploiting modification direction	The diamond encoding technique controls payload and image quality by a k value. Although a small increase in k can increase payload, it also increases image distortion. In this paper, we proposed an improvement scheme to the diamond encoding technique for reducing image distortion during a k change. Another aim is to increase payload size. A square matrix conversion of the diamond matrix ccepted 31 December 2012 vailable online 18 January 2013 eywords: iamond encoding iamond characteristics xploiting modification direction is used to lower the MSE values in high payload when k = 3. A lower MSE reduces image distortion. In low payload, that is when k changes from 1 to 2, a one dimension matrix modulus is used to reduce image distortion. Experimental results showed that payload may be increased while image quality is not significantly reduced. © 2013 Elsevier Inc. All rights reserved. . Introduction Information hiding techniques have been proposed for hiding n the frequency domain and spatial domain (Hong and Chen, 011a, 2011b; Lin et al., 2008). Changes in the high frequency omain are not easy to detect by raw eye vision. Much information n frequency-domain hiding can be found in Perez-Freire et al.’s 2006) and Rykaczewski’s (2007) methods. Hiding techniques in he spatial domain embed messages by replacing the pixel bits. ost replacements take place in the insignificant bits so as to ecrease image distortion (Wu and Tsai, 2003; Feng et al., 2007; u and Hwang, 2007). The popular least significant bit (LSB) (Chan and Cheng, 2004) ata hiding is a simple hiding technique in spatial domain. Howver, it can result in high image distortion. Mielikainen proposed he LSB matching revisited (LSBMR) technique to improve image uality in 2006 (Mielikainen, 2006). In this proposed method, or every two pixels only one pixel value was added or subracted 1 to embed a 4-ary digit. Results showed improved image uality without reducing the payload. Zhang and Wang (2006) proosed the exploiting modification direction (EMD) technique to mprove Mielikainen’s method where n pixels was used to embed a ∗ Corresponding author. Tel.: +886 4 2219 6617; fax: +886 4 2219 6341. E-mail addresses: jeanne@nutc.edu.tw, jeanne.m.chen@gmail.com (J. Chen), hihwei.shiu@gmail.com (C.-W. Shiu), mcwu0715@gmail.com (M.-C. Wu). 164-1212/$ – see front matter © 2013 Elsevier Inc. All rights reserved. ttp://dx.doi.org/10.1016/j.jss.2012.12.053 (2n + 1)-ary secret data. The simplest case of EMD only considered hiding one 5-ary secret in two pixels, n = 2. Only one pixel was added or subtracted by 1 in n pixels. Although EMD showed significantly high image quality, payload was small since the number of pixels for embedding was restricted. Chao et al. (2009) proposed the diamond encoding (DE) technique to improve EMD. The proposed DE made use of the diamond characteristic value (DCV) function to calculate pixel-value change during information hiding. A variable k was used to adjust the payload size. Every two pixels was used to embed (2k2 + 2k + 1)-ary secret data. The k value can be increased to increase payload. When k > 1, the PSNR is significantly lowered. For example, suppose a 512 × 512 sized image with the payload of k = 1. After embedding, the PSNR is 52.09 dB. If the same capacity is applied with k = 2, the PSNR value is reduced to 49.82 dB. When k > 1, larger displacement of pixels for higher payload will result in poorer image quality. In this paper, we proposed a two part improvement scheme for the DE technique. The first part, called Square Matrix Encoding (SME), is used to improve image quality when k = 3 by reducing the distance between the outer points of the diamond matrix and the center point. The outer points are relocated to convert to a square matrix which can lower the MSE values to improve image quality and to increase payload. The second part, called Modulus Function Encoding (MFE), applies the modulus function to increase payload and improve image quality. In DE, a larger k value resulted in larger image distortion. For example, when k = 1 and the payload is small. To increase payload, k must be changed to 2 or greater. However, 1 ms and Software 86 (2013) 1377– 1383	chao (sonic);decibel;displacement mapping;distortion;entity–relationship model;fax;image quality;least significant bit;local interconnect network;modulus of continuity;most significant bit;payload (computing);peak signal-to-noise ratio;pixel	Jeanne Chen;Chih-Wei Shiu;Mei-Chen Wu	2013	Journal of Systems and Software	10.1016/j.jss.2012.12.053	electronic engineering;forensic engineering;engineering drawing	AI	41.05894072957615	-12.774964497393235	180525
f8edeacea2b40605a39778c6c76204f90ec38a36	robust watermarking of digital image based on feature and dwt	discrete wavelet transforms;laplace equations discrete wavelet transforms embedded systems image watermarking;robustness watermarking modified harris laplace image correction;image geometric attacks robust watermarking digital image dwt geometric distortion correcting algorithm wavelet based watermarked image low frequency subbands embedding capacity modified harris laplace detector steady feature points image correcting;embedded systems;laplace equations;robustness watermarking detectors digital images discrete wavelet transforms abstracts wavelet analysis;image watermarking	This paper proposes a new geometric distortion correcting algorithm for wavelet-based watermarked image. The watermark is embedded into the low frequency sub-bands of the host image, which guarantees large embedding capacity and good invisibility. The modified Harris-Laplace detector is utilized to extract steady feature points used for image correcting. Simulation results show that the proposed algorithm is not only invisible but also robust against many image geometric attacks such as rotation, translation and scaling, etc.	algorithm;digital image;digital watermarking;discrete wavelet transform;distortion;embedded system;harris affine region detector;image scaling;simulation	Jun Zhang;Mei-Na Sun;Mei-Jing Ge;Zheng-Ling Yang;Jun-Tao Xue;Liang Wang	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6359562	computer vision;mathematical optimization;feature detection;computer science;theoretical computer science;mathematics	EDA	41.53674774347123	-10.624435237564578	180556
f601591873a49292d4912b1af5e4dca40cc1e21f	linearly constrained generalized lloyd algorithm for reduced codebook vector quantization	sphere packing;optimisation;image coding;algoritmo busqueda;image processing;image resolution;optimizacion;algorithme recherche;efficient algorithm;codage source;search algorithm;compresion senal;procesamiento imagen;low complexity;indexing terms;traitement image;three dimensional;compression signal;lossy lcvq compressed images linearly constrained generalized lloyd algorithm reduced codebook vector quantization lcgla block based image compression low complexity decompression 3d graphics cards texture compression efficient algorithm lcvq codebook design vq codebooks codewords linearly constrained nearest neighbor techniques pattern recognition signal compression linearly constrained codebooks sphere packing low complexity implementation fine resolution guaranteed convergence fast nn search algorithms initialization procedure encoding complexity reduction psnr;image texture;generalized lloyd algorithm;vecino mas cercano;cuantificacion vectorial;vector quantization;image compression;computational complexity;clustering method;signal compression;nearest neighbor;image coding algorithm design and analysis neural networks convergence vector quantization graphics nearest neighbor searches pattern recognition signal resolution psnr;pattern recognition;plus proche voisin;nearest neighbour;optimization;source code;search problems;vector quantizer;search problems vector quantisation image coding pattern recognition image resolution computational complexity image texture;reconnaissance forme;reconocimiento patron;vector quantisation;source coding;quantification vectorielle	"""As linearly constrained vector quantization (LCVQ) is efficient for block-based compression of images that require low complexity decompression, it is a """"de facto"""" standard for three-dimensional (3-D) graphics cards that use texture compression. Motivated by the lack of an efficient algorithm for designing LCVQ codebooks, the generalized Lloyd (1982) algorithm (GLA) for vector quantizer (VQ) codebook improvement and codebook design is extended to a new linearly constrained generalized Lloyd algorithm (LCGLA). This LCGLA improves VQ codebooks that are formed as linear combinations of a reduced set of base codewords. As such, it may find application wherever linearly constrained nearest neighbor (NN) techniques are used, that is, in a wide variety of signal compression and pattern recognition applications that require or assume distributions that are locally linearly constrained. In addition, several examples of linearly constrained codebooks that possess desirable properties such as good sphere packing, low-complexity implementation, fine resolution, and guaranteed convergence are presented. Fast NN search algorithms are discussed. A suggested initialization procedure halves iterations to convergence when, to reduce encoding complexity, the encoder considers the improvement of only a single codebook for each block. Experimental results for image compression show that LCGLA iterations significantly improve the PSNR of standard high-quality lossy 6:1 LCVQ compressed images."""	codebook;linde–buzo–gray algorithm;vector quantization	Lowell LeRoy Winger	2001	IEEE Trans. Signal Processing	10.1109/78.928703	image processing;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;linde–buzo–gray algorithm;source code	EDA	44.573801527792895	-13.223257922827592	180888
606cb60078d9033882ac867bb483211d7509038d	wavelet compression of ecg signals by jpeg2000	image coding;codecs;data compression;transform coding electrocardiography medical image processing data compression image coding codecs image sequences matrix algebra wavelet transforms;matrix encoding wavelet compression ecg signal jpeg2000 still image compression jpeg2000 codec precise rate control progressive quality ecg data sequence;transform coding;matrix algebra;wavelet transforms;rate control;electrocardiography;electrocardiography transform coding image coding codecs chromium biomedical engineering data compression radiology signal design image databases;medical image processing;internal standard;image sequences	This paper describes the wavelet compression of ECG signals by JPEG2000. JPEG2000 is the latest international standard for compression of still images. JPEG2000 codec is designed to compress images and it can also be used to compress other signals. The desirable characteristics of the JPEG2000 codec, such as precise rate control and progressive quality, are retained in the presented ECG compression scheme. To compress the ECG data using a JPEG2000 code, the one-dimensional ECG sequence is processed to produce a two-dimensional matrix. This matrix is then encoded using JPEG2000.	codec;jpeg 2000;wavelet transform	Ali Bilgin;Michael W. Marcellin;Maria I. Altbach	2004	Data Compression Conference, 2004. Proceedings. DCC 2004	10.1109/DCC.2004.1281503	data compression;wavelet;computer vision;codec;transform coding;speech recognition;image compression;computer science;internal standard;mathematics;lossless compression;context-adaptive binary arithmetic coding;texture compression;statistics;wavelet transform	Robotics	43.59108446936428	-16.858486334426832	181012
c56f867cb929900de173640c05e8aebc5506c4ef	improvement of multi-bit information embedding algorithm for palette-based images	data embedding;quality improvement;steganography;palette based images;capacity enhancement	We propose a new approach that is an improvement on the conventional of a multi-bit information embedding algorithm for palette-based images. The proposed method embeds secret information by changing the pixel values for each of the $$2^{k-2}+1$$2k-2+1 pixels. Hence, our scheme can embed more information compared to the conventional method. Furthermore, the developed algorithm does not drastically change the pixel values with large color differences between before and after the changes. Therefore, it reduces the degradation of the image quality. Our experimental results show that the proposed scheme is superior to the conventional method.	algorithm;palette (computing)	Anu Aryal;Kazuma Motegi;Shoko Imaizumi;Naokazu Aoki	2015		10.1007/978-3-319-23318-5_28	computer vision;theoretical computer science;mathematics;multimedia	Vision	40.93286882055161	-12.715798231301699	181101
ca99db5141388bf7acd7b8fcc69076cf23cbf6ff	a multi-bit digital watermarking algorithm with side information	optimisation;image coding;digital watermark;correlation methods;watermarking algorithm design and analysis correlation bit error rate robustness signal processing algorithms educational institutions;optimisation correlation methods image coding image watermarking;watermark system fidelity control multibit digital watermarking algorithm side information coding signal watermarking linear correlation value;robustness digital watermarking side information effectiveness perceptual model;image watermarking;side information	In this paper, a multi-bit digital watermarking algorithm with side information has been proposed, which use the max linear correlation value between the corresponding blocks of cover work and watermarking coding signals as the side information to optimize the embedding region and detection critical value; use the linear correlation value between the cover work and watermark coding signals as side information to adjust the intensity of watermarking embedding in order to improve the effectiveness of watermark system; use the perceptual character of cover work as side information participate in the watermarking coding and embedding process to control the fidelity of watermark system. The result of test indicates that the algorithms which are composed here can not only effectively improve the effectiveness of watermark system but also own the better robustness.	algorithm;digital watermarking	Na Xu;Changsheng Liu;Ying Wang	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023734	computer vision;digital watermarking;computer science;engineering;electrical engineering;theoretical computer science;mathematics;multimedia;watermark	EDA	42.11700325511232	-12.515862469898549	181109
bb44454a2c4036bf2d9989dda67fa56a5f93d774	accounting for companding nonlinearities in lossless audio compression	public domain players lossless audio compression digital audio tape recorders digital video camcorders high definition compatible digital nonlinearities detection audio resolution;companding;lossless audio compression;public domain players;data compression;video signal processing;nonlinear prediction;audio formats;lossless compression;audio recording;indexing terms;digital video camcorders;linear predictive;audio compression predictive models audio tapes video equipment costs switches dynamic range signal to noise ratio audio recording video signal processing;public domain;audio tapes;high definition compatible digital;audio formats companding audio compression nonlinear prediction lossless compression;audio coding;digital audio tape recorders;audio resolution;video equipment;dynamic range;predictive models;digital video;data compression audio coding audio recording;signal to noise ratio;switches;audio compression;nonlinearities detection	This paper introduces a novel prediction structure for improving the lossless compression ratio, by accounting for companding nonlinearities of different sample-based audio formats. This applies to a wide class of formats including a-law, μ-law, DAT-LP (digital audio tape recorders), DV-LP (digital video camcorders), and HDCD (high definition compatible digital). The proposed prediction structure obtains significant compression improvements (8-12%) over traditional linear prediction for a-law, μ-law, DAT-LP, DV-LP and also small compression improvements for HDCD. The improvement in compression can also be used for the detection of nonlinearities in HDCD format, making possible to play HDCD CDs at improved audio resolution in ordinary public domain players.	companding;data compression;digital video;intel dynamic acceleration;lossless compression	Florin Ghido;Ioan Tabus	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366666	data compression;lossy compression;computer vision;data compression ratio;dynamic range;public domain;dynamic range compression;speech recognition;index term;audio normalization;network switch;computer science;speech coding;companding;lossless compression;predictive modelling;multimedia;signal-to-noise ratio;g.711	Vision	50.264408160918315	-10.05702009554176	181127
4c79062f70013e0c5990fdc6dde9631536b4f6a6	computation-aware scheme for software-based block motion estimation	diamond search;optimisation;estimation mouvement;image processing;data compression;optimizacion;computation distortion optimization;complexite calcul;image matching;estimacion movimiento;procesamiento imagen;three step search;motion estimation;power allocation;indexing terms;traitement image;software based;motion estimation computer science computational complexity codecs application software high performance computing videos iso standards psnr computational modeling;algorithme;algorithm;rate control;video coding;complejidad computacion;prediction theory;computational complexity;video compression software based block motion estimation block matching algorithms computational complexity checking points computation aware scheme computation distortion optimization predicted computation distortion benefit heuristic;correspondencia bloque;block matching;optimization;search problems;correspondance bloc;full search block matching;prediction theory motion estimation video coding data compression image matching computational complexity search problems optimisation;speedup;block matching algorithm;algoritmo;block based motion estimation	Many fast block-matching algorithms (BMAs) reduce the computational complexity of motion estimation by sophisticatedly inspecting a subset of checking points, and stop only when all those checking points have been examined. This means that the searching process for each current block cannot be interrupted, even when it is performed in a software-based computation environment. Our main goal is to allow the searching process to stop once a specified amount of computation has been performed. A novel computation-aware scheme is proposed, which first dynamically determines the target amount of computation power allocated to a frame, and then allocates this to each block in a computation-distortion-optimized manner. We propose a rate-control-like procedure and a predicted computation-distortion benefit heuristic to realize this scheme. Conventional BMAs, such as full-search block matching, three-step search, new three-step search, four-step search, and diamond search, can be transformed into their corresponding computation-aware BMA versions. In our simulations, the resulting computation-aware BMAs not only exhibit higher efficiency than conventional BMAs, but also allow the motion estimation to terminate after any specified amount of computation has been performed (in units of checking points).	algorithm;computation;motion estimation	Pol-Lin Tai;Shih-Yu Huang;Chii-Tung Liu;Jia-Shung Wang	2003	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2003.816510	data compression;computer vision;mathematical optimization;combinatorics;index term;image processing;speedup;computer science;theoretical computer science;machine learning;motion estimation;mathematics;block-matching algorithm;computational complexity theory;algorithm	EDA	48.337094344811966	-19.002245919556064	181208
ccb7875b965d6c1889b59822843513e23770f1c4	a high secure reversible visible watermarking scheme	discrete wavelet transforms;reversible data embedding algorithm;watermarking;image coding;normal distribution;watermarking image restoration;color;key based scheme;data embedding;image restoration;reversible visible watermarking scheme;discrete cosine transforms;pixel;random variable;key based scheme reversible visible watermarking scheme inverse mapping reversible data embedding algorithm;robustness;humans;side information;digital images;watermarking image restoration digital images discrete cosine transforms pixel robustness humans discrete wavelet transforms color image coding;inverse mapping	A novel reversible visible watermarking algorithm is proposed. It can fully remove the watermark from the visible watermarked image such that the original image can be restored. Pixel values of original image beneath the watermark are mapped to a small range [alpha, alpha + 127] to generate a visible watermarked image. Since the mapping is many-to-one, taking inverse mapping can only approximate the original image. To restore the original image, the difference image of subtracting the approximated image from the original image and other side information are losslessly compressed to be embedded in the visible watermarked image by a reversible data embedding algorithm. We proposed a key-based scheme for the compromise between transparency and robustness. The key is a random variable with discrete normal distribution. In addition, only users with correct key can restore the original image. In the experimental results, we show the transparent degree of watermark can be controlled by the variance of the key. Users with wrong key can not restore the original image from the visible watermarked image.	approximation algorithm;digital watermarking;embedded system;lossless compression;one-to-many (data model);pixel	Han-Min Tsai;Long-Wen Chang	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4285098	normal distribution;random variable;image restoration;computer vision;feature detection;discrete mathematics;digital watermarking;computer science;theoretical computer science;mathematics;digital image;pixel;statistics;robustness	Vision	40.37158060262958	-11.230654276325856	181269
2f133cbe98fd15e862498fd58cdfca7ab6884cae	a selective encryption scheme of cabac based on video context in high efficiency video coding		In this paper, we propose a selective encryption scheme with a chaotic encryption system for the High Efficiency Video Coding (HEVC) standard in which Context Adaptive Binary Arithmetic Coding (CABAC) is the only entropy coder for transform coefficient coding. Our method focuses on the binstrings of truncated rice with a context “p” (TRp) code suffix and kth order Exp-Golomb (k = p + 1) code suffix before Binary Arithmetic Coding (BAC) for the remaining absolute level coding, which is coded by the bypass mode in the entropy coding stage. The probability of symbols does not change and CABAC decoding has no effect after encryption. Several different YUV sequences are used for experimental evaluation of the proposed algorithm. Compared to previous researches, our approach has good protection for video information, which keeps a constant bitrate and format compatibility, and meantime it has a negligible impact on encoding performance.	algorithm;batman: arkham city;binary number;bitstream;coefficient;context-adaptive binary arithmetic coding;data compression;exptime;encryption;entropy encoding;golomb coding;golomb ruler;high efficiency video coding;high-level programming language;real-time clock;real-time computing	Jianjun Li;Chenyan Wang;Xie Chen;Zheng Tang;Guobao Hui;Chin-Chen Chang	2017	Multimedia Tools and Applications	10.1007/s11042-017-4916-2	computer science;artificial intelligence;encryption;variable-length code;shannon–fano coding;pattern recognition;context-adaptive variable-length coding;theoretical computer science;entropy encoding;constant bitrate;context-adaptive binary arithmetic coding;coding tree unit	DB	47.74013375414174	-17.8723685681393	181408
6d800bf2e8296d4aba624b4a286970ea0ac73ad1	robust decoding of arithmetic codes for image transmission over error-prone channels	belief networks;error resilience performance context based arithmetic code image transmission error prone channel bayesian formalism soft synchronization jpeg 2000;image coding;arithmetic coding;data compression;data compression arithmetic codes visual communication image coding belief networks synchronisation;visual communication;robustness decoding arithmetic image communication bayesian methods error correction codes clocks algorithm design and analysis context modeling synchronization;synchronisation;arithmetic codes;image transmission;bayesian estimator;error resilience;stochastic model	"""This paper addresses the issue of robust decoding of arithmetic codes. We first analyze dependencies between the variables involved in arithmetic coding by means of the Bayesian formalism. This provides a suitable framework for designing a soft decoding algorithm that provides high error-resilience. It also provides a natural setting for """"soft synchronization"""", i.e., to introduce anchors favoring the likelihood of """"synchronized"""" paths. In order to maintain the complexity of the estimation within a realistic range, a simple, yet efficient, pruning method is described. Models and algorithms are then applied to context-based arithmetic coding widely used in practical systems (e.g. JPEG-2000). Experimentation results with both theoretical sources and with real images coded with JPEG-2000 reveal very good error resilience performances."""	code;cognitive dimensions of notations	Thomas Guionnet;Christine Guillemot	2003		10.1109/ICIP.2003.1246899	data compression;arithmetic coding;synchronization;real-time computing;variable-length code;computer science;stochastic modelling;theoretical computer science;machine learning;mathematics;context-adaptive binary arithmetic coding;statistics;visual communication	Theory	49.46454877505846	-15.140828554017055	181411
417032097ea84f37adf45448fb8635af8c6f3a76	fully-scalable wavelet video coding using in-band motion compensated temporal filtering	image sampling;discrete wavelet transforms;complexity requirements;filtering;in band motion compensated temporal filtering;image resolution fully scalable wavelet video coding in band motion compensated temporal filtering open loop motion compensated temporal filtering wavelet domain spatial domain mctf image data critically sampled wavelet transform in band mctf discrete wavelet transform dwt temporal scalability snr spatial scalability sdmctf ibmctf coding efficiency sequence content complexity requirements;coding efficiency;dwt;discrete wavelet transform;ibmctf;image resolution;data compression;motion compensation;decoding;critically sampled wavelet transform;open loop motion compensated temporal filtering;snr;motion estimation;transform coding;bit rate;spatial domain mctf;sdmctf;sequence content;image data;motion compensated temporal filtering;video coding;mpeg 4 standard;wavelet transform;temporal scalability;video coding scalability discrete wavelet transforms filtering spatial resolution wavelet domain bit rate decoding motion estimation mpeg 4 standard;in band mctf;scalability;wavelet domain;fully scalable wavelet video coding;filtering theory;image resolution video coding data compression discrete wavelet transforms transform coding motion compensation filtering theory image sampling image sequences;spatial scalability;image sequences;spatial resolution	This paper presents a novel fully-scalable wavelet video coding scheme that performs efficient open-loop motion compensated temporal filtering (MCTF) in the wavelet domain (in-band). Unlike the conventional spatial-domain MCTF (SDMCTF) schemes, which apply MCTF on the original image data and then encode the residual image using a critically-sampled wavelet transform, the framework presented here applies the in-band MCTF (IBMCTF) after the discrete wavelet transform (DWT) is performed in the spatial dimensions. To overcome the inefficiency of motion estimation (ME) in the wavelet domain, a complete-to-overcomplete DWT (CODWT) is performed. The proposed framework provides improved quality (SNR) and temporal scalability as compared with existing in-band closed-loop temporal prediction schemes with ODWT and improved spatial scalability as compared to SDMCTF. We present a thorough comparison between SDMCTF and the proposed IBMCTF in terms of coding efficiency and scalability. Furthermore, we describe several extensions that enable the filtering of the various bands to be performed independently, based on the resolution, sequence content, complexity requirements and desired scalability.	algorithmic efficiency;data compression;discrete wavelet transform;encode;motion compensation;motion estimation;requirement;scalability;signal-to-noise ratio;spatial anti-aliasing	Yiannis Andreopoulos;Mihaela van der Schaar;Adrian Munteanu;Joeri Barbarien;Peter Schelkens;Jan Cornelis	2003		10.1109/ICASSP.2003.1199500	computer vision;image resolution;second-generation wavelet transform;computer science;theoretical computer science;wavelet packet decomposition;stationary wavelet transform	Vision	45.359286294558615	-16.458663227714336	181471
caf7cead5fee6e178a2937d8bd8d332c9eb342a3	a scalable information security technique: joint authentication-coding mechanism for multimedia over heterogeneous wireless networks	estensibilidad;protection information;theorie vitesse distorsion;red sin hilo;detection erreur;wireless networks;deteccion error;error correcting code;document multimedia;reseau sans fil;information security;securite;video signal processing;telecommunication sans fil;codigo corrector error;correction erreur;articulation;authentication;wireless network;simulation;simulacion;multimedia document;correction directe erreur;vulnerability;articulacion;authentification;propagation erreur;rate distortion theory;video coding;joint source channel coding;vulnerabilite;vulnerabilidad;autenticacion;forward error correction;codage video;proteccion informacion;error propagation;telecomunicacion sin hilo;error correction;information protection;robustesse;multimedia communication;safety;traitement signal video;error resilience;multimedia security;robustness;heterogeneous wireless networks;codificacion fuente canal;temps retard;extensibilite;scalability;correccion error;delay time;propagacion error;joint;error detection;growth of error;communication multimedia;code correcteur erreur;rate distortion optimization;seguridad;tiempo retardo;codage source canal;robustez;wireless telecommunication;documento multimedia	As multimedia is expected to be a major traffic source in the next-generation wireless networks, there have been increasing concerns about the security issues of wireless transmission of multimedia in recent years. Wireless networks, by their natures, are more vulnerable to external intrusions than wired ones. Therefore, many applications demand authenticating the integrity of multimedia content delivered wirelessly. In this work, we propose a framework for jointly authenticating and coding multimedia to be transmitted over heterogeneous wireless networks. We firstly provide a novel graphbased authentication scheme which can not only construct the authentication graph flexibly but also trade-off well among some practical requirements such as overhead, robustness and delay. And then, a rate-distortion optimized joint source-channel coding (JSCC) approach for error-resilient scalable encoded video is presented, in which the video is encoded into multiple independent streams and each stream is assigned forward error correction (FEC) codes to avoid error propagation. Furthermore, we consider integrating authentication with the specific JSCC scheme to achieve a satisfactory authentication results and end-to-end reconstruction quality by optimally applying the appropriate authentication and coding rate. Simulation results show the effectiveness of the proposed authentication-coding scheme for multimedia over wireless networks.	authentication;code;distortion;end-to-end encryption;error detection and correction;forward error correction;information security;overhead (computing);propagation of uncertainty;requirement;scalability;simulation;software propagation	Liang Zhou;Baoyu Zheng;Anne Wei;Benoit Geller;Jingwu Cui	2009	Wireless Personal Communications	10.1007/s11277-008-9595-x	simulation;error detection and correction;telecommunications;computer science;information security;wireless network;authentication;computer security;statistics;computer network	Mobile	47.223801865709966	-13.606776193197533	181841
58d505e9ceddb91f106a376e2429b57d6941118a	a very low bit-rate video coding algorithm by focusing on moving regions	videoconference;encoding;prediction error;motion estimation;macroblock;video compression;shape;motion compensation	Block-based motion estimation and compensation are the most popular techniques for video coding. As the shape and the structure of an object in a picture are arbitrary, the performance of such conventional block-based methods may not be satisfactory. In this paper, a very low bit-rate video coding algorithm by focussing on moving regions is proposed. The objective is to improve the coding performance, which gives better subjective and objective quality than that of the conventional coding methods at the same bit rate. We predefined eight patterns to represent the moving regions in a macroblock. The patterns are then used for motion estimation and compensation to reduce the prediction errors. Furthermore, in order to increase the compression performance, the residual errors of a macroblock are rearranged into a block with no significant increase of high order DCT coefficients. As a result, both the prediction efficiency and the compression efficiency are improved.	algorithm;coefficient;data compression;discrete cosine transform;macroblock;motion estimation	Kwok-Wai Wong;Kin-Man Lam;Wan-Chi Siu	2000			video compression picture types;data compression;scalable video coding;sub-band coding;computer vision;quarter-pixel motion;computer science;theoretical computer science;mean squared prediction error;video tracking;coding tree unit;motion estimation;block-matching algorithm;rate–distortion optimization;context-adaptive binary arithmetic coding;macroblock;motion compensation;global motion compensation;h.261;statistics;sum of absolute transformed differences;multiview video coding;computer graphics (images)	Vision	46.12548933084036	-17.713224630899404	181879
d17bb88bfeb4389210899fb138c1ba15ba029a3c	extended optimization method of lsb steganalysis	optimization methods steganography pixel laboratories pattern recognition automation performance evaluation estimation error data encapsulation internet;optimisation;image resolution;estimation method;numerical method;optimal method;natural images;data encapsulation;least significant bit;cryptography;image resolution cryptography data encapsulation optimisation;estimation error;image pixel extended optimization method image steganalysis secret message estimation errors least significant bit embedding;eestimation steganography lsb steganalysis	Image steganalysis has attracted increasing attention recently. LSB steganalysis is one of the most active research topics. The paper proposes a method for LSB steganalysis of images, where the secret message is embedded in a given number L of the least significant bits. The proposed estimation method is an extension of Fridrich's method from the case L = 1 to arbitrary L > 0. A weighted stego image is defined first and then estimation formula is derived. To evaluate the proposed steganalytic method, two experiments of detection and estimation are performed. It is shown that the accuracy of detecting the existence of secret messages in images and of estimating the embedding ratio of secret messages is relatively high. Estimation errors and further studies are also discussed. Experimental results and theoretical verification show that this method is an effective method of LSB steganalysis.	effective method;embedded system;experiment;least significant bit;mathematical optimization;sensor;steganalysis;steganography	Xiaoyi Yu;Tieniu Tan;Yunhong Wang	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530252	least significant bit;computer vision;image resolution;steganalysis;computer science;cryptography;theoretical computer science;mathematics;internet privacy;statistics	Robotics	41.0963484799719	-10.905126462210289	181971
eb41a141258c56e4948b258b851ab6bc6ed1a902	source adaptive software 2d idct with simd	simd;histograms;degradation;image coding;adaptive decoding;data compression;execution time;decoding;inverse discrete cosine transform;source adaptive software 2d idct;adaptive control;video compression;video quality;parallel programming;code standards;source adaptive method;transform coding;bit rate;mpeg2 natural video;murata s method;single instruction multiple data;software video decoding source adaptive software 2d idct simd fast two dimensional inverse discrete cosine transform compressed video source statistics execution time idct algorithms sparse blocks quad word parallel single instruction multiple data multimedia instructions end of block marker value histograms adaptive control mechanism software overheads penalties decoded video quality mpeg2 natural video murata s method source adaptive method;penalties;multimedia computing;video coding;software overheads;software video decoding;discrete cosine transforms;discrete cosine transforms decoding statistics adaptive control bit rate image coding algorithm design and analysis video compression histograms degradation;end of block marker value histograms;adaptive method;statistics;decoded video quality;fast two dimensional inverse discrete cosine transform;sparse blocks;algorithm design and analysis;quad word parallel single instruction multiple data multimedia instructions;compressed video;adaptive control mechanism;idct algorithms;compressed video source statistics;parallel programming video coding adaptive decoding adaptive control transform coding discrete cosine transforms data compression multimedia computing code standards	This paper presents a fast two-dimensional inverse discrete cosine transform that adapts to compressed video source statistics to reduce execution time. iDCT algorithms for sparse blocks eliminate calculations for some zero coefficients and are implemented with quad-word parallel single-instruction-multiple-data (SIMD) multimedia instructions. It is observed that end-of-block marker value histograms vary little within single shots. An adaptive control mechanism is proposed that chooses the optimal set of iDCTs to prepare for an entire shot from its 1st frames (to reduce software overheads and penalties). This introduces no degradation of decoded video quality compared with a conventional SIMD 8/spl times/8 iDCT implemented with Intel MMX instructions. It is confirmed that execution time is reduced an additional 15% with Murata's method for 4 Mbps MPEG2 natural video. In comparison, execution time is reduced 22% with a modified version Murata's method, and by 35% with the new source adaptive method.	discrete cosine transform;simd	Lowell L. Winger	2000		10.1109/ICASSP.2000.860191	data compression;parallel computing;real-time computing;simd;adaptive control;computer science;theoretical computer science;mathematics;statistics	Arch	47.33930981872719	-17.258296513952335	182041
d828e30aa1db061cda9ccb98d78272ba7b3a9264	atom-based-segmented mp compression algorithm for seismic data		Data compression is an effective way to improve the seismic data transmission efficiency. The features of seismic exploration are long sampling time and large data quantity, so the compression algorithm should achieve high-fidelity, high-compression ratio (CR) and low-compression time. Since the existing compression algorithms cannot meet the requirements of site-collected seismic data compression, the segmented matching pursuit (SMP) compression algorithm based on new atom dictionary is proposed. This method is based on the principle of MP. A novel-segmented compression structure is adopted. The modified Morlet wave atom dictionary is designed to replace the previous dictionaries. The results of comparative experiments show that the proposed algorithm makes improvements in CR and compression time with the same fidelity requirement.	algorithm;atom;data compression	Zhiyuan Yin;Yan Zhou;Yongxin Li	2018	IET Signal Processing	10.1049/iet-spr.2017.0226	mathematics;matching pursuit;theoretical computer science;data transmission;sampling (statistics);data compression;computer vision;compression (physics);artificial intelligence	ML	41.52955009015983	-16.76292347595173	182071
3097bfb1ead1924b91cf7d8935be0ddca8b2943d	no reference metric of video coding quality based on parametric analysis of video bitstream	image features;image coding;psnr;measurement;measurement feature extraction image coding psnr transforms training streaming media;training;perceived picture quality;video coding;quality assessment;streaming media;feature extraction;objective picture quality assessment;transforms;coding artifacts no reference metric video coding quality parametric analysis video bitstream coded bitstream spatiotemporal image features;parametric analysis;correlation coefficient;video coding feature extraction;no reference objective picture quality assessment perceived picture quality parametric framework;parametric framework;computer simulation;no reference	In this paper, we propose a novel method to measure the perceived picture quality of H.264 coded video based on parametric analysis of the coded bitstream. The parametric analysis means that the proposed method utilizes only bitstream parameters, while it does not have any access to the baseband signal (pixel level information) of the decoded video. The proposed method extracts quantiser-scale, slice type and transformed coefficients from each macroblock. These parameters are used to calculate spatiotemporal image features to reflect coding artifacts which have a strong relation to the subjective quality. A computer simulation shows that the proposed method can estimate the subjective quality at a correlation coefficient of 0.867 whereas the PSNR metric, which is referred to as a benchmark, correlates the subjective quality at a correlation coefficient of 0.773.	baseband;benchmark (computing);bitstream;coefficient;computer simulation;data compression;h.264/mpeg-4 avc;image quality;macroblock;peak signal-to-noise ratio;pixel;quantization (signal processing)	Osamu Sugimoto;Sei Naito;Yoshinori Hatori	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116385	scalable video coding;computer simulation;computer vision;peak signal-to-noise ratio;feature extraction;computer science;video quality;theoretical computer science;multimedia;parametric statistics;feature;measurement;statistics	Vision	45.22601456692298	-21.119050774462536	182131
93c7ba68d9dd137872154480143003c9131f1c59	depth assisted adaptive workload balancing for parallel view synthesis	depth image-based rendering;free viewpoint television;parallel view synthesis;workload balancing;workload prediction	Depth image-based rendering has been adopted by MPEG as the recommended view synthesis technique for free viewpoint TV applications. In this paper, a workload balancing algorithm is proposed for parallel view synthesis on multicore platforms. First, view synthesis workload is defined as the function of the number of hole-pixels in the warped images. Then, a novel depth assisted prediction method is proposed to predict the number of hole-pixels in the current frame by exploiting the depth differences between the neighboring frames, which reflects the movement of objects in video content. Feeding the predicted workload to the proposed cost function, each input frame is partitioned adaptively to balance the synthesis workload among the cores. The proposed workload prediction method outperforms the existing approaches both in terms of frame average prediction error and standard deviation in prediction error. Applying the proposed workload balancing method, the parallel view synthesis system provides higher acceleration ratio and better synchronization performance among the cores compared with other parallel processing systems without sacrificing the subjective and objective quality. It is also robust to different platforms, which shows high potential in being applied to mobile oriented applications.	algorithm;digital video;load balancing (computing);loss function;moving picture experts group;multi-core processor;parallel computing;pixel;view synthesis	Xin Jin;Qian Li;Qionghai Dai	2018	IEEE Transactions on Multimedia	10.1109/TMM.2018.2827781	real-time computing;view synthesis;rendering (computer graphics);artificial intelligence;workload;acceleration;computer vision;computer science;synchronization;standard deviation;transform coding;multi-core processor	Visualization	46.169714618552646	-20.814576074889892	182233
77389a7f59f3f0e511a825f80d8a963d36afd152	novel h.264/avc entropy coding mode decision	rate distortion;h 264 avc;rate distortion and decoding complexity rdc optimization;variable length code;entropy coding mode decision;entropy coding;cabac;video coding;decoding complexity profiling;cavlc uvlc;decoding complexity model;rate distortion rd optimization;mode decision;context based adaptive binary arithmetic coding	1047-3203/$ see front matter 2011 Elsevier Inc. A doi:10.1016/j.jvcir.2011.03.004 ⇑ Corresponding author. E-mail addresses: szuweile@usc.edu (S.-W. Lee), sw sipi.usc.edu (C.-C.J. Kuo). In this work, we propose a novel entropy coding mode decision algorithm to balance the tradeoff between the rate-distortion (R-D) performance and the entropy decoding complexity for the H.264/ AVC video coding standard. Context-based adaptive binary arithmetic coding (CABAC), context-based adaptive variable length coding (CAVLC), and universal variable length coding (UVLC) are three entropy coding tools adopted by H.264/AVC. CABAC can be used to encode the texture and the header data while CAVLC and UVLC are employed to encode the texture and the header data, respectively. Although CABAC can provide better R-D performance than CAVLC/UVLC, its decoding complexity is higher. Thus, by taking the entropy decoding complexity into account, CABAC may not be the best tool, which motivates us to examine the entropy coding mode decision problem in depth. It will be shown experimentally that the proposed mode decision algorithm can help the encoder generate the bit streams that can be decoded at much lower complexity with little R-D performance loss. 2011 Elsevier Inc. All rights reserved.	algorithm;binary number;bitstream;context-adaptive binary arithmetic coding;context-adaptive variable-length coding;data compression;decision problem;distortion;encode;elegant degradation;encoder;entropy encoding;experiment;h.264/mpeg-4 avc;peak signal-to-noise ratio;requirement;resultant;variable-length code;video coding format	Szu-Wei Lee;C.-C. Jay Kuo	2011	J. Visual Communication and Image Representation	10.1016/j.jvcir.2011.03.004	arithmetic coding;real-time computing;shannon–fano coding;variable-length code;computer science;entropy encoding;theoretical computer science;context-adaptive variable-length coding;mathematics;tunstall coding;context-adaptive binary arithmetic coding;statistics	AI	47.292819933726335	-16.148448707914195	182389
1b6de3accf9314e69f29ae08a2d4eb8f0d861e2f	watermarking for 3d polygonal meshes using normal vector distributions of each patch	patch watermarking 3d polygonal mesh normal vector distribution;watermarking;image coding;image coding watermarking multidimensional signal processing;polygonal meshes;multidimensional signal processing;watermarking robustness mobile communication gaussian distribution business communication computer science copyright protection solid modeling mpeg 4 standard clustering algorithms	We proposed a watermarking for 3D polygonal meshes using normal vector distribution of each patch. The distribution of norm vector for 3D polygonal mesh is a consistent factor to be robust against various attacks such as af£ne transform and mesh connectivity altering. Therefore, the proposed algorithm embedded watermark into these consistent normal vectors and equal watermark bit was embedded into each patch of subdivided mesh to have robustness against the partial geometric deformation. Experiment results exhibited the proposed algorithm is robust by extracting watermark bit for various attacked models.	algorithm;digital watermarking;embedded system;emoticon;normal (geometry);polygon mesh;stanford bunny;xfig	Ki-Ryong Kwon;Seong Geun Kwon;Suk Hwan Lee;Tae-Su Kim;Kuhn-Il Lee	2003		10.1109/ICIP.2003.1246726	multidimensional signal processing;computer vision;discrete mathematics;digital watermarking;computer science;theoretical computer science;mathematics	Graphics	41.25427686264541	-11.377345844048644	182550
eafbd8a0c7ddc55a25e3822df92b34b43a1c75e9	a de-blocking algorithm and a blockiness metric for highly compressed images	artefacto;metodo adaptativo;evaluation performance;image coding;performance evaluation;image processing;adaptive de blocking;data compression;methode mesure;evaluacion prestacion;transformation cosinus discrete;procesamiento imagen;metodo medida;image coding discrete cosine transforms frequency image reconstruction video compression smoothing methods bit rate quantization computational efficiency psnr;methode adaptative;transform coding;indexing terms;traitement image;discrete cosine transform;artefact;reduccion ruido;algorithme;algorithm;reconstruction image;reconstruccion imagen;image reconstruction data compression image coding transform coding discrete cosine transforms;discrete cosine transforms;image reconstruction;noise reduction;peak signal to noise ratio;adaptive method;reduction bruit;quantization parameter;low computation cost blockiness metric compressed images image reconstruction block based discrete cosine transform bdct compressed video noniterative de blocking algorithms pixels block boundaries homogeneous region connected blocks quantization step size 1d dct coefficients odd dct coefficients even dct coefficients pixel value dct coefficient subjective evaluation peak signal to noise ratio blocking artifacts texture areas;measurement method;subjective evaluation;blocking artifact metric;based discrete cosine transform;blocking artifact;video post processing;algoritmo	Blockiness is a typical artifact in reconstructed images that have been coded by a block-based discrete cosine transform (BDCT). In highly compressed images and video, the blocking artifacts are easily noticeable as the discontinuities between relatively homogeneous regions. Many current noniterative de-blocking algorithms attempt to remove the blocking artifacts by smoothing a few pixels around the block boundaries; however, the results are not satisfactory, especially at very low bit rates. We propose a de-blocking algorithm based on the number of connected blocks in a relatively homogeneous region, the magnitude of abrupt changes between neighboring blocks, and the quantization step size of DCT coefficients. Due to its adaptability, the proposed algorithm can smooth out the blocking artifacts while keeping the strong edges and texture areas untouched. Since this algorithm is noniterative and only identifies those block pairs that actually need de-blocking, its computation cost is low. In addition, we have developed a new metric to measure the blocking artifacts in images. Through analyzing the 2N-point (N is the block size) one-dimensional DCT coefficients of the two neighboring blocks with blocking artifacts, we show that all of the even DCT coefficients of the combined 2N points are zeros (except frequency k=0). The odd DCT coefficients are proportional to the pixel value difference between these two blocks with their magnitudes almost inversely proportional to frequency k. We selected the first DCT coefficient (frequency k=1) as an indicator for the strength of blocking artifacts in the reconstructed images. For the postprocessed images, we used a weighted summation of the squared first DCT coefficient to measure their blocking artifacts. Experimental results demonstrate that our proposed de-blocking algorithm produces better results than other methods, both visually and quantitatively, while the proposed blocking artifact metric is more consistent with subjective evaluation than the peak signal-to-noise ratio.	algorithm;blocking (computing)	Wenfeng Gao;Coskun Mermer;Yongmin Kim	2002	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2002.806817	data compression;iterative reconstruction;computer vision;mathematical optimization;transform coding;index term;peak signal-to-noise ratio;image processing;computer science;theoretical computer science;discrete cosine transform;noise reduction;mathematics;video post-processing;algorithm	Vision	45.54788717567165	-13.881147853170365	182869
9611deb9ffa13fe20b587ecfb22d410b9de5fac7	a reversible data hiding based on adaptive prediction technique and histogram shifting	histograms;sorting method reversible data hiding adaptive prediction technique histogram shifting;sorting;distortion reduction stego image reversible data hiding method adaptive prediction technique prediction error histogram data embedding histogram shifting method sorting method;data mining;histograms sorting data mining image edge detection prediction methods correlation decision support systems;sorting adaptive estimation data encapsulation distortion image watermarking prediction theory;prediction methods;image edge detection;decision support systems;correlation	Reversible data hiding recovers the original image from the stego-image without distortion after data extraction. In this paper, we propose a novel reversible data hiding method based on adaptive prediction techniques and histogram shifting. Because most natural images always contain edges, it is not suitable to predict these pixels using existing prediction methods. For more precise prediction, two prediction methods are adaptively used to calculate prediction error according to the characteristic of a pixel. As a result, two prediction error histograms are built. One is for pixels located at edges, and the other is for the rest pixels. Data are embedded in the image by using histogram shifting method. In addition, a new sorting method is applied to histogram shifting, which considers the differences of all pixel pairs in the neighborhood and better reflects the correlation among pixels. Through the sorting method, the prediction errors with small absolute values are arranged in the front and more embeddable pixels are preferentially processed. Therefore, the number of shifting pixels is decreased if the peaks in the histograms are all dealt with or the capacity is satisfied, which is beneficial to distortion reduction. Experimental results demonstrate that the proposed method acquires greater capacity and higher quality compared with other state-of-the-art schemes.	adaptive histogram equalization;distortion;embedded system;pitch shift;pixel;sorting;steganography	Rui Liu;Rongrong Ni;Yao Zhao	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041698	computer vision;histogram matching;pattern recognition;data mining;mathematics;adaptive histogram equalization;image histogram	Vision	42.01944157512374	-14.2672053253669	182933
feb29faad7e10d9a7c5b0b34017c3cd0bcaf89e5	novel fast pu decision algorithm for the hevc video standard	image motion analysis;block motion complexity high efficiency video coding inter frame;image classification;video coding;video coding image classification image motion analysis spatiotemporal phenomena;hm 7 0 reference software hevc video standard fast pu decision method encoder complexity reduction spatio temporal analysis motion activity classification random access main profile configuration psnr video coding test model;spatiotemporal phenomena	In this paper, we propose a fast PU decision method to reduce encoder complexity. We use an early PU decision in each CU-level based on spatio-temporal analyses and depth correlations. We also consider a classification of motion activity. Experimental results show that the encoding complexity can be reduced by up to 37.98% on average in the random access Main profile configuration with only a small bit-rate increment and a PSNR decrement, compared to high efficiency video coding test model (HM) 7.0 reference software.	algorithm;data compression;encoder;high efficiency video coding;ibm systems network architecture;increment and decrement operators;one-class classification;peak signal-to-noise ratio;random access;tip (unix utility)	Jong-Hyeok Lee;Chan-seob Park;Byung-Gyu Kim;Dong-San Jun;Soon-Heung Jung;Jin Soo Choi	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738408	computer vision;contextual image classification;quarter-pixel motion;computer science;video quality;theoretical computer science;machine learning;coding tree unit;motion estimation;block-matching algorithm;multimedia;motion compensation;h.261;multiview video coding	Robotics	46.721166379520845	-19.33502136396967	183336
94442a63ca6569bf253c56d26f4b641b88d38d6f	two-channel predictive multiple description coding	available bit rate;channel coding;rate distortion;erasure channel;multiple description;parity check codes;sequential decoding;binary codes;sequential decoding channel coding video coding video codecs binary codes parity check codes;video codec;video coding;video coding two channel predictive coding multiple description coding rate distortion performance video codec principles side information coding binary ldpc codes decoder reconstruction sequential decoders packet erasure channels;decoding parity check codes streaming media video codecs video coding degradation predictive coding video sequences entropy coding computer architecture;ldpc code;multiple description coding;video codecs;random permutation;side information;empirical evaluation;high light	This paper presents a multiple description (MD) video codec based on the principles side-information coding. In particular, we highlight certain key components of the codec design that contribute significantly to the rate-distortion performance of the proposed codec. These include the use of randomized permutations of the quantization codebook in conjunction with binary LDPC codes for partitioning the available bit-rate among the coefficient bit-planes. Another key component of the proposed codec is the use of pdf estimation for improved decoder reconstruction. Lastly, we use a bank of sequential LDPC decoders to efficiently decode the transmitted coset information. Empirical evaluation demonstrates the superior performance of the proposed codec for the communication of encoded video over packet erasure channels.	binary erasure channel;codebook;codec;coefficient;distortion;low-density parity-check code;multiple description coding;network packet;portable document format;randomized algorithm	Ashish Jagmohan;Anshul Sehgal;Narendra Ahuja	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530144	adaptive multi-rate audio codec;binary code;binary erasure channel;random permutation;real-time computing;low-density parity-check code;channel code;telecommunications;sequential decoding;computer science;theoretical computer science;multiple description coding;mathematics;statistics	Robotics	47.592930080372945	-16.052243187728894	183740
02b8a15934c3f2d2320fe09b90d9fa866a02e68b	codestream domain scrambling of moving objects based on dct sign-only correlation for motion jpeg movies	object detection discrete cosine transforms image coding image motion analysis;moving object;image motion analysis;image coding;surveillance;discrete cosine transforms motion pictures image coding decoding object detection cameras shape privacy protection quantization;indexing terms;discrete cosine transform;image coding privacy surveillance object detection discrete cosine transforms correlation;motion jpeg;discrete cosine transforms;moving object detection;correlation;discrete cosine transform object scrambling method motion jpeg movies dct sign only correlation image coding;privacy;object detection	This paper proposes a moving objects scrambling method for Motion JPEG movies that consists of two parts: a moving objects detection based on DCT sign-only correlation (DSOC) and a partial scrambling. DSOC is the correlation of the positive and negative signs of DCT coefficients. Utilizing the codestream structure of Motion JPEG movies and the relation between motion and DSOC, the proposed moving objects detection is achieved in codestream domain. The proposed scrambling is also achieved in codestream domain by inverting signs of DCT coefficients that each sign is independently encoded as one bit. The codestream domain processing in this method serves low processing time and keeps coding efficiency. Moreover, the proposed method completely descrambles scrambled codestreams to the original without any knowledge on the position and shape of moving objects. Simulation results show the effectiveness of the proposed method.	algorithmic efficiency;coefficient;discrete cosine transform;jpeg;simulation	Keijiro Kuroiwa;Masaaki Fujiyoshi;Hitoshi Kiya	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379789	computer vision;index term;computer science;theoretical computer science;discrete cosine transform;mathematics;privacy;correlation;motion jpeg;computer graphics (images)	Robotics	39.65612181149345	-13.113553580649539	183746
7bd19f37bd85824d52ecdd9a4141c841508dcb24	optimizing 360 video delivery over cellular networks	virtual reality;cellular networks;head movement prediction;360 degree video	As an important component of the virtual reality (VR) technology, 360-degree videos provide users with panoramic view and allow them to freely control their viewing direction during video playback. Usually, a player displays only the visible portion of a 360 video. Thus, fetching the entire raw video frame wastes bandwidth. In this paper, we consider the problem of optimizing 360 video delivery over cellular networks. We first conduct a measurement study on commercial 360 video platforms. We then propose a cellular-friendly streaming scheme that delivers only 360 videos' visible portion based on head movement prediction. Using viewing data collected from real users, we demonstrate the feasibility of our approach, which can reduce bandwidth consumption by up to 80% based on a trace-driven simulation.	360-degree video;optimizing compiler;simulation;uncompressed video;viewing cone;virtual reality	Feng Qian;Lusheng Ji;Bo Han;Vijay Gopalakrishnan	2016		10.1145/2980055.2980056	video compression picture types;cellular network;simulation;telecommunications;computer science;operating system;video tracking;virtual reality;multimedia;video processing;computer graphics (images)	Mobile	42.62678734276468	-22.559703246702906	184043
6181d4bf59205c5d1a0eb6bc06cc8518b8e387d4	video compression for lossy packet networks with mode switching and a dual-frame buffer	engineering;theorie vitesse distorsion;data transmission;evaluation performance;rate distortion;decision feedback;estimation mouvement;visual communication video coding video codecs data compression packet switching buffer storage motion compensation rate distortion theory motion estimation feedback;performance evaluation;data compression;motion compensation;red conmutacion por paquete;video signal processing;multiple frame prediction;commutation mode;packet loss;evaluacion prestacion;estimacion movimiento;visual communication;video compression;reference frame;buffer storage;motion estimation;packet switched;packet switching;indexing terms;packet switched network;buffer system;perdida transmision;video codec;reseau commutation paquet;motion compensated;sistema amortiguador;perte transmission;rate distortion theory;compensation mouvement;video coding;feedback;compression image;video compression packet switching streaming media protocols image coding motion compensation rate distortion feedback video codecs motion estimation;image compression;codage video;motion vector;transmission donnee;transmission loss;traitement signal video;codec;video codecs;packet networks;mode switching;packet switched networks;algoritmo optimo;systeme tampon;algorithme optimal;optimal algorithm;dual frame buffer;transmision datos;algorithms artifacts artificial intelligence data compression image enhancement internet pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique video recording;half pel motion vectors video compression lossy packet networks dual frame buffer video codecs motion compensation near optimal intra mode switching rate distortion framework multiple frame prediction motion estimation mode switching algorithm;compresion imagen	Video codecs that use motion compensation benefit greatly from the development of algorithms for near-optimal intra/inter mode switching within a rate-distortion framework. A separate development has involved the use of multiple-frame prediction, in which more than one past reference frame is available for motion estimation. In this paper, we show that using a dual-frame buffer (one short-term frame and one long-term frame available for prediction) together with intra/inter mode switching improves the compression performance of the coder. We improve the mode-switching algorithm with the use of half-pel motion vectors. In addition, we investigate the effect of feedback in making more informed and effective mode-switching decisions. Feedback information is used to limit drift errors due to packet losses by synchronizing the long-term frame buffers of both the encoder and the decoder.	algorithm;approximation;bi-directional text;biologic preservation;bit error rate;bitstream;buffers;choose (action);codec;coder device component;computational complexity theory;data compression;decoder device component;distortion;dual;encoder device component;frame (physical object);frame language;framebuffer;greater than;hearing loss, high-frequency;jumbo frame;kerrison predictor;lossy compression;motion compensation;motion estimation;network packet;osteoporosis, postmenopausal;pixel;population parameter;reference frame (video);rendering (computer graphics);rule (guideline);visual inspection	Athanasios Leontaris;Pamela C. Cosman	2004	IEEE Transactions on Image Processing	10.1109/TIP.2004.828429	data compression;inter frame;residual frame;real-time computing;fast packet switching;telecommunications;quarter-pixel motion;computer science;cut-through switching;block-matching algorithm;motion compensation;algorithm;packet switching;statistics	Visualization	47.95754242811476	-15.079724570035802	184058
08f8c76181be734a40ec5c0f5954a5d74b49ca3e	enhancement of error-correction coding of spatial watermarks for jpeg compression	discrete wavelet transforms;channel coding;gray code;error correction codes;image watermarking channel coding data compression discrete wavelet transforms error correction codes gray codes image coding;jpeg dct compression;image coding;spatial domain;data compression;digital watermark;gray code digital watermark spatial domain jpeg dct compression error correction coding;error correction coding;single error correcting block code error correction coding enhancement spatial watermark jpeg compression channel coding spatial image watermark jpeg dct based compression vertical error correction coding information bits encoding gray code performance improvement horizontal error correction coding;watermarking image coding encoding transform coding robustness psnr error correction codes;image watermarking;gray codes	This paper demonstrates how channel coding can improve the robustness of spatial image watermarks against JPEG DCT-based compression. Two error-correction coding (ECC) schemes are used here. One scheme, referred to as the vertical ECC (VECC), is to encode information bits in pixel levels by error-correction coding where the Gray code is used to improve the performance. The other scheme, referred to as the horizontal ECC (HECC), is to encode information bits in an image plane by error-correction coding. VECC is also used to encode the code bits of HECC in pixels. Simple single-error-correcting block codes are used in VECC and HECC. Several experiments of these schemes were conducted on test images. The result demonstrates that the error-correcting performance of HECC depends on that of VECC, and accordingly, HECC enhances the capability of VECC. Consequently, HECC with appropriate codes can achieve stronger robustness to JPEG-caused distortions than non-channel-coding watermarking schemes.	bch code;baseline (configuration management);convolutional code;discrete cosine transform;distortion;ecc memory;encode;error detection and correction;experiment;forward error correction;hyperelliptic curve cryptography;image plane;jpeg;low-density parity-check code;pixel;scheme;trellis quantization;turbo code	Tadahiko Kimoto	2012	2012 Eighth International Conference on Signal Image Technology and Internet Based Systems	10.1109/SITIS.2012.39	gray code;computer vision;telecommunications;computer science;theoretical computer science	Robotics	43.64965054636431	-16.520817332276934	184090
17b2dc86ce112e24858f32f9f93e188992afb39d	vector quantization clustering using lattice growing search	psnr;lattices;training;clustering algorithms training merging vector quantization hypercubes lattices psnr;vector quantization;merging;clustering algorithms;hypercubes	In this paper we introduce a non-iterative algorithm for vector quantization clustering based on the efficient search for the two clusters whose merging gives the minimum distortion increase. The search is performed within the A'-dimensional cells of a lattice having a generating matrix that changes from one step of the algorithm to another. The generating matrix is modified gradually so that the lattice cells grow in volume, allowing the search of the two closest clusters in an enlarged neighborhood. We call this algorithm Lattice Growing Search (LGS) clustering. Preliminary results on 512 × 512 images encoded at 0.5 bits/pixel showed that the LGS technique can produce codebooks of similar quality in less than 1/10 of the time required by the LBG algorithm [9].	cluster analysis;codebook;distortion;iterative method;linde–buzo–gray algorithm;linear code;location-based game;pixel;vector quantization	Dorin Comaniciu;Cristina Comaniciu	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		correlation clustering;mathematical optimization;learning vector quantization;theoretical computer science;canopy clustering algorithm;machine learning;mathematics;linde–buzo–gray algorithm;vector quantization	Vision	43.98644766879739	-13.067216096832167	184119
c0a5bb7858579b1e1cf85ced17bc7ff1b7943b5b	coding distortion elimination of virtual view synthesis for 3d video system: theoretical analyses and implementation	three dimensional television;wiener filters;video coding;coding bit rate coding distortion elimination virtual view synthesis 3d video system texture videos depth maps wiener filter coefficients 3dv server 3dv terminal;three dimensional displays encoding wiener filter noise measurement virtualization transform coding;wiener filters three dimensional television video coding;wiener filter 3dv video coding virtual view synthesis	For a three dimensional video (3DV) system, coding distortion of texture videos and depth maps could be propagated to virtual views during the view synthesis procedure. In this paper, the coding distortion is considered in the virtual view synthesis. In order to reduce the coding distortion as well as improve the qualities of synthesized virtual views, Wiener filter is utilized in 3DV systems. To ensure that Wiener filter could be implemented on virtual views, the principles of virtual view synthesis and Wiener filter are presented and analyzed in detail. Subsequently, the implementation of Wiener filter is designed for 3DV system. In the proposed method, Wiener filter coefficients of each frame are calculated in the 3DV server, and then transmitted to the 3DV terminal. By employing those Wiener filter coefficients, post filtering on the virtual views could be implemented after the virtual view synthesis procedure. Experimental results demonstrate that a maximum 0.742 dB PSNR gain could be achieved when comparing the proposed method (taking the coding bits of depth maps, texture videos, and that of Wiener filter coefficients into consideration) with the virtual view synthesis method without Wiener filter at the same coding bit rate of a 3DV system.	algorithm;autocorrelation;coefficient;depth map;dirac delta function;distortion;peak signal-to-noise ratio;pixel;server (computing);stationary process;view synthesis;wiener filter;zcam	Hui Yuan;Ju Liu;Hongji Xu;Zhibin Li;Wei Liu	2012	IEEE Transactions on Broadcasting	10.1109/TBC.2012.2187612	computer vision;computer science;theoretical computer science;multimedia	Visualization	44.73440480732975	-17.89597107201711	184610
1a8a2f6ddb6a1f1f434c223a069c6c3d3af17752	empirical study on performance comparisons of block-based motion estimation on multi sub-pixel displacement with multiples block size	psnr block based motion estimation spd;diamond search;hs;empirical study;interpolation;novel four step search algorithm;psnr motion estimation containers interpolation accuracy diamond like carbon image reconstruction;full search;bbgds;psnr;search algorithm;performance comparison;motion estimation;nfss;accuracy;psnr performance comparisons block based motion estimation multi sub pixel displacement multiples block size novel four step search algorithm nfss block based gradient descent search algorithm bbgds diamond search algorithm hexagon search algorithm hs peak signal to noise ratio;gradient descent;search problems gradient methods motion estimation;image reconstruction;peak signal to noise ratio;multi sub pixel displacement;diamond search algorithm;gradient methods;spd;block based gradient descent search algorithm;multiples block size;search problems;hexagon search algorithm;diamond like carbon;performance comparisons;containers;level 1;block based motion estimation	This paper presents the impact of sub-pixel displacement and block/windows size on the motion estimation performance (3 different levels (1, 0.5, and 0.25) for sub-pixel displacement (SPD) and 2 different block/windows size (8×8/16×16 and 16×16/32×32)). Our empirical study concentrates on full search (FS), a novel four-step search algorithm (NFSS), a block-based gradient descent search algorithm (BBGDS), a new diamond search algorithm (DS) and hexagon search algorithm (HS). Peak Signal to Noise Ratio (PSNR) is referenced as the indicator on our performance comparison results. These experiment results are comprehensively tested and conclude on several standard sequences such as AKIYO, COASTGUARD, CONTAINER, and FOREMAN that have different foreground and background movement characteristic.	block size (cryptography);displacement mapping;gradient descent;microsoft windows;motion estimation;peak signal-to-noise ratio;pixel;search algorithm	Darun Kesrarat;Vorapoj Patanavijit	2012	2012 26th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2012.44	computer vision;mathematical optimization;peak signal-to-noise ratio;computer science;statistics	Robotics	48.58600268826971	-19.119746048361456	184795
4d893a319497efc70e257c877b034f678a2ec576	integer computation of lossy jpeg2000 compression	quantization;image coding;psnr;data compression;integer computation;transform coding;wavelet transforms transform coding quantization image coding engines psnr;cohen daubechies feauvea cdf 9 7 wavelet;quantisation signal;wavelet transforms;wavelet transform;engines;integer programming;wavelet transforms block codes data compression image coding integer programming quantisation signal;jpeg2000;equivalent rate distortion curve integer computation lossy compression jpeg2000 cohen daubechies feauvea 9 7 wavelet transform integer quantization method integer transform wavelet filter embedded block coding;block codes;jpeg2000 cohen daubechies feauvea cdf 9 7 wavelet integer computation	In this paper, an integer-based Cohen-Daubechies-Feauvea (CDF) 9/7 wavelet transform as well as an integer quantization method used in a lossy JPEG2000 compression engine is presented. The conjunction of both an integer transform and quantization step allows for a complete integer computation of lossy JPEG2000 compression. The lossy method of compression utilizes the CDF 9/7 wavelet filter, which transforms integer input pixel values into floating-point wavelet coefficients that are then quantized back into integers and finally compressed by the embedded block coding with optimal truncation tier-1 encoder. Integer computation of JPEG2000 allows a reduction in computational complexity of the wavelet transform as well as ease of implementation in embedded systems for higher computational performance. The results of the integer computation show an equivalent rate/distortion curve to the JasPer JPEG2000 compression engine, as well as a 30% reduction in computation time of the wavelet transform and a 56% reduction in computation time of the quantization processing on an average.	coefficient;cohen syndrome;computation (action);computational complexity theory;distortion;embedded system;encoder device component;float;integer (number);jpeg 2000;jasper;lossy compression;pixel;protein truncation abnormality;reference implementation;time complexity;wavelet transform	Eric J. Balster;Benjamin T. Fortener;William F. Turri	2011	IEEE Transactions on Image Processing	10.1109/TIP.2011.2114353	mathematical optimization;discrete mathematics;integer programming;theoretical computer science;mathematics;stationary wavelet transform;discrete wavelet transform;statistics;wavelet transform	Graphics	43.12913395085285	-15.881798133933522	185012
de09b1ea42ec7d4ecc6160769eab679733f76d9a	motion estimation without integer-pel search	optimisation;interpolation;video coding integer pel search motion compensation motion estimation motion vector prediction;motion vector prediction;motion compensation;integer pel search;motion estimation;adaptive macroblock mode motion estimation spatial temporal prediction fractional pel search video encoding video sequences high resolution videos boundary blocks fractional pel interpolation rate distortion optimization;video coding;期刊论文;video coding image sequences interpolation motion estimation optimisation;motion estimation accuracy video sequences spatial resolution rate distortion vectors interpolation;image sequences	The typical motion estimation (ME) consists of three main steps, including spatial-temporal prediction, integer-pel search, and fractional-pel search. The integer-pel search, which seeks the best matched integer-pel position within a search window, is considered to be crucial for video encoding. It occupies over 50% of the overall encoding time (when adopting the full search scheme) for software encoders, and introduces remarkable area cost, memory traffic, and power consumption to hardware encoders. In this paper, we find that video sequences (especially high-resolution videos) can often be encoded effectively and efficiently even without integer-pel search. Such counter-intuitive phenomenon is not only because that spatial-temporal prediction and fractional-pel search are accurate enough for the ME of many blocks. In fact, we observe that when the predicted motion vector is biased from the optimal motion vector (mainly for boundary blocks of irregularly moving objects), it is also hard for integer-pel search to reduce the final rate-distortion cost: the deviation of reference position could be alleviated with the fractional-pel interpolation and rate-distortion optimization techniques (e.g., adaptive macroblock mode). Considering the decreasing proportion of boundary blocks caused by the increasing resolution of videos, integer-pel search may be rather cost-ineffective in the era of high-resolution. Experimental results on 36 typical sequences of different resolutions encoded with x264, which is a widely-used video encoder, comply with our analysis well. For 1080p sequences, removing the integer-pel search saves 57.9% of the overall H.264 encoding time on average (compared to the original x264 with full integer-pel search using default parameters), while the resultant performance loss is negligible: the bit-rate is increased by only 0.18%, while the peak signal-to-noise ratio is decreased by only 0.01 dB per frame averagely.	acute erythroblastic leukemia;data compression;default;distortion;encoder device component;h.264/mpeg-4 avc;image resolution;integer (number);interpolation imputation technique;macroblock;mathematical optimization;motion estimation;peak signal-to-noise ratio;physical object;pixel;primary effusion lymphoma;rate–distortion optimization;resultant;videocassette;x264	Ling Fei Li;Shaoli Liu;Yunji Chen;Tianshi Chen;Tao Luo	2013	IEEE Transactions on Image Processing	10.1109/TIP.2012.2228495	computer vision;mathematical optimization;interpolation;quarter-pixel motion;computer science;motion estimation;mathematics;block-matching algorithm;rate–distortion optimization;motion compensation;algorithm;computer graphics (images)	Vision	47.67247131292146	-18.992927117864806	185275
7cb31b4d64699bc95bd9cb5799e38ecfccbdc817	fast inter-mode decision based on rate-distortion cost characteristics	rate distortion;h 264 avc;video coding;peak signal to noise ratio;quantization parameter;fast mode decision;rate distortion cost;mode decision	In this paper, a new fast mode decision (FMD) algorithm is proposed for the state-of-the-art video coding standard H.264/AVC. Firstly, based on Rate-Distortion (RD) cost characteristics, all inter modes are classified into two groups, one is Skip mode (including both Skip and Direct modes) and all the other inter modes are called non-Skip modes. In order to select the best mode for coding a Macroblock (MB), minimum RD costs of these two mode groups are predicted respectively. Then for Skip mode, an early Skip mode detection scheme is proposed; for non-Skip modes, a three-stage scheme is developed to speed up the mode decision process. Experimental results demonstrate that the proposed algorithm has good robustness in coding efficiency with different Quantization parameters (Qps) and various video sequences and is able to achieve about 54% time saving on average while with negligible degradation in Peak-Signal-to-Noise-Ratio (PSNR) and acceptable increase in bit rate.	algorithm;algorithmic efficiency;data compression;distortion;elegant degradation;h.264/mpeg-4 avc;macroblock;peak signal-to-noise ratio;rate–distortion theory;ruby document format;simulation;video coding format	Sudeng Hu;Tiesong Zhao;Hanli Wang;Sam Kwong	2010		10.1007/978-3-642-15696-0_14	computer vision;real-time computing;peak signal-to-noise ratio;telecommunications;computer science;statistics	AI	46.95459870296467	-19.166944644722758	185283
fb39a4687555c51f72537c606d9ce2b40540e39f	fast fractal image encoder using non-overlapped block classification and simplified isometry testing scheme	neural network	This paper aims at reducing the encoding time of a fractal image encoder. For this purpose, a non-overlapped block classification method and a simplified isometry testing scheme are proposed. The non-overlapped block classification method avoids the repeated classification operations needed for finding the domain blocks having the same type with the range block, by memorizing the classification results of the domain blocks and using them for the overlapped blocks in a new searching area. For reducing the time required for calculating a similarity between blocks, a simplified isometry testing scheme is used. It tests the isometry between a domain block and a range block using only those types of isometry having the similar features with the type of the range block. For speeding up the calculation time, the SOFM neural network is used as the block classifier and the spiral searching scheme is used. The experimental results have shown that the proposed algorithm reduces the encoding time by 50% on average while maintaining the same PSNR and bit rate, compared to the other's recent approaches.	encoder;fractal	Youngjoon Han;Hawik Chung;Hernsoo Hahn	2004		10.1007/978-3-540-30543-9_27	combinatorics;discrete mathematics;topology;computer science;machine learning;mathematics	Vision	42.78441266982623	-13.756186442794338	185365
b4aa3ef428568cabc10fbef99b4b27f4f266e472	robust watermark extraction using svd-based dynamic stochastic resonance	digital watermarking;stochastic resonance;watermarking;dynamic stochastic resonance;image coding;robust watermarking;digital watermark;singular value decomposition;stochastic processes discrete cosine transforms feature extraction image coding image enhancement image watermarking singular value decomposition;visual quality;image enhancement;stochastic processes;discrete cosine transforms;feature extraction;noise dynamic stochastic resonance digital watermarking singular value decomposition;transforms;robustness;image watermarking;watermarking noise correlation robustness stochastic resonance image coding transforms;correlation;hybrid dct svd based technique robust watermark extraction algorithm svd based dynamic stochastic resonance nonblind watermark extraction technique grayscale watermark extraction singular value decomposition dsr based extraction algorithm visual quality watermarked image;noise	In this paper, a novel dynamic stochastic resonance (DSR)-based non-blind watermark extraction technique has been proposed for robust extraction of a grayscale watermark. The watermark embedding has been carried out using singular value decomposition (SVD). Dynamic stochastic resonance has been strategically used to improve the robustness of the extraction algorithm by utilizing the noise added during attacks itself. Resilience of this technique to attacks has been tested in the presence of various noise, geometrical, enhancement, compression and filtering attacks. Using the DSR-based proposed extraction algorithm, a very robust extraction of watermark can be done without trading-off with visual quality of the watermarked image. Performance of the proposed technique has also been compared with the plain SVD-based and hybrid DCT-SVD based technique and is found to give better performance.	ansi escape code;algorithm;digital watermarking;discrete cosine transform;grayscale;image noise;singular value decomposition;stochastic resonance	Rajlaxmi Chouhan;Rajib Kumar Jha;Apoorv Chaturvedi;Toshihiko Yamasaki;Kiyoharu Aizawa	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116238	stochastic process;computer vision;speech recognition;digital watermarking;computer science;theoretical computer science;mathematics	EDA	41.53623297152808	-10.556216649078983	185643
3d6d2eccc9714037ee928bfbeec6348f0e48bf95	wavelet -based geometry coding for 3d mesh using space frequency quantization	3d semi regular meshes compression;quantization;rate distortion;image coding;data compression;mesh reconstruction;three dimensional displays geometry image coding encoding quantization rate distortion prediction algorithms;geometry;computational geometry;prediction algorithms;virtual reality;visual quality;three dimensional;3d semi regular meshes coding;quantisation signal;wavelet transforms;data storage;visualization;wavelet based geometry coding;wavelet transform;three dimensional displays;distortion error minimisation;wavelet transforms computational geometry data compression encoding mesh generation quantisation signal;animations;mesh reconstruction wavelet based geometry coding space frequency quantization animations virtual reality visualization 3d semi regular meshes compression 3d semi regular meshes coding wavelet transform distortion error minimisation;mesh generation;encoding;space frequency quantization	3D objects are used in several applications such as animations, virtual reality and visualization. These applications often require a huge amount of data storage and transmission. Since most applications require compact storage, fast transmission, in this paper, we present a novel procedure for compression and coding of three-dimensional (3-D) semi-regular meshes using wavelet transform. This procedure is based on space frequency quantization (SFQ) which is used to minimize distortion error of the reconstructed mesh for a different bit-rate constraint. Our experiments show that the 3D SFQ code over performs progressive geometry coder (PGC) in terms of the quality of compressed meshes. Moreover, bit-stream can be truncated at any point and still decode reasonable visual quality meshes.	bitstream;computer data storage;distortion;experiment;network scheduler;professional graphics controller;semiconductor industry;virtual reality;wavelet transform	S. T. EL-Leithy;Walaa M. Sheta	2008	2008 IEEE Symposium on Computers and Communications	10.1109/ISCC.2008.4625754	computer vision;computational geometry;computer science;theoretical computer science;virtual reality;wavelet transform	Visualization	43.282570821126264	-18.73118164816225	185769
0b64888c09fe505868dcaea42ef17ba67a4bfc79	mpeg-4: a multimedia standard for the third millennium. 1	digital satellite television broadcasting mpeg 4 multimedia standard multimedia system interoperable communication complex scenes audio coding video coding synthetic audio graphics;multimedia system specification mpeg 4 standard multimedia standard iso iec standard 14496 interoperable communication complex scenes moving pictures expert group video material synthetic audio material graphics material;computer graphics;iso standards;direct broadcasting by satellite code standards multimedia systems audio coding video coding computer graphics television broadcasting;audio video;code standards;television broadcasting;multimedia systems;mpeg 4 standard layout streaming media decoding multimedia systems graphics mpeg standards satellite broadcasting iso standards tv;direct broadcasting by satellite;video coding;audio coding;iec standards;iec standards multimedia systems code standards video coding open systems iso standards;mpeg 4 standard satellite broadcasting multimedia systems layout graphics multimedia communication digital video broadcasting digital audio broadcasting tv broadcasting digital multimedia broadcasting;open systems;internal standard	MPEG-4 defines a multimedia system for interoperable communication of complex scenes containing audio, video, synthetic audio, and graphics material. In part 1 of this two-part article (Battista et al., 1999) we provided a comprehensive overview of the technical elements. In part 2 we describe an application scenario based on digital satellite television broadcasting, discuss the standard's envisaged evolution, and compare it to other activities in forums addressing multimedia specifications.		Stefano Battista;Franco Casalino;Claudio Lande	1999	IEEE MultiMedia	10.1109/93.809236	embedded system;telecommunications;computer science;internal standard;multimedia;open system;computer graphics	Vision	42.77802087903492	-21.374362350560798	185899
61407b976f23c61c1198b58e15d9c916fd3c58e8	a watermark algorithm of binary images using adaptable matrix	watermarking;adaptable matrix;image coding;binary image;authentication;nuclear magnetic resonance;watermarking image coding;data mining;watermarking signal processing intelligent control information processing videos protection data encapsulation humans tv visual perception;image connectivity watermark algorithm binary images adaptable matrix image smoothness;binary image watermark adaptable matrix;pixel;watermark;signal processing algorithms;algorithm design and analysis	A watermark algorithm of binary images using adaptable matrix is presented. An adaptable matrix is designed to evaluate the smoothness and the connectivity of binary images. The watermark is embedded according to the adaptable matrix in this algorithm. Experimental results show that proposed scheme has a performance.	algorithm;binary image;embedded system	Fan Zhang;Junliang Zhang	2008	2008 International Conference on Cyberworlds	10.1109/CW.2008.51	algorithm design;computer vision;binary image;digital watermarking;computer science;theoretical computer science;authentication;multimedia;watermark;pixel	Robotics	41.767317303116066	-11.40377154910991	186289
58c13530d6775f362e7b7c209d0e4af8b1b896d5	near lossless image compression by local packing of histogram	compression algorithm;image coding histograms pixel image reconstruction entropy coding computer errors image converters compression algorithms mobile communication internet;image coding;data compression;image histogram near lossless compression lossless compression;minimum entropy methods;local histogram packing;lossless image compression;lossless compression;minimum entropy;low complexity;spatial correlation;image reconstruction;near lossless compression;pixelwise error tolerance;near lossless image compression;image histogram;spatial correlation near lossless image compression local histogram packing image reconstruction pixelwise error tolerance minimum entropy;minimum entropy methods data compression image coding image reconstruction	In this paper a low complexity algorithm is proposed for near lossless compression of images. The reconstructed near lossless image can differ from the original one within a pixelwise error tolerance. This property is used to convert the histogram of the original image, by the proposed algorithm, to a new histogram which is proved to have minimum entropy. Hence, a new image is formed which has minimum entropy and high spatial correlation among its pixels and can efficiently be compressed. Simulation results show the effectiveness of this compression algorithm.	algorithm;entropy (information theory);error-tolerant design;image compression;lossless compression;pixel;set packing;simulation	Ebrahim Nasr-Esfahani;S Abdolvahab Samavi;Nader Karimi;Shahram Shirani	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517830	data compression;lossy compression;computer vision;data compression ratio;image compression;computer science;histogram matching;theoretical computer science;pattern recognition;mathematics;lossless compression;statistics;image histogram	Robotics	44.23488295857232	-13.79721638433284	186470
837fe04dae51df50c90d9e26272c14bf7565d64a	correlation estimation with particle-based belief propagation for distributed video coding	video sequence;distributed video coding scheme;correlation statistics;decoding;distributed source coding designs;source information;bit plane coding;belief propagation distributed source and video coding correlation estimation particle filtering;slepian wolf decoding;video coding correlation methods decoding image sequences source coding statistical analysis;correlation methods;video coding;decision support system;statistical analysis;distributed source;belief propagation;particle filter;decision support systems;particle based belief propagation;particle filtering;distributed source and video coding;distributed video coding;zinc;correlation estimation;zinc decision support systems;online correlation estimation;side information;particle based bp tracking;particle based bp tracking particle based belief propagation distributed video coding scheme video sequence slepian wolf decoding distributed source coding designs particle filtering standard bp decoding online correlation estimation source information side information correlation statistics bit plane coding;electrical engineering electronics nuclear engineering;source coding;standard bp decoding;image sequences	In this paper, we propose an adaptive Distributed Video Coding (DVC) scheme that dynamically estimates correlation statistics of the scene in a video sequence to enhance belief-propagation (BP) Slepian-Wolf (SW) decoding. In order to exploit the robustness of distributed source coding (DSC) designs, we integrate particle filtering with standard BP decoding in one factor graph to estimate online correlation among source and side information. Our proposed system boasts improved performance over classical DVC without correlation estimation, due to improved knowledge of correlation statistics via the combination of bit-plane coding and particle-based BP tracking in each frame, as shown by our results.	belief propagation;bit plane;data compression;distributed source coding;factor graph;particle filter;slepian–wolf coding;software propagation	Lina Stankovi&#x0107;;Vladimir Stankovic;Shuang Wang;Samuel Cheng	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946779	decision support system;particle filter;telecommunications;computer science;theoretical computer science;coding tree unit;pattern recognition;statistics	Vision	49.42788509181227	-16.980052514382294	186502
44d6c3ed34c56b5820eac3f5bedd399ad3864469	a new steganographic method for color and grayscale image hiding	cuantificacion senal;anotacion;quantization;vision ordenador;steganographie;stego image;annotation;qualite image;gray scale;journal article;computer vision;algorithm;steganography;esteganografia;signal quantization;lsb substitution;jpeg;image quality;quantification signal;vision ordinateur;calidad imagen;imagen color;echelle gris;color image hiding;color quantization;image couleur;escala gris;color image	In this paper, we present a steganographic method for embedding a color or a grayscale image in a true color image. Three types of secret images can be carried by the proposed method: hiding a color secret image, hiding a palette-based 256-color secret image, and hiding a grayscale image in a true color image. Secret data are protected by the conventional crypto system DES. We compare the image quality and hiding capacity of the proposed method with those of the scheme in Lin et al.’s scheme. According to the experimental results, the image quality of the proposed method is better than that of the Lin et al.’s scheme. In addition, annotation data can be hidden with the secret image in the host image. The hiding capacity of the proposed method is greater than that of other compared schemes. The experimental results show that the proposed method is a secure steganographic method that provides high hiding capacity and good image quality. 2006 Elsevier Inc. All rights reserved.	algorithm;color depth;color image;color quantization;cryptosystem;data compression;embedded system;encryption;grayscale;image quality;jpeg;palette (computing);peak signal-to-noise ratio;steganography;visual instruction set	Yuan-Hui Yu;Chin-Chen Chang;Iuon-Chang Lin	2007	Computer Vision and Image Understanding	10.1016/j.cviu.2006.11.002	image quality;computer vision;color quantization;color image;quantization;binary image;computer science;theoretical computer science;jpeg;mathematics;steganography;grayscale;computer graphics (images)	Vision	40.502624580422214	-11.400039504714636	186507
9aa1ff349311fd93fe77be87009b9bf3d71eb663	optimal bit-rate allocation and synthesis filter bank design for multirate subband coding systems	minimisation;quantization;multirate subband coding systems;optimisation;filter bank;signal sampling;filter bank finite impulse response filter quantization design methodology signal synthesis sampling methods design optimization signal analysis channel bank filters signal reconstruction;signal analysis;performance;finite impulse response filter;design optimization;digital filters optimisation signal reconstruction minimisation iterative methods encoding channel bank filters signal sampling;iterative methods;channel bank filters;rate allocation;digital filters;reconstruction error;optimal design;signal reconstruction;optimal bit rate allocation;signal synthesis;sampling methods;encoding;synthesis filter bank design;subband coding;iterative approach;performance optimal bit rate allocation synthesis filter bank design multirate subband coding systems subband quantizers reconstruction error variance minimization iterative approach;subband quantizers;variance minimization;design methodology	In this paper, a joint optimal design method for multirate subband coding systems is presented. For a multirate subband coding system with given analysis filters and average bit number of data quantizers, the design is to simultaneously find the synthesis filters and bit number allocation of the subband quantizers such that the variance of the reconstruction error of the system is minimized. We propose an iterative approach for obtaining the synthesis filters and the bit number allocation that minimize the reconstruction error of the system. Our simulation example demonstrate the favorable performance of the proposed method as compared with existing methods.	filter bank;iterative method;optimal design;simulation;sub-band coding	Huan Zhou;Lihua Xie;Cishen Zhang	2003		10.1109/ICASSP.2003.1201629	signal reconstruction;sub-band coding;sampling;computer vision;minimisation;mathematical optimization;multidisciplinary design optimization;digital filter;quantization;design methods;performance;computer science;optimal design;finite impulse response;signal processing;filter bank;control theory;mathematics;iterative method;encoding;statistics	EDA	47.5253343833748	-12.583478484333233	186577
289122d1eeb88b60c761334ded0c94104e50e1a7	low-bitrate video quality enhancement by frame rate up-conversion and adaptive frame encoding	desciframiento;metodo adaptativo;estimation mouvement;image coding;motion compensated frame interpolation;multimedia;image processing;adaptive frame skip;decodage;decoding;factor conversion;estimacion movimiento;procesamiento imagen;video quality;motion estimation;methode adaptative;traitement image;motion compensated;codage image;video coding;codage video;linear interpolation;adaptive method;conversion rate;interpolation lineaire;frame rate up conversion;taux conversion;interpolacion lineal	Frame rate up-conversion (FRUC) is a useful technique for a lot of practical applications, such as display format conversion, low bitrate video coding and slow motion playback. Unlike traditional approaches, such as frame repetition or linear frame interpolation, motion-compensated frame interpolation (MCFI) technique which takes block motion into account is regarded as a more efficient scheme. By considering the deficiencies in previous works, new criteria and coding schemes for improving motion derivation and interpolation processes are suggested. We then integrate the proposed MCFI scheme into the decoding process of the latest coding standard, H.264/AVC. In addition, adaptive frame skip is fulfilled at the encoder side to maximize the power of MCFI in video coding applications. As a result, the encoder can adopt the MCFI dynamically and can decide whether the input frame should be coded or dropped then interpolated. Experimental results show that our proposal indeed enhances the overall quality, both subjectively and objectively, especially for the low bitrate video coding.	data compression;encoder;frame language;h.264/mpeg-4 avc;motion interpolation;video post-processing	Ya-Ting Yang;Yi-Shin Tung;Ja-Ling Wu;Chung-Yi Weng	2005		10.1007/11582267_73	residual frame;computer vision;telecommunications;image processing;computer science;video quality;motion estimation;mathematics;linear interpolation;conversion marketing;computer graphics (images)	Graphics	46.51327749810215	-16.245327047017692	186847
c5a422b4c3f3c41779b4d2c0a734ab43041c1ed0	mesh-based video coding for low bit-rate communications	image quality mesh based video coding low bit rate communications intraframe coding feature map extraction node distribution hilbert scan entropy coding transmitted information reduction color parameters node position;multimedia communication video coding video communication feature extraction image colour analysis;entropy coding;indexing terms;video coding;research paper;image colour analysis;feature extraction;image quality;multimedia communication;content adaptation;video communication;video coding image coding video compression videoconference image reconstruction motion estimation mesh generation multimedia systems dvd digital video broadcasting;mesh generation;high frequency	In this paper, a new method for low bit-rate content-adaptive mesh-based video coding is proposed. Intra-frame coding of this method employs feature map extraction for node distribution at specific threshold levels to achieve higher density placement of initial nodes for regions that contain high frequency features and conversely sparse placement of initial nodes for smooth regions. Insignificant nodes are largely removed using a subsequent node elimination scheme. The Hilbert scan is then applied before quantization and entropy coding to reduce amount of transmitted information. For moving images, both node position and color parameters of only a subset of nodes may change from frame to frame. It is sufficient to transmit only these changed parameters. The proposed method is well-suited for video coding at very low bit rates, as processing results demonstrate that it provides good subjective and objective image quality at a lower number of required bits.	artifact (software development);blocking (computing);brute-force search;claire;color;computation;data compression;discrete cosine transform;distortion;encoder;entropy encoding;h.264/mpeg-4 avc;image quality;interpolation;intra-frame coding;iteration;mobile phone;peak signal-to-noise ratio;polygon mesh;real-time clock;real-time computing;ringing (signal);sparse matrix;wavelet	Preecha Kocharoen;Kazi M. Ahmed;R. M. A. P. Rajatheva;Warnakulasuriya Anil Chandana Fernando	2006	IEEE Transactions on Consumer Electronics	10.1109/TCE.2006.1649687	video compression picture types;scalable video coding;image quality;sub-band coding;mesh generation;computer vision;index term;telecommunications;feature extraction;computer science;entropy encoding;context-adaptive variable-length coding;video tracking;high frequency;coding tree unit;multimedia;video processing;smacker video;context-adaptive binary arithmetic coding;motion compensation;h.261;multiview video coding	Vision	45.31530758340742	-16.411929387192345	187058
9f2a7812cca154d130480dbdf7cfdd3b9b0291c1	automatic development of robot behaviour using monte carlo methods	optimal solution;robot navigation;general methods;monte carlo method;markov decision process;behavioural problems	Designing behaviour-based control mechanisms is a notoriously difficult task. The problem for an individual behaviour can often be simply stated, but there are typically a myriad of possible approaches. Also, the efficiency of a behaviour is dependent on characteristics of the environment that may be difficult for the designer to take account of. Therefore, it would seem useful to be able to optimise a behaviour automatically. Of the available techniques for optimisation, Markov Decision Process (MDP) based methods are particularly applicable because they allow for non-determinism in the effect of actions. In this paper, we phrase the behavioural problem as a Markov Decision Process (MDP) and find an approximately optimal solution for a simple robot navigation problem. We use this as a basis for demonstrating a general method for improving behaviours automatically.	approximation algorithm;bang file;bang–bang control;control system;machine learning;markov chain;markov decision process;mathematical optimization;maximal set;mobile robot;monte carlo method;motion planning;online and offline;optimal control;reinforcement learning;robot;robotic mapping;simulation;steering wheel	James Brusey	2000		10.1007/3-540-44533-1_95	markov decision process;simulation;computer science;artificial intelligence;machine learning;statistics;monte carlo method	Robotics	52.271131999693715	-21.76722263418002	187069
135a54c90352e59700f482987274e7289db234cf	extended multiresolution lossless video coding using in-band spatio-temporal prediction	tecnologia electronica telecomunicaciones;lossless compression;video coding;wavelet transform;progressive transmission;prediction coding;tecnologias;grupo a;super high definition images	Lossless video coding is required in the fields of archiving and editing digital cinema or digital broadcasting contents. This paper combines a discrete wavelet transform and adaptive inter/intra-frame prediction in the wavelet transform domain to create multiresolution lossless video coding. Based on the image statistics of the wavelet transform domains in successive frames, inter/intra frame adaptive prediction is applied to the appropriate wavelet transform domain. This adaptation offers superior compression performance. A progressive transmission scheme is also proposed for effective resolution scalability. Experiments on test sequences confirm the effectiveness of the proposed algorithm.	lossless compression	Takayuki Nakachi;Tomoko Sawabe;Tetsuro Fujii	2006	IEICE Transactions	10.1093/ietfec/e89-a.3.698	data compression;wavelet;lossless jpeg;computer vision;second-generation wavelet transform;telecommunications;computer science;context-adaptive variable-length coding;mathematics;lossless compression;tunstall coding;wavelet packet decomposition;stationary wavelet transform;context-adaptive binary arithmetic coding;discrete wavelet transform;fast wavelet transform;lifting scheme;wavelet transform;computer graphics (images)	Vision	45.21852830198469	-17.988596315144225	187074
3e24e1587ed6a45274c5615f069e488f670d7575	digital image watermarking with partial embedding on blue color component	watermarking;psnr;color;extracted watermark image watermarking partial embedding;interference;image watermarking feature extraction image colour analysis;manganese;watermarking psnr decision support systems manganese handheld computers interference color;decision support systems;embedding process digital image watermarking blue color component partial embedding approach watermark bits watermark embedding position selection watermark extraction;handheld computers	This paper presents an image watermarking method where partial embedding approach is implemented in order to avoid interference from other nearby watermark bits. With cautious selection of watermark embedding positions, the maximum number of embedding bits can also be achieved. Precisely, the embedding positions are pseudo-randomly selected from the blue component of the host color image in accordance with a secret key, while the watermark extraction is blindly achieved by using pixels around the embedding bit in the same position and sequence used in the embedding process. The experimental results verify the better performance in term of accuracy of the extracted watermark compared to the previous related work [3].	color image;color space;digital image;digital watermarking;interference (communication);key (cryptography);pixel;randomness	Kharittha Thongkor;Thumrongrat Amornraksa	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041667	computer vision;theoretical computer science;mathematics;multimedia;watermark	Vision	40.01901391464467	-11.255491930449157	187206
9b971a7b2f75c1c7bb89f084feda374cea9ec8cd	fast intra coding algorithm for hevc based on depth range prediction and mode reduction		The latest High Efficiency Video Coding (HEVC) standard only requires 50% bitrate of the H.264/AVC at the same video quality. Due to introduction of more advanced coding tools and techniques, its computational complexity rises rapidly. In this paper, we propose a novel fast Intra coding algorithm. Firstly, a novel feature is proposed to measure the complexity of video content based on the analysis of mode information obtained from a previous frame. Then, a model is built based on the relationship between this feature and Coding Tree Unit (CTU) depth range. According to the model, the unnecessary operations of Coding Unit (CU) split are skipped. Secondly, the correlation between CU distribution structure and mode selection is established. For the prediction of unlikely CU, the last few intra modes in candidate list are eliminated before rate-distortion optimization process. Experimental results demonstrate that the proposed algorithm reduces the encoding time by 43.2% with just 0.47% BD-rate increase compared with HEVC test model. Compared with other state-of-art algorithms, the proposed algorithm achieves better trade-off between complexity reduction and RD performance.	algorithm;algorithmic efficiency;blu-ray;coding tree unit;computational complexity theory;digital video;distortion;h.264/mpeg-4 avc;high efficiency video coding;huffman coding;intra-frame coding;mathematical optimization;rate–distortion optimization;reduction (complexity);remote data objects;ruby document format;tip (unix utility)	Fen Chen;Defu Jin;Zongju Peng;Gangyi Jiang;Mei Yu;Hua Chen	2018	Multimedia Tools and Applications	10.1007/s11042-018-6011-8	computer science;reduction (complexity);coding (social sciences);artificial intelligence;computational complexity theory;video quality;pattern recognition;algorithm;coding tree unit	AI	46.73783456895361	-19.456380380780892	187231
031cc38435648ba77e2eea8bf3e9643c7f1505ef	strategic frequency adaptation for mid-range magnetic induction-based wireless underground sensor networks	wireless sensor networks antennas crops electric properties electromagnetic induction geophysical techniques irrigation leak detection moisture measurement optimisation pipelines soil underground communication;irrigation;sensors;wires;soil coils wires pipelines wireless sensor networks irrigation sensors;mi wusn nodes strategic frequency adaptation midrange magnetic induction wireless underground sensor networks energy subsystem soil moisture sensing system oil pipeline leak detection system pld system magnetic induction technology mi technology wireless underground communication underground mi systems electrical properties mi nodes soil moisture variability dynamic frequency adaptation;coils;pipelines;soil;wireless sensor networks	Some classes of sensing applications target scenarios where the overall system, including antenna and energy sub-system, is placed under the ground, through concrete, or under-the-debris (disaster scenario). A real-time soil moisture sensing system deployed at the root zone of a crop area for precise irrigation and an oil pipeline leak detection (PLD) system are particular examples of interest discussed in this work. Communication solutions for these scenarios have been recently investigated in the Wireless Underground Sensor Networks (WUSN) literature and magnetic induction (MI) technology is usually regarded as the best candidate for low-power and mid-range (i.e., 15..30m) wireless underground communication. Nonetheless, underground MI systems are still significantly impacted by changes at the electrical properties of the medium surrounding the MI nodes, such as due to the soil moisture variability. The design of a MI system for the worst-case scenario (e.g., high soil moisture) is a strategic approach but it may significantly reduce the application bandwidth. Therefore, the dynamic frequency adaptation has the potential of balancing robustness and higher bandwidth for MI-WUSNs. In this work, a novel design procedure is proposed for the optimization problem of selecting the proper operational frequency for MI-WUSN nodes according to the medium conditions.	low-power broadcasting;mathematical optimization;optimization problem;programmable logic device;real-time locating system;spatial variability;underground;worst-case scenario	Agnelo R. Silva;Mahta Moghaddam	2015	2015 Annual IEEE Systems Conference (SysCon) Proceedings	10.1109/SYSCON.2015.7116842	electronic engineering;hydrology;engineering;geotechnical engineering	Mobile	52.576560589419415	-10.57756375727266	187348
9c0375ef40e906f7e609e2a57a08094ff0b70616	view-dependent, scalable texture streaming in 3-d qos with mpeg-4 visual texture coding	estensibilidad;extensible markup language;rendu image;optimisation;image coding;image processing;modelo 3 dimensiones;restitucion imagen;modele 3 dimensions;mpeg 21;texture image;procesamiento imagen;three dimensional model;mpeg 4 standard streaming media bandwidth multimedia computing computer networks energy consumption impedance decoding xml computer displays;multimedia application;traitement image;three dimensional;optimization texture streaming 3d qos quality of service mpeg 4 visual texture coding multimedia resource savings content adaptation xml techniques extensible markup language;qualite service;image texture;codage image;efficient implementation;image transmission;multimedia communication;image rendering;xml;visual texture coding;mpeg 4;content adaptation;audio visual;extensibilite;scalability;power consumption;quality of service;transmission image;optimisation image texture multimedia communication quality of service image coding xml;service quality;transmision imagen;three dimensional 3 d view dependent transmission;calidad servicio	Multimedia applications are characterized by high resource demands (computing power, memory, network bandwidth, and power consumption). Efficient implementations aim at reducing these resources to a minimum, which is of the utmost importance for small, low-cost terminals in low-bandwidth networks. Resource savings can also be obtained by content adaptation without impeding the quality of the decoded audio-visual media. In this context, the paper analyzes texture adaptation and streaming for three-dimensional applications, using MPEG-4's Visual Texture Coding tool, in conjunction with eXtensible Markup Language (XML)-based description techniques. Augmented features for content adaptation are supported, such as region selection, accompanied by resolution and SNR settings. As a result, quality is optimized for the terminal's computing capabilities and display resolution, taking the user's viewing conditions into account. Moreover, the instantaneous bandwidth utilization is highly reduced in streaming scenarios.	bitstream;content adaptation;data dependency;display resolution;markup language;overhead (computing);quality of service;real-time clock;scalability;signal-to-noise ratio;transmitter;wavelet transform;xml	Gauthier Lafruit;Eric Delfosse;Roberto R. Osorio;Wolfgang van Raemdonck;Vissarion Ferentinos	2004	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2004.830674	computer vision;xml;image processing;computer science;theoretical computer science;multimedia	Visualization	42.85231746660761	-17.771834222178008	187393
1a979b412e5b4c938acbcb6c9c71654a99c1809b	fragile watermarking using karhunen-loève transform: the klt-f approach	content authentication;fragile watermarking;genetic algorithms;security;karhunen loeve transform;fragile image watermarking;data security	The paper presents a fragile watermarking technique that may be used to discover image manipulations at block level. The proposed algorithm, based on the Karhunen-Loève linear transform (KLT), modifies some of the KLT coefficients obtained from a secret base (defined by a secret key image) so that they contain a binary pseudo-random message. A genetic algorithm (GA) is used to compensate for rounding errors introduced by inverse transforming in the integer pixel domain. An extensive experimentation has been performed to test the effectiveness of the method and to show the sensitivity of the algorithm to single pixel modifications (also as a function of the number of modified coefficients). A comparison with other fragile watermarking methods is reported. It should be noted that the proposed approach results in both a high PSNR (more than 53 dB on average) and a high subjective quality. The system may be tested online by submitting images to be watermarked and subsequently verifying the presence of modifications in a previously marked image.		Marco Botta;Davide Cavagnino;Victor Pomponiu	2015	Soft Comput.	10.1007/s00500-014-1373-y	computer vision;speech recognition;genetic algorithm;computer science;information security;theoretical computer science;mathematics;data security;karhunen–loève theorem	EDA	39.657185626779814	-10.972817266542725	187635
547dffb7f8d7ba33c6bfb4f9fc083fb6ff901f43	experimental investigation of the shvc scalable video encoder architecture	scalable video coding;shvc	Mobile and portable technologies frequently adopt heterogeneous multimedia networks. In this context it was designed scalable video encoders where a scalable video stream is transmitted containing a base layer (responsible by the minimum video requirements) and additional enhancement layers (each one improving the video characteristics). Recently MPEG and ITU-T groups published the scalable HEVC standard specification. However, the whole implementation of this encoder demands large computational resources and memory size. The current work presents a detailed analysis of this technology and its internal modules, indicating a simplified configuration that reach a tradeoff among low latency time and encoding quality. Comparison results point to a fast and low complexity solution with small video quality loss.	computational resource;encoder;high efficiency video coding;moving picture experts group;requirement;scalability;scalable vector graphics;streaming media	Ronaldo Husemann;Valter Roesler;José Valdeni de Lima	2016		10.1145/2976796.2988172	scalable video coding;real-time computing;h.263;computer science;theoretical computer science;multimedia;multiview video coding	HPC	44.7077247483156	-20.622171824817737	187729
9adfbbf172cde28e1e58dffdd20ac789620ed6a2	adaptive frame prediction for foveation scalable video coding	discrete wavelet transforms;prediction error;scalable video coding;motion compensation;decoding;video compression;motion estimation;bit rate;data mining;temporal information;motion compensated;video coding;redundancy;error propagation;human visual system;multimedia communication;video coding decoding humans discrete wavelet transforms video compression data mining bit rate motion estimation motion compensation redundancy;humans;scalable coding	Embedded rate scalable video coding allows for the extraction of coded visual information at continuously varying bit rates from a single compressed bitstream. This is a very attractive feature for many multimedia communication applications. Motion estimation (ME) /motion compensation (MC) techniques are widely employed in various video coding systems to reduce temporal information redundancy. One of the major challenging problems in ME/MC based rate scalable video coding is how to generate the prediction frame from the previous frame to match the current frame. This problem is more difficult in rate scalable coding than in fixed rate coding because the decoding data rate is unavailable to the encoder. We propose an adaptive frame prediction scheme for foveation scalable video coding (FSVC), which is a new video coding algorithm that combines a foveation-based human visual system (HVS) model with a wavelet-based rate scalable coding algorithm. The new frame prediction algorithm provides an adaptive mechanism to control the prediction errors while reduce error propagation.	algorithm;bitstream;data compression;data rate units;encoder;human visual system model;motion compensation;motion estimation;neural coding;propagation of uncertainty;redundancy (information theory);scalability;scalable video coding;scheme;software propagation;wavelet	Ligang Lu;Zhou Wang;Alan C. Bovik;Jack Kouloheris	2001	IEEE International Conference on Multimedia and Expo, 2001. ICME 2001.	10.1109/ICME.2001.1237818	data compression;scalable video coding;inter frame;sub-band coding;residual frame;computer vision;real-time computing;harmonic vector excitation coding;computer science;propagation of uncertainty;theoretical computer science;mean squared prediction error;coding tree unit;motion estimation;redundancy;context-adaptive binary arithmetic coding;human visual system model;motion compensation;statistics;multiview video coding	Robotics	48.4512440936668	-16.75894674281783	187875
a0868c155f33be6383b1288d23ea5d38025f13c6	a spatio-temporal competing scheme for the rate-distortion optimized selection and coding of motion vectors	rate distortion;video coding data compression error statistics image motion analysis image texture prediction theory rate distortion theory redundancy;information cost;video coding;encoding abstracts subspace constraints complexity theory radio access networks;motion vector;spatial predictor spatiotemporal competing scheme rate distortion optimized selection motion vector coding h 264 standard mpeg4 standard avc standard video coding bitrate reduction high performance texture coding tool temporal predictor redundancy;rate distortion optimization;high performance	The recent H.264/MPEG4-AVC video coding standard has achieved a significant bitrate reduction compared to its predecessors. High performance texture coding tools and 1 over 4-pel motion accuracy have however contributed to an increased proportion of bits dedicated to the motion information. The future ITU-T challenge, to provide a codec with 50% bitrate reduction compared to the current H.264, may result in even more accurate motion models. It is consequently of highest interest to reduce the motion information cost. This paper proposes a competitive framework, with spatial and temporal predictors optimally selected by a rate-distortion criterion taking into account the cost of the motion vector and the predictor information. These new methods take benefit of temporal redundancies in the motion field, where the standard spatial median usually fails. Compared to an H.264/MPEG4-AVC standard codec, a systematic bitrate saving reaching up to 20% for complex sequences is reported.	codec;data compression;distortion;elegant degradation;h.264/mpeg-4 avc;jumbo frame;kerrison predictor;macroblock;motion field;peak signal-to-noise ratio;reference frame (video);video coding format	Guillaume Laroche;Joël Jung;Béatrice Pesquet-Popescu	2006	2006 14th European Signal Processing Conference		average bitrate;computer vision;simulation;quarter-pixel motion;computer science;theoretical computer science;coding tree unit;variable bitrate;constant bitrate;context-adaptive binary arithmetic coding;motion compensation	Vision	46.52623431895046	-18.59403540417945	187916
1927cb6bf9617676bfef8abc2bd32cee945388d9	efficient prediction of cu depth and pu mode for fast hevc encoding using statistical analysis	cu coding unit size decision;pu prediction unit mode decision;statistical analysis;k means clustering algorithm;hevc high efficiency video coding	We propose a fast algorithm with both CU size and PU mode decision method for HEVC.All of the 13 neighboring CTUs are analyzed to predict the current CU depth.k-means method is used to classify the neighboring CTUs into different groups.PU mode candidates are selected according to the CU depth.The proposed method gets 56.71% and 59.76% time reduction with LP and RA profiles. High Efficiency Video Coding (HEVC) adopts complex quadtree-structured CU (coding unit) and PU (prediction unit) modes. Thus, the search process for optimal modes causes high computational complexity in HEVC. To solve this problem, a fast coding algorithm is proposed. Thirteen neighboring CTUs (Coding Tree Unit) are divided into three classes with the k-means method, and are selectively used as the reference CTUs to calculate the predicted CU depth levels for the current CTU. Additionally, for every CU depth, we skip some rarely used PU modes to reduce the complexity of the PU selection algorithm. Experimental results show that compared with the HEVC reference software, our algorithm can achieve 56.71% time saving with Low Delay Configuration profile and 59.76% with Random Access Configuration profile, whereas values of BDBR (Bjontegaard Delta Bit Rate) are only 1.0517% and 0.9918%, respectively. Compared with five state-of-the-art algorithms, the proposed algorithm has significant encoding time reduction with good bitrate performances.	algorithm;coding tree unit;computational complexity theory;encoder;high efficiency video coding;ibm systems network architecture;performance;tip (unix utility);traffic collision avoidance system	Zhaoyi Liu;Ting-Lan Lin;Chi-Chan Chou	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2016.03.025	real-time computing;simulation;computer science;theoretical computer science;machine learning;statistics;k-means clustering	Vision	47.08381931319405	-19.614018885499622	187950
c447f21566bbfdb6c4269fe530fe3994639333c4	spatial prediction in distributed video coding using wyner-ziv dpcm	dc component compression;quantization;decoding encoding quantization correlation video coding gain information filters;rate distortion;wyner ziv;high resolution;image resolution;data compression;video coding approximation theory data compression filtering theory image resolution rate distortion theory;decoding;optimal prediction filters;rate distortion gains;gain;wyner ziv dpcm;spatial prediction;rate distortion theory;approximation theory;video coding;wyner ziv video coding;distributed video coding;correlation;optimal prediction;information filters;encoding;filtering theory;rate distortion gains spatial prediction distributed video coding wyner ziv dpcm high resolution approximation optimal prediction filters dc component compression;high resolution approximation	This paper deals with spatial prediction in Wyner-Ziv video coding. Coding structures that extend DPCM to the distributed coding scenario are presented. Using a high resolution approximation, the optimal prediction filters for these structures are derived. The resulting predictive coder is employed to compress the DC component of a transform based distributed video coding system. Preliminary results show rate-distortion gains compared to the case without spatial prediction.	approximation;data compression;distortion;image resolution	Jayanth Nayak;Gagan Rath;Denis Kubasov;Christine Guillemot	2008	2008 IEEE 10th Workshop on Multimedia Signal Processing	10.1109/MMSP.2008.4665095	computer vision;image resolution;computer science;theoretical computer science;coding tree unit;mathematics;context-adaptive binary arithmetic coding;statistics	ML	48.34933514460936	-12.722950653081256	188085
52a16287f9388a0ac615a61a9875660b80851312	dual watermarking scheme with encryption	image encryption;digital watermark;logistic map;copyright protection	Digital Watermarking is used for copyright protection and authentication. In the proposed system, a Dual Watermarking Scheme based on DWT-SVD with chaos encryption algorithm, will be developed to improve the robustness and protection along with security. DWT and SVD have been used as a mathematical tool to embed watermark in the image. Two watermarks are embedded in the host image. The secondary is embedded into primary watermark and the resultant watermarked image is encrypted using chaos based logistic map. This provides an efficient and secure way for image encryption and transmission. The watermarked image is decrypted and a reliable watermark extraction scheme is developed for the extraction of the primary as well as secondary watermark from the distorted image.	algorithm;authentication;digital watermarking;discrete wavelet transform;embedded system;encryption;logistic map;resultant;singular value decomposition	R. Dhanalakshmi;K. Thaiyalnayaki	2010	CoRR		logistic map;digital watermarking;theoretical computer science;internet privacy;watermark;computer security	EDA	39.719866296711515	-10.650952368109492	188173
fc982a677b637c19efa0c689c3e02e5d388b9193	image compression with adaptive local cosines: a comparative study	metodo adaptativo;image coding psnr bit rate image segmentation laplace equations concrete rate distortion polynomials inspection image reconstruction;evaluation performance;rate distortion;image coding;image segmentation;performance evaluation;image processing;fourier transform;data compression;estudio comparativo;evaluacion prestacion;transformation cosinus discrete;rate distortion theory data compression image reconstruction image segmentation image coding transform coding transforms;smooth localized cosine transforms image compression adaptive local cosines wavelets image coding image segmentation scalar quantizer laplacian distributions optimized bells rate distortion visual image quality polynomials psnr reconstructed images fourier transform low bit rate;procesamiento imagen;lapped transform;methode adaptative;transform coding;indexing terms;visual quality;traitement image;algorithme;rate distortion theory;etude comparative;algorithm;codage image;compression image;scalar quantization;image compression;discrete cosine transforms;image reconstruction;adaptive method;comparative study;transforms;cosine transform;algoritmo;compresion imagen	The goal of this work is twofold. First, we demonstrate that an advantage can be gained by using local cosine bases over wavelets to encode images that contain periodic textures. We designed a coder that outperforms one of the best wavelet coders on a large number of images. The coder finds the optimal segmentation of the image in terms of local cosine bases. The coefficients are encoded using a scalar quantizer optimized for Laplacian distributions. This new coder constitutes the first concrete contribution of the paper. Second, we used our coder to perform an extensive comparison of several optimized bells in terms of rate-distortion and visual quality for a large collection of images. This study provides for the first time a rigorous evaluation in realistic conditions of these bells. Our experiments show that bells that are designed to reproduce exactly polynomials of degree 1 resulted in the worst performance in terms of the PSNR. However, a visual inspection of the compressed images indicates that these bells often provide reconstructed images with very few visual artifacts, even at low bit rates. The bell with the most narrow Fourier transform gave the best results in terms of the PSNR on most images. This bell tends however to create annoying visual artifacts in very smooth regions at low bit rate.	base;coder device component;coefficient;distortion;encode;experiment;gain;image compression;morphologic artifacts;peak signal-to-noise ratio;polynomial;quantization (signal processing);visual artifact;visual inspection;wavelet;biologic segmentation	François G. Meyer	2002	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2002.1014993	data compression;iterative reconstruction;fourier transform;computer vision;discrete mathematics;transform coding;lapped transform;index term;rate–distortion theory;image processing;image compression;computer science;theoretical computer science;comparative research;discrete cosine transform;mathematics;image segmentation;algorithm	Vision	45.678253561166464	-13.76264372132301	188215
