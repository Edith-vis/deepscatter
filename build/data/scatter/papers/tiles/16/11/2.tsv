id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
c680439186775b5a1a77d18dd55de83183895d5e	segmentation of medical images using adaptive region growing	image acquisition	Interaction increases flexibility of segmentation but it leads to undesirable behavior of an algorithm if knowledge being requested is inappropriate. In region growing, this is the case for defining the homogeneity criterion, as its specification depends also on image formation properties that are not known to the user. We developed a region growing algorithm that learns its homogeneity criterion automatically from characteristics of the region to be segmented. The method is based on a model that describes homogeneity and simple shape properties of the region. Parameters of the homogeneity criterion are estimated from sample locations in the region. These locations are selected sequentially in a random walk starting at the seed point, and the homogeneity criterion is updated continuously. This approach was extended to a fully automatic and complete segmentation method by using the pixels with the smallest gradient length in the not yet segmented image region as a seed point. The methods were tested for segmentation on test images and of structures in CT and MR images. We found the methods to work reliable if the model assumption on homogeneity and region characteristics are true. Furthermore, the model is simple but robust, thus allowing for a certain degree of deviation from model constraints and still delivering the expected segmentation result.	algorithm;gradient;image formation;pixel;region growing;robustness (computer science)	Regina Pohle;Klaus D. Tönnies	2001		10.1117/12.431013	computer vision;mathematical optimization;segmentation-based object categorization;mathematics;region growing;minimum spanning tree-based segmentation;scale-space segmentation;engineering drawing	Vision	46.1046231678589	-72.00175784492738	47645
76563e6c1769dae8945144c541ce33e2a08cc52c	adaptive reproducing kernel particle method for extraction of the cortical surface	kernel deformable models shape equations merging data mining magnetic resonance magnetic resonance imaging computer interfaces computational efficiency;kernel;brain;galerkin method;cortex extraction;image segmentation;mesh adaptive reproducing kernel particle method cortical surface extraction brain three dimensional magnetic resonance images flexible particle shape function galerkin approximation support generation method particle shape function dilation parameter particle insertion particle merging highly convolved structure representation self intersection fast marching segmented structure cerebral sulci finite element method cruise method;support generation method;particle shape;reproducing kernel particle method;segmented structure;algorithms artificial intelligence cerebral cortex humans image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval magnetic resonance imaging pattern recognition automated reproducibility of results sensitivity and specificity;galerkin approximation;cerebral sulci;prior knowledge;adaptive refinement;deformable models;finite element method;indexing terms;data mining;magnetic resonance image;distance field;fast marching;three dimensional magnetic resonance images;shape;general methods;reproducing kernel particle;cortical surface extraction;gradient descent;magnetic resonance;mesh;feature extraction;medical image processing;self intersection;particle insertion;magnetic resonance imaging;dilation parameter;mri;merging;reproducing kernel particle adaptive refinement cortex extraction mri;biomedical image processing;particle merging;flexible particle shape function;mesh generation biomedical mri brain feature extraction galerkin method image segmentation medical image processing;cruise method;computational efficiency;computer interfaces;mesh generation;particle shape function;deformable model;highly convolved structure representation;biomedical mri	We propose a novel adaptive approach based on the Reproducing Kernel Particle Method (RKPM) to extract the cortical surfaces of the brain from three-dimensional (3-D) magnetic resonance images (MRIs). To formulate the discrete equations of the deformable model, a flexible particle shape function is employed in the Galerkin approximation of the weak form of the equilibrium equations. The proposed support generation method ensures that support of all particles cover the entire computational domains. The deformable model is adaptively adjusted by dilating the shape function and by inserting or merging particles in the high curvature regions or regions stopped by the target boundary. The shape function of the particle with a dilation parameter is adaptively constructed in response to particle insertion or merging. The proposed method offers flexibility in representing highly convolved structures and in refining the deformable models. Self-intersection of the surface, during evolution, is prevented by tracing backward along gradient descent direction from the crest interface of the distance field, which is computed by fast marching. These operations involve a significant computational cost. The initial model for the deformable surface is simple and requires no prior knowledge of the segmented structure. No specific template is required, e.g., an average cortical surface obtained from many subjects. The extracted cortical surface efficiently localizes the depths of the cerebral sulci, unlike some other active surface approaches that penalize regions of high curvature. Comparisons with manually segmented landmark data are provided to demonstrate the high accuracy of the proposed method. We also compare the proposed method to the finite element method, and to a commonly used cortical surface extraction approach, the CRUISE method. We also show that the independence of the shape functions of the RKPM from the underlying mesh enhances the convergence speed of the deformable model	algorithm;algorithmic efficiency;anatomic structures;approximation;cerebral cortex;clinical use template;clinical act of insertion;computation;computational anatomy;convergence (action);convolution;descent direction;dilate procedure;dilation (morphology);distance transform;extraction;fast marching method;finite element method;galerkin method;gradient descent;hl7publishingsubsection <operations>;heart rate variability;insertion mutation;interpolation;intersection of set of elements;intracisternal a-particle elements;kernel;medical device incompatibility problem;medical imaging;meshfree methods;numerical analysis;particle;population parameter;rate of convergence;refinement (computing);resonance;runge–kutta methods;scott continuity	Meihe Xu;Paul M. Thompson;Arthur W. Toga	2006	IEEE Transactions on Medical Imaging	10.1109/TMI.2006.873614	gradient descent;mesh generation;computer vision;mathematical optimization;kernel;index term;radiology;feature extraction;shape;computer science;magnetic resonance imaging;machine learning;finite element method;fast marching method;galerkin method;mathematics;image segmentation;distance transform	Vision	45.15487221563789	-76.60567797299834	47803
354173da6dae6a1e4ad50f1caa8ee7af53af001f	a variational approach to adaptive correlation for motion estimation in particle image velocimetry	particle image velocimetry;cross correlation;fluid flow;motion estimation;variational approach;multiscale method;estimation error	In particle image velocimetry (PIV) a temporally separated image pair of a gas or liquid seeded with small particles is recorded and analysed in order to measure fluid flows therein. We investigate a variational approach to cross-correlation, a robust and well-established method to determine displacement vectors from the image data. A “soft” Gaussian window function replaces the usual rectangular correlation frame. We propose a criterion to adapt the window size and shape that directly formulates the goal to minimise the displacement estimation error. In order to measure motion and adapt the window shapes at the same time we combine both sub-problems into a bi-level optimisation problem and solve it via continuous multiscale methods. Experiments with synthetic and real PIV data demonstrate the ability of our approach to solve the formulated problem. Moreover window adaptation yields significantly improved results.	black and burst;cross-correlation;displacement mapping;experiment;mathematical optimization;microsoft windows;motion estimation;synthetic intelligence;temporal logic;variational principle;weight function;window function	Florian Becker;Bernhard Wieneke;Jing Yuan;Christoph Schnörr	2008		10.1007/978-3-540-69321-5_34	classical mechanics;computer vision;particle tracking velocimetry;mathematical optimization;mathematics	Vision	52.21301011677206	-72.1668434452343	47901
582e0784a4072fc5bcd562a2ec79f948f227a91b	image segmentation with optimal control techniques	image segmentation;optimal control;parameter estimation	We transpose an optimal control technique to the image segmentation problem. The idea is to consider image segmentation as a parameter estimation problem. The parameter to estimate is the color of the pixels of the image. We use the adaptive parameterization technique which builds iteratively an optimal representation of the parameter into uniform regions that form a partition of the domain, hence corresponding to a segmentation of the image. We minimize an error function during the iterations, and the partition of the image into regions is optimally driven by the gradient of this error. The resulting segmentation algorithm inherits desirable properties from its optimal control origin: soundness, robustness, and flexibility.	image segmentation;optimal control	François Clément;Hend Ben Ameur;Guy Chavent;Pierre Weis	2010	CoRR		segmentation-based object categorization;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Robotics	52.369100416512275	-71.12801575562355	48007
cc7ce008ff637017ded8249f86ce98e8192d3e3f	organ pose distribution model and an map framework for automated abdominal multi-organ localization	pose distribution model;computer aided diagnosis;search space;organ localization;gaussian mixture model;shape similarity;distributed models;probabilistic atlas;image analysis;maximum a posteriori	Abdominal organ localization is required as an initialization step for most automated abdominal organ analysis tasks, i.e. segmentation, registration, and computer aided-diagnosis. Automated abdominal organ localization is difficult because of the large variability of organ shapes, similar appearances of different organs in images, and organs in close proximity to each other. Previous methods predicted only the organ locations, but not the full organ poses including additionally sizes and orientations. Thus they were often not accurate enough to initialize other image analysis tasks. In this work we proposed a maximum a posteriori (MAP) framework to estimate the poses of multiple abdominal organs from non-contrast CT images. A novel organ pose distribution model is proposed to model the organ poses and limit the search space. Additionally the method uses probabilistic atlases for organ shapes, and Gaussian mixture models for organ intensity profile. An MAP problem is then formulated and solved for organ poses. The method was applied for the localization of liver, left and right kidneys, spleen, and pancreas, and showed promising results, especially on liver and spleen (with mean location and orientation errors under 5.3 mm and 7 degrees respectively).	ct scan;image analysis;mixture model;orientation (graph theory);probabilistic automaton;spatial variability	Xiaofeng Liu;Marius George Linguraru;Jianhua Yao;Ronald M. Summers	2010		10.1007/978-3-642-15699-1_41	computer vision;mathematics;engineering drawing;anatomy	Vision	42.03336134005067	-78.65481040326327	48019
aafc06046510e216245dcffaf0e1bda096ac67b1	face detection using fuzzy granulation and genetic algorithm in color images	skin color segmentation;image segmentation;image pixel face detection fuzzy granulation genetic algorithm color image complex background classification problem image region skin color segmentation fuzzy information granulation fig classifier fuzzy granules;image classification;classification;fuzzy set theory;skin color;face recognition;image colour analysis;object detection face recognition fuzzy set theory genetic algorithms image classification image colour analysis image segmentation;face image color analysis skin face detection image segmentation mathematical model genetic algorithms;genetic algorithm;genetic algorithms;genetic algorithm face detection fuzzy information granulation classification skin color segmentation;face detection;fuzzy information granulation;object detection;color image	Face Detection in images with complex background is a difficult and challenging problem. It can be considered as a classification problem in the sense that a given image region can be classified as face or non-face classes. In this paper, we propose a method based on skin color segmentation and classification with fuzzy information granulation (FIG) for robust and fast face detection in color images. The proposed FIG-classifier constructs fuzzy granules based on pixels of image train data and classifies image regions using these fuzzy granules. We use Genetic algorithm to select representative features to generate the FIG-Classifier. Face detection task is performed based on classification of normalized skin color segments using the proposed classifier. Experimental results show effectiveness of the proposed method in comparison with the previous methods.	face detection;genetic algorithm;image segmentation;pixel	Mehrdad Shemshaki;Roya Amjadifard	2011	The 5th International Conference on Automation, Robotics and Applications	10.1109/ICARA.2011.6144889	facial recognition system;computer vision;object-class detection;genetic algorithm;computer science;machine learning;pattern recognition;mathematics	Robotics	41.430751782809665	-66.90808212330347	48028
754e8c758142e928573a6753625f0ef2fbac93a2	brushlet segmentation for automatic detection of lumen borders in ivus images: a comparison study	brushlet;histograms;speckle;probability;image segmentation;phantoms;ultrasonic imaging;haemodynamics;multiscale analysis;transducers;three dimensional;energy function;border detection;biomedical transducers;image segmentation transducers blood ultrasonic imaging histograms harmonic analysis algorithm design and analysis;automatic detection;medical image processing;blood;ivus;lumen;frequency 45 mhz brushlet segmentation lumen border automatic detection scattering effects intravascular ultrasound images high frequency transducers three dimensional multiscale overcomplete brushlet driven harmonic analysis spatial frame incoherence blood speckle patterns two dimensional brushlet coefficient clustering surface function active framework phantom fluid flow data acquisition threshold based algorithm 2d shape driven technique nonparametric probabilistic energy functions frequency 40 mhz;data acquisition;intravascular ultrasound;high frequency;algorithm design;algorithm design and analysis;biomedical ultrasonics;ultrasonic imaging biomedical transducers biomedical ultrasonics blood data acquisition haemodynamics haemorheology harmonic analysis image segmentation medical image processing phantoms probability speckle;haemorheology;multiscale analysis brushlet ivus border detection lumen;harmonic analysis	Due to high scattering effects inside lumen, detection of luminal borders in intravascular ultrasound (IVUS) images becomes challenging when high frequency transducers are employed. In this paper, we further study previously developed three-dimensional (3D) multiscale overcomplete brushlet-driven harmonic analysis, motivated by what experts visually do, to trace the lumen borders by exploiting spatial frame incoherence within blood speckle patterns. Two-dimensional (2D) brushlet coefficient clustering was designed to isolate blood pool and estimate the lumen borders with the surface function active (SFA) framework. We evaluated our proposed algorithm on phantom with flowing fluid and 1081 clinical IVUS images acquired from six patients with single-element 40 MHz and 45 MHz transducers. We quantified and compared the results with a threshold-based algorithm and a 2D shape-driven technique driven by non-parametric probabilistic energy functions. We highlight the advantages of each approach and discuss the robustness of proposed algorithm.	3d computer graphics;algorithm;cluster analysis;coefficient;phantom reference;preclinical imaging;transducer;video-in video-out	Amin Katouzian;Elsa D. Angelini;Bernhard Sturm;Andrew F. Laine	2012	2012 9th IEEE International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2012.6235529	algorithm design;computer vision;speech recognition;computer science;harmonic analysis;mathematics;statistics	Vision	40.10973164005367	-76.22753352481574	48084
a74d9501cd9686dfe97e98c6068c74ba80b97979	wavelet domain nonlinear filtering for evoked potential signal enhancement	transformation ondelette;electrodiagnostic;systeme nerveux central;electroencefalografia;non linear filter;edge detection;signal analysis;diagnostico;nonlinear filter;potentiel evoque auditif;hombre;analisis de senal;encefalo;electroencephalographie;electrodiagnostico;deteccion contorno;detection contour;sistema nervioso central;potencial evocado auditivo;evoked potential;encephale;human;smooth transition;filtro no lineal;rapport signal bruit;relacion senal ruido;transformacion ondita;electroencephalography;electrical engineering;electrodiagnosis;signal to noise ratio;diagnosis;central nervous system;analyse signal;wavelet transformation;filtre non lineaire;homme;auditory evoked potential;transition region;diagnostic;brain vertebrata	A wavelet domain nonlinear filtering method for improving the signal-to-noise ratio (SNR) of the evoked potentials (EP) is proposed. The method modifies the selective filtering technique proposed for edge detection in images by Xu et al. for the case of signals which require a smooth transition at the edge points. It identifies the significant features of a noisy signal based on the correlation between the scales of its nonorthogonal subband decompositions. The signal transition information from interscale correlation coupled with the change in variance around the identified transition region is used to differentiate between noise and the signal. A nonlinear function such as a Gaussian smoothing function applied around the identified edge in the wavelet domain leads to smoothing in the signal space also. Numerical results obtained by applying the proposed nonlinear filtering method on middle latency responses of auditory evoked potentials show that the method is well suited for signal enhancement applications.	acoustic evoked brain stem potentials;action potential;biologic preservation;coefficient;edge detection;expectation propagation;gaussian blur;hearing problem;image scaling;morphologic artifacts;neural oscillation;nonlinear system;normal statistical distribution;numerical method;sample variance;selectivity (electronic);signal transition;signal-to-noise ratio;smoothing (statistical technique);stationary process;test scaling;thresholding (image processing);visual artifact;wavelet	G. Sita;A. G. Ramakrishnan	2000	Computers and biomedical research, an international journal	10.1006/cbmr.2000.1555	nonlinear filter;computer vision;speech recognition;edge detection;electroencephalography;electrodiagnosis;computer science;central nervous system;signal processing;mathematics;signal-to-noise ratio	ML	51.455643270864165	-76.06135345132039	48091
ff9440439246ce3a3c788057e3580c10481f512a	image based plant phenotyping using graph based method and circular hough transform			hough transform	J. Praveen Kumar;S. Domnic	2018	J. Inf. Sci. Eng.		distributed computing;computer science;computer vision;hough transform;graph;artificial intelligence	DB	41.60445080685929	-70.99511087275111	48117
09a8ccd9308d18cf521cded64e8131794fa09a01	quantitative analysis of vascular parameters for micro-ct imaging of vascular networks with multi-resolution		Previous studies showed that all the vascular parameters from both the morphological and topological parameters were affected with the altering of imaging resolutions. However, neither the sensitivity analysis of the vascular parameters at multiple resolutions nor the distinguishability estimation of vascular parameters from different data groups has been discussed. In this paper, we proposed a quantitative analysis method of vascular parameters for vascular networks of multi-resolution, by analyzing the sensitivity of vascular parameters at multiple resolutions and estimating the distinguishability of vascular parameters from different data groups. Combining the sensitivity and distinguishability, we designed a hybrid formulation to estimate the integrated performance of vascular parameters in a multi-resolution framework. Among the vascular parameters, degree of anisotropy and junction degree were two insensitive parameters that were nearly irrelevant with resolution degradation; vascular area, connectivity density, vascular length, vascular junction and segment number were five parameters that could better distinguish the vascular networks from different groups and abide by the ground truth. Vascular area, connectivity density, vascular length and segment number not only were insensitive to multi-resolution but could also better distinguish vascular networks from different groups, which provided guidance for the quantification of the vascular networks in multi-resolution frameworks.	blood supply aspects;doctor of law;dopamine;elegant degradation;estimated;ground truth;jd - java decompiler;megalencephalic leukoencephalopathy with subcortical cysts;numerous;pixel;quantitation;quantity;relevance;ventral lateral thalamic nucleus;verification and validation;viewtiful joe;x-ray microtomography;quantitative	Fengjun Zhao;Jimin Liang;Xueli Chen;Junting Liu;Dongmei Chen;Xiang Yang;Jie Tian	2015	Medical & Biological Engineering & Computing	10.1007/s11517-015-1337-0	engineering;nanotechnology;engineering drawing;statistics	Vision	39.745108876605265	-78.83348770648513	48237
b9178b986cae45f7c5fdffd91bfc9020570f3dec	adaptive regularization for image segmentation using local image curvature cues	active contour;image segmentation;low noise;medical image;parameter selection;graph cut	Image segmentation techniques typically require proper weighting of competing data fidelity and regularization terms. Conventionally, the associated parameters are set through tedious trial and error procedures and kept constant over the image. However, spatially varying structural characteristics, such as object curvature, combined with varying noise and imaging artifacts, significantly complicate the selection process of segmentation parameters. In this work, we propose a novel approach for automating the parameter selection by employing a robust structural cue to prevent excessive regularization of trusted (i.e. low noise) high curvature image regions. Our approach autonomously adapts local regularization weights by combining local measures of image curvature and edge evidence that are gated by a signal reliability measure. We demonstrate the utility and favorable performance of our approach within two major segmentation frameworks, graph cuts and active contours, and present quantitative and qualitative results on a variety of natural and medical images.	adaptive histogram equalization;cut (graph theory);image segmentation;matrix regularization	Josna Rao;Rafeef Abugharbieh;Ghassan Hamarneh	2010		10.1007/978-3-642-15561-1_47	image texture;computer vision;mathematical optimization;cut;computer science;segmentation-based object categorization;pattern recognition;active contour model;mathematics;image segmentation;scale-space segmentation	Vision	47.53065767981349	-73.20105813093296	48466
d6d3c2ff11ae1ce6e6c4eb9d4a0241db6dd18f6d	two preprocessing techniques based on grey level and geometric thickness to improve segmentation results	computer vision and robotics autonomous systems;grey level thickness;datorseende och robotik autonoma system;segmentation;real world application;random walk;preprocessing	Two different techniques of performing preprocessing of an image to improve segmentation results are presented. The methods use the grey level thickness of the objects, in order to find the resulting image, by varying the size of a neighbourhood depending on the sum of the included grey levels. The first method, RW, uses the random walk of a particle, defined in the neighbourhood of the position of the particle. The resulting image holds the number of times the particle visits a pixel. Instead of randomization to find the number of visits, the second method, IP, scans the image iteratively and calculates the expected value of the same number. Three different kinds of real world applications are demonstrated to get better segmentation results with the preprocessing techniques included than without. 2005 Elsevier B.V. All rights reserved.	grayscale;neighbourhood (graph theory);pixel;preprocessor;read-write memory;thickness (graph theory)	Mats Erikson	2006	Pattern Recognition Letters	10.1016/j.patrec.2005.07.010	computer vision;computer science;artificial intelligence;mathematics;scale-space segmentation;segmentation;preprocessor;random walk;statistics	Vision	46.06869258135711	-70.99266883681409	48496
739106b37e6862e1ef7ff510433b667fb497f564	deformable registration using scale space keypoints	deformable registration;brain;liver;ultrasound;sift;medical image processing;magnetic resonance imaging;image registration;mri;b splines;ultrasonography;scale space keypoints	In this paper, we describe a new methodology for keypoint-based affine and deformable medical image registration. This fast and computationally efficient method is automatic and does not rely on segmentation of images. The keypoint pixels used in this technique are extreme points in the scale space and are characterized by descriptor vectors which summarize the intensity gradient profile of the surrounding pixels. For each of the keypoints in the scene image∗, a corresponding keypoint is identified in the model image using the feature space nearest neighbor criteria. For deformable registration, B-splines are used to extrapolate a regular deformation grid for all of the pixels in the scene image based on the relative displacement vectors of the corresponding pairs. This approach results in a fast and accurate registration in the brain MRI images (an average target registration error of less than 2mm was acquired). We have also studied the affine registration problem in the liver ultrasound and brain MRI images and have acquired acceptable registrations using a mean square solution for affine parameters based on only around 30 corresponding keypoint pairs.	algorithmic efficiency;b-spline;displacement mapping;extrapolation;feature vector;gradient;image registration;mean squared error;pixel;scale space	Mehdi Moradi;Purang Abolmaesumi;Parvin Mousavi	2006		10.1117/12.652132	computer vision;geography;image registration;pattern recognition;computer graphics (images)	Vision	45.00338759400147	-76.05352722776091	48648
dcad543cca9be07a6f27e6ad2b9fd17a6e5ce522	post-processing techniques for making reliable measurements from curve-skeletons	vascular network;computed tomography ct;tortuosity;curve skeleton;local thickness;medial axis transform mat	Interconnected 3-D networks occur widely in biology and the geometry of such branched networks can be described by curve-skeletons, allowing parameters such as path lengths, path tortuosities and cross-sectional thicknesses to be quantified. However, curve-skeletons are typically sensitive to small scale surface features which may arise from noise in the imaging data. In this paper, new post-processing techniques for curve-skeletons are presented which ensure that measurements of lengths and thicknesses are less sensitive to these small scale surface features. The techniques achieve sub-voxel accuracy and are based on a minimal sphere-network representation in which the object is modelled as a string of minimally overlapping spheres, and as such samples the object on a scale related to the local thickness. A new measure of cross-sectional dimension termed the modal radius is defined and shown to be more robust in comparison with the standard measure (the internal radius), while retaining the desirable feature of capturing the size of structures in terms of a single measure. The techniques are demonstrated by application to trabecular bone and tumour vascular network case studies where the volumetric data was obtained by high resolution computed tomography.		Robert S. Bradley;Philip J. Withers	2016	Computers in biology and medicine	10.1016/j.compbiomed.2016.03.008	tortuosity;mathematics;geometry	Visualization	42.94339055303529	-79.82669009944486	48822
1cafd0a6bc9ac887d381bd8efbd12d327d9e8eed	high-dimensional normalized mutual information for image registration using random lines	high dimensionality;normalized mutual information;image registration;mutual information;similarity measure	Mutual information has been successfully used as an effective similarity measure for multimodal image registration. However, a drawback of the standard mutual information-based computation is that the joint histogram is only calculated from the correspondence between individual voxels in the two images. In this paper, the normalized mutual information measure is extended to consider the correspondence between voxel blocks in multimodal rigid registration. The ambiguity and highdimensionality that appears when dealing with the voxel neighborhood is solved using uniformly distributed random lines and reducing the number of bins of the images. Experimental results show a significant improvement with respect to the standard normalized mutual information.	computation;image registration;interpolation;multimodal interaction;mutual information;quantization (signal processing);similarity measure;voxel	Anton Bardera;Miquel Feixas;Imma Boada;Mateu Sbert	2006		10.1007/11784012_32	computer vision;image registration;machine learning;pattern recognition;mathematics;mutual information;statistics	Vision	48.475790533041746	-75.7660046489807	48879
478314bc6fedce6447f37660750d1663804eb97c	3d volume extraction of densely packed cells in em data stack by forward and backward graph cuts	minimisation;graph theory;minimization;map mrf energy function;image segmentation;densely packed cells;marker controlled watershed algorithm;scanning electron microscopy;grouping method;scanning electron microscope;data mining;gray scale;energy function;nanoscale image sequences;watershed transform;medical image;image edge detection;three dimensional displays;3d volume extraction;graph cut;image reconstruction;medical image processing;serial block face scanning electron microscope;image sequence;watershed segmentation;transforms;global optimization;dense nanoscale medical image segmentation;synthetic data;distance transform;em data stack;graph cuts;transforms graph theory image reconstruction image segmentation image sequences medical image processing minimisation scanning electron microscopy;nanoscale image sequences em data stack 3d volume extraction densely packed cells graph cuts 3d reconstruction dense nanoscale medical image segmentation interactive segmentation technique marker controlled watershed algorithm distance transform grouping method minimization process map mrf energy function serial block face scanning electron microscope;data mining image segmentation biomedical imaging image reconstruction clustering algorithms gray scale scanning electron microscopy image generation image resolution optical noise;3d reconstruction;minimization process;interactive segmentation;interactive segmentation technique;image sequences	3D reconstruction on dense nanoscale medical images is a very challenging research topic. The challenge comes from the fact that boundaries of objects on such images are not always very clear due to imperfect staining. This makes the segmentation of dense nanoscale medical images very difficult and thus increases the difficulty in 3D reconstruction. In this paper, we proposed a method based on watershed and an interactive segmentation technique, graph cuts, to extract 3D volumes from dense nanoscale medical images. In our method, images are first segmented by a marker-controlled watershed algorithm. Markers for watershed segmentation algorithm are seed points generated by using distance transform, followed by a new grouping method that clusters seed points that are too close. Regions obtained by watershed transform segmentation algorithms are considered as nodes in a graph. Edges are to connect between the nodes in adjacent image slices. The weight on each edge is defined based on the overlapped area between nodes. User-selected nodes (regions) in an initial image slice serve as hard constraints in the minimization process. A globally optimal 3D volume is obtained by minimizing MAP-MRF energy function via graph cuts. In our application, in order to obtain a complete 3D volume structures including branching, the final 3D volume is the union of two 3D volumes obtained by performing the minimization of MAP-MRF energy function using graph cuts forwards and backwards through the image stack. Experiments are conducted both on synthetic data and on nanoscale image sequences from the Serial Block Face Scanning Electron Microscope (SBF-SEM). The results show that our method can successfully extract 3D volumes.	3d computer graphics;3d reconstruction;algorithm;computation;cross section (geometry);cut (graph theory);distance transform;electron;markov random field;mathematical optimization;maxima and minima;stack (abstract data type);synthetic data;voxel;watershed (image processing)	Huei-Fang Yang;Yoonsuck Choe	2009	2009 IEEE Symposium on Computational Intelligence for Multimedia Signal and Vision Processing	10.1109/CIMSVP.2009.4925647	computer vision;machine learning;mathematics;image segmentation;computer graphics (images)	Vision	45.697982219748766	-71.78243086924336	49130
abca50dce8da1982610632b300385e7633a43611	image segmentation via weighted carving decompositions		In this paper we propose a graph-theoretic method of image segmentation, borrowing ideas from finding community structure in social networks using edge betweenness. This method constructs a weighted carving decomposition, or a partial carving decomposition to some resolution, of the image. From this structure image segments can be obtained in a hierarchical manner. We apply this method to multiple generated images with well defined image segments of varying complexity. Additionally, we apply this method to the Mona Lisa, an image without such well defined partitions. Results suggest that the method provides a hierarchical segmentation framework that is well suited for finding features in images.	image segmentation	Derek Mikesell;Illya V. Hicks	2017		10.1007/978-3-319-59108-7_21	computer vision;computer science;artificial intelligence;carving;scale-space segmentation;segmentation-based object categorization;social network;community structure;image segmentation;segmentation;betweenness centrality	NLP	45.62548309960228	-68.78327359456102	49169
8ac2ac7244fe3b6621f05f2bbbf7f0fda0031eb4	demons registration of high angular resolution diffusion images	rsh space;real spherical harmonic;tensile stress;demons registration;measurement;image resolution;biodiffusion;orientation distribution function demons registration diffusion image nonrigid registration high angular resolution diffusion imaging hardi antipodally symmetric spherical function real spherical harmonic multichannel demons algorithm similarity function rsh space finite strain based algorithm fiber orientation;spherical harmonic;hardi diffusion imaging registration;high resolution imaging;data model;computational modeling;neurophysiology biodiffusion biomedical mri image registration image resolution medical image processing;finite strain based algorithm;non rigid registration;orientation distribution function;antipodally symmetric spherical function;medical image processing;image registration;image resolution diffusion tensor imaging data models high resolution imaging humans tensile stress distribution functions capacitive sensors computational modeling spatial resolution;imaging;registration;mathematical model;fiber orientation;finite strain;multichannel demons algorithm;diffusion image;hardi;humans;neurophysiology;distribution functions;diffusion tensor imaging;nonrigid registration;high angular resolution diffusion imaging;spherical function;similarity function;capacitive sensors;data models;biomedical mri;diffusion imaging;spatial resolution;harmonic analysis	In this work we present a method for non-rigid registration of high angular resolution diffusion imaging (HARDI) datasets that are modeled by a field of antipodally symmetric spherical functions, represented by their expansion in the real spherical harmonic (RSH) basis. We use a multichannel demons algorithm which utilizes a computationally simple, rotationally invariant similarity function defined in the RSH space. Additionally, we describe a finite strain based algorithm for reorientation of the HARDI data model. We validate our framework on simulated fiber orientation distribution datasets and on human in-vivo data.	algorithm;angularjs;data model;similarity measure;video-in video-out	Luke Bloy;Ragini Verma	2010	2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2010.5490161	computer vision;radiology;image resolution;computer science;harmonic analysis;mathematics;geometry;optics;neurophysiology	Vision	47.499223616764986	-77.45395446714971	49198
1fb7c238243ea5ab446f2a0dc85cb495cd15b583	segmentation of colour images using variational expectation-maximization algorithm		The approach proposed in this paper takes into account the uncertainty in color modeling by employing variational Bayesian estimation. Mixtures of Gaussians are considered for modeling color images. Distributions of parameters characterizing color regions are inferred from data statistics. The variational expectation-maximization (VEM) algorithm is used for estimating the hyperparameters corresponding to distributions of parameters. A maximum a posteriori approach employing a dualexpectation maximization (EM) algorithm is considered for the hyperparameter initialization of the VEM algorithm.	calculus of variations;color image;expectation–maximization algorithm;image segmentation;mixture model;variational principle	Nikolaos Nasios;Adrian G. Bors	2004		10.5244/C.18.73	mathematical optimization;machine learning;pattern recognition;mathematics	ML	50.78353699871052	-70.23581231717922	49201
68c8c1a2db1f51b216a8ecdc66473db0b72b335d	two-level model averaging techniques in drosophila brain imaging	wireframe models;piecewise linear techniques;image reconstruction brain models medical image processing solid modelling;model averaging techniques;brain models;brain modeling skeleton solid modeling brain mapping laboratories data mining principal component analysis image reconstruction humans piecewise linear techniques;wireframe models model averaging techniques drosophila brain imaging molecular brain mapping brain atlas 3d reference template;data mining;skeleton;drosophila brain imaging;large scale;model averaging;brain modeling;brain mapping;image reconstruction;medical image processing;principal component analysis;solid modeling;brain imaging;humans;brain atlas;3d reference template;molecular brain mapping;solid modelling	For molecular brain mapping research, a reference template is necessary for integration of experimental data from different laboratories. However, the concept of conventional brain atlas cannot provide such a reference template that fulfills our requirements. In this paper, we propose a specific algorithm for model averaging to construct the 3D reference template. It extracts the geometric information of all data sets by reconstructing the individual wireframe models, and extracts their skeletons for determining the average skeleton. Largescale averaged models, pseudoaverage models, can be obtained by warping the individual models toward the average skeleton. And the final average model can be yielded after a small-scale model averaging procedure that determines the geometric median of pseudoaverage models. In contrast with other algorithms, our proposed algorithm allows more severe variations because of its two-level hierarchy.	algorithm;brain atlas;brain mapping;ensemble learning;geometric median;requirement;wire-frame model	Ying-Cheng Chen;Yung-Chang Chen;Ann-Shyn Chiang	2002		10.1109/ICIP.2002.1040107	iterative reconstruction;computer vision;computer science;bioinformatics;machine learning;solid modeling;brain mapping;skeleton;neuroimaging;principal component analysis	Vision	44.92739773952557	-77.32681224183786	49328
03a447b406d10a3b1b504ccfc410521fc48639f2	unsupervised statistical sketching for non-photorealistic rendering models	optimisation;non photorealistic rendering;bayes methods;bayesian inference;realistic images maximum likelihood estimation bayes methods optimisation image texture rendering computer graphics;maximum likelihood estimation;image texture;stochastic optimization;posterior distribution;hand sketched pencil drawings unsupervised statistical sketching nonphotorealistic rendering models bayesian inference gradient vector field distribution contour detection global prior deformation model maximum a posteriori estimation posterior distribution stochastic optimization;realistic images;vector field;rendering computer graphics;deformable model;bayesian methods rendering computer graphics shape control electronic mail deformable models maximum a posteriori estimation stochastic processes computational modeling image processing ink	This paper investigates the use of the Bayesian inference for devising an unsupervised sketch rendering procedure. As likelihood model of this inference, we exploit the recent statistical model of the gradient vector field distribution proposed by Destrempes et al. for contour detection. A global prior deformation model for each pencil stroke is also considered. In this Bayesian framework, the placement of each stroke is viewed as the search of the Maximum A Posteriori estimation of the posterior distribution of its deformations. We use a stochastic optimization algorithm in order to find these optimal deformations. This yields an unsupervised method to create realistic hand-sketched pencil drawings. Combined with an example-based local rendering model, used to transfer the textural tone value of a given depiction style, the proposed scheme allows to simulate automatic synthesis of various artistic illustration styles.	algorithm;bayesian network;gradient;mathematical optimization;non-photorealistic rendering;rendering (computer graphics);simulation;statistical model;stochastic optimization;unbiased rendering;unsupervised learning	Max Mignotte	2003		10.1109/ICIP.2003.1247309	image texture;computer vision;vector field;rendering;computer science;stochastic optimization;machine learning;pattern recognition;mathematics;maximum likelihood;posterior probability;bayesian inference;statistics	Vision	49.76253911421593	-72.91447965927385	49348
de7dec148c42ec6121a7bdd964ca1a21ad1bfd6e	integrated segmentation of cellular structures	laplacian of gaussian;cluster algorithm;cellular structure;tissues;fluorescence imaging;automatic segmentation;luminescence;histopathology;blob detection;cytology;segmentation;error threshold;cell nuclei;image acquisition;algorithms;decision process;transform theory;ground truth;cellular structures;histology;distance transform;cell biology;prostate cancer	Automatic segmentation of cellular structures is an essential step in image cytology and histology. Despite substantial progress, better automation and improvements in accuracy and adaptability to novel applications are needed. In applications utilizing multi-channel immuno-fluorescence images, challenges include misclassification of epithelial and stromal nuclei, irregular nuclei and cytoplasm boundaries, and over and under-segmentation of clustered nuclei. Variations in image acquisition conditions and artifacts from nuclei and cytoplasm images often confound existing algorithms in practice. In this paper, we present a robust and accurate algorithm for jointly segmenting cell nuclei and cytoplasm using a combination of ideas to reduce the aforementioned problems. First, an adaptive process that includes top-hat filtering, Eigenvalues-of-Hessian blob detection and distance transforms is used to estimate the inverse illumination field and correct for intensity non-uniformity in the nuclei channel. Next, a minimum-error-thresholding based binarization process and seed-detection combining Laplacian-of-Gaussian filtering constrained by a distance-mapbased scale selection is used to identify candidate seeds for nuclei segmentation. The initial segmentation using a local maximum clustering algorithm is refined using a minimum-error-thresholding technique. Final refinements include an artifact removal process specifically targeted at lumens and other problematic structures and a systemic decision process to reclassify nuclei objects near the cytoplasm boundary as epithelial or stromal. Segmentation results were evaluated using 48 realistic phantom images with known ground-truth. The overall segmentation accuracy exceeds 94%. The algorithm was further tested on 981 images of actual prostate cancer tissue. The artifact removal process worked in 90% of cases. The algorithm has now been deployed in a high-volume histology analysis application.	algorithm;blob detection;circuit complexity;cluster analysis;ground truth;hessian;imaging phantom;maxima and minima;thresholding (image processing)	Peter O. Ajemba;Yousef Al-Kofahi;Richard Scott;Michael J. Donovan;Gerardo Fernandez	2011		10.1117/12.876722	computer vision;blob detection;bioinformatics;histology;histopathology;scale-space segmentation;cytology	Vision	40.30487443180737	-73.16058576836909	49396
60ed48d4b94b9b1d3a2eb0fb0def38b172ef0cbd	noise removal of cbct images using an adaptive anisotropic diffusion filter		In this work we proposed an adaptive anisotropic filtering method for removing unwanted noise information that may occur in cone beam computed tomography (CBCT) images. The data used in this study consist of 1200 different image sections obtained from 30 different patients who came to Karadeniz Technical University, Faculty of Dentistry, Department of Oral Diagnosis and Radiology Clinic for routine controls. At first, to identify 2D image sections that do not contain noise information, we measured noise levels in CBCT dataset sections using a noise level estimation method. Then, we applied different levels of noise to those noise-free images. We used anisotropic diffusion filter (Perona and Malik's filter), an automatic anisotropic filter (Tsiotsios and Petrou's method), and our adaptive anisotropic filtering method to remove noise information from those images. Afterward, we obtained peak signal to noise ratio (PSNR) and mean absolute error (MAE) values derived from the results. Proposed adaptive anisotropic diffusion filter seems to be a good choice for removing noise that may occur on CBCT image sections.	anisotropic diffusion;anisotropic filtering;approximation error;ct scan;cone beam computed tomography;mean squared error;noise (electronics);peak signal-to-noise ratio;radiology	Ercument Yilmaz;Temel Kayikçioglu;Saadettin Kayipmaz	2017	2017 40th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2017.8076067	image noise;salt-and-pepper noise;gaussian noise;median filter;noise measurement;filter (signal processing);anisotropic diffusion;computer vision;computer science;artificial intelligence;anisotropic filtering	Vision	52.74640472798214	-75.37115579845707	49768
62e9a76a2c19145fb635a62625318fcbba8c609f	directional local contrast based blood vessel detection in retinal images	performance measure;biology computing;background pixels directional local contrast blood vessel detection retinal image integral computing parameter adjustment illumination condition blood vessel mapping blood vessel pixels;eye;blood vessels biomedical imaging retina lighting computer vision concurrent computing robustness costs pigmentation image databases;retinal images directional local contrast blood vessel detection;indexing terms;medical image processing biology computing blood vessels eye;blood vessel;blood vessel detection;retinal images;directional local contrast;medical image processing;retinal imaging;blood vessels	In this paper, we proposed a novel algorithm to detect blood vessels on retinal images. By using directional local contrast as its detection feature, our algorithm is highly sensitive, fast and accurate. The algorithm only needs integral computing with very simple parameter adjustments and highly suitable for parallelization. It is much more robust to illumination conditions than intensity based counterparts and equally effective for large and small blood vessel detections. Traditional blood vessel mapping solutions focused on detecting the most number of blood vessel pixels at the cost of least number of falsely identified background pixels. This performance criterion works for well illuminated images with sharp boundary, but it does not address two major concerns. The first is that it favors detection of large blood vessels, and the second is that for darker images (due to poor illumination or pigment colors) it can be very difficult to generate hand traced maps. To overcome these problems, we propose using central lines of the blood vessels as a new performance measure for blood vessel mapping. The new performance measure is easy to evaluate, and it complements the existing performance measure. Experiment results on two public retinal image databases show that our algorithm outperforms two well known existing algorithms in terms of speeds and accuracy.	algorithm;color;complement (complexity);database;map;parallel computing;pigment;pixel;sensor	Ming Zhang;Jyh-Charn Liu	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4380018	computer vision;index term;computer science;computer graphics (images)	Vision	44.19685875882839	-70.15038718298526	49894
707ad662b66227dca4268dca889c8ffafd24a7b4	the research of architectural color evaluation based on image processing	pattern clustering;town and country planning;building;k means;chromatology;town and country planning building image colour analysis pattern clustering;image colour analysis;chromatic aberration;color evaluation;urban planning image processing urban architecture k means clustering algorithm background environment chromatology building color composition architectural color analysis;color evaluation chromatology chromatic aberration k means color quantization;image color analysis buildings color clustering algorithms algorithm design and analysis computer architecture architecture;color quantization	In order to improve the efficiency of the analysis of building color, and enhance the objectivity of evaluation, we propose a novel method to analyze and evaluate the color of urban architecture. First, a developed K-means clustering algorithm is adopted, which classifies the surface color of a building after extracting the building from its background environments. Then we set up an evaluation model formula based on the quantization of color's three elements in the chromatology. Finally, we analyze the composition of building color and give the evaluation results based on image processing technology. The experimental results show that the new method has significant advantage over traditional methods in the architectural color analysis, which provides a convenient and efficient foundation to the work of urban planning.	algorithm;cluster analysis;color;computer;image processing;k-means clustering;objectivity/db	Tian Tian;Xiujin Wang;Yu Bai	2011	2011 Fourth International Symposium on Computational Intelligence and Design	10.1109/ISCID.2011.80	color histogram;computer vision;chromatic aberration;color quantization;hsl and hsv;color normalization;computer science;machine learning;mathematics;multimedia;building;k-means clustering;computer graphics (images)	Vision	43.32365451423455	-67.4460746411688	49935
10745748e76209bde186e26efb2f8e05d19bc391	unsupervised segmentation using fuzzy logic based texture spectrum for mri brain images	fuzzy logic;brain imaging;global analysis;spectrum	Textures are replications, symmetries and combinations of various basic patterns, usually with some random variation one of the gray-level statistics. This article proposes a new approach to Segment texture images. The proposed approach proceeds in 2 stages. First, in this method, local texture information of a pixel is obtained by fuzzy texture unit and global texture information of an image is obtained by fuzzy texture spectrum. The purpose of this paper is to demonstrate the usefulness of fuzzy texture spectrum for texture Segmentation. The 2 Stage of the method is devoted to a decision process, applying a global analysis followed by a fine segmentation, which is only focused on ambiguous points. The above Proposed approach was applied to brain image to identify the components of brain in turn, used to locate the brain tumor and its Growth rate. Keywords—Fuzzy Texture Unit, Fuzzy Texture Spectrum, and Pattern Recognition, segmentation.	fuzzy logic;pattern recognition;pixel;texture mapping unit	G. Wiselin Jiji;L. Ganesan	2005			fuzzy logic;scale-space segmentation;image segmentation;computer vision;segmentation-based object categorization;segmentation;artificial intelligence;computer science;pattern recognition	Vision	42.032639797489914	-67.69047515775188	50027
84204756fa9818935acbdd43ff479e0266e45cd5	a morphology-based approach for interslice interpolation of anatomical slices from volumetric images	topology;mathematical morphology;interpolation;computer assisted tomography interslice interpolation interslice topology shape based interpolation volumetric imaging magnetic resonance imaging mathematical morphology;computed tomography;current transformers;ultrasonic imaging;volumetric imaging;biomedical imaging;shape based interpolation;computer assisted tomography;interslice interpolation;interpolation shape biomedical imaging current transformers computed tomography data visualization medical diagnostic imaging ultrasonic imaging medical treatment magnetic resonance imaging;topology biomedical mri computerised tomography interpolation mathematical morphology medical image processing;shape;algorithms anatomy cross sectional artificial intelligence computer simulation image enhancement image interpretation computer assisted imaging three dimensional models anatomic pattern recognition automated reproducibility of results sensitivity and specificity subtraction technique;medical image processing;magnetic resonance imaging;data visualization;interslice topology;computerised tomography;medical treatment;volumetric imaging mathematical morphology shape based interpolation;current transformer;algorithm design;medical diagnostic imaging;biomedical mri	This paper proposes a new morphology-based approach for the interslice interpolation of current transformer (CT) and MRI datasets composed of parallel slices. Our approach is object based and accepts as input data binary slices belonging to the same anatomical structure. Such slices may contain one or more regions, since topological changes between two adjacent slices may occur. Our approach handles explicitly interslice topology changes by decomposing a many-to-many correspondence into three fundamental cases: one-to-one, one-to-many, and zero-to-one correspondences. The proposed interpolation process is iterative. One iteration of this process computes a transition sequence between a pair of corresponding input slices, and selects the element located at equal distance from the input slices. This algorithmic design yields a gradual, smooth change of shape between the input slices. Therefore, the main contribution of our approach is its ability to interpolate between two anatomic shapes by creating a smooth, gradual change of shape, and without generating over-smoothed interpolated shapes.	accepting of extremity;alignment;anatomic structures;anatomy, regional;arabic numeral 0;biologic preservation;closing (morphology);dilate procedure;dilation (morphology);displacement mapping;entity–relationship model;erosion lesion;galaxy morphological classification;interpolation imputation technique;iteration;matching;many-to-many;mathematical morphology;maximal set;morphometric analysis;morphometrics;object-based language;one-to-many (data model);one-to-one (data model);pathological dilatation;pixel;smoothing (statistical technique);subpixel rendering;transformer	Alexandra Branzan Albu;Trevor Beugeling;Denis Laurendeau	2008	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2008.921158	current transformer;computer vision;radiology;computer science;magnetic resonance imaging;mathematics;geometry;data visualization;computer graphics (images)	Visualization	45.086141310185354	-77.65628950857598	50253
d85fcb4cee57ef8309743590ad4e3c4ef5dfff12	medical image clustering algorithm based on graph entropy	clustering algorithm;entropy based;component;sparsification;medical image;clustering algorithm component medical image entropy based sparsification;medical diagnostic imaging image edge detection computed tomography data mining clustering methods gray scale;pattern clustering computational complexity entropy graph theory medical image processing;time complexity medical image clustering algorithm completed graph weighted graph clustering method sparsified graph pruned graph undirected graph clustering method graph entropy	Recently, a variety of medical imaging technologies have been used widely in clinical diagnosis. As a large number of medical images are produced everyday, it becomes a hot issue of data mining on medical image in current that how to make full use of these medical images and cluster efficiently to help doctors to diagnose. In this paper, we propose a medical image clustering method. Firstly, medical image dataset is represented as a weighted, undirected and completed graph. Secondly, the graph is sparsified and pruned. This model can describe the similarity between medical images very well. Last, weighted and undirected graph clustering method based on graph entropy is proposed to cluster these medical images. The experimental results show that this method can cluster medical images efficiently and run well in time complexity and clustering results.	algorithm;cluster analysis;data mining;graph (discrete mathematics);imaging technology;medical imaging;time complexity	Yu Zhan;Haiwei Pan;Qilong Han;Xiaoqin Xie;Zhiqiang Zhang;Xiao Zhai	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7382105	correlation clustering;combinatorics;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;component;mathematics;cluster analysis	ML	42.34770519154457	-73.0380246738122	50345
d552133aabb4f4ddb6d6231faa1f929cb4e4ef03	ganglion cyst region extraction from ultrasound images using possibilistic c-means clustering method				Alethea Suryadibrata;Kwang-Baek Kim	2017	J. Inform. and Commun. Convergence Engineering	10.6109/jicce.2017.15.1.49	ultrasound;computer vision;cluster analysis;ganglion cyst;artificial intelligence;computer science	AI	41.6835541929225	-71.8794197134871	50437
8a351d1d82d9f4a77a7eedb01fc21e3fb4704a29	high performance adaptive fidelity algorithms for multi-modality optic nerve head image fusion	image fusion;biomedical imaging;feature extraction;image registration;heuristic optimization	A high performance adaptive fidelity approach for multi-modality Optic Nerve Head (ONH) image fusion is presented. The new image fusion method, which consists of the Adaptive Fidelity Exploratory Algorithm (AFEA) and the Heuristic Optimization Algorithm (HOA), is reliable and time efficient. It has achieved an optimal fusion result by giving the visualization of fundus image with a maximum angiogram overlay. Control points are detected at the vessel bifurcations using the AFEA. Shape similarity criteria are used to match the control points that represent same salient features of different images. HOA adjusts the initial good-guess of control points at the subpixel level in order to maximize the objective function Mutual-Pixel-Count (MPC). In addition, the performance of the AFEA and HOA algorithms was compared to the Centerline Control Point Detection Algorithm, Root Mean Square Error (RMSE) minimization objective function employed by the traditional Iterative Closest Point (ICP) algorithm, Genetic Algorithm, and some other existing image fusion approaches. The evaluation results strengthen the AFEA and HOA algorithms in terms of novelty, automation, accuracy, and efficiency.	control point (mathematics);experiment;exploratory testing;genetic algorithm;heuristic;image fusion;iterative closest point;iterative method;loss function;mean squared error;modality (human–computer interaction);optic nerve (gchq);optimization problem;pixel;selection algorithm	Hua Cao;Nathan E. Brener;Bahram Khoobehi;S. Sitharama Iyengar	2011	Signal Processing Systems	10.1007/s11265-010-0496-3	medical imaging;computer vision;simulation;feature extraction;computer science;image registration;machine learning;image fusion	Robotics	45.90394680969147	-70.44670348460846	50730
26ddba2a973b0ca449942988a617116a9f7e211a	an analytical model for compton scatter in a homogeneously attenuating medium	radioisotope scanning and imaging;poisson noise;metodo monte carlo;emission computed tomography;modele mathematique;coeficiente atenuacion;coefficient attenuation;photon;transporte;collimateur;simulation;compton effect;nonlinear effect;methode monte carlo;simulacion;psi_mic;radioisotope scanning and imaging compton effect computerised tomography image reconstruction modelling;tomocentelleografia;attenuation coefficient;modelo matematico;positron emission tomography;foton;algorithm;reconstruction image;transport;point source;compton scattering;reconstruccion imagen;diffusion compton;point spread function;image reconstruction;monte carlo method;computerised tomography;mathematical model;empirical model;collimator;monte carlo simulation;colimador;difusion compton;empirical model accurate image reconstruction nuclear medicine analytical model compton scatter homogeneously attenuating medium spect images statistical noise poisson noise collimator acceptance angle monte carlo simulations line source measurements;analytical models image reconstruction gaussian processes collimators electromagnetic scattering particle scattering attenuation fourier transforms cameras iterative algorithms;maximum likelihood reconstruction;tomoscintigraphie;analytical model	Accurate reconstruction of SPECT images is hampered by four nonlinear effects in the acquisition process: attenuation, scatter, collimator acceptance angle, and statistical noise. A good mathematical description of these effects is obviously crucial for the reconstruction. Poisson noise, attenuation, and collimator acceptance angle are relatively easy to model. Scattering, however, is a very complex process, and is mostly described using empirical models. A new model for the scatter point spread function in a homogeneous medium that is based on physical considerations and is capable of predicting scatter contributions of point sources as a function of depth in homogeneous media is presented. The model is verified with Monte Carlo simulations and line source measurements, and is compared to an existing empirical model.	collimator device component;guided ray;line source;mathematics;monte carlo method;noise-induced hearing loss;nonlinear system;simulation	Johan Nuyts;Hilde Bosmans;Paul Suetens	1993	IEEE transactions on medical imaging	10.1109/42.241869	compton scattering;optics;nuclear medicine;physics;quantum mechanics;empirical modelling;statistics;monte carlo method;medical physics	Vision	53.208653500187204	-78.91493701167437	50867
20508885b277f38da9976b07f9fb5f2ceec38b49	non-local staple: an intensity-driven multi-atlas rater model	sensitivity and specificity;radiology information systems;image enhancement;image interpretation computer assisted;reproducibility of results;algorithms;pattern recognition automated;humans;subtraction technique;computer simulation;information storage and retrieval;models anatomic	"""Multi-atlas segmentation provides a general purpose, fully automated class of techniques for transferring spatial information from an existing dataset (""""atlases"""") to a previously unseen context (""""target"""") through image registration. The method used to combine information after registration (""""label fusion"""") has a substantial impact on the overall accuracy and robustness. In practice, weighted voting techniques have dramatically outperformed algorithms based on statistical fusion (i.e., algorithms that incorporate rater performance into the estimation process--STAPLE). We posit that a critical limitation of statistical techniques (as generally proposed) is that they fail to incorporate intensity seamlessly into the estimation process and models of observation error. Herein, we propose a novel statistical fusion algorithm, non-local STAPLE, which merges the STAPLE framework with a non-local means perspective. Non-local STAPLE (1) seamlessly integrates intensity into the estimation process, (2) provides a theoretically consistent model of multi-atlas observation error, and (3) largely bypasses the need for group-wise unbiased registrations. We demonstrate significant improvements in two empirical multi-atlas experiments."""		Andrew J. Asman;Bennett A. Landman	2012	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-33454-2_53	computer simulation;computer vision;simulation;computer science;machine learning;data mining;algorithm	Vision	43.481203120843475	-78.00802260928039	51128
829066b507e8e8c85eb5205e6536b8d1999ee3b3	automatic tissue segmentation of neonate brain mr images with subject-specific atlases	automatic;biological patents;expectation maximization algorithms;population;brain;biomedical journals;image segmentation;magnetism;tissues;text mining;europe pubmed central;citation search;segmentation;citation networks;neonate;research articles;abstracts;magnetic resonance imaging;open access;mri;life sciences;clinical guidelines;tissue;atlas;diagnostics;full text;diffusion tensor imaging;diffusion;rest apis;orcids;europe pmc;subject specific;biomedical research;bioinformatics;literature search	Automatic tissue segmentation of the neonate brain using Magnetic Resonance Images (MRI) is extremely important to study brain development and perform early diagnostics but is challenging due to high variability and inhomogeneity in contrast throughout the image due to incomplete myelination of the white matter tracts. For these reasons, current methods often totally fail or give unsatisfying results. Furthermore, most of the subcortical midbrain structures are misclassified due to a lack of contrast in these regions. We have developed a novel method that creates a probabilistic subject-specific atlas based on a population atlas currently containing a number of manually segmented cases. The generated subject-specific atlas is sharp and adapted to the subject that is being processed. We then segment brain tissue classes using the newly created atlas with a single-atlas expectation maximization based method. Our proposed method leads to a much lower failure rate in our experiments. The overall segmentation results are considerably improved when compared to using a non-subject-specific, population average atlas. Additionally, we have incorporated diffusion information obtained from Diffusion Tensor Images (DTI) to improve the detection of white matter that is not visible at this early age in structural MRI (sMRI) due to a lack of myelination. Although this necessitates the acquisition of an additional sequence, the diffusion information improves the white matter segmentation throughout the brain, especially for the mid-brain structures such as the corpus callosum and the internal capsule.	atlases;body of uterus;class;corpus callosum;diffusion magnetic resonance imaging;diffusion tensor imaging;expectation–maximization algorithm;experiment;failure rate;heart rate variability;infant, newborn;internal capsule of brain;midbrain structure;text corpus;white matter;biologic segmentation;brain development;myelination	Marie Cherel;François Budin;Marcel Prastawa;Guido Gerig;Kevin Lee;Claudia Buss;Amanda E. Lyall;Kirsten Zaldarriaga Consing;Martin Styner	2015	Proceedings of SPIE--the International Society for Optical Engineering	10.1117/12.2082209	diffusion mri;magnetism;computer science;artificial intelligence;atlas;magnetic resonance imaging;diffusion;image segmentation;automatic transmission;segmentation;physics;population	Vision	42.17463806518725	-78.77246653426819	51279
63b41397da11dd83b399ca9a8237dbb55401031e	an iterative multiresolution scheme for sfm	analisis imagen;iterative method;image recognition;reconocimiento imagen;vision ordenador;image processing;dato que falta;noisy data;remplissage;procesamiento imagen;filling;analyse multiresolution;traitement image;computer vision;reduccion ruido;metodo iterativo;donnee manquante;estructura segun moviemento;methode iterative;noise reduction;image sequence;reconnaissance image;reduction bruit;image analysis;vision ordinateur;secuencia imagen;rapport signal bruit;relacion senal ruido;missing data;synthetic data;signal to noise ratio;multiresolution analysis;analyse image;structure from motion;evaluation studies;analisis multiresolucion;sequence image;relleno;structuration d apres le mouvement	Abstract. Several factorization techniques have been proposed for tackling the Structure from Motion problem. Most of them provide a good solution, while the amount of missing and noisy data is within an acceptable ratio. Focussing on this problem, we propose to use an incremenal multiresolution scheme, with classical factorization techniques. Information recovered following a coarse-to-fine strategy is used for both, filling in the missing entries of the input matrix and denoising original data. An evaluation study, by using two different factorization techniques–the Alternation and the Damped Newton–is presented for both synthetic data and real video sequences. 1	alternation (formal language theory);iterative method;missing data;multiresolution analysis;newton;newton's method;noise reduction;scheme;signal-to-noise ratio;structure from motion;synthetic data	Carme Julià;Angel Domingo Sappa;Felipe Lumbreras;Joan Serrat;Antonio M. López	2006		10.1007/11867586_73	multiresolution analysis;computer vision;structure from motion;image analysis;missing data;image processing;computer science;artificial intelligence;noise reduction;mathematics;iterative method;signal-to-noise ratio;synthetic data	Vision	53.56611702742061	-66.30575397862864	51289
b8ca48e69801b7a03b4f8a378040eabbacd9eebb	a new algorithm of infrared gait detection based on immune ant colony	optimisation;evolutionary computation;image segmentation;neural nets;gaussian processes;optimal segmentation parameters infrared gait detection immune ant colony thermal imaging characteristics background subtraction method single gaussian background model pulse coupled neural network pcnn;single gauss model pulse coupled neural network pcnn immune ant colony infrared gait detection;ant colony;image segmentation immune system mathematical model biological neural networks neurons adaptation models approximation algorithms;infrared imaging;gait analysis;optimisation evolutionary computation gait analysis gaussian processes image segmentation infrared detectors infrared imaging neural nets;infrared;infrared detectors;pulse coupled neural network	The accurate infrared gait detection is a research difficulty because of the unique thermal imaging characteristics. Therefore, the coarse result of infrared human detection is obtained by using the background subtraction method based on the single Gaussian background model, and is fine segmented by applying the improved pulse coupled neural network (PCNN). At the same time, the effect of the adaptive segmentation is reached by introducing the immune ant colony algorithm to fast determine the optimal segmentation parameters of PCNN. The simulation results show that this algorithm can achieve the ideal detection effect.	algorithm;ant colony optimization algorithms;artificial neural network;background subtraction;image segmentation;pixel;pulse-coupled networks;simulation	Jianhui Tan	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6024055	computer vision;gait analysis;infrared;computer science;artificial intelligence;ant colony;machine learning;gaussian process;image segmentation;artificial neural network;evolutionary computation	Robotics	42.955164309829385	-68.6228272014593	51446
12af09aaa2e8897f30c693f777919294e162c61c	rician noise removal in diffusion tensor mri	diffusion weighted images;magnetic resonance image;diffusion tensor mri;noise removal;quantitative evaluation;diffusion tensor;diffusion weighted	Rician noise introduces a bias into MRI measurements that can have a significant impact on the shapes and orientations of tensors in diffusion tensor magnetic resonance images. This is less of a problem in structural MRI, because this bias is signal dependent and it does not seriously impair tissue identification or clinical diagnoses. However, diffusion imaging is used extensively for quantitative evaluations, and the tensors used in those evaluations are biased in ways that depend on orientation and signal levels. This paper presents a strategy for filtering diffusion tensor magnetic resonance images that addresses these issues. The method is a maximum a posteriori estimation technique that operates directly on the diffusion weighted images and accounts for the biases introduced by Rician noise. We account for Rician noise through a data likelihood term that is combined with a spatial smoothing prior. The method compares favorably with several other approaches from the literature, including methods that filter diffusion weighted imagery and those that operate directly on the diffusion tensors.	addresses (publication format);bio-informatics;bioinformatics;digimon world;distortion;ephrin type-b receptor 1, human;evaluation;glyph;lin-pravastatin;lobular neoplasia;medical image computing;mental orientation;monte carlo method;nih roadmap initiative tag;national center for research resources;national centers for biomedical computing;noise reduction;resonance;signal-to-noise ratio;simulation;smoothing (statistical technique);united states national institutes of health;biomedical engineering field;digital tomosynthesis	Saurav Basu;P. Thomas Fletcher;Ross T. Whitaker	2006	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/11866565_15	diffusion mri;computer vision;radiology;medicine;magnetic resonance imaging;mathematics;anisotropic diffusion	Vision	44.70392956803639	-79.16846981963418	51498
5c041be5bfbcf513caae6d8b782e71ea2fdeba2b	a multiphase level set formulation for image segmentation using a mrf-based nonsymmetric student's-t mixture model			image segmentation;markov random field;mixture model	Hongqing Zhu;Qunyi Xie	2018	Signal, Image and Video Processing	10.1007/s11760-018-1314-9		Vision	50.75461293569235	-71.17962268102899	51561
5247ffbef9aee2451c2b7be0ed708ecf588f84b1	blind background subtraction in dental panoramic x-ray images: an application approach	conjunto independiente;subtraction;tissu;noise estimation;contenu image;image content;aplicacion medical;separacion ciega;illumination;x ray imaging;panoramic photography;independent set;image databank;racinisation;spatial coherence;x ray source;error sistematico;sustraccion;rayon x;soustraction;hombre;blind;ruido no gaussiano;analyse multiresolution;non gaussian noise;photographie panoramique;fotografia panoramica;rayos x;tejido;blind separation;ensemble independant;bias;mean square error;banco imagen;systeme osteoarticulaire;banque image;background subtraction;sistema osteoarticular;human;tissue;pattern recognition;separation aveugle;osteoarticular system;x ray radiography;a priori information;medical application;cross validation;reconnaissance forme;reconocimiento patron;radiographie rx;x ray;contenido imagen;multiresolution analysis;eclairement;stemming;ciego;analisis multiresolucion;radiografia rx;erreur systematique;alumbrado;homme;application medicale;x rays;bruit non gaussien;aveugle	Dental Panoramic X-ray images are images having complex content, because several layers of tissue, bone, fat, etc. are superimposed. Non-uniform illumination, stemming from the X-ray source, gives extra modulation to the image, which causes spatially varying X-ray photon density. The interaction of the X-ray photons with the density of matter causes spatially coherent varying noise contribution. Many algorithms exist to compensate background effects, by pixel based or global methods. However, if the image is contaminated by a non-negligible amount of noise, that is usually non-Gaussian, the methods cannot approximate the background efficiently. In this paper, a dedicated approach for background subtraction is presented, which operates blind, that means the separation of a set of independent signals from a set of mixed signals, with at least, only little a priori information about the nature of the signals, using the A-Trous multiresolution transform to alleviate this problem. The new method estimates the background bias from a reference scan, which is taken without a patient. The background values are rescaled by a polynomial compensation factor, given by mean square error criteria, thus subtracting the background will not produce additional artifacts in the image. The energy of the background estimate is subtracted from the energy of the mixture. The method is capable to remove spatially varying noise also, allocating an appropriate spatially noise estimate. This approach has been tested on 50 images from a database of panoramic X-ray images, where the results are cross validated by medical experts.		Peter Michael Goebel;Ahmed Nabil Belbachir;Michael Truppe	2005		10.1007/11550518_54	multiresolution analysis;computer vision;independent set;background subtraction;subtraction;computer science;bias;stemming;mean squared error;algorithm;cross-validation	Vision	51.71017631275373	-75.62814649482397	51743
2ee8b79c5d5e6bd7fd8771ff35b4beb63252d096	improved estimates of partial volume coefficients from noisy brain mri using spatial context	spatial context;non local filtering;finite resolution;markov random fields;segmentation;magnetic resonance image;non local means;markov random field;random noise;partial volume;mri;brain tissue;human brain	This paper addresses the problem of accurate voxel-level estimation of tissue proportions in the human brain magnetic resonance imaging (MRI). Due to the finite resolution of acquisition systems, MRI voxels can contain contributions from more than a single tissue type. The voxel-level estimation of this fractional content is known as partial volume coefficient estimation. In the present work, two new methods to calculate the partial volume coefficients under noisy conditions are introduced and compared with current similar methods. Concretely, a novel Markov Random Field model allowing sharp transitions between partial volume coefficients of neighbouring voxels and an advanced non-local means filtering technique are proposed to reduce the errors due to random noise in the partial volume coefficient estimation. In addition, a comparison was made to find out how the different methodologies affect the measurement of the brain tissue type volumes. Based on the obtained results, the main conclusions are that (1) both Markov Random Field modelling and non-local means filtering improved the partial volume coefficient estimation results, and (2) non-local means filtering was the better of the two strategies for partial volume coefficient estimation.	addresses (publication format);coefficient;estimated;greater than;histocompatibility testing;magnetic resonance imaging;markov chain;markov random field;noise (electronics);non-local means;voxel	José V. Manjón;Jussi Tohka;Montserrat Robles	2010	NeuroImage	10.1016/j.neuroimage.2010.06.046	mathematical optimization;radiology;medicine;spatial contextual awareness;magnetic resonance imaging;machine learning;mathematics;segmentation;non-local means;partial volume;statistics	ML	49.944541452852704	-77.4885976751836	51861
a5e065d54213e7298e6f9664e972e388211b9c5a	multiresolution edge detection and classification: noise characterization	second order;gaussian noise;edge detection;image classification;gaussian noise edge detection image classification wavelet transforms;wavelet transforms;image edge detection polynomials noise level multiresolution analysis wavelet analysis gaussian processes wavelet coefficients numerical analysis gaussian noise filters;numerical analysis;multiresolution analysis;gaussian noise edge detection classification noise characterization gray level images multiresolution analysis edge classification wavelet decision algorithm	In this work we present the method we have developed in order to achieve edge detection and classification in gray level images for five different contour types: step, ramp, stair, pulse and noise. The edge detection method is based in a multiresolution analysis using Mallat and Zhong's wavelet, which is compared with the gaussian-based one. The edge classification has been made analyzing the first six wavelet coefficient evolution across scales at the edge position. We have implemented a decision algorithm based on a simple second order polynomial fitting, and a subsequent numerical analysis of the obtained polynomial in order to discriminate between the five contour types. At the end of the process we obtain the edge position and the belonging class for each contour pixel. The main advance of this work is the characterization of the edge class 'noise'. In this class are included the gaussian noise and the irrelevant low-level contours that appear in non-thresholded image. We can filter this new edge simply by eliminating the edges classified in this group without affecting low-level, but important, contours.	edge detection	José Ramón Beltrán;Fernando Beltrán Blázquez;Angel Estopanan	1998		10.1109/ICSMC.1998.727555	multiresolution analysis;gaussian noise;computer vision;contextual image classification;speech recognition;edge detection;numerical analysis;computer science;deriche edge detector;pattern recognition;mathematics;canny edge detector;second-order logic;wavelet transform	Vision	51.061731156323376	-66.22872887748728	52162
3a873f25422e532e0a09711d846812fa36e105bc	3-d mouse brain model reconstruction from a sequence of 2-d slices in application to allen brain atlas	high resolution;rigid body;elastic deformations;3d model;morphing;neuroimaging;image registration;linear transformation;b splines;brain atlas;3d reconstruction	The paper describes a method of fully automatic 3D-reconstruction of a mouse brain from a sequence of histological coronal 2D slices. The model is constructed via non-linear transformations between the neighboring slices and further morphing. We also use rigid-body transforms in the preprocessing stage to align the slices. Afterwards, the obtained 3D-model is used to generate virtual 2D-images of the brain in arbitrary section-plane. We use this approach to construct a highresolution anatomic 3D-model of a mouse brain using well-known Allen Brain Atlas which is publicly available.	3d modeling;align (company);allen brain atlas;morphing;nonlinear system;preprocessor	Anton Osokin;Dmitry P. Vetrov;Dmitry Kropotov	2009		10.1007/978-3-642-14571-1_22	3d reconstruction;b-spline;computer vision;rigid body;image resolution;computer science;image registration;artificial intelligence;linear map;morphing;neuroimaging;computer graphics (images)	Vision	47.13812539769288	-75.8629884481634	52438
91f099113249b983c76dcc7ad60210311e95d0aa	cloud computing and security	information security;cloud computing	Centerline extraction is the basis to understand three dimensional structure of the lung. Since the bronchus has a complex tree structure, bronchoscopists easily tend to get disoriented path to a target location. In this paper, an automatic centerline extraction algorithm for 3D virtual bronchoscopy is presented. This algorithm has three main components. Firstly, a new airway tree segmentation method based on region growing is applied to extract major airway branches and sub-branches. Secondly, the original center is adjusted according to the geometry features of Jacobian matrix, and modified Dijkstra shortest path algorithm is applied in the centerline algorithm to yield the centerline of the bronchus. Then, the airway tree structure and feature calculation are represented from many features. Our algorithm is tested with various CT image data and its performance is efficient.	cloud computing;dijkstra's algorithm;jacobian matrix and determinant;region growing;shortest path problem;tree structure	Xingming Sun;Zhaoqing Pan;Elisa Bertino	2018		10.1007/978-3-030-00006-6	computer security model;cloud computing security;cloud computing;computer science;end-user computing;cloud testing;distributed computing;utility computing;world wide web;computer security;grid computing;autonomic computing	Visualization	40.68089670392937	-70.72142790889214	52496
9fcc79de2e2883ac97031fb21d27d7571a9adbc7	nonparametric hierarchical bayesian model for functional brain parcellation	unsupervised learning;health research;uk clinical guidelines;unsupervised analysis;belief networks;biological patents;dirichlet process;generic model;nonparametric statistics;unsupervised learning belief networks biomedical mri brain models medical image processing nonparametric statistics;europe pubmed central;mri nonparametric hierarchical bayesian model unsupervised analysis dirichlet process functional brain images functional brain response;citation search;functional brain response;brain models;bayesian methods;functional response;brain modeling;hierarchical bayesian model;uk phd theses thesis;medical image processing;nonparametric hierarchical bayesian model;functional brain images;mri;life sciences;mathematical model;clustering algorithms;approximation methods;hierarchical dirichlet process;uk research reports;bayesian methods brain modeling independent component analysis layout testing computer science artificial intelligence laboratories image analysis pattern analysis;medical journals;functional brain imaging;visual cortex;algorithm design and analysis;europe pmc;biomedical research;data models;biomedical mri;bioinformatics	We develop a method for unsupervised analysis of functional brain images that learns group-level patterns of functional response. Our algorithm is based on a generative model that comprises two main layers. At the lower level, we express the functional brain response to each stimulus as a binary activation variable. At the next level, we define a prior over the sets of activation variables in all subjects. We use a Hierarchical Dirichlet Process as the prior in order to simultaneously learn the patterns of response that are shared across the group, and to estimate the number of these patterns supported by data. Inference based on this model enables automatic discovery and characterization of salient and consistent patterns in functional signals. We apply our method to data from a study that explores the response of the visual cortex to a collection of images. The discovered profiles of activation correspond to selectivity to a number of image categories such as faces, bodies, and scenes. More generally, our results appear superior to the results of alternative data-driven methods in capturing the category structure in the space of stimuli.	bayesian network;categories;cerebral cortex;coherence (physics);collections (publication);face;functional response;generative model;ibm notes;inference;internet information services;national center for research resources;national institute of biomedical imaging and bioengineering (u.s.);nephrogenic systemic fibrosis;neurotechnology;selectivity (electronic);united states national institutes of health;variational principle;voxel;algorithm;anatomical layer;fmri;funding grant	Danial Lashkari;Ramesh Sridharan;Ed Vul;Po-Jang Hsieh;Nancy Kanwisher;Polina Golland	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops	10.1109/CVPRW.2010.5543434	unsupervised learning;functional response;nonparametric statistics;data modeling;algorithm design;computer vision;bayesian probability;computer science;machine learning;pattern recognition;mathematical model;data mining;cluster analysis;statistics;hierarchical dirichlet process	Vision	43.58944255454387	-75.7276800677607	52608
035859494abc68b2d4d241fd5d4ee85833444308	multi-label energy minimization for object class segmentation	image processing;object class segmentation;graph cuts algorithm;graph cuts;multilabel energy minimization;semantic class;image segmentation;random walker algorithm;power watershed algorithm;semantic class segmentation problems;random walker;object recognition;energy function minimization;graph theory;watershed;minimisation;optimization algorithms;graph-based optimization	The task of associating a semantic class to the objects present in an image is challenging because this problem involves the joint segmentation and recognition of the objects. In this work, we use a recent approach embedding several optimization algorithms into a common framework named Power watershed to perform this task. We show how the fast watershed algorithm can be used to minimize an energy function for which the minimizer corresponds to the desired object class segmentation. Higher order potentials are then added to improve label consistency. We also demonstrate that the random walker algorithm can be successfully applied to semantic class segmentation problems. Comparisons with the Graph Cuts algorithm show that the proposed approaches yield better segmentation results, obtained up to twelve times faster on a very challenging indoor scenes dataset.	amiga walker;cut (graph theory);energy minimization;graph cuts in computer vision;mathematical optimization;multi-label classification;random walker algorithm;watershed (image processing)	Camille Couprie	2012	2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)		graph cuts in computer vision;computer vision;grabcut;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;random walker algorithm;minimum spanning tree-based segmentation;scale-space segmentation;connected-component labeling	Vision	46.10981696045006	-68.93672985568627	52757
8de965aae7d39f82e7dce3d41ccf2ee8a9fc6c2a	discriminative gaussian mixtures for interactive image segmentation	graph theory;image segmentation pixel image color analysis parameter estimation filter bank image analysis image texture analysis algorithm design and analysis cost function assembly;image graph;gaussian mixture;image segmentation;gaussian processes;perforation;image texture gaussian processes graph theory image colour analysis image segmentation;indexing terms;image texture;interactive systems image segmentation;graph cut optimization;interactive system;images texture discriminative gaussian mixtures interactive image segmentation graph cut optimization image graph;image colour analysis;graph cut;images texture;interactive image segmentation;interactive systems;discriminative gaussian mixtures	Recently graph-cut optimization has been extensively explored for interactive image segmentation. In this paper we propose discriminative Gaussian mixtures (DGMs) to boost the performance of graph-cut-based segmentation. Given the user specified pixels, our algorithm analyzes their distributions in color, texture and spatial spaces and produces optimized Gaussian mixtures to set the data cost in the image graph, under the criteria of maximizing the discriminant power. We also show how to assemble novel training data to train DGMs for the link cost in the graph. Experimental results demonstrate that DGMs can noticeably improve the performance of graph-cut segmentation on texture-rich images.	algorithm;cut (graph theory);discriminant;graph cuts in computer vision;image segmentation;mathematical optimization;pixel	Jue Wang	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.365979	image texture;computer vision;index term;cut;computer science;graph theory;machine learning;pattern recognition;gaussian process;mathematics;image segmentation;scale-space segmentation	Vision	45.77759654174204	-69.28189645186475	52762
18069bf84605a819ed80676790ad875e18708408	search for the best matching ultrasound frame based on spatial and temporal saliencies	saliency map;image match;saliency;ultrasound;ultrasound image search;prior information;ultrasound imaging;region of interest;ultrasonography;video;similarity measure	In this paper we present a generic system for fast and accurate retrieval of the best matching frame from Ultrasound video clips given a reference Ultrasound image. It is challenging to build a generic system to handle various lesion types without any prior information of the anatomic structures of the Ultrasound data. We propose to solve the problem based on both spatial and temporal saliency maps calculated from the Ultrasound images, which implicitly analyze the semantics of images and emphasize the anatomic regions of interest. The spatial saliency map describes the importance of the pixels of the reference image while the temporal saliency map further distinguishes the subtle changes of the anatomic structure in a video. A hierarchical comparison scheme based on a novel similarity measure is employed to locate the most similar frames quickly and precisely. Our system ensures the robustness, accuracy and efficiency. Experiments show that our system achieves more accurate results with fast speed.© (2011) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.		Shaolei Feng;Xiaoyan Xiang;Shaohua Kevin Zhou;Roee Lazebnik	2011		10.1117/12.878341	computer vision;simulation;video;computer science;ultrasonography;salience;ultrasound;multimedia;region of interest	Vision	39.352801595842635	-70.02908178216097	52880
c33a1afd6326a08b581eb1470e65f8267118aa50	characterization of neuropathological shape deformations	brain;elastic properties;magnetic heads;magnetic resonance images;normal pressure hydrocephalus;biomedical nmr;biomechanics brain image classification medical image processing shape measurement finite element analysis statistical analysis biomedical nmr;biomechanics;statistical modes;image classification;neuropathological shape deformation characterization;deformable models;shape measurement;finite element method;fem;shape deformation;classification;pharmaceutical technology;nonpathological variation;finite element modeling;medical image analysis;schizophrenia;shape pathology diseases magnetic heads laboratories deformable models brain pharmaceutical technology computer science surgery;thesis;shape;statistical analysis;clustering;medical image processing;principal component analysis;mri;brain structure;surgery;diseases;nuclear engineering;eigenanalysis;disease discrimination;finite element analysis;computer science;modal analysis;shape description;alzheimer disease;classification neuropathological shape deformation characterization human brain structures finite element modeling fem fea statistical modes disease discrimination elastic properties pathology nonpathological variation disease processes magnetic resonance images mri schizophrenia alzheimer disease normal pressure hydrocephalus;fea;pathology;human brain;human brain structures;disease processes	We present a framework for analyzing the shape deformation of structures within the human brain. A mathematical model is developed describing the deformation of any brain structure whose shape is affected by both gross and detailed physical processes. Using our technique, the total shape deformation is decomposed into analytic modes of variation obtained from finite element modeling, and statistical modes of variation obtained from sample data. Our method is general, and can be applied to many problems where the goal is to separate out important from unimportant shape variation across a class of objects. In this paper, we focus on the analysis of diseases that affect the shape of brain structures. Because the shape of these structures is affected not only by pathology but also by overall brain shape, disease discrimination is difficult. By modeling the brain’s elastic properties, we are able to compensate for some of the nonpathological modes of shape variation. This allows us to experimentally characterize modes of variation that are indicative of disease processes. We apply our technique to magnetic resonance images of the brains of individuals with schizophrenia, Alzheimer’s disease, and normal-pressure hydrocephalus, as well as to healthy volunteers. Classification results are presented. Index Terms —Medical image analysis, shape description, deformable models, finite element method, modal analysis, principal component analysis, eigenanalysis, clustering. —————————— ✦ ——————————	cluster analysis;experiment;finite element method;image analysis;mathematical model;modal logic;principal component analysis;resonance;shape context	John Martin;Alex Pentland;Stan Sclaroff;Ron Kikinis	1998	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.659928	computer science;biomechanics;machine learning;finite element method;shape analysis;mathematics	Vision	41.59187742226444	-77.29986975767946	52901
fdb44028d6a9e73c9a7694f626b8d9df02c61b4f	filter design and performance evaluation for fingerprint image segmentation		Fingerprint recognition plays an important role in many commercial applications and is used by millions of people every day, e.g. for unlocking mobile phones. Fingerprint image segmentation is typically the first processing step of most fingerprint algorithms and it divides an image into foreground, the region of interest, and background. Two types of error can occur during this step which both have a negative impact on the recognition performance: 'true' foreground can be labeled as background and features like minutiae can be lost, or conversely 'true' background can be misclassified as foreground and spurious features can be introduced. The contribution of this paper is threefold: firstly, we propose a novel factorized directional bandpass (FDB) segmentation method for texture extraction based on the directional Hilbert transform of a Butterworth bandpass (DHBB) filter interwoven with soft-thresholding. Secondly, we provide a manually marked ground truth segmentation for 10560 images as an evaluation benchmark. Thirdly, we conduct a systematic performance comparison between the FDB method and four of the most often cited fingerprint segmentation algorithms showing that the FDB segmentation method clearly outperforms these four widely used methods. The benchmark and the implementation of the FDB method are made publicly available.	algorithm;b-spline;benchmark (computing);bessel filter;biometrics;butterworth filter;coefficient;database;directional coronary atherectomy;download;fdb (file format);filter design;fingerprint verification competition;fingerprint recognition;gabor filter;generalization (psychology);ground truth;hilbert transform;image editing;image segmentation;kilo;liveness;maxima and minima;minutiae;mobile phone;performance evaluation;phil bernstein;preprocessor;region of interest;structure tensor;summation (document);thresholding (image processing);tracer;yvo g. desmedt;biologic segmentation	Duy Hoang Thai;Stephan Huckemann;Carsten Gottschlich	2016	PloS one	10.1371/journal.pone.0154160	scale-space segmentation	Vision	39.32177100738635	-70.45642606157271	53025
ee3d69d7b34b0b4a5c6620f5931ed92da6a00df6	segmentation using region merging with edges		Segmentation is the division of an image into a number of contained self-consistent regions. Each region must be homogeneous with respect to a particular property such as intensity, colour or texture. Ideally the regions will clearly relate to distinct elements or objects within the scene. In general segmentation can be achieved by one of two approaches — either using a region based method which forms regions by considering their overall properties, or by using an edge linking approach which looks for rapid luminance changes signifying regions boundaries.	algorithm;alvey;map;texture mapping	Michel Gay	1989		10.5244/C.3.20	region growing;scale-space segmentation	Vision	47.197944578363796	-67.56187932160161	53216
caf18f15dedbb12f9eb483505a87210b92156801	supervised evaluation methodology for curvilinear structure detection algorithms	curvilinear structure detection algorithms;detection quality;detection accuracy;curvilinear structure detection;precise performance characterization;detection rate;curvilinear structure detection algorithm;earlier approach;supervised evaluation methodology;proposed performance measure;performance evaluation;curvilinear structure;edge detection	Curvilinear structures are useful features in a variety of applications. Compared to other commonly used features such as edges, there is relatively few work on curvilinear structure detection and its performance evaluation. In this paper we propose a novel supervised methodology for evaluating the performance of curvilinear structure detection algorithms. We consider the two aspects of performance, namely detection rate and detection accuracy, separately, in contrast to their mixed handling in earlier approaches that typically produces biased impression of detection quality. By doing so, the proposed performance measures give us a more informative and precise performance characterization. We will demonstrate the advantages of our approach using both synthetic and real examples.	algorithm;embedded system;information;performance evaluation;protein structure prediction;synthetic intelligence	Xiaoyi Jiang;Daniel Mojon	2002		10.1109/ICPR.2002.10009	computer vision;feature detection;simulation;object-class detection;edge detection;computer science;computer graphics (images)	Vision	40.38122345913768	-68.3335498878029	53298
6c066a9060f3d625443f73966c4ec66de6a9277a	a harmonic decomposition reconstruction algorithm for spatially-varying focal length collimators	artefacto;reconstruction algorithms image reconstruction interpolation optical collimators single photon emission computed tomography convolution fourier series fourier transforms numerical simulation robustness;evaluation performance;fourier series;performance evaluation;fourier transform;detection efficiency;photon;evaluacion prestacion;collimateur;medical diagnostic imaging fourier series semifrequency resampling technique projection data parallel beam collimators angular variables inverse cormack transform spect pentium ii 266 pc computer numerical simulations nuclear medicine;simulacion numerica;spatial variation;armonica;tomocentelleografia;harmonic;indexing terms;artefact;foton;medical image processing single photon emission computed tomography image reconstruction;reconstruction image;distance focale;focal length;harmonique;reconstruccion imagen;exploration radioisotopique;distancia focal;image reconstruction;medical image processing;emission tomography;simulation numerique;variacion espacial;single photon emission computed tomography;tecnica;radionuclide study;robust performance;variation spatiale;collimator;algorithms computer simulation equipment design fourier analysis humans models theoretical phantoms imaging tomography emission computed single photon;reconstruction algorithm;exploracion radioisotopica;colimador;technique;tomoscintigraphie;numerical simulation	Spatially varying focal length fan-beam collimators can be used in single photon emission computed tomography to improve detection efficiency and to reduce reconstruction artifacts resulting from the truncation of projection data. It has been proven that there exists no convolution backprojection algorithm for this type of collimator, so a complicated interpolation between two nonparallel projection rays is necessary for existing algorithms. The interpolation may generate blurring and artifacts in the reconstructed images. Based on a harmonic decomposition technique and the translation property of Fourier series, a semifrequency resampling technique is proposed to avoid the above mentioned interpolations. By this technique, the harmonic decomposition of projection data for spatially varying focal length fan-beam collimators has the same form as that for parallel-beam collimators in the semifrequency domain (Fourier transform with respect to angular variables only). An alternative version of the inverse Cormack transform is then proposed to reconstruct the images. The derived reconstruction algorithm was implemented in a Pentium II/266 PC computer. Numerical simulations demonstrated its efficiency (3 s for 128 x 128 reconstruction arrays) and its robust performance (compared to the existing algorithms).		Jiangsheng You;Zhengrong Liang;Shanglian Bao	1998	IEEE transactions on medical imaging	10.1109/42.746632	iterative reconstruction;computer simulation;focal length;fourier transform;spatial variability;computer vision;index term;photon;harmonic;mathematics;optics;fourier series;computer graphics (images)	Vision	52.908698290906756	-79.01097919221728	53363
36bcd350fb15505352a0a10df600a425db1ef365	morphological segmentation of histology cell images	mathematical morphology;biological tissues;image segmentation;binary image;medical image processing;split merge segmentation morphological segmentation histology cell images morphological gradient object border extraction border thinning fast gray scale thinning algorithm cell extraction;image segmentation gray scale image analysis biomedical imaging kernel algorithm design and analysis file systems image edge detection cybernetics informatics;mathematical morphology biological tissues medical image processing image segmentation	Two algorithms for segmentation of cell images are proposed. They have a unique part that contains computation of morphological gradient to extract object borders and thinning the obtained borders to get a line of one-pixel thickness. For this task, we propose the fast gray-scale thinning algorithm that is based on the idea of the analysis of binary image layers. Then, the obtained one-pixel lines are used to extract cells and compute their characteristics. The algorithms based on morphological and split/merge segmentation are developed and used for this task.	algorithm;binary image;computation;experiment;grayscale;mathematical morphology;morphological gradient;pixel;thickness (graph theory);thinning	Alexandr Nedzved;Sergey Ablameyko;Ioannis Pitas	2000		10.1109/ICPR.2000.905385	image texture;computer vision;range segmentation;mathematical morphology;binary image;computer science;morphological gradient;segmentation-based object categorization;pattern recognition;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;computer graphics (images)	Vision	43.8654640233552	-68.99116763191219	53510
63d0a53ce5d10761aafde3dffa4e3e181e67efde	a new framework for fitting shape models to range scans: local statistical shape priors without correspondences	volumetric;surface fitting;image representation;i 4 8 image processing and computer vision;i 5 1 pattern recognition;statistical;i 4 10 image processing and computer vision;models;scene analysis		ct scan;curve fitting	Carsten Last;Simon Winkelbach;Friedrich M. Wahl	2013		10.2312/PE.VMV.VMV13.153-160	computer vision;feature detection;volumetric display;computer science;machine learning;pattern recognition;computer graphics (images)	Vision	45.133189024909804	-75.19615281390818	53549
177846109eb641e04a199b36645465e26f9a5b4a	a non-stationary mrf model for image segmentation from a soft boundary map	energy based model;color textured image segmentation;berkeley image database;non stationary markovian mrf model;segmentation into regions from edge map;segmentation from soft contour	In this paper, we address the problem of estimating a segmentation map into regions from a soft (or possibly probabilistic) boundary representation. For this purpose, we have defined a simple non-stationary MRF model with long-range pairwise interactions whose potentials are estimated from the likelihood of the presence of an edge at each considered pair of pixels. Another contribution of this paper is also to demonstrate that an efficient and interesting alternative strategy to complex and elaborate region-based segmentation models consists in averaging several (quickly estimated) soft contour maps (obtained, for example, with simpler edge-based segmentation models) and to use this MRF reconstruction model to achieve a reliable and accurate segmentation map into regions. This model has been successfully applied on the Berkeley image database. The experiments reported in this paper demonstrate that the proposed segmentation model from an edge map is reliable and that this segmentation strategy is efficient (in terms of visual evaluation and quantitative performance measures) and performs well compared with the best existing state-of-the-art segmentation methods recently proposed in the literature.	algorithm;boundary representation;contour line;experiment;image segmentation;interaction;map;markov random field;mathematical optimization;pixel;stationary process	Max Mignotte	2012	Pattern Analysis and Applications	10.1007/s10044-012-0272-z	computer vision;range segmentation;segmentation-based object categorization;pattern recognition;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Vision	49.283787816903896	-70.45973115119294	53552
d0166cecd5e172ea6a2068806dff962b54ba0142	moving frames for heart fiber geometry	diffusion mri;connection forms;differential geometry;moving frames;heart myofibers;generalized helicoids	Elongated cardiac muscle cells named cardiomyocytes are densely packed in an intercellular collagen matrix and are aligned to helical segments in a manner which facilitates pumping via alternate contraction and relaxation. Characterizing the geometrical variation of their groupings as cardiac fibers is central to our understanding of normal heart function. Motivated by a recent abstraction by Savadjiev et al. of heart wall fibers into generalized helicoid minimal surfaces, this paper develops an extension based on differential forms. The key idea is to use Maurer-Cartan's method of moving frames to study the rotations of a frame field attached to the local fiber direction. This approach provides a new set of parameters that are complimentary to those of Savadjiev et al. and offers a framework for developing new models of the cardiac fiber architecture. This framework is used to compute the generalized helicoid parameters directly, without the need to formulate an optimization problem. The framework admits a straightforward numerical implementation that provides statistical measurements consistent with those previously reported. Using Diffusion MRI we demonstrate that one such specialization, the homeoid, constrains fibers to lie locally within ellipsoidal shells and yields improved fits in the rat, the dog and the human to those obtained using generalized helicoids.	alignment;cardiac wall structure;emoticon;fits;frame (physical object);heart diseases;hospital admission;linear programming relaxation;mathematical optimization;muscle cells;myocytes, cardiac;name;numerical analysis;optical fiber;optimization problem;partial template specialization;pumping (computer systems);scale space;self-information;the baseball network;tissue fiber;ueli maurer (cryptographer);pump (device)	Emmanuel Piuze;Jon Sporring;Kaleem Siddiqi	2013	Information processing in medical imaging : proceedings of the ... conference	10.1007/978-3-642-38868-2_44	diffusion mri;differential geometry;simulation;radiology;mathematics;geometry	Vision	43.329745099954835	-80.14374112864782	53591
0722e9aa35416bd870502b493f61de148cf889fd	segmentation and morphometric analysis of cells from fluorescence microscopy images of cytoskeletons	rabbits;animals;microscopy confocal;imaging three dimensional;cytochalasin d;actin cytoskeleton;cytoskeleton;image processing computer assisted;cell shape;colchicine;cells cultured;fibroblasts;microscopy fluorescence;patellar ligament;computational biology;microtubules	We developed a method to reconstruct cell geometry from confocal fluorescence microscopy images of the cytoskeleton. In the method, region growing was implemented twice. First, it was applied to the extracellular regions to differentiate them from intracellular noncytoskeletal regions, which both appear black in fluorescence microscopy imagery, and then to cell regions for cell identification. Analysis of morphological parameters revealed significant changes in cell shape associated with cytoskeleton disruption, which offered insight into the mechanical role of the cytoskeleton in maintaining cell shape. The proposed segmentation method is promising for investigations on cell morphological changes with respect to internal cytoskeletal structures.	cytoskeletal filaments;cytoskeleton;delay-tolerant networking;denial-of-service attack;focal (programming language);focal adhesions;mechanics;microscopy, fluorescence;microtubules;morphometrics;region growing;scanning electron microscopy;tissue adhesions;biologic segmentation;interest	Yoshihiro Ujihara;Masanori Nakamura;Hiroshi Miyazaki;Shigeo Wada	2013		10.1155/2013/381356	computational biology;biology;microtubule;cell biology;cytoskeleton;genetics;anatomy	Visualization	40.44224248947169	-73.88824674236939	53800
fc0f3c9ddf68ffa8b280b66134ed046b12a266c0	on the computational aspects of gibbs-markov random field modeling of missing-data in image sequences	modelizacion;optimal solution;optimisation;image sequence processing;restauration image;signal detection image sequences markov processes random processes video signals optimisation;image processing;modelo markov;optimizacion;iterated conditional mode;dato que falta;deteccion;optimal method;signal detection;stochastic simulation;procesamiento imagen;image sequence processing computational aspects gibbs markov random field modeling missing data detection gmrf video restoration application maximum a posteriori probability estimation map estimation optimization algorithms continuous relaxation labeling combinatorial optimization performance stochastic simulated annealing iterated conditional modes mean field annealing;image restoration;detection;indexing terms;simulated annealing;traitement image;markov random field;donnee manquante;optimization problem;modelisation;restauracion imagen;recuit simule;markov model;campo aleatorio;senal video;signal video;image sequences simulated annealing robustness image restoration computational modeling labeling performance analysis optimization methods stochastic processes analytical models;image sequence;random processes;robust method;map estimation;video signal;recocido simulado;secuencia imagen;optimization;video signals;missing data;markov processes;modele markov;combinatorial optimization;optimal algorithm;modeling;mean field annealing;sequence image;champ aleatoire;image sequences;random field	Gibbs-Markov random field (GMRF) modeling has been shown to be a robust method in the detection of missing-data in image sequences for a video restoration application. However, the maximum a posteriori probability (MAP) estimation of the GMRF model requires computationally expensive optimization algorithms in order to achieve an optimal solution. The continuous relaxation labeling (RL) is explored in this paper as an efficient approach for solving the optimization problem. The conversion of the original combinatorial optimization into a continuous RL formulation is presented. The performance of the RL formulation is analyzed and compared with that of other optimization methods such as stochastic simulated annealing, iterated conditional modes, and mean field annealing. The results show that RL holds out promise as an optimization algorithm for problems in image sequence processing.	algorithm;analysis of algorithms;circuit restoration;combinatorial optimization;electrophoresis, gel, pulsed-field;iterated conditional modes;iteration;lagrangian relaxation;linear programming relaxation;markov chain;markov random field;mathematical optimization;mean field annealing;optimization problem;simulated annealing	Dilip Krishnan;Man-Nang Chong;Showbhik Kalra	1999	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.777096	optimization problem;image restoration;mathematical optimization;random field;systems modeling;index term;simulated annealing;missing data;image processing;combinatorial optimization;computer science;machine learning;pattern recognition;stochastic simulation;mathematics;markov process;markov model;random optimization;statistics;detection theory	Vision	51.50789292226202	-69.20868828631012	53831
79a1f981659bd2bbb78e8dd53797db57678459ea	dou-edge evaluation algorithm for automatic thin crack detection in pipelines	inspection image segmentation skeleton image edge detection machine vision acoustics morphology;image segmentation;acoustics;pipes computer vision crack detection edge detection fracture inspection mechanical engineering computing;inspection;skeleton;morphology;edge detection dou edge evaluation algorithm automatic thin crack detection pipelines computer vision algorithm dee inspection fractures machine vision;image edge detection;machine vision	This paper describes and evaluates a novel computer vision algorithm for automatic thin crack detection in pipelines using dou-edge evaluation (DEE). Inspection for pipes is crucial and it is performed periodically to ensure that the structured integrity of the pipe systems is maintained. Thin cracks and fractures are among the defects which can cause critical damage to pipe systems. Numerous techniques have been used to detect cracks in pipes including machine vision, mostly based on edge-detection algorithms (i.e. Sobel, Laplace). However, these algorithms encounter difficulties in extracting cracks from complicated and noisy environments (i.e. sewer pipes). The DEE algorithm overcomes this problem by evaluating the size and shape of each object in the inspection environment. The results show that thin cracks were automatically extracted by the proposed algorithm.	algorithm;computer vision;edge detection;machine vision;named pipe;pipeline (computing);sobel operator	Phat Huynh;Robert Ross;Andrew Martchenko;John Devlin	2015	2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)	10.1109/ICSIPA.2015.7412188	computer vision;inspection;morphology;machine vision;computer science;image segmentation;skeleton	Robotics	41.24410893054914	-69.64290646391977	53903
cf07fae0896ea09eab5686776b98ffd29dd1d0fd	a laplacian of gaussian-based approach for spot detection in two-dimensional gel electrophoresis images	laplacian of gaussian;gel electrophoresis;two dimensions;two dimensional gel electrophoresis;biological systems;image analysis;reconstruction algorithm	Two-dimension gel electrophoresis (2-DE) is a proteomic technique that allows the analysis of protein profiles expressed in a given cell, tissue or biological system at a given time. The 2-DE images depict protein as spots of various intensities and sizes. Due to the presence of noise, the inhomogeneous background, and the overlap between the spots in 2-DE image, the protein spot detection is not a straightforward process. In this paper, we present an improved protein spot detection approach, which is based on Laplacian of Gaussian algorithm, and we extract the regional maxima by morphological grayscale reconstruction algorithm, which can reduce the impact of noisy and background in spot detection. Experiments on real 2-DE images show that the proposed approach is more reliable, precise and less sensitive to noise than the traditional Laplacian of Gaussian algorithm and it offers a good performance in our gel image analysis software.	algorithm;biological system;blob detection;grayscale;image analysis;mathematical optimization;maxima;parallel computing;proteomics;usability	Feng He;Bangshu Xiong;Chengli Sun;Xiaobin Xia	2010		10.1007/978-3-642-18369-0_2	chromatography;bioinformatics;analytical chemistry;mathematics	Comp.	39.682555561759216	-73.09641160069697	54122
425e41b02be6da1bf80c598631f610351b7d7eda	an improved computer vision method for white blood cells detection	cell shape;image interpretation computer assisted;hematologic tests;algorithms;pattern recognition automated;humans;computational biology;computer simulation;leukocytes;automation	The automatic detection of white blood cells (WBCs) still remains as an unsolved issue in medical imaging. The analysis of WBC images has engaged researchers from fields of medicine and computer vision alike. Since WBC can be approximated by an ellipsoid form, an ellipse detector algorithm may be successfully applied in order to recognize such elements. This paper presents an algorithm for the automatic detection of WBC embedded in complicated and cluttered smear images that considers the complete process as a multiellipse detection problem. The approach, which is based on the differential evolution (DE) algorithm, transforms the detection task into an optimization problem whose individuals represent candidate ellipses. An objective function evaluates if such candidate ellipses are actually present in the edge map of the smear image. Guided by the values of such function, the set of encoded candidate ellipses (individuals) are evolved using the DE algorithm so that they can fit into the WBCs which are enclosed within the edge map of the smear image. Experimental results from white blood cell images with a varying range of complexity are included to validate the efficiency of the proposed technique in terms of its accuracy and robustness.	approximation algorithm;blood cells;complexity;computer vision;detectors;differential evolution;embedded system;embedding;genetic programming;genetic algorithm;horner's method;iterative method;leukocytes;mathematical optimization;medical imaging;optimization problem;otsu's method;pixel;sensor;smear - instruction imperative;smear campaign;subpixel rendering	Erik Valdemar Cuevas Jiménez;Margarita Díaz;Miguel Manzanares;Daniel Zaldivar;Marco A. Pérez Cisneros	2013		10.1155/2013/137392	computer simulation;computational biology;computer vision;simulation;pathology;computer science;artificial intelligence;automation;machine learning;algorithm	Vision	39.465715968065716	-71.2159045348759	54413
4fd24105e1956752e19bca5ec1b75f511c0d34e1	a comparative study of staff removal algorithms	omr;music images;evaluation performance;image recognition;lute tablature staff removal algorithms staffline removal music images skeletonization based approach error metrics image defects computer generated scores image deformations real world data modern western music notation historic music notation mensural notation;page description languages;music optical recognition;image segmentation;performance evaluation;measurement;defecto;document page segmentation;helium;perforation;lute tablature;performance evaluation segmentation pixel classification music optical recognition;analisis forma;evaluacion prestacion;error metrics;musica;pixel classification;modern western music notation;page segmentation;esqueltizacion;metric;testing;intelligence artificielle;segmentation;indexing terms;classification;image defects;performance metric;optical character recognition software;real world data;mensural notation;squelettisation;musique;skeletonization based approach;evaluation methodology;ordinary magnetoresistance;staffline removal;traitement document;staff removal algorithms;robustesse;defect;defaut;artificial intelligence;metrico;robustness;historic music notation;image segmentation ordinary magnetoresistance optical character recognition software robustness testing page description languages computer errors measurement humans;pattern analysis;humans;skeletonization;document processing;inteligencia artificial;image deformations;algorithms artificial intelligence automatic data processing documentation image enhancement image interpretation computer assisted information storage and retrieval music pattern recognition automated reproducibility of results sensitivity and specificity subtraction technique;computer generated scores;music;clasificacion;segmentacion;computer errors;metrique;analyse forme;tratamiento documento;robustez;image segmentation image recognition	This paper presents a quantitative comparison of different algorithms for the removal of stafflines from music images. It contains a survey of previously proposed algorithms and suggests a new skeletonization-based approach. We define three different error metrics, compare the algorithms with respect to these metrics, and measure their robustness with respect to certain image defects. Our test images are computer-generated scores on which we apply various image deformations typically found in real-world data. In addition to modern western music notation, our test set also includes historic music notation such as mensural notation and lute tablature. Our general approach and evaluation methodology is not specific to staff removal but applicable to other segmentation problems as well.	algorithm;computer-generated holography;dental percussion procedure;emulator;expect;postscript;rasterisation;residual (numerical analysis);source code;test set;topological skeleton;vector graphics;biologic segmentation;bleomycin/doxorubicin/lomustine/mechlorethamine/vincristine protocol;notation	Christoph Dalitz;Michael Droettboom;Bastian Pranzas;Ichiro Fujinaga	2008	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2007.70749	skeletonization;computer vision;speech recognition;document processing;metric;computer science;artificial intelligence;machine learning;music;image segmentation;mensural notation;helium;segmentation;measurement;robustness	Vision	40.02336864351882	-68.5612290643885	54418
68ee4f1a71dac637a9167dfda074bd2e5bb13484	nonlinear phase portrait modeling of fingerprint orientation	fingerprint recognition polynomials noise robustness differential equations;orientation pattern nonlinear phase portrait modeling fingerprint orientation automatic fingerprint identification;phase portrait;image restoration;image denoising fingerprint identification image restoration;visual analysis;image denoising;fingerprint identification;nonlinear model;singular point	Fingerprint orientation is crucial for automatic fingerprint identification. However, recovery of orientation is still difficult especially in noisy region. A way to aid recovery of the orientation is to provide an orientation model. In this paper, an orientation model for the entire fingerprint orientation using high order phase portrait is suggested. Proper analysis of the orientation pattern at the singular point regions is provided. Then a low-order phase portrait near each of the singular point is added as constraint to the high-order phase portrait to provide accurate orientation modeling for the entire fingerprint image. The main advantage of the proposed approach is that the nonlinear model itself is able to model all type of fingerprint orientations completely. Experiments and visual analysis show the effectiveness of the proposed model.	algorithm;coefficient;experiment;fingerprint;hoc (programming language);nonlinear system;pattern language	Wei-Yun Yau;Jun Li;Han Wang	2004	ICARCV 2004 8th Control, Automation, Robotics and Vision Conference, 2004.	10.1109/ICARCV.2004.1469027	image restoration;fingerprint;computer vision;phase portrait;singular point of a curve;visual analytics;computer science;machine learning;pattern recognition;control theory;mathematics	Vision	49.41425611677167	-66.20840645501765	54562
8ad870702f5f925e442b910c76e836ff48f502ed	lung segmentation with improved graph cuts on chest ct images	image segmentation;computed tomography;lungs;lung segmentation lung image edge information sobel operator graph cuts energy function expectation maximization algorithm gaussian mixture model graph cuts algorithm lung disease chest ct images chest computed tomography images;gaussian mixture model;image edge detection;medical image processing computerised tomography diseases gaussian processes image segmentation lung;robustness;lungs image segmentation computed tomography image edge detection gaussian mixture model robustness	Lung segmentation is often performed as a preprocessing step on chest Computed Tomography (CT) images because it is important for identifying lung diseases in clinical evaluation. Hence, researches on lung segmentation have received much attention. In this paper, we propose a new lung segmentation method based on an improved graph cuts algorithm from the energy function. First, the lung CT images is modeled with Gaussian mixture models (GMMs), and the optimized distribution parameters can be obtained with expectation maximization (EM) algorithm. With that parameters, we can construct the improved regional penalty item in the graph cuts energy function. Second, considering the image edge information, the Sobel operator is adopted to detect and extract the lung image edges, and the lung image edges information is used to improve the boundary penalty item of graph cuts energy function. Finally, the improved energy function of graph cuts algorithm is obtained, then the corresponding graph is created, and lung is segmented with the minimum cut theory. The experiments demonstrate that the proposed method is very accurate and efficient for lung segmentation.	academy;ct scan;coefficient;cut (graph theory);expectation–maximization algorithm;experiment;image processing;mathematical optimization;minimum cut;mixture model;preprocessor;sobel operator;tomography	Shuangfeng Dai;Ke Lu;Jiyang Dong	2015	2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)	10.1109/ACPR.2015.7486502	computer vision;mathematical optimization;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	43.827500242110936	-74.32555453340868	54693
41b8bcf944a18bcbd6df534edd05d23d943c7b15	a viscous fluid model for multimodal non-rigid image registration using mutual information	root mean square difference;psi_mic;viscous fluid;non rigid image registration;mr imaging;non rigid registration;brain imaging;mutual information;ground truth;brain tissue	We propose a multimodal free-form registration algorithm based on maximization of mutual information. The warped image is modeled as a viscous fluid that deforms under the influence of forces derived from the gradient of the mutual information registration criterion. Parzen windowing is used to estimate the joint intensity probability of the images to be matched. The method is evaluated for non-rigid inter-subject registration of MR brain images. The accuracy of the method is verified using simulated multi-modal MR images with known ground truth deformation. The results show that the root mean square difference between the recovered and the ground truth deformation is smaller than 1 voxel. We illustrate the application of the method for atlas-based brain tissue segmentation in MR images in case of gross morphological differences between atlas and patient images.	atlases;cervical atlas;expectation–maximization algorithm;gradient;ground truth;image registration;kernel density estimation;mean squared error;modal logic;multimodal interaction;muscle rigidity;mutual information;numerous;patients;plant roots;small;voxel;biologic segmentation;registration - actclass	Emiliano D'Agostino;Frederik Maes;Dirk Vandermeulen;Paul Suetens	2002	Medical image analysis	10.1007/3-540-45787-9_68	computer vision;mathematical optimization;viscous liquid;ground truth;mathematics;mutual information;statistics;neuroimaging	Vision	43.518413449582006	-79.92624504949491	54754
786d20729a2c64a67270069ad65cc46335962455	cpm: a deformable model for shape recovery and segmentation based on charged particles	electrostatic field;coulomb force;deformable models shape electrostatics electrodynamics computational modeling image segmentation skeleton performance analysis active contours noise shaping;comparative analysis;image segmentation;automatic segmentation;shape recovery;electric field;image indexing;segmentation;indexing terms;skeleton;charged particle system;algorithms artificial intelligence computer simulation image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval models biological models statistical numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted stochastic processes;charged particles;electric fields;electrodynamics;image reconstruction;index terms deformable model;skeleton index terms deformable model charged particle system electrostatic field coulomb force segmentation shape recovery;noisy images shape recovery deformable model charged particle model image segmentation electrodynamics electrostatic field gradient magnitude image snake model repulsive coulomb forces skeleton computation active contour model;deformable model;active contour model;electric fields electrodynamics image segmentation image reconstruction	A novel, physically motivated deformable model for shape recovery and segmentation is presented. The model, referred to as the charged-particle model (CPM), is inspired by classical electrodynamics and is based on a simulation of charged particles moving in an electrostatic field. The charges are attracted towards the contours of the objects of interest by an electrostatic field, whose sources are computed based on the gradient-magnitude image. The electric field plays the same role as the potential forces in the snake model, while internal interactions are modeled by repulsive Coulomb forces. We demonstrate the flexibility and potential of the model in a wide variety of settings: shape recovery using manual initialization, automatic segmentation, and skeleton computation. We perform a comparative analysis of the proposed model with the active contour model and show that specific problems of the latter are surmounted by our model. The model is easily extendable to 3D and copes well with noisy images.	active contour model;charge (electrical);computation;cope brand of aspirin-caffeine;extensibility;gradient;inspiration function;interaction;ions;particle;physical object;qualitative comparative analysis;simulation;biologic segmentation	Andrei Jalba;Michael H. F. Wilkinson;Jos B. T. M. Roerdink	2004	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2004.84	active shape model;computer vision;simulation;computer science;electric field	Vision	46.761607282671726	-73.73469476968934	54815
7b12c1a72c03419587eded8e902bd84d2426e02a	segmentation of brain images using adaptive atlases with application to ventriculomegaly	sensitivity and specificity;brain;image enhancement;image interpretation computer assisted;magnetic resonance imaging;reproducibility of results;algorithms;pattern recognition automated;humans;subtraction technique;hydrocephalus;computer simulation;models anatomic	"""Segmentation of brain images often requires a statistical atlas for providing prior information about the spatial position of different structures. A major limitation of atlas-based segmentation algorithms is their deficiency in analyzing brains that have a large deviation from the population used in the construction of the atlas. We present an expectation-maximization framework based on a Dirichlet distribution to adapt a statistical atlas to the underlying subject. Our model combines anatomical priors with the subject's own anatomy, resulting in a subject specific atlas which we call an """"adaptive atlas"""". The generation of this adaptive atlas does not require the subject to have an anatomy similar to that of the atlas population, nor does it rely on the availability of an ensemble of similar images. The proposed method shows a significant improvement over current segmentation approaches when applied to subjects with severe ventriculomegaly, where the anatomy deviates significantly from the atlas population. Furthermore, high levels of accuracy are maintained when the method is applied to subjects with healthy anatomy."""		Navid Shiee;Pierre-Louis Bazin;Jennifer L. Cuzzocreo;Ari Blitz;Dzung L. Pham	2011	Information processing in medical imaging : proceedings of the ... conference	10.1007/978-3-642-22092-0_1	computer simulation;computer vision;speech recognition;radiology;computer science;artificial intelligence;magnetic resonance imaging	Vision	43.12210557475717	-78.56896761456774	55034
b456e8e9fdabb578a6c25f5326d43dab52f25113	medical image denoising via optimal implementation of non-local means on hybrid parallel architecture	parallel computing;high performance computing;image enhancement;image denoising	The Non-local means denoising filter has been established as gold standard for image denoising problem in general and particularly in medical imaging due to its efficiency. However, its computation time limited its applications in real world application, especially in medical imaging. In this paper, a distributed version on parallel hybrid architecture is proposed to solve the computation time problem and a new method to compute the filters' coefficients is also proposed, where we focused on the implementation and the enhancement of filters' parameters via taking the neighborhood of the current voxel more accurately into account. In terms of implementation, our key contribution consists in reducing the number of shared memory accesses. The different tests of the proposed method were performed on the brain-web database for different levels of noise. Performances and the sensitivity were quantified in terms of speedup, peak signal to noise ratio, execution time, the number of floating point operations. The obtained results demonstrate the efficiency of the proposed method. Moreover, the implementation is compared to that of other techniques, recently published in the literature.		Tuan-Anh Nguyen;Amir Nakib;Huy-Nam Nguyen	2016	Computer methods and programs in biomedicine	10.1016/j.cmpb.2016.02.002	mathematical optimization;supercomputer;simulation;computer science;theoretical computer science;machine learning	EDA	50.858683024099584	-78.39605201576926	55111
445155321052b9d5be2dc4c99e2ff768458fedb2	a global unimodal thresholding based on probabilistic reference maps for the segmentation of muscle images	connective tissue;segmentation;unimodal thresholding;muscle	A global probabilistic maps thresholding (PMT) method was applied to characterise intramuscular connective tissue (IMCT) distribution on images of muscle histological sections exhibiting unimodal histograms. Probabilistic reference maps were defined and then used to set-up thresholding rules, derived from linear combinations of parameters calculated from the intensity histogram of the images. This PMT method was objectively compared to Rosin’s unimodal thresholding algorithm (RT) and validated by a histochemical quantification of IMCT collagen. Morphometrical parameters of the IMCT (area, length and thickness of the extracted network) were determined for different muscles and used to quantify IMCT distribution differences. q 2006 Elsevier B.V. All rights reserved.	algorithm;delimiter;logical connective;map;morphometrics;resonance;thickness (graph theory);thresholding (image processing);unimodal thresholding	Laurence Sifre-Maunier;Richard G. Taylor;Philippe Berge;Joseph Culioli;Jean-Marie Bonny	2006	Image Vision Comput.	10.1016/j.imavis.2006.03.004	computer vision;muscle;connective tissue;pattern recognition;thresholding;segmentation	Vision	41.38153179225628	-75.27669238666562	55142
064cbb71959a556f49f116bcd02824962d0bb41e	surface parameterization using riemann surface structure	conformal structure;surface fitting computational geometry image segmentation;surface parameterization;image segmentation;surface structures shape mathematics biomedical imaging topology neuroimaging geometry grid computing conformal mapping brain;critical graph;riemann surface;computational geometry;surface fitting;riemann surface structure;curvilinear coordinate systems;conformal structures;surface geometry;constrained harmonic maps;surface topology;general methods;signal processing;human face surface surface parameterization riemann surface structure conformal structures curvilinear coordinate systems surface partition critical graph iso parametric curves surface topology surface geometry constrained harmonic maps surface similarity brain surface;harmonic map;surface similarity;human face surface;iso parametric curves;brain surface;parametric curve;surface partition;coordinate system	We propose a general method that parameterizes general surfaces with complex (possible branching) topology using Riemann surface structure. Rather than evolve the surface geometry to a plane or sphere, we instead use the fact that all orientable surfaces are Riemann surfaces and admit conformal structures, which induce special curvilinear coordinate systems on the surfaces. We can then automatically partition the surface using a critical graph that connects zero points in the global conformal structure on the surface. The trajectories of iso-parametric curves canonically partition a surface into patches. Each of these patches is either a topological disk or a cylinder and can be conformally mapped to a parallelogram by integrating a holomorphic I-form defined on the surface. The resulting surface subdivision and the parameterizations of the components are intrinsic and stable. For surfaces with similar topology and geometry, we show that the parameterization results are consistent and the subdivided surfaces can be matched to each other using constrained harmonic maps. The surface similarity can be measured by direct computation of distance between each pair of corresponding points on two surfaces. To illustrate the technique, we computed conformal structures for anatomical surfaces in MRI scans of the brain and human face surfaces. We found that the resulting parameterizations were consistent across subjects, even for branching structures such as the ventricles, which are otherwise difficult to parameterize. Our method provides a surface-based framework for statistical comparison of surfaces and for generating grids on surfaces for PDE-based signal processing	algorithm;brain mapping;computation;critical graph;cylinder-head-sector;distortion;facial recognition system;iterative closest point;map;mesh generation;signal processing;subdivision surface;transformational grammar;video post-processing	Yalin Wang;Xianfeng Gu;Kiralee M. Hayashi;Tony F. Chan;Paul M. Thompson;Shing-Tung Yau	2005	Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1	10.1109/ICCV.2005.233	mathematical analysis;topology;parametric equation;computational geometry;coordinate system;signal processing;harmonic map;mathematics;geometry;image segmentation;critical graph;riemann surface;constant-mean-curvature surface;riemann's minimal surface	Vision	46.4829192496034	-76.56815410789841	55251
04bcdff6dc26b367c099260100cf4eae7548d7cd	interactive segmentation based on iterative learning for multiple-feature fusion	active learning;journal article;parameter learning;conditional random field;interactive object segmentation	This paper proposes a novel interactive segmentation method based on conditional random field (CRF) model to utilize the location and color information contained in user input. The CRF is configured with the optimal weights between two features, which are the color Gaussian Mixture Model (GMM) and probability model of location information. To construct the CRF model, we propose a method to collect samples for the cuttraining tasks of learning the optimal weights on a single image's basis and updating the parameters of features. To refine the segmentation results iteratively, our method applies the active learning strategy to guide the process of CRF model updating or guide users to input minimal training data for training the optimal weights and updating the parameters of features. Experimental results show that the proposed method demonstrates qualitative and quantitative improvement compared with the state-of-the-art interactive segmentation methods. The proposed method is also a convenient tool for interactive object segmentation. HighlightsAn iterative interactive segmentation method is proposed.A strategy for learning CRF parameters on a single image's basis.A strategy for updating the parameters of features iteratively.The CRF model is updated adaptively using active learning strategy.Users are guided to input new scribbles to improve the segmentation accuracy.	iterative method	Lei Zhou;Yu Qiao;Yijun Li;Xiangjian He;Jie Yang	2014	Neurocomputing	10.1016/j.neucom.2013.12.026	computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;active learning;image segmentation;scale-space segmentation;conditional random field	Vision	47.269964150246864	-69.87128786877214	55373
52f7a832384fb8a8e244440bf427a4ac44babed3	bacterial foraging optimization based brain magnetic resonance image segmentation		Segmentation partitions an image into its constituent parts. It is essentially the pre-processing stage of image analysis and computer vision. In this work, T1 and T2 weighted brain magnetic resonance images are segmented using multilevel thresholding and bacterial foraging optimization (BFO) algorithm. The thresholds are obtained by maximizing the between class variance (multilevel Otsu method) of the image. The BFO algorithm is used to optimize the threshold searching process. The edges are then obtained from the thresholded image by comparing the intensity of each pixel with its eight connected neighbourhood. Post processing is performed to remove spurious responses in the segmented image. The proposed segmentation technique is evaluated using edge detector evaluation parameters such as figure of merit, Rand Index and variation of information. The proposed brain MR image segmentation technique outperforms the traditional edge detectors such as canny and sobel.	algorithm;basic formal ontology;canny edge detector;computer vision;edge detection;image analysis;image segmentation;mathematical optimization;otsu's method;pixel;preprocessor;rs-232;rand index;resonance;sensor;sobel operator;thresholding (image processing);variation of information	Abdul kayom Md Khairuzzaman	2016	CoRR		image texture;computer vision;range segmentation;image gradient;binary image;morphological gradient;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	42.987986290577396	-71.30171023082622	55377
4ce6927ca1d4968c7733d00536d475aaf25a6f25	energy minimization by \alpha -erosion for supervised texture segmentation			energy minimization	Karl Skretting;Kjersti Engan	2014		10.1007/978-3-319-11758-4_23	computer vision;machine learning;pattern recognition	Vision	41.895743164395576	-70.61840473909612	55442
b6b1aba7883036429c151234169a522c7f380299	fully automatic segmentation of ap pelvis x-rays via random forest regression and hierarchical sparse shape composition	image segmentation;shape model;610 medicine health;visual feature selection;biology;570 life sciences	Knowledge of landmarks and contours in anteroposterior AP pelvis X-rays is invaluable for computer aided diagnosis, hip surgery planning and image-guided interventions. This paper presents a fully automatic and robust approach for landmarking and segmentation of both pelvis and femur in a conventional AP X-ray. Our approach is based on random forest regression and hierarchical sparse shape composition. Experiments conducted on 436 clinical AP pelvis x-rays show that our approach achieves an average point-to-curve error around 1.3 mm for femur and 2.2 mm for pelvis, both with success rates around 98%. Compared to existing methods, our approach exhibits better performance in both the robustness and the accuracy.	random forest;sparse	Cheng Chen;Guoyan Zheng	2013		10.1007/978-3-642-40261-6_40	computer vision;simulation;computer science;machine learning;image segmentation	AI	41.396012101286196	-79.01975330124418	55600
b0fbe8519e0228b4746a26ad5cbf8962aec8ad9b	a reproducing kernel hilbert space approach for q-ball imaging	q ball imaging;radon transforms;kernel;gaussian process model;brain;funk radon transform;white matter;image processing;radon transform;phantoms;hilbert spaces;normal distribution;hilbert space biomedical image processing smoothing methods density functional theory transforms magnetic resonance imaging;smoothing method;nerve fibers;reproducing kernel hilbert space diffusion magnetic resonance mr imaging funk radon transform gaussian process model laplace beltrami operator;hilbert space;density functional theory;image interpretation;image enhancement;mr imaging;diffusion magnetic resonance mr imaging;smoothing methods;magnetic resonance;density function theory;medical image processing;reproducing kernel hilbert space;imaging;transforms;neurological;algorithms;humans;computer assisted;gaussian process;tikhonov regularization;gaussian process model reproducing kernel hilbert space approach q ball imaging diffusion magnetic resonance imaging white matter geometry living human brain orientational heterogeneity intravoxel fiber architecture funk radon transform spherical laplace beltrami operator tikhonov regularization framework;radon transforms biomedical mri brain hilbert spaces medical image processing;algorithms brain computer simulation diffusion magnetic resonance imaging humans image enhancement image interpretation computer assisted image processing computer assisted models neurological nerve fibers normal distribution phantoms imaging;computer simulation;models;human brain;noise;laplace beltrami operator;biomedical mri;diffusion magnetic resonance imaging	Diffusion magnetic resonance (MR) imaging has enabled us to reveal the white matter geometry in the living human brain. The Q-ball technique is widely used nowadays to recover the orientational heterogeneity of the intra-voxel fiber architecture. This article proposes to employ the Funk-Radon transform in a Hilbert space with a reproducing kernel derived from the spherical Laplace-Beltrami operator, thus generalizing previous approaches that assume a bandlimited diffusion signal. The function estimation problem is solved within a Tikhonov regularization framework, while a Gaussian process model allows for the selection of the smoothing parameter and the specification of confidence bands. Shortcomings of Q-ball imaging are discussed.	bandlimiting;bands;gaussian process;hilbert space;kernel;matrix regularization;normal statistical distribution;population parameter;process modeling;radon;resonance;smoothing (statistical technique);specification;tissue fiber;voxel;white matter	Enrico Kaden;Frithjof Kruggel	2011	IEEE Transactions on Medical Imaging	10.1109/TMI.2011.2157517	computer simulation;mathematical optimization;mathematical analysis;radiology;image processing;kernel principal component analysis;magnetic resonance imaging;reproducing kernel hilbert space;mathematics;geometry;density functional theory;statistics;hilbert space	Vision	50.78054194431746	-77.08841437081355	55670
f6fa6f913a4487bf9a426d75c2a0d9b3dcde9dcd	covariance-driven retinal image registration initialized from small sets of landmark correspondences	feature poor images covariance driven retinal image registration landmark correspondences automatic retinal image registration algorithm initial correspondences invariant indexing vasculature alignment estimated transformation parameters diseased eyes model orders high order transformation bootstrapping visible retinal changes detection disease progress treatment impact;eye;transformation model;parameter estimation eye image registration medical image processing diseases biomedical optical imaging;retina image registration diseases parameter estimation cameras eyes switches pixel computer science indexing;medical image processing;image registration;indexation;diseases;parameter estimation;biomedical optical imaging;retinal imaging;covariance matrix	An automatic retinal image registration algorithm would be an important tool for detecting visible changes in the retina caused by the progress of a disease or by the impact of a treatment. Developing such an algorithm is difficult, especially for feature-poor images of diseased eyes. In this paper, a new retinal image registration algorithm is described that bootstraps an estimate of the parameters of a high-order, inter-image transformation model based on just one or two initial retinal image landmark correspondences. Hypothesized sets of initial correspondences are obtained through invariant indexing. For each such set, an initial, low-order transformation covering a small image region is estimated. Sufficiently accurate initial estimates are gradually expanded to a high-order transformation that covers the entire retina using constraints generated by alignment of the vasculature. The expansion and switch in model orders is entirely driven by the covariance matrix of the estimated transformation parameters. The resulting algorithm registers images to accuracies of less than a pixel in just a few seconds.	algorithm;image registration;pixel;sensor	Chia-Ling Tsai;Charles V. Stewart;Badrinath Roysam;Howard L. Tanenbaum	2002		10.1109/ISBI.2002.1029261	computer vision;covariance matrix;image processing;image registration;mathematics;optics;estimation theory;statistics;computer graphics (images)	Vision	40.95203044518068	-75.64492054148073	55816
b9c04b17d883ae197044f4440d7d560f42734adf	automated method for measuring microscopic particle contamination in a fluid	edge detection;model based approach;particle contamination	An automated procedure for measuring the size and num- ber ofmicroscopic particles contaminating a fluid is described. The procedure, based on edge detection and region filling, exhibits substantially improved accuracy when compared to simple thresh- olding techniques, while being only slightly more complicated. The procedure is much less complex than model-based approaches and is shown to be more robust than simple threshold-based tech- niques in the presence of image blur caused by out-of-focus par- tides. A clever filling scheme is presented that efficiently segments particles from the background by first identifying pixels that are not particles. Results for two particle types and several focus conditions are given for comparison.	particle filter	Wei R. Chen;Keith A. Teague	1993	J. Electronic Imaging	10.1117/12.130162	computer vision;edge detection;computer science	Vision	50.45660393142066	-78.39937354456286	55839
741ddbb721b907a0f5a4c701fdad0ff4b4344d3a	efficient classifier generation and weighted voting for atlas-based segmentation: two small steps faster and closer to the combination oracle	brain;atlas os;statistical significance;atlas bone;optimum combining;upper bound;mr imaging;atlas hueso;mutual information;ground truth	Atlas-based segmentation has proven effective in multiple applications. Usually, several reference images are combined to create a representative average atlas image. Alternatively, a number of independent atlas images can be used, from which multiple segmentations of the image of interest are derived and later combined. One of the major drawbacks of this approach is its large computational burden caused by the high number of required registrations. To address this problem, we introduce One Registration, Multiple Segmentations (ORMS), a procedure to obtain multiple segmentations with a single online registration. This can be achieved by pre-computing intermediate transformations from the initial atlas images to an average image. We show that, compared to the usual approach, our method reduces time considerably with little or no loss in accuracy. On the other hand, optimum combination of these segmentations remains an unresolved problem. Different approaches have been adopted, but they are all far from the upper bound of any combination strategy. This is given by the Combination Oracle, which classifies a voxel correctly if any individual segmentation coincides with the ground truth. We present here a novel combination approach, based on weighting the different segmentations according to the mutual information between the test image and the atlas image after registration. We compare this method with other existing combination strategies using microscopic MR images of mouse brains, achieving statistically significant improvement in segmentation accuracy.	computation;ground truth;mutual information;precomputation;standard test image;voxel	Xabier Artaechevarria;Arrate Muñoz-Barrutia;Carlos Ortiz-de-Solórzano	2008		10.1117/12.769401	computer vision;ground truth;computer science;pattern recognition;data mining;statistical significance;mutual information;upper and lower bounds	Vision	41.805928536826215	-78.92393194724423	55944
d3bae38966e56deafb17ce48c7d075355c217d12	image segmentation for intensity inhomogeneity in presence of high noise		Automated segmentation of fine objects details in a given image is becoming of crucial interest in different imaging fields. In this paper, we propose a new variational level-set model for both global and interactive\selective segmentation tasks, which can deal with intensity inhomogeneity and the presence of noise. The proposed method maintains the same performance on clean and noisy vector-valued images. The model utilizes a combination of locally computed denoising constrained surface and a denoising fidelity term to ensure a fine segmentation of local and global features of a given image. A two-phase level-set formulation has been extended to a multi-phase formulation to successfully segment medical images of the human brain. Comparative experiments with state-of-the-art models show the advantages of the proposed method.	adams oliver syndrome;additive model;calculus of variations;correctness (computer science);discretization;edge detection;equivalent weight;experiment;finite difference;high-level programming language;image segmentation;iteration;latent class model;low insertion force;lymphocytic choriomeningitis virus;manuscripts;medical imaging;mental recall;noise reduction;numerous;physical object;population parameter;rewrite (programming);segmentation action;semiconductor industry;the matrix;tridiagonal matrix algorithm;two-phase locking;biologic segmentation	Haider Ali;Lavdie Rada;Noor Badshah	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2825101	computer vision;fidelity;image segmentation;artificial intelligence;noise reduction;segmentation;mathematics;pattern recognition	Vision	50.7692091952025	-73.89180468069496	55983
978637e2797cfb0602b0682058cd1e6eab843fc2	feature detection using oriented local energy for 3d confocal microscope images	feature detection;fourier transform;3d imaging;confocal microscopy	The ability to detect features within confocal microscope images is important for the interpretation and analysis of such data. Most detectors are gradient-based, and so are sensitive to noise, and fail to accurately locate some feature types that are important in confocal microscopy. The local energy feature detector developed by Morrone and Owens marks locations where there is maximal congruence of phase in the Fourier components of an image. Points of maximal phase congruency occur at all common feature types: step and roof edges, line features and Mach bands. A 3D implementation of the local energy feature detector, suitable for confocal microscope data, is presented. The detector computes local energy by convolving an image with oriented pairs of 3D filters that are 3D versions of Morlet wavelets. To increase the speed of the convolution, the filters are designed in frequency-space and multiplied by the image's Fourier transform. Results are presented for real confocal images and a synthetic 3D image volume. These results are compared with those from a 3D implementation of the Sobel edge detector.	feature detection (computer vision)	Chris Pudney;Peter Kovesi;Ben Robbins	1995		10.1007/3-540-60697-1_112	stereoscopy;fourier transform;computer science;confocal laser scanning microscopy;feature detection;scanning confocal electron microscopy	Vision	39.707284352748786	-72.47627528369703	55984
4bbc78a1407fe098f54c8acf9a740f409a266165	efficient sparse icp	lp minimization;3d scan alignment;digital geometry processing;surface registration;l p	The registration of two geometric surfaces is typically addressed using variants of the Iterative Closest Point (ICP) algorithm. The Sparse ICP method formulates the problem using sparsity-inducing norms, significantly improving the resilience of the registration process to large amounts of noise and outliers, but introduces a significant performance degradation. In this paper we first identify the reasons for this performance degradation and propose a hybrid optimization system that combines a Simulated Annealing search along with the standard Sparse ICP, in order to solve the underlying optimization problem more efficiently. We also provide several insights on how to further improve the overall efficiency by using a combination of approximate distance queries, parallel execution and uniform subsampling. The resulting method provides cumulative performance gain of more than one order of magnitude, as demonstrated through the registration of partially overlapping scans with various degrees of noise and outliers.	approximation algorithm;chroma subsampling;data structure;elegant degradation;graphics processing unit;iterative closest point;iterative method;mathematical optimization;maxima and minima;network switch;optimization problem;simulated annealing;sparse matrix	Pavlos Mavridis;Anthousis Andreadis;Georgios Papaioannou	2015	Computer Aided Geometric Design	10.1016/j.cagd.2015.03.022	mathematical optimization;theoretical computer science	Vision	53.23293730276355	-73.69574212474039	56083
13055d3a9c64aace891f593015ed0e392fbc9f90	non-rigid image registration by neural computation	neural computation;computer aided diagnosis;multilayer perceptrons;image fusion;correspondence problem;mixture of principal axes registration;multilayer perceptron;medical image analysis;relative entropy;non rigid image registration;hybrid approach;maximum likelihood estimate;medical image processing;interpolation method;image registration;probability distribution;medical diagnosis registration process piece wise fashion multiple rigid transforms neural computation registration methodology multi object principal axes multi layer perceptrons prostate cancer registration breast sequence analysis medical image fusion computer vision;error resilience;multi layer perceptron;sequence analysis;multilayer perceptrons image registration medical image processing;synthetic data;nonrigid registration;finite mixture model;change analysis;neural network;prostate cancer;finite mixture;control points selection	Non-rigid image registration is a prerequisite for many medical image analysis applications such as image fusion of multimodality images and quantitative change analysis of a temporal sequence in computer-aided diagnosis. By establishing the point correspondence of the extracted feature points, it is possible to recover the deformation using nonlinear interpolation methods. However, it may be very difficult to establish such correspondence at an initial stage when confronted with large and complex deformation. In this paper, a mixture of principal axes registration (mPAR) method is proposed to resolve the correspondence problem through a neural computational approach. The novel feature of mPAR is the alignment of two point sets without the need of establishing explicit point correspondence. Instead, it aligns the two point sets by minimizing the relative entropy between their probability distributions resulting in a maximum likelihood estimate of the transformation matrix. The registration process consists of two steps: (1) a finite mixture scheme to establish an improved point correspondence and (2) a multilayer perceptron neural network (MLP) to recover the nonlinear deformation. The neural computation for registration uses a committee machine to obtain a mixture of piece-wise rigid registrations, which gives a reliable point correspondence using multiple extracted objects in a finite mixture scheme. Then the MLP is used to determine the coefficients of a polynomial transform using extracted control points. We have applied our mPAR method to register synthetic data sets, surgical prostate models, and a temporal sequence of mammograms of a single patient. The experimental results show that mPAR not only improves the accuracy of the point correspondence but also results in a desirable error-resilience property for control point selection errors.	algorithm;artificial neural network;autostereogram;coefficient;committee machine;computation;control point (mathematics);correspondence problem;distortion;image analysis;image fusion;image registration;interpolation;kullback–leibler divergence;medical image computing;medical imaging;memory-level parallelism;multilayer perceptron;nonlinear system;polynomial;quad flat no-leads package;scheme;synthetic data;transformation matrix	Rujirutana Srikanchana;Jianhua Xuan;Matthew T. Freedman;Charles C. Nguyen;Yue Joseph Wang	2004	VLSI Signal Processing	10.1023/B:VLSI.0000027488.23703.ba	computer vision;computer science;machine learning;pattern recognition;multilayer perceptron;artificial neural network;statistics	Vision	43.72634166456382	-76.07914948177941	56108
f180ee61a3a874e223bf00718875fb2535e23634	nonnegative matrix factorization with bounded total variational regularization for face recognition	reconnaissance visage;metodo cuadrado menor;television;approximation l2;methode moindre carre;matrix factorization;mise a jour;image processing;least squares method;matriz no negativa;least square error;biometrie;biometrics;biometria;procesamiento imagen;non negative matrix;aproximacion l2;total variation regularization;discontinuous solution;traitement image;factorisation matricielle;actualizacion;automatic recognition;numerical scheme;face recognition;theoretical analysis;nonnegative matrix factorization;l2 approximation;pattern recognition;matrice non negative;total variation;reconnaissance forme;numerical experiment;reconocimiento patron;factorizacion matricial;updating;reconocimiento automatico;reconnaissance automatique	Nonnegative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of nonnegative data based on minimizing least square error (L2 norm). However it has been observed that the proper norm for images is the bounded total variation (TV) norm other than the L2 norm. The space of functions of bounded TV allows discontinuous solution and plays an important role in image processing. In this paper, we propose a new NMF model with bounded TV regularization for identifying discriminate representation of image patterns. We provide a simple update rule for computing the factorization and give supporting theoretical analysis. Finally, we perform a series of numerical experiments to show evidence of the good behavior of the numerical scheme.	algorithm;bounded variation;facial recognition system;non-negative matrix factorization;total variation denoising;variational principle;yang	Haiqing Yin;Hongwei Liu	2010	Pattern Recognition Letters	10.1016/j.patrec.2010.08.001	computer vision;mathematical optimization;image processing;computer science;machine learning;calculus;mathematics;television;matrix decomposition;total variation;least squares;non-negative matrix factorization;biometrics	Vision	52.56225816325638	-68.93446024541902	56125
af746525cc2413e459c826fdccea41ece6134282	vessel extraction under non-uniform illumination: a level set approach	contrast enhanced;vessel extraction active contours level set method local contrast enhancement medical image analysis nonuniform illumination;image motion analysis;active contour;image segmentation;angiogram segmentation vessel extraction nonuniform illumination level set based active contour image contrast synthetic images clinical angiograms local contrast enhancement medical image analysis;angiogram segmentation;level set approach;vessel extraction;level set;biomedical imaging;active contours;image contrast;medical image analysis;image enhancement;shape;image edge detection;non uniform illumination;algorithms angiography artificial intelligence blood vessels humans imaging three dimensional information storage and retrieval lighting pattern recognition automated radiographic image enhancement radiographic image interpretation computer assisted reproducibility of results sensitivity and specificity;feature extraction;medical image processing;solid modeling;synthetic images;clinical practice;statistics;clinical angiograms;medical image processing blood vessels diagnostic radiography feature extraction image enhancement image segmentation;lighting;level set based active contour;lighting level set active contours image segmentation shape solid modeling statistics biomedical imaging image edge detection image motion analysis;local contrast enhancement;level set method;article;diagnostic radiography;blood vessels;nonuniform illumination	Vessel extraction is one of the critical tasks in clinical practice. This communication presents a new approach for vessel extraction using a level-set-based active contour by defining a novel local term that takes local image contrast into account. The proposed model not only preserves the performance of the existing models on blurry images, but also overcomes their inability to handle nonuniform illumination. The efficacy of the approach is demonstrated with experiments involving both synthetic images and clinical angiograms.	active contour model;blood vessel tissue;experiment;synthetic intelligence;angiogram	K. W. Sum;Paul Y. S. Cheung	2008	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2007.896587	medical imaging;computer vision;feature extraction;shape;computer science;level set;pattern recognition;active contour model;lighting;mathematics;image segmentation;solid modeling;level set method;statistics;computer graphics (images)	Vision	40.638029067988825	-76.19450319852042	56181
e4d035dee44e514dc44bccfb0daf818323334c12	joint 3d cell segmentation and classification in the arabidopsis root using energy minimization and shape priors	biology computing;image segmentation;image classification;shape recognition;image segmentation shape training imaging minimization merging optimization;physiology;shape recognition biological techniques biology computing botany cellular biophysics image classification image segmentation physiology;arabidopsis root cell segmentation shape prior energy minimization;biological techniques;cell type 3d cell segmentation 3d cell classification arabidopsis root shape prior discrete energy minimization approach prior knowledge cue prior image cue;cellular biophysics;botany	This paper presents a discrete energy minimization approach to integrate different prior knowledge and image cues for simultaneous cell segmentation and classification. When there are multiple types of cells to segment, the segmentation of cells and the classification of the cell types are dependent on each other. The presented approach selects the optimal segmentations from hypotheses and infers the cell types in the same process. The approach is applied to the volumetric data of Arabidopsis roots.	energy minimization;statistical classification	Kun Liu;Thorsten L Schmidt;Thomas Blein;Jasmin Dürr;Klaus Palme;Olaf Ronneberger	2013	2013 IEEE 10th International Symposium on Biomedical Imaging	10.1109/ISBI.2013.6556502	computer vision;contextual image classification;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Visualization	43.55880303238046	-75.90808071832411	56344
d0ecdbc3fcf5d1266a789946ba41a52fe0579cba	sparse reproducing kernels for modeling fiber crossings in diffusion weighted imaging	probability biodiffusion biomedical mri gaussian processes image denoising image reconstruction image representation image resolution integration medical image processing;probability;image resolution;gaussian processes;biodiffusion;integration;image representation;image reconstruction;medical image processing;sparse representation diffusion weighted imaging high angular resolution diffusion imaging orientation probability distribution function opdf mathematical framework hardi spherical harmonics kernel formalism gaussian like quadratures image reconstructions angular resolution;image denoising;proceedings international;harmonic analysis kernel imaging image reconstruction approximation methods image resolution eigenvalues and eigenfunctions;biomedical mri	Using existing estimators found in High Angular Resolution Diffusion Imaging (HARDI), such as the Orientation Probability Distribution Function (OPDF) of Aganj et al., we develop a new mathematical framework for implementing HARDI. In contrast to traditional methods based on spherical harmonics, the framework is based on a reproducing kernel formalism combined with recently developed Gaussian-like quadratures for the sphere, which leads to an efficient and sparse representation for HARDI estimators. We demonstrate that the new framework results in reconstructions that are more robust to noise and have better angular resolution, compared to spherical harmonic based reconstructions.	angularjs;formal system;sparse approximation;sparse matrix	Cory Ahrens;Jennifer Nealy;Fernando Pérez;Stéfan van der Walt	2013	2013 IEEE 10th International Symposium on Biomedical Imaging	10.1109/ISBI.2013.6556568	iterative reconstruction;computer vision;mathematical optimization;image resolution;theoretical computer science;probability;gaussian process;mathematics;statistics	Vision	52.141496078459156	-75.94609955960249	56427
f2ffe03b9e90a631fcd0cf10c75bf48fe2249b0f	offset compensated baseline restoration and computationally efficient hybrid interpolation for the brain pet				Saeed Mian Qaisar	2018	Bio-Algorithms and Med-Systems	10.1515/bams-2018-0031	interpolation;computer vision;offset (computer science);artificial intelligence;computer science	Crypto	52.219305705396806	-76.96207346855937	56475
761a6960db9e48f08f8fa4c333d73f3615297644	an evolutionary and graph-based method for image segmentation	segment image;human visual perception;graph-based method;genetic algorithm;graph-based approach;fitness function;nearest neighbor;new concept;locus-based representation;image segmentation	A graph-based approach for image segmentation that employs genetic algorithms is proposed. An image is modeled as a weighted undirected graph, where nodes correspond to pixels, and edges connect similar pixels. A fitness function, that extends the normalized cut criterion, is employed, and a new concept of nearest neighbor, that takes into account not only the spatial location of a pixel, but also the affinity with the other pixels contained in the neighborhood, is defined. Because of the locus-based representation of individuals, the method is able to partition images without the need to set the number of segments beforehand. As experimental results show, our approach is able to segment images in a number of regions that well adhere to the human visual perception.	affinity analysis;fitness function;genetic algorithm;graph (discrete mathematics);image segmentation;locus;pixel	Alessia Amelio;Clara Pizzuti	2012		10.1007/978-3-642-32937-1_15	computer vision;machine learning;pattern recognition;mathematics;image segmentation;pixel connectivity;random walker algorithm;minimum spanning tree-based segmentation;non-local means	Vision	45.43383855515889	-68.69137582069226	56620
c7aa29e7e2d15afc1bdbf80736f883d4d73274b8	an independent active contours segmentation framework for bone micro-ct images	active contours;image segmentation;micro-ct;roi extraction;trabecular bone	Micro-CT is an imaging technique for small tissues and objects that is gaining increased popularity especially as a pre-clinical application. Nevertheless, there is no well-established micro-CT segmentation method, while typical procedures lack sophistication and frequently require a degree of manual intervention, leading to errors and subjective results. To address these issues, a novel segmentation framework, called Independent Active Contours Segmentation (IACS), is proposed in this paper. The proposed IACS is based on two autonomous modules, namely automatic ROI extraction and IAC Evolution, which segments the ROI image using multiple Active Contours that evolve simultaneously and independently of one another. The proposed method is applied on a Phantom dataset and on real datasets. It is tested against several established segmentation methods that include Adaptive Thresholding, Otsu Thresholding, Region Growing, Chan-Vese (CV) AC, Geodesic AC and Automatic Local Ratio-CV AC, both qualitatively and quantitatively. The results prove its superior performance in terms of object identification capability, accuracy and robustness, under normal circumstances and under four types of artificially introduced noise. These enhancements can lead to more reliable analysis, better diagnostic procedures and treatment evaluation of several bone-related pathologies, and to the facilitation and further advancement of bone research.	automated lip reading;autonomous robot;biological evolution;body tissue;bone tissue;ct scan;calcium-independent phospholipase a2;chan's algorithm;imaging techniques;imaging phantom;medical image;noise (electronics);otsu's method;pathology;phantom reference;phantoms, imaging;region growing;region of interest;silo (dataset);thresholding (image processing);x-ray microtomography;x-ray microtomography;biologic segmentation;chan-yu-bao-yuan-tang;diagnostic procedure;facilitation;fibrinogen otsu i, human	Vasileios Ch. Korfiatis;Simone Tassani;George K. Matsopoulos	2017	Computers in biology and medicine	10.1016/j.compbiomed.2017.06.016	computer science;robustness (computer science);artificial intelligence;computer vision;pattern recognition;geodesic;scale-space segmentation;imaging phantom;image segmentation;thresholding;region growing;segmentation	Vision	42.180278320214555	-73.90129750313456	56808
9573ce9c451051da66ffc79c6dc02610f320e537	2d and 3d deformable models with narrowband region energy	active contour;image segmentation;region energy;3d imaging;segmentation quality assessment 2d deformable models narrow band region energy 3d image segmentation 2d parametric contour 3d triangular mesh greedy algorithm edge based active model;active surface segmentation narrow band region energy active contour;narrow band;greedy algorithms;triangular mesh;active surface;segmentation;indexing terms;medical image;mesh generation greedy algorithms image segmentation;greedy algorithm;energy minimization;mesh generation;deformable model;deformable models narrowband image segmentation level set active contours minimization methods greedy algorithms quality assessment biomedical imaging bones	We introduce a narrow band region approach in explicit de-formable models for 2D and 3D image segmentation. Embedding a region term into the evolution process, we derive a general formulation which is applied both on a 2D parametric contour and a 3D triangular mesh. Evolution of deformable models is performed by means of energy minimization using the computationally efficient greedy algorithm. The use of a region energy related to the vicinity of the evolving surface overcomes limitations of edge-based active models while remaining time effective. Experiments with segmentation quality assessment are carried out on medical images.	algorithmic efficiency;computation;energy minimization;experiment;gradient;greedy algorithm;image segmentation;polygon mesh;triangle mesh;triangulated irregular network	Julien Mille;Romuald Boné;Pascal Makris;Hubert Cardot	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379091	computer vision;mathematical optimization;greedy algorithm;computer science;pattern recognition;mathematics	Vision	49.37440646448563	-71.49711788389666	56818
8f4653154606c2e3fa1b76e68fd1ab3dbfb62e21	automatic nacre thickness measurement of tahitian pearls		In this paper a methodology for an automatized measurement of the nacre thickness of Tahitian pearls is presented. An adapted snake approach as well as our own developed circle detection algorithm are implemented to extract the nacre boundaries out of X-ray images. The results are validated by experts currently performing manually the obligatory nacre thickness control for millions of Tahitian pearls that are exported each year. Equivalent articles propose methods suitable for round pearls, whereas this paper contains methods to evaluate the nacre profile of pearls independently of their shape. As the algorithms are not specifically parametrized for Tahitian pearls, the methods can be adapted for quality assessment of other pearls as well.	thickness (graph theory)	Martin Loesdau;Sébastien Chabrier;Alban Gabillon	2015		10.1007/978-3-319-20801-5_49	computer vision;artificial intelligence;computer science	ECom	39.38168259624529	-77.36393375260367	56922
348d4db4e48515b78125a28a76fbad4894ee8930	a subpixel target detection approach to hyperspectral image classification		Hyperspectral image classification faces various levels of difficulty due to the use of different types of hyperspectral image data. Recently, spectral–spatial approaches have been developed by jointly taking care of spectral and spatial information. This paper presents a completely different approach from a subpixel target detection view point. It implements four stage processes, a preprocessing stage, which uses band selection (BS) and nonlinear band expansion, referred to as BS-then-nonlinear expansion (BSNE), a detection stage, which implements constrained energy minimization (CEM) to produce subpixel target maps, and an iterative stage, which develops an iterative CEM (ICEM) by applying Gaussian filters to capture spatial information, and then feeding the Gaussian-filtered CEM-detection maps back to BSNE band images to reprocess CEM in an iterative manner. Finally, in the last stage Otsu’s method is applied to converting ICEM-detected real-valued maps to discrete values for classification. The entire process is called BSNE-ICEM. Experimental results demonstrate BSNE-ICEM, which has advantages over support vector machine-based approaches in many aspects, such as easy implementation, fewer parameters to be used, and better false classification and precision rates.	care-of address;computer vision;confusion matrix;energy minimization;feedback;iteration;iterative method;kernelization;map;nonlinear system;olap cube;otsu's method;pixel;preprocessor;subpixel rendering;support vector machine	Bai Xue;Chuanyan Yu;Yulei Wang;Meiping Song;Sen Li;Lin Wang;Hsian-Min Chen;Chein-I Chang	2017	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2017.2702197	support vector machine;mathematics;computer vision;spatial analysis;subpixel rendering;object detection;artificial intelligence;contextual image classification;hyperspectral imaging;pattern recognition	Vision	40.829330246953624	-67.09743782401463	57260
606eca50fd626ec3d0388e731829a5c3aa0d958a	road region segmentation of remote sensing images based on k-means and pcnn		Depend on K-means clustering algorithm has the characteristics of simplicity, fast and good clustering effect, and Dynamics and characteristics of synchronous pulse in pulse coupled model image segmentation, an image segmentation method based on combination of K-means and PCNN is proposed. The clustering results of K-means algorithm are used as the initial ignition value of PCNN. Then, the internal firing of neuron firing in PCNN model excites similar neurons to improve the segmentation of the region caused by the weak difference of gray value, and improve the effect of image segmentation. Finally, this method is applied to remote sensing image segmentation, and compared with the single K-means clustering method and the single PCNN model image segmentation method. The results show that the combination of K-means and PCNN method can effectively improve the quality of image segmentation.	algorithm;cluster analysis;excited state;image segmentation;k-means clustering;neuron;pulse-coupled networks	Xiaocui Yang;Meng Wanli	2018		10.1145/3195588.3195600	fuzzy set;k-means clustering;remote sensing;cluster analysis;image segmentation;segmentation;pulse (signal processing);computer science	Vision	43.657223118290844	-70.42487163164294	57351
e1084f298562aa7b5ded1b44333c10bc55cfb5ff	segmentation of volume images using a multiscale transform	multiscale segmentation;charge coupled image sensors;brain;spatial scales;image segmentation;information extraction;image segmentation data mining photometry video sequences biomedical imaging charge coupled devices charge coupled image sensors magnetic resonance imaging humans brain;biomedical nmr;nonlinear transform;homogeneity scales;biomedical imaging;video sequences;data mining;charge coupled devices;three dimensional;multiscale transform;volume image segmentation;well characterized regions;pyramid;nonlinear transformation;photometry;magnetic resonance;magnetic resonance imaging;magnetic resonance data;transforms;humans;three dimensional region detection;multiscale volumetric structure;intensity scales;video sequences volume image segmentation multiscale transform multiscale segmentation nonlinear transform well characterized regions spatial scales intensity scales three dimensional region detection pyramid homogeneity scales multiscale volumetric structure magnetic resonance data;image sequences	Thi s paper presents a new method for multiscale segmentation of volume images. The segmentation i s achieved using a recent nonlinear transform which leads to well-characterized regions at dif ferent spatial and intens i ty scales. T h e detected three-dimensional regions are closed and are homogeneous relative to their surround. A pyramid is generated containing the region information extracted across a range of homogeneity scales. The pyramid represents the multiscale volumetric structure. Experimental results are given for magnetic resonance data as well as video sequences.	algorithm;box counting;image segmentation;nonlinear system;region growing;resonance;synthetic data;voxel	Tod Courtney;Narendra Ahuja	1996		10.1109/ICPR.1996.546863	three-dimensional space;computer vision;photometry;computer science;magnetic resonance imaging;image segmentation;scale-space segmentation;pyramid	Vision	49.121118305742534	-66.88912552147643	57411
082c70c11e1303688184baf17fc236b38e1658e8	distance-based shape statistics	graph theory;graph laplacian technique;scalar product;statistics shape measurement laplace equations energy measurement;image processing;smooth approximation;statistical analysis approximation theory gradient methods graph theory image processing;shape measurement;approximation theory;laplace equations;statistical analysis;energy measurement;gradient descent;nonsupervised warpings;statistics;gradient methods;hausdorff distance;graph laplacian;distance based shape statistics;local minima;graph laplacian technique distance based shape statistics hausdorff distance smooth approximation nonsupervised warpings gradient descent	This article deals with statistics on sets of shapes. The approach is based on the Hausdorff distance between shapes, The choice of the Hausdorff distance between shapes is itself not fundamental since the same framework could be applied with another distance. We first define a smooth approximation of the Hausdorff distance and build non-supervised warpings between shapes by a gradient descent of the approximation. Local minima can be avoided by changing the scalar product in the tangent space of the shape being warped. When non-supervised warping fails, we present a way to guide the evolution with a small number of landmarks. Thanks to the warping fields, we can define the mean of a set of shapes and express statistics on them. Finally, we come back to the initial distance between shapes and use it to represent a set of shapes by a graph, which with the technique of graph Laplacian leads to a way of projecting shapes onto a low dimensional space	approximation;gradient descent;hausdorff dimension;laplacian matrix;maxima and minima;statistical shape analysis;supervised learning	Guillaume Charpiat;Olivier D. Faugeras;Renaud Keriven;Pierre Maurel	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1661428	gradient descent;hausdorff distance;combinatorics;topology;laplacian matrix;dot product;image processing;graph theory;maxima and minima;shape analysis;mathematics;geometry;statistics;approximation theory	Visualization	49.22765583302498	-72.2031446317025	57655
6120508ad3b0ab6cf84b24ee2a34bd68ab80d473	multiresolution segmentation of natural images: from linear to nonlinear scale-space representations	equation derivee partielle;partial di erential equation;equation non lineaire;ecuacion no lineal;partial differential equation;ecuacion derivada parcial;image segmentation;image processing;image resolution;linear scale space;visual front end;edge detection;metodo arborescente;procesamiento imagen;natural images;unsupervised segmentation;pde;trees mathematics;analyse multiresolution;hierarchical trees;image resolution image segmentation partial differential equations signal resolution nonlinear equations image edge detection gabor filters filtering energy resolution biomedical imaging;traitement image;image structure;scale space;non linear scale space;nonlinear scale space;partial differential equations;image representation;ecuacion difusion;segmentation image;image representation image segmentation trees mathematics image resolution edge detection partial differential equations;diffusion equation;partial differential equation pde;multiresolution;tree structured method;methode arborescente;multi resolution;non linear equation;equation diffusion;multiresolution analysis;diffusion;nonscalar data multiresolution segmentation natural images nonlinear scale space representation linear scale space representations tree pruning nonlinear partial differential equation diffusion equation;analisis multiresolucion;algorithms image enhancement image interpretation computer assisted linear models nonlinear dynamics pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted;lts2	In this paper, we introduce a framework that merges classical ideas borrowed from scale-space and multiresolution segmentation with nonlinear partial differential equations. A nonlinear scale-space stack is constructed by means of an appropriate diffusion equation. This stack is analyzed and a tree of coherent segments is constructed based on relationships between different scale layers. Pruning this tree proves to be a very efficient tool for unsupervised segmentation of different classes of images (e.g., natural, medical, etc.). This technique is light on the computational point of view and can be extended to nonscalar data in a straightforward manner.	class;coherence (physics);computation;nonlinear system;normal statistical distribution;numerical analysis;numerous;scale space;text simplification;unsupervised learning;anatomical layer;biologic segmentation	Ana Petrovic;Òscar Divorra Escoda;Pierre Vandergheynst	2004	IEEE Transactions on Image Processing	10.1109/TIP.2004.828431	computer vision;mathematical optimization;discrete mathematics;image processing;computer science;mathematics;scale-space segmentation;partial differential equation	Vision	52.35668079220004	-69.99063558109006	57991
5c6a50a50f94f16978f58d1f74e99b048005042a	high resolution non-rigid dense matching based on optimized sampling		A high resolution dense matching algorithm is presented for non-rigid image feature matching in the paper. For high resolution non-rigid images, telephoto lens is helpful in capturing fine scale features like cloth fold, pigmentation and skin pores. It brings us serious image noises which are less texture and bokeh, respectively. In order to avoid mismatch and non-uniform matching, we propose an optimized sampling method based on Gibbs dense sampling considering both texture feature similarity and spatial consistency. In the processing, first we extract connected image patches by triangulation among confidence matched point sets. Then our sampling method is executed in each connected image patch. We propose a judgment for matching points on the image patch boundary. Markov Random Field (MRF) model formulates the problem of dense matching as a Bayes decision task. Experiments are design to demonstrate the effective and efficiency of our method with active skin image data.	sampling (signal processing)	Qian Zhang;Changhe Tu	2017	Neurocomputing	10.1016/j.neucom.2016.07.076	computer vision;template matching;machine learning;pattern recognition;mathematics	Vision	47.0581986060629	-66.59345272379379	58035
773a41beb803045f51e2ef56ef1a3b3e9fc38e40	application of an enhanced fuzzy algorithm for mr brain tumor image segmentation	performance measure;aide diagnostic;systeme temps reel;quantization;fuzzy c mean;image numerique;brain;comparative analysis;digital image processing;image segmentation;image processing;magnetism;0705p;cancer;0130c;algoritmo borroso;real time;tumours;convergence rate;quantification;traitement image;brain tumor;algorithme;feature vector;encephale;magnetic resonance;fuzzy algorithm;tratamiento digital;segmentation image;imagen numerica;brain imaging;algorithme flou;tumeur;nmr imaging;algorithms;feature selection;imagerie rmn;digital processing;tumeur maligne;digital image;convergence time;4230v;resonance magnetique;traitement numerique;diagnostic aid;real time systems;ayuda diagnostica	Image segmentation is one of the significant digital image processing techniques commonly used in the medical field. One of the specific applications is tumor detection in abnormal Magnetic Resonance (MR) brain images. Fuzzy approaches are widely preferred for tumor segmentation which generally yields superior results in terms of accuracy. But most of the fuzzy algorithms suffer from the drawback of slow convergence rate which makes the system practically non-feasible. In this work, the application of modified Fuzzy C-means (FCM) algorithm to tackle the convergence problem is explored in the context of brain image segmentation. This modified FCM algorithm employs the concept of quantization to improve the convergence rate besides yielding excellent segmentation efficiency. This algorithm is experimented on real time abnormal MR brain images collected from the radiologists. A comprehensive feature vector is extracted from these images and used for the segmentation technique. An extensive feature selection process is performed which reduces the convergence time period and improve the segmentation efficiency. After segmentation, the tumor portion is extracted from the segmented image. Comparative analysis in terms of segmentation efficiency and convergence rate is performed between the conventional FCM and the modified FCM. Experimental results show superior results for the modified FCM algorithm in terms of the performance measures. Thus, this work highlights the application of the modified algorithm for brain tumor detection in abnormal MR brain images.	algorithm;digital image processing;feature selection;feature vector;fuzzy cognitive map;image segmentation;mask (computing);mathematical optimization;network performance;rate of convergence;resonance	D. Jude Hemanth;C. Kezi Selva Vijila;J. Anitha	2010		10.1117/12.852315	computer vision;computer science;artificial intelligence;segmentation-based object categorization;image segmentation;scale-space segmentation;algorithm	Vision	42.983284684546845	-72.41880265523068	58216
2927d17f1c4aa9ebcab3a601e35d70aa2bc84387	detecting mitochondria in fluorescence images	mitochondria;fluorescence image;fluorescence imaging;segmentation;otsu s method	In this paper, we propose a method to localize mitochondria in fluorescence images of axons. The proposed method first finds a moderate threshold to detect differently illuminated mitochondria in the image. A region based analysis is then performed to filter out noisy pixels that are detected during the thesholding process. The method can locate the mitochondria more effectively than the traditional methods. Experimental results also show a better performance of the proposed approach.	pixel	Mohammad Abdullah-Al-Wadud;Yoojin Chung	2009		10.1145/1655925.1656056	computer vision;mitochondrion;otsu's method;fluorescence-lifetime imaging microscopy;segmentation	Robotics	39.303620083657	-72.82285956381048	58466
d9012974df94dbd93f4b9dce96b6fffbf18c3b24	structure specific atlas generation and its application to pancreas segmentation from contrasted abdominal ct volumes		Patient-specific atlas is a key technology for the recognition of the human anatomy from 3D medical images. Automated recognition of the pancreas is one main issue for computer-assisted diagnosis and therapy systems in the abdomen because many diseases of the pancreas are not accompanied by noticeable symptoms. In patient-specific atlas generation, hierarchical and mosaicing methods have been proposed to cope with individual differences in the position, orientation, and shape of the pancreas. Even though segmentation accuracy was improved by these methods, it remains lower than for other abdominal organs, such as the liver and the kidneys. The location of the pancreas strongly correlates with the location of vasculature systems, especially the splenic vein. In this paper, we propose a new structure specific atlas generation method that considers the structural information in atlas generation. As for the structural information, we enhance the vasculature using a vesselness filter. Similar volumes in a training dataset with respect to the vasculature structure are selected and used for atlas generation. Using 150 cases of contrast-enhanced 3D abdominal CT volumes, our experiment improved the mis-segmentation of the surrounding organs or such soft tissues as the duodenum.		Kenichi Karasawa;Takayuki Kitasaka;Masahiro Oda;Yukitaka Nimura;Yuichiro Hayashi;Michitaka Fujiwara;Kazunari Misawa;Daniel Rueckert;Kensaku Mori	2015		10.1007/978-3-319-42016-5_5	radiology;nuclear medicine;anatomy	Vision	40.02734267759474	-80.04233674370015	58623
876ec5748f29d2d26bfd5c539bdf4012361f7e0a	improved watershed transform for medical image segmentation using prior information	sensitivity and specificity;biological tissues;brain;probability;white matter;image segmentation;mr images;imaging three dimensional;medical signal detection;filters;prior information;hospitals;biomedical imaging;models biological;indexing terms;improved watershed transform;signal processing computer assisted;medical image analysis;mr images improved watershed transform medical image segmentation oversegmentation noise sensitivity atlas registration knee cartilage gray matter segmentation white matter segmentation;image enhancement;algorithms brain cartilage computer simulation image enhancement image interpretation computer assisted imaging three dimensional knee joint magnetic resonance imaging models biological models statistical pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique;mr imaging;watershed transform;cartilage;morphological operations;knee cartilage;image interpretation computer assisted;tissue classification;medical image processing;gray matter segmentation;magnetic resonance imaging;image registration;reproducibility of results;surgery;gray matter;white matter segmentation;biomedical imaging image segmentation signal to noise ratio morphological operations surgery hospitals filters image analysis medical signal detection probability;models statistical;algorithms;image analysis;pattern recognition automated;subtraction technique;signal to noise ratio;knee joint;medical image segmentation;computer simulation;noise sensitivity;oversegmentation;image registration medical image processing biomedical mri image segmentation biological tissues brain;biomedical mri;atlas registration	The watershed transform has interesting properties that make it useful for many different image segmentation applications: it is simple and intuitive, can be parallelized, and always produces a complete division of the image. However, when applied to medical image analysis, it has important drawbacks (oversegmentation, sensitivity to noise, poor detection of thin or low signal to noise ratio structures). We present an improvement to the watershed transform that enables the introduction of prior information in its calculation. We propose to introduce this information via the use of a previous probability calculation. Furthermore, we introduce a method to combine the watershed transform and atlas registration, through the use of markers. We have applied our new algorithm to two challenging applications: knee cartilage and gray matter/white matter segmentation in MR images. Numerical validation of the results is provided, demonstrating the strength of the algorithm for medical image segmentation.	academic medical centers;algorithm;atlas autocode;atlases;authorization;cartilage;excretory function;gradient;gray matter;ieee xplore;image analysis;image segmentation;medical image computing;numerical method;parallel computing;phantoms, imaging;physical object;segmentation action;signal-to-noise ratio;simulation;simulators;topological skeleton;voxel;watershed (image processing);white matter;biologic segmentation;brain segmentation	Vicente Grau;Andrea J. U. Mewes;Mariano Alcañiz Raya;Ron Kikinis;Simon K. Warfield	2004	IEEE Transactions on Medical Imaging	10.1109/TMI.2004.824224	computer simulation;computer vision;image analysis;index term;radiology;watershed;computer science;image registration;magnetic resonance imaging;segmentation-based object categorization;probability;image segmentation;scale-space segmentation;signal-to-noise ratio;medical physics	Vision	40.553953619189215	-76.28409277286656	58676
80357039ede63929fbb1df2b2f3582a1b8d2005b	optimal median-type filtering under structural constraints	simulation ordinateur;image features;traitement signal;exponential distribution;restauration image;non linear filtering;image processing;median filter;gaussian processes;boolean functions;normal distribution;optimal filtering;impulse noise;filtrado no lineal;boolean function;statistical methods;image restoration;image enhancement optimal median type filtering structural constraints median type filters design maximum noise attenuation image features signal features optimal weighted median filters linear inequalities boolean function analytic closed form representation filter design iid noise removal simulations uniform distribution gaussian distribution double exponential distribution impulsive noise environment;traitement image;reduccion ruido;exponential distribution median filters filtering theory image enhancement signal processing boolean functions gaussian distribution gaussian processes normal distribution;restauracion imagen;filter design;image enhancement;methode statistique;signal processing;noise reduction;filtro mediano;reduction bruit;filtering boolean functions working environment noise design methodology signal design attenuation nonlinear filters constraint theory gaussian noise image enhancement;filtre median;computer simulation;gaussian distribution;filtering theory;median filters;filtrado optimo;filtrage non lineaire;filtrage optimal	This paper presents a method for the design of median-type filters that achieve the maximum noise attenuation under structural constraints imposed by the requirement of preserving certain signal or image features. As compared with the design of optimal weighted median (WM) filters that calls for solving a set of linear inequalities, this method is extremely simple yet general enough. The filter is obtained by modifying directly the Boolean function of a median filter, and an analytic, closed-form representation of its Boolean function can be obtained. Furthermore, it is proven theoretically that under the same set of structural constraints, the filter designed in this way will never do worse in removing i.i.d. noise with any distribution than the optimal WM filter-improvements as high as 41, 46, and 52% have been achieved in simulations in a 2-D case for the uniform, Gaussian, and double-exponential distributions, respectively. Also, the improvement in the impulsive noise environment is very significant, as is demonstrated by an image enhancement application.		Bing Zeng	1995	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.392334	normal distribution;computer simulation;median filter;computer vision;mathematical optimization;image processing;computer science;signal processing;mathematics;boolean function;statistics	Visualization	53.61504246937662	-66.87069489897927	58708
cdb10a52625089569b4f445e3974af1936d84bda	multi-scale non-local means with shape prior for enhancement of cell membrane images	optical microscopy biodiffusion biological techniques biology computing biomembranes cellular biophysics fluorescence genetics image denoising image enhancement image segmentation;shape image segmentation laplace equations noise image edge detection anisotropic magnetoresistance filtering;cell membrane image enhancement non local means multi scale representation shape prior;nonlocal image denoising methods multiscale nonlocal means shape prior image enhancement cell membrane images light sheet microscopy rapid optical sectioning whole chicken embryos fluorescent cell membrane label transgenic expression signal to noise ratios image segmentation discontinuous cell edges ms nlm sp periodicity anisotropic diffusion detail enhanced multiscale representation selective backward diffusion anisotropic forward diffusion cell specific features	Light sheet microscopy allows rapid optical sectioning of whole chicken embryos during early development, visualizing cells through transgenic expression of a fluorescent cell membrane label. Low signal-to-noise ratios and large variability in intensity make segmentation of these images difficult, often resulting in discontinuous cell edges. We propose a multi-scale non-local means with shape prior (MS-NLM-SP) method for image enhancement, which takes into account cell-specific characteristics, such as size, geometry, and periodicity. Investigating theoretical connections between our approach and anisotropic diffusion reveals that the first step, detail-enhanced multi-scale representation, corresponds to selective backward diffusion, while the second step of MS-NLM-SP resembles anisotropic forward diffusion. Experimental results show that incorporating cell-specific features into non-local image denoising methods improves subsequent segmentation.	anisotropic diffusion;image editing;netware loadable module;noise reduction;non-local means;quasiperiodicity;signal-to-noise ratio;spatial variability	Cheng-Jin Du;R Weslie Tyson;Emil Rozbicki;Cornells J. Weijer;Till Bretschneider	2014	2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2014.6867916	computer vision;mathematics;optics;anisotropic diffusion	Vision	47.54947292745281	-72.4546507773236	58747
023ad786f91e49f8e3d22fe66808b7f27c509a9f	segmentation and labelling of intra-operative laparoscopic images using structure from point cloud	three dimensional displays cameras surgery liver laparoscopes image segmentation labeling;scene understanding intra operative image segmentation minimally invasive surgery;intra operative image segmentation;minimally invasive surgery;scene understanding	We present in this paper an automatic method for segmenting and labelling the liver and its surrounding tissues in intraoperative laparoscopic images. The goal is to distinguish between the different structures that compose a intra-operative hepatic surgery scene to improve common registration tasks. Our segmentation method considers the scene as a 3D structured point cloud to exploit powerful informations such as curvature and normals, in addition to visual cues that permit to efficiently classify the scene. Experiments performed on challenging human hepatic surgery confirm that accurate segmentation and labelling are possible using our approach.	experiment;normal (geometry);point cloud	Nazim Haouchine;Stephane Cotin	2016	2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2016.7493224	computer vision;pathology;scale-space segmentation;surgery	Vision	40.29509376687913	-78.12103594535581	58910
f31eca130bf9c4ffad085b5877c5baaa98715293	potts model parameter estimation in bayesian segmentation of piecewise constant images	image segmentation;bayes methods;image segmentation estimation computational modeling bayes methods stochastic processes noise mathematical model;potts model bayes methods image sampling image segmentation markov processes monte carlo methods piecewise constant techniques;computational modeling;stochastic sampling bayes potts model normalizing constant unsupervised segmentation;estimation;stochastic processes;mathematical model;normalizing constant potts model parameter estimation piecewise constant image segmentation additive noise bayesian approach posterior law mcmc stochastic sampling gibbs algorithm;noise	The paper presents a method for estimating the parameter of a Potts model jointly with the unknowns of an image segmentation problem. The method addresses piecewise constant images degraded by additive noise. The proposed solution follows a Bayesian approach, that yields the posterior law for all the unknowns (labels, gray levels, noise level and Potts parameter). It is explored by means of MCMC stochastic sampling, more precisely, by Gibbs algorithm. The estimates are then computed from these samples. The estimation of the Potts parameter is challenging due to the intractable normalizing constant of the model. The proposed solution is based on pre-computing the value of this normalizing constant for different image dimensions and number of classes, this being the novelty of this paper. The segmentation results are as satisfying as those obtained when tuning the parameter by hand.	additive white gaussian noise;estimation theory;gibbs algorithm;grayscale;image segmentation;markov chain monte carlo;noise (electronics);potts model;precomputation;sampling (signal processing);utility functions on indivisible goods	Roxana-Gabriela Rosu;Jean-François Giovannelli;Audrey Giremus;Cornelia Paula Vacar	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178738	stochastic process;econometrics;estimation;noise;machine learning;mathematical model;mathematics;image segmentation;scale-space segmentation;computational model;statistics	Vision	51.93425782880605	-68.82741317868363	59022
f3c8439de032f3b0cb62940d55e2db811cae4de8	improved segmentation of abnormal cervical nuclei using a graph-search based approach	image segmentation;computer programming;cell biology	Reliable segmentation of abnormal nuclei in cervical cytology is of paramount importance in automation-assisted screening techniques. This paper presents a general method for improving the segmentation of abnormal nuclei using a graph-search based approach. More specifically, the proposed method focuses on the improvement of coarse (initial) segmentation. The improvement relies on a transform that maps round-like border in the Cartesian coordinate system into lines in the polar coordinate system. The costs consisting of nucleus-specific edge and region information are assigned to the nodes. The globally optimal path in the constructed graph is then identified by dynamic programming. We have tested the proposed method on abnormal nuclei from two cervical cell image datasets, Herlev and H&E stained liquid-based cytology (HELBC), and the comparative experiments with recent state-of-the-art approaches demonstrate the superior performance of the proposed method.	active galactic nucleus;dynamic programming;embedded system;experiment;loss function;map;maxima and minima	Ling Zhang;Shaoxiong Liu;Tianfu Wang;Siping Chen;Milan Sonka	2015		10.1117/12.2082856	computer vision;simulation;computer science;theoretical computer science;segmentation-based object categorization;computer programming;image segmentation;scale-space segmentation	Vision	40.45395580817125	-73.19322250963272	59161
ec81e3ff64919a776eec38300d7998b7ea7960c7	speckle suppression in medical ultrasound images through schur decomposition		A technique based on Schur decomposition to supress the multiplicative (speckle) noise from medical ultrasound images is presented in this study. An image which carries the speckle noise is divided into small overlapping segments, size of these segments depends on the nature of speckle carried by the image and a global covariance matrix is calculated for the whole image by averaging the covariances of all segments. The global covariance matrix is decomposed through Schur decomposition to obtain the orthogonal vectors. A subset of these orthogonal vectors that correspond to largest magnitudes of eigenvalues are selected to filter out the speckle noise from the image. The proposed approach is compared with four benchmark filtering techniques, homomorphic wavelet despeckling, Wiener, Frost and Gamma. Two types of simulated ultrasound images and five types of real ultrasound images of foetal neck, left kidney, right kidney, musculo skeletal nerve and lymph node are tested. The proposed approach performed maximum suppression of speckle noise in all types of the images with optimal resolution and edge detection. The despeckling performance of the proposed approach is even better compared with the benchmark schemes once the speckle noise is rough, which is usually the case for soft tissue. © The Institution of Engineering and Technology 2017.	medical ultrasound;zero suppression	Adil H. Khan;Jawad F. Al-Asad;Ghazanfar Latif	2018	IET Image Processing	10.1049/iet-ipr.2017.0411	wavelet;artificial intelligence;computer vision;covariance matrix;eigenvalues and eigenvectors;filter (signal processing);mathematics;speckle noise;schur decomposition;speckle pattern;pattern recognition;edge detection	Vision	40.325620184563036	-75.38830796592676	59163
a347948eeb689fa238e118566e834613f714c18e	smooth, volume-accurate material interface reconstruction	material interface reconstruction;topology;piecewise linear approximation;ocean temperature;reconstruction algorithms;material interface reconstruction method;segmentation;segmentation material interface reconstruction volume fractions embedded boundary active interfaces;surface reconstruction;data visualisation;volume fraction;smoothing methods;active interface model;data visualization reconstruction algorithms topology surface reconstruction mesh generation statistics piecewise linear approximation ocean temperature smoothing methods combustion;material boundary visualizations material interface reconstruction method volume fraction data interface topology active interface model;data visualization;statistics;interface model;volume fraction data;interface topology;active interfaces;smoothing methods data visualisation;material boundary visualizations;mesh generation;volume fractions;combustion;embedded boundary	A new material interface reconstruction method for volume fraction data is presented. Our method is comprised of two components: first, we generate initial interface topology; then, using a combination of smoothing and volumetric forces within an active interface model, we iteratively transform the initial material interfaces into high-quality surfaces that accurately approximate the problem's volume fractions. Unlike all previous work, our new method produces material interfaces that are smooth, continuous across cell boundaries, and segment cells into regions with proper volume. These properties are critical during visualization and analysis. Generating high-quality mesh representations of material interfaces is required for accurate calculations of interface statistics, and dramatically increases the utility of material boundary visualizations.	adaptive multi-rate audio codec;anatomy, regional;approximation algorithm;boundary element method;core dump;develop;dimensions;discretization;energy citations database;ephrin type-b receptor 1, human;gerris;imagery;interface device component;iterative method;laplacian smoothing;leukemic hematopoietic stem cell;linear algebra;mathematical optimization;mesh networking;numerous;open-source software;optimal design;parallel computing;piecewise linear continuation;polygon mesh;population parameter;scientific visualization;semi-continuity;silo (dataset);simulation;smoothing (statistical technique);solutions;solver;time complexity;topology control;uc browser;unstructured grid;visit;volume fraction;weight	John C. Anderson;Christoph Garth;Mark A. Duchaineau;Kenneth I. Joy	2010	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2010.17	mesh generation;sea surface temperature;mathematical optimization;surface reconstruction;computer science;theoretical computer science;mathematics;volume fraction;segmentation;data visualization;statistics;combustion	Visualization	48.976184371007584	-77.1218001868584	59291
672b631af9ffb11ae59ae6c226459d3da19e95dc	double markov random fields and bayesian image segmentation	bayes estimation;modelizacion;traitement signal;teledetection;evaluation performance;metodo estadistico;mcmc methods;chaine markov;cadena markov;metodo monte carlo;image segmentation;performance evaluation;image processing;bayesian statistics;bayesian approach;bayes methods;evaluacion prestacion;simulation;procesamiento imagen;methode monte carlo;model based approach;simulacion;statistical method;indexing terms;traitement image;journal article;markov random field;image texture;modelisation;estimacion bayes;campo aleatorio;markov chain monte carlo;methode statistique;signal processing;remote sensing;monte carlo method;segmentation image;teledeteccion;random processes;satellite image double markov random fields bayesian image segmentation markov chain monte carlo methods mcmc methods image texture hierarchical model simulation;markov random fields bayesian methods image segmentation satellites monte carlo methods statistics remote sensing image texture analysis digital images;simulation study;markov processes;digital simulation bayes methods image segmentation image texture random processes markov processes monte carlo methods;procesamiento senal;modeling;hierarchical model;monte carlo methods;digital simulation;champ aleatoire;estimation bayes;markov chain;random field	Markov random fields are used extensively in modelbased approaches to image segmentation and, under the Bayesian paradigm, are implemented through Markov chain Monte Carlo (MCMC) methods. In this paper, we describe a class of such models (the double Markov random field) for images composed of several textures, which we consider to be the natural hierarchical model for such a task. We show how several of the Bayesian approaches in the literature can be viewed as modifications of this model, made in order to make MCMC implementation possible. From a simulation study, conclusions are made concerning the performance of these modified models.	hierarchical database model;image segmentation;markov chain monte carlo;markov random field;monte carlo method;programming paradigm;simulation	Dina E. Melas;Simon P. Wilson	2002	IEEE Trans. Signal Processing	10.1109/78.978390	econometrics;markov chain;maximum-entropy markov model;markov kernel;variable-order bayesian network;markov chain monte carlo;markov property;image processing;computer science;signal processing;pattern recognition;mathematics;graphical model;markov process;markov model;hidden markov model;statistics;monte carlo method;variable-order markov model	Vision	51.36154700953421	-68.81542569557057	59399
c0568a668cfae0876117a4dd62ee5d2aaaf44fa0	active contour method with locally computed signed pressure force function: an application to brain mr image segmentation	medical image processing biomedical mri brain gaussian processes image segmentation;brain;image segmentation;local binary fitted;gaussian processes;image segmentation nonhomogeneous media image edge detection active contours level set computational modeling brain modeling;level set;active contours;segmentation;spf function;spf function active contour method local binary fitted local image fitted segmentation;brain modeling;computational modeling;nonhomogeneous media;level set function regularisation signed pressure force function brain mr image segmentation region based active contour method image local information local binary fitted lbf energy model spf function local fitted image lfi intensity inhomogeneity gaussian kernel;image edge detection;medical image processing;active contour method;biomedical mri;local image fitted	This paper presents a region-based active contour method that embeds both region and gradient information. In the proposed algorithm area term practices a new region-based signed pressure force (SPF) function which utilizes the image local information obtained using the local binary fitted (LBF) energy model. By introducing the SPF function based on local fitted image (LFI), the proposed model is able to segment images with intensity in homogeneities. A Gaussian kernel is used to regularize the level set function which not only regularizes it but also removes the need of computationally expensive re-initialization. The proposed segmentation algorithm is applied to synthetic and real images in order to demonstrate the accuracy, effectiveness, and robustness of the algorithm.	active contour model;algorithm;analysis of algorithms;contour line;file inclusion vulnerability;gaussian blur;gradient descent;image segmentation;synthetic intelligence	Farhan Akram;Jeong-Heon Kim;Kwang Nam Choi	2013	2013 Seventh International Conference on Image and Graphics	10.1109/ICIG.2013.37	computer vision;mathematical optimization;computer science;level set;pattern recognition;gaussian process;mathematics;image segmentation;computational model;segmentation	Vision	50.559050653925645	-71.66436309836367	59469
07733ef280fa86f3c25c2e146375935e5754e1e4	evaluation of different statistical shape models for segmentation of the left ventricular endocardium from magnetic resonance images	image segmentation;image segmentation three dimensional displays biomedical imaging magnetic resonance imaging motion segmentation image edge detection statistical analysis;statistical analysis biomedical mri echocardiography image segmentation medical image processing;biomedical imaging;motion segmentation;statistical analysis;bland altman analysis statistical shape models patient specific modeling magnetic resonance images left ventricular endocardium cardiac magnetic resonance 3d echocardiographic lv surfaces end diastolic frames end systolic frames linear correlation analysis;image edge detection;three dimensional displays;magnetic resonance imaging	Statistical shape models (SSMs) represent a powerful tool used in patient-specific modeling to segment medical images because they incorporate a-priori knowledge that guide the model during deformation. Our aim was to evaluate segmentation accuracy in terms of left ventricular (LV) volumes obtained using four different SSMs versus manual gold standard tracing on cardiac magnetic resonance (CMR) images. A database of 3D echocardiographic (3DE) LV surfaces obtained in 435 patients was used to generate four different SSMs, based on cardiac phase selection. Each model was scaled and deformed to detect LV endocardial contours in the end-diastolic (ED) and end-systolic (ES) frames of a CMR short-axis (SAX) stack for 15 patients with normal LV function. Linear correlation and Bland-Altman analyses versus gold-standard showed in all cases high correlation (r2>0.95), non-significant biases and narrow limits of agreement.	apache axis;inter-rater reliability;logical volume management;resonance;statistical model	Concetta Piazzese;Maria Chiara Carminati;Andrea Colombo;Rolf Krause;Mark Potse;Lynn Weinert;Gloria Tamborini;Mauro Pepi;Roberto M. Lang;Enrico G. Caiani	2015	2015 Computing in Cardiology Conference (CinC)	10.1109/CIC.2015.7408597	computer vision;radiology;mathematics;biological engineering	Vision	41.37095628335516	-80.09205698860436	59484
190fc0e9a39c3fee6a54a37ba856b4b880a393bd	hybrid retinal image registration	medical image processing blood vessels eye image registration image resolution image segmentation;vascular tree extraction;feature based image registration hybrid retinal image registration national institutes of health early treatment diabetic retinopathy study etdrs imaging protocol high resolution images homogeneous nonvascular region vascular tree extraction local entropy based thresholding technique zeroth order translation mutual information image quality assessment landmark points sampling points affine model quadratic model area based image registration;feature based image registration;eye;image segmentation;image resolution;vascular tree extraction area based registration feature based registration mutual information mi retinal image registration;binary image;high resolution images;zeroth order translation;quadratic model;retinal image registration;national institute of health;hybrid retinal image registration;higher order;algorithms artificial intelligence diabetic retinopathy fluorescein angiography image enhancement image interpretation computer assisted information storage and retrieval pattern recognition automated reproducibility of results retina retinal vessels sensitivity and specificity united states;national institutes of health;homogeneous nonvascular region;local entropy based thresholding technique;retina image registration diabetes retinopathy protocols data acquisition data mining mutual information image quality image sampling;medical image processing;image registration;mutual information mi;success rate;image quality assessment;area based image registration;biomedical image processing;landmark points;mutual information;high resolution imager;sampling points;affine model;visual system;retinal imaging;data acquisition;early treatment diabetic retinopathy study;blood vessels;feature based registration;area based registration;etdrs imaging protocol	This work studies retinal image registration in the context of the National Institutes of Health (NIH) Early Treatment Diabetic Retinopathy Study (ETDRS) standard. The ETDRS imaging protocol specifies seven fields of each retina and presents three major challenges for the image registration task. First, small overlaps between adjacent fields lead to inadequate landmark points for feature-based methods. Second, the non-uniform contrast/intensity distributions due to imperfect data acquisition will deteriorate the performance of area-based techniques. Third, high-resolution images contain large homogeneous nonvascular/texureless regions that weaken the capabilities of both feature-based and area-based techniques. In this work, we propose a hybrid retinal image registration approach for ETDRS images that effectively combines both area-based and feature-based methods. Four major steps are involved. First, the vascular tree is extracted by using an efficient local entropy-based thresholding technique. Next, zeroth-order translation is estimated by maximizing mutual information based on the binary image pair (area-based). Then image quality assessment regarding the ETDRS field definition is performed based on the translation model. If the image pair is accepted, higher-order transformations will be involved. Specifically, we use two types of features, landmark points and sampling points, for affine/quadratic model estimation. Three empirical conditions are derived experimentally to control the algorithm progress, so that we can achieve the lowest registration error and the highest success rate. Simulation results on 504 pairs of ETDRS images show the effectiveness and robustness of the proposed algorithm	algorithm;binary image;cerebrovascular disorders;data acquisition;diabetic neuropathies;experiment;extraction;image quality;image registration;image resolution;landmark point;mutual information;protocols documentation;quadratic equation;retina;retinal diseases;sampling (signal processing);simulation;thresholding (image processing);united states national institutes of health;cell transformation;registration - actclass	Thitiporn Chanwimaluang;Guoliang Fan;Stephen R. Fransen	2006	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2005.856859	computer vision;higher-order logic;image resolution;visual system;binary image;computer science;image registration;image segmentation;mutual information;data acquisition;computer graphics (images)	Vision	40.74220457028498	-77.39475910315709	59517
3345bc85fb1a844d029e55e578b9b2f5217fa1c5	recognition of pulmonary nodules in thoracic ct scans using 3-d deformable object models of different classes	pulmonary nodule;bayes theorem;maximum a posteriori estimation;ct scan;deformable objects;recognition of pulmonary nodules;thoracic computed tomography scans;three dimensional deformable object models	The present paper describes a novel recognition method of pulmonary nodules (i.e., cancer candidates) in thoracic computed tomography scans by use of three-dimensional spherical and cylindrical models that represent nodules and blood vessels, respectively. The anatomical validity of these object models and their fidelity to computed tomography scans are evaluated based on the Bayes theorem. The nodule recognition is employed by the maximum a posteriori estimation. The proposed method is applied to 26 actual computed tomography scans, and experimental results are shown.	ct scan;tomography	Hotaka Takizawa;Shinji Yamamoto;Tsuyoshi Shiina	2010	Algorithms	10.3390/a3020125	computer vision;computer science;maximum a posteriori estimation;mathematics;computed tomography;bayes' theorem;statistics	Vision	41.489890428746655	-77.75617745701584	59773
e4c05429bf026cbc2547f81359753a5a113a10fa	robust anatomical landmark detection with application to mr brain image registration	deformable registration;brain mri;anatomical landmark detection;random forest regression	Comparison of human brain MR images is often challenged by large inter-subject structural variability. To determine correspondences between MR brain images, most existing methods typically perform a local neighborhood search, based on certain morphological features. They are limited in two aspects: (1) pre-defined morphological features often have limited power in characterizing brain structures, thus leading to inaccurate correspondence detection, and (2) correspondence matching is often restricted within local small neighborhoods and fails to cater to images with large anatomical difference. To address these limitations, we propose a novel method to detect distinctive landmarks for effective correspondence matching. Specifically, we first annotate a group of landmarks in a large set of training MR brain images. Then, we use regression forest to simultaneously learn (1) the optimal sets of features to best characterize each landmark and (2) the non-linear mappings from the local patch appearances of image points to their 3D displacements towards each landmark. The learned regression forests are used as landmark detectors to predict the locations of these landmarks in new images. Because each detector is learned based on features that best distinguish the landmark from other points and also landmark detection is performed in the entire image domain, our method can address the limitations in conventional methods. The deformation field estimated based on the alignment of these detected landmarks can then be used as initialization for image registration. Experimental results show that our method is capable of providing good initialization even for the images with large deformation difference, thus improving registration accuracy.		Dong Han;Yaozong Gao;Guorong Wu;Pew-Thian Yap;Dinggang Shen	2015	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1016/j.compmedimag.2015.09.002	random forest;computer vision;computer science;machine learning;pattern recognition	Vision	42.24788096694973	-77.29722294303676	59787
8c15a28b22ce8d9a63a27e07a909cca6f9125a7e	a robust warping method for fingerprint matching	robustness fingerprint recognition deformable models nonlinear distortion optimization methods spline data mining skin fingers ansi standards;deformable fingerprint surface;spline;optimisation;convergence;convergent deformation pattern;thin plate spline warping fingerprint matching deformable fingerprint surface triangular mesh model convergent deformation pattern gradient based energy optimization;image matching;skin;ansi standards;gradient based energy optimization;fingerprint matching;triangular mesh;deformable models;data mining;energy function;nonlinear distortion;optimisation convergence fingerprint identification image matching mesh generation;energy optimization;fingerprint recognition;visual inspection;fingers;robustness;triangular mesh model;mesh generation;energy estimate;thin plate spline;warping;fingerprint identification;optimization methods	This paper presents a robust warping method for minutiae based fingerprint matching approaches. In this method, a deformable fingerprint surface is described using a triangular mesh model. For given two extracted minutiae sets and their correspondences, the proposed method constructs an energy function using a robust correspondence energy estimator and smoothness measuring of the mesh model. We obtain a convergent deformation pattern using an efficient gradient based energy optimization method. This energy optimization approach deals successfully with deformation errors caused by outliers, which are more difficult problems for the thin-plate spline (TPS) model. The proposed method is fast and the run-time performance is comparable with the method based on the TPS model. In the experiments, we provide a visual inspection of warping results on given correspondences and quantitative results using database.	algorithm;emoticon;enhanced entity–relationship model;experiment;fingerprint recognition;gradient;image warping;mathematical optimization;minutiae;nonlinear conjugate gradient method;nonlinear system;polygon mesh;receiver operating characteristic;thin plate spline;visual inspection	Dongjin Kwon;Il Dong Yun;Sang Uk Lee	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383391	image warping;spline;mesh generation;fingerprint;computer vision;mathematical optimization;nonlinear distortion;convergence;computer science;triangle mesh;mathematics;skin;thin plate spline;fingerprint recognition;robustness;visual inspection	Vision	45.73121291652974	-76.62188160020018	59891
099dc6bb7a2478db231f1835bf727534b01b7771	regional assessment of cardiac left ventricular myocardial function via mri statistical features	regional wall motion abnormality detection bhattacharyya coefficient image statistics linear discriminant analysis lda linear support vector machine lsvm magnetic resonance imaging mri;image segmentation motion segmentation myocardium magnetic resonance imaging standards shape accuracy;image segmentation;support vector machines;cardiology;image classification;overall classification accuracy regional assessment cardiac left ventricular myocardial function mri statistical features segmental regional left ventricle abnormality detection segmental regional left ventricle abnormality localization magnetic resonance imaging endocardial boundaries epicardial boundaries cardiac sequence real time machine learning approach image features segmental cardiac function minimum user input subject dataset blood segmental contraction linear discriminant analysis linear support vector machine classifier standard cardiac segments radiologists;support vector machines biomedical mri blood cardiology feature extraction image classification image segmentation image sequences learning artificial intelligence medical image processing muscle statistical analysis;statistical analysis;feature extraction;medical image processing;blood;learning artificial intelligence;muscle;biomedical mri;image sequences	Automating the detection and localization of segmental (regional) left ventricle (LV) abnormalities in magnetic resonance imaging (MRI) has recently sparked an impressive research effort, with promising performances and a breadth of techniques. However, despite such an effort, the problem is still acknowledged to be challenging, with much room for improvements in regard to accuracy. Furthermore, most of the existing techniques are labor intensive, requiring delineations of the endo- and/or epi-cardial boundaries in all frames of a cardiac sequence. The purpose of this study is to investigate a real-time machine-learning approach which uses some image features that can be easily computed, but that nevertheless correlate well with the segmental cardiac function. Starting from a minimum user input in only one frame in a subject dataset, we build for all the regional segments and all subsequent frames a set of statistical MRI features based on a measure of similarity between distributions. We demonstrate that, over a cardiac cycle, the statistical features are related to the proportion of blood within each segment. Therefore, they can characterize segmental contraction without the need for delineating the LV boundaries in all the frames. We first seek the optimal direction along which the proposed image features are most descriptive via a linear discriminant analysis. Then, using the results as inputs to a linear support vector machine classifier, we obtain an abnormality assessment of each of the standard cardiac segments in real-time. We report a comprehensive experimental evaluation of the proposed algorithm over 928 cardiac segments obtained from 58 subjects. Compared against ground-truth evaluations by experienced radiologists, the proposed algorithm performed competitively, with an overall classification accuracy of 86.09% and a kappa measure of 0.73.	anterior descending branch of left coronary artery;axis vertebra;cardiomyopathies;computation;congenital abnormality;description;evaluation;frame (physical object);genetic algorithm;ground truth;heart ventricle;left ventricular structure;linear discriminant analysis;local-density approximation;logical volume management;machine learning;magnetic resonance imaging;myocardium;optic axis of a crystal;performance;radiology;real-time clock;silo (dataset);support vector machine;synthetic intelligence;biologic segmentation	Mariam Afshin;Ismail Ben Ayed;Kumaradevan Punithakumar;Max W. K. Law;Ali Islam;Aashish Goela;Terry M. Peters;Shuo Li	2014	IEEE Transactions on Medical Imaging	10.1109/TMI.2013.2287793	support vector machine;computer vision;contextual image classification;muscle;feature extraction;computer science;machine learning;pattern recognition;image segmentation	Vision	39.652465102499114	-78.64627445744397	60075
d602d94c5d2fc9b6778e2bb74145455e4b6b65e3	connected filtering by graylevel classification through morphological histogram processing	set theory image classification filtering theory mathematical morphology;histograms;filtering;mathematical morphology;image segmentation;connected filter;computer graphics;color;morphological histogram processing;filters;image classification;morphological operation;histogram connected filtering graylevel classification morphological histogram processing connected filter image simplification flat zone graylevel reduction morphological operators;set theory;graylevel reduction;histogram;connected filtering;pixel;flat zone;statistics;pattern recognition;graylevel classification;filtering histograms filters pixel image segmentation statistics pattern recognition digital images computer graphics color;image simplification;morphological operators;digital images;filtering theory	The paper introduces a new connected filter which provides image simplification in terms of flat zone and graylevel reduction. This filtering consists of the application of morphological operators to the histogram. It is done because each object in the image has a significative distribution of graylevels in the histogram. In other words, it is enough to classify the distributions in the histogram for simplification of those objects. The article also shows some experimental results, where the proposed method is compared to other classical methods applied to flat zone and graylevel reduction.		Franklin César Flores;Roberto de Alencar Lotufo	2001		10.1109/SIBGRAPI.2001.963046	computer vision;histogram matching;machine learning;pattern recognition;balanced histogram thresholding;mathematics;adaptive histogram equalization;image histogram	Vision	43.74989497703701	-67.08201844093527	60149
3e3f7d39b59f67be97ecda8542c04ea141d6e944	performance assessment of mammography image segmentation algorithms	statistical validation;breast cancer detection;computer aided cancer detection;snake model;image segmentation;potential field model;cancer;maximum likelihood;mammogram segmentation;potential field;statistical validation performance assessment image segmentation mammogram segmentation computer aided cancer detection cad systems breast cancer detection malignant mammography images maximum likelihood modeling kinnard model active deformable contour model snake model potential field model standard model;standard model;statistical analysis;medical image processing;mammography image segmentation deformable models cancer detection performance analysis algorithm design and analysis breast cancer image databases delta sigma modulation maximum likelihood detection;statistics;kinnard model;biomedical image processing;active deformable contour model;mammography;maximum likelihood modeling;point of view;digital database for screening mammography;region growing;statistical analysis cancer diagnostic radiography image segmentation mammography medical image processing;early detection;cad systems;breast cancer;diagnostic radiography;performance assessment;malignant mammography images;biomedical x ray imaging	In this paper, we present a comprehensive validation analysis to evaluate the performance of three existing mammogram segmentation algorithms against manual segmentation results produced by two expert radiologists. These studies are especially important for the development of computer-aided cancer detection (CAD) systems, which will significantly help improve early detection of breast cancer. Three typical segmentation methods were implemented and applied to 50 malignant mammography images chosen from the University of South Florida's Digital Database for Screening Mammography (DDSM): (a) region growing combined with maximum likelihood modeling (Kinnard model), (b) an active deformable contour model (snake model), and (c) a standard potential field model (standard model). A comprehensive statistical validation protocol was applied to evaluate the computer and expert outlined segmentation results; both sets of results were examined from the inter- and intra-observer points of view. Experimental results are presented and discussed in this communication	algorithm;computer-aided design;emoticon;image segmentation;radiology;region growing;statistical model	Kenneth Byrd;Jianchao Zeng;Mohamed F. Chouikha	2005	34th Applied Imagery and Pattern Recognition Workshop (AIPR'05)	10.1109/AIPR.2005.39	computer vision;simulation;computer science;medical physics	Vision	39.88199265869748	-75.96232355758453	60358
e792f6cfa17939b07b1bfe4b6a40d4ac9a70cdf4	liver mri segmentation with edge-preserved intensity inhomogeneity correction		The accurate liver segmentation for MRI is challenging because of the intensity inhomogeneity. However, most existing intensity inhomogeneity correction sometimes leads to detail smoothing. In this paper, a novel model is proposed for liver segmentation based on the level set method with edge-preserved intensity inhomogeneity correction (EPIICLS). EPIICLS corrects the intensity inhomogeneity with the minimization of the local entropy. And the edge-preserving filter is applied to compensate the detail smoothing by cooperating with the original image. The intensity inhomogeneity correction works on the internal energy of the level set method, which efficiently makes the level set function automatically approximate the signed distance function. And the edge preservation works on the external energy function, which precisely drives the zero-level curve toward the liver boundaries. Experiment results show that the proposed model leads to better segmentation results.		Hui Liu;Pinpin Tang;Dongmei Guo;Haixia Liu;Yuanjie Zheng;Guo Dan	2018	Signal, Image and Video Processing	10.1007/s11760-017-1221-5	artificial intelligence;computer vision;level set;signed distance function;mathematics;internal energy;pattern recognition;smoothing;segmentation;level set method	Vision	47.201218841131926	-72.84834289846816	60525
b6b4ce1f0400e216185bd150684e67f6a160c99f	image segmentation via edge contour finding: a graph theoretic approach	graph theory;maximum flow;image segmentation;edge elements;image segmentation image edge detection pixel tree graphs biomedical image processing radiology signal processing image processing biomedical engineering petroleum;fast algorithm;pattern recognition;partially cut equivalent tree image segmentation edge contour finding graph theoretic approach pixels undirected adjacency graph linked vertices partitioning closed contours isolated strong edges;pattern recognition graph theory image segmentation;minimum cut	We present a novel graph theoretic approach for image segmentation. The pisels of the image under st,udy are represented by the vertices of an undirected adja.cency graph G. All neighboring pairs of pixels are linked bv arcs rvit.11 capacities assigned to reflect the st,rength of an “edge element” between the linked vertices. Segmentation is achieved by removing arcs corresponding t,o selected minimum cuts of 8 to form mutually exclusive subgraphs such t,liat, the largest inter-subgraph maximum flow is minimized. This is equivalent to partitioning the image using closed contours of edge elements, which consist, mostly of strong edges. This method is able to accurately locate region boundaries and at the same time rejects contours containing isolated strong edges. The minimuni cuts in G can be computed from a partially cut-equivalent t,ree of G . A fa.st, algorithm is developed for const,ructing part,ially equivalent, trees, which is able to handle very large graphs with several hundred thousand vertices.	algorithm;const (computer programming);graph (discrete mathematics);graph theory;image segmentation;maximum flow problem;pixel;vertex (geometry)	Zhenyu Wu;Richard M. Leahy	1992		10.1109/CVPR.1992.223127	loop;image texture;graph power;edge contraction;maximum flow problem;combinatorics;range segmentation;discrete mathematics;topological graph;minimum cut;multiple edges;graph bandwidth;level structure;computer science;graph theory;edge cover;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;path;minimum spanning tree-based segmentation;scale-space segmentation;complement graph;neighbourhood;strength of a graph;matching;connected-component labeling	ML	40.21774635261132	-71.28183304041936	60623
87eeb214a9c5f82bf89ca0b192cf908ff660fdab	construction of a 3d physically-based multi-object deformable model	vibration modes amplitude;patient diagnosis;3d image;shape variations;anatomical structure;deformable spherical mesh;image segmentation;mr images;intra variability;biological system modeling;brain models;patient image data;biomedical imaging;anatomical variability;deformable models;biology;3d image data sets;training set;brain modeling;karhunen loeve transforms;3d multi object deformable model;shape;statistical analysis;inter variability;deformable models anatomical structure brain modeling image segmentation biomedical imaging shape equations biological system modeling biology mesh generation;statistically learned deformable model;patient image data 3d multi object deformable model 3d probabilistic physically based deformable model intra variability inter variability 3d image data sets statistically learned deformable model spatial relationships object surfaces shape variations vibration modes amplitude deformable spherical mesh 3d image training set statistically constrained random vector karhunen loeve expansion 3d brain structures segmentation mr images anatomical variability 3d medical images;statistically constrained random vector;3d probabilistic physically based deformable model;spatial relationships;patient diagnosis biomedical mri karhunen loeve transforms statistical analysis brain models image segmentation;mesh generation;object surfaces;karhunen loeve expansion;3d brain structures segmentation;3d medical images;biomedical mri	This paper addresses the problem of describing the significant intra- and inter-variability of 3D deformable structures within 3D image data sets. In pursuing it, a 3D probabilistic physically based deformable model is defined. The statistically learned deformable model captures the spatial relationships between the different objects surfaces, together with their shape variations. The structures of interest in each volume are parameterized by the amplitudes of the vibration modes of a deformable spherical mesh. For a given 3D image in the training set, a vector containing the largest vibration modes describing the desired object is created. This random vector is statistically constrained by retaining the most significant variation modes of its Karhunen-Loeve (KL) expansion on the considered population. The surfaces of the modeled structures thus deform according to the variability observed in the training set. A preliminary application of a 3D multi-object model for the segmentation of 3D brain structures from MR images is presented.		Gloria Bueno;Christophoros Nikou;Olivier Musse;Fabrice Heitz;Jean-Paul Armspach	2000		10.1109/ICIP.2000.900946	spatial relation;medical imaging;stereoscopy;mesh generation;computer vision;training set;shape;computer science;machine learning;pattern recognition;mathematics;image segmentation	Vision	44.59238434076041	-76.93820017762448	60630
cd10ce1d6adb1fa6bbed3e773cb5be0c6e4d1201	steering second-order tensor voting by vote clustering	dti tensor voting spherical clustering;tensor voting;dti;spherical clustering;tensile stress diffusion tensor imaging iterative methods receivers brain modeling	Among the various diffusion MRI techniques, diffusion tensor imaging (DTI) is still most commonly used in clinical practice in order to investigate connectivity and fibre anatomy in the human brain. Besides its apparent advantages of a short acquisition time and noise robustness compared to other techniques, it suffers from its major weakness of assuming a single fibre model in each voxel. This constitutes a problem for DTI fibre tracking algorithms in regions with crossing fibres. Methods approaching this problem in a postprocessing step employ diffusion-like techniques to correct the directional information. We propose an extension of tensor voting in which information from voxels with a single fibre is used to infer orientation distributions in multi fibre voxels. The method is able to resolve multiple fibre orientations by clustering tensor votes instead of adding them up. Moreover, a new vote casting procedure is proposed which is appropriate even for small neighbourhoods. To account for the locality of DTI data, we use a small neighbourhood for distributing information at a time, but apply the algorithm iteratively to close larger gaps. The method shows promising results in both synthetic cases and for processing DTI-data of the human brain.	algorithm;cluster analysis;locality of reference;neighbourhood (graph theory);synthetic intelligence;voxel	Daniel Jörgens;Örjan Smedby;Rodrigo Moreno	2016	2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2016.7493492	computer vision;artificial intelligence;machine learning;mathematics	Vision	42.76183517840464	-76.78444094773567	60742
5820b1f6e1fe0ffdb357585b3e03a8428a8decea	adaptive fuzzy clustering algorithm with local information and markov random field for image segmentation		Fuzzy c-means (FCM) clustering as one of the clustering method is widely used in image segmentation field, but some methods based on FCM are unable to obtain satisfactory performance for image segmentation under intense noise condition. This paper presents a novel local spatial information based fuzzy c-means clustering and Markov random field method for image segmentation. In the method, a new dissimilarity function is proposed by using the prior relationship degree and local neighbor distances, which enhances its resistance to noise. And a novel prior probability approximation is considered with spatial Euclidean distance and the difference of the mean color level between the center pixel and its neighborhoods. Experiments over synthetic images, real-world images and brain MR images indicate that the proposed method obtains better segmentation performance, compared to the FCM extended methods.		Jialiang Hu;Ying Wen	2018		10.1007/978-3-030-04212-7_15	fuzzy logic;markov random field;prior probability;cluster analysis;pattern recognition;euclidean distance;fuzzy clustering;image segmentation;segmentation;computer science;artificial intelligence	Vision	43.975335748973244	-72.34696598267233	60752
2ade564e682169844b7779b1ae8322e70c0010f7	theoretical evaluation of the detectability of random lesions in bayesian emission reconstruction	resolution;processing;radiology and nuclear medicine;positrons;bayesian method;positron emission tomography;72 physics of elementary particles and fields;62 radiology and nuclear medicine;theoretical analysis;penalized maximum likelihood;life sciences;evaluation;nuclear medicine;physics of elementary particles and fields;filtered backprojection;signal to noise ratio;monte carlo;monte carlo simulation;tomography;elementary particles	Detecting cancerous lesion is an important task in positron emission tomography (PET). Bayesian methods based on the maximum a posteriori principle (also called penalized maximum likelihood methods) have been developed to deal with the low signal to noise ratio in the emission data. Similar to the filter cut-off frequency in the filtered backprojection method, the prior parameters in Bayesian reconstruction control the resolution and noise trade-off and hence affect detectability of lesions in reconstructed images. Bayesian reconstructions are difficult to analyze because the resolution and noise properties are nonlinear and object-dependent. Most research has been based on Monte Carlo simulations, which are very time consuming. Building on the recent progress on the theoretical analysis of image properties of statistical reconstructions and the development of numerical observers, here we develop a theoretical approach for fast computation of lesion detectability in Bayesian reconstruction. The results can be used to choose the optimum hyperparameter for the maximum lesion detectability. New in this work is the use of theoretical expressions that explicitly model the statistical variation of the lesion and background without assuming that the object variation is (locally) stationary. The theoretical results are validated using Monte Carlo simulations. The comparisons show good agreement between the theoretical predications and the Monte Carlo results.	bayesian network;ct scan;computation;filtered backprojection;microscopy, energy-filtering transmission electron;monte carlo method;nonlinear system;numerical analysis;positron-emission tomography;positrons;signal-to-noise ratio;simulation;stationary process;tomography, emission-computed;observers	Jinyi Qi	2003	Information processing in medical imaging : proceedings of the ... conference	10.1007/978-3-540-45087-0_30	econometrics;computer science;mathematics;tomography;nuclear medicine;statistics;monte carlo method	ML	52.91393237810066	-76.11972685929551	60781
96fb36eceda31765af7cc4e4cbfa4bdcf2a5cd53	propagating uncertainties in statistical model based shape prediction	databases;reliability;statistical shape model;confidence region;accuracy assessment;statistical models;statistical model;statistical shape models;shape reconstruction;field of view;surgery;partial observation;cross validation;shape prediction;confidence regions;gaussian distribution	This paper addresses the question of accuracy assessment and confidence regions estimation in statistical model based shape prediction. Shape prediction consists in estimating the shape of an organ based on a partial observation, due e.g. to a limited field of view or poorly contrasted images, and generally requires a statistical model. However, such predictions can be impaired by several sources of uncertainty, in particular the presence of noise in the observation, limited correlations between the predictors and the shape to predict, as well as limitations of the statistical shape model – in particular the number of training samples. We propose a framework which takes these into account and derives confidence regions around the predicted shape. Our method relies on the construction of two separate statistical shape models, for the predictors and for the unseen parts, and exploits the correlations between them assuming a joint Gaussian distribution. Limitations of the models are taken into account by jointly optimizing the prediction and minimizing the shape reconstruction error through cross-validation. An application to the prediction of the shape of the proximal part of the human tibia given the shape of the distal femur is proposed, as well as the evaluation of the reliability of the estimated confidence regions, using a database of 184 samples. Potential applications are reconstructive surgery, e.g. to assess whether an implant fits in a range of acceptable shapes, or functional neurosurgery when the target’s position is not directly visible and needs to be inferred from nearby visible structures.	cross-validation (statistics);fits;shape context;software framework;statistical model;statistical shape analysis;tibia	Ekaterina Syrkina;Rémi Blanc;Gábor Székely	2011		10.1117/12.877960	active shape model;statistical model;shape of the distribution;point distribution model;pattern recognition	Vision	43.69200744205382	-77.79623474412678	60817
965208c9616d047cda0a3569b9a819b1a9452681	evaluation of various wavelet bases for use in wavelet-based multiresolution expectation maximization image reconstruction algorithm for pet	transformation ondelette;noise estimation;expectation maximization algorithms;algorithm performance;systeme nerveux central;maximum likelihood;maximization;objet test;maximum vraisemblance;pet imaging;reconstruction algorithms;hombre;image restoration;tomocentelleografia;encefalo;analyse multiresolution;wavelet transforms;reconstruction image;sistema nervioso central;wavelet transform;expectation maximization;encephale;reconstruccion imagen;exploration radioisotopique;resultado algoritmo;image reconstruction;emission tomography;human;positron;performance algorithme;radionuclide study;expectation;transformacion ondita;reconstruction algorithm;positon;exploracion radioisotopica;multiresolution analysis;objeto prueba;test object;maximizacion;wavelets;central nervous system;maxima verosimilitud;tomoscintigraphie;wavelet transformation;expectacion;analisis multiresolucion;maximisation;homme;brain vertebrata	Maximum Likelihood (ML) estimation based Expectation Maximization (EM) reconstruction algorithm has shown to provide good quality reconstruction for PET. Our previous work introduced the multigrid EM (MGEM) and multiresolution (MREM) and Wavelet based Multiresolution EM (WMREM) algorithm for PET image reconstruction. This paper investigates the use of various wavelets in the new Wavelet based Multiresolution EM (WMREM) algorithm. The wavelets are used to construct a multiresolution data space, which is then used in the estimation process. The beauty of the wavelet transform to provide localized frequency-space representation of the data allows us to perform the estimation using these decomposed components. The advantage of this method lies with the fact that the noise in the acquired data becomes localized in the high-high or diagonal frequency bands and not using these bands for estimation at coarser resolution helps speed up the recovery of various frequency components with reduced noise estimation. Different wavelet bases result in different reconstructions. Custom wavelets are designed for the reconstruction process and these wavelets provide better results than the commonly known wavelets. The WMREM reconstruction algorithm is implemented to reconstruct simulated phantom data and real data.© (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	expectation–maximization algorithm;iterative reconstruction;polyethylene terephthalate;wavelet	Amar Raheja;Atam P. Dhawan	2000		10.1117/12.387651	multiresolution analysis;wavelet;computer vision;mathematical optimization;mathematics;discrete wavelet transform;fast wavelet transform;gabor wavelet;statistics	Vision	51.66675608409399	-75.93113784179668	60980
73270ec8cd8193785c4b26762962138a4eb73233	stopping rules for active contour segmentation of ultrasound cardiac images	speckle;stochastic process;active contour;cardiac imaging;ultrasound;stopping rule;conceptual framework;energy function;ultrasound imaging;first order;stochastic processes;gradient descent;echocardiography;evolution strategy;ultrasonography;experimental evaluation;local minima	The presence of speckle (a spatial stochastic process in an ultrasound image) makes ultrasound segmentation difficult. Speckle introduces local minima in the MAP energy function of an active contour, and when evolving under gradient descent, the contour gets trapped in a spurious local minimum. In this paper, we propose an alternate technique for evolving a MAP active contour. The technique has two parts: a deterministic evolution strategy called tunneling descent which escapes from spurious local minima, and a stopping rule for terminating the evolution. The combination gives an algorithm that is robust and gives good segmentations. The algorithm also benefits from having only a few free parameters which do not require tweaking. We present the conceptual framework of the algorithm in this paper, and study the impact of different stopping rules on the performance of the algorithm. The algorithm is used to segment the endocardium in cardiac ultrasound images. We present segmentation results in this paper and an experimental evaluation of different stopping rules on the performance of the algorithm. Although the algorithm is presented as an ultrasound segmentation technique, in fact, it can be used to segment any first-order texture boundary.© (2005) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	active contour model	Zhong Tao;Hemant D. Tagare	2005		10.1117/12.595611	computer vision;simulation;engineering;artificial intelligence	Vision	45.93079256450827	-72.44258426220638	61001
994e15543e0d3fc0c54ec4b9d7c2e510d9f7d8c2	3d volume localization using miniatures	regression analysis image classification image matching image retrieval;human body atlas 3d volume localization image retrieval cad systems classifier regression histogram matching method;histograms three dimensional displays volume measurement accuracy feature extraction estimation shape	The prediction of the position of a given volume sample in a full body atlas, also known as a volume localization, is a part of an initial stage of image retrieval in most of the dedicated CAD systems. In this paper we present two methods for volume localization, namely histogram matching and classifier regression. Since the histogram matching method ignores the spatial orientation, it is used when the orientation of the volume cubes are not the same. On the other hand the classifier regression is much faster and can be used as a quick estimation and as a tool to reduce the scope of the initial problem. Both presented methods were tested on a dataset with 3962 volumes of a human body atlas. The accuracy and the speed of execution was compared and is presented in this paper.	computer-aided design;histogram matching;image retrieval;naive bayes classifier;olap cube	Luka Sajn;Miroslav Radojevic;Tomaz Dobravec	2014	2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2014.6859601	computer vision;pattern recognition;information retrieval	Vision	45.12717921864195	-75.16230277496693	61098
774880daf193a7ec4899a217131ddc7d46667a22	a method to measure foot print similarity for gait analysis	inverse discrete fourier transform;binary image;edge detection;image matching;patient rehabilitation;inverse discrete fourier transformation foot print pattern similarity measurement method gait analysis health care provider patient recovery patient rehabilitation patient foot print image edge detection noise reduction technique foot print image binarization;health care provider;foot image edge detection legged locomotion computer simulation computational modeling medical services pattern recognition analytical models pattern analysis smoothing methods;feature extraction;medical image processing;noise reduction;discrete fourier transform;patient recovery;gait analysis;patient treatment discrete fourier transforms edge detection feature extraction gait analysis health care image matching medical image processing patient rehabilitation;patient treatment;inverse discrete fourier transformation;noise reduction technique;patient foot print image edge detection;discrete fourier transforms;foot print pattern similarity measurement method;high frequency;foot print image binarization;health care	In Japan, many health care providers evaluate the recovery status of patients by observing a change in the patient's manner of walking. In the initial stage of rehabilitation, the patient's manner of walking is unstable. As the rehabilitation progresses, the manner of walking of the patients becomes stable. Therefore, changes in the foot print images of the patients may be detected. The techniques of noise-reduction, binarization of a foot print image, and erosion and dilation to smooth the edge of the binary image to detect the edge of the foot print image are described, In addition to these, discrete Fourier transformation to measure the similarity of the foot print patterns and inverse discrete Fourier transformation to remove high frequency components in the foot print pattern are described. Using these basic techniques, a method to estimate gait condition in examined	binary image;control theory;dilation (morphology);gait analysis	Tetsuzo Kuragano;Akira Yamaguchi;Susumu Furukawa	2005	International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)	10.1109/CIMCA.2005.1631569	computer vision;simulation;speech recognition;gait analysis;edge detection;binary image;feature extraction;computer science;discrete fourier transform;high frequency;noise reduction;health care	Robotics	40.745013370305166	-75.7073235966434	61216
48098616047669d581ca969b8984a4605e658518	a riemannian framework for linear and quadratic discriminant analysis on the tangent space of shapes		We present a Riemannian framework for linear and quadratic discriminant classification on the tangent plane of the shape space of curves. The shape space is infinite dimensional and is constructed out of square root velocity functions of curves. We introduce the idea of mean and covariance of shape-valued random variables and samples from a tangent space to the pre-shape space (invariant to translation and scaling) and then extend it to the full shape space (rotational invariance). The shape observations from the population are approximated by coefficients of a Fourier basis of the tangent space. The algorithms for linear and quadratic discriminant analysis are then defined using reduced dimensional features obtained by projecting the original shape observations on to the truncated Fourier basis. We show classification results on synthetic data and shapes of cortical sulci, corpus callosum curves, as well as facial midline curve profiles from patients with fetal alcohol syndrome (FAS).	approximation algorithm;basis function;body of uterus;coefficient;corpus callosum;fetal alcohol syndrome;image scaling;linear discriminant analysis;patients;quadratic classifier;synthetic data;velocity (software development)	Susovan Pal;Roger P. Woods;Suchit Panjiyar;Elizabeth R. Sowell;Katherine Narr;Shantanu H. Joshi	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.102	cotangent space;tangent cone;rotational invariance;tangent space;mathematical analysis;covariance;discriminant;basis function;tangent vector;mathematics	Vision	47.63321203650215	-77.0005159066383	61288
3af35b5521f38c01d70102e0bda4827d909e0730	solving mrf minimization by mirror descent		Markov Random Fields (MRF) minimization is a well-known problem in computer vision. We consider the augmented dual of the MRF minimization problem and develop a Mirror Descent algorithm based on weighted Entropy and Euclidean Projection. The augmented dual problem consists of maximizing a non-differentiable objective function subject to simplex and linear constraints. We analyze the convergence properties of the algorithm and sharpen its convergence rate. In addition, we also use the convergence analysis to identify an optimal stepsize strategy for weighted entropy projection and an adaptive stepsize strategy for weighted Euclidean projection. Experimental results on synthetic and vision problems demonstrate the effectiveness of our approach.	adaptive stepsize;algorithm;computer vision;descent;duality (optimization);entropy (information theory);loss function;markov chain;markov random field;optimization problem;rate of convergence;synthetic intelligence	Duy V. N. Luong;Panos Parpas;Daniel Rueckert;Berç Rustem	2012		10.1007/978-3-642-33179-4_56	computer science;adaptive stepsize;pattern recognition;rate of convergence;random field;artificial intelligence;duality (optimization);euclidean geometry;simplex;markov chain;convergence (routing)	ML	51.08391202007404	-72.64154909028711	61319
45a5ff0416f339e1c3ac2ab240ffe79d3c593363	sectored snakes: evaluating learned-energy segmentations	optimisation;snakes;learning sectored snakes energy minimising shapes trained deformable models image segmentation optimization ct images medical images;image segmentation;learning;edge detection;trained deformable models;segmentation evaluation;performance characterization;objective function;deformable models shape measurement image segmentation abdomen probability distribution image quality image sampling computed tomography minimax techniques performance evaluation;energy minimizing shapes;medical image processing;computerised tomography;ground truth;learning artificial intelligence;deformable model;optimisation medical image processing edge detection image segmentation learning artificial intelligence computerised tomography	t uVvxw uzy<{}| {}~xy+Y~|HWxy<{Qxa 0y0~yavM~ |Ly+~w >L}{}|Ly<{sJQ{Q{QuV+~yw|Ly<!~w| vLuB}YWuz}}~yvH| 0~V| {}~xyV  uz~| 0uWxy<{Qxa: | }uB}|LFxy!<|WxwH0~y| {}~xy^L<w!|Ly!|Ly^Wxw 0a{QuV ~w| vLu|Ly|LY}~V~{} {}au w|Ly^~y<{QuV:|LW{}~Luz_vx~0~yav{}auoWxw0a{QuV|Ly0uz~{}~yav,{}au}uz:{}V y!{}auuz L`w uz~V|L\~w| vLu Quz¡ auzyWuH|Ly0|L :~Va uH x ~¢Lu,{Q^w~y0~w~£Vu, w|Ly ~y<{QuV:|LW{}~xy <+{}| ¢Y~yav!|L-|Ly<{}| vLuL¤WxauV:uzyWu >uV{¥ uVuzy;yauz~vx>L:~yv+~w| vLuzM~y¦| Quz¡<uzyWuL!§&u!|L~w ̈{Ql©a{Q:|L:¢ «¬Q{Q:0W{}a}uzM}0:¦|LH:|L~y`}¢ ~yL Luz}Quz~y^Quz¡ auzyWuzL®­   ̄|Ly° ±l}V|Ly0|Ly^Quo{}auo:uz}{}~yvMWxy{QxaJ{Q}YWu|{Q:~|Lyvx| {Quz }a:F|LWuL 2vLx|L~y3W}uz| {}~yv;}LvL:uz}}~Lu ~LuV~}u ́ uV}u@μa¶ ̧·_{Q;0} 0Wu |;Wxw \uV{Qu Wxy<{Qxa+| {!0~o1Yuzo» uVLuzB|LVVa:|LWL μ1⁄21⁄4x·!{Q7}uz¡ ~:u |L+~{Q{}u QuV+~y{QuVJ|LW{}~xy3⁄4|L^>x}:~0uL >L{}l~yl{}au¬:Q{~w| vLuR|Ly ~y@WxyQuzV{}~Lu ~w| vLuzz|Ly μ1⁄2¿x· {Q;}YWu+|RF|LQ{|Ly0'|L| {}| 0u ́{Q x {}| {!V|Ly3Cu ́}uz ~y'w|Ly | 0~V| {}~xy ~{}+OuVÀw!|Ly<|L`|L ÁQ}{}w uzy{}z	snakes and ladders	Samuel D. Fenster;John R. Kender	2001	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.955115	computer vision;simulation;edge detection;ground truth;computer science;machine learning;image segmentation	Vision	43.44043774566477	-76.42781750070831	61343
d42ae245e33e87fd3247c0a33c7cd09f13a589d4	connectivity-based parcellation of the cortical mantle using q-ball diffusion imaging	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	This paper exploits the idea that each individual brain region has a specific connection profile to create parcellations of the cortical mantle using MR diffusion imaging. The parcellation is performed in two steps. First, the cortical mantle is split at a macroscopic level into 36 large gyri using a sulcus recognition system. Then, for each voxel of the cortex, a connection profile is computed using a probabilistic tractography framework. The tractography is performed from q fields using regularized particle trajectories. Fiber ODF are inferred from the q-balls using a sharpening process focusing the weight around the q-ball local maxima. A sophisticated mask of propagation computed from a T1-weighted image perfectly aligned with the diffusion data prevents the particles from crossing the cortical folds. During propagation, the particles father child particles in order to improve the sampling of the long fascicles. For each voxel, intersection of the particle trajectories with the gyri lead to a connectivity profile made up of only 36 connection strengths. These profiles are clustered on a gyrus by gyrus basis using a K-means approach including spatial regularization. The reproducibility of the results is studied for three subjects using spatial normalization.	alignment;cerebral cortex;diffusion anisotropy;fascicle - nerve fibers;groove;inference;intersection of set of elements;k-means clustering;mantle;maxima and minima;sampling (signal processing);software propagation;voxel	Muriel Perrin;Yann Cointepas;Arnaud Cachia;Cyril Poupon;Bertrand Thirion;Denis Rivière;Pascal Cathier;Vincent El Kouby;André Constantinesco;Denis Le Bihan;Jean-Francois Mangin	2008	International Journal of Biomedical Imaging	10.1155/2008/368406	biology;medical research;computer science;bioinformatics;artificial intelligence	Vision	48.22130986366612	-77.86275343219442	61607
51202b031f5965f3acd3f4c4687d2cfde0ef4515	multiresolution parameterization of meshes for improved surface-based registration	institutional repositories;brain;fedora;global convergence;vital;medical image analysis;smoothing;magnetic resonance imaging;medical imaging;image sequence;algorithms;multi resolution;vtls;ils	Common problems in medical image analysis involve surface-based registration. The applications range from atlas matching to tracking an object's boundary in an image sequence, or segmenting anatomical structures out of images. Most proposed solutions are based on deformable surface algorithms. The main problem of such methods is that the local accuracy of the matching must often be traded o against global smoothness of the surface in order to reach global convergence of the deformation process. Our contribution is to rst build a Multi-Resolution (M-R) surface from a reference segmented image, and then match this surface onto the target image in an M-R fashion using a deformable surface-like algorithm. As we proceed from lower to higher resolution, the smoothing e ect of the deformable surface is more and more localized, and the surface gets closer and closer to the target boundary. We present initial results of our algorithm for atlas registration onto brain MRI showing improved convergence and accuracy over classical deformable surface methods.	algorithm;atlas autocode;euclidean distance;image analysis;iterative closest point;local convergence;medical image computing;medical imaging;multiresolution analysis;reference model;smoothing	Sylvain Jaume;Matthieu Ferrant;Simon Keith Warfield;Benoit M. Macq	2001		10.1117/12.431137	computer vision;mathematical optimization;simulation;geography	Vision	45.66114423025012	-75.51923482131384	61666
59dde1f8178f738e45daabfae727765a55b69cc1	detection of protein spots from complex region on real gel image	gel electrophoresis;spot detection;watershed transform	Protein spot detection is an important step in gel image analysis. The results of spot detection may substantially influence gel image analysis. One of the most challenges of spot detection is separation of spots from complex region, where spots are overlapped and saturated. In this paper, we propose a spot detection algorithm which is capable of detection of spots from complex region. The proposed approach is realized by using European distance transform and centroid marker based watershed. Firstly, we apply a standard marker-controlled watershed algorithm to roughly segment the gel image into multiple regions. For those complex region containing several overlapped spots, we estimate spot centroids through a European distance transform. The estimated centroids are then served as spot markers in marker-controlled watershed algorithm to make a fine spot segmentation. Experimental results show the proposed method can obtain high detection performance in real gel images, and can accurately identify overlapped spots in complex region, when compared to other popular spot detection methods.	algorithm;distance transform;image analysis;watershed (image processing)	Cheng-li Sun;Yong Xu;Jie Jia;Yu He	2011		10.1007/978-3-642-24553-4_84	watershed;computer science;gel electrophoresis	Vision	39.37554783517914	-72.74446166895291	61882
233cd18f47a89501758b61d113a6fea20ef2f71b	tsallis entropy in fuzzy clustering: a multi-resolution approach	fuzzy clustering;tsallis entropy		fuzzy clustering;tsallis entropy	Pierre-André Dardignac;Claude C. Chibelushi;Michel Ménard;John S. Rees	2003			artificial intelligence;fuzzy clustering;machine learning;pattern recognition;computer science;tsallis entropy	ML	41.92131491328169	-70.71968106370602	61987
f468b40fb34bfeb285d11fa1d4e63a78bb70f4ee	a fast anisotropic mumford-shah functional based segmentation	mumford shah functional;image processing;binary image;energy function;medical image	Digital (binary) image segmentation is a critical step in most image processing protocols, especially in medical imaging where accurate and fast segmentation and classification are a challenging issue. In this paper we present a fast relaxation algorithm to minimize an anistropic Mumford-Shah energy functional for piecewise constant approximation of corrupted data. The algorithm is tested with synthetic phantoms and some CT images of the abdomen. Our results are finally compared with manual segmentations in order to validate the proposed model.		Juan Francisco Garamendi;Norberto Malpica;Emanuele Schiavi	2009		10.1007/978-3-642-02172-5_42	image texture;computer vision;mathematical optimization;binary image;image processing;computer science;theoretical computer science;segmentation-based object categorization;mathematics;image segmentation;scale-space segmentation	Vision	45.68938658836661	-74.4607920065702	62011
6d3b5ceb48073477a4a5c1b10fc1f233a42a6b8f	variational bayesian subgroup adaptive sparse component extraction for diagnostic imaging system		A novel unsupervised sparse component extraction algorithm is proposed for detecting micro defects while employing a thermography imaging system. The proposed approach is developed using the variational Bayesian framework. This enables a fully automated determination of the model parameters and bypasses the need for human intervention in manually selecting the appropriate image contrast frames. An internal subsparse grouping mechanism and adaptive fine-tuning strategy have been built to control the sparsity of the solution. The proposed algorithm is computationally affordable and yields a high-accuracy objective performance. Experimental tests on both artificial and natural defects have been conducted to verify the efficacy of the proposed method.	algorithm;calculus of variations;f1 score;flaw hypothesis methodology;interference (communication);medical imaging;pattern recognition;sensor;software bug;sparse matrix;variational principle	Bin Gao;Peng Lu;Wai Lok Woo;Gui Yun Tian;Yuyu Zhu;Martin Johnston	2018	IEEE Transactions on Industrial Electronics	10.1109/TIE.2018.2801809	control engineering;engineering;sparse matrix;principal component analysis;medical imaging;artificial intelligence;bayesian probability;pattern recognition;thermography	Robotics	53.09367661936742	-77.33208880685534	62090
282600c5609206ab23528a73b3bcef28ebaf2d2a	interactive registration of intracellular volumes with radial basis functions	intracellular volume;radial basis function;non rigid registration;least squares rbf;live cell imaging	We propose a novel approach to 3D image registration of intrac ellular volumes. The approach extends a standard image registration framework to the curved ce ll geometry. An intracellular volume is mapped onto another intracellular domain by using two pairs of point set surfaces approximating their nuclear and plasma membranes. The mapping function consists of t he a fine transformation, tetrahedral barycentric interpolation, and least-squares formula tion of radial basis functions for extracted cell geometry features. An interactive volume registration syste m is also developed based on our approach. We demonstrate that our approach is capable to create cell mode ls containing multiple organelles from observed data of living cells.	barycentric subdivision;cell (microprocessor);image registration;interpolation;lagrange polynomial;least squares;plasma active;radial (radio);radial basis function;simulation;yutaka yamamoto (mathematician)	Shin Yoshizawa;Satoko Takemoto;Miwa Takahashi;Makoto Muroi;Sayaka Kazami;Hiromi Miyoshi;Hideo Yokota	2010	International Journal of Computational Intelligence and Applications	10.1142/S1469026810002847	computer vision;mathematical optimization;radial basis function;computer science;machine learning;live cell imaging	Vision	46.528813984341205	-76.98208925437515	62232
60b8a23b198e8eef756987f1e5cba3133bc8727f	graph-based guide-wire segmentation through fusion of contrast-enhanced and fluoroscopic images	spectral methods;guide wire segmentation curvilinear structures discrete optimization spectral methods graph matching;contrast enhanced;discrete optimization;bottom up;image segmentation;image segmentation wires biomedical imaging joining processes blood vessels visualization image reconstruction;image fusion;medical image processing biomedical optical imaging blood blood vessels diagnostic radiography feature extraction graphs image enhancement image fusion image segmentation;wires;biomedical imaging;graph matching;blood vessel;graphs;multiple curvilinear structures graph based guide wire segmentation contrast enhanced fusion fluoroscopic images graph matching blood vessel segmentation contrast enhanced images local geometric primitive extraction;visualization;image enhancement;feature extraction;image reconstruction;medical image processing;blood;spectral method;joining processes;curvilinear structures;biomedical optical imaging;diagnostic radiography;blood vessels;guide wire segmentation	In this paper, we present a novel method that fuses, through a graph matching, segmentation of the blood vessels in contrast-enhanced images with segmentation of the guide-wires in the fluoroscopic images. This is achieved through a bottom up approach that first extracts local geometric primitives of interest in both images. Fusion between two graphs built with these primitives is performed through spectral matching and allows the definition of an improved criterion of ordering of the wire primitives. Given such criterion, local ordering is used towards reconstruction of multiple curvilinear structures that inherit visual support from both images. An evaluation performed on a broad variety of clinical situations validates the effectiveness of our approach.	matching (graph theory);top-down and bottom-up design	Nicolas Honnorat;Régis Vaillant;Nikos Paragios	2012	2012 9th IEEE International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2012.6235713	discrete optimization;computer vision;radiology;computer science;pattern recognition;mathematics;spectral method	Vision	42.39901762261322	-75.01692315075032	62255
4454abde59ad25e7b39dc2de01363711d55a3623	computation of discontinuous optical flow fields based on spatiotemporal bilateral filtering	bilateral filtering;optical flow estimation;energy function;smoothing;image sequence;optical flow;gauss seidel	In this study, we propose a new multi-frame method to compute a smooth and boundary preserving optical flow estimate. The approach is based on the bilateral filtering, a fast edge preserving smoothing tool. Inspired by the variational origin of the bilateral filter, we construct an energy functional that takes into account the image brightness conservation constraint and the bilateral smoothness of the optical flow. Minimization of this energy functional is performed using the Gauss Seidel iterations. Experimental results on synthetic and real image sequences demonstrate that the proposed approach yields an optical flow that is smooth and yet motion boundary preserving.© (2004) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	bilateral filter;computation;optical flow	Rosario El-Feghali;André Vincent	2004		10.1117/12.526837	mathematical optimization;theoretical computer science;calculus;mathematics	Vision	52.565142502721486	-71.54580405452735	62768
abc221a2254ee343ff3860346b78c8e7404bce8f	active contours driven by local image fitting energy	filtrage gauss;detection forme;active contour;image segmentation;image processing;edge detection;level set;procesamiento imagen;gaussian filtering;lbf model;boundary regularity;shape detection;filtrado gaussiano;traitement image;deteccion contorno;detection contour;accuracy;deteccion forma;precision;contorno activo;heterogeneidad;segmentation image;active contour models;contour actif;active contour model;heterogeneity;heterogeneite;chan vese c v model	A new region-based active contour model that embeds the image local information is proposed in this paper. By introducing the local image fitting (LIF) energy to extract the local image information, our model is able to segment images with intensity inhomogeneities. Moreover, a novel method based on Gaussian filtering for variational level set is proposed to regularize the level set function. It can not only ensure the smoothness of the level set function, but also eliminate the requirement of re-initialization, which is very computationally expensive. Experiments show that the proposed method achieves similar results to the LBF (local binary fitting) energy model but it is much more computationally efficient. In addition, our approach maintains the sub-pixel accuracy and boundary regularization properties. & 2009 Elsevier Ltd. All rights reserved.	active contour model;algorithmic efficiency;analysis of algorithms;calculus of variations;computational complexity theory;euler;euler–lagrange equation;expanded memory;gaussian elimination;gradient descent;low insertion force;maxima and minima;pixel;variational principle	Kaihua Zhang;Huihui Song;Lei Zhang	2010	Pattern Recognition	10.1016/j.patcog.2009.10.010	computer vision;image processing;computer science;pattern recognition;active contour model;mathematics;geometry;accuracy and precision	Vision	51.54611240417456	-71.03869404625178	62944
2821ed39fd50126d68f06f621af17be7f7efbc8f	partial difference equations over graphs: morphological processing of arbitrary discrete data	partial differential equation;mathematical morphology;high dimensionality;discrete data;image processing;difference equation;morphological operation;weighted graph	Mathematical Morphology (MM) offers a wide range of operators to address various image processing problems. These processing can be defined in terms of algebraic set or as partial differential equations (PDEs). In this paper, a novel approach is formalized as a framework of partial difference equations (PdEs) on weighted graphs. We introduce and analyze morphological operators in local and nonlocal configurations. Our framework recovers classical local algebraic and PDEs-based morphological methods in image processing context; generalizes them for nonlocal configurations and extends them to the treatment of any arbitrary discrete data that can be represented by a graph. It leads to considering a new field of application of MM processing: the case of highdimensional multivariate unorganized data.	aharonov–bohm effect;database;discrete mathematics;finite difference;image processing;lattice graph;linear algebra;mathematical morphology;noise reduction;nonlocal lagrangian;numerical partial differential equations;quantum nonlocality;recurrence relation;semantics (computer science)	Ta Vinh Thong;Abderrahim Elmoataz;Olivier Lézoray	2008		10.1007/978-3-540-88690-7_50	computer vision;mathematical optimization;combinatorics;discrete mathematics;mathematical morphology;image processing;computer science;mathematics;differential equation;partial differential equation	ML	52.6738017172813	-70.03859645061975	63031
2275510834ef2604ee66b60f76c81f491498a86a	spatial logics and model checking for medical imaging (extended version)		Recent research on spatial and spatio-temporal model checking provides novel image analysis methodologies, rooted in logical methods for topological spaces. Medical Imaging (MI) is a field where such methods show potential for ground-breaking innovation. Our starting point is SLCS, the Spatial Logic for Closure Spaces— Closure Spaces being a generalisation of topological spaces, covering also discrete space structures— and topochecker, a model-checker for SLCS (and extensions thereof). We introduce the logical language ImgQL (“Image Query Language”). ImgQL extends SLCS with logical operators describing distance and region similarity. The spatio-temporal model checker topochecker is correspondingly enhanced with state-of-the-art algorithms, borrowed from computational image processing, for efficient implementation of distancebased operators, namely distance transforms. Similarity between regions is defined by means of a statistical similarity operator, based on notions from statistical texture analysis. We illustrate our approach by means of two examples of analysis of Magnetic Resonance images: segmentation of glioblastoma and its oedema, and segmentation of rectal carcinoma. keywords: Spatial logics; Closure spaces; Model checking; Medical Imaging; Segmentation; Magnetic Resonance Imaging; Distance Transform; Statistical Texture Analysis ∗This work is partially supported by Azienda Ospedaliera Universitaria Senese. 1 ar X iv :1 81 1. 06 06 5v 1 [ cs .L O ] 1 4 N ov 2 01 8	algorithm;distance transform;image analysis;image processing;logical connective;medical imaging;model checking;query language;resonance	Fabrizio Banci Buonamici;Gina Belmonte;Vincenzo Ciancia;Diego Latella;Mieke Massink	2018	CoRR			Logic	44.22706869987017	-73.53932412501163	63084
c3ff55c8329418eb30e8438f4db31f2e1a06ae0d	unsupervised texture segmentation using resonance algorithm for natural scenes	unsupervised texture segmentation;threshold auto detection;unsupervised segmentation;texture segmentation;spatial relation;natural scenes;resonance algorithm	Many texture segmentation methods in the literature assume that the changes of intensity can be ascribed to the texture themselves. However, the real-world images may contain wide-ranged gradations in intensity which have nothing to do with local texture, such as those caused by the environment illuminations and cameras. To overcome the problem, an unsupervised texture segmentation method is proposed in this paper. Emphasizing the spatial relations between the adjacent texture pixels, the algorithm begins from a set of seed pixels and the texture region is generated by including those similar pixels. To suppress the noise in ̄uence, special attention is paid to the similarity criterion. Furthermore, to meet the requirement of unsupervised segmentation, the threshold in the similarity checking is automatically determined via iteratively applying the algorithm. The experimental results on Brodatz texture images and real-world images are presented. Ó 2000 Published by Elsevier Science B.V. All rights reserved.	algorithm;autoregressive model;cluster analysis;distortion;gradient;graph theory;k-means clustering;pixel;requirement;resonance;yet another;zero suppression	Hao He;Yan Qiu Chen	2000	Pattern Recognition Letters	10.1016/S0167-8655(00)00035-0	spatial relation;image texture;computer vision;machine learning;pattern recognition;mathematics;image segmentation;scale-space segmentation;texture compression;texture filtering	Vision	45.93334608120876	-66.43222602923103	63099
0822c17901cd9c1e8fbbcc40e58f3f8e1de1b4d8	a multi-task learning approach for compartmental model parameter estimation in dce-ct sequences		Today's follow-up of patients presenting abdominal tumors is generally performed through acquisition of dynamic sequences of contrast-enhanced CT. Estimating parameters of appropriate models of contrast intake diffusion through tissues should help characterizing the tumor physiology, but is impeded by the high level of noise inherent to the acquisition conditions. To improve the quality of estimation, we consider parameter estimation in voxels as a multi-task learning problem (one task per voxel) that takes advantage from the similarity between two tasks. We introduce a temporal similarity between tasks based on a robust distance between observed contrast-intake profiles of intensity. Using synthetic images, we compare multi-task learning using this temporal similarity, a spatial similarity and a single-task learning. The similarities based on temporal profiles are shown to bring significant improvements compared to the spatial one. Results on real CT sequences also confirm the relevance of the approach.	abdominal neoplasms;body tissue;computer multitasking;estimated;estimation theory;high-level programming language;multi-compartment model;multi-task learning;numerous;patients;plant extracts;population parameter;relevance;semantic similarity;synthetic intelligence;voxel;physiological aspects	Blandine Romain;Véronique Letort;Olivier Lucidarme;Laurence Rouet;Florence d'Alché-Buc	2013	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-40763-5_34	computer vision;computer science;machine learning;pattern recognition	Vision	43.9219388630353	-79.9974032054578	63314
f89172fcf277060ce943f4544dd622534001d0e6	evolving mean shift with adaptive bandwidth: a fast and noise robust approach	cluster algorithm;image segmentation;noisy data;mean shift;noise robustness;spike sorting;energy function;local features;em algorithm	This paper presents a novel nonparametric clustering algorithm called evolving mean shift (EMS) algorithm. The algorithm iteratively shrinks a dataset and generates well formed clusters in just a couple of iterations. An energy function is defined to characterize the compactness of a dataset and we prove that the energy converges to zero at an exponential rate. The EMS is insensitive to noise as it automatically handles noisy data at an early stage. The single but critical user parameter, i.e., the kernel bandwidth, of the mean shift clustering family is adaptively updated to accommodate the evolving data density and alleviate the contradiction between global and local features. The algorithm has been applied and tested with image segmentation and neural spike sorting, where the improved accuracy can be obtained at a much faster performance, as demonstrated both qualitatively and quantitatively.	algorithm;algorithmic efficiency;areal density (computer storage);bandwidth (signal processing);cluster analysis;image segmentation;iteration;mathematical optimization;mean shift;signal-to-noise ratio;sorting;time complexity	Qi Zhao;Zhi Yang;Hai Tao;Wentai Liu	2009		10.1007/978-3-642-12307-8_24	computer vision;mathematical optimization;mean-shift;expectation–maximization algorithm;computer science;theoretical computer science;machine learning;mathematics;image segmentation;statistics	ML	50.303106759318574	-71.27235302411734	63715
0db9c2a745ff824608c4aa0d6758e85d0c0684f3	deconvolution of high dimensional mixtures via boosting, with application to diffusion-weighted mri of human brain	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. The diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. Typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization [1, 2]. The difficulties inherent in modeling DWI data are shared by many other problems involving fitting non-parametric mixture models. Ekanadaham et al. [3] proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). Here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. Our algorithm uses the principles of L2-boost [4], together with refitting of the weights and pruning of the parameters. The addition of these steps to L2-boost both accelerates the algorithm and assures its accuracy. We refer to the resulting algorithm as elastic basis pursuit, or EBP, since it expands and contracts the active set of kernels as needed. We show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. In simulations of DWI, we find that EBP yields better parameter estimates than a non-negative least squares (NNLS) approach, or the standard model used in DWI, the tensor model, which serves as the basis for diffusion tensor imaging (DTI) [5]. We demonstrate the utility of the method in DWI data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers.	active set method;basis pursuit;bias–variance tradeoff;contract agreement;deconvolution;diffusion tensor imaging;diffusion weighted imaging;discretization error;estimated;fits;fascicle - nerve fibers;mixture model;non-negative least squares;optical fiber;population parameter;resonance;sample variance;simulation;sorting algorithm;tissue fiber;weight;white matter;x86	Charles Y. Zheng;Franco Pestilli;Ariel Rokem	2014	Advances in neural information processing systems		text mining;medical research;computer science;artificial intelligence;machine learning;data mining;mathematics;statistics	ML	49.25751649830454	-79.24567426189543	64052
83ea42938b37d809dd954128a2b5bbeebff04fd2	an ant-inspired algorithm for detection of image edge features	image features;comparative analysis;image processing;edge detection;ant colony system;ant colony algorithm;feature extraction;ant colony systems;quantitative evaluation;image edge analysis	This paper presents a technique inspired by swarm methodologies such as ant colony algorithms for processing simple and complicated images. It is shown that the proposed technique for image processing is capable of performing feature extraction for edge detection and segmentation, even in the presence of noise. Our proposed approach, Ant-based Correlation for Edge Detection (ACED), is tested on different samples and the results are compared to typical established non-swarm-based methods. The comparative analysis highlights the advantages of the proposed method which generates less distortion when noise is added to the test images. Both qualitative and quantitative evaluations support the claim, confirming the significance of our swarm-based method for image feature extraction and segmentation.		S. Ali Etemad;Tony White	2011	Appl. Soft Comput.	10.1016/j.asoc.2011.06.011	qualitative comparative analysis;computer vision;feature detection;ant colony optimization algorithms;edge detection;image processing;feature extraction;computer science;machine learning;pattern recognition;feature	Robotics	42.390207697369455	-68.62644387548872	64246
da2513f05c24a1abe47b3bf04c1d9fe02759569d	evaluation of segmentation algorithms in ct scanning	engineering;image segmentation;medical image processing computerised tomography image segmentation;feature recovery;segmentation;medical image processing;feature recovery segmentation evaluation;computerised tomography;nonuniform costs segmentation algorithm evaluation ct scanning machine segmented images systematic errors optimal operating ranges medical imaging multiobject feature recovery;image segmentation signal processing algorithms computed tomography systematics usa councils image edge detection security;evaluation	We developed a method to evaluate the accuracy of segmentation algorithms. Oversegmentation, undersegmentation, missing and spurious labels may all appear concurrently in machine segmented images. Segmentation algorithms make systematic errors and have different optimal operating ranges. Existing methods of segmentation evaluation do not evaluate these details. Our method, based on multiple feature recovery, reports systematic errors and indicates optimal operating ranges of features, besides measuring overall errors. A knowledge of the magnitude and type of errors can be used for tuning or selecting segmentation algorithms. Although our method was developed for CT scanning for security, it is applicable to other fields, including medical imaging, where multi-object feature recovery, non-uniform costs and a knowledge of optimal operating ranges are helpful.	algorithm;ct scan;medical imaging	Seemeen Karimi;Xiaoqian Jiang;Pamela C. Cosman;Harry E. Martz	2012	2012 IEEE Second International Conference on Healthcare Informatics, Imaging and Systems Biology	10.1109/HISB.2012.64	computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Visualization	39.88705913157382	-76.96398064875206	64443
3b2abf139240aea4fea602528d02c4278f7c0f9a	a new fast surface generation scheme and its application in medical image				Huiguang He;Jie Tian;Mingchang Zhao;Hua Yang	2001			computer vision;artificial intelligence;computer science	Vision	41.77235248384429	-71.10843190987224	64680
25ada29210db1fd38f0cd31ec726b16ca147993c	a new stochastic image model based on markov random fields and its application to texture modeling	texture modeling stochastic image models markov random field image joint density function;joint density function;gibbs distribution;stochastic image modeling;lattices;texture modeling;computer model;random processes computational complexity image texture markov processes;joints;markov random field;image texture;density functional theory;computational complexity markov random field texture modeling stochastic image modeling joint density function texture pattern generation;computational modeling;stochastic image models;image joint density function;mathematical model joints computational modeling equations lattices pixel density functional theory;computational complexity;density function theory;pixel;texture pattern generation;random processes;mathematical model;markov processes;image modeling;density functional	Stochastic image modeling based on conventional Markov random fields is extensively discussed in the literature. A new stochastic image model based on Markov random fields is introduced in this paper which overcomes the shortcomings of the conventional models easing the computation of the joint density function of images. As an application, this model is used to generate texture patterns. The lower computational complexity and easily controllable parameters of the model makes it a more useful model as compared to the conventional Markov random field-based models.	computation;computational complexity theory;markov chain;markov random field	Siamak Yousefi;Nasser Kehtarnavaz	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946646	markov decision process;stochastic process;computer vision;markov chain;maximum-entropy markov model;markov kernel;random field;markov property;computer science;machine learning;random function;pattern recognition;mathematics;markov algorithm;markov process;markov model;density functional theory;hidden markov model;statistics;variable-order markov model	Vision	51.46105100772146	-68.99745365533659	64829
1393c512f4abeb6d18c249ee17b5a85d8d53af0d	image segmentation by spatially adaptive color and texture features	pattern clustering;image segmentation;data compression;edge detection;low resolution;filters;photography;texture segmentation;texture features;image segmentation filters feature extraction clustering algorithms iterative algorithms layout robustness neutron spin echo focusing image resolution;image texture;feature extraction;steerable filters;feature extraction image segmentation image texture filters edge detection pattern clustering photography data compression;low resolution compressed image image segmentation spatially adaptive color feature spatially adaptive texture feature steerable filter decomposition border estimation procedure adaptive clustering photographic image	We presentanimagesegmentationalgorithmthat is basedon spatially adapti ve color and texture features. The proposedalgorithm is basedon a previously proposedalgorithm but introducesa numberof new elements. We usea new set of texture featuresbasedon a steerablefilter decomposition.The steerable filters combinedwith a new spatialtexture segmentationscheme provide a finer andmorerobustsegmentationinto textureclasses. The proposedalgorithmincludesan elaborateborderestimation procedure,which extendsthe ideaof Pappas’adapti ve clustering segmentationalgorithmto color texture. The performanceof the proposedalgorithmis demonstratedin thedomainof photographic images,includinglow resolutioncompressedimages.	cluster analysis;image segmentation;steerable filter	Junqing Chen;Thrasyvoulos N. Pappas;Aleksandra Mojsilovic;Bernice E. Rogowitz	2003		10.1109/ICIP.2003.1247135	data compression;image texture;computer vision;edge detection;image resolution;feature extraction;computer science;photography;segmentation-based object categorization;pattern recognition;region growing;image segmentation;scale-space segmentation;texture compression;texture filtering;computer graphics (images)	Vision	44.80161657608665	-67.8285534410367	65282
660f1356433d2695c367035b1f4433522ceec439	iterative fuzzy segmentation for breast skin-line detection			edge detection	Asma Touil;Karim Kalti	2016			fuzzy logic;scale-space segmentation;segmentation-based object categorization;artificial intelligence;pattern recognition;segmentation;computer science	Vision	41.79082156096764	-71.76459541260449	65469
703c406b6033f9a90035879db5deb197cb39eb56	automatic extraction of sulcal lines on the cortical surface using shortest path probability maps	manuals;shortest path;brain;algorithms cerebral cortex data interpretation statistical humans image enhancement image interpretation computer assisted imaging three dimensional magnetic resonance imaging pattern recognition automated reproducibility of results sensitivity and specificity;extremities;computational geometry;biomedical imaging;brain modeling;depth constrained geodesic paths automatic sulcal line extraction shortest path probability maps cortical surface meshes human brain landmark extraction sulci morphometry sulcal basin segmentation local curvature information;feature extraction;medical image processing;level measurement;robustness;humans;neurophysiology biomedical mri brain computational geometry feature extraction medical image processing;neurophysiology;human brain;extremities manuals humans robustness level measurement biomedical imaging brain modeling;biomedical mri	This paper describes an automatic procedure for extracting sulcal lines from cortical surface meshes of the human brain, which will serve as a tool for landmark extraction as well as for investigating the morphometry of sulci. The procedure consists in a sequence of steps, including sulcal basin segmentation based on local curvature information, estimating a bundle of depth-constrained geodesic paths and determining a robust probability map of sulcal lines crossing. In this experiment, we present quantitative validations on two main sulci to observe the agreement of our method with manually traced curves.	cns disorder;estimated;frontal sulcus;groove;map;morphometric analysis;morphometrics;performance;polygon mesh;short;shortest path problem;sulcal connective tissue of fibrous ring of tricuspid valve;thresholding (image processing);biologic segmentation;dinoflagellate sulcus	Arnaud Le Troter;Guillaume Auzias;Olivier Coulon	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091279	computer vision;radiology;feature extraction;computational geometry;computer science;artificial intelligence;machine learning;shortest path problem;neurophysiology;robustness	Vision	40.833840067574194	-78.60223004094244	65508
c225c75e83e15f9593e584a426c304a39b712fe1	parallel implementation of elastic grid matching using cellular neural networks	cellular neural network;location update;parallel implementation;deformable template	The following paper presents a method that allows for a parallel implementation of the most computationally expensive element of the deformable template paradigm, which is a grid-matching procedure. Cellular Neural Network Universal Machine has been selected as a framework for the task realization. A basic idea of deformable grid matching is to guide node location updates in a way that minimizes dissimilarity between an image and grid-recorded information, and that ensures minimum grid deformations. The proposed method provides a parallel implementation of this general concept and includes a novel approach to grid's elasticity modeling. The method has been experimentally verified using two different analog hardware environments, yielding high execution speeds and satisfactory processing accuracy.	artificial neural network;neural network software	Krzysztof Slot;Piotr Korbel;Hyongsuk Kim;Malrey Lee;Suhong Ko	2007		10.1007/978-3-540-71457-6_43	cellular neural network;computer science;theoretical computer science;machine learning;distributed computing	Robotics	48.371642278141	-71.12709867371677	65551
3ac07effd9c0915749fb048118fe91ac132ea8bc	coupling oriented hidden markov random field model with local clustering for segmenting blood vessels and measuring spatial structures in images of tumor microenvironment	pattern clustering;tumor microenvironment;spatial dependence;image segmentation;cancer;biomedical imaging blood vessels tumors image segmentation signal to noise ratio cells biology hidden markov models;tumours;spatial structure;hidden markov random field;blood vessel;quantisation signal;tumours blood vessels cancer hidden markov models image segmentation medical image processing pattern clustering quantisation signal random processes;hidden markov models;noise suppression coupling oriented hidden markov random local clustering cancer cell tumor microenvironment tumor development blood vessel cell tumor initiating cells blood vessel segmentation quantization signal to noise ratio hidden markov random field model ori hmrf;medical image processing;blood vessel segmentation;random processes;random variable;experimental validation;hidden markov random field blood vessel segmentation superpixel tumor microenvironment;spatial relationships;signal to noise ratio;superpixel;breast cancer;blood vessels	Interactions between cancer cells and factors within the tumor microenvironment (mE) are essential for understanding tumor development. The spatial relationships between blood vessel cells and cancer cells, e.g. tumor initiating cells (TICs), are an important parameter. Accurate segmentation of blood vessel is necessary for the quantization of their spatial relationships. However, this remains an open problem due to uneven intensity and low signal to noise ratio (SNR). To overcome these challenges, we propose a novel approach that integrates an oriented hidden Markov random field model (Ori-HMRF) with local clustering. The local clustering delineates boundaries of blood vessel segments with low SNR. Then blood vessel segments are viewed as random variables in the Ori-HMRF and their spatial dependence is defined based on directional information. The Ori-HMRF model suppresses noise and generates accurate blood vessel segmentation results. Experimental validations were conducted on both normal mammary and breast cancer tissues.	cluster analysis;clustering coefficient;habitat;hidden markov random field;interaction;markov chain;quantization (signal processing);signal-to-noise ratio	Yanqiao Zhu;Fuhai Li;Derek Cridebring;Jinwen Ma;Stephen T. C. Wong;Tegy J. Vadakkan;Mei Zhang;John Landua;Wei Wei;Mary E. Dickinson;Jeffrey M. Rosen;Michael T. Lewis	2011	2011 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2011.104	spatial relation;random variable;stochastic process;computer vision;speech recognition;spatial dependence;computer science;breast cancer;pattern recognition;image segmentation;signal-to-noise ratio;hidden markov model;statistics;cancer	Vision	42.47273661950353	-76.30684046141542	65597
7fe432143946498bd9b2438ba9d3d2b612da0e92	a combined voxel and surface based method for topology correction of brain surfaces	databases;brain;white matter;magnetism;binary image;point to point;magnetic resonance image;magnetic resonance imaging;clinical study;gray matter;topology preservation;marching cube;surface area	Brain surfaces provide a reliable representation for cortical mapping. The construction of correct surfaces from magnetic resonance images (MRI) segmentation is a challenging task, especially when genus zero surfaces are required for further processing such as parameterization, partial inflation and registration. The generation of such surfaces has been approached either by correcting a binary image as part of the segmentation pipeline or by modifying the mesh representing the surface. During this task, the preservation of the structure may be compromised because of the convoluted nature of the brain and noisy/imperfect segmentations. In this paper, we propose a combined, voxel and surfacebased, topology correction method which preserves the structure of the brain while yielding genus zero surfaces. The topology of the binary segmentation is first corrected using a set of topology preserving operators applied sequentially. This results in a white matter/gray matter binary set with correct sulci delineation, homotopic to a filled sphere. Using the corrected segmentation, a marching cubes mesh is then generated and the tunnels and handles resulting from the meshing are finally removed with an algorithm based on the detection of nonseparating loops. The approach was validated using 20 young individuals MRI from the OASIS database, acquired at two different time-points. Reproducibility and robustness were evaluated using global and local criteria such as surface area, curvature and point to point distance. Results demonstrated the method capability to produce genus zero meshes while preserving geometry, two fundamental properties for reliable and accurate cortical mapping and further clinical studies.	algorithm;binary image;block truncation coding;brain implant;code;electron hole;genus (mathematics);global brain;image segmentation;marching cubes;olap cube;resonance;software bug;voxel	Florence Gris;Jean-Marie Favreau;Oscar Acosta;Vincent Barra;Olivier Salvado	2010		10.1117/12.844624	computer vision;magnetism;binary image;point-to-point;magnetic resonance imaging;surface area;marching cubes;physics	Vision	42.79379383671959	-79.13971032895478	65750
d72566965091a8a40ed3afe09cb6eee19ba8ddc6	hyperbolic wavelet-fisz denoising for a model arising in ultrasound imaging	fisz transformation;data driven denoising;data driven denoising hyperbolic wavelets fisz transformation variance stabilization gaussianization ultrasound imaging;speckle;ultrasonic imaging;gaussianization;wavelet transforms adaptation models imaging ultrasonic imaging noise reduction speckle;wavelet transforms;ultrasound imaging;hyperbolic wavelets;noise reduction;imaging;variance stabilization;adaptation models	We present an algorithm and its fully data-driven extension for noise reduction in ultrasound imaging. The proposed method computes the hyperbolic wavelet transform of the image, before applying a multiscale variance stabilization technique, via a Fisz transformation. This adapts the wavelet coefficients statistics to the wavelet thresholding paradigm. The use of hyperbolic wavelets makes it possible to recover the image while respecting the anisotropic nature of structural details. The data-driven extension obviates the need for any prior knowledge of the noise model parameters by estimating the noise variance using an isotonic Nadaraya–Watson estimator. Experiments on synthetic and real data demonstrate the potential of the proposed algorithm to recover ultrasound images while preserving tissue details. Furthermore, comparisons with other noise-reduction methods show that our technique is competitive with the state-of-the-art OBNLM filter. Finally, the variance estimation procedure is applied to real images emphasizing the noise model.	bowyer–watson algorithm;coefficient;estimation theory;experiment;isotonic regression;medical ultrasound;noise reduction;programming paradigm;synthetic intelligence;thresholding (image processing);wavelet transform	Younes Farouj;Jean-Marc Freyermuth;Laurent Navarro;Marianne Clausel;Philippe Delachartre	2017	IEEE Transactions on Computational Imaging	10.1109/TCI.2016.2625740	medical imaging;speckle pattern;computer vision;mathematical optimization;radiology;medicine;computer science;noise reduction;mathematics;statistics;wavelet transform	Vision	51.04850483482267	-76.42718718204739	65801
e17855c539febdd65ef1663330079c047463c0ba	prior-shape-based segmentation of various objects in ultrasound images after speckle-reduction using level-set based curvature evolution	speckle reduction;level sets;speckle;heart;mean curvature flow;level set;segmentation;prior shape;ultrasound imaging;acoustic noise;algorithms;ultrasonography;mean curvature;boundary detection	Medical ultrasound images are noisy with speckle, acoustic noise and other artifacts. Reduction of speckle in particular is useful for CAD algorithms. We use two algorithms, namely, mean curvature evolution of the ultrasound image surface and a variation of the mean-curvature flow, to reduce speckle. The premise is that when we view the ultrasound image as a surface, the speckle appears as a high-curvature jagged layer over the true objects intensities and will reduce quickly on curvature evolution. We compare the two speckle reduction algorithms. We apply the speckle reduction to an image of a cyst and a 4-chamber view of the heart. We show significant, if not complete, speckle reduction, while keeping the relevant organ boundaries intact. On the speckle-reduced images, we apply a segmentation algorithm to detect objects. The segmentation algorithm is two-stepped. In the first step we choose a prior-shape and optimize the pose parameters to maximize the edge-pixels the curve falls into, using gradient ascent. In the second step, a radial motion is used to draw the contour points to the local-edges. We apply the algorithm on a cyst and obtain satisfactory results. We compare the total area inside the boundary output of our segmentation algorithm and to the total area covered by a hand-drawn boundary of the cyst, and the ratio is about 97%.© (2006) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Joyoni Dey;Dennis A. Tighe;Gopal Vijayaraghavan;Vikramjit Mitra;Peder Pedersen	2006		10.1117/12.655037	computer vision;mathematics;geometry;optics	Vision	45.44824674505887	-72.67104745264773	65802
243b26dc741838512129aa5b4422b283d096ef95	a fast and automatic method for 3d rigid registration of mr images of the human brain	three dimensional displays image segmentation lesions silicon surgery image registration biomedical imaging;silicon;deformable registration;brain;image segmentation;mr images;image processing;brain segmentation;magnetic resonance images;search algorithm;search problems biomedical mri brain greedy algorithms image registration image segmentation;biomedical imaging;greedy algorithms;mag netic resonance imaging;medical image analysis;greedy search algorithm;mr imaging;medical image;lesions;inter subject deformable registration;three dimensional displays;image registration;medical imaging;registration;medical image analysis registration image processing;surgery;search problems;formid sagittal plane location;human brain;greedy search algorithm 3d rigid registration mr images human brain image registration medical imaging inter subject deformable registration magnetic resonance images formid sagittal plane location brain segmentation;biomedical mri;3d rigid registration	Image registration is an important problem with several applications in medical imaging. Intra-subject rigid registration requires a minimal set of parameters to be computed, and is sufficient for organs with no significant movement or deformation, such as the human brain. Rigid registration has also been used as the first step before inter-subject deformable registration. In this paper we present a fast and automatic method for 3D rigid registration of magnetic resonance images of the human brain. The method combines previous approaches formid-sagittal plane location and brain segmentation with a greedy-search algorithm to find the best match between source and target images. We evaluated the method on 200 image pairs: 100 without structural abnormalities and 100 with artificially created lesions, such that it was possible to quantify the registration errors. The method achieved very accurate registration within a few seconds.	greedy algorithm;image registration;max;medical imaging;preprocessor;qualitative comparative analysis;resonance;search algorithm;voxel	Fernanda O. Favretto;Felipe P. G. Bergo;Alexandre X. Falcão	2008	2008 XXI Brazilian Symposium on Computer Graphics and Image Processing	10.1109/SIBGRAPI.2008.33	computer vision;radiology;image registration;medical physics	Vision	41.26094160222125	-79.16089167430519	65819
4a8510cd2c1def41848d168b6302dfd33b9a5eb0	non-rigid registration of cervical spine mri volumes	whiplash injuries;chronic disease;magnetic resonance imaging histograms joints muscles registers interpolation three dimensional displays;image processing computer assisted;adipose tissue;optimisation biomedical mri bone edge detection image enhancement image registration image segmentation medical disorders medical image processing muscle neurophysiology newton method;scsdm method neck injuries intra muscular fat chronic whiplash associated disorder treatment automatic cervical spine muscle segmentation multimodal mri volumes boundary enhancement partial volume interpolation similarity measure gauss newton optimization multimodal cervical spine mri volume registration scv based registration algorithm sum of the conditional squared deviation from the mode method;magnetic resonance imaging;cervical vertebrae;algorithms;humans;muscles	Whiplash is the colloquial term for neck injuries caused by sudden extension of the cervical spine. Patients with chronic whiplash associated disorder (WAD) can experience neck pain for many years after the original injury. Researchers have found some evidence to suggest that chronic whiplash is related to the amount of intra-muscular fat in the cervical spine muscles. Hence, an important step towards developing a treatment for chronic WAD is a technique to accurately and efficiently measure the amount of intra-muscular fat in the muscles of the cervical spine. Our proposed technique for making this measurement is to automatically segment the cervical spine muscles using a fused volume created from multi-modal MRI volumes of the cervical spine. Multiple modes are required to enhance the boundaries between the different muscles to assist the following automatic segmentation process. However, before these multiple modes can be fused it is first necessary to accurately register these volumes. Hence, in this paper, we have proposed a new non-rigid multi-modal registration algorithm using the sum of conditional variance (SCV) with partial volume interpolation (PVI) similarity measure and Gauss-Newton (GN) optimization for the accurate registration of multi-modal cervical spine MRI volumes. The performance of the proposed approach is compared with the existing SCV based registration algorithm and the sum of the conditional squared deviation from the mode (SCSDM) method. The experimental results demonstrate that the proposed approach provides superior performance than the best existing approaches.	cervical spine;gauss;gauss–newton algorithm;grid north;interpolation imputation technique;kidney failure, chronic;mathematical optimization;modal logic;muscle rigidity;neck injuries;neck pain;newton;numerous;patients;platelet glycoprotein 4, human;sample variance;similarity measure;single customer view;whiplash injuries;registration - actclass	Nargis Aktar;Md. Jahangir Alam;Mark R. Pickering;Alexandra Webb;Diana Perriman	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7318777	radiology;medicine;pathology;magnetic resonance imaging;adipose tissue;anatomy;surgery	Vision	41.175627210451196	-80.15269545171986	66061
6100e8b9bf79f3ffd7614f3128e3c7dc06b06fd1	quality assessment and restoration of typewritten document images	restauration image;typewritten;optical character recognition;caracter impreso;image restoration;printed character;qualite image document;qualite image;restauracion imagen;quality assessment;image quality;document quality assessment;reconocimento optico de caracteres;calidad imagen;quality measures;caractere imprime;reconnaissance optique caractere	We present a useful method for assessing the quality of a typewritten document image and automatically selecting an optimal restoration method based on that assessment. We use five quality measures that assess the severity of background speckle, touching characters, and broken characters. A linear classifier uses these measures to select a restoration method. On a 139-document corpus, our methodology reduced the corpus OCR character error rate from 20.27% to 12.60%.	circuit restoration;linear classifier;mojibake	Michael Cannon;Judith Hochberg;Patrick Kelly	1999	International Journal on Document Analysis and Recognition	10.1007/s100320050039	image quality;image restoration;computer vision;speech recognition;computer science;artificial intelligence;optical character recognition	Vision	39.847578682563835	-68.68205118317219	66335
45581967d3fb24f143d0f91628208af02862dedc	counting white blood cells using morphological granulometries	white blood cell;bone marrow;prior knowledge;spectrum;estimation algorithm;blood	d at r. ted 999 Abstract. We describe a modification of the mixture proportion estimation algorithm based on the granulometric mixing theorem. The modified algorithm is applied to the problem of counting different types of white blood cells in bone marrow images. In principle, the algorithm can be used to count the proportion of cells in each class without explicitly segmenting and classifying them. The direct application of the original algorithm does not converge well for more than two classes. The modified algorithm uses prior statistics to initially segment the mixed pattern spectrum and then applies the oneprimitive estimation algorithm to each initial component. Applying the algorithm to one class at a time results in better convergence. The counts produced by the modified algorithm on six classes of cells—myeloblast, promyelocyte, myelocyte, metamyelocyte, band, and PolyMorphoNuclear (PMN)—are very close to the human expert’s numbers; the deviation of the algorithm counts is similar to the deviation of counts produced by human experts. The important technical contributions are that the modified algorithm uses prior statistics for each shape class in place of prior knowledge of the total number of objects in an image, and it allows for more than one primitive from each class. © 2000 SPIE and IS&T. [S1017-9909(00)00602-4]	converge;genetic algorithm;granulometry (morphology)	Nipon Theera-Umpon;Paul D. Gader	2000	J. Electronic Imaging	10.1117/1.482737	spectrum	ML	43.03188176452663	-67.68680869929362	66505
d8d7d4317093482753e569f88614fc1495c1e85e	improved stability of whole brain surface parcellation with multi-atlas segmentation		Whole brain segmentation and cortical surface parcellation are essential in understanding the brain anatomical-functional relationship. Multi-atlas segmentation has been regarded as one of the leading segmentation methods for the whole brain segmentation. In our recent work, the multi-atlas technique has been adapted to surface reconstruction using a method called Multi-atlas CRUISE (MaCRUISE). The MaCRUISE method not only performed the consistent volume-surface analyses but also shown advantages on robustness compared with FreeSurfer method. However, a detailed surface parcellation was not provided by MaCRUISE, which hindered the region of interests (ROI) based analyses on surfaces. Herein, the MaCRUISE surface parcellation (MaCRUISEsp) method is proposed to perform the surface parcellation upon the inner, central and outer surfaces that are reconstructed from MaCRUISE. MaCRUISEsp parcellates inner, central and outer surfaces with 98 cortical labels respectively using a volume segmentation based surface parcellation (VSBSP), following a topological correction step. To validate the performance of MaCRUISEsp, 21 scan-rescan magnetic resonance imaging (MRI) T1 volume pairs from the Kirby21 dataset were used to perform a reproducibility analyses. MaCRUISEsp achieved 0.948 on median Dice Similarity Coefficient (DSC) for central surfaces. Meanwhile, FreeSurfer achieved 0.905 DSC for inner surfaces and 0.881 DSC for outer surfaces, while the proposed method achieved 0.929 DSC for inner surfaces and 0.835 DSC for outer surfaces. Qualitatively, the results are encouraging, but are not directly comparable as the two approaches use different definitions of cortical labels.	atlases;cervical atlas;coefficient;freesurfer;hepatitis b surface antigens;magnetic resonance imaging;numerous;region of interest;silo (dataset);biologic segmentation;brain segmentation	Yuankai Huo;Shunxing Bao;Prasanna Parvathaneni;Bennett A. Landman	2018	Proceedings of SPIE--the International Society for Optical Engineering	10.1117/12.2281509	robustness (computer science);computer science;artificial intelligence;region of interest;pattern recognition;brain surface;surface reconstruction;atlas (anatomy);segmentation;brain segmentation	Vision	42.948550243517026	-79.1175494319101	66507
ab09728245701ec82168f4b6f96f18699562a9e7	a 3-d display method of fuzzy shapes obtained from medical images	aide diagnostic;affichage;aplicacion medical;image processing;visualizacion;espacio 3 dimensiones;procesamiento imagen;traitement image;visualization;medical image;display;visualisation;espace 3 dimensions;three dimensional space;medical application;diagnostic aid;application medicale;ayuda diagnostica	Abstract#R##N##R##N#A three-dimensional (3-D) display of medical images allows better perception of the continuity of structures and spatial relationships than a display of individual slices. These 3-D display methods consist of a contour extraction, a shape reconstruction and a shading. However, the boundary judgment of soft tissues, like a tumor, is difficult even for expert doctors. Especially, 3-D display depends largely on a result of the contour extraction. The 3-D display method is required to display the presentation of the continuity of organ shapes.#R##N##R##N##R##N##R##N#This paper presents the development of a display method to preserve the continuity of organ shapes. The display method consists of the extraction of fuzzy shapes and their shading. The extraction of fuzzy shapes is executed by operation of the fuzzy c-means clustering algorithm on medical images and the use of a connectivity of an organ shape. The shading of fuzzy shapes uses a volume rendering, which is analyzed numerically and improved to realize a powerfully interactive 3-D display. The effectiveness of the proposed 3-D display method is shown using head MRI images.	stereo display	Noboru Niki;Yoshiki Kawata;Hitoshi Satoh	1991	Systems and Computers in Japan	10.1002/scj.4690221108	computer vision;visualization;image processing;computer science;computer graphics (images)	Robotics	46.44012433857462	-79.49579622212566	66629
5296f8085fde5d7c3bad40ca7a3c2d70f7027bda	improved fiber tractography with bayesian tensor regularization	optimal solution;bayesian regularization;fiber tracking;diffusion tensor images;regularization;diffusion tensor tractography;partial volume;multivariate normal distribution;bayes decision rule;fractional anisotropy;diffusion tensor imaging;monte carlo simulation;path following;decision rule;diffusion tensor	Diffusion tensor tractography suffers from the effects of noise and partial volume averaging (PVA). For reliable reconstruction of fiber pathways, tracking algorithms that are robust to these artifacts are called for. To meet this need, the present study establishes a novel Bayesian regularization framework for fiber tracking that takes into account the effects of noise and PVA, thereby improving tracking accuracy and precision. With this framework, the propagation of a fiber path follows an optimal vector determined by Bayes decision rule; the probability functions involved are modeled on the basis of multivariate normal distributions of diffusion tensor elements, which allows the optimal solution with maximum a posteriori probability to be derived analytically. Parameters for the probability functions are estimated from the uncertainty of tensor elements and the variance among tensors within an oriented sampling volume weighted by fractional anisotropy. Experiments with Monte Carlo simulations, synthetic, and in vivo human diffusion tensor data demonstrate that this specialized scheme enhances the immunity of fiber tracking to noise and PVA, and hence enables fibers to be more faithfully reconstructed.	a fibers;algorithm;fractional anisotropy;mental suffering;monte carlo method;morphologic artifacts;sample variance;sampling (signal processing);simulation;software propagation;synthetic data;tissue fiber;video-in video-out	Yonggang Lu;Akram Aldroubi;John C. Gore;Adam W. Anderson;Zhaohua Ding	2006	NeuroImage	10.1016/j.neuroimage.2006.01.043	diffusion mri;mathematical optimization;radiology;machine learning;mathematics;statistics	Vision	48.216871014620565	-79.89940093196732	66731
0c8d68d3e80cd0a96f7283af1b82499ab01294ca	left atrium segmentation for atrial fibrillation ablation	pulmonary vein;radio frequency catheter ablation;interfaces;image guided procedures;level set;veins;euclidean distance;catheter ablation;angiography;atrial fibrillation;visualization;radio frequency;left atrium;partial volume;blood;mri;radiofrequency catheter ablation surgery;surgery;medical image segmentation;region growing;mr angiography	Segmentation of the left atrium is vital for pre-operative assessment of its anatomy in radio-frequency catheter ablation (RFCA) surgery. RFCA is commonly used for treating atrial fibrillation. In this paper we present an semi-automatic approach for segmenting the left atrium and the pulmonary veins from MR angiography (MRA) data sets. We also present an automatic approach for further subdividing the segmented atrium into the atrium body and the pulmonary veins. The segmentation algorithm is based on the notion that in MRA the atrium becomes connected to surrounding structures via partial volume affected voxels and narrow vessels, the atrium can be separated if these regions are characterized and identified. The blood pool, obtained by subtracting the preand post-contrast scans, is first segmented using a regiongrowing approach. The segmented blood pool is then subdivided into disjoint subdivisions based on its Euclidean distance transform. These subdivisions are then merged automatically starting from a seed point and stopping at points where the atrium leaks into a neighbouring structure. The resulting merged subdivisions produce the segmented atrium. Measuring the size of the pulmonary vein ostium is vital for selecting the optimal Lasso catheter diameter. We present a second technique for automatically identifying the atrium body from segmented left atrium images. The separating surface between the atrium body and the pulmonary veins gives the ostia locations and can play an important role in measuring their diameters. The technique relies on evolving interfaces modelled using level sets. Results have been presented on 20 patient MRA datasets.	algorithm;computed tomography angiography;distance transform;euclidean distance;lasso;pool (computer science);radio frequency;semiconductor industry;voxel	Rashed Karim;Raad Mohiaddin;Daniel Rueckert	2008		10.1117/12.771023	radiology;medicine;surgery;cardiology	Vision	39.49299116397822	-80.14779828132001	66915
21c3863dfb15de38dd0a712274ae2e3d8f499110	segmentation of lip pixels for lip tracker initialisation	predictive validation technique;mouth;predictive validity;image segmentation;pixel rgb values;gaussian processes;bayes methods;bayes methods image segmentation image colour analysis tracking gaussian processes prediction theory covariance matrices;image database;distributed computing;mouth predictive models image segmentation testing shape hair teeth video sequences probability distribution distributed computing;video sequences;bayesian rule labelling;testing;lip pixels segmentation;gaussian mixture model;shape;prediction theory;covariance matrices;image colour analysis;probability distribution;teeth;pattern recognition;k means clustering lip pixels segmentation lip tracker initialisation image segmentation gaussian mixture model pixel rgb values predictive validation technique covariance matrices mixture components bayesian rule labelling image database;predictive models;lip tracker initialisation;mixture components;k means clustering;tracking;hair	We propose a novel image segmentation method for lip tracker initialisation which is based on a Gaussian mixture model of the pixel RGB values. The model is built using the predictive validation technique advocated by Kittler, Messer and Sadeghi (see Second International Conference on Advances in Pattern Recognition, Brazil, March 2001) which has been modified to allow modelling with full covariance matrices. A subsequent grouping of the mixture components provides the basis for a Bayesian rule labelling of the pixels as lip or non-lip. We test the proposed method on a database of 145 images and demonstrate that its accuracy is significantly better than the segmentation obtained by k-means clustering. Moreover, the proposed method does not require the number of segments to be specified a priori.	pixel	Josef Kittler;Kieron Messer;Mohammad Sadeghi	2001		10.1109/ICIP.2001.958950	probability distribution;computer vision;predictive validity;shape;computer science;machine learning;pattern recognition;mixture model;gaussian process;mathematics;predictive modelling;tracking;software testing;image segmentation;tooth;statistics;k-means clustering	Vision	49.68971893619686	-69.23452244467775	67231
a06d1b76704d279981c8811b5f268302fd2aa3c8	neighbor-constrained segmentation with level set based 3-d deformable models	sensitivity and specificity;elasticity;brain;euler lagrange equation;image segmentation;imaging three dimensional;level set;prior information;algorithms brain computer simulation elasticity humans image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval magnetic resonance imaging models biological models statistical numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted;models biological;indexing terms;maximum likelihood estimation;three dimensional;signal processing computer assisted;image enhancement;multiple objectives;numerical analysis computer assisted;medical image;image interpretation computer assisted;level set deformable models shape image segmentation biomedical imaging active contours object detection radiology computed tomography density functional theory;shape prior model neighbor constrained segmentation level set based 3 d deformable models three dimensional medical images interobject constraints maximum a posteriori estimation joint density function joint probability distributions euler lagrange equations robust global atlases level set representation point distribution model neighbor prior model;medical image processing;magnetic resonance imaging;probability distribution;reproducibility of results;models statistical;algorithms;pattern recognition automated;humans;synthetic data;shape priors;maximum likelihood estimation biomedical mri brain medical image processing image segmentation;deformable model;computer simulation;information storage and retrieval;density functional;biomedical mri;point distribution model	A novel method for the segmentation of multiple objects from three-dimensional (3-D) medical images using interobject constraints is presented. Our method is motivated by the observation that neighboring structures have consistent locations and shapes that provide configurations and context that aid in segmentation. We define a maximum a posteriori (MAP) estimation framework using the constraining information provided by neighboring objects to segment several objects simultaneously. We introduce a representation for the joint density function of the neighbor objects, and define joint probability distributions over the variations of the neighboring shape and position relationships of a set of training images. In order to estimate the MAP shapes of the objects, we formulate the model in terms of level set functions, and compute the associated Euler-Lagrange equations. The contours evolve both according to the neighbor prior information and the image gray level information. This method is useful in situations where there is limited interobject information as opposed to robust global atlases. In addition, we compare our level set representation of the object shape to the point distribution model. Results and validation from experiments on synthetic data and medical imagery in two-dimensional and 3-D are demonstrated.	atlases;euler;euler–lagrange equation;experiment;grayscale;medical imaging;phenylephrine hydrochloride 10 mg oral tablet;physical object;point distribution model;synthetic data;biologic segmentation	Jing Yang;Lawrence H. Staib;James S. Duncan	2004	IEEE Transactions on Medical Imaging	10.1109/TMI.2004.830802	computer simulation;probability distribution;three-dimensional space;point distribution model;computer vision;index term;computer science;level set;magnetic resonance imaging;machine learning;pattern recognition;mathematics;maximum likelihood;image segmentation;elasticity;statistics;synthetic data	Vision	44.79283862742434	-77.45556523145441	67394
ccff36df5147c28678c97bfc904a4867161c068e	evaluation of edge detection methods through psychological tests-is the detected edge really desirable for humans?	psychology edge detection statistical analysis;edge detection;optimal method;statistical test;optimization edge detection method evaluation psychological tests image processing human visual processing statistical tests signal processing experiments;psychology;statistical analysis;signal processing;image processing methods;image processing techniques;visual processing;quantitative evaluation;psychological tests;image edge detection psychology testing humans image processing image recognition optimization methods signal processing proposals detectors	We propose a method to quantitatively evaluate image processing methods on how well they simulate human visual processing through psychological and statistical tests. First, we focus on edge detection, an image processing technique. To set all edge detection methods fairly, we collect the edge data detected by humans. Then, the signal processing parameters for each edge detection method are adjusted with the same optimizing method based on minimizing the error between the edges detected by the detector and by humans. Two psychological experiments are conducted, confirming whether the edge detection parameters are optimized and evaluating the performance of edge detection methods. The results indicate that the optimizing method functioned comparatively well. The effectiveness rank of edge detection methods is also obtained.		Miho Ohsaki;Takahiro Sugiyama;Hideaki Ohno	2000		10.1109/ICSMC.2000.885072	step detection;computer vision;statistical hypothesis testing;feature detection;edge detection;image processing;computer science;theoretical computer science;machine learning;signal processing;digital image processing;psychological testing;statistics	Vision	43.17402979841133	-68.24088197262778	67690
f6f4e7910d78b3f331ebac5a1834c6f427b0b8f3	robust principal curvatures on multiple scales	multiple scale;principal component analysis;kernel radius r;feature extraction;principal curve;principal curvature;asymptotic analysis;robust principal curvature;curvature information;consistent behavior;boundary sphere sr;kernel ball br;marching cubes;topology	"""Geometry processing algorithms often require the robust extraction of curvature information. We propose to achieve this with principal component analysis (PCA) of local neighborhoods, defined via spherical kernels centered on the given surface Φ Intersection of a kernel ball Br or its boundary sphere Sr with the volume bounded by Φ leads to the so-called ball and sphere neighborhoods. Information obtained by PCA of these neighborhoods turns out to be more robust than PCA of the patch neighborhood Br∩Φ previously used. The relation of the quantities computed by PCA with the principal curvatures of Φ is revealed by an asymptotic analysis as the kernel radius r tends to zero. This also allows us to define principal curvatures """"at scale r"""" in a way which is consistent with the classical setting. The advantages of the new approach are discussed in a comparison with results obtained by normal cycles and local fitting; whereas the former method somewhat lacks in robustness, the latter does not achieve a consistent behavior at features on coarse scales. As to applications, we address computing principal curves and feature extraction on multiple scales."""	algorithm;feature extraction;geometry processing;kernel (operating system);principal component analysis	Yong-Liang Yang;Yu-Kun Lai;Shi-Min Hu;Helmut Pottmann	2006			mathematical optimization;asymptotic analysis;topology;feature extraction;kernel principal component analysis;machine learning;mathematics;geometry;marching cubes;principal component analysis	ML	48.564517141441094	-69.22299210415093	68009
cff329898342eadb1c7f20cd3017e5be933789ca	normalized similarity measures for medical image registration	normalized mutual information;medical image;image registration;mutual information;similarity measure;information theory	Two new similarity measures for rigid image registration, based on the normalization of Jensen’s difference applied to Rényi and Tsallis-Havrda-Charvát entropies, are introduced. One measure is normalized by the first term of Jensen’s difference, which in our proposal coincides with the marginal entropy, and the other by the joint entropy. These measures can be seen as an extension of two measures successfully applied in medical image registration: the mutual information and the normalized mutual information. Experiments with various registration modalities show that the new similarity measures are more robust than the normalized mutual information for some modalities and a determined range of the entropy parameter. Also, a certain improvement on accuracy can be obtained for a different range of this parameter.	experiment;image registration;interpolation;jensen's inequality;joint entropy;linear algebra;marginal model;modality (human–computer interaction);mutual information;non-maskable interrupt;similarity learning;twisted nematic field effect	Anton Bardera;Miquel Feixas;Imma Boada	2004		10.1117/12.536106	variation of information;machine learning;pattern recognition;mathematics;mutual information;statistics;pointwise mutual information	Vision	45.09704456464715	-78.11767625387574	68041
c6c84d7407dc68edf0c4ee8c6d151ecf2a224e54	comparison of color space models based on human color classification and image segmentation	image segmentation;color space	"""In t,his paper we compared color space models l,ased on hrunan color perception. For this purpose, we made t.wo experinlents to see how human suhject,s classify color pixels in color ima.ges. In the first. experiment, subject,^ classify color pixels without seeing the ima.ge. In the second experiment, the subjects segment the images int.0 regions considered t,o I'e of similar colors, seeing the images. We define three crit,eria, two of which are based on the color classification and segmentation made by humans, for comparison of eleven color space models. As a conclusion, we found t,hat L*u*v* and L*a*b* color moclels are most closer t,o human color percepIn color image segmentation based on clustering met.hotls, segment,at.ion resu1t.s vary according t.o color spa.ce model used. There is, however, no guideline for selecting t,he color space moclel. In our earlier work[l] we compa.red eight color moilels fro111 t,he viewpoint, of using then1 in t,he ISODATA clustering method for color image segmentat.ion. However, the problenl of choosing an appropriate color nod el arises not only in segmentmation of images. By generalizing the problem, we would like to find color space(s) close to human color percept,ion. \Ve compa.re eleven color space nlodels ba.sed on t.he result,s of pixel color classificat,ion and region segment.at.ion performed by hulllan subjects. 2 Color spaces for conlparison TI)(. following eleven color space ~nodels are com['""""'""""I."""	cluster analysis;color image;color space;color vision;image segmentation;pixel	Keiko Takahashi;Masaki Matsuura;Takahiro Sugiyama;Keiichi Abe	2000			mathematics;computer vision;artificial intelligence;cornea;color histogram;color balance;image segmentation;hsl and hsv;color normalization;color image;color space	Vision	42.77320674937487	-67.28399553411674	68197
09ef9fef024f3f801e2e0c8f15271f91dee4a99a	local refinement for 3d deformable parametric surfaces	local refinement;splines;splines mathematics three dimensional displays surface morphology deformable models shape biomedical imaging biological system modeling;biological system modeling;biomedical imaging;deformable models;segmentation;splines deformable model parametric surface refinable function local refinement segmentation;splines mathematics;surface morphology;refinable function;shape;three dimensional displays;parametric surface;deformable model	Biomedical image segmentation is an active field of research where deformable models have proved to be efficient. The geometric representation of such models determines their ability to approximate the shape of interest as well as the speed of convergence of related optimization algorithms. We present a new tensor-product parameterization of surfaces that offers the possibility of local refinement. The goal is to allocate additional degrees of freedom to the surface only where an increase in local detail is required. We introduce the possibility of locally increasing the number of control points by inserting basis functions at specific locations. Our approach is generic and relies on refinable functions which satisfy the refinement relation. We show that the proposed method improves brain segmentation in 3D MRI images.	approximation algorithm;basis function;image segmentation;mathematical optimization;rate of convergence;refinable function;refinement (computing)	Anais Badoual;Daniel Schmitter;Michael Unser	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532525	medical imaging;spline;computer vision;mathematical optimization;shape;parametric surface;mathematics;geometry;segmentation	Vision	45.69292951691461	-76.74472214199018	68221
06d101c93882579763a14f889c8ed4ccf4de99f2	a dual pso-adaptive mean shift for preprocessing optimization on degraded document images		In the two past decades, solving complex search and optimization problems with bioinspired metaheuristic algorithms has received considerable attention among researchers. In this paper, the image preprocessing is considered as an optimization problem and the PSO Particle Swarm Optimization algorithm was been chosen to solve it in order to select the best parameters. The document image preprocessing step is the basis of all other steps in OCR Optical Character Recognition system, such as binarization, segmentation, skew correction, layout extraction, textual zones detection and OCR. Without preprocessing, the presence of degradation in the image significantly reduces the performance of these steps. The authors' contribution focuses on the preprocessing of type: smoothing and filtering document images using a new Adaptive Mean Shift algorithm based on the integral image. The local adaptation to the image quality accelerates the conventional smoothing avoiding the preprocessing of homogeneous zones. The authors' goal is to show how PSO algorithm can improve the results quality and the choice of parameters in pre-processing's methods of document images. Comparative studies as well as tests over the existing dataset have been reported to confirm the efficiency of the proposed approach.	lagrange multiplier;mean shift;particle swarm optimization;preprocessor	Aicha Eutamene;Mohamed-Khireddine Kholladi;Djamel Gaceb;Hacene Belhadef	2017	Int. J. of Applied Metaheuristic Computing	10.4018/IJAMC.2017010104	mathematical optimization;computer science;artificial intelligence;machine learning;pattern recognition;data mining	NLP	42.31708447757291	-69.03390753638084	68779
bda178b7a42feb14bb1f4fd2e83e6a501784f969	computerized diagnosis of breast calcifications using specimen radiography and simulated calcifications	aide diagnostic;digital radiography;glandula mamaria patologia;female;radiodiagnostic;medical imagery;informatica biomedical;biomedical data processing;analisis textura;cancer;tumor maligno;mastografia;radiografia numerica;extraction forme;mammary gland;simulation;informatique biomedicale;hombre;simulacion;calcificacion;breast;radiography;glandula mamaria;3d simulation;radiodiagnostico;texture analysis;hembra;medical diagnostics;mammographie;radiographie numerique;extraccion forma;human;imagerie medicale;mammary gland diseases;computing systems;differential diagnosis;tumeur maligne;glande mammaire;imageneria medical;radiodiagnosis;femelle;mammography;calcification;analyse texture;pattern extraction;diagnostic aid;glande mammaire pathologie;malignant tumor;homme;ayuda diagnostica	Several image-degrading factors limit the diagnosis of breast calcification from preoperative mammograms. We use specimen radiographs and algorithmically 3D-simulated calcification to produce high-quality data for testing a computerized differential diagnosis system. The preliminary results show that a computer can indeed differentiate explicitly between several mammographically characteristic calcification types. For a radiologist, such a detailed diagnosis could be more useful than simply characterizing the malignancy of the calcification. The results show new possibilities for the future diagnosis systems based on direct digital preoperative imaging.© (1999) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	biological specimen;radiography	Janne J. Näppi;Peter B. Dean;Olli Nevalainen;Sakari Toikkanen	1999		10.1117/12.348533	radiology;medicine;pathology;surgery	NLP	46.27445065274432	-79.5981022866939	68791
80017889e6296564c5a991db12931ca1fffd6d3f	a level set method with shape priors by using locality preserving projections	active contour;selective image segmentation;locality preserving projections;期刊论文;shape priors;level set method	A novel level set method (LSM) with the constraint of shape priors is proposed to implement a selective image segmentation. Firstly, the shape priors are aligned by using image moment to deprive the spatial related information. Secondly, the aligned shape priors are projected into the subspace expanded by using locality preserving projection to measure the similarity between the shapes. Finally, a new energy functional is built by combing data-driven and shape-driven energy items to implement a selective image segmentation method. We assess the proposed method and some representative LSMs on the synthetic, medical and natural images, the results suggest that the proposed one is superior to the pure data-driven LSMs and the representative LSMs with shape priors.	computation;dimensionality reduction;image moment;image segmentation;iterative method;krylov subspace;locality of reference;pure data;synthetic intelligence;time complexity	Bin Wang;Xinbo Gao;Jie Li;Xuelong Li;Dacheng Tao	2015	Neurocomputing	10.1016/j.neucom.2014.07.086	computer vision;computer science;machine learning;pattern recognition;active contour model;mathematics;level set method	Vision	48.66736322207617	-71.99427548388637	68802
1c0073b3229102d8e2bde1049228a4b2999902ca	generalized multiscale connected operators with applications to granulometric image analysis	granulometric image analysis;threshold analysis synthesis;image analysis histograms image edge detection image motion analysis image texture analysis application software lattices distributed computing shape measurement soil measurements;histograms;opening type operators;fast implementation;image content;image motion analysis;image processing;lattices;closing type operators;application software;nonlinear multiscale image analysis;generalized multiscale lattice operators;distributed computing;granular structure;morphological operation;shape measurement;pattern spectra;generalized granulometric size distributions;mathematical operators;histogram;size histograms;size distribution;generalized size histograms;geophysics computing;statistical analysis;image edge detection;area openings;nonlinear multiscale image analysis generalized multiscale connected operators granulometric image analysis generalized granulometric size distributions size histograms pattern spectra generalized multiscale lattice operators opening type operators closing type operators soil section images fast implementation histogram area openings generalized size histograms threshold analysis synthesis morphological operators image content scale dependent geometric attributes;generalized multiscale connected operators;image analysis;image texture analysis;soil section images;geophysics computing image processing statistical analysis spectral analysis soil mathematical operators granular structure geophysical techniques;spectral analysis;morphological operators;soil;connected operator;soil measurements;scale dependent geometric attributes;geophysical techniques;scale dependence	In this paper, generalized granulometric size distributions and size histograms (a.k.a ‘pattern spectra’) are developed using generalized multiscale lattice operators of the opening and closing type. The generalized size histograms are applied to granulometric analysis of soilsection images. An interesting structure is obtained when the histogram is based on area openings. Furthermore, a fast implementation of the generalized size histograms is presented using threshold analysis-synthesis. Comparisons with size distributions based on conventional morphological operators indicate that the generalized histograms provide a more direct and informative description of the image content in objects with scale-dependent geometric attributes. Applications are also developed for studying the structure of soilsection images.	closing (morphology);image analysis;information;mathematical morphology	Anastasios D. Doulamis;Nikolaos D. Doulamis;Petros Maragos	2001		10.1109/ICIP.2001.958211	computer vision;mathematical optimization;combinatorics;discrete mathematics;image analysis;image processing;computer science;histogram;mathematics;statistics	Vision	49.54432914313617	-66.45813223926241	68814
08d5835e9689e68e03ae11f3e51ff446c64b9c00	digital image interpolation using adaptive gaussian basis functions	image numerique;interpolation;image processing;algoritmo adaptativo;implementation;interpolacion;fonction base radiale;variance analysis;image interpolation;procesamiento imagen;matrice covariance;matriz covariancia;imagen nivel gris;traitement image;adaptive algorithm;matrices;algorithme adaptatif;radial basis function;analisis variancia;image niveau gris;imagen numerica;digital image;implementacion;funcion radial base;grey level image;analyse variance;covariance matrix	Digital image interpolation using Gaussian radial basis functions has been implemented by several investigators, and promising results have been obtained; however, determining the basis function variance has been problematic. Here, adaptive Gaussian basis functions fit the mean vector and covariance matrix of a non-radial Gaussian function to each pixel and its neighbors, which enables edges and other image characteristics to be more effectively represented. The interpolation is constrained to reproduce the original image mean gray level, and the mean basis function variance is determined using the expected image smoothness for the increased resolution. Test outputs from the resulting Adaptive Gaussian Interpolation algorithm are presented and compared with classical interpolation techniques.	algorithm;digital image;gaussian blur;grayscale;interpolation;pixel;radial (radio);radial basis function	Terence D. Hunt;Steven C. Gustafson	2004		10.1117/12.555598	spline interpolation;gaussian random field;mathematical optimization;bilinear interpolation;interpolation;polynomial interpolation;stairstep interpolation;basis function;bicubic interpolation;mathematics;geometry;linear interpolation;nearest-neighbor interpolation;multivariate interpolation;gaussian function;statistics;trilinear interpolation;image scaling	Vision	53.4129143619629	-67.4855094243693	68827
431dc677b9a453145c188f140fbd2e6263c5555e	image segmentation based on inscribed circle	region based technique image segmentation inscribed circle multistep method binary image shape feature gray level feature boundary based technique;image segmentation image edge detection partitioning algorithms shape phase detection remote sensing focusing pixel robustness smoothing methods;multistep method;image segmentation;region based technique;binary image;shape feature;boundary based technique;gray level feature;inscribed circle	We present an image segmentation algorithm based on inscribed circle (ICseg) in this paper. The algorithm includes multi-step. It examines edges at first, and then produces a binary image with edges as foreground. After that, inscribed circles are created to cover the background of the binary image. In the first partition, the image is subdivided into separated regions by combining inscribed circles. Finally, the regions are merged by computing their shape and gray level features. This approach integrates the boundary-based and region-based techniques. It is a simple and efficient way for full image segmentation	algorithm;binary image;grayscale;image segmentation;linear multistep method	Zhanrong Li;Jianqing Zhang	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.689	image texture;computer vision;feature detection;range segmentation;binary image;computer science;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;scale-space segmentation;incircle and excircles of a triangle	Vision	45.08395570379922	-67.40335725190121	69007
1aa118360295ac1abc4d20aaa7a547ba580db0bf	continuous voxel classification by stochastic relaxation: theory and application to mr imaging and mr angiography	psi_mic;bayesian decision theory;mr imaging;medical image;mr angiography	In this paper we present a stochastic relaxation method based on Bayesian decision theory for voxel classification in medical images. The labels are continuous (as opposed to discrete) values representing the degree of belief that a voxel belongs to a certain object class.	relaxation (approximation);voxel	Dirk Vandermeulen;Rudi Verbeeck;L. Berben;Paul Suetens;Guy Marchal	1993		10.1007/BFb0013807	computer vision;radiology;bayes estimator;computer science;mathematics;statistics;medical physics	Vision	43.91237501140701	-75.7168992237909	69557
60f39a6c1452b5925566895e97b2853d32441917	unsupervised video segmentation by dynamic volume growing and multivariate volume merging using color-texture-gradient features	image segmentation;spatiotemporal phenomena feature extraction image colour analysis image segmentation image sequences image texture;3 d gradient detection video segmentation volume growing multivariate volume merging;image texture;image color analysis merging image segmentation algorithm design and analysis streaming media cascading style sheets signal processing algorithms;image colour analysis;feature extraction;spatiotemporal phenomena;simple to complex video sequences dynamic volume growing multivariate volume merging procedure color texture gradient features unsupervised digital video segmentation technique homogeneous subvolume identification three dimensional spatiotemporal volume 3d gradient detection method volume growing procedure spatiotemporal locations gradient magnitudes entropy based texture descriptor;image sequences	We propose a new unsupervised technique for segmentation of digital video that partitions its constituents by identifying homogeneous sub-volumes within the data treated as a three dimensional (3-D) spatio-temporal volume. Our approach is commenced by subjecting the input video to a 3-D gradient detection method that determines the magnitude of color changes across the volume. The computed gradient is utilized to guide a volume growing procedure, initiated at spatio-temporal locations with small gradient magnitudes and concluded at locations with large gradient magnitudes, to yield an initial set of homogeneous sub-volumes. These partitions are further refined by integrating them with an entropy-based texture descriptor as well as color and gradient features in a multivariate volume merging procedure that fuses sub-volumes with similar attributes, to yield the final segmentation. Our approach was tested on several simple-to-complex video sequences with favorable results.	digital video;entropy (information theory);gradient;texture filtering;unsupervised learning	Sreenath Rao Vantaram;Eli Saber	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6466856	image texture;computer vision;image gradient;feature extraction;computer science;morphological gradient;machine learning;pattern recognition;mathematics;region growing;image segmentation;scale-space segmentation	Vision	48.63703412040037	-67.35724197016908	69582
8f6612cca029a7dd39c22abebe55c28a2a9bc2c7	image partioning by level set multiregion competition	level set;image segmentation	The purpose of this study is to investigate a new representation of a partition of an image domain into a given number of regions and its use in the context of region competition to provide an extensional level set multiregion competition algorithm. In contrast with the standard region competition formulation, this multiregion competition formulation leads to a system of coupled curve evolution equations which is easily amenable to a level set implementation. Minimization of the functional guarantees an unambiguous segmentation. We provide a common statement of the multiregion competition algorithm for intensity, motion, and disparity based segmentation. Experimental results are shown.	algorithm;binocular disparity;dvd region code;nist hash function competition	Abdol-Reza Mansouri;Amar Mitiche;Carlos Vázquez	2004	2004 International Conference on Image Processing, 2004. ICIP '04.		image texture;computer vision;mathematical optimization;discrete mathematics;computer science;level set;segmentation-based object categorization;mathematics;image segmentation;scale-space segmentation	Robotics	51.76755346285053	-71.29127251245987	69746
b35dfa866b57b7d4238a0f84fbb7c159c0815d89	unsupervised image segmentation using a simple mrf model with a new implementation scheme	image features;k means clustering;feature space;color image;em algorithm;image segmentation;expectation maximization;synthetic aperture radar;sea ice	A Markov random field (MRF) model with a new implementation scheme is proposed for unsupervised image segmentation based on image features. The traditional two-component MRF model for segmentation requires training data to estimate necessary model parameters and is thus unsuitable for unsupervised segmentation. The new MRF model overcomes this problem by introducing a function-based weighting parameter between the two components. This new MRF model is able to automatically estimate model parameters and is demonstrated to produce more accurate image segmentations than the traditional model using a variety of imagery.	color image;image segmentation;markov chain;markov random field;unsupervised learning	Huawu Deng;David A. Clausi	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334353	computer vision;image resolution;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;markov process;markov model;estimation theory;scale-space segmentation;feature;statistics	Vision	50.90851496524721	-68.7267313007973	69760
7c4f4d8f44611bb1fb3093d2cbc2e0bc7e2e11d6	separation of overlapping dental objects using normal vectors to image region boundaries	dentistry;image segmentation;orthodontics overlapping dental object separation normal vectors image region boundaries dental arch analysis image processing methods computational intelligence region growing method watershed method dental bodies image denoising image components analysis separate dental arch component detection dental plaster casts;stomathology image analysis computational intelligence in biomedicine watershed transform region growing method data fusion object detection;biomedical imaging;indexes;dentistry image segmentation transforms biomedical imaging algorithm design and analysis indexes data acquisition;transforms;vectors dentistry edge detection image denoising image segmentation medical image processing orthotics transforms;data acquisition;algorithm design and analysis	The paper presents a new method for separation of overlapping dental objects allowing more precise analysis of the dental arch using image processing methods and computational intelligence. The methodological part of the paper is devoted to the separation of dental objects using the watershed and region growing methods applied to denoised images of dental bodies. The main part of the paper is devoted to analysis of image components and to the use of normal vectors to image segments boundaries to detect separate dental arch components in the complex environment of overlapping bodies. Experimental part of the paper is devoted to the application of the proposed method to the processing of real images of dental plaster casts acquired with the different illumination. Results include the proposed general algorithm for image components separation allowing further evaluation of dental arch parameters during the operation to allow the efficient treatment in the clinical environment.	algorithm;boyce–codd normal form;computational intelligence;image processing;normal (geometry);region growing;type conversion;watershed (image processing)	Mohammadreza Yadollahi;Ales Procházka;Magdaléna Kasparová;Oldrich Vysata	2015	2015 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM)	10.1109/IWCIM.2015.7347096	computer vision;feature detection;image processing;engineering;biological engineering;dentistry	Robotics	41.7734326191827	-74.02263078934577	69885
12bda87e07d2be59cfcbe2c21b104e9f1e0b9ab2	detection of the pulse waveform characteristic points by wavelet transform using multiscale differential operator	wavelet analysis;cardiovascular parameters;spline;medical signal detection;pulse waveform characteristic points detection;cardiology;differential operators;data mining;wavelet transforms;wavelet transform;vanishing moment;detection rate;wavelet transforms cardiovascular system medical signal detection;vanishing moment pulse waveform characteristic points detection wavelet transform multiscale differential operator cardiovascular parameters;cardiovascular system;multiscale differential operator;wavelet transforms cardiology filter bank cutoff frequency logic band pass filters algorithm design and analysis wavelet analysis risk analysis cardiovascular diseases;algorithm design and analysis;noise	The pulse waveform characteristic point detection is important for non-invasively detecting cardiovascular parameters. A novel designed algorithm based on wavelet transform (WT) is developed. For some pulse waveforms, characteristic points exactly correspond to the zero-crossing of a wavelet with one vanishing moment, while the others incompletely correspond to. And characteristic points also correspond to the local extrema of a wavelet with two vanishing moment. So an algorithm combining a wavelet with one vanishing moment and another one with two vanishing moment is used to improve the detection rate of characteristic points. The results show that automatic identification of characteristics points has a high rate of accuracy. KeywordsPulse waveform; Wavelet analysis; Characteristic point	algorithm;automatic identification and data capture;biorthogonal wavelet;experiment;maxima and minima;sensor;spline (mathematics);spline wavelet;waveform graphics;wavelet transform;zero crossing	Qun Wang;Zhiwen Liu	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5305400	mathematical optimization;mathematical analysis;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;wavelet transform	Visualization	51.446852069417496	-66.64088161923496	70069
115bc8dc3df61e1633d20f9caf39af6d3e68817d	palmprint recognition based on pfi and fuzzy logic	image features;probability feature image;image recognition;fuzzy logic palmprint recognition pfi merged features;probability;merged features;probability feature image palmprint recognition fuzzy logic irregular geometrical shape;stability feature extraction image recognition probability;irregular geometrical shape;data mining;gray scale;stability;fuzzy logic;accuracy;random noise;image edge detection;feature extraction;pixel;palmprint recognition;pfi;fuzzy logic data mining fingers image edge detection gray scale image recognition stability noise shaping thumb fuzzy systems;noise	Palmprint is one of the most reliable features in personal identification because of its stability and uniqueness. This paper proposed a novel palmprint recognition algorithm based on principal lines: a new irregular geometrical shape was employed to get valid palmprint region, which decreased the influence of large noises; not only structure feature, but also intension information were included in the final extracted principal lines, which provides much sufficient clues for palmprint recognition; the probability feature image (PFI) was used in order to suppress random noises in feature image; features from several training samples were merged into one template, which guaranteed the integrity of feature; fuzzy logic was employed in matching algorithm. The high recognition rates obtained in the experiments show the efficiency of proposed algorithm.	algorithm;experiment;fingerprint;fuzzy logic;intension;noise (electronics);palm print;thickness (graph theory)	Leqing Zhu;Sanyuan Zhang;Rui Xing;Yin Zhang	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.30	fuzzy logic;computer vision;stability;feature extraction;computer science;noise;machine learning;pattern recognition;probability;mathematics;accuracy and precision;feature;pixel;grayscale;private finance initiative;statistics	Vision	40.167545643589705	-66.6609415054974	70456
32ff53f0a367d5062c94d5e7d304639fbb052fa0	boundary-trimmed 3d triangular mesh segmentation based on iterative merging strategy	hierarchical clustering;boundary trimming;partitioning;triangular mesh;segmentation;mesh;mesh segmentation;region merging	This paper presents a segmentation algorithm for 3D triangular mesh data. The proposed algorithm uses iterative merging of adjacent triangle pairs based on their orientations. The oversegmented regions are merged again in an iterative region merging process. Finally, the noisy boundaries of each region are refined. The boundaries of each region contain perceptually important geometric information of the entire mesh model. According to the purpose of the segmentation, the proposed mesh-segmentation algorithm supports various types of segmentation by controlling parameters. 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	algorithm;approximation algorithm;cluster analysis;iteration;iterative method;normal (geometry);pattern recognition;polygon mesh;priority queue;randomness;semiconductor industry	Dong Hwan Kim;Il Dong Yun;Sang Uk Lee	2006	Pattern Recognition	10.1016/j.patcog.2005.11.022	mathematical optimization;computer science;triangle mesh;machine learning;segmentation-based object categorization;mathematics;hierarchical clustering;scale-space segmentation;segmentation	Vision	46.48876109991586	-69.90881309545598	70656
5fea4f1877c1bd0043a3459437472398dc870591	a disjoint set algorithm for the watershed transform	computers;image segmentation;queueing theory image segmentation;vegetation;arrays;morphology;watershed transform;transforms;parallel implementation disjoint set algorithm watershed transform image segmentation tarjan union find algorithm fifo queue algorithm;parallel implementation;vegetation signal processing algorithms transforms image segmentation computers morphology arrays;signal processing algorithms	In this paper the implementation of a watershed transform based on Tarjan's Union-Find algorithm is described. The algorithm computes the watershed as defined by Meyer in [4]. The algorithm consists of two stages. In the first stage the image to be segmented is transformed into a lower complete image, using a FIFO-queue algorithm. In the second stage, the watershed of the lower complete image is computed. In this stage no FIFO-queues are used. This feature makes parallel implementation of the watershed transform much easier.	algorithm;fifo (computing and electronics);watershed (image processing)	Arnold Meijster;Jos B. T. M. Roerdink	1998	9th European Signal Processing Conference (EUSIPCO 1998)		computer vision;mathematical optimization;computer science;theoretical computer science;image segmentation;scale-space segmentation;top-hat transform	Vision	44.530621719011556	-68.20359811454739	70909
172d6b72e770c9ec5aa95318411d5c37c2645b96	a robust medical image segmentation method using kl distance and local neighborhood information	journal;local neighborhood information;medical image segmentation;chan vese model;kullback leibler distance	In this paper, we propose an improved Chan-Vese (CV) model that uses Kullback-Leibler (KL) distances and local neighborhood information (LNI). Due to the effects of heterogeneity and complex constructions, the performance of level set segmentation is subject to confounding by the presence of nearby structures of similar intensity, preventing it from discerning the exact boundary of the object. Moreover, the CV model cannot usually obtain accurate results in medical image segmentation in cases of optimal configuration of controlling parameters, which requires substantial manual intervention. To overcome the above deficiency, we improve the segmentation accuracy by the usage of KL distance and LNI, thereby introducing the image local characteristics. Performance evaluation of the present method was achieved through experiments on the synthetic images and a series of real medical images. The extensive experimental results showed the superior performance of the proposed method over the state-of-the-art methods, in terms of both robustness and efficiency.	angular defect;chan's algorithm;experiment;image segmentation;kullback–leibler divergence;medical image;performance evaluation;radiology information systems;synthetic intelligence;biologic segmentation;chan-yu-bao-yuan-tang	Qian Zheng;Zhentai Lu;Wei Yang;Minghui Zhang;Qianjin Feng;Wufan Chen	2013	Computers in biology and medicine	10.1016/j.compbiomed.2013.01.002	computer vision;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;kullback–leibler divergence;scale-space segmentation;statistics	Vision	44.05156940722672	-72.6530690740957	70931
9a9b9549a461eaeaa2d4b5f82724b54b17e9cd7e	applying regional level-set formulation to postsawing four-element led wafer inspection	image segmentation;approximation method;edge detection;light emitting diodes;level set;automatic optical inspection;light emitting diodes filtering theory image segmentation inspection;light emitting diode;indexing terms;inspection;led wafer;artificial neural networks;image edge detection;inspection image segmentation automatic optical inspection light emitting diodes approximation methods artificial neural networks image edge detection;level set formulation;approximation methods;postsawing inspection;automatic threshold regional level set formulation postsawing four element led wafer inspection defect inspection system zero level contour postsawing wafer level set image segmentation;postsawing inspection led wafer level set formulation;filtering theory;artificial neural network	With level-set formulation, new contours can emerge during the evolution of contours. A defect inspection system that utilizes the evolution of zero-level contours for segmenting postsawing wafer is proposed in this study. The system utilizes a regional formulation, which improves the level-set segmentation in images with intensity inhomogeneity. An automatic threshold is used to set the initial contour to a contour near the die region. Fewer iterations are thus required to evolve the zero-level set to segment the wafer. Without the needs for filtering in advance, the inspection can be performed directly on the segmented results. The proposed approach outperforms other postsawing inspection methods in terms of accuracy.	die (integrated circuit);iteration;process (computing);robustness (computer science);sampling (signal processing);software bug;two-phase locking;wafer (electronics)	ChunHsi Li;Chuan-Yu Chang;MuDer Jeng	2011	IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)	10.1109/TSMCC.2010.2088119	computer vision;computer science;machine learning;artificial neural network;light-emitting diode	Vision	45.48528883440722	-71.08965050331616	71030
9c60033d02c1e60f69474536c9340aac41cf33c5	tracking leukocytes from in vivo video microscopy using morphological anisotropic diffusion	image registration tracking video signal processing mathematical morphology diseases image enhancement medical image processing partial differential equations adaptive filters edge detection smoothing methods image sequences;filtering;morphological anisotropic diffusion;partial differential equation;mathematical morphology;video signal processing;adaptive filtering;edge detection;arthritis;endothelium;microscopy;anisotropic diffusion;image enhancement;smoothing methods;adaptive filters;white blood cells in vivo microscopy anisotropic magnetoresistance adaptive filters diseases fasteners in vitro partial differential equations filtering;partial differential equations;in vivo video microscopy;partial differential equation model;medical image processing;image registration;anisotropic magnetoresistance;video frames;diseases;inflammatory disease;enhancing capability;fasteners;adaptive smoothing;edge based registration;edge preservation;leukocyte tracking in vivo video microscopy inflammatory disease endothelium enhancing capability partial differential equation model adaptive filtering edge preservation adaptive smoothing video frames edge based registration background removal arthritis multiple sclerosis morphological anisotropic diffusion;multiple sclerosis;leukocyte tracking;in vivo;adaptive filter;in vitro;tracking;white blood cells;background removal;image sequences	The study of inflammatory disease hinges upon the behavior and movement of leukocytes and their interaction with the endothelium. We put forth a method for tracking leukocytes in vivo, whereas tracking has been demonstrated previously only for in vitro experiments. The tracker is based on the enhancing capability of morphological anisotropic diffusion, a partial differential equation model for adaptively filtering imagery that retains structures of interest. Morphological anisotropic diffusion excels over standard diffusion in the ability to preserve objects of a certain shape and scale, and it improves upon standard morphological filters in terms of edge preservation and adaptive smoothing. We use the video frames enhanced by morphological diffusion for edge-based registration and background removal in the tracking process.		Scott T. Acton;Klaus Ley	2001		10.1109/ICIP.2001.958487	adaptive filter;computer vision;computer science;microscopy;mathematics	Vision	45.65900003749328	-73.80510332455574	71223
275bba7ed6e6ee0c824847144ac8347a24f0dd1e	a hybrid boundary–region left ventricle segmentation in computed tomography	computed tomography;left ventricle	An automatic approach based on the generalized Hough transform (GHT) and unsupervised clustering technique to obtain the endocardial surface is proposed. The approach is applied to multi slice computerized tomography (MSCT) images of the heart. The first step is the initialization, where a GHT–based segmentation algorithm is used to detect the edocardial contour in one MSCT slice. The centroid of this contour is used as a seed point for initializing a clustering algorithm. A two stage segmentation algorithm is used for segmenting the three–dimensional MSCT database. First, the complete database is filtered using mathematical morphology operators in order to improve the left ventricle cavity information in these images. The second stage is based on a region growing method. A seed point located inside the cardiac cavity is used as input for the clustering algorithm. This seed point is propagated along the image sequence to obtain the left ventricle surfaces for all instants of the cardiac cycle. The method is validated by comparing the estimated surfaces with respect to left ventricle shapes drawn by a cardiologist. The average error obtained was 1.52 mm.	algorithm;c++;ct scan;cluster analysis;generalised hough transform;grayscale;image segmentation;library (computing);logical volume management;mathematical morphology;pneumatic artificial muscles;region growing;smoothing;software system;thread (computing);tomography;word lists by frequency	Antonio Bravo;José Clemente;Miguel A. Vera;José Avila;Rubén Medina	2010			computer science	Vision	41.41557976856822	-76.0131534058004	71272
b1a7b9970b1c1fb1ca0097c525bfbe09d13a0e96	carotid automated ultrasound double line extraction system (cadles) via edge-flow	image edge detection ultrasonic imaging image segmentation carotid arteries image resolution gray scale kernel;kernel;image segmentation;image resolution;ultrasound;edge detection;ultrasonic imaging;gray scale;carotid arteries;intima media thickness;image edge detection;carotid artery;figure of merit;algorithms carotid arteries carotid artery diseases humans image enhancement image interpretation computer assisted pattern recognition automated reproducibility of results sensitivity and specificity ultrasonography	This paper presents a completely user-independent algorithm, that automatically extracts the far (distal) double line (lumen-intima and media-adventitia) in the carotid artery using an Edge Flow technique (a class of AtheroEdge™ systems) based on directional probability maps using the attributes of intensity and texture. The extracted double line translates into a measure of the intima-media thickness (IMT), a validated marker for the progression of atherosclerosis. The Carotid Automated Double Line Extraction System based on Edge-Flow (CADLES-EF) is characterized and validated by comparing the output of the algorithm with two other completely automatic techniques (CALEXia and CULEXsa) published by the same authors. Validation was performed on a multi-institutional database of 300 longitudinal B-mode carotid images with normal and pathologic arteries. CADLES-EF showed an intima-media thickness (IMT) bias of 0.043±0.097 mm in comparison to CALEXia and CULEXsa that showed 0.134±0.0.88 mm and 0.74±0.092 mm, respectively. The system's Figure of Merit (FoM) showed an improvement when compared to previous automated methods: CALEXia and CULEXsa, leading to values of 84.7%, 91.5%, while our new approach, CADLES-EF performed the best with 94.8%.	adventitia;atherosclerosis;color gradient;culture media, serum-free;entity framework;extraction;interactive machine translation;international medical terminology;map;numerous;otitis media;scientific publication;structure of lumen of body system;thickness (graph theory);algorithm	Kristen M. Meiburger;Filippo Molinari;Guang Zeng;Luca Saba;Jasjit S. Suri	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090107	computer vision;figure of merit;kernel;edge detection;radiology;image resolution;computer science;ultrasound;image segmentation;engineering drawing;grayscale	Robotics	39.266989719028224	-77.56956194063173	71289
f63725bbebc90d5d4ca5829c384193a84b52e1bc	retinal thickness estimation from sd-oct macular scans	topology;image segmentation;retina image segmentation deformable models topology optical imaging shape biomedical optical imaging;retinal thickness;heidelberg spectralis segmentation retinal thickness estimation sd oct macular scans spectral domain optical coherence tomography scans glaucoma blindness inner limiting membrane retinal pigmented epithelium local surface smoothness inter surface distance smoothness oct image stack segmentation;deformable models;vision defects biomedical optical imaging biomembranes diseases eye image segmentation medical image processing optical tomography thickness measurement;interacting deformable surfaces retinal thickness early detection of glaucoma;optical imaging;shape;retina;early detection of glaucoma;interacting deformable surfaces;biomedical optical imaging	Glaucoma, a leading cause of blindness worldwide, can be detected using retinal thicknesses from spectral-domain optical coherence tomography (SD-OCT) scans of the macula. We calculate the desired thickness maps as the distance between the inner-limiting membrane (ILM) and retinal pigmented epithelium (RPE) of the retina. To delineate these two layers, we use a set of two deformable open surfaces that are driven by intensity contrast, while preserving their shape and topology properties, i.e. local surface smoothness and inter-surface distance smoothness. To evaluate our method, qualified graders manually segmented 30 random sections from 20 OCT image stacks, in triplicate; we make comparisons with obtained ground-truth and the clinically tested Heidelberg Spectralis segmentation. We show the superiority of our method with respect to accuracy and average execution time (~7 secs), validating it as a clinical tool.	ground truth;map;run time (program lifecycle phase);thickness (graph theory);tomography;virtual retinal display	N. Hammes;Lyne Racette;B. C. Samuels;Gavriil Tsechpenakis	2015	2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2015.7163852	computer vision;shape;computer science;optical imaging;mathematics;image segmentation;optics	Vision	40.20725490170816	-78.71261077994879	71406
4d97e0c839b3e7789896af55e7ee4ad51d09c179	pixel-level snakes on the cnnum: algorithm design, on-chip implementation and applications	cnn universal machine;diseno circuito;active contour;image processing;implementation;analogic algorithms;circuit design;procesamiento imagen;deformable models;cellular neural nets;traitement image;chip;algorithme;reseau neuronal cellulaire;algorithm;modelo deformable;contorno activo;feature extraction;modele deformable;cellular active contours;conception circuit;contour actif;extraction caracteristique;cellular neural networks;implementacion;deformable model;algorithm design;algoritmo	Abstract#R##N##R##N#In this paper, a new algorithm for the cellular active contour technique called pixel-level snakes is proposed. The motivation is twofold: on the one hand, a higher efficiency and flexibility in the contour evolution towards the boundaries of interest are pursued. On the other hand, a higher performance and suitability for its hardware implementation onto a cellular neural network (CNN) chip-set architecture are also required. Based on the analysis of previous schemes the contour evolution is improved and a new approach to manage the topological transformations is incorporated. Furthermore, new capabilities in the contour guiding are introduced by the incorporation of inflating/deflating terms based on the balloon forces for the parametric active contours. The entire algorithm has been implemented on a CNN universal machine (CNNUM) chip set architecture for which the results of the time performance measurements are also given. To illustrate the validity and efficiency of the new scheme several examples are discussed including real applications from medical imaging. Copyright © 2005 John Wiley & Sons, Ltd.	algorithm design;pixel	David López Vilariño;Csaba Rekeczky	2005	I. J. Circuit Theory and Applications	10.1002/cta.302	chip;algorithm design;computer vision;cellular neural network;image processing;feature extraction;computer science;artificial intelligence;circuit design;active contour model;implementation;algorithm	EDA	48.54563399377246	-70.94647701525234	71604
d0edadf33b409c900146f88de0f86740f3a0066c	parallel optimization approaches for medical image registration	distributed memory;similarity metric;medical image;image registration;parallel computer;parallel implementation;optimal algorithm	Optimization of a similarity metric is an essential component in most medical image registration approaches based on image intensi- ties. The increasing availability of parallel computers makes parallelizing some registration tasks an attractive option. In this paper, two relatively new, deterministic, direct optimization algorithms are parallelized for distributed memory systems, and adapted for image registration. DI- RECT is a global technique, and the multidirectional search is a recent local method. The performance of several variants are compared. Ex- perimental results show that both methods are robust, accurate, and, in parallel implementations, can significantly reduce computation time.	image registration;paradiseo	Mark P. Wachowiak;Terry M. Peters	2004		10.1007/978-3-540-30135-6_95	computer vision;mathematical optimization;distributed memory;computer science;image registration;theoretical computer science	Vision	50.75005875030048	-78.52763924665689	71625
4f8fb7c6425fccdd2f40c277882e2284c5deb99c	prediction of coefficients from coarse to fine scales in the complex wavelet transform	resolution enhancement systems coarse scale fine scales complex wavelet transform coefficients dwt coefficients redundancy shift invariant complex wavelet transform coarse to fine scale prediction magnitude phase step responses coefficient prediction algorithm edge based image model local edge image coding;image coding;image resolution;high pass filters;complex wavelet transform;high pass filters wavelet transforms redundancy prediction theory image coding image resolution image enhancement low pass filters;wavelet transforms;image enhancement;redundancy;prediction theory;resolution enhancement;low pass filters;image modeling;shift invariant;wavelet transforms continuous wavelet transforms discrete wavelet transforms image coding gabor filters frequency noise reduction predictive models nonlinear filters biomedical signal processing	Prediction of fine scale DWT coefficients from coarser scales is not generally possible. On the other hand, the redundancy of the shift-invariant complex wavelet transform (CWT) should allow good coarse-to-fine scale prediction. The CWT's magnitude and phase step responses are well-behaved functions of a coefficient's normal distance to an edge. We present a coefficient prediction algorithm which assumes an edge-based image model. The algorithm infers information about local edges from coarse scale coefficients, and predicts the effect of these edges on the magnitudes and phases of finer level coefficients. Accurate coarse-to-fine prediction of coefficients has important potential benefits for image coding and resolution enhancement systems.	coefficient;complex wavelet transform	Tanya H. Reeves;Nick G. Kingsbury	2000		10.1109/ICASSP.2000.862029	wavelet;computer vision;speech recognition;harmonic wavelet transform;image resolution;second-generation wavelet transform;continuous wavelet transform;low-pass filter;computer science;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;redundancy;high-pass filter;discrete wavelet transform;lifting scheme;shift-invariant system;wavelet transform	Vision	52.01956920929416	-66.29096858978252	71668
a1f4757dc72055cbc6600642cab7022b7a2ba472	nonlocal means two dimensional histogram-based image segmentation via minimizing relative entropy				Chundi Jiang;Wei Yang;Yu Guo;Fei Wu;Yinggan Tang	2018	Entropy	10.3390/e20110827		Vision	50.36896273092575	-70.55847807639609	71681
d12aad4327aaa12b40c3b521e9acecffefa3ace8	shape constrained vessel centerline extraction by integrating surface evolution and topology analysis	topology;shape constraints;image segmentation;bifurcation;calcifications;bone structures;cta;biomedical imaging;skull;active contours;data mining;vessel axis tracking;skeleton;medical image processing bifurcation blood vessels computerised tomography diagnostic radiography image segmentation;shape;internal carotid arteries;shape constrained vessel centerline extraction;medical image processing;internal carotid artery;computerised tomography;biomedical image processing;skull base;image analysis;surface evolution;topology analysis;carotid bifurcation;shape topology bifurcation data mining image segmentation skull active contours image analysis biomedical imaging skeleton;diagnostic radiography;blood vessels;calcifications shape constrained vessel centerline extraction surface evolution topology analysis vessel axis tracking image segmentation shape constraints cta internal carotid arteries skull base bone structures carotid bifurcation;biomedical x ray imaging	A novel approach for vessel axis tracking is presented based on surface evolution in 3D. The main idea is to guide the evolution by analyzing the topology of intermediate segmentation results, and in particular, to impose shape constraints on the topology. For example, the topology can be constrained to represent a bifurcation, which can be imposed by extracting three different connected paths with maximum length from the skeleton of intermediate segmentation results. The evolving surface is then re-initialized with the newly found topology. Re-initialization is a crucial step since it creates probing behavior of the evolving front and prevents the surface from leaking into the background. The method was evaluated in two CTA applications (i) extracting the internal carotid arteries including the region in which they traverse through the skull base, which is challenging due to the close proximity of bone structures and overlap in intensity values, and (ii) extracting the carotid bifurcation, some of them severely stenosed and most of them containing calcifications. Using only the image gradient as the image force in the surface evolution and a single seed point as initialization, the method was successful in 80% of ten internal carotids in five patients, and 80% of ten carotid bifurcations in five patients, respectively	apache axis;bifurcation theory;evolution;image gradient;traverse	Rashindra Manniesing;Wiro J. Niessen	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1624878	computer vision;image analysis;radiology;shape;computer science;image segmentation;skeleton	Vision	41.22481892281297	-76.62732121709456	71693
0d2ac4fe8eedc2a1280051994d4166a35defbe48	level set-based bimodal segmentation with stationary global minimum	equation derivee partielle;equation non lineaire;traitement signal;ecuacion no lineal;nonlinear partial differential equation active contour image segmentation level set mumford shah model;partial differential equation;ecuacion derivada parcial;euler lagrange equation;ecuacion euler lagrange;detection forme;heaviside functions level set based bimodal segmentation stationary global minimum level set based partial differential equation energy functional chan vese model level set function euler lagrange equation convergence state termination criterion;active contour;image segmentation;image processing;equation euler lagrange;edge detection;nonlinear partial differential equation;relacion convergencia;algorithms artificial intelligence image enhancement image interpretation computer assisted information storage and retrieval nonlinear dynamics pattern recognition automated reproducibility of results sensitivity and specificity;level set;procesamiento imagen;partial differential equations image segmentation;taux convergence;convergence rate;shape detection;level set image segmentation active contours convergence partial differential equations steady state image converters tracking power engineering and energy mathematics;traitement image;energy function;deteccion contorno;algorithme;algorithm;detection contour;deteccion forma;contorno activo;shah model;partial differential equations;partial differential equations image segmentation heaviside functions level set based bimodal segmentation stationary global minimum level set based partial differential equation energy functional chan vese model level set function euler lagrange equation convergence state termination criterion image segmentation partial differential equations nonlinear partial differential equation active contour image segmentation level set mumford 8211;feature extraction;signal processing;segmentation image;contour actif;extraction caracteristique;mumford shah model;non linear equation;procesamiento senal	In this paper, we propose a new level set-based partial differential equation (PDE) for the purpose of bimodal segmentation. The PDE is derived from an energy functional which is a modified version of the fitting term of the Chan-Vese model . The energy functional is designed to obtain a stationary global minimum, i.e., the level set function which evolves by the Euler-Lagrange equation of the energy functional has a unique convergence state. The existence of a global minimum makes the algorithm invariant to the initialization of the level set function, whereas the existence of a convergence state makes it possible to set a termination criterion on the algorithm. Furthermore, since the level set function converges to one of the two fixed values which are determined by the amount of the shifting of the Heaviside functions, an initialization of the level set function close to those values can result in a fast convergence	chan's algorithm;convergence (action);epilepsies, partial;euler;euler–lagrange equation;maxima and minima;stationary process;biologic segmentation;chan-yu-bao-yuan-tang	Suk Ho Lee;Jin Keun Seo	2006	IEEE Transactions on Image Processing	10.1109/TIP.2006.877308	computer vision;mathematical optimization;mathematical analysis;image processing;computer science;calculus;signal processing;mathematics;partial differential equation;set function;level set method	Vision	50.959702950333515	-70.82408038215358	71830
9bdc14e4694c2cfa1cbc922da5706df56c7fb479	overlay network testing by opendht	image fusion image segmentation image sensors sensor fusion software engineering artificial intelligence distributed computing mechanical engineering level set convergence;region based image fusion;image segmentation;image segmentation image fusion image registration;piecewise smooth;image fusion;level set;image registration region based image fusion energy estimation image segmentation piecewise smooth mumford shah model level set based optimal algorithm;level set based optimal algorithm;side effect;image registration;energy estimation;piecewise smooth mumford shah model;fusion rule;numerical experiment;optimal algorithm;energy estimate	A problem of research on overlay network is lack of testing platform. Recent research has shown that one can use distributed hash tables (DHTs) to build scalable, robust and efficient applications. However, large-scale distributed systems are hard to deploy, nor are DHTs . Open DHT is a publicly accessible DHT service. In contrast to the usual DHT model, clients of OpenDHT can issue put and get operations to any DHT node, which processes the operations on their behalf. This paper introduces Open DHT and concentrates on how to do the network testing based on OpenDHT. The testing about resource location is also expounded.	distributed computing;distributed hash table;overlay network;scalability	Xun Duan;Jian-shi Li	2007	Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)	10.1109/SNPD.2007.444	computer vision;mathematical optimization;computer science;image registration;level set;pattern recognition;image segmentation;image fusion;programming language;side effect	SE	45.91063697982868	-75.22374657536561	71836
8522e48ae1279206d7ad6a512c154996c3d7f63f	automatic segmentation of pathological lung using incremental nonnegative matrix factorization	incremental nonnegative matrix factorization absolute lung volume difference modified hausdorff distance dice coefficient realistic lung phantoms lung tissues inmf based segmentation automatic pathological lung segmentation computer assisted lung cancer diagnostics large size chest computed tomographic images;lungs image segmentation pathology computed tomography three dimensional displays matrix decomposition context;phantoms cancer image segmentation lung matrix decomposition medical image processing;incremental learning nonnegative matrix factorization lung segmentation	Accurate segmentation of pathological lungs from large-size chest computed tomographic images is crucial for computer-assisted lung cancer diagnostics. In this paper, a new framework for automatic pathological lung segmentation is proposed. The proposed INMF-based segmentation approach has the ability to handle the in-homogeneities caused by the arteries, veins, bronchi, and possible pathologies that may exist in the lung tissues, and to detect the number of clusters in the image in an automated manner. The proposed INMF-based segmentation framework is quantitatively validated on simulated realistic lung phantoms that mimic different lung pathologies (7 datasets), in vivo data sets for 17 subjects, and for lung disease with severe pathologies. Three metrics are used: the Dice coefficient, modified Hausdorff distance, and absolute lung volume difference. Results show that the proposed approach outperforms existing lung segmentation techniques and can handle in-homogenities caused by different pathologies.	ct scan;hausdorff dimension;non-negative matrix factorization;sørensen–dice coefficient;video-in video-out	Ehsan Hosseini-Asl;Jacek M. Zurada;Ayman El-Baz	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351376	computer vision;mathematical optimization;mathematics	Vision	40.74103530795429	-79.04899138888099	71865
27f38daa9dc7d74d677470e2cf6b727ca99c6791	a hybrid scheme for contour detection and completion based on topological gradient and fast marching algorithms - application to inpainting and segmentation	communication conference;fast marching;topological gradient;contour completion	We combine in this paper the topological gradient, which is a powerful method for edge detection in image processing, and a variant of the minimal path method in order to find connected contours. The topological gradient provides a more global analysis of the image than the standard gradient, and identifies the main edges of an image. Several image processing problems (e.g. inpainting and segmentation) require continuous contours. For this purpose, we consider the fast marching algorithm, in order to find minimal paths in the topological gradient image. This coupled algorithm quickly provides accurate and connected contours. We present then two numerical applications, to image inpainting and segmentation, of this hybrid algorithm.	contour line;edge detection;fast fourier transform;fast marching method;gradient;hybrid algorithm;image processing;inpainting;numerical analysis;reflections of signals on conducting lines;thresholding (image processing);topological derivative	Y. Ahipo;Didier Auroux;Laurent D. Cohen;Mohamed Masmoudi	2011		10.1007/978-3-642-24785-9_33	computer vision;mathematical optimization;topology;marching squares;image gradient;morphological gradient;mathematics;image segmentation	Vision	52.79269429748496	-69.70808373038368	71867
e7278ffcbbc9e5a4ee25b60c68d36ba88858512b	automatic mass segmentation based on adaptive pyramid and sublevel set analysis	image segmentation;inclusion tree sublevel set mammogram segmentation;cancer;level set;segmentation;sublevel refinement automatic mass segmentation adaptive pyramid analysis sublevel set analysis screening mammograms anisotropic smoothing component merging;set theory;breast;medical image processing;pixel;image segmentation shape mammography testing anisotropic magnetoresistance smoothing methods breast level set digital images computer applications;anisotropic magnetoresistance;merging;inclusion tree;sublevel set;mammography;mammogram;article;set theory image segmentation mammography medical image processing	A method based on sublevel sets is presented for refining segmentation of screening mammograms. Initial segmentation is provided by an adaptive pyramid (AP) scheme which is viewed as seeding of the final segmentation by sublevel sets. Performance is tested with and without prior anisotropic smoothing and is compared to refinement based on component merging. The combination of anisotropic smoothing, AP segmentation and sublevel refinement is found to outperform other combinations.	anisotropic diffusion;refinement (computing);smoothing;virtual screening	Fei Ma;Mariusz Bajger;Murk J. Bottema	2009	2009 Digital Image Computing: Techniques and Applications	10.1109/DICTA.2009.47	magnetoresistance;computer vision;computer science;level set;machine learning;mathematics;image segmentation;scale-space segmentation;segmentation;pixel;cancer;set theory;computer graphics (images)	Vision	42.02391923837779	-74.49006645229579	71874
7a9bfd62b505d1a2e06ef44747720e69d319bb43	texture-based graph regularization process for 2d and 3d ultrasound image segmentation	graph theory;image segmentation;indicator function diffusion texture based graph regularization process 2d ultrasound image segmentation 3d ultrasound image segmentation unsupervised segmentation algorithm graph diffusion regularization model haralick texture features nonlocal processing techniques graph structure nonadjacent pixels segmentation quality improvement biomedical imaging image texture analysis;image resolution;edge detection;image texture;image segmentation mathematical model equations ultrasonic imaging image edge detection active contours;medical image processing biomedical ultrasonics edge detection feature extraction graph theory image resolution image segmentation image texture;image texture analysis image segmentation biomedical imaging;feature extraction;medical image processing;biomedical ultrasonics	In this paper, we propose to improve an unsupervised segmentation algorithm based on the graph diffusion and regularization model described by Ta in [1] by using Haralick texture features. With this framework, segmentation is performed by diffusing an indicator function over a graph representing an image. The benefit of our approach is to combine two non-local processing techniques: at pixel level with texture features and through the use of a graph structure, which allows to efficiently express relations between non-adjacent pixels. Applied on ultrasound images, and compared to a vector-valued Chan & Vese active contour, our method shows an improvement of the quality of segmentation.	active contour model;algorithm;co-occurrence matrix;contour line;graph (discrete mathematics);image segmentation;matrix regularization;pixel;robert haralick;unsupervised learning	Cyrille Faucheux;Julien Olivier;Romuald Boné;Pascal Makris	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467364	image texture;computer vision;feature detection;range segmentation;edge detection;image resolution;binary image;image processing;feature extraction;computer science;graph theory;morphological gradient;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;connected-component labeling	Vision	45.7888655873961	-70.17568814405715	71928
ac02784d096994e92c7e2b6637c415f3eaba30e0	analysis of kernel method for surface curvature estimation	convolution;smoothing	We examine a method of curvature estimation that computes the curvature directly from three dimensional data. We refer to this as the kernel method of curvature estimation and our experiments indicate that several parameters be modified from those originally suggested to achieve more accurate and reliable results. This improved performance is essential for analysis of medical volume data by computer aided diagnosis algorithms, many of which use curvature estimation for shape computations and pattern recognition. We also examine cases in which the kernel method yields inaccurate responses based on specific topologies. D 2004 CARS and Elsevier B.V. All rights reserved.	algorithm;computation;experiment;kernel density estimation;kernel method;pattern recognition	Shannon R. Campbell;Ronald M. Summers	2004			econometrics;mathematical optimization;mathematics;statistics;smoothing	Vision	49.04273986570935	-73.81491639932234	71937
1bcdc6e144a2c7b5c3c7b9d7e31120a6d2e236c4	non-flat clustering with alpha-divergences	histograms;cluster algorithm;generators;approximate algorithm;image segmentation;approximation algorithms;approximation method;k means;k means alpha divergence clustering information geometry;euclidean distance;image segmentation nonflat clustering alpha divergences k means algorithm k means initialization method bregman k means algorithm bregman divergences bregman seeding α divergence;information geometry;clustering;clustering algorithms;k means algorithm;approximation methods;alpha divergence;clustering algorithms image segmentation approximation algorithms histograms generators approximation methods euclidean distance	The scope of the well-known k-means algorithm has been broadly extended with some recent results: first, the k-means++ initialization method gives some approximation guarantees; second, the Bregman k-means algorithm generalizes the classical algorithm to the large family of Bregman divergences. The Bregman seeding framework combines approximation guarantees with Bregman divergences. We present here an extension of the k-means algorithm using the family of α-divergences. With the framework for representational Bregman divergences, we show that an α-divergence based k-means algorithm can be designed. We present preliminary experiments for clustering and image segmentation applications. Since α-divergences are the natural divergences for constant curvature spaces, these experiments are expected to give information on the structure of the data.	algorithm;approximation;bregman divergence;cluster analysis;experiment;image segmentation;k-means clustering;k-means++;whole earth 'lectronic link	Olivier Schwander;Frank Nielsen	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946740	mathematical optimization;computer science;machine learning;pattern recognition;mathematics;cluster analysis;approximation algorithm;bregman divergence;k-means clustering	Robotics	49.22914895438146	-72.30493336521886	71956
7c7ec6069c035a8c69c9b38854bb3013909c260d	motion tracking of the outer tips of microtubules	microtubule dynamics;microtubule outer tips;motion tracking;neurodegenerative disease;fluorescence microscopy;visual inspection;subcellular motion tracking;live cell imaging;cell division;tubular structure segmentation;confocal microscopy	Microtubules are tubular biopolymers of the cytoplasm. They play numerous critical roles in a cell such as providing mechanical support and structural tracks for the anchoring and transport of chromosomes, organelles, and vesicles. They also form the microtubule assembly, which is critical for the coordination of mitosis and cell migration. The most dynamic part of the assembly are the microtubule outer, plus, tips located close to the cortex of the cell. Abnormal function of the assembly has been implicated in cell pathology such as neurodegenerative diseases and cancer. To date the study of the dynamics of the microtubule assembly is often performed qualitatively by visual inspection or quantitatively by manual annotation of the locations of the tips over time in an image sequence, which is very tedious. In this work we have developed a method to automatically track microtubule tips so as to enable a more extensive and higher throughput quantitative study of the microtubule assembly. Our approach first uses the entire image sequence to estimate the region in which a tip oscillates. In that region a tip feature is computed for all time and subsequently used to form the tip trajectory. Last, we evaluate our method with phantom as well as real data. The real data show fluorescently tagged living cells imaged with epifluorescent microscopy or confocal microscopy.	assembly language;biopolymers;microscopy, confocal;microtubule polymerization process;microtubules;mitosis;neurodegenerative disorders;organelles;phantom reference;phantoms, imaging;throughput;tracer;track (course);vesicle (morphologic abnormality);visual inspection	Stathis Hadjidemetriou;Derek Toomre;James S. Duncan	2008	Medical image analysis	10.1016/j.media.2008.04.004	fluorescence microscope;confocal laser scanning microscopy;cell division;live cell imaging;visual inspection	Visualization	40.36505413533747	-73.94283333324192	72072
89eefe90413eb9fa56c507858a86b9af997718d6	automated analysis of nerve-cell images using active contour models	traitement automatise;systeme nerveux pathologie;biology computing;biological patents;electron microscopy brain cellular biophysics biology computing biological techniques physiological models image processing neurophysiology;poor image quality;neurone;informatica biomedical;axon boundaries;brain;biomedical data processing;biomedical journals;snakes;optimization scheme;electron micrographic images;image processing;systeme nerveux central;text mining;genie biomedical;europe pubmed central;edge detection;image converters;citation search;informatique biomedicale;procesamiento imagen;nerve fibers;active contours;data mining;citation networks;axon;traitement image;false axon detection;image data;deteccion contorno;algorithme;cells;algorithm;detection contour;sistema nervioso central;important neuroanatomical features;brain modeling;neurona;nervous system diseases;electron microscopy;nerve cell images;biomedical engineering;research articles;microscopia electronica;abstracts;tratamiento automatizado;image quality;open access;sistema nervioso patologia;mathematical transformations;active contour models;life sciences;accurate sheath measurement;clinical guidelines;conflict resolution scheme;axone;image analysis;evaluation;ingenieria biomedica;hough transform;irrelevant cell features;neurons;computer science;neurophysiology;evaluacion;biological techniques;conflict resolution scheme nerve cell images active contour models nerve fibers snakes axon size important neuroanatomical features electron micrographic images elliptical hough transform procedure axon boundaries image data optimization scheme accurate sheath measurement false axon detection poor image quality irrelevant cell features;full text;elliptical hough transform procedure;axon size;conflict resolution;article;computer simulation;physiological models;cellular biophysics;active shape model;rest apis;central nervous system;neuron	The number of nerve fibers (axons) in a nerve, the axon size, and shape can all be important neuroanatomical features in understanding different aspects of nerves in the brain. However, the number of axons in a nerve is typically in the order of tens of thousands and a study of a particular aspect of the nerve often involves many nerves. Potentially meaningful studies are often prohibited by the huge number involved when manual measurements have to be employed. A method that automates the analysis of axons from electron-micrographic images is presented. It begins with a rough identification of all the axon centers by use of an elliptical Hough transform procedure. Boundaries of each axons are then extracted based on active contour model, or snakes, approach where physical properties of the axons and the given image data are used in an optimization scheme to guide the snakes to converge to axon boundaries for accurate sheath measurement. However, false axon detection is still common due to poor image quality and the presence of other irrelevant cell features, thus a conflict resolution scheme is developed to eliminate false axons to further improve the performance of detection. The developed method has been tested on a number of nerve images and its results are presented.	active contour model;axon;converge;electron;extraction;hough transform;image quality;mathematical optimization;nerve fibers;neuron;relevance;snakes;tissue fiber;transcutaneous electric nerve stimulation	Ying-Lun Fok;Joseph C. K. Chan;Roland T. Chin	1996	IEEE transactions on medical imaging	10.1109/42.500144	computer simulation;computer vision;text mining;image analysis;image processing;computer science;artificial intelligence;conflict resolution;active contour model;neurophysiology	Vision	45.74897636240249	-79.17804537133735	72139
eb3344eeef17290d91d503afcc89566788e0690d	white matter lesion extension to automatic brain tissue segmentation on mri	white matter hyperintensities;white matter lesions;white matter;brain tissue segmentation;brain atlases;automatic segmentation;magnetic resonance image;mri;fluid attenuated inversion recovery;gray matter;white matter hyperintensity;k nearest neighbor;false positive;brain tissue;quantitative evaluation;cerebrospinal fluid	A fully automated brain tissue segmentation method is optimized and extended with white matter lesion segmentation. Cerebrospinal fluid (CSF), gray matter (GM) and white matter (WM) are segmented by an atlas-based k-nearest neighbor classifier on multi-modal magnetic resonance imaging data. This classifier is trained by registering brain atlases to the subject. The resulting GM segmentation is used to automatically find a white matter lesion (WML) threshold in a fluid-attenuated inversion recovery scan. False positive lesions are removed by ensuring that the lesions are within the white matter. The method was visually validated on a set of 209 subjects. No segmentation errors were found in 98% of the brain tissue segmentations and 97% of the WML segmentations. A quantitative evaluation using manual segmentations was performed on a subset of 6 subjects for CSF, GM and WM segmentation and an additional 14 for the WML segmentations. The results indicated that the automatic segmentation accuracy is close to the interobserver variability of manual segmentations.	atlases;cerebrospinal fluid;gray matter;k-nearest neighbors algorithm;leukocytes;magnetic resonance imaging;modal logic;nearest neighbour algorithm;numerous;registration;single linkage cluster analysis;spatial variability;subgroup;website meta language;white matter;biologic segmentation	Renske de Boer;Henri A. Vrooman;Fedde van der Lijn;Meike W. Vernooij;M. Arfan Ikram;Aad van der Lugt;Monique M. B. Breteler;Wiro J. Niessen	2009	NeuroImage	10.1016/j.neuroimage.2009.01.011	psychology;computer vision;radiology;medicine;hyperintensity;pathology;type i and type ii errors;magnetic resonance imaging;fluid-attenuated inversion recovery;nuclear medicine;k-nearest neighbors algorithm	Vision	40.52493847094886	-79.82364746872965	72334
b15d59843fc47d446eb3c8314981dc2a891dc144	diffusion kurtosis imaging with tract-based spatial statistics reveals white matter alterations in preschool children	statistical analysis biodiffusion biological tissues biomedical mri brain cellular biophysics neurophysiology;biological tissues;brain;white matter;conference_paper;biodiffusion;tract based spatial statistics tbss;diffusion tensor imaging pediatrics correlation sensitivity educational institutions anisotropic magnetoresistance;statistical analysis;dti parameters diffusion kurtosis imaging tract based spatial statistics white matter alterations preschool children nongaussian water diffusion neural tissues brain structures diffusion tensor imaging correlation analysis regions of interest fractional anisotropy mean kurtosis axial kurtosis radial kurtosis mean diffusivity radial diffusivity dki parameters;preschool children diffusion kurtosis imaging dki tract based spatial statistics tbss white matter;neurophysiology;diffusion kurtosis imaging dki;cellular biophysics;biomedical mri;algorithms brain child child preschool data interpretation statistical diffusion tensor imaging female humans image interpretation computer assisted imaging three dimensional infant male nerve fibers myelinated reproducibility of results sensitivity and specificity;preschool children	Diffusion kurtosis imaging (DKI), an extension of diffusion tensor imaging (DTI), provides a practical method to describe non-Gaussian water diffusion in neural tissues. The sensitivity of DKI to detect the subtle changes in several chosen brain structures has been studied. However, intuitive and holistic methods to validate the merits of DKI remain to be explored. In this paper, tract-based spatial statistics (TBSS) was used to demonstrate white matter alterations in both DKI and DTI parameters in preschool children (1-6 years; n=10). Correlation analysis was also performed in multiple regions of interest (ROIs). Fractional anisotropy, mean kurtosis, axial kurtosis and radial kurtosis increased with age, while mean diffusivity and radial diffusivity decreased significantly with age. Fractional anisotropy of kurtosis and axial diffusivity were found to be less sensitive to the changes with age. These preliminary findings indicated that TBSS could be used to detect subtle changes of DKI parameters on the white matter tract. Kurtosis parameters, except fractional anisotropy of kurtosis, demonstrated higher sensitivity than DTI parameters. TBSS may be a convenient method to yield higher sensitivity of DKI.	diffusion kurtosis imaging;diffusion tensor imaging;folic acid;fractional anisotropy;holism;leukoaraiosis;menkes kinky hair syndrome;molecular dynamics;nerve tissue;normal statistical distribution;radial (radio);region of interest;spatial analysis;tract (literature);white matter;brain development	Xianjun Li;Jie Gao;Xin Hou;Kevin C. Chan;Abby Ding;Qinli Sun;Mingxi Wan;Ed X. Wu;Jian Yang	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346422	psychology;neuroscience;nuclear magnetic resonance;neurophysiology;statistics	Vision	43.15603235983057	-78.86386544507945	72423
f104431adf2be528bb8858a16b473ae5bad4154b	reconstruction of electrical impedance tomography images using chaotic ring-topology particle swarm optimization and non-blind search	tomography particle swarm optimization electric potential image reconstruction impedance electrodes conductivity;ring topology pso electrical impedance tomography image reconstruction chaotic ring topology particle swarm optimization noninvasive imaging e health diagnostic technology nonionizing radiation electric potentials surface electrodes eit image reconstruction nonblind initial search anatomically consistent noisy solution gauss newton reconstruction method saha criterion bandyopadhyay criterion iterative process ground truth images relative mean squared error error magnitudes classical pso;chaos electrical impedance tomography image reconstruction reconstruction algorithms particle swarm optimization;particle swarm optimisation bioelectric potentials electric impedance imaging image reconstruction mean square error methods medical image processing newton method	Non-invasive imaging and e-health have been increasing in the last decades, as a result of the efforts to generate diagnostic technology based on non-ionizing radiation. Electrical Impedance Tomography (EIT) is a low-cost, non-invasive, portable, and safe of handling imaging technique based on measuring the electric potentials generated by the application of currents in pairs of surface electrodes. Nevertheless, EIT image reconstruction is still an open problem, due to its nature as an ill-posed problem governed by the Equation of Poison. Several numerical methods are used in order to solve this equation without generating anatomically inconsistent results. Particle swarm algorithms can be used as alternatives to Gauss-Newton and Backprojection well-known approaches, which frequently generate low-resolution blurred images. Furthermore, Gauss-Newton convergence to anatomically consistent images is not guaranteed, needing to set constraints like the number of anatomical structures present on the image domain. Herein this work we present EIT reconstruction methods based on the optimization of the relative error of reconstruction using chaotic particle swarm algorithms with non-blind initial search. We studied two forms of initialization: totally random and including an imperfect but anatomically consistent noisy solution based on Gauss-Newton reconstruction method, according to Saha and Bandyopadhyay's criterion for non-blind initial search in optimization algorithms, in order to guide the iterative process to avoid anatomically inconsistent solutions and avoid convergence to local minima. Results were quantitatively evaluated with ground-truth images using the relative mean squared error, showing that our results reached low error magnitudes. Qualitative evaluation also indicated that our results were morphologically consistent, mainly for classical PSO and ring-topology PSO with non-blind initial search.	approximation error;characteristic impedance;electromagnetically induced transparency;gauss–newton algorithm;ground truth;iterative method;iterative reconstruction;mathematical optimization;maxima and minima;mean squared error;newton;nominal impedance;numerical method;particle swarm optimization;ring network;software portability;swarm intelligence;tomography;well-posed problem	Allan R. S. Feitosa;Reiga R. Ribeiro;Valter A. F. Barbosa;Ricardo Emmanuel de Souza;Wellington Pinheiro dos Santos	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974322	iterative reconstruction;computer vision;mathematical optimization	Vision	51.88160991572562	-79.88419638855304	72603
1028fb97c096ce46a3ad14594c5abed5b58d288e	efficient image reconstruction under sparsity constraints with application to mri and bioluminescence tomography	fista;optimisation;convergence;order of magnitude speed enhancement sparsity constraints mri bioluminescence tomography optimization problem biomedical image reconstruction molecular tomography;biological system modeling;restoration;biomedical imaging;thresholding algorithm;magnetic resonance image;fwista bioluminescence tomography parallel mri inverse problem fista;sensitivity;magnetic resonance imaging image reconstruction convergence sensitivity biological system modeling biomedical imaging tomography;linear inverse problems;inverse problem;fwista;image reconstruction;medical image processing;magnetic resonance imaging;parallel mri;optimisation biomedical mri image reconstruction medical image processing;tomography;biomedical mri;bioluminescence tomography	Most bioimaging modalities rely on indirect measurements of the quantity under investigation. The image is obtained as the result of an optimization problem involving a physical model of the measurement system. Due to the ill-posedness of the above problem, the impact of the noise on the reconstructed images must be controlled. The recent emphasis in biomedical image reconstruction is on regularization schemes that favor sparse solutions, which renders the optimization problem nonsmooth. In this work, we show how step-size adaptation can be used to speed up the most recent multi-step algorithms (e.g. FISTA) employed in sparse image recovery. We present experiments in MRI and Fluorescence Molecular Tomography with specifically tailored step-adaptation strategies. Our results demonstrate the possibility of an order-of-magnitude speed enhancement over state-of-the-art algorithms.	algorithm;electron tomography;experiment;iterative reconstruction;mathematical optimization;optimization problem;rendering (computer graphics);sparse matrix;speedup;system of measurement;well-posed problem	Matthieu Guerquin-Kern;Jean-Charles Baritaux;Michael Unser	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947669	iterative reconstruction;computer vision;mathematical optimization;convergence;sensitivity;inverse problem;magnetic resonance imaging	Vision	51.47272482847019	-79.73241791856306	72681
6e80e7daac20b308a54eefe57187e2eb205302e2	evolutionary algorithm based automated medical image fusion technique: comparative study with fuzzy fusion approach	image multiresolution decomposition automated medical image fusion technique fuzzy fusion approach multi modal medical images wavelet transform genetic based evolutionary algorithm;evolutionary computation;computed tomography;image resolution;genetic algorithm image fusion multiresolution analysis wavelet transform;image fusion;biomedical imaging;data mining;genetics;wavelet transforms;wavelet transform;medical image;medical image processing;evolutionary computation biomedical imaging image fusion data mining image resolution genetics robustness wavelet transforms image analysis mutual information;comparative study;clustering algorithms;mutual information;genetic algorithm;genetic algorithms;evolutionary algorithm;wavelet transforms evolutionary computation fuzzy systems genetic algorithms image fusion image resolution medical image processing;multiresolution analysis;similarity measure;fuzzy systems	Medical image fusion has been used to derive the useful information from multi modal medical images. The proposed methodology introduces evolutionary approaches for robust and automatic extraction of information from different modality images. This evolutionary fusion strategy implements multiresolution decomposition of the input images using wavelet transform. It is because, the analysis of input images at multiple resolutions able to extracts more fine details and improves the quality of the composite fused image. The proposed approach is also independent of any manual marking or knowledge of fiducial points and starts the fusion procedure automatically. The performance of the genetic based evolutionary algorithm is compared with fuzzy based fusion technique using mutual information as the similarity measuring metric. Experimental results show that genetic searching based fusion technique improves the quality of the fused images significantly over the fuzzy approaches.	cluster analysis;evolutionary algorithm;fiducial marker;fuzzy clustering;image fusion;item unique identification;iteration;mathematical optimization;modal logic;modality (human–computer interaction);multiresolution analysis;mutual information;performance;randomized algorithm;search algorithm;selection algorithm;software release life cycle;swarm;wavelet transform	Mahua Bhattacharya;Arpita Das	2009	2009 World Congress on Nature & Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2009.5393715	computer vision;genetic algorithm;computer science;artificial intelligence;machine learning;evolutionary algorithm;pattern recognition;evolutionary computation;wavelet transform	Robotics	41.928567431663566	-73.11947586588873	72705
205c8fc733ac4b4ce07d3bb8f3c6929b86da289c	tracking kidney tumor dimensional measurements via image morphing	image morphing;manuals;kidney tumor dimensional measurement tracking;computed tomography;cancer;edge detection;image matching;tumours;tumors feature extraction manuals pixel computed tomography cancer image edge detection;serial medical images;tumor measurement kidney tumor image morphing;nearest neighbor algorithm;target images;medical image;image edge detection;tumours cancer edge detection feature extraction image matching image morphing kidney medical image processing patient monitoring;feature extraction;medical image processing;renal cancer;pixel;tumor measurement;kidney tumor;tumors;clinical oncology;approximate nearest neighbor;image matching kidney tumor dimensional measurement tracking image morphing renal cancer monitoring clinical oncology manual tumor measurements serial medical images feature extraction edge detection target images nearest neighbor algorithm;patient monitoring;manual tumor measurements;kidney;renal cancer monitoring	Monitoring the evolution of renal cancer is essential in clinical oncology trials. However, manual tumor measurements are subjective and inconsistent. We have developed and implemented an efficient image-morphing-based method to track dimensional measurements in serial medical images. Our method extracts feature blocks centered at edges detected in source and target images, and then applies a fast, approximate nearest-neighbor algorithm to match these features. This matching gives rise to a global feature flow, and this feature flow can be used to track dimensional measurements.	approximation algorithm;k-nearest neighbors algorithm;morphing	Nathaniel Strawn;Jianhua Yao	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5652976	computer vision;edge detection;feature extraction;computer science;machine learning;pattern recognition;remote patient monitoring;computed tomography;k-nearest neighbors algorithm;pixel;cancer	Robotics	39.96688553432356	-76.91864210835219	72755
ad8276bcdb6c424f04674df7adfe402173b5dfbf	algebraic reconstruction for magnetic resonance imaging under b/sub 0/ inhomogeneity	nuclear magnetic resonance imaging;evaluation performance;medical imagery;image reconstruction magnetic resonance imaging equations nonlinear distortion inverse problems encoding magnetic fields resonant frequency trajectory robustness;magnetic field;performance evaluation;objet test;evaluacion prestacion;singular value decomposition;biomedical nmr;hombre;medical diagnostic imaging algebraic reconstruction magnetic resonance imaging b sub 0 inhomogeneity spatial localization fourier encoding magnetic field gradient linear correspondence dimension of interest resonance frequency spatial location larmor equation linear mapping inhomogeneous field linear fredholm equation field mapping k space trajectory ill posed inverse problem;humans image processing computer assisted magnetic resonance imaging phantoms imaging;indexing terms;conjugate gradient method;problema inverso;singular value decomposition image reconstruction medical image processing biomedical nmr inverse problems;methode algebrique;equation fredholm;magnetic resonance image;imageria rmn;reconstruction image;ecuacion fredholm;inverse problem;reconstruccion imagen;heterogeneidad;resonant frequency;image reconstruction;medical image processing;frequence spatiale;algebraic method;image sequence;human;tecnica;imagerie medicale;fast imaging;imagerie rmn;imageneria medical;metodo algebraico;spatial locality;frecuencia espacial;probleme inverse;objeto prueba;test object;technique;spatial frequency;heterogeneity;heterogeneite;inverse problems;homme;fredholm equation	In magnetic resonance imaging, spatial localization is usually achieved using Fourier encoding which is realized by applying a magnetic field gradient along the dimension of interest to create a linear correspondence between the resonance frequency and spatial location following the Larmor equation. In the presence of B/sub 0/ inhomogeneities along this dimension, the linear mapping does not hold and spatial distortions arise in the acquired images. In this paper, the problem of image reconstruction under an inhomogeneous field is formulated as an inverse problem of a linear Fredholm equation of the first kind. The operators in these problems are estimated using field mapping and the k-space trajectory of the imaging sequence. Since such inverse problems are known to be ill-posed in general, robust solvers, singular value decomposition and conjugate gradient method, are employed to obtain corrected images that are optimal in the Frobenius norm sense. Based on this formulation, the choice of the imaging sequence for well-conditioned matrix operators is discussed, and it is shown that nonlinear k-space trajectories provide better results. The reconstruction technique is applied to sequences where the distortion is more severe along one of the image dimensions and the two-dimensional reconstruction problem becomes equivalent to a set of independent one-dimensional problems. Experimental results demonstrate the performance and stability of the algebraic reconstruction methods.	condition number;conjugate gradient method;dimensions;distortion;immunostimulating conjugate (antigen);iterative reconstruction;linear algebra;magnetic resonance imaging;matrix multiplication;nonlinear system;reconstruction conjecture;singular value decomposition;well-posed problem	Yasser M. Kadah;Xiaoping Hu	1998	IEEE Transactions on Medical Imaging	10.1109/42.712126	mathematical optimization;radiology;inverse problem;magnetic resonance imaging;calculus;mathematics;geometry	Vision	52.80378431111029	-79.10159275662056	73121
072c9bb328d8144493c37d570522e8c2890a6aed	a new algorithm based on fuzzy gibbs random fields for image segmentation	image segmentation;unsupervised segmentation;potts model;random field	In this paper a new unsupervised segmentation algorithm based on Fuzzy Gibbs Random Field (FGRF) is proposed. This algorithm, named as FGS, can deal with fuzziness and randomness simultaneously. A Classical Gibbs Random Field (CGRF) servers as bridge between prior FGRF and original image. The FGRF is equivalent to CGRF when no fuzziness is considered; therefore, the FGRF is obviously a generalization of the CGRF. The prior FGRF is described in the Potts model, whose parameter is estimated by the maximum pesudolikelihood (MPL) method. The segmentation results are obtained by fuzzifying the image, updating the membership of FGRF based on maximum a posteriori (MAP) criteria, and defuzzifying the image according to maximum membership principle (MMP). Specially, this algorithm can filter the noise effectively. The experiments show that this algorithm is obviously better than CGRF_ based methods and conventional FCM methods as well.	algorithm;experiment;fuzzy cognitive map;image segmentation;markov random field;mathematics-mechanization platform;multiphoton lithography;potts model;principle of maximum entropy;randomness;whole genome sequencing	Gang Yan;Wufan Chen	2004		10.1007/978-3-540-28626-4_20	mathematical optimization;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	50.24590229061786	-69.34443763416743	73564
6b94456dd204af08ac0c91ac81a46689951e1bf6	robust segmentation of tubular structures in 3-d medical images by parametric object detection and tracking	sample size;randomized hough transform;image segmentation;spinal cord;expectation dependent weighting robust segmentation tubular structures 3d medical images parametric object detection tracking coarse segmentation minimal user interaction randomized bough transform generalized cylinders discrete kalman filter feature adaptive selection;kalman filters;kalman filter;kalman filters image segmentation medical image processing object detection hough transforms;robustness image segmentation biomedical imaging object detection deformable models shape anatomical structure spinal cord computed tomography magnetic resonance imaging;indexing terms;three dimensional;medical image;medical image processing;robust method;hough transforms;user interaction;deformable model;object detection	We present a novel approach to the coarse segmentation of tubular structures in three-dimensional (3-D) image data. Our algorithm, which requires only few initial values and minimal user interaction, can be used to initialize complex deformable models and is based on an extension of the randomized hough transform (RHT), a robust method for low-dimensional parametric object detection. Tubular structures are modeled as generalized cylinders. By means of a discrete Kalman filter, they are tracked through 3-D space. Our extensions to the RHT are a feature adaptive selection of the sample size, expectation-dependent weighting of the input data, and a novel 3-D parameterization for straight elliptical cylinders. Experimental results obtained for 3-D synthetic as well as for 3-D medical images demonstrate the robustness of our approach w.r.t. image noise. We present the successful segmentation of tubular anatomical structures such as the aortic arc and the spinal cord.		Thorsten Behrens;Karl Rohr;H. Siegfried Stiehl	2003	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/TSMCB.2003.814305	kalman filter;computer vision;computer science;machine learning;pattern recognition;mathematics	Vision	44.791785206509005	-76.66556070245417	73730
a23791db14998744278d5441e883ac3e204d96b7	skull segmentation from mr scans using a higher-order shape model based on convolutional restricted boltzmann machines		convolutional restricted Boltzmann machines DTU Orbit (05/12/2018)  Skull segmentation from MR scans using a higher-order shape model based on convolutional restricted Boltzmann machines Transcranial brain stimulation (TBS) techniques such as transcranial magnetic stimulation (TMS), transcranial direct current stimulation (tDCS) and others have seen a strong increase as tools in therapy and research within the last 20 years. In order to precisely target the stimulation, it is important to accurately model the individual head anatomy of a subject. Of particular importance is accurate reconstruction of the skull, as it has the strongest impact on the current pathways due to its low conductivity. Thus providing automated tools, which can reliably reconstruct the anatomy of the human head from magnetic resonance (MR) scans would be highly valuable for the application of transcranial stimulation methods. These head models can also be used to inform source localization methods such as EEG and MEG.Automated segmentation of the skull from MR images is, however, challenging as the skull emits very little signal in MR. In order to avoid topological defects, such as holes in the segmentations, a strong model of the skull shape is needed. In this paper we propose a new shape model for skull segmentation based on the so-called convolutional restricted Boltzmann machines (cRBMs). Compared to traditionally used lower-order shape models, such as pair-wise Markov random fields (MRFs), the cRBMs model local shapes in larger spatial neighborhoods while still allowing for efficient inference. We compare the skull segmentation accuracy of our approach to two previously published methods and show significant improvement.		Oula Puonti;Koenraad Van Leemput;Jesper D. Nielsen;Christian Bauer;Hartwig R. Siebner;Kristoffer Hougaard Madsen;Axel Thielscher	2018		10.1117/12.2293073	human head;computer vision;transcranial magnetic stimulation;artificial intelligence;transcranial direct-current stimulation;boltzmann machine;electroencephalography;stimulation;physics;skull;segmentation	Vision	42.52387930285963	-79.10847320914192	73789
c7b226460fdb11567f05b673b5c035ca155db82b	segmentation of dental radiographs using a swarm intelligence approach	dentistry;smart pixels;knowledge based reasoning;swarm intelligence;digital image processing;image segmentation;dental radiograph digital image processing image segmentation swarm intelligence cellular automata sensor function dynamic flow algorithm;biological system modeling;sensor function;dynamic flow algorithm;knowledge based reasoning swarm intelligence image segmentation cellular automata;radiography;region segmentation;dentistry radiography particle swarm optimization image segmentation intelligent sensors smart pixels digital images biological system modeling intelligent agent biosensors;medical image processing;particle swarm optimization;dental radiograph;medical image processing artificial intelligence cellular automata dentistry diagnostic radiography image segmentation;intelligent agent;artificial intelligence;advance directive;cellular automata;digital images;intelligent sensors;diagnostic radiography;biosensors;knowledge base	One of the most complex tasks in digital image processing is image segmentation. This paper proposes a novel image segmentation algorithm that uses a biologically inspired technique based on swarm intelligence and a cellular automata model. The proposed swarm intelligence-based algorithm operates on the image pixel data and a region/neighborhood map to form a context in which they can merge. The swarm intelligent algorithm also tries to find similar pixels using a sensor function, which is then utilized by swarm agents to determine the next appreciate pixel in the region/segment area. In addition, the paper introduces a cellular automata-based dynamic flow algorithm to guide swarm agents to choose the best possible advancing direction to avoid traffic jam and inconsistency. The suggested image segmentation strategy is tested on a set of dental radiographs	algorithm;automata theory;cellular automaton;digital image processing;image segmentation;jam;knowledge base;maximum flow problem;multi-agent system;pixel;problem solving;radiography;real-time clock;swarm intelligence	Fazel Keshtkar;Wail Gueaieb	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277656	computer vision;knowledge base;multi-swarm optimization;radiography;swarm intelligence;computer science;artificial intelligence;machine learning;segmentation-based object categorization;digital image processing;image segmentation;scale-space segmentation;particle swarm optimization;intelligent agent;digital image;biosensor;intelligent sensor	Robotics	42.88281589699266	-71.3739037009357	73794
8ecc9b2d2b19e7508dd8d42a938eff2a76c92ac7	a new algorithm to threshold the courtesy amount of brazilian bank checks	histograms;image recognition;image preprocessing;image segmentation;image processing;entropy text recognition histograms optical character recognition software character recognition image recognition image analysis algorithm design and analysis feature extraction focusing;image segmentation cheque processing entropy image recognition;threshold;courtesy amount image processing threshold;cheque processing;thresholding algorithms brazilian bank checks automatic recognition system tsallis entropy histogram specification image preprocessing bi level images;distance measurement;bi level images;image color analysis;pixel;histogram specification;courtesy amount;entropy;signal processing algorithms;proposals;thresholding algorithms;tsallis entropy;automatic recognition system;brazilian bank checks	This paper describes a new algorithm for thresholding the courtesy amount of Brazilian bank checks. These images have complex backgrounds which is a problem for an automatic recognition system. The new approach is based on Tsallis entropy to find the best threshold value. Histogram specification is also used for preprocessing some images. The bi-level images are analyzed through several quantitative measures and it achieved the best results when compared with the images produced by other classical thresholding algorithms.	algorithm;black and burst;preprocessor;thresholding (image processing);tsallis entropy	Renata F. P. Neves;Carlos A. B. Mello;Mara S. Silva;Byron L. D. Bezerra	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811450	computer vision;entropy;speech recognition;image processing;computer science;pattern recognition;balanced histogram thresholding;histogram;image segmentation;pixel;tsallis entropy;statistics	Robotics	43.33395659968888	-68.13765138195684	73925
f490a506fa6d3512c087cd6dc22f0297c0e04884	human part segmentation in depth images with annotated part positions	grid graph;human parts;interactive image segmentation;occlusion	We present a method of segmenting human parts in depth images, when provided the image positions of the body parts. The goal is to facilitate per-pixel labelling of large datasets of human images, which are used for training and testing algorithms for pose estimation and automatic segmentation. A common technique in image segmentation is to represent an image as a two-dimensional grid graph, with one node for each pixel and edges between neighbouring pixels. We introduce a graph with distinct layers of nodes to model occlusion of the body by the arms. Once the graph is constructed, the annotated part positions are used as seeds for a standard interactive segmentation algorithm. Our method is evaluated on two public datasets containing depth images of humans from a frontal view. It produces a mean per-class accuracy of 93.55% on the first dataset, compared to 87.91% (random forest and graph cuts) and 90.31% (random forest and Markov random field). It also achieves a per-class accuracy of 90.60% on the second dataset. Future work can experiment with various methods for creating the graph layers to accurately model occlusion.	algorithm;alveolar rhabdomyosarcoma;body part;coat of arms;cut (graph theory);default;digital image;graph (abstract data type);graph - visual representation;hidden surface determination;image segmentation;incised wound;lattice graph;markov chain;markov random field;node - plant part;part dosing unit;pixel;plant seeds;random forest;revision procedure;seeds (cellular automaton);silo (dataset);anatomical layer;biologic segmentation	Andrew Hynes;Stephen Czarnuch	2018		10.3390/s18061900	markov random field;lattice graph;engineering;pixel;image segmentation;electronic engineering;computer vision;cut;random forest;segmentation;pose;artificial intelligence	Vision	40.21148080086057	-77.93044142502617	74522
42969fd8afbbaa18556fecf45181c8f7a0b37d72	a compact linear programming relaxation for binary sub-modular mrf		We propose a novel compact linear programming (LP) relaxation for binary sub-modular MRF in the context of object segmentation. Our model is obtained by linearizing an l 1 -norm derived from the quadratic programming (QP) form of the MRF energy. The resultant LP model contains significantly fewer variables and constraints compared to the conventional LP relaxation of the MRF energy. In addition, unlike QP which can produce ambiguous labels, our model can be viewed as a quasi-total-variation minimization problem, and it can therefore preserve the discontinuities in the labels. We further establish a relaxation bound between our LP model and the conventional LP model. In the experiments, we demonstrate our method for the task of interactive object segmentation. Our LP model outperforms QP when converting the continuous labels to binary labels using different threshold values on the entire Oxford interactive segmentation dataset. The computational complexity of our LP is of the same order as that of the QP, and it is significantly lower than the conventional LP relaxation.	computational complexity theory;experiment;linear programming relaxation;markov random field;quadratic programming;resultant	Junyan Wang;Sai-Kit Yeung	2014		10.1007/978-3-319-14612-6_3	mathematical optimization;mathematics;algorithm	ML	53.43671907432509	-71.54432036421206	74604
d87d9d81b97fb9102a0a7b1d4c04c58e3436fa13	maximum likelihood, least squares, and penalized least squares for pet	metodo cuadrado menor;simulation ordinateur;radioisotope scanning and imaging;radioisotope scanning and imaging computerised tomography;detectors;methode moindre carre;reliability;convergence;emission computed tomography;nonnegativity constraints;image processing;computed tomography;numerical solution;least squares method;radiology and nuclear medicine;maximum likelihood;genie biomedical;convolution;objet test;merit function;pet;maximum vraisemblance;procesamiento imagen;tomocentelleografia;analisis matematico;mathematical logic;penalized least square;mathematical analysis;least squares merit function;traitement image;positron emission tomography;acceleration;test objet;algorithme;objective function;algorithm;least square fit;reconstruction image;accuracy;conjugate gradient;positron computed tomography;reconstruction problem;biomedical engineering;diagnostic techniques;reconstruccion imagen;exploration radioisotopique;penalized least squares;maximum likelihood fit;image reconstruction;least square;smoother image;computerized tomography;positron;expectation maximization algorithm;maximum likelihood detection;computerised tomography;radionuclide study;algorithms;least squares methods positron emission tomography detectors blood flow convolution convergence maximum likelihood detection acceleration biochemistry sugar;ingenieria biomedica;nuclear medicine;blood flow;simulacion computadora;conjugate gradient approach;sugar;positon;log likelihood objective function;exploracion radioisotopica;em algorithm;analyse mathematique;scaled steepest descent algorithm;computer simulation;objeto prueba;scaled steepest ascent algorithm;least squares methods;biochemistry;maxima verosimilitud;tomoscintigraphie	The EM algorithm is the basic approach used to maximize the log likelihood objective function for the reconstruction problem in positron emission tomography (PET). The EM algorithm is a scaled steepest ascent algorithm that elegantly handles the nonnegativity constraints of the problem. It is shown that the same scaled steepest descent algorithm can be applied to the least squares merit function, and that it can be accelerated using the conjugate gradient approach. The experiments suggest that one can cut the computation by about a factor of 3 by using this technique. The results are applied to various penalized least squares functions which might be used to produce a smoother image.	ct scan;computation;conjugate gradient method;expectation–maximization algorithm;experiment;gradient descent;immunostimulating conjugate (antigen);least squares;loss function;optimization problem;polyethylene terephthalate;positron-emission tomography;positrons;reconstruction conjecture;smoothing spline;times ascent;tomography, emission-computed	Linda Kaufman	1993	IEEE transactions on medical imaging	10.1109/42.232249	computer simulation;generalized least squares;total least squares;iteratively reweighted least squares;econometrics;mathematical optimization;non-linear iterative partial least squares;least mean squares filter;expectation–maximization algorithm;image processing;computer science;mathematics;non-linear least squares;least squares;statistics;recursive least squares filter	Vision	52.99533096780552	-78.63021164351967	74706
84deb299c67038312504fd3dfc9c6b5c34bdc263	accuracy improvement and objective evaluation of annotation extraction from printed documents	printing;document image degradation;degradation;image processing;image degradations;printed documents;local displacement objective evaluation printed documents document image annotation extraction method image degradations;objective evaluation;image color analysis;image colour analysis;feature extraction;degradation printing image processing shape printers text analysis image analysis color pixel;local displacement;pixel;image colour analysis document image processing feature extraction;document image processing;document image;annotation extraction;document image degradation annotation extraction objective evaluation;noise;annotation extraction method	There is an approach of annotation extraction from printed documents in which annotations are extracted by comparing the image of an annotated document and its original document image. In one of the previous methods, the image of an original document is actually printed and scanned in order to reproduce image degradations of the image of the annotated document. However such a method lacks convenience since users have to use the same printer and scanner to obtain images of an annotated document and its original document. In this paper, we propose an improved annotation extraction method in which the image degradations are compensated by image processing. In the proposed method, the difference between original and annotated document images due to image degradations is reduced by not only removal of the degradations in the annotated document images but also reproduction of the degradations in the original document images. The proposed method consists of three steps of processing which are for dithering, for color change, and for local displacement. We also propose an objective evaluation of extracted annotations to compare the experimental results accurately. Experimental results of the proposed method have shown that the recall of extracted annotations was 80.94% and the precision was 85.59%.	circuit restoration;displacement mapping;dither;ground truth;image processing;pixel;printer (computing);printing	Tomohiro Nakai;Kazumasa Iwata;Koichi Kise	2008	2008 The Eighth IAPR International Workshop on Document Analysis Systems	10.1109/DAS.2008.80	computer vision;degradation;image processing;feature extraction;computer science;noise;multimedia;information retrieval;pixel	Vision	39.68884135511517	-68.66163995419063	74819
9dcf0906cdfed4d8e302ad3496f9f06a5125a392	a novel region-based level set method initialized with mean shift clustering for automated medical image segmentation	mean shift clustering;global region information;level set methods;local region information;medical image segmentation	Appropriate initialization and stable evolution are desirable criteria to satisfy in level set methods. In this study, a novel region-based level set method utilizing both global and local image information complementarily is proposed. The global image information is extracted from mean shift clustering without any prior knowledge. Appropriate initial contours are obtained by regulating the clustering results. The local image information, as extracted by a data fitting energy, is employed to maintain a stable evolution of the zero level set curves. The advantages of the proposed method are as follows. First, the controlling parameters of the evolution can be easily estimated by the clustering results. Second, the automaticity of the model increases because of a reduction in computational cost and manual intervention. Experimental results confirm the efficiency and accuracy of the proposed method for medical image segmentation.	algorithmic efficiency;approximation algorithm;apricot kernel oil;arabic numeral 0;cluster analysis;computation;curve fitting;extraction;futures studies;image segmentation;kernel (operating system);kernel bandwidth;ms-dos editor;mean shift;medical image;rsf1 wt allele;reasonable server faces;biologic segmentation;statistical cluster	Peirui Bai;Qingyi Liu;Lei Li;Shenghua Teng;Jing Li;Maoyong Cao	2013	Computers in biology and medicine	10.1016/j.compbiomed.2013.08.024	correlation clustering;computer vision;mean-shift;segmentation-based object categorization;pattern recognition;data mining;region growing;cluster analysis	Vision	45.17011173971831	-73.70403388566226	74857
6b1a0f5b4c02eadf9aa98f4308a737e27eb10a2e	variation-based approach to image segmentation	image segmentation;image segmentation variation optimization relaxation algorithm global convergence;non convex optimization;global convergence;convex optimization;relaxation algorithm;energy function;optimization problem;optimization;variation;variation optimization relaxation algorithm global convergence image segmentation	A new approach to image segmentation is presented using a variation framework. Regarding the edge points as interpolating points and minimizing an energy functional to interpolate a smooth threshold surface it carries out the image segmentation. In order to preserve the edge information of the original image in the threshold surface, without unduly sharping the edge of the image, a non-convex energy functional is adopted. A relaxation algorithm with the property of global convergence, for solving the optimization problem, is proposed by introducing a binary energy. As a result the non-convex optimization problem is transformed into a series of convex optimization problems, and the problem of slow convergence or nonconvergence is solved. The presented method is also tested experimentally. Finally the method of determining the parameters in optimizing is also explored.	algorithm;convex optimization;experiment;image segmentation;interpolation;linear programming relaxation;local convergence;mathematical optimization;optimization problem;relaxation (iterative method)	Yongping Zhang;Nanning Zheng;Rongchun Zhao	2001	Science in China Series : Information Sciences	10.1007/BF02714714	optimization problem;mathematical optimization;multi-swarm optimization;combinatorics;discrete mathematics;convex optimization;lagrangian relaxation;computer science;segmentation-based object categorization;mathematics;image segmentation;scale-space segmentation;theme and variations;global optimization	Vision	51.52119564579231	-71.31261831535731	74893
2bc1f1fad4af7315c4e9c7e3d8f846175e48783c	an improved manifold ranking based method for saliency detection		This paper present an improved Manifold ranking based saliency detection method. Our method constructs a similarity matrix to represent the connection between each superpixel of image. To achieve the optimization, we select some foreground regions as the foreground labels, by using the objective likelihood map technique, and a part of boundary regions as background labels, by using color distinction measure. Based on these prior information, we generate the rough results by manifold ranking algorithm, and merge the results, obtaining our final saliency map. To verify the robustness of our proposed algorithm, we conduct extensive experiments.	algorithm;battle isle series;database;experiment;mathematical optimization;similarity measure	Xiabao Wu;Xiao Lin;Linhua Jiang;Dongfang Zhao	2017	2017 4th International Conference on Systems and Informatics (ICSAI)	10.1109/ICSAI.2017.8248282	robustness (computer science);merge (version control);computer science;control theory;manifold;visualization;salience (neuroscience);ranking;saliency map;pattern recognition;similarity matrix;artificial intelligence	Robotics	46.38326572680633	-68.52068979612169	74978
af63a1f125bfc3288a090cbc2139af2eb5ad012b	a new approach of arc skeletonization for tree-like objects using minimum cost path	biological patents;biomedical journals;computed tomography;arc skeletonization;phantoms;text mining;human intrathoracic airway ct imaging arc skeletonization algorithms tree like objects blum transform arc skeleton extraction 3d elongated fuzzy objects minimum cost geodesic path path cost function fuzzy distance transform computer generated blurred phantoms computer generated noisy phantoms;europe pubmed central;citation search;citation networks;noise measurement;skeleton;geodesic distance;trees mathematics computational geometry computerised tomography fuzzy set theory image thinning medical image processing transforms;research articles;abstracts;open access;minimum cost path;life sciences;clinical guidelines;full text;distance transform;signal to noise ratio;airway tree;algorithm design and analysis;rest apis;orcids;europe pmc;biomedical research;skeleton noise measurement signal to noise ratio phantoms computed tomography algorithm design and analysis;airway tree arc skeletonization distance transform geodesic distance minimum cost path;bioinformatics;literature search	Traditional arc skeletonization algorithms using the principle of Blum's transform, often, produce unwanted spurious branches due to boundary irregularities and digital effects on objects and other artifacts. This paper presents a new robust approach of extracting arc skeletons for three-dimensional (3-D) elongated fuzzy objects, which avoids spurious branches without requiring post-pruning. Starting from a root voxel, the method iteratively expands the skeleton by adding a new branch in each iteration that connects the farthest voxel to the current skeleton using a minimum-cost geodesic path. The path-cost function is formulated using a novel measure of local significance factor defined by fuzzy distance transform field, which forces the path to stick to the centerline of the object. The algorithm terminates when dilated skeletal branches fill the entire object volume or the current farthest voxel fails to generate a meaningful branch. Accuracy of the algorithm has been evaluated using computer-generated blurred and noisy phantoms with known skeletons. Performance of the method in terms of false and missing skeletal branches, as defined by human expert, has been examined using in vivo CT imaging of human intrathoracic airways. Experimental results from both experiments have established the superiority of the new method as compared to a widely used conventional method in terms of accuracy of medialness as well as robustness of true and false skeletal branches.	algorithm;blum axioms;clcn5 gene;ct scan;computation (action);computational complexity theory;computer-generated holography;digital effects (studio);distance (graph theory);distance transform;experiment;fill;iteration;loss function;morphologic artifacts;neural coding;phantoms, imaging;physical object;powerflasher fdt;s transform;skeleton;video-in video-out;voxel	Dakai Jin;Krishna S. Iyer;Eric A. Hoffman;Punam K. Saha	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.172	algorithm design;computer vision;text mining;geodesic;computer science;noise measurement;artificial intelligence;machine learning;mathematics;geometry;distance transform;signal-to-noise ratio;skeleton;statistics	Vision	45.894012701365654	-72.0103841231779	75184
ee2b6f3f6526303c113b2f506bf6ae90a4f8454b	threshold selection based on interval-valued fuzzy sets		We propose a thresholding method based on interval-valued fuzzy sets which are used to define the grade of a gray level belonging to one of the two classes, an object and the background of an image. The effectiveness of the proposed method is demonstrated by comparing our classification results on eight test images to results from the conventional methods.	fuzzy set	Chang Sik Son;Suk Tae Seo;In Keun Lee;Hye Cheun Jeong;Soon Hak Kwon	2009	IEICE Transactions		computer vision;image analysis;defuzzification;fuzzy classification;computer science;artificial intelligence;mathematics;fuzzy set;algorithm	Visualization	41.5468332797123	-67.4475513225027	75220
4b5393b98c92a3f4285be0ea1662e847642f106d	segmentation of focal brain lesions	cerebral infarction;head trauma;automatic segmentation;intracerebral hemorrhage;mirror symmetry;magnetic resonance image;first order;clinical practice	Focal brain lesions are a consequence of head trauma, cerebral infarcts or intracerebral hemorrhages. In clinical practice, magnetic resonance imaging (MRI) is commonly used to reveal them. The segmentation task consists of finding the lesion borders. This problem is non-trivial because the lesion may be connected to other intracranial compartments with similar intensities. A new method for the automatic segmentation of unilateral lesions is proposed here. The signal statistics of multichannel MR are examined w.r.t. the first-order mirror symmetry of the brain. The algorithm is discussed in detail, and its properties are evaluated on synthetic and real MRI data.	algorithm;claire;contrast ratio;dhrystone;focal (programming language);first-order predicate;message passing interface;resonance;sensor;voxel	Frithjof Kruggel	2004		10.1007/978-3-540-28626-4_2	radiology;medicine;pathology;surgery	Vision	40.38951230992992	-79.67008158705977	75558
0b5592a197693dc47fff02402d48fc88b6326bed	seamline determination based on pkgc segmentation for remote sensing image mosaicking	graph cuts;image mosaicking;image segmentation;multi-scale morphological gradient (msmg);remote sensing;seamline detection	This paper presents a novel method of seamline determination for remote sensing image mosaicking. A two-level optimization strategy is applied to determine the seamline. Object-level optimization is executed firstly. Background regions (BRs) and obvious regions (ORs) are extracted based on the results of parametric kernel graph cuts (PKGC) segmentation. The global cost map which consists of color difference, a multi-scale morphological gradient (MSMG) constraint, and texture difference is weighted by BRs. Finally, the seamline is determined in the weighted cost from the start point to the end point. Dijkstra's shortest path algorithm is adopted for pixel-level optimization to determine the positions of seamline. Meanwhile, a new seamline optimization strategy is proposed for image mosaicking with multi-image overlapping regions. The experimental results show the better performance than the conventional method based on mean-shift segmentation. Seamlines based on the proposed method bypass the obvious objects and take less time in execution. This new method is efficient and superior for seamline determination in remote sensing image mosaicking.	apricot kernel oil;comment (computer programming);complementarity determining regions;conflict (psychology);cut (graph theory);dijkstra's algorithm;entity name part qualifier - adopted;execution;experiment;extraction;image stitching;incised wound;large;lithium;manuscripts;mathematical optimization;mean shift;morphological gradient;numerous;physical object;pixel;real-time transcription;requirement;revision procedure;segmentation action;semidalis bicornis;short;shortest path problem;source code;biologic segmentation;bypass;endopeptidase clp;remote sensing;yang wei kang liu	Qiang Dong;Jinghong Liu	2017		10.3390/s17081721	kernel (linear algebra);engineering;color difference;remote sensing;cut;parametric statistics;image segmentation;computer vision;morphological gradient;segmentation;artificial intelligence;dijkstra's algorithm	Vision	43.94723625398864	-70.02789434554363	75614
3d899fda3e3ab7bca7d3acc4c0d69f9ddb049641	adaptive image restoration using a proximity measure to boundary	optimal solution;4230;restauration image;image processing;modelo markov;distribution intensite;0130c;bayes methods;methode bayes;standard deviation;texture image;prior information;imagerie;analisis objetivos;spatial interaction;contextual information;estimation a posteriori;image restoration;intensity distribution;markov random field;a posteriori estimation;image texture;algorithme;iterative methods;chemical compounds;imagery;markov model;campo aleatorio;stochastic processes;estimacion a posteriori;methode iterative;pixel;processus stochastique;map estimation;algorithms;imagineria;modele markov;analyse objective;radiometric corrections;objective analysis;champ aleatoire;bayesian model;random field	In this study, an iterative maximum a posteriori (MAP) approach using a Bayesian model of Markov random field (MRF) was proposed for image restoration to reduce or remove the noise resulted from imperfect sensing. Image process is assumed to combine the random fields associated with the observed intensity process and the image texture process respectively. The objective measure for determining the optimal restoration of this “double compound stochastic” image process is based on Bayes’ theorem, and the MAP estimation employs the Point-Jacobian iteration to obtain the optimal solution. In the proposed algorithm, MRF is used to quantify the spatial interaction probabilistically, that is, to provide a type of prior information on the image texture and the neighbor window of any size is defined for contextual information on a local region. However, the window of a certain size would result in using wrong information for the estimation from adjacent regions with different characteristics at the pixels close to or on the boundary. To overcome this problem, the new method is designed to use less information from more distant neighbors as the pixel is closer to the boundary. It can reduce the possibility to involve the pixel values of adjacent region with different characteristics. The proximity to the boundary is estimated using a non-uniformity measurement based on edge value, standard deviation, entropy, and the 4th moment of intensity distribution. This study evaluated the new scheme using simulation data, and the experimental results show a considerable improvement in image restoration.	algorithm;bayesian network;circuit complexity;circuit restoration;entropy (information theory);image restoration;image texture;iteration;jacobian matrix and determinant;markov chain;markov random field;pixel;simulation	Sanghoon Lee	2009		10.1117/12.806780	image texture;image restoration;random field;image processing;free boundary condition;pattern recognition;iterative method;markov model;standard deviation;bayesian inference;pixel	Vision	50.57197518369834	-68.70130796826577	75799
b7fac65d71861998707c0dfb1b2bb3410de4f637	efficient feature extraction for fast segmentation of mr brain images	fuzzy c means algorithm;fuzzy c mean;image segmentation;level set;surface reconstruction;magnetic resonance image;mr imaging;noise elimination;feature extraction;magnetic resonance imaging;brain imaging;processing speed	Automated brain MR image segmentation is a challenging problem and received significant attention lately. Various techniques have been proposed, several improvements have been made to the standard fuzzy c-means (FCM) algorithm, in order to reduce its sensitivity to Gaussian, impulse, and intensity non-uniformity noises. In this paper we present a modified FCM algorithm, which aims at accurate segmentation in case of mixed noises, and performs at a high processing speed. As a first step, a scalar feature value is extracted from the neighborhood of each pixel, using a filtering technique that deals with both spatial and gray level distances. These features are clustered afterwards using the histogram-based approach of the enhanced FCM algorithm. The experiments using 2-D synthetic phantoms and real MR images show, that the proposed method provides better results compared to other reported FCM-based techniques. The produced segmentation and fuzzy membership values can serve as excellent support for level set based cortical surface reconstruction techniques.	feature extraction	László Szilágyi;Sándor M. Szilágyi;Zoltán Benyó	2007		10.1007/978-3-540-73040-8_62	computer vision;surface reconstruction;feature extraction;computer science;level set;magnetic resonance imaging;machine learning;pattern recognition;image segmentation;scale-space segmentation	Vision	43.07503494329614	-72.55650790004114	75925
0209594f15d0bcfd918decb0ea145383789c751b	super resolution of 3d mri images using a gaussian scale mixture model constraint	wavelet transforms biomedical mri gaussian processes image denoising image resolution medical image processing;image resolution;gaussian processes;strontium;wavelet transforms;gaussian scale mixture model;medical image processing;magnetic resonance imaging;signal resolution;super resolution;image denoising;signal processing algorithms;single 3d volume 3d mri images gaussian scale mixture model constraint multislice magnetic resonance imaging slice direction resolution acquisition times noise reduction image super resolution algorithm sr algorithm complex wavelet based deblurring approach gaussian scale mixture model sparseness constraint multislice volumes anatomical region low resolution images;image resolution strontium magnetic resonance imaging signal resolution signal processing algorithms wavelet transforms noise;wavelet regularization magnetic resonance imaging super resolution gaussian scale mixture model;wavelet regularization;noise;biomedical mri	In multi-slice magnetic resonance imaging (MRI) the resolution in the slice direction is usually reduced to allow faster acquisition times and to reduce the amount of noise in each 2-D slice. In this paper, a novel image super resolution (SR) algorithm is presented that is used to improve the resolution of the 3D MRI volumes in the slice direction. The proposed SR algorithm uses a complex wavelet-based de-blurring approach with a Gaussian scale mixture model sparseness constraint. The algorithm takes several multi-slice volumes of the same anatomical region captured at different angles and combines these low-resolution images together to form a single 3D volume with much higher resolution in the slice direction. Our results show that the 3D volumes reconstructed using this approach have higher quality than volumes produced by the best previously proposed approaches.	3d computer graphics;algorithm;mixture model;neural coding;resonance;super-resolution imaging;wavelet	Rafiqul Islam;Andrew J. Lambert;Mark R. Pickering	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288017	computer vision;image resolution;strontium;noise;magnetic resonance imaging;pattern recognition;gaussian process;mathematics;statistics;superresolution;wavelet transform	Robotics	50.99306883291966	-76.99959727001632	75940
4299f93090f272a6fdc20070dab086266712220d	a framework for geometric analysis of vascular structures: application to cerebral aneurysms	geometric quantification;aneurisk project;image segmentation;ingegneria industriale e dell informazione;cerebral aneurysm;medical image processing biomedical mri blood vessels computerised tomography diagnostic radiography diseases haemodynamics image representation image segmentation;vmtk;computational hemodynamics;geometric analysis;cerebral aneurysms;haemodynamics;computational geometry;atherosclerosis;aneurysm diseases robustness image analysis computational geometry blood flow atherosclerosis pathology pathogens biomedical imaging;biomedical imaging;three dimensional modeling;vascular diseases;geometric characterization;geometric feature;three dimensional;medical image;image representation;medical image processing;medical imaging;pathogenesis;vascular disease;computerised tomography;diseases;quantitative analysis;vascular geometry;robustness;image analysis;blood flow;blood flow dynamics;mesh generation;aneurysm;pathology;aneurisk project geometric analysis blood flow dynamics vascular diseases atherosclerosis cerebral aneurysms vascular geometry pathogenesis medical imaging 3d morphology vmtk image segmentation geometric characterization mesh generation computational hemodynamics;diagnostic radiography;blood vessels;vascular geometry cerebral aneurysms geometric quantification three dimensional modeling;algorithms brain carotid artery internal cerebral angiography humans image interpretation computer assisted image processing computer assisted intracranial aneurysm middle cerebral artery models cardiovascular;3d morphology;biomedical mri;open source;pathogens	There is well-documented evidence that vascular geometry has a major impact in blood flow dynamics and consequently in the development of vascular diseases, like atherosclerosis and cerebral aneurysmal disease. The study of vascular geometry and the identification of geometric features associated with a specific pathological condition can therefore shed light into the mechanisms involved in the pathogenesis and progression of the disease. Although the development of medical imaging technologies is providing increasing amounts of data on the three-dimensional morphology of the in vivo vasculature, robust and objective tools for quantitative analysis of vascular geometry are still lacking. In this paper, we present a framework for the geometric analysis of vascular structures, in particular for the quantification of the geometric relationships between the elements of a vascular network based on the definition of centerlines. The framework is founded upon solid computational geometry criteria, which confer robustness of the analysis with respect to the high variability of in vivo vascular geometry. The techniques presented are readily available as part of the VMTK, an open source framework for image segmentation, geometric characterization, mesh generation and computational hemodynamics specifically developed for the analysis of vascular structures. As part of the Aneurisk project, we present the application of the present framework to the characterization of the geometric relationships between cerebral aneurysms and their parent vasculature.	arabic numeral 0;atherosclerosis;bifurcation theory;blood supply aspects;bone structure of radius;cerebrovascular disorders;color gradient;computation;computational geometry;echo-planar imaging;extraction;galaxy morphological classification;geometric analysis;heart rate variability;hemodynamics;image segmentation;imaging technology;intracranial aneurysm;isosurface;line level;manuscripts;medical imaging;mesh generation;norm (social);open-source software;phantoms, imaging;preparation;quantitation;quantity;regression testing;regular grid;sampling (signal processing);specimen source codes - tube;torsion (gastropod);vascular diseases;vendor information documentation;video-in video-out;voronoi diagram	Marina Piccinelli;Alessandro Veneziani;David A. Steinman;Andrea Remuzzi;Luca Antiga	2009	IEEE Transactions on Medical Imaging	10.1109/TMI.2009.2021652	medical imaging;three-dimensional space;mesh generation;radiology;medicine;pathology;computer science;quantitative analysis;blood flow;hemodynamics;mathematics;image segmentation;robustness	Visualization	41.16428936100139	-78.63507878561371	75966
c7e2f9f0377990763d6dbca6d9cd11a914e5e9b2	a bayesian multiscale framework for poisson inverse problems	smoothness bayesian multiscale framework poisson inverse problems maximum a posteriori estimation method map estimation method linear inverse problems multiscale prior probability distribution multiscale partition intensity map estimation procedure expectation maximization algorithm em update equations closed form expressions noninformative member maximum likelihood solution;image processing inverse problems bayes methods poisson distribution maximum likelihood estimation iterative methods;bayesian methods inverse problems maximum likelihood estimation partitioning algorithms state estimation algorithm design and analysis probability distribution closed form solution biomedical engineering data engineering;linear inverse problem;image processing;maximum likelihood;bayes methods;maximum likelihood estimation;iterative methods;inverse problem;expectation maximization;probability distribution;map estimation;em algorithm;poisson distribution;inverse problems	This paper describes a maximum a postetioti (MAP) estimation method for linear inverse problems involving Poisson data based on a novel multiscale framework. The framework itself is founded on a carefully designed multiscale prior probability distribution placed on the “splits” in the multiscale partition of the underlying intensity, and it admits a remarkably simple MAP estimation procedure using an expectation-maximization (EM) algorithm. Unlike many other approaches to this problem, the EM update equations for our algorithm have simple, closed-form expressions. Additionally, our class of priors has the interesting feature that the “non-informative” member yields the traditional maximum likelihood solution; other choices are made to reflect prior belief as to the smoothness of the unknown intensity.	expectation–maximization algorithm;information	Robert D. Nowak;Eric D. Kolaczyk	1999		10.1109/ICASSP.1999.756331	econometrics;mathematical optimization;expectation–maximization algorithm;image processing;inverse problem;mathematics;maximum likelihood;statistics	Vision	52.80167573540849	-74.58939696740825	76084
d81cdb1b47873d144d91cb8b95a4507e01451328	complexity-based border detection for textured images	minimisation;image recognition;textured images segmentation bayesian classifier model complexity markov random fields;bayesian classifier;complexity theory;image segmentation;gaussian markov random fields;gaussian processes;mdl decision rule;bayes methods;bayesian complexity minimization;markov random fields;bayesian methods;testing;gaussian markov random field;object detection bayes methods gaussian processes image segmentation image texture markov processes minimisation;minimization methods;maximum likelihood estimation;complexity based border detection;markov random field;model complexity;image texture;bayesian methods image segmentation testing markov random fields minimization methods image edge detection image recognition pixel;markov model;image edge detection;textured images segmentation;pixel;textured images;mathematical model;approximation methods;mdl decision rule complexity based border detection textured images bayesian complexity minimization gaussian markov random fields bayesian information criterion;markov processes;model fitting;bayesian information criterion;decision rule;object detection	In this paper we address the problem of border detection for textured images by exploiting the concept of Bayesian complexity minimization as a generalization of the MDL paradigm in the Bayesian context. We aim at determining if a small window at a certain location corresponds to a single or several texture classes, which here are modeled as Gaussian Markov Random Fields (GMRF), thereby detecting the presence of borders. For doing this, a set of possible border configurations are tested by applying a Bayesian decision rule that includes two terms: a classical likelihood term related to the model fitting error, and a complexity penalizing term for the number and size of the window subdivisions in each configuration. The latter is derived from the Bayesian Information Criterion (BIC) for non-causal Markov models related to the MDL decision rule. Experiments on synthetic and real textured images segmentation support the approach with promising results.	bayesian information criterion;causal filter;curve fitting;edge detection;mdl (programming language);markov chain;markov model;markov random field;minimum description length;programming paradigm;sensor;synthetic intelligence	Tomás Crivelli;Agustin Mailing;Agustin Cernuschi-Frías	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495339	image texture;minimisation;naive bayes classifier;bayesian probability;computer science;machine learning;pattern recognition;mathematical model;decision rule;gaussian process;mathematics;software testing;maximum likelihood;image segmentation;markov process;markov model;bayesian information criterion;pixel;statistics	Vision	49.93806203902773	-69.40962163201723	76187
689db94b1d9bd412351e9dd1521b064e410fa0d2	content based representation of colour image sequences	video signal processing image colour analysis image sequences image representation image motion analysis minimax techniques image segmentation;image motion analysis;image segmentation;video signal processing;k means;minimax techniques;automatic segmentation content based representation colour image sequences spatiotemporal segmentation object tracking connectivity constraint algorithm k means motion information colour distance regularisation parameters min max criterion spatiotemporal regions real colour data synthetic colour data digital video synthetic colour test image gaussian noise;image colour analysis;image representation;image sequence;image sequences image segmentation clustering algorithms standards development informatics telematics algorithm design and analysis video compression video coding mpeg 4 standard;image sequences	In this paper, a procedure is described for the spatiotemporal segmentation and tracking of objects in colour image sequences. For this purpose, we propose the novel procedure of K-Means with connectivity constraint algorithm as a general segmentation algorithm combining several types of information including colour, motion and compactness. A new colour distance is also defined for this algorithm. The regularisation parameters are evaluated automatically using the min-max criterion. In this algorithm, the use of spatiotemporal regions is introduced since a number of frames is analyzed simultaneously and as a result the same region is present in consequent frames. Experimental results on real and synthetic colour data demonstrate the performance of the data.	color image;constraint algorithm;image segmentation;k-means clustering;maxima and minima;synthetic intelligence	Yiannis Kompatsiaris;Michael G. Strintzis	2001		10.1109/ICASSP.2001.941265	image texture;computer vision;feature detection;range segmentation;binary image;image processing;computer science;machine learning;segmentation-based object categorization;digital image processing;pattern recognition;mathematics;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;standard test image;k-means clustering	Vision	44.22226204621443	-67.0957250966084	76251
023a8b7c05f43963ccfe723e9b360492ece5c227	dermoscopic image segmentation by a self-organizing map and fuzzy genetic clustering	tecnologia electronica telecomunicaciones;self organizing maps;medical image processing;genetic algorithms;cluster validity;tecnologias;grupo a;color image segmentation			Harald Galda;Hajime Murao;Hisashi Tamaki;Shinzo Kitamura	2004	IEICE Transactions		image texture;computer vision;genetic algorithm;self-organizing map;computer science;artificial intelligence;machine learning;segmentation-based object categorization;image segmentation;scale-space segmentation;computer graphics (images)	Vision	42.14501049137153	-69.21295758610304	76292
82f53822e1663ab79d7da6417ca8e3d5413a2ed3	image restoration by partial differential equations	snake method;partial differential equation;partial differential equations image denoising image restoration;mathematical analysis image restoration partial differential equation images noises snake method;image restoration;mathematical analysis;image restoration partial differential equations smoothing methods image color analysis mathematical analysis image representation mathematics cameras electrical capacitance tomography pixel;images noises;partial differential equations;image denoising;numerical experiment	In this paper, we propose a technique that has been recently introduced for a research in contours. This has been applied to different physical problems to detect noises in images and signals. For this purpose, our technique is based on the snake's (Serpent) method. Here, we present a mathematical analysis for the method and some numerical experiments	algorithm;circuit restoration;contour line;experiment;geometric modeling;gradient;image restoration;numerical analysis;pixel;smoothing	Maounil Messaoud;Fatma Zohra Nouri	2006	Geometric Modeling and Imaging--New Trends (GMAI'06)	10.1109/GMAI.2006.28	image restoration;computer vision;mathematical optimization;mathematical analysis;scale space;mathematics;partial differential equation	Vision	52.93046087700809	-69.62944838876592	76399
17e1e9764bfc0051d0c98113c34fe07df3ad3bac	an adaptive nonlinear diffusion algorithm for filtering medical images	nonlinear diffusion;image segmentation;image processing;edge detection;single peak histogram adaptive nonlinear diffusion algorithm medical image filtering noise elimination edge accuracy image threshold edge cut off contrast image region intensity distortion contrast variation central limit theorem gaussian distribution rayleigh distribution visual object distributions regression;nonlinear filter;indexing terms;statistical analysis;medical image;central limit theorem;medical image processing;fast algorithm;diffusion process;filtering algorithms adaptive filters biomedical imaging histograms diffusion processes image processing nonlinear distortion gaussian distribution signal to noise ratio anisotropic magnetoresistance;adaptive filter;gaussian distribution;filtering theory;edge detection medical image processing filtering theory gaussian distribution statistical analysis image segmentation;algorithms angiography anisotropy diffusion humans image interpretation computer assisted nonlinear dynamics radiographic image interpretation computer assisted	The nonlinear anisotropic diffusive process has shown the good property of eliminating noise while preserving the accuracy of edges and has been widely used in image processing. However, filtering depends on the threshold of the diffusion process, i.e., the cut-off contrast of edges. The threshold varies from image to image and even from region to region within an image. The problem compounds with intensity distortion and contrast variation. We have developed an adaptive diffusion scheme by applying the central limit theorem to selecting the threshold. Gaussian distribution and Rayleigh distribution are used to estimate the distributions of visual objects in images. Regression under such distributions separates the distribution of the major object from other visual objects in a single-peak histogram. The separation helps to automatically determine the threshold. A fast algorithm is derived for the regression process. The method has been successfully used in filtering various medical images.	algorithm;anisotropic diffusion;distortion;filter (signal processing);histogram;image processing;medical imaging;nonlinear system;physical object;rayleigh–ritz method;visual objects	Jesse S. Jin;Yung Wang;John B. Hiller	2000	IEEE Transactions on Information Technology in Biomedicine	10.1109/4233.897062	normal distribution;adaptive filter;nonlinear filter;computer vision;mathematical optimization;edge detection;index term;image processing;computer science;central limit theorem;theoretical computer science;diffusion process;mathematics;image segmentation;anisotropic diffusion;statistics	Vision	49.897691628193236	-76.06773369756372	76953
6a426730df4347a9c4e5595175b9dd16c681907b	crack detection using enhanced thresholding on uav based collected images		This paper proposes a thresholding approach for crack detection in an unmanned aerial vehicle (UAV) based infrastructure inspection system. The proposed algorithm performs recursively on the intensity histogram of UAV-taken images to exploit their crack-pixels appearing at the low intensity interval. A quantified criterion of interclass contrast is proposed and employed as an object cost and stop condition for the recursive process. Experiments on different datasets show that our algorithm outperforms different segmentation approaches to accurately extract crack features of some commercial buildings.		Qi Zhu;Tung H. Dinh;Van Truong Hoang;Manh Duong Phung;Quang P. Ha	2018	CoRR			Robotics	46.17386043200786	-66.9740004287338	77306
50cd2304b0172e13d140f273aeea69d317cff605	multi-scale feature based land cover change detection in mountainous terrain using multi-temporal and multi-sensor remote sensing images		Land use and land cover (LULC) change is frequent in mountainous terrain of southern China. Although remote sensing technology has become an important tool for gathering and monitoring LULC dynamics, image pairs can occur scale changes, noises, geometrical distortions, and illuminated variations if these are acquired from different types of sensors (e.g., satellites). Meanwhile, how to design an efficient land cover change detection algorithm that ensures a high detection rate remains a critical and challenging step. To address these problems, we propose a robust multi-temporal change detection framework for land cover change in mountainous terrain which contains the following contributions. i) To transform multi-temporal remote sensing image pairs acquired by different type of sensors into the same coordinate system by image registration, a multi-scale feature description is generated using layers formed via a pretrained VGG network. ii) A gradually increasing selection of inliers is defined for improving the robustness of feature points registration, and  ${L_{2}}$ -minimizing estimate ( ${L_{2}E}$ )-based energy optimization is formulated to calculate a reasonable position in a reproducing kernel Hilbert space. iii) Fuzzy C-Means classifier is adopted to generate a similarity matrix between image pair of geometric correction, and a robust and contractive change map is built through feature similarity analysis. Extensive experiments on multi-temporal image pairs taken by different type of satellites (e.g., Chinese GF and Landsat) or small unmanned aerial vehicles are conducted. Experimental results show that our method provides better performances in most cases after comparing with the five state-of-the-art image registration methods and the four state-of-the-art change detection methods.		Fei Song;Zhuoqian Yang;Xueyan Gao;Tingting Dan;Yang Yang;Wanjing Zhao;Rui Yu	2018	IEEE Access	10.1109/ACCESS.2018.2883254	terrain;robustness (computer science);land cover;feature extraction;image sensor;remote sensing;computer science;reproducing kernel hilbert space;change detection;image registration	Vision	46.118022900701675	-66.82143175839134	77418
b92780d8046524fab831551144cf261c983e1360	image contrast enhancement by distances among points in fuzzy hyper-cubes		A new geometrical fuzzy approach for image contrast enhancement is here presented. Synergy among ascending order statistics and entropy evaluations are exploited to get contrast enhancement by evaluation of distances among points inside fuzzy unit hyper-cube. The obtained results can be considered interesting, especially compared with consolidated techniques which encourages further studies in this direction.	cubes	Mario Versaci;Salvatore Calcagno;Francesco Carlo Morabito	2015		10.1007/978-3-319-23117-4_43	computer vision;mathematical optimization;machine learning;mathematics	DB	41.41718282176861	-67.47906919109096	77483
f11f8b712f2977c7b6a9167d99560de5be58fdaf	a framework of perceptual features for the characterisation of 3d textured images		This paper presents a multiresolution system for volumetric texture analysis. The originality of this system partially originates from its use of combinations of perceptual texture features that correspond to adjectives commonly used by humans to describe textures. To approximate these features, we use a combination of different families of texture analysis methods rather than a single texture analysis model. This choice is necessary to obtain a good perceptual feature approximation and allows our system to be robust and generic. Moreover, by using our human-understandable features (HUF), it is convenient for a user to manipulate and select the features that are, according to the user, relevant for a given application. Two experiments are presented: the first experiment demonstrates the strong correspondence between our features and a human’s description of textures, and the second demonstrates the performance of our proposed method. Finally, the proLudovic Paulhac Laboratoire Informatique de l’Université François Rabelais de Tours 64 avenue Jean Portalis 37200 Tours Tel.: +332 47 36 14 14 Fax: +332 47 36 14 22 E-mail: ludovic.paulhac@univ-tours.fr Pascal Makris Laboratoire Informatique de l’Université François Rabelais de Tours E-mail: pascal.makris@univ-tours.fr Jean-Yves Ramel Laboratoire Informatique de l’Université François Rabelais de Tours E-mail: jean-yves.ramel@univ-tours.fr Jean-Marc Gregoire UMR INSERM U930, CNRS ERL 3106, équipe 5, Université François Rabelais de Tours E-mail: jean-marc.gregoire@univ-tours.fr posed HUF are integrated into an interactive segmentation system and are compared to previously proposed descriptors through analysis of several segmentation results of 3D ultrasound images.	3d computer graphics;approximation algorithm;belief propagation;binary image;cluster analysis;computation;connected component (graph theory);discrete wavelet transform;erlang (programming language);experiment;fax;françois lionet;graph (abstract data type);jean;marc (archive);medical ultrasound;multiresolution analysis;region of interest;relevance;software system;succession;texture mapping;time complexity;usability	Ludovic Paulhac;Pascal Makris;Jean-Yves Ramel;Jean-Marc Gregoire	2015	Signal, Image and Video Processing	10.1007/s11760-013-0438-1	computer vision	Vision	44.305313437213364	-73.47275064327822	77688
940796f367e4d04fafaac01e58315564288e1e19	ct prostate deformable segmentation by boundary regression		Automatic and accurate prostate segmentation from CT images is challenging due to low image contrast, uncertain organ motion, and variable organ appearance in different patient images. To deal with these challenges, we propose a new prostate boundary detection method with a boundary regression strategy for prostate deformable segmentation. Different from the previous regression-based segmentation methods, which train one regression forest for each specific point (e.g., each point on a shape model), our method learns a single global regression forest to predict the nearest boundary points from each voxel for enhancing the entire prostate boundary. The experimental results show that our proposed boundary regression method outperforms the conventional prostate classification method. Compared with other state-of-the-art methods, our method also shows a competitive performance.	ct scan	Yeqin Shao;Yaozong Gao;Xin Yang;Dinggang Shen	2014		10.1007/978-3-319-13972-2_12	artificial intelligence;computer science;voxel;computer vision;pattern recognition;prostate;regression;organ motion;entire prostate;ct prostate;segmentation	Vision	41.11656064849567	-77.88477807589594	77744
3f04db828e24ac046145d7a1d0578ee379e1851e	vector-based active surfaces for segmentation of dynamic pet images	positron emission tomography image segmentation vectors shape force biomedical imaging;3d segmentation;dynamic pet;image segmentation;image resolution;vector field convolution 3 d segmentation active surface dynamic pet;edge detection;single frame based active surface model vector based active surface dynamic pet imaging radiotracer concentration kinetics limited spatial resolution low signal to noise ratio time dependent contrast biological volume segmentation deformable model boundary continuity high noise sensitivity dynamic pet image segmentation parametric active surface vectorial image gradient signal spatial consistency signal temporal consistency gate monte carlo simulation;active surface;positron emission tomography;medical image processing;radioactive tracers edge detection gradient methods image resolution image segmentation medical image processing monte carlo methods physiological models positron emission tomography;gradient methods;radioactive tracers;vector field convolution;physiological models;monte carlo methods	Dynamic PET imaging enables the study of radiotracers concentration kinetics along time. However, PET images suffer from limited spatial resolution, low signal-to-noise ratio, and time dependent contrast between tissues, making segmentation of biological volumes difficult. Deformable models are of great interest due to their inherent boundary continuity, but their applications to PET images still remains challenging due to high sensitivity to noise. To address these limitations, we propose a method to perform 3-D+time dynamic PET image segmentation using parametric active surfaces based on a gradient of the vectorial image. This method takes advantage of both spatial and temporal consistency of the signal along the acquisition. We validate our method using GATE Monte Carlo simulations and compare it with single frame based active surface models. We show significant improvement of several figures of merit.	gate;gradient;image segmentation;kinetics internet protocol;monte carlo method;polyethylene terephthalate;scott continuity;signal-to-noise ratio;simulation	Vincent Jaouen;Paulo González;Simon Stute;Denis Guilloteau;Irène Buvat;Clovis Tauber	2013	2013 IEEE 10th International Symposium on Biomedical Imaging	10.1109/ISBI.2013.6556412	computer vision;edge detection;image resolution;computer science;image segmentation;nuclear medicine;monte carlo method;medical physics	Vision	45.870496119152484	-78.35629469873743	77935
f8cdd0d3d27f59b62e9b5a43cb9aebe86277490b	using snakes to detect the intimal and adventitial layers of the common carotid artery wall in sonographic images	tratamiento automatico;error medida;intima media layer;oscillations;optimisation;informatica biomedical;image numerique;biomedical data processing;paroi vasculaire;active contour;measurement error;image processing;optimizacion;cost function;common carotid;algorithme snake;genie biomedical;edge detection;estudio comparativo;pared vascular;adventice;informatique biomedicale;diagnostico;adventicia;procesamiento imagen;hombre;echography;snake;carotida primitiva;traitement image;common carotid artery;deteccion contorno;erreur mesure;frequency response;etude comparative;detection contour;circulatory system;automatic processing;exploration ultrason;biomedical engineering;automatic detection;vascular wall;imagen numerica;human;comparative study;adventitia;carotide primitive;intima;exploracion ultrasonido;ingenieria biomedica;optimization;digital image;traitement automatique;appareil circulatoire;diagnosis;aparato circulatorio;ecografia;echographie;sonography;homme;diagnostic	This study presents an innovative automatic system for detecting the intima-media complex of the far wall of the common carotid artery by applying the snake techniques. Cohen's snake was modified and some criteria were added for our applications. In addition, the oscillating problem of using snakes was solved by properly choosing the time step from analysis of the frequency response of the filters. A time-diminishing gravity window, external forces, and a cost function assist the snake in selecting the optimal shape of intimal and adventitia layers. We compared the proposed snake and ziplock snake with respect to the manual extraction contour. The results show that the system can automatically detect the intimal and adventitial layers without any manual correction.	adventitia;carotid arteries;choose (action);frequency response;loss function;sensor;snakes;anatomical layer	Da-Chuan Cheng;Arno Schmidt-Trucksäss;Kuo-Sheng Cheng;Hans Burkhardt	2002	Computer methods and programs in biomedicine	10.1016/S0169-2607(00)00149-8	computer vision;frequency response;edge detection;image processing;computer science;comparative research;circulatory system;active contour model;oscillation;digital image;anatomy;surgery;observational error	HCI	45.95075841945079	-79.09445617491934	78070
a60e5c07373cedbdd39ccb6e5c17be024b72b20d	semi-automatic three-dimensional vessel segmentation using a connected component localization of the region-scalable fitting energy	computed tomography semiautomatic three dimensional vessel segmentation connected component localization region scalable fitting energy patient specific vascular segment vascular structures three dimensional medical image segmentation region scalable fitting energy split bregman method magnetic resonance;image segmentation biomedical imaging computed tomography level set signal processing algorithms three dimensional displays image reconstruction;image segmentation biomedical mri computerised tomography	Segmentation of patient-specific vascular segments of interest from medical images is an important topic for numerous applications. Despite the great importance of having semi-automatic segmentation methods in this field, the process of image segmentation is still based on several operator-dependent steps which make large-scale segmentation a non trivial and time consuming task. In this work we present a semi-automatic segmentation method to reconstruct vascular structures from three-dimensional medical images. We start from the minimization of the Region Scalable Fitting Energy using the Split-Bregman method and we modify the resulting algorithm adding a connected component extraction of the solution starting from a point that identifies the vascular structure of interest. In this way, we add a constraint to the algorithm focusing it only on the vascular structure we want to reconstruct and avoiding the attachment with the nearby objects. Finally, we describe a strategy to minimize the number of involved parameters in order to limit the user effort. The results obtained on two different images (a Magnetic Resonance and a Computed Tomography) demonstrate that our method outperforms the original method in segmenting the vascular region of interest without the inclusion of nearby objects in the result.	algorithm;attachments;bregman method;ct scan;connected component (graph theory);curve fitting;image segmentation;iteration;iterative method;medical imaging;region of interest;resonance;semiconductor industry;tomography	Marco Fedele;Elena Faggiano;Luca Barbarotta;Francesco Cremonesi;Luca Formaggia;Simona Perotto	2015	2015 9th International Symposium on Image and Signal Processing and Analysis (ISPA)	10.1109/ISPA.2015.7306035	computer vision;mathematical optimization;range segmentation;segmentation-based object categorization;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;computer graphics (images)	Vision	44.603015187737796	-74.80949172940188	78273
85384a8871030bbd1681adee9e9956dce4d751ba	scale-invariant anomaly detection with multiscale group-sparse models	detectors;scanning electron microscopy;training;monitoring;dictionaries;production;encoding	The automatic detection of anomalies, defined as patterns that are not encountered in representative set of normal images, is an important problem in industrial control and biomedical applications. We have shown that this problem can be successfully addressed by the sparse representation of individual image patches using a dictionary learned from a large set of patches extracted from normal images. Anomalous patches are detected as those for which the sparse representation on this dictionary exceeds sparsity or error tolerances. Unfortunately, this solution is not suitable for many real-world visual inspection-systems since it is not scale invariant: since the dictionary is learned at a single scale, patches in normal images acquired at a different magnification level might be detected as anomalous. We present an anomaly-detection algorithm that learns a dictionary that is invariant to a range of scale changes, and overcomes this limitation by use of an appropriate sparse coding stage. The algorithm was successfully tested in an industrial application by analyzing a dataset of Scanning Electron Microscope (SEM) images, which typically exhibit different magnification levels.	algorithm;anomaly detection;dictionary;electron;image scanner;neural coding;sparse approximation;sparse matrix;visual inspection	Diego Carrera;Giacomo Boracchi;Alessandro Foi;Brendt Wohlberg	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7533089	computer vision;detector;k-svd;computer science;machine learning;pattern recognition;scanning electron microscope;encoding	Vision	48.28333202572606	-66.443746700147	78491
297c097eee53462747749012e83f178fdfcded6c	an evolutionary spline fitting algorithm for identifying filamentous cyanobacteria	bright field microscopy;filament segmentation;filamentous cyanobacteria;evolutionary strategy	Bright field cellular microscopy is a simple and non-invasive method for capturing cytological images. However, the resulting micrographs prove challenging for image segmentation, especially with samples that have tightly clustered or overlapping cells. Filamentous cyanobacteria grow as linearly arranged cells forming chain-like filaments that often touch and overlap. Existing bright field cell segmentation methods perform poorly with these bacteria, and are incapable of identifying the filaments. Existing filament tracking methods are rudimentary, and cannot reliably account for overlapping or parallel touching filaments. We propose a new approach for identifying filaments in bright field micrographs by combining information about both filaments and cells. This information is used by an evolutionary strategy to iteratively construct a continuous spline representation that tracks the medial line of the filaments. We demonstrate that overlapping and parallel touching filaments are segmented correctly in many difficult cases.	algorithm;image segmentation;medial graph;smoothing spline;spline (mathematics)	Jeremy Porter;Dirk V. Arnold	2013		10.1145/2480362.2480372	computer science;bioinformatics;evolution strategy;bright-field microscopy	Vision	40.651932861316716	-73.38611217126841	78646
0c54f88e0e05454d55faed3ec435a974601ca72d	reconstruction of 3-d head geometry from digitized point sets: an evaluation study	model selection;image segmentation;evaluation method;indexing terms;magnetoencephalography;geometry surface reconstruction surface treatment scalp brain modeling deformable models image reconstruction skull image segmentation head;image segmentation medical image processing image reconstruction image registration bone electroencephalography magnetoencephalography biomedical mri;magnetic resonance;image reconstruction;medical image processing;brain electroencephalography head humans image interpretation computer assisted imaging three dimensional magnetic resonance imaging magnetoencephalography models biological reproducibility of results scalp sensitivity and specificity signal processing computer assisted skull subtraction technique;image registration;surface model;bone;free form deformation;sparse data;geometric model;electroencephalography;deformable model;biomedical application;evaluation studies;magnetoencephalography 3 d head geometry reconstruction patient specific scalp skull brain surface image registration segmented magnetic resonance volume image affine registration free form deformation registration electroencephalography;biomedical mri	In this paper, we evaluate different methods to estimate patient-specific scalp, skull, and brain surfaces from a set of digitized points from the target's scalp surface. The reconstruction problem is treated as a registration problem: An a priori surface model, consisting of the scalp, skull, and brain surfaces, is registered to the digitized surface points. The surface model is generated from segmented magnetic resonance (MR) volume images. We study both affine and free-form deformation (FFD) registration, the use of average models, the averaging of individual registration results, a model selection procedure, and statistical deformation models. The registration algorithms are mainly previously published, and the objective of this paper is to evaluate these methods in this particular application with sparse data. The main interest of this paper is to generate geometric head models for biomedical applications, such as electroencephalography and magnetoencephalographic. However, the methods can also be applied to other anatomical regions and to other application areas. The methods were validated using 15 MR volume images, from which the scalp, skull, and brain were manually segmented. The best results were achieved by averaging the results of the FFD registrations of the database: the mean distance from the manually segmented target surface to a deformed a priori model surface for the studied anatomical objects was 1.68-2.08 mm, depending on the point set used. The results support the use of the evaluated methods for the reconstruction of geometric models in applications with sparse data.	algorithm;electroencephalography;free-form deformation;magnetoencephalography;model selection;patients;physical object;reconstruction conjecture;resonance;scientific publication;sparse matrix;registration - actclass	Juha Koikkalainen;Jyrki Lötjönen	2004	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2004.834401	iterative reconstruction;computer vision;index term;radiology;medicine;sparse matrix;electroencephalography;computer science;image registration;geometric modeling;magnetic resonance imaging;mathematics;image segmentation;model selection;magnetoencephalography;medical physics	Vision	43.774394735155994	-79.87245078845025	78905
fdfa6f5dd0fefc223fa2de06bd5ba7e03a952fea	automatic gray level thresholding based on relevant features for subjective decisions	fuzzy membership function;image processing;uncertainty handling;fuzzy set theory;uncertainty measure automatic image gray level thresholding global criteria subjective decision fuzzy membership functions;testing humans entropy measurement uncertainty image segmentation layout linear systems spline vectors pixel;uncertainty handling fuzzy set theory image processing	This work searches the relationship between global criteria for automatic image gray level thresholding, and the subjective decision concerned to best image gray level threshold. An approach based on fuzzy membership functions is proposed to confirm this relationship. As aside, an efficient uncertainty measure arose. A set of 100 images was trained and tested, and their results will be discussed.	grayscale;thresholding (image processing)	Fabrício Martins Lopes;Luís Augusto Consularo	2002		10.1109/SIBGRA.2002.1167183	computer vision;membership function;fuzzy classification;fuzzy number;machine learning;pattern recognition;mathematics;fuzzy set operations	Vision	41.78633325671288	-67.63585142342716	78966
b20c3f8ae34e54090424490d77e971bd61668dfc	variational segmentation of the white and gray matter in the spinal cord using a shape prior		Segmenting the inner structure of the spinal cord on magnetic resonance (MR) images is difficult because of poor contrast between white and gray matter (WM/GM). We present a variational formulation to automatically detect cerebrospinal fluid and WM/GM. The segmentation results are obtained by continuous cuts combined with a shape prior. Intensity-based segmentation guarantees high accuracy while the shape prior aims at precision. We tested the algorithm on a set of MR images with visual WM/GM contrast and evaluated it w.r.t. manual GM segmentations. The automated GM segmentations are on a par with the manual results.	variational principle	Antal Horváth;Simon Pezold;Matthias Weigel;Katrin Parmar;Oliver Bieri;Philippe C. Cattin	2016		10.1007/978-3-319-55050-3_3	spinal cord;computer vision;magnetic resonance imaging;mathematics;active appearance model;artificial intelligence;segmentation;cerebrospinal fluid;anatomy	Vision	40.47434572660767	-79.20637399238421	79021
250ca873661a21b82149bcdd55a05d9b0f9fee59	algorithmic 3d simulation of breast calcifications for digital mammography	aide diagnostic;proyeccion;glandula mamaria patologia;computer assisted diagnosis;female;radiodiagnostic;image numerique;formation image tridimensionnelle;high resolution;image processing;3d imaging;tumor maligno;mastografia;mammary gland;simulation;procesamiento imagen;hombre;simulacion;calcificacion;traitement image;three dimensional;algorithme;glandula mamaria;algorithm;3d simulation;radiodiagnostico;digital mammography;hembra;mammographie;projection;imagen numerica;human;mammary gland diseases;evaluation;tumeur maligne;glande mammaire;digital image;evaluacion;radiodiagnosis;femelle;mammography;calcification;formacion imagen tridimensional;diagnostic aid;glande mammaire pathologie;malignant tumor;breast calcifications;homme;algoritmo;ayuda diagnostica;spatial resolution	We present a framework for algorithmic three-dimensional simulation of breast calcifications. The simulated calcifications can be viewed from any angle at a higher spatial resolution than currently available for digital mammography, and they can be placed onto a simulated or real mammographic background to provide example cases for computers and radiologists. In order to simulate calcification clusters, we also show how to simulate duct networks and terminal ductal lobular units. We evaluated the model with a double-blind evaluation of 60 cases with four experienced radiologists by mixing 30 cases of simulated calcification clusters on a real or simulated mammographic background with 30 cases of real breast calcification clusters digitized at a spatial resolution of 15 microm from high-resolution radiographs of 5 mm slices of breast specimens. The results indicate that the majority of the 2D projections of the 3D simulated calcifications compare favorably with the radiographic images of real breast calcifications.		Janne Näppi;Peter B. Dean;Olli Nevalainen;Sakari Toikkanen	2001	Computer methods and programs in biomedicine	10.1016/S0169-2607(01)00145-6	computer vision;radiology;image resolution;medicine;pathology;image processing;computer science;nuclear medicine	Graphics	46.39178865143214	-79.62703386382546	79057
00a27f1dbfbe86d946a61787d79c1324554d085a	a new medical image segmentation model based on fractional order differentiation and level set		Segmenting medical images is still a challenging task for both traditional local and global methods because the image intensity inhomogeneous. In this paper, two contributions are made: (i) on the one hand, a new hybrid model is proposed for medical image segmentation, which is built based on fractional order differentiation, level set description and curve evolution; and (ii) on the other hand, three popular definitions of Fourier-domain, Grunwald-Letnikov (G-L) and Riemann-Liouville (R-L) fractional order differentiation are investigated and compared through experimental results. Because of the merits of enhancing high frequency features of images and preserving low frequency features of images in a nonlinear manner by the fractional order differentiation definitions, one fractional order differentiation definition is used in our hybrid model to perform segmentation of inhomogeneous images. The proposed hybrid model also integrates fractional order differentiation, fractional order gradient magnitude and difference image information. The widely-used dice similarity coefficient metric is employed to evaluate quantitatively the segmentation results. Firstly, experimental results demonstrated that a slight difference exists among the three expressions of Fourier-domain, G-L, RL fractional order differentiation. This outcome supports our selection of one of the three definitions in our hybrid model. Secondly, further experiments were performed for comparison between our hybrid segmentation model and other existing segmentation models. A noticeable gain was seen by our hybrid model in segmenting intensity inhomogeneous images.	image segmentation	Bo Chen;Shan Huang;Feifei Xie;Lihong C. Li;Wensheng Chen;Zhengrong Liang	2018		10.1117/12.2292931	market segmentation;level set;image segmentation;nonlinear system;active contour model;magnitude (mathematics);expression (mathematics);pattern recognition;segmentation;artificial intelligence;mathematics	Vision	45.8964002247841	-73.05791615374984	79062
56b7f384487c18ce246cf1ebe762963421af840e	quick matting: a matting method based on pixel spread and propagation	image sampling;color sampling;adjacent pixel alpha values;matting;quick matting method;pixel robustness image color analysis computational efficiency bayesian methods optimization arrays;bayesian methods;foreground pixel;image sampling feature extraction image colour analysis;arrays;alpha value matting trimap;foreground pixel quick matting method color sampling adjacent pixel alpha values robust matting;image color analysis;image colour analysis;feature extraction;pixel;trimap;alpha value;robustness;optimization;robust matting;computational efficiency	The problem of matting is always solved by finding the alpha value for each pixel in the image. Many recent methods combine color sampling and affinity definition in different steps, leading to large computational cost. In the proposed method, when the alpha value of a pixel Pi is calculated, the pixel is regarded as a foreground pixel to help calculate its adjacent pixels' alpha values, resulted in a faster solution. This spreading way of traversal also ensures local continuity of foreground object and improves the visual result. Experiments show our Quick Matting can achieve comparable alpha mattes as Robust Matting, while the speed is enhanced by about 25 times.	affinity analysis;algorithmic efficiency;chroma subsampling;computation;experiment;pixel;sampling (signal processing);scott continuity;software propagation;tree traversal	Yiyang Gu;Cheng Jin;Xiangyang Xue	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5651679	computer vision;feature extraction;bayesian probability;computer science;pattern recognition;pixel;robustness;computer graphics (images)	Vision	49.46020427639822	-67.76646811416215	79099
4d5a42d69247b18001816ba9e36388430d928271	image segmentation based on statistically principled clustering	1 dimensional histogram segmentation;histograms;smoothest density;spline;pattern clustering;optimisation;gaussian processes optimisation image segmentation pattern clustering statistical analysis splines mathematics;mathematics;heart;image segmentation;image segmentation histograms distribution functions heart clustering algorithms mathematics computer science spline fasteners gaussian processes;gaussian processes;spline functions;higher dimensions image segmentation statistically principled clustering 1 dimensional clustering smoothest density optimisation problem spline functions gaussians 1 dimensional histogram segmentation local nonparametric cluster validity criterion;1 dimensional;splines mathematics;local nonparametric cluster validity criterion;statistical analysis;mixture model;1 dimensional clustering;spline function;higher dimensions;clustering algorithms;gaussians;optimisation problem;fasteners;computer science;cluster validity;distribution functions;statistically principled clustering	A statistically principled approach to 1-dimensional clustering was introduced by Pauwels abd Frederix (2000). In this approach clustering is achieved by finding the smoothest density that is statistically compatible with the observed data. In the current contribution we propose two solutions for the optimisation problem that is at the heart of this algorithm. The first solution is based on spline-functions, while the second hinges on an expansion of the density in terms of Gaussians. The latter is reminiscent of mixture-models but fundamentally different in its interpretation. Finally, we argue that 1-dimensional histogram segmentation yields a powerful local nonparametric cluster-validity criterion that can be used to check the quality of proposed clusterings in higher dimensions.	cluster analysis;image segmentation	Greet Frederix;Geert Caenen;Eric J. Pauwels	2001		10.1109/ICIP.2001.958052	spline;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	49.16595473787075	-72.32636698958362	79114
100e153018bd71265c514db2042577075174bdeb	an artificial ant colonies approach to medical image segmentation	image segmentation;ant colony;artificial ant colonies;computer graphic;pheromone;biological systems;self organization;image analysis;medical image segmentation;human brain	The success of image analysis depends heavily upon accurate image segmentation algorithms. This paper presents a novel segmentation algorithm based on artificial ant colonies (AC). Recent studies show that the self-organization of ants is similar to neurons in the human brain in many respects. Therefore, it has been used successfully for understanding biological systems. It is also widely used in many applications in robotics, computer graphics, etc. Considering the features of artificial ant colonies, we present an extended model for image segmentation. In our model, each ant can memorize a reference object, which will be refreshed when it finds a new target. A fuzzy connectedness measure is adopted to evaluate the similarity between target and the reference object. The behavior of an ant is affected by the neighbors and the cooperation between ants is performed by exchanging information through pheromone updating. Experimental results show that the new algorithm can preserve the detail of the object and is also insensitive to noise.	algorithm;ant venoms;ant colony;ants;biological system;computer graphics;entity name part qualifier - adopted;image analysis;image segmentation;medical specialities;memory refresh;pheromone;robotics;self-organization;biologic segmentation	Peng Huang;Huizhi Cao;Shuqian Luo	2008	Computer methods and programs in biomedicine	10.1016/j.cmpb.2008.06.012	computer vision;self-organization;image analysis;ant robotics;computer science;artificial intelligence;ant colony;machine learning;segmentation-based object categorization;image segmentation	Robotics	43.109663278432535	-71.50539638025727	79396
4ae18b790bf861e225544ad46f4c2ecce45d0596	multi-scale approach for retinal vessel segmentation using medialness function	databases;eigenvalues and eigenfunctions;retinal vessels image segmentation eigenvalues and eigenfunctions retina filters image databases blood vessels biomedical imaging vectors pathology;eye;image segmentation;radius estimation retinal vessel segmentation medialness function eigenvalue;hessian matrix;drive database;automated segmentation;eigenvalues;eigenvalue;optic fundus images;multiscale approach;medialness function;feature extraction;image reconstruction;medical image processing;pixel;stare database;vessel segmentation;retinal vessel segmentation;retinal vessels;vessel reconstruction;vessel medial lines extraction;medical image processing blood vessels eigenvalues and eigenfunctions eye feature extraction hessian matrices image reconstruction image segmentation;weighted 2d medialness function;blood vessels;radius estimation;noise;vessel reconstruction multiscale approach retinal vessel segmentation automated segmentation optic fundus images weighted 2d medialness function eigenvalues hessian matrix vessel medial lines extraction drive database stare database;hessian matrices	Automated segmentation of retinal vessels in optic fundus images has been the most prevailing effort in many researches during recent years. In this paper, we propose a multi-scale method based on a weighted 2D medialness function. The result of the medialness function is first multiplied by the eigenvalues of the Hessian matrix in every pixel of the image in order to extract vessel's medial-lines. Next, by extracting the centerlines of vessels and estimation of radius of vessels, the retinal vessels are segmented. Finally, the performance of our proposed method is evaluated by the DRIVE and STARE databases and compared with those of several recent methods.	database;hessian;medial graph;pixel	Elahe Moghimirad;Seyed Hamid Rezatofighi;Hamid Soltanian-Zadeh	2010	2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2010.5490423	computer vision;radiology;eigenvalues and eigenvectors;computer science;machine learning;mathematics;optics	Vision	40.404702807959325	-75.59529431655834	79462
1ded556ebd9c577c113d99e8d88c2557e69c5544	detection of focal changes in human cortical thickness: spherical wavelets <ce:italic>versus</ce:italic> gaussian smoothing	sensitivity and specificity;shape descriptor;smoothing method;aging;spherical wavelets;normal aging;neurodegenerative disease;gaussian smoothing;degeneration;gaussian kernel;spatial filtering;neurodegenerative diseases;cortical thickness;cortical atrophy	Subtle but progressive variations in human cortical thickness have been associated with the initial phases of prevalent neurological and psychiatric conditions. But slight changes in cortical thickness at preclinical stages are typically masked by effects of the Gaussian kernel smoothing on the cortical surface shape descriptors. Here we present the first study aimed at detecting changes in human cortical thickness maps by applying soft-thresholding to multiresolution spherical wavelet coefficients. In order to make Gaussian and wavelet smoothing methods comparable, the trade-off between sensitivity and specificity was optimized to detect simulated thickness changes in various cortical areas of healthy elderly subjects. Results revealed a better sensitivity-specificity trade-off when using wavelet-based methods as compared to Gaussian smoothing in both the whole neocortex (p<10(-7)) and cortical region-based statistical analyses (p<10(-9)), which was mainly due to the higher specificity obtained with the wavelet approach. The lower smoothing introduced by wavelets and their adaptive properties may account for the enhanced specificity and sensitivity when compared with Gaussian spatial filters. These results strongly support the use of spherical wavelet methods to detect subtle variations in cortical thickness maps, which may be crucial in better understanding the course of neuronal loss in normal aging and in finding early markers of cortical degeneration.	apricot kernel oil;blinded;cerebral cortex;coefficient;cortical implant;focal (programming language);gaussian blur;legendre wavelet;map;mental disorders;neocortex;nerve degeneration;normal statistical distribution;sensitivity and specificity;sensor;smoothing (statistical technique);thickness (graph theory);thresholding (image processing)	Jorge L. Bernal-Rusiel;Mercedes Atienza;Jose Luis Cantero	2008	NeuroImage	10.1016/j.neuroimage.2008.03.022	computer vision;mathematical optimization;gaussian blur;mathematics;gaussian function;spatial filter;statistics	ML	43.13755617419057	-78.85456686134607	79467
d45692c82173d60eb105d3f579319fe3aad690a2	penalized geodesic tractography for mitigating gyral bias		In this paper, we introduce a penalized geodesic tractography (PGT) algorithm for mitigating gyral bias in cortical tractography, which is essential for improving cortical connectomics. Unlike deterministic and probabilistic tractography algorithms that perform one-way tracking, PGT solves a global optimization problem in estimating the pathways connecting multiple regions, instead of local step-by-step orientation tracing. PGT is unconfounded by local false-positive or falsenegative fiber orientations and ensures that fiber streamlines that are intended to connect two regions do not terminate prematurely. We show that PGT reduces gyral bias by allowing streamlines to make sharper turns into the cortical gyral matter and results in a significantly more uniform spatial distribution of cortical connections.	algorithm;connectomics;exponent bias;global optimization;mathematical optimization;one-way function;optimization problem;terminate (software)	Ye Wu;Yuanjing Feng;Dinggang Shen;Pew-Thian Yap	2018		10.1007/978-3-030-00931-1_2	computer science;artificial intelligence;computer vision;global optimization;pattern recognition;streamlines, streaklines, and pathlines;probabilistic logic;geodesic;spatial distribution;connectomics;tractography	ML	48.81245160581008	-78.09770959341061	79642
5d4ecaffedc053c71a75eec0aa3cb1c3ac2f7a5d	noddi-sh: a computational efficient noddi extension for fodf estimation in diffusion mri		Diffusion Magnetic Resonance Imaging (DMRI) is the only non-invasive imaging technique which is able to detect the principal directions of water diffusion as well as neurites density in the human brain. Exploiting the ability of Spherical Harmonics (SH) to model spherical functions, we propose a new reconstruction model for DMRI data which is able to estimate both the fiber Orientation Distribution Function (fODF) and the relative volume fractions of the neurites in each voxel, which is robust to multiple fiber crossings. We consider a Neurite Orientation Dispersion and Density Imaging (NODDI) inspired single fiber diffusion signal to be derived from three compartments: intracellular, extracellular, and cerebrospinal fluid. The model, called NODDI-SH, is derived by convolving the single fiber response with the fODF in each voxel. NODDI-SH embeds the calculation of the fODF and the neurite density in a unified mathematical model providing efficient, robust and accurate results. Results were validated on simulated data and tested on in-vivo data of human brain, and compared to and Constrained Spherical Deconvolution (CSD) for benchmarking. Results revealed competitive performance in all respects and inherent adaptivity to local microstructure, while sensibly reducing the computational cost. We also investigated NODDI-SH performance when only a limited number of samples are available for the fitting, demonstrating that 60 samples are enough to obtain reliable results. The fast computational time and the low number of signal samples required, make NODDI-SH feasible for clinical application.	algorithmic efficiency;cambridge structural database;computation;deconvolution;lib sh;mathematical model;resonance;simulation;time complexity;video-in video-out;voxel	Mauro Zucchelli;Maxime Descoteaux;Gloria Menegaz	2017	CoRR		voxel;relative volume;spherical harmonics;fiber;diffusion mri;biological system;distribution function;deconvolution;physics	Vision	49.16770869656587	-79.59753679716938	79761
3033118d0ab97f3982b7fa5c44c2e3e9b37dcf27	an evaluation of wavelet features subsets for mammogram classification	transformation ondelette;texture;image processing;mastografia;image databank;procesamiento imagen;metric;classification;traitement image;mammographie;banco imagen;banque image;textura;pattern recognition;metrico;feature selection;transformacion ondita;reconnaissance forme;mammography;reconocimiento patron;clasificacion;metrique;wavelet transformation	This paper is about an evaluation for a feature selection strategy for mammogram classification. An earlier solution to this problem is revisited, which constructed a supervised classifier for two problems in mammogram classification: tumor nature, and tumor geometric type. The approach works by transforming the data of the images in a wavelet basis and by using a minimum subset of representative features of these textures based in a specific threshold (λT). In this paper different wavelet bases, variation of the selection strategy for the coefficients, and different metrics are all evaluated with known labelled images. This is a suitable solution worth further exploration. For the experiments we have used samples of images labeled by physicians. Results shown are promising, and we describe possible lines for future directions.		Cristiane Bastos Rocha Ferreira;Díbio Leandro Borges	2005		10.1007/11578079_65	computer vision;metric;image processing;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;texture;feature selection	Vision	39.91530584449779	-69.21101799148414	79818
29e888b08fc72c3e1035476161e283cbb5b8a165	recovering affine deformations of fuzzy shapes	object representation;computer vision and robotics autonomous systems;fuzzy set;x ray imaging;shape descriptor;datorseende och robotik autonoma system;image processing and analysis;shape matching;fuzzy segmentation	Fuzzy sets and fuzzy techniques are attracting increasing attention nowadays in the field of image processing and analysis. It has been shown that the information preserved by using fuzzy representation based on area coverage may be successfully utilized to improve precision and accuracy of several shape descriptors; geometric moments of a shape are among them. We propose to extend an existing binary shape matching method to take advantage of fuzzy object representation. The result of a synthetic test show that fuzzy representation yields smaller registration errors in average. A segmentation method is also presented to generate fuzzy segmentations of real images. The applicability of the proposed methods is demonstrated on real X-ray images of hip replacement implants.	active contour model;boundary representation;contour line;discretization;image moment;image processing;synthetic intelligence;x-ray (amazon kindle)	Attila Tanács;Csaba Domokos;Natasa Sladoje;Joakim Lindblad;Zoltan Kato	2009		10.1007/978-3-642-02230-2_75	computer vision;computer science;machine learning;mathematics;geometry;fuzzy set	Vision	45.1331367950233	-74.88447497751565	79952
1caa09619a6048b7f58925495c653fd0f5aec6d0	model-based approach to tomographic reconstruction including projection deblurring. sensitivity of parameter model to noise on data	model based approach;tomographic reconstruction;data acquisition	Classical techniques for the reconstruction of axisymmet- rical objects are all creating artefacts (smooth or unstable solutions). Moreover, the extraction of very precise features related to big density transitions remains quite delicate. In this paper, we develop a new approach -in one dimension for the moment- that allows us both to reconstruct and to extract characteristics: an a priori is provided thanks to a density model. We show the interest of this method in regard to noise effects quantification ; we also explain how to take into account some physical perturbations occuring with real data acquisition.	deblurring;tomographic reconstruction	Jean Michel Lagrange;Isabelle Abraham	2004		10.1007/978-3-540-24673-2_4	computer vision;data acquisition;tomographic reconstruction	Vision	51.163701252100275	-77.22882784625038	79984
6318d4575a29c211f12ef6fcf8b8bedce83b37eb	wavelet transform domain filters: a spatially selective noise filtration technique	filtrado wiener;transformation ondelette;filtering;filtrage;medical imagery;wavelet domain wavelet transforms filters filtration spatial resolution image resolution signal resolution image edge detection wavelet analysis image analysis;spatial dependence;image processing;filtre selectif;methode echelle multiple;edge detection;filtrado;ruido;procesamiento imagen;motion artifact removal wavelet transform domain filters spatially selective noise filtration technique multiresolution decompositions direct spatial correlation simulated signals phantom images real mr images noise content resolution filtered images artifacts noise filtration technique spatially dependent noise filtration edge detection enhancement image restoration;selective filter;filtro selectivo;image restoration;metodo escala multiple;correlation methods;traitement image;wavelet transforms;correlation methods wavelet transforms edge detection filtering theory image restoration image enhancement medical image processing medical signal processing;image enhancement;mr imaging;correlation spatiale;wavelet transform;spatial correlation;correlacion espacial;medical image processing;wiener filtering;bruit;imagerie medicale;multiscale method;imageneria medical;filtrage wiener;transformacion ondita;motion artifact;medical signal processing;filtering theory;wavelet transformation;noise	Wavelet transforms are multiresolution decompositions that can be used to analyze signals and images. They describe a signal by the power at each scale and position. Edges can be located very effectively in the wavelet transform domain. A spatially selective noise filtration technique based on the direct spatial correlation of the wavelet transform at several adjacent scales is introduced. A high correlation is used to infer that there is a significant feature at the position that should be passed through the filter. The authors have tested the technique on simulated signals, phantom images, and real MR images. It is found that the technique can reduce noise contents in signals and images by more than 80% while maintaining at least 80% of the value of the gradient at most edges. The authors did not observe any Gibbs' ringing or significant resolution loss on the filtered images. Artifacts that arose from the filtration are very small and local. The noise filtration technique is quite robust. There are many possible extensions of the technique. The authors see its applications in spatially dependent noise filtration, edge detection and enhancement, image restoration, and motion artifact removal. They have compared the performance of the technique to that of the Weiner filter and found it to be superior.	artifact (error);circuit restoration;edge detection;eighty;gradient;greater than;image restoration;inference;morphologic artifacts;multiresolution analysis;phantom reference;phantoms, imaging;ringing (signal);treatment - filtration;wavelet transform;wiener filter;contents - htmllinktype	Yansun Xu;John B. Weaver;Dennis M. Healy;Jian Lu	1994	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.336245	computer vision;speech recognition;image processing;computer science;mathematics;statistics;wavelet transform	Visualization	50.63973003042919	-76.40054394111301	80248
583e200c93c5c9c72fae6b9a28b7deacffd4ee31	robust pointwise tracking of countours	discrete optimization;heart;cost function;pointwise;level set;dynamic program;segmentation;computer programming;connectors;distortion;minimum distance;contours;medicine;tracking	For several applications in Medicine, it is fundamental to determine the spatial tracking, along the time, of structure parts such as the apex of heart. The objective of this work is to present a robust technique for pointwise tracking of contours. Given two n-dimensional closed contours (S, D) derived from two consecutive image scenes, the basic idea is to find the cheapest path that connects each point in S to a point in D. The critical steps are the definition of the cost function and the numerical approach for the global discrete minimization. For the cost function, we have used the minimum distance to the contours and curvature of a level set function. The global discrete optimization can be achieved using dynamic programming on a constrained region with privileged tracks. A privileged path is associated to all elements that form destination contour D and a point of this contour is set as seed for the dynamic programming. By this artifice, all paths can be obtained. Simulations with several polygons showed encouraging results. For instance, for 10 prominent points and distortion of 15 plus 20% of expansion, in a 400 x 400 pixels image, the mean distance error was below 2 pixels.	apex (geometry);computer simulation;discrete optimization;distortion;dynamic programming;loss function;mathematical optimization;numerical analysis;pixel	Sérgio Shiguemi Furuie;A. V. Dias	2002		10.1117/12.467148	computer vision;mathematical optimization;mathematics;geometry	Vision	46.485444857448805	-75.30586507463495	80430
22f610cb31f9a93e9a116b861f0e6adf56c50d57	tissue characterization using dimensionality reduction and fluorescence imaging	tissue characterization;fluorescence imaging;biological tissue;time resolved;data association;multi spectral;dimensional reduction	Multidimensional fluorescence imaging is a powerful molecular imaging modality that is emerging as an important tool in the study of biological tissues. Due to the large volume of multi-spectral data associated with the technique, it is often difficult to find the best combination of parameters to maximize the contrast between different tissue types. This paper presents a novel framework for the characterization of tissue compositions based on the use of time resolved fluorescence imaging without the explicit modeling of the decays. The composition is characterized through soft clustering based on manifold embedding for reducing the dimensionality of the datasets and obtaining a consistent differentiation scheme for determining intrinsic constituents of the tissue. The proposed technique has the benefit of being fully automatic, which could have significant advantages for automated histopathology and increasing the speed of intraoperative decisions. Validation of the technique is carried out with both phantom data and tissue samples of the human pancreas.	biologic preservation;body tissue;cluster analysis;composition;dimensionality reduction;embedding;entity class - imaging modality;excitation;explicit modeling;fluorescence imaging;histocompatibility testing;histopathology;isomap;map;molecular imaging;numerous;pancreas extract;phantom reference;phantoms, imaging;sampling (signal processing);spatial reference system;spatial variability;test set;manifold;statistical cluster	Karim Lekadir;Daniel S. Elson;Jose Requejo-Isidro;Christopher Dunsby;James McGinty;Neil Galletly;Gordon Stamp;Paul M. W. French;Guang-Zhong Yang	2006	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/11866763_72	pathology;fluorescence-lifetime imaging microscopy	Visualization	44.01611800745078	-79.8629046765334	80656
0fdff943755bb220aebf5eca91ebc1b8ae08c84c	3d reconstruction of tumors for applications in laparoscopy using conformal geometric algebra	histograms;tumours algebra biomedical ultrasonics endoscopes geometry image reconstruction medical image processing particle filtering numerical methods set theory stereo image processing;laparoscopy;ultrasound;3d tumors reconstruction;ultrasonic imaging;level set;geometry;tumours;morphological operation;set theory;probes;morphological operators 3d tumors reconstruction laparoscopy conformal geometric algebra stereo endoscopic ultrasound images particle filter hsv space ultrasound probe 3d pose level sets method;stereo endoscopic ultrasound images;ultrasound imaging;medical image;algebra;particle filter;three dimensional displays;image reconstruction;medical image processing;medical imaging;endoscopes;stereo image processing;endoscopic ultrasound;ultrasound probe 3d pose;tumors;ultrasonic imaging three dimensional displays cameras tumors probes histograms level set;conformal geometric algebra medical imaging neurosonography;level sets method;morphological operators;level set method;hsv space;conformal geometric algebra;geometric algebra;biomedical ultrasonics;neurosonography;3d reconstruction;cameras;particle filtering numerical methods	This paper presents a method for 3D reconstruction of tumors for applications in laparoscopy. This uses stereo endoscopic ultrasound images, which are simultaneously recorded. To do this, the ultrasound probe is tracked throughout the stereo endoscopic images using a particle filter and an auxiliary method based on thresholding in the HSV-space is used in order to improve the tracking. Then, the 3D pose of the ultrasound probe is calculated using conformal geometric algebra. The 2D ultrasound images have been segmented using two methods: the level sets method and morphological operators, and a comparison between their performances has been done. Finally, the processed ultrasound images are compounded into a 3D volume, using the calculated ultrasound pose.	3d reconstruction;conformal geometric algebra;mathematical morphology;particle filter;performance;thresholding (image processing)	Ruben Machucho-Cadena;Eduardo Bayro-Corrochano	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.620	3d reconstruction;iterative reconstruction;geometric algebra;medical imaging;computer vision;particle filter;level set;ultrasound;histogram;mathematics;conformal geometric algebra;level set method;set theory	Vision	45.818705973657934	-75.91628581410464	80707
012e2d748e7c9123e024e2101835add78889762b	a novel edge-based criterion for determining the number of thresholds for multilevel image thresholding		In order to determine the appropriate number of thresholds that a grey-level image should be thresholded by so that the resulting image preserves as much information as possible from the original image using the least possible bits, we propose in this paper, a novel criterion for multilevel image thresholding. The criterion is a weighted sum of within-class variance and the number of edge pixels in the thresholded image. To determine the appropriate number of thresholds, an image has to be thresholded iteratively with increasing number of thresholds by any standard thresholding method and the solution that minimizes the proposed criterion is chosen as the appropriate solution. We also present an efficient technique to compute the number of edge pixels. Experiments on a variety of real-world images show that the proposed criterion gives visually more consistent results compared to the most widely used automatic thresholding criterion proposed by Yen et al. (1995).© (2012) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	thresholding (image processing)	Nadim Jahangir;Chin-Kuan Ho;Junaidi Abdullah	2012		10.1117/12.976299	mathematical optimization;discrete mathematics;mathematics;thresholding;statistics	Vision	45.69967769562851	-67.58359765398296	80844
387a4676e4ec30d5702f89862b1f3ff09699b0a0	mixtures of gaussians on tensor fields for dt-mri segmentation	gaussian mixture;white matter;magnetic resonance image;corpus callosum;medical image processing;mixture of gaussians;diffusion tensor	In this paper, an original approach for the segmentation of tensor fields is proposed. Based on the modeling of the data by means of Gaussian mixtures directly in the tensor domain, this technique presents a wide range of applications in medical image processing, particularly for Diffusion Tensor Magnetic Resonance Imaging (DT-MRI). The performance of the segmentation method proposed is shown through the segmentation of the corpus callosum from a dataset of 32 DT-MRI volumes. Comparison with a recent and related segmentation approach is favorable to our method, showing its capability for the automatic extraction of anatomical structures in the white matter.	anatomic structures;body of uterus;corpus callosum;image processing;medical image;medical imaging;mixture model;normal statistical distribution;resonance;shape analysis (digital geometry);silo (dataset);white matter;biologic segmentation;digital tomosynthesis	Rodrigo de Luis García;Carlos Alberola-López	2007	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-540-75757-3_39	diffusion mri;computer vision;speech recognition;radiology;medicine;magnetic resonance imaging;pattern recognition;mixture model;mathematics;image segmentation;scale-space segmentation	Vision	44.441922432688486	-78.23142586866895	80942
a35cb104301e002f6692529ea344c35cbb99f760	extraction of tongue contour in x-ray videos	tongue continuous motion analysis;biology computing;x ray imaging;video signal processing;x ray imaging biological techniques biology computing biomechanics edge detection feature extraction speech splines mathematics video signal processing;edge detection;high ratio outliers;biomechanics;speech;x ray high noise image;ransac;tongue x ray imaging videos speech production image edge detection splines mathematics;splines mathematics;ransac x ray detection maxlikehood boundary estimation outliers rejection;ransac approximation;image edge detection;tongue contour extraction;automatic contour extraction method;feature extraction;b spline approximation;region gradient based edge detector;tongue;production;tongue continuous motion analysis tongue contour extraction x ray videos speech production phenomena automatic contour extraction method region gradient based edge detector initial boundary points x ray high noise image high ratio outliers cluster based point to point distance ratio filter ransac approximation b spline approximation;maxlikehood boundary estimation;speech production phenomena;biological techniques;x ray videos;x ray detection;outliers rejection;initial boundary points;cluster based point to point distance ratio filter;videos	In spite of the development of new image techniques, X-ray remains an important technique in studying speech production phenomena. In this study, we propose an automatic contour extraction method of tongue in X-ray videos. At first, we take a region gradient based edge-detector to find the initial boundary points. As X-ray is high noise image and tongue is frequently occluded by teeth, there are high ratio outliers in the initial boundary point set. To solve this problem, we propose a cluster based point-to-point distance ratio filter to remove the outliers, which greatly reduces the iteration times of later RANSAC and B-Spline approximation for the final boundary points. Our method is nearly full-automatic, and obtains tongue's accurate contour. The experiments show that the proposed method could be an effective tool for tongue's continuous motion analysis.	approximation;b-spline;edge detection;experiment;gradient;iteration;point-to-point protocol;random sample consensus;x-ray (amazon kindle)	Minghao Yang;Jianhua Tao;Dawei Zhang	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637819	computer vision;ransac;speech recognition;edge detection;feature extraction;computer science;speech;biomechanics;machine learning;pattern recognition;mathematics	Robotics	45.53767945868723	-71.87523977611049	81028
56c7d73584acb9da66ea5440056c14f735352bfa	retinal vessel extraction by combining radial symmetry transform and iterated graph cuts	stare database retinal vessel extraction iterated graph cut blood vessels hessian based multiscale filtering method gray retinal image radial symmetry transformation line kernels vessel structure nonvessel structure iterated segmentation algorithm drive database;medical image processing blood vessels eye image segmentation;eye;image segmentation;biomedical imaging;blood vessel;graph cut;medical image processing;transforms;biomedical imaging image segmentation blood vessels transforms retinal vessels pathology;radial symmetry;retinal vessels;retinal imaging;pathology;blood vessels;algorithms fundus oculi humans retinal vessels	In this paper, we propose a new method for the extraction of blood vessels in retinal images. This approach starts with a Hessian-based multiscale filtering method to enhance blood vessels in gray retinal images. Subsequently, a new radial symmetry transformation, which is based on line kernels, is proposed to improve the detection of vessel structures and restrain the response of nonvessel structures. Finally, an iterated segmentation algorithm is used to extract retinal vessels. The proposed approach has been tested on the two publicly available databases, DRIVE and STARE. The experimental results show the feasibility of the proposed method.	blood vessel tissue;cut (graph theory);database;graph - visual representation;hessian;incised wound;iteration;maximum flow problem;minimum cut;optic disk;parallel computing;published database;radial (radio);structure of blood vessel of retina;algorithm;biologic segmentation	Dehui Xiang;Jie Tian;Kexin Deng;Xing Zhang;Fei Yang;Xiaonan Wan	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090981	medical imaging;computer vision;radiology;cut;medicine;symmetry in biology;computer science;mathematics;image segmentation;optics;engineering drawing	Vision	40.557008663714704	-75.61084730076438	81613
15a8ebeb381e36e948499c33c48a7d38c0d33b75	cortical shape analysis in the laplace-beltrami feature space	sensitivity and specificity;imaging three dimensional;shape analysis;feature space;image enhancement;image interpretation computer assisted;detection algorithm;reproducibility of results;cerebral cortex;artificial intelligence;algorithms;pattern recognition automated;humans;laplace beltrami operator	For the automated analysis of cortical morphometry, it is critical to develop robust descriptions of the position of anatomical structures on the convoluted cortex. Using the eigenfunction of the Laplace-Beltrami operator, we propose in this paper a novel feature space to characterize the cortical geometry. Derived from intrinsic geometry, this feature space is invariant to scale and pose variations, anatomically meaningful, and robust across population. A learning-based sulci detection algorithm is developed in this feature space to demonstrate its application in cortical shape analysis. Automated sulci detection results with 10 training and 15 testing surfaces are presented.	anatomic structures;cerebral cortex;description;feature vector;lattice boltzmann methods;lb substance;morphometric analysis;morphometrics;shape analysis (digital geometry);algorithm	Yonggang Shi;Ivo D. Dinov;Arthur W. Toga	2009	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-04271-3_26	computer vision;feature vector;computer science;artificial intelligence;machine learning;pattern recognition;shape analysis;mathematics;laplace–beltrami operator;feature	Vision	42.47519021783318	-77.93250791758726	81796
eec9528562f53d850be50a916f7e8af4f13a0020	a robust kalman filter design for image restoration	filtering theory deconvolution parameter estimation kalman filters image restoration awgn stochastic processes;awgn robust kalman filter design image restoration blind image deconvolution image model blur model identification stage estimated models image noise additive white gaussian noise;additive white gaussian noise;image restoration deconvolution noise robustness gaussian noise iterative algorithms filtering algorithms degradation kalman filters pixel testing;kalman filters;kalman filter;image restoration;awgn;filter design;stochastic processes;deconvolution;parameter estimation;filtering theory;robust design	In image deconvolution or restoration using Kalman filter, the image and blur models are required to be known for the restoration process. Generally, the accuracy of the restoration depends on the accuracy of the given models. Unfortunately, the image and blur models are normally unknown in practice. To solve the problem, an identification stage is employed to estimate the image and blur models. However, the estimated models are seldom accurate especially with the presence of noise in the image. This paper presents a robust Kalman filter design for image deconvolution that can accommodate the inaccuracy in the estimated image and blur models. If the inaccuracy can be modelled as addictive white Gaussian noise with a known variance, it can be stochastically account for in the robust filter design. In the simulation tests performed, the robust design achieved improved accuracy in the image restoration even though inaccurate image and blur models were used.	box blur;circuit restoration;deconvolution;filter design;gaussian blur;image restoration;kalman filter;simulation	Yew Kun Chee;Yeng Chai Soh	2001		10.1109/ICASSP.2001.941297	kalman filter;image restoration;stochastic process;additive white gaussian noise;computer vision;ensemble kalman filter;computer science;gaussian blur;control theory;mathematics;extended kalman filter;moving horizon estimation;statistics	Vision	52.397799307324846	-68.26703477626731	82301
a731c65196cea42fc1c06ae2660daf63385ab216	an enhancement in automatic seed selection in breast cancer ultrasound images using texture features	active contour;segmentation and automatic seed selection;otsu method;k means algorithm	Automatic seed selection is an important and crucial step toward the boundary detection in ultrasound B-scan images. This paper focuses on a methodological framework that can automatically detect a seed point of an ultrasound image by using texture features. Based on the selected seeds of cluster the ultrasound images are segmented using active contour, K-means and Otsu methods. The comparative analysis of these segmentation techniques is also reported. The proposed method is applied on 116 ultrasound images in which 45 are benign cases and 71 malignant cases. The quantitative experimental results show that the proposed method can successfully find an accurate seed point based on texture features and it has the ability to segment the image with high accuracy of 89.65 %. The proposed method is faster and performs more accurate segmentation than existing algorithms.	active contour model;algorithm;ct scan;contour line;energy minimization;fuzzy logic;k-means clustering;otsu's method;pixel;qualitative comparative analysis;random seed;sensor	Lipismita Panigrahi;Keshri Verma;Bikesh Kumar Singh	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732191	computer vision;computer science;engineering;machine learning;otsu's method;pattern recognition;active contour model;k-means clustering	Vision	41.86159702597666	-72.94033682000071	82599
3a171da0871eb1e5d1e622be9b5dc0ce29c7f08d	improved fine structure modeling via guided stochastic clique formation in fully connected conditional random fields	fully connected stochastic cliques sfcrf guided stochastic cliques crf segmentation;statistical distributions image segmentation markov processes random processes;image segmentation probability distribution silicon image color analysis standards markov processes;statistical similarities fine structure modeling stochastic clique formation in fully connected conditional random fields markov random fields image modeling image segmentation local mrf local nodal interactions boundaries smoothness short boundary bias problem long range nodal interactions computational complexity segmentation accuracy fine structure boundaries preservation g sfcrf statistical distributions probability distributions	Markov random fields (MRFs) and conditional random fields (CRFs) are influential tools in image modeling, particularly for applications such as image segmentation. Local MRFs and CRFs utilize local nodal interactions when modeling, leading to excessive smoothness on boundaries (i.e., the short-boundary bias problem). Recently, the concept of fully connected conditional random fields with stochastic cliques (SFCRF) was proposed to enable long-range nodal interactions while addressing the computational complexity associated with fully connected random fields. While SFCRF was shown to provide significant improvements in segmentation accuracy, there were still limitations with the preservation of fine structure boundaries. To address these limitations, we propose a new approach to stochastic clique formation for fully connected random fields (G-SFCRF) that is guided by the structural characteristics of different nodes within the random field. In particular, fine structures surrounding a node are modeled statistically by probability distributions, and stochastic cliques are formed by considering the statistical similarities between nodes within the random fields. Experimental results show that G-SFCRF outperforms existing fully connected CRF frameworks, SFCRF, and the principled deep random field framework for image segmentation.	computational complexity theory;conditional random field;image segmentation;interaction;markov chain;markov random field;stochastic process	Mohammad Javad Shafiee;Audrey G. Chung;Alexander Wong;Paul W. Fieguth	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351406	combinatorics;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation;variable-order markov model	Vision	50.67181319885397	-72.33963020463815	82683
c69347ea5010cbd1ddeda4540b421849903b8a05	automated 3d-stent localization from intravascular ultrasound image sequences	image features;espace intravasculaire;3d visualizations;stent;analyse surface;3d visualization;automatic system;localization;projection method;hombre;localizacion;segmentation;facteur geometrique;surface reconstruction;reconstruction image;in stent restenosis;localisation;exploration ultrason;sistema automatico;methode projection;reconstruccion imagen;geometrical factor;image reconstruction;analisis superficie;image sequence;metodo proyeccion;human;systeme automatique;cross sectional area;algorithms;cross section;exploracion ultrasonido;intravascular space;local search;intravascular ultrasound;in vivo;segmentacion;espacio intravascular;in vitro;sonography;homme;surface analysis;factor geometrico	This paper presents a new method for automated detection of stents. The method consists of three sequential steps. At first, a 3D to 2D projection procedure is applied to find the global stent location. A local search strategy was designed, using a combination of characteristic image features to extract the sampling point candidates of the stent in each of the original cross-sectional images. Finally, the preselected points are accepted or rejected depending on a set of a priori criteria for position and shape. Using the resulting stent points, geometrical parameters of the stent are automatically calculated and a wire frame model is generated for 3D surface reconstruction. Thus, in combination with our algorithms for automated detection of the lumen cross-sectional area, the new method is an essential component for 3D visualization of stents and the automated quantification of the degree of in- stent restenosis. The evaluation is based on in vitro and in vivo recordings. The results show that the new algorithm is well-suited to replace time consuming manual segmentation and measurements.© (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Michael Schmauder;Steffen Zeiler;C. M. Gross;Juergen Waigand;Reinhold Orglmeister	2000		10.1117/12.387656	computer vision;engineering;biological engineering;engineering drawing	Vision	45.84415310811118	-79.18088500779797	82707
d20a204174c7a9148798d48a74dc2941a4728ca9	graph consistency checking: a tool to check the semantic consistency of a segmentation	arc consistency analysis;segmentation;adaptive pyramid;semantic graph	This paper describes an automatic process to check the semantic consistency of a segmentation. This process is made possible through the formalism of graphs. In this article we propose to apply this process to the checking of the relevancy of merging criteria used in an adaptive pyramid by matching the obtained segmentation with a conceptual graph describing the objects under consideration. This matching is performed by using a discrete relaxation method. It checks the arc-consistency with bilevel constraints of the chosen semantic graph. The efficiency of this approach is illustrated on synthetic and real images.	conceptual graph;graph (discrete mathematics);linear programming relaxation;local consistency;relaxation (iterative method);relevance;semantics (computer science);synthetic intelligence	Aline Deruyver;Yann Hodé;Jean-Michel Jolion	2011	Int. J. Semantic Computing	10.1142/S1793351X11001213	theoretical computer science;pattern recognition;data mining;database;scale-space segmentation;segmentation;graph database	AI	45.220013852473826	-69.40471400517016	82955
6b75f9899795d4c691a01e9d057736feda061f9b	spinal cord grey matter segmentation challenge	challenge;spinal cord;segmentation;mri;grey matter;evaluation metrics	An important image processing step in spinal cord magnetic resonance imaging is the ability to reliably and accurately segment grey and white matter for tissue specific analysis. There are several semi- or fully-automated segmentation methods for cervical cord cross-sectional area measurement with an excellent performance close or equal to the manual segmentation. However, grey matter segmentation is still challenging due to small cross-sectional size and shape, and active research is being conducted by several groups around the world in this field. Therefore a grey matter spinal cord segmentation challenge was organised to test different capabilities of various methods using the same multi-centre and multi-vendor dataset acquired with distinct 3D gradient-echo sequences. This challenge aimed to characterize the state-of-the-art in the field as well as identifying new opportunities for future improvements. Six different spinal cord grey matter segmentation methods developed independently by various research groups across the world and their performance were compared to manual segmentation outcomes, the present gold-standard. All algorithms provided good overall results for detecting the grey matter butterfly, albeit with variable performance in certain quality-of-segmentation metrics. The data have been made publicly available and the challenge web site remains open to new submissions. No modifications were introduced to any of the presented methods as a result of this challenge for the purposes of this publication.	cervical segment of spinal cord;cross-sectional data;gradient;gray matter;image processing;magnetic resonance imaging;numerous;sensor;silo (dataset);web site;white matter;algorithm;biologic segmentation	Ferran Prados;John Ashburner;Claudia Blaiotta;Tom Brosch;Julio Carballido-Gamio;Manuel Jorge Cardoso;Benjamin N. Conrad;Esha Datta;Gergely Dávid;Benjamin De Leener;Sara M. Dupont;Patrick Freund;Claudia A. M. Gandini Wheeler-Kingshott;Francesco Grussu;Roland G. Henry;Bennett A. Landman	2017		10.1016/j.neuroimage.2017.03.010	neuroscience;radiology;pathology;magnetic resonance imaging;segmentation;anatomy	Vision	41.36035550134714	-79.95525445572854	82961
f5397b64c2fc1344dee90cf21f501e90530805ea	a practical salient region feature based 3d multi-modality registration method for medical images	multi modality;hybrid registration;change detection;rigid body;robust estimator;3d imaging;saliency;search space;clinical evaluation;rotation invariance;medical image;non rigid registration;single photon emission computed tomography;medical application;region features;region growing	We present a novel representation of 3D salient region features and its integration into a hybrid rigid-body registration framework. We adopt scale, translation and rotation invariance properties of those intrinsic 3D features to estimate a transform between underlying monoor multi-modal 3D medical images. Our method combines advantageous aspects of both featureand intensity-based approaches and consists of three steps: an automatic extraction of a set of 3D salient region features on each image, a robust estimation of correspondences and their sub-pixel accurate refinement with outliers elimination. We propose a region-growing based approach for the extraction of 3D salient region features, a solution to the problem of feature clustering and a reduction of the correspondence search space complexity. Results of the developed algorithm are presented for both monoand multi-modal intra-patient 3D image pairs (CT, PET and SPECT) that have been acquired for change detection, tumor localization, and time based intra-person studies. The accuracy of the method is clinically evaluated by a medical expert with an approach that measures the distance between a set of selected corresponding points consisting of both anatomical and functional structures or lesion sites. This demonstrates the robustness of the proposed method to image overlap, missing information and artefacts. We conclude by discussing potential medical applications and possibilities for integration into a non-rigid registration framework.	algorithm;cluster analysis;dspace;modal logic;modality (human–computer interaction);pixel;refinement (computing);region growing	Dieter A. Hahn;Gabriele Wolz;Yiyong Sun;Joachim Hornegger;Frank Sauer;Torsten Kuwert;Chenyang Xu	2006		10.1117/12.653071	computer vision;geography;machine learning;pattern recognition	Vision	44.26326819889199	-74.43842141016671	83099
1eb725c3dc62060c42f22f2519bb284f2323e411	motion-compensated dynamic image reconstruction for gated cardiac spect	myocardium;gated cardiac perfusion imaging;time varying;dynamic expectation maximization;phantoms;motion compensation;cardiac cycle;cardiology;image sequence analysis;reconstruction algorithms;spatial smoothness;biomedical imaging;motion estimation;tc99m teboroxime dynamics;motion compensated;spatial smoothness motion compensated dynamic image reconstruction gated cardiac spect cardiac cycle dynamic expectation maximization motion compensated temporal regularization image sequence cardiac motion time varying tracer distribution gated cardiac perfusion imaging mathematical cardiac torso phantom tc99m teboroxime dynamics;imaging phantoms;electrocardiography;image reconstruction myocardium electrocardiography motion estimation reconstruction algorithms biomedical engineering biomedical imaging nuclear medicine image sequences imaging phantoms;biomedical engineering;expectation maximization;image reconstruction;medical image processing;image sequence;single photon emission computed tomography;biomedical image processing;cardiography;gated cardiac spect;nuclear medicine;blood flow;time varying tracer distribution;mathematical cardiac torso phantom;motion compensated dynamic image reconstruction;motion compensated temporal regularization;cardiac motion;haemorheology;single photon emission computed tomography cardiology expectation maximisation algorithm haemorheology image reconstruction image sequences medical image processing motion compensation phantoms;gated spect;image sequences;expectation maximisation algorithm	In this paper we propose a unified image reconstruction procedure for dynamic images from traditional gated SPECT acquisition. We divide the cardiac cycle into a number of gate intervals as in gated SPECT, but treat the tracer distribution for each gate as a time-varying signal. By using dynamic expectation maximization and motion-compensated temporal regularization, our reconstruction procedure can produce an image sequence that shows both cardiac motion and time-varying tracer distribution simultaneously. We simulated gated cardiac perfusion imaging using the gated mathematical cardiac-torso (gMCAT) phantom with Tc99m-Teboroxime dynamics. Our experimental results show that the proposed methods can produce more accurate reconstruction of gated dynamic images than independent reconstruction of individual gate frames with spatial smoothness alone	expectation–maximization algorithm;iterative reconstruction;matrix regularization;perfusion scanning;phantom reference	Mingwu Jin;Yongyi Yang;Miles N. Wernick;Michael A. King	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1624904	iterative reconstruction;medical imaging;computer vision;radiology;cardiac cycle;medicine;expectation–maximization algorithm;blood flow;motion estimation;mathematics;nuclear medicine;motion compensation;medical physics	Vision	47.68647145531059	-80.06737593016487	83107
da8e3fe64b881cd619f563f86e2794228e784930	piecewise linear patch reconstruction for segmentation and description of non-smooth image structures		In this paper, we propose a unified energy minimization model for the segmentation of non-smooth image structures. The energy of piecewise linear patch reco nstruction is considered as an objective measure of the quality of the segmentation of non-smooth str uctu es. The segmentation is achieved by minimizing the single energy without any separate process o f feature extraction. We also prove that the error of segmentation is bounded by the proposed energy f unctional, meaning that minimizing the proposed energy leads to reducing the error of segmentation . As a by-product, our method produces a dictionary of optimized orthonormal descriptors for each segmented region. The unique feature of our method is that it achieves the simultaneous segmentatio nd description for non-smooth image structures under the same optimization framework. The expe riments validate our theoretical claims and show the clear superior performance of our methods over othe r related methods for segmentation of various image textures. We show that our model can be coupled with the piecewise smooth model to handle both smooth and non-smooth structures, and we demons trate that the proposed model is capable of coping with multiple different regions through the one-a gainst-all strategy. Index Terms Object segmentation, Mumford-Shah model, Active contour, Eigen-patch, piecewise linear patch reconstruction, error bound of segmentation. Junyan Wang and Kap Luk Chan are with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798 e-mail: {wa0009an,eklcan }@ ntu.edu.sg	active contour model;chan's algorithm;dictionary;eigen (c++ library);electronic engineering;email;energy minimization;feature extraction;mathematical optimization;piecewise linear continuation	Junyan Wang;Kap Luk Chan	2012	CoRR			Vision	52.275509863521634	-71.27709589618752	83298
e6d7a8bbdc3bcdd66983551f2f282fb15e8c44d0	ellipse detection on embryo image using modification of arc particle swarm optimization (arcpso) based arc segment	image segmentation;standards;microscopy;image edge detection;particle swarm optimization;transforms;embryo	In Vitro Fertilization (IVF) technology is used to help couple that have problem in their reproduction organs. However, the success rate of IVF is just 30%, so it is a challenging task to increase that success rate. In this paper, we proposed an ellipse detection method on single and multiple embryo image by modifying the ArcPSO method on ellipse fitting process and the process on extract the arc segment for ellipse detection. The experiment results on single embryo data showed that the proposed method has a 24% better on F-Measure, 21% better on precision, and 28% better on recall as compared to the original ArcPSO method. While the experiment results on multi embryo data showed that the proposed method has a 7% better on F-Measure, 11% better on Precision, and 2% better on Recall as compared to the original ArcPSO method.	baseline (configuration management);canny edge detector;curve fitting;edge detection;f1 score;particle swarm optimization	Arie Rachmad Syulistyo;Hanif Arief Wisesa;Aprinaldi;Anom Bowolaksono;Budi Wiweko;Wisnu Jatmiko	2015	2015 International Symposium on Micro-NanoMechatronics and Human Science (MHS)	10.1109/MHS.2015.7438307	computer vision;engineering;artificial intelligence;engineering drawing	Vision	39.782552299793124	-68.99505641792325	83406
b46fe753c1ae285742428c3803fbbd390581693e	precise lumen segmentation in coronary computed tomography angiography		Coronary computed tomography angiography (CCTA) allows for non-invasive identification and grading of stenoses by evaluating the degree of narrowing of the blood-filled vessel lumen. Recently, methods have been proposed that simulate coronary blood flow using computational fluid dynamics (CFD) to compute the fractional flow reserve non-invasively. Both grading and CFD rely on a precise segmentation of the vessel lumen from CCTA. We propose a novel, model-guided segmentation approach based on a Markov random field formulation with convex priors which assures the preservation of the tubular structure of the coronary lumen. Allowing for various robust smoothness terms, the approach yields very accurate lumen segmentations even in the presence of calcified and non-calcified plaques. Evaluations on the public Rotterdam segmentation challenge demonstrate the robustness and accuracy of our method: on standardized tests with multi-vendor CCTA from 30 symptomatic patients, we achieve superior accuracies as compared to both state-of-the-art methods and medical experts.	ct scan;computational fluid dynamics;computed tomography angiography;markov chain;markov random field;robustness (computer science);simulation	Felix Lugauer;Yefeng Zheng;Joachim Hornegger;B. Michael Kelm	2014		10.1007/978-3-319-13972-2_13	radiology	Vision	40.15485445994668	-79.51074694702828	83434
3e8d428c77290af0de33d568fb851a2496415357	joint 6d k-q space compressed sensing for accelerated high angular resolution diffusion mri		High Angular Resolution Diffusion Imaging (HARDI) avoids the Gaussian. diffusion assumption that is inherent in Diffusion Tensor Imaging (DTI), and is capable of characterizing complex white matter micro-structure with greater precision. However, HARDI methods such as Diffusion Spectrum Imaging (DSI) typically require significantly more signal measurements than DTI, resulting in prohibitively long scanning times. One of the goals in HARDI research is therefore to improve estimation of quantities such as the Ensemble Average Propagator (EAP) and the Orientation Distribution Function (ODF) with a limited number of diffusion-weighted measurements. A popular approach to this problem, Compressed Sensing (CS), affords highly accurate signal reconstruction using significantly fewer (sub-Nyquist) data points than required traditionally. Existing approaches to CS diffusion MRI (CS-dMRI) mainly focus on applying CS in the q-space of diffusion signal measurements and fail to take into consideration information redundancy in the k-space. In this paper, we propose a framework, called 6-Dimensional Compressed Sensing diffusion MRI (6D-CS-dMRI), for reconstruction of the diffusion signal and the EAP from data sub-sampled in both 3D k-space and 3D q-space. To our knowledge, 6D-CS-dMRI is the first work that applies compressed sensing in the full 6D k-q space and reconstructs the diffusion signal in the full continuous q-space and the EAP in continuous displacement space. Experimental results on synthetic and real data demonstrate that, compared with full DSI sampling in k-q space, 6D-CS-dMRI yields excellent diffusion signal and EAP reconstruction with low root-mean-square error (RMSE) using 11 times less samples (3-fold reduction in k-space and 3.7-fold reduction in q-space).	cs-blast;compressed sensing;data point;diffusion spectrum imaging;diffusion tensor imaging;diffusion weighted imaging;displacement mapping;experiment;imaging techniques;normal statistical distribution;plant roots;propagator;psychologic displacement;quantity;radiology;redundancy (information theory);sampling (signal processing);sampling - surgical action;signal reconstruction;synthetic data;synthetic intelligence;united states national institutes of health;white matter	Jian Cheng;Dinggang Shen;Peter J. Basser;Pew-Thian Yap	2015	Information processing in medical imaging : proceedings of the ... conference	10.1007/978-3-319-19992-4_62	mathematical analysis;compressed sensing;data point;angular resolution;gaussian;distribution function;diffusion mri;physics;signal reconstruction;mean squared error	ML	49.699418840673395	-79.38279849674889	83479
eca790a7f03ebdeaea7eee9d2d3b95a4e38cc16f	fingerprint enhancement using circular gabor filter	gabor filter	  Fingerprint minutiae are prevalently used in AFISs. The extraction of fingerprint minutiae is heavily affected by the quality  of fingerprint images. This leads to the incorporation of a fingerprint enhancement module in AFIS to make the system robust  with respect to the quality of input fingerprint images. Most of existing enhancement methods suffer from mainly two kinds  of defects: (1) time consuming and thus unusable in time critical applications; and (2) blocky effect in the enhanced image.  This paper follows Hong’s Gabor filter based enhancement scheme (IEEE Trans. PAMI, vol.20, no.8, pp. 777-789, 1998) but uses  a circle support filter and tunes the filter’s frequency and size differently. This scheme can enhance the fingerprint image  rapidly and overcome the blocky effect effectively and does improve the performance of minutiae detection.    	fingerprint;gabor filter	En Zhu;Jianping Yin;Guomin Zhang	2004		10.1007/978-3-540-30126-4_91	computer science	Vision	39.293089957474464	-66.23503960203165	84052
79e02e82af2f0fd5662d98383cb2d402cd316884	colour retinal image enhancement based on domain knowledge	image sampling;retina image enhancement lighting image segmentation color diseases cameras surface topography automatic control histograms;histograms;eye;colour retinal image enhancement;enhancement;medical image processing eye image colour analysis image enhancement image sampling;nonuniform sampling;nonuniform sampling colour retinal image enhancement domain knowledge;domain knowledge;image enhancement;diagnosis retinal image enhancement colour normalisation;colour normalisation;image color analysis;image colour analysis;retina;retinal image;medical image processing;pixel;imaging;lighting;diagnosis;retinal imaging	Retinal images are widely used to manually or automatically detect and diagnose many diseases. Due to the complex imaging setup, there is a large luminosity and contrast variability within and across images. Here, we use the knowledge of the imaging geometry and propose an enhancement method for colour retinal images, with a focus on contrast improvement with no introduction of artifacts. The method uses non-uniform sampling to estimate the degradation and derive a correction factor from a single plane. We also propose a scheme for applying the derived correction factor to enhance all the colour planes of a given image. The proposed enhancement method has been tested on a publicly available dataset. Results show marked improvement over existing methods.	elegant degradation;image editing;nonuniform sampling;sampling (signal processing);spatial variability	Gopal Datt Joshi;Jayanthi Sivaswamy	2008	2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing	10.1109/ICVGIP.2008.70	computer vision;computer science;optics;computer graphics (images)	Vision	39.40526376409988	-75.72938924806601	84080
271fe1aa9fd911a16d10987288cf4853bbadd282	fast approximate surface evolution in arbitrary dimension	health research;uk clinical guidelines;biological patents;distance function;interfaces;image segmentation;numerical method;europe pubmed central;evolucion;citation search;level set;proceedings;fast numerical methods;uk phd theses thesis;life sciences;level set methods;floating point;medical image segmentation;uk research reports;level set method;medical journals;europe pmc;biomedical research;bioinformatics;evolution	The level set method is a popular technique used in medical image segmentation; however, the numerics involved make its use cumbersome. This paper proposes an approximate level set scheme that removes much of the computational burden while maintaining accuracy. Abandoning a floating point representation for the signed distance function, we use integral values to represent the signed distance function. For the cases of 2D and 3D, we detail rules governing the evolution and maintenance of these three regions. Arbitrary energies can be implemented in the framework. This scheme has several desirable properties: computations are only performed along the zero level set; the approximate distance function requires only a few simple integer comparisons for maintenance; smoothness regularization involves only a few integer calculations and may be handled apart from the energy itself; the zero level set is represented exactly removing the need for interpolation off the interface; and evolutions proceed on the order of milliseconds per iteration on conventional uniprocessor workstations. To highlight its accuracy, flexibility and speed, we demonstrate the technique on intensity-based segmentations under various statistical metrics. Results for 3D imagery show the technique is fast even for image volumes.	approximation algorithm;arabic numeral 0;computation;energy, physics;float;guided imagery;image segmentation;integer (number);interpolation imputation technique;iteration;manifold regularization;matrix regularization;medical image;rule (guideline);segmentation action;signature;uniprocessor system;workstation	James G. Malcolm;Yogesh Rathi;Anthony J. Yezzi;Allen R. Tannenbaum	2008	Proceedings of SPIE--the International Society for Optical Engineering	10.1117/12.771080	telecommunications;computer science;floating point;level set;theoretical computer science;data mining;evolution;image segmentation;level set method;algorithm	Vision	49.29927495513746	-77.82421101302366	84089
1c92d2700ca29a0ebdc43ea625c9eac63c708e29	a study on particle swarm optimization and artificial bee colony algorithms for multilevel thresholding	artificial bee colony;between class variance;image segmentation;kapur s entropy;multilevel thresholding;particle swarm optimization	Segmentation is a critical task in image processing. Bi-level segmentation involves dividing the whole image into partitions based on a threshold value, whereas multilevel segmentation involves multiple threshold values. A successful segmentation assigns proper threshold values to optimise a criterion such as entropy or between-class variance. High computational cost and inefficiency of an exhaustive search for the optimal thresholds leads to the use of global search heuristics to set the optimal thresholds. An emerging area in global heuristics is swarm-intelligence, which models the collective behaviour of the organisms. In this paper, two successful swarm-intelligence-based global optimisation algorithms, particle swarm optimisation (PSO) and artificial bee colony (ABC), have been employed to find the optimal multilevel thresholds. Kapur's entropy, one of the maximum entropy techniques, and between-class variance have been investigated as fitness functions. Experiments have been performed on test images using various numbers of thresholds. The results were assessed using statistical tools and suggest that Otsu's technique, PSO and ABC show equal performance when the number of thresholds is two, while the ABC algorithm performs better than PSO and Otsu's technique when the number of thresholds is greater than two. Experiments based on Kapur's entropy indicate that the ABC algorithm can be efficiently used in multilevel thresholding. Moreover, segmentation methods are required to have a minimum running time in addition to high performance. Therefore, the CPU times of ABC and PSO have been investigated to check their validity in real-time. The CPU time results show that the algorithms are scalable and that the running times of the algorithms seem to grow at a linear rate as the problem size increases.	artificial bee colony algorithm;mathematical optimization;particle swarm optimization;thresholding (image processing)	Bahriye Akay	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.03.072	mathematical optimization;computer science;artificial intelligence;machine learning;mathematics;image segmentation;particle swarm optimization	EDA	42.78162149859757	-68.7759932682214	84147
5b702a9ba57aa1d782c9d26d874cc93c447647b0	quantifying challenging images of fiber-like structures	probability;problem specific algorithm image quantification fiber like structures general computational statistical technique 2d image quantitative analysis experimental biomedicine scenarios arbitrary segments detector probabilistic outputs fiber density estimation;statistical analysis medical image processing probability;segmentation;medical image quantification;fibre like structures;statistical analysis;medical image processing;fibre like structures medical image quantification segmentation;image segmentation detectors training estimation probabilistic logic retina biomedical imaging	We present a practical, parameter-free, general computational-statistical technique for quantitative analysis of 2D images representing fiber-like structures (vessels, neurons, elongated objects, cell boundaries...), which is a common task in many experimental biomedicine scenarios. Our approach does not require segmentation or tracing of fibers; instead, it relies on a learned detector of intersections between fibers and arbitrary segments. The detector's probabilistic outputs are used to compute an estimate of the density of fibers and of its uncertainty; the latter accounts for several factors, including the intrinsic difficulty of the problem, i.e. the inaccuracy of the detector. After few minutes of training by the user, the procedure performs well in a variety of challenging scenarios, and compares favorably even with problem-specific algorithms.	algorithm;computation	Alessandro Giusti;Jonathan Masci;Paola M. V. Rancoita	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738240	computer vision;computer science;theoretical computer science;machine learning;probability;mathematics;image segmentation;scale-space segmentation;segmentation;statistics	Vision	42.17833612264493	-75.81211038186262	84436
e40de028046290b6d4ae45344ea06ae9f4661600	fast image and video segmentation based on alpha-tree multiscale representation	image segmentation;video signal processing;video signal processing image representation image segmentation;video matting;multiscale representation;alpha tree;image representation;image segmentation vegetation image color analysis sea measurements computational modeling brightness;video matting alpha tree multiscale representation image segmentation;berkeley segmentation dataset video segmentation alpha tree multiscale representation fast image segmentation image representation model quasiflat zones image pixel feature interactive segmentation algorithms	In this paper, we propose to rely on a recent image representation model, namely the alpha-tree, to achieve efficient segmentation of images and videos. The alpha-tree is a multiscale representation of an image, based on its quasi-flat zones. An in-depth study of this tree reveals some interesting features of image pixels and regions. These features are then used in the design of both automatic and interactive segmentation algorithms. Interactivity is achieved thanks to a new and efficient implementation scheme. Experiments on the Berkeley Segmentation Dataset lead to very promising results.	algorithm;genetic algorithm;image segmentation;interactivity;machine learning;map;maximally stable extremal regions;memory segmentation;pixel;programming paradigm;tree structure	François Merciol;Sébastien Lefèvre	2012	2012 Eighth International Conference on Signal Image Technology and Internet Based Systems	10.1109/SITIS.2012.56	image texture;computer vision;pyramid;feature detection;range segmentation;image processing;computer science;morphological gradient;segmentation-based object categorization;pattern recognition;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;computer graphics (images)	Robotics	44.860548597746146	-68.08189518279853	84482
8aa1b8a7b6439b766ecc135b50019ab28fbfeb1c	example based lesion segmentation	lesion segmentation;magnetic resonance imaging;mri;alzheimer s disease;patches;ms;diseases and disorders	Automatic and accurate detection of white matter lesions is a significant step toward understanding the progression of many diseases, like Alzheimer's disease or multiple sclerosis. Multi-modal MR images are often used to segment T2 white matter lesions that can represent regions of demyelination or ischemia. Some automated lesion segmentation methods describe the lesion intensities using generative models, and then classify the lesions with some combination of heuristics and cost minimization. In contrast, we propose a patch-based method, in which lesions are found using examples from an atlas containing multi-modal MR images and corresponding manual delineations of lesions. Patches from subject MR images are matched to patches from the atlas and lesion memberships are found based on patch similarity weights. We experiment on 43 subjects with MS, whose scans show various levels of lesion-load. We demonstrate significant improvement in Dice coefficient and total lesion volume compared to a state of the art model-based lesion segmentation method, indicating more accurate delineation of lesions.	alzheimer's disease;atlases;cervical atlas;coefficient of determination;color gradient;heuristics;magnetic resonance imaging;modal logic;multiple sclerosis;numerous;patch (computing);peripheral demyelination;sørensen–dice coefficient;weight;white matter;biologic segmentation	Snehashis Roy;Qing He;Aaron Carass;Amod Jog;Jennifer L. Cuzzocreo;Daniel S. Reich;Jerry L. Prince;Dzung L. Pham	2014	Proceedings of SPIE--the International Society for Optical Engineering	10.1117/12.2043917	computer vision;magnetic resonance imaging	ML	40.44443737747385	-79.05491155078701	84538
043f6586a4b1ee5460c96b41169eaf6f4d6b4fdc	image segmentation based on grabcut framework integrating multiscale nonlinear structure tensor	escena natural;sistema interactivo;iterative method;analisis contenido;traitement signal;locality preserving projection;processus gauss;metodo adaptativo;evaluation performance;convergence;image segmentation;tensile stress;image texture feature;performance evaluation;image processing;fonction energie;analisis textura;distance measure;gaussian processes;multiscale nonlinear structure tensor msnst;color space;methode echelle multiple;color;implementation;evaluacion prestacion;texture image;color feature;projection method;metodo imagen;procesamiento imagen;adaptive fusion image segmentation grabcut framework integrating multiscale nonlinear structure tensor color feature image texture feature gaussian mixture model conjugate norm locality preserving projections technique;natural images;metodo escala multiple;methode adaptative;layout;satisfiability;texture features;traitement image;method integration;systems engineering and theory;energy function;image texture;computer vision;systeme conversationnel;metodo iterativo;algorithme;algorithm;accuracy;distance measurement;content analysis;texture analysis;gaussian mixture model;algorithms artificial intelligence image enhancement image interpretation computer assisted imaging three dimensional nonlinear dynamics reproducibility of results sensitivity and specificity subtraction technique systems integration;precision;natural scene;locality preserving projections technique;adaptive fusion;methode projection;interactive system;methode iterative;medicion distancia;graph cut;feature extraction;signal processing;grabcut framework integrating multiscale nonlinear structure tensor;adaptive method;image method;metodo proyeccion;segmentation image;scene naturelle;statistics;funcion energia;tensors feature extraction gaussian processes image segmentation;robustness;interactive image segmentation;multiscale method;teoria mezcla;multiscale nonlinear structure tensor msnst adaptive fusion graph cuts interactive image segmentation	In this paper, we propose an interactive color natural image segmentation method. The method integrates color feature with multiscale nonlinear structure tensor texture (MSNST) feature and then uses GrabCut method to obtain the segmentations. The MSNST feature is used to describe the texture feature of an image and integrated into GrabCut framework to overcome the problem of the scale difference of textured images. In addition, we extend the Gaussian Mixture Model (GMM) to MSNST feature and GMM based on MSNST is constructed to describe the energy function so that the texture feature can be suitably integrated into GrabCut framework and fused with the color feature to achieve the more superior image segmentation performance than the original GrabCut method. For easier implementation and more efficient computation, the symmetric KL divergence is chosen to produce the estimates of the tensor statistics instead of the Riemannian structure of the space of tensor. The Conjugate norm was employed using Locality Preserving Projections (LPP) technique as the distance measure in the color space for more discriminating power. An adaptive fusing strategy is presented to effectively adjust the mixing factor so that the color and MSNST texture features are efficiently integrated to achieve more robust segmentation performance. Last, an iteration convergence criterion is proposed to reduce the time of the iteration of GrabCut algorithm dramatically with satisfied segmentation accuracy. Experiments using synthesis texture images and real natural scene images demonstrate the superior performance of our proposed method.	algorithm;color image;color space;computation;convergence (action);estimated;experiment;gabor filter;google map maker;grabcut;image segmentation;iteration;kl-one;kullback–leibler divergence;locality of reference;mathematical optimization;mixture model;nonlinear system;normal statistical distribution;physical object;projection defense mechanism;structure tensor;synthetic intelligence;biologic segmentation	Shoudong Han;Wenbing Tao;Desheng Wang;Xue-Cheng Tai;Xianglin Wu	2009	IEEE Transactions on Image Processing	10.1109/TIP.2009.2025560	computer vision;cut;content analysis;image processing;computer science;grabcut;machine learning;signal processing;pattern recognition;gaussian process;mathematics;accuracy and precision;statistics	Vision	52.51634867122723	-68.95546267517523	84730
eca87e41dedef8cb3076738c103d4380c30601af	a novel approach for edge detection in images based on cellular learning automata	image processing;edge detection;learning automata;cellular learning automata cla;images	Cellular Learning Automata (CLA) has been used in many fields of image processing such as noise elimination, smoothing, retrieval, fractionated and extraction of the content Characteristics of the images. The edge detection in images and methods if edge detection, have a great role in machine vision and cognizance systems. This method uses operands for analyzing images and digital image processing. Many studios here been conducted till now in edge detection algorithms of various conditions. In this study a new method for edge detection in images with the use of CLA is recommended. The proposed method of edge detection in images was tested with different sizes and the results were compared with Sobel edge detector classic method. The result show that the method based on CLA has a desirable performance in edge detection and compares the images with a more uniformity during a minimum period of time. DOI: 10.4018/ijcvip.2012100105 52 International Journal of Computer Vision and Image Processing, 2(4), 51-61, October-December 2012 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. images with little background successful and complete edge detection is too hard (Maini & Aggarwal, 2007). The algorithms which are presented for edge detection in images have two main problems [3, 4]. One is that due to the presence of noise in images, the points which are not edges, are detected as edges and the other one is that the quality of the original image is low that causes the boundary of the objects to be undetectable. In this paper each image cell is considered as a different structure in automata and each learning automata has two state edges or non-edges. The local rules of cellular learning automata in repetitive continues methods are defined with edge and non-edge pixels and noises. In this paper, the new method of edge detection based on CLA is compared with other edge detection methods. The rest of the paper is organized as following: section 2 is a brief review of the related works and section 3 a brief description about CLA is presented. In section4, the process of edge detection based on CLA is presented. Section 5 includes examination and the results of simulation. Finally, section 6 of the paper involves conclusion. 2. LITERATURE REVIEW In an image, the distance between an object and background or between overlapped objects is called edge. Assuming that each image has uniform light intensity, the light intensity of the adjacent objects have different amounts and any variation in light intensity is considered linear filters results in ease of edge detection and decrease in calculations in order to get desired results. In the reference Pithadiya (2009) ̨ edge extraction concepts and edge detection processes in canny algorithm as well as morphological algorithm have been reviewed, the practical results of forenamed algorithms for clarity of edges in aerial photography of different areas of the city have been investigated. Ke et al. (2008) presented the cloud model is used for analyzing the edge information of the images in evolutionary modes of cell automata, Also it is used for finding the relationship between the adjacent pixels. Abin et al. in 2009 proposed a new method which was presented for segmenting the color images by using soft and hard segmenting processes. The process of soft segmentation of images and provides the threshold mode till it reaches the final segmentation and in hard segmentation, CLA analysis are done on input image and pixels of each part of the image. The algorithms presented to edge detection (Michael, Sudeep, Thomas, & Kevin, 1997; Abin, Fotouhi, & Kasaei, 2009; Abin, Fotouhi, & Kasaei, 2008). Are classic methods of edge detection such as Robert, Sobel which are calculates the gradient local maximum in the domain of location. If pixels are placed on the picture borders, their adjacency will be on graylevels. For detecting edges on crossover areas in Laplace conversion domain, the crossing points of second derivative of image function is considered as edges. In Abin, Fotouhi, and Kasaei (2009) and Abin, Fotouhi, and Kasaei (2008) canny method, edge detection and noise elimination from image, is defined by Gaussian function in which points with maximum gradient are delineated and the rest of the points are elimination from probability distribution. Chang et al. (2004) and Mirzai et al. (2011) investigated the edge detection using cellular automata for measuring the information or scaling of image and comparing the scale of previous matrix without people’s intervention, and they investigated the edge detection based on cloud model of cellular automata. In cloud model of cellular automata, the specific information of the edge and edge properties are discussed with using the information resulted from edge detection which is resulted from evolutionary cellular automata. The algorithm of edge detection on images using learning cellular automata is implemented through the use of original information of image and edge after receiving feedback from input data. In the reference Meybodi and Enayatifar (2009), internal image is an evolutionary model International Journal of Computer Vision and Image Processing, 2(4), 51-61, October-December 2012 53 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. of original image, and external image detects the edge with the best quality and creates a background o the original image. Abin et al. (2008) proposed a fuzzy cellular automaton which is used in gray surfaces the images for noise elimination. In this method specific adjacency status is allocated for each pixel. The fuzzy logic recognizes the status of each pixel precisely, and in this recommended method, the edge detection is implemented by various instructions, and edge is detected in noised images. 3. LEARNING CELLULAR AUTOMATA 3.1. Cellular Automata Cellular automata (Koza, 1993; Thomas, 2012; Zhang, Zhong, & Zhao, 2007) was posed by Neuman (Zhang, Zhong, & Zhao, 2007; Neuman, 1966) in the late 1940s and after him it was proposed by mathematician Ulam (Ulam, 1972; Zhang, Zhong, & Zhao, 2007) as a model of producing calculations and simulation of systems in which multiple simple components cooperate for producing more complex patterns. Cellular automata are discrete dynamic systems that the operation is as local relations. In this model space is defined as a network in which each part is called a cell. And it is composed of a limited number of cells. For example two simple components and One-dimensional cellular automata are considered as a liner cell and each of the cells takes either zero or one. The system status in cellular automata is specified with using the total cell status the relationship between adjacencies and environmental rules (Zhang, Zhong, & Zhao, 2007; Cheng-hu, Zhan-li, & Yi-chun, 1999). The environment rules, includes either the simulation of definitive cellular automata rules or probable cellular automata, which is a set of local cellular updating. The updating of all of the cells is done according to the previous status of adjacent cell and not according to the present cell status. The locality means that each cell obtains its new status considering the status of adjacent cells. In cellular automata space and time are discrete. The problems of cellular automata are its inability in specifying the definite form of rules because of presence of noise in some systems and its indefiniteness, which is a hard and impossible work. 3.2. Learning Automata Learning automata (Nooraliei & Altun, 2009; Bwigy & Mybodi, 2010; Thathachar & Sastry, 2002) is machines which have a limited number of probabilities and for each of the learning automata a probability vector is allocated. This vector determines that each operation be implemented with what probability. Each selected operation is estimated by probability environment and the result is given to automata as a positive and negative signal. The next operation automaton chooses the best operation. The best operation is the one in which the probability of receiving reward from the environment is higher. In Figure 1 the function of learning automata in connected with environment in a feedback loop is shown. The environment is in the form of E = { } α β , , c , in which α α α α = ... { , , , } 1 2 r is the set of inputs and β β β β = ... { , , , } 1 2 r is the set of outputs and c c c cr = ... { } 1 2 , , , is the set of unsuccessful probabilities. When β of the environment is considered as a binary set, if the response of the environment equals zero, it means the reward mode, and if not, it is considered as the punishment mode (Thathachar & Sastry, 2002); Dong-Su, Wang-Heon, & InSo, 2004). The learning automaton is derided to two types according to the structure. The learning automata with fixed structure is shown as five mode of α β , , , , f g ∅ { } in which α = ... { , , , } α α α 1 2 r is the set of operations of automata { , , , } β = ... β β β 1 2 r , is the set of input data of automata,∅ = ∅ ∅ ... ∅ { , , , } 1 2 r is the set of internal status of automata of is the product function resulted from the new status of automata and is the output function which announces the automata status to the next out54 International Journal of Computer Vision and Image Processing, 2(4), 51-61, October-December 2012 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. put. The learning automata is shown by fore m o d e o f { , , , } α β p T , i n w h i c h α = ... { , , , } α α α 1 2 r is the set of operations of automata β = ... { , , , } β β β 1 2 r , is the set of input data of automata , { , , , } = ... p p p r 1 2 i s t h e s e l e c t i o n p r o b a b i l i t y v e c t o r, p n T n n p n + ( ) =	aerial photography;algorithm;ant colony;artificial neural network;canny edge detector;cellular automaton;circuit complexity;digital image processing;dynamical system;edge detection;edge dominating set;effective method;feedback;fuzzy logic;gradient;holographic principle;image noise;image scaling;international journal of computer vision;learning automata;locality of reference;machine vision;maxima and minima;models of dna evolution;operand;pixel;sensor;simulation;smoothing;sobel operator;systematic review;víctor neumann-lara;wang tile;zhi-li zhang	Farhad Soleimanian Gharehchopogh;Samira Ebrahimi	2012	IJCVIP	10.4018/ijcvip.2012100105	computer vision;edge detection;computer science;theoretical computer science;machine learning	Vision	43.09802798765033	-67.85812067769594	84755
6799891dc82ef0522d8394a288629535cbda2067	a new hierarchical brain parcellation method based on discrete morse theory for functional mri data	stability criteria;medical image processing biomedical mri brain;spatial resolution stability criteria noise magnetic resonance imaging smoothing methods face;smoothing methods;magnetic resonance imaging;face;hierarchical brain parcellation method persistent homology rs fmri data parcel spatial contiguity brain parcel lation functional brain connectivity haemodynamic fluctuation fast acquisition sequence development resting state functional mri complex network analysis method functional mri data discrete morse theory;discrete morse theory parcellation resting state fmri ultra high resolution;noise;spatial resolution	Parcellation of the brain into functionally meaningful regions is a crucial step in studies of brain connectivity using complex network analysis methods based on resting-state functional MRI (rs-fMRI). With the recent development of fast acquisition sequences at ultra-high-field (7T), high-spatial-resolution rs-fMRI can now be collected from the whole-brain with sufficient temporal resolution to capture the slow haemodynamic fluctuations underlying functional brain connectivity. A method for obtaining individual brain parcel-lations based on rs-fMRI has recently been proposed, which grows a set of stable seeds into an initial detailed parcellation that is further clustered using a hierarchical approach that enforces spatial contiguity of the parcels. Smoothing is performed to remove spurious features before the growing step, which precludes the exploration of the ultra-high spatial resolution of our data. In this paper, we propose an approach for brain parcellation that takes advantage of the topological structure present in the spatial organization exhibited by rs-fMRI data, using methods based on discrete Morse theory and persistent homology. This framework provides a region importance measure derived from local functional homogeneity and a topologically-informed simplification procedure that enables the analysis of high resolution rs-fMRI data at different levels of detail.	complex network;discrete morse theory;hemodynamics;homology (biology);image resolution;level of detail;persistent homology;smoothing;spatial organization	Afonso Dias;Marta Bianciardi;Sandro Nunes;Rodolfo Abreu;Juliana Rodrigues;Luis Miguel Silveira;Lawrence L. Wald;Patricia Figueiredo	2015	2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2015.7164122	face;computer vision;radiology;image resolution;medicine;computer science;noise;artificial intelligence;magnetic resonance imaging;machine learning;mathematics	Vision	48.4734691734997	-78.06450282962498	84831
ab347466021e0991b72be68ab4cddd9b944b1945	detection of 3d filamentous networks from tomographic electron microscopy	eigenvalues and eigenfunctions;microprocessors;image segmentation;tensile stress;tensile stress tomography computer architecture microscopy microprocessors junctions eigenvalues and eigenfunctions;mechanical property;microscopy;junctions;filament segmentation;computer architecture;junctions segmentation 3d filamentous network detection 3d tomographic electron microscopy computational approach chemical staining phenotypic signature polysaccharides plant cell wall hessian filtering signal enhancement tensor voting curve tracking filaments segmentation;electron microscopy;tomography biological techniques cellular biophysics electron microscopy image segmentation;biological techniques;plant cell wall filament segmentation perceptual organization electron microscopy;tomography;cellular biophysics;plant cell wall;perceptual organization	This paper presents a computational approach for detection of filamentous networks in 3D tomographic electron microscopy. Due to the general heterogeneity of chemical staining, imaged signatures may appear punctate and discontinuous. Very often, there is a necessity to characterize organization and phenotypic signatures of stained structures. This is the example of polysaccharides in the plant cell wall, where characterization of its inner structure allows for studies of their mechanical properties and accessibility. Our approach consists of three steps: (i) Hessian filtering for signal enhancement; (ii) tensor voting for detection and completion of organized patterns; and (iii) curve tracking for segmentation of filaments and junctions. We validated our approach by detecting filaments in synthetic images and real plant cell walls imaged by 3D tomographic electron microscopy.	accessibility;antivirus software;electron tomography;hessian;sensor;synthetic intelligence	Leandro A. Loss;Hang Chang;Purbasha Sarkar;Manfred Auer;Bahram Parvin	2012	2012 9th IEEE International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2012.6235826	computer vision;radiology;medicine;computer science;microscopy;nanotechnology;tomography;image segmentation;stress;cell wall;electron microscope	Vision	40.4745415635536	-74.21062619333763	85264
6e6bf20d12cf26301329cd04c0a4529efac6c916	automatic segmentation of vessels in breast mr sequences as a false positive elimination technique for automatic lesion detection and segmentation using the shape tensor	second order;mr;computer aided diagnosis;automatic segmentation;segmentation;breast;image structure;first order;shape;automatic detection;smoothing;motion correction;contrast agent;false positive	We present a new algorithm for automatic detection of bright tubular structures and its performance for automatic segmentation of vessels in breast MR sequences. This problem is interesting because vessels are the main type of false positive structures when automatically detecting lesions as regions that enhance after injection of the contrast agent. Our algorithm is based on the eigenvalues of what we call the shape tensor. It is new in that it does not rely on image derivatives of either first order, like methods based on the eigenvalues of the mean structure tensor, or second order, like methods based on the eigenvalues of the Hessian. It is therefore more precise and less sensitive to noise than those methods. In addition, the smoothing of the output which is inherent to approaches based on the Hessian or structure tensor is avoided. The output of our filter does not present the typical over-smoothed look of the output of the two differential filters that affects both their precision and sensitivity. The scale selection problem appears also less difficult in our approach compared to the differential techniques. Our algorithm is fast, needing only a few seconds per sequence. We present results of testing our method on a large number of motion-corrected breast MR sequences. These results show that our algorithm reliably segments vessels while leaving lesions intact. We also compare our method to the differential techniques and show that it significantly out-performs them both in sensitivity and localization precision and that it is less sensitive to scale selection parameters.© (2006) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Gerardo Hermosillo;Xuguang Jiang	2006		10.1117/12.654010	computer vision;mathematical optimization;machine learning;mathematics;scale-space segmentation	Vision	39.60827336955332	-70.3830972041706	85586
061c5af0f19a41dc3c661621a9a9f7d4eac02cbd	maximum likelihood dosage estimation for bayesian transmission tomography	synthetic phantoms maximum likelihood dosage estimation bayesian transmission tomography reconstruction restoration stochastic image models joint maximum likelihood estimation x ray dosage parameter spl gamma ray dosage parameter estimation algorithm expectation maximization method parameter estimator;bayesian transmission tomography;x ray imaging;maximum likelihood;maximum likelihood dosage estimation;bayes methods;reconstruction;parameter estimator;restoration;synthetic phantoms;bayesian methods;image restoration;dosimetry;maximum likelihood estimation bayesian methods image reconstruction tomography parameter estimation image restoration stochastic processes x ray imaging yield estimation imaging phantoms;yield estimation;maximum likelihood estimation;estimation algorithm;tomographic reconstruction;imaging phantoms;gamma ray applications;stochastic image models;stochastic processes;expectation maximization;γ ray dosage parameter;expectation maximization method;image reconstruction;computerised tomography;x ray dosage parameter;joint maximum likelihood estimation;parameter estimation;image modeling;tomography;nondestructive testing maximum likelihood estimation bayes methods image restoration computerised tomography x ray imaging gamma ray applications dosimetry;nondestructive testing;x rays	Bayesian reconstruction and restoration methods require the choice of parameters related to variance in both stochastic image models and observed data. In practice these parameters, or their ratio, is often chosen heuristically. The authors present a method for joint maximum-likelihood (ML) estimation of these two parameters in transmission tomography, with emphasis on the X-ray//spl gamma/-ray dosage parameter. The estimation algorithm employs the expectation-maximization method, with the unobserved image as the complete data. The ML parameter estimator is shown to yield values which are practical for tomographic reconstruction, both with synthetic phantoms and real data. >	tomography;transmission (bittorrent client)	Ken D. Sauer;Charles A. Bouman	1994		10.1109/ICIP.1994.413474	econometrics;mathematical optimization;mathematics;maximum likelihood;tomography;statistics	Metrics	53.51721573507429	-75.84802671909266	85638
3f0f7ae4b86213141b2264c8dc63b0bc6a74a8ea	ground truth delineation for medical image segmentation based on local consistency and distribution map analysis	distributed boundary pixel level medical image segmentation distribution map analysis computer aided detection systems medical image pattern recognization cad segmentation algorithm systematic ground truth delineation method local consistency set analysis approach systematic ground truth representation;image segmentation computational modeling biomedical imaging measurement robustness computed tomography design automation;medical image processing edge detection image representation image segmentation	Computer-aided detection (CAD) systems are being increasingly deployed for medical applications in recent years with the goal to speed up tedious tasks and improve precision. Among others, segmentation is an important component in CAD systems as a preprocessing step to help recognize patterns in medical images. In order to assess the accuracy of a CAD segmentation algorithm, comparison with ground truth data is necessary. To-date, ground truth delineation relies mainly on contours that are either manually defined by clinical experts or automatically generated by software. In this paper, we propose a systematic ground truth delineation method based on a Local Consistency Set Analysis approach, which can be used to establish an accurate ground truth representation, or if ground truth is available, to assess the accuracy of a CAD generated segmentation algorithm. We validate our computational model using medical data. Experimental results demonstrate the robustness of our approach. In contrast to current methods, our model also provides consistency information at distributed boundary pixel level, and thus is invariant to global compensation error.	algorithm;computation;computational model;computer-aided design;contour line;ground truth;image segmentation;local consistency;map analysis;medical image;pixel;preprocessor;biologic segmentation	L. Irene Cheng;Xinyao Sun;Noura Alsufyani;Zhihui Xiong;Paul Major;Anup Basu	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319041	image texture;computer vision;feature detection;computer science;machine learning;segmentation-based object categorization;pattern recognition;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Robotics	40.053894214899884	-76.85784191203327	85844
890d176bb26087213fe3f144b0921b5d70e01be0	bayesian tracking for blood vessel detection in retinal images	blood vessels retina biomedical imaging image segmentation image edge detection bayes methods bifurcation;image segmentation;bifurcation;bayes methods;biomedical imaging;image edge detection;retina;synthetic retinal images bayesian tracking blood vessel detection retinal images statistical based tracking method statistic sample scheme blood vessel vessel geometric properties grey level profile maximum a posteriori map probability criterion;probability blood vessels eye maximum likelihood estimation medical image processing;blood vessels	A new statistical-based tracking method is proposed for the detection of blood vessels in retinal images. Our algorithm adopts a statistic sample scheme to estimate the candidate edge points of local blood vessel. This sampling scheme combines local grey level profile and the vessel geometric properties, which improves the accuracy and robustness of the tracking process. Edge points of blood vessels are detected iteratively based on a Bayesian approach with the Maximum a posteriori (MAP) Probability criterion. Evaluation of our algorithm is presented on both synthetic and real retinal images.	algorithm;experiment;grayscale;sampling (signal processing);scott continuity;synthetic intelligence	Yi Yin;Mouloud Adel;Mireille Guillaume;Salah Bourennane	2010	2010 18th European Signal Processing Conference		computer vision;pattern recognition;mathematics;statistics	Vision	44.04453680564272	-74.14766138017997	85880
18bc39207b4d24eabf9d98649db53563d9c2e3fd	multiresolution sampling procedure for analysis and synthesis of texture images	probability density;lip sync;facial animation;spatial frequency;structural properties	Approach: The approach presented here uses the results psychophysical models to provide constraints on a statistical sampling procedure. In a two-phase process, the input texture is first analyzed by measuring the joint occurrence, across multiple resolutions, of several of the features used in psychophysical models. In the second phase, a new texture is synthesized by sampling successive spatial frequency bands from the input texture, conditioned on the similar joint occurrence of features at all lower spatial frequencies. By rearranging textural components at locations and resolutions where the discriminability is below threshold, new texture samples are generated which have similar visual characteristics. Several results of this method are shown in Figure 1.	frequency band;sampling (signal processing);two-phase commit protocol	Jeremy S. De Bonet	1997		10.1145/258734.258882	computer vision;probability density function;speech recognition;computer facial animation;computer science;spatial frequency;computer graphics (images)	Vision	47.23221862378098	-67.1656082829222	85983
1515a5360dc3e23d42578cb4a6178c5848e71bf3	texture removal by pixel classification using a rotating filter	corner points image texture removal pixel classification smoothing rotating filter pixel signals homogenous region pixel anisotropic diffusion edge points;detectors;texture;kernel;image segmentation;edge detection;smoothing method;pixel classification;image classification;diffusion processes;anisotropic diffusion smoothing filter rotating filter texture image segmentation;anisotropic diffusion;corner points;image texture;image edge detection pixel anisotropic magnetoresistance smoothing methods detectors kernel diffusion processes;smoothing methods;rotating filter;image edge detection;edge points;pixel;anisotropic magnetoresistance;diffusion process;pixel signals;image texture removal;smoothing rotating filter;homogenous region pixel;smoothing methods edge detection image classification image texture;smoothing filter	In this paper, we present a new method for removing texture in images using a smoothing rotating filter. From this filter, a bank of smoothed images provides pixel signals able to classify a pixel as a texture pixel, a homogenous region pixel or an edge pixel. Then, we introduce a new method for anisotropic diffusion which controls accurately the diffusion near edge and corner points and diffuses isotropically inside textured regions. Several results applied on real images and a comparison with anisotropic diffusion methods show that our model is able to remove the texture and control the diffusion.	anisotropic diffusion;pixel;smoothing	Baptiste Magnier;Philippe Montesinos;Daniel Diep	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946599	magnetoresistance;image texture;computer vision;mathematical optimization;detector;contextual image classification;kernel;edge detection;computer science;diffusion process;mathematics;image segmentation;texture;bilateral filter;anisotropic diffusion;texture filtering;pixel;computer graphics (images)	Vision	48.06402334962387	-67.25677572845201	85991
19fc97db7fcfca26eed393d3c466c15e4396ec64	topomorphologic separation of fused isointensity objects via multiscale opening: separating arteries and veins in 3-d pulmonary ct	morphological operators multiscale topomorphology fused isointensity objects 3 d pulmonary multidetector x ray computed tomography arteries veins fuzzy distance transform topologic fuzzy connectivity 3 d pulmonary ct morphological reconstruction blind seed selection semiautomated separation object specific intensity variation conjoining locations multiscale opening;pulmonary imaging;arteries veins computed tomography cities and towns optical imaging magnetic resonance imaging x ray imaging radiology image resolution optical microscopy;computed tomography;computed tomography ct;x ray computed tomography;morphological operation;scale;vein artery computed tomography ct fuzzy connectivity fuzzy distance transform fdt morphology pulmonary imaging scale vascular tree;algorithms computer simulation contrast media female fourier analysis fuzzy logic humans imaging three dimensional lung models cardiovascular phantoms imaging pulmonary artery pulmonary veins sensitivity and specificity tomography x ray computed young adult;lung;fuzzy logic;vein;vascular tree;large scale;morphology;fuzzy connectivity;fuzzy distance transform fdt;image reconstruction;medical image processing;computerised tomography;medical image processing blood vessels computerised tomography fuzzy logic image reconstruction lung;distance transform;artery;blood vessels	A novel multiscale topomorphologic approach for opening of two isointensity objects fused at different locations and scales is presented and applied to separating arterial and venous trees in 3-D pulmonary multidetector X-ray computed tomography (CT) images. Initialized with seeds, the two isointensity objects (arteries and veins) grow iteratively while maintaining their spatial exclusiveness and eventually form two mutually disjoint objects at convergence. The method is intended to solve the following two fundamental challenges: how to find local size of morphological operators and how to trace continuity of locally separated regions. These challenges are met by combining fuzzy distance transform (FDT), a morphologic feature with a topologic fuzzy connectivity, and a new morphological reconstruction step to iteratively open finer and finer details starting at large scales and progressing toward smaller scales. The method employs efficient user intervention at locations where local morphological separability assumption does not hold due to imaging ambiguities or any other reason. The approach has been validated on mathematically generated tubular objects and applied to clinical pulmonary noncontrast CT data for separating arteries and veins. The tradeoff between accuracy and the required user intervention for the method has been quantitatively examined by comparing with manual outlining. The experimental study, based on a blind seed selection strategy, has demonstrated that above 95% accuracy may be achieved using 25-40 seeds for each of arteries and veins. Our method is very promising for semiautomated separation of arteries and veins in pulmonary CT images even when there is no object-specific intensity variation at conjoining locations.	arterial system;business continuity;convergence (action);diagnostic radiologic examination;distance transform;experiment;linear separability;mathematical morphology;multiscale modeling;physical object;plant seeds;powerflasher fdt;scott continuity;trees (plant);veins;x-ray computed tomography	Punam K. Saha;Zhiyun Gao;Sara K. Alford;Milan Sonka;Eric A. Hoffman	2010	IEEE Transactions on Medical Imaging	10.1109/TMI.2009.2038224	iterative reconstruction;fuzzy logic;computer vision;scale;radiology;morphology;computer science;mathematics;distance transform;vein;medical physics	Vision	41.37989078920865	-77.26817008266212	86062
718ade3ff660472194b055c94991d780e7a30808	a novel hyperspectral images destriping method based on edge reconstruction and adaptive morphological operators	adaptive morphological operator;adaptive morphological operator hyperspectral image destriping edge restoring svm;hsi completion hyperspectral images destriping method edge reconstruction adaptive morphological operators specialized destriping methods remote sensing image automatic completing method boundary information broken region support vector machines svm surrounding detected edges adaptive morphological filling operator;destriping;edge restoring;support vector machines edge detection hyperspectral imaging image reconstruction mathematical morphology remote sensing;svm;image edge detection image restoration remote sensing support vector machines filling algorithm design and analysis satellites;hyperspectral image	In recent years, the specialized destriping methods for remote-sensing image emphasize efficiency and flexibility but lose the particulars in wide stripes. Though many excellent inpainting algorithms have been proposed, most of them are too complex and inefficient for hyperspectral image (HSI). In this paper, we propose a novel automatic completing method which is appropriate for the feature of HSI and easy to be implemented in parallel. The boundary information in broken region is restored primarily based on SVM with the input of the surrounding detected edges. Then we introduce an adaptive morphological filling operator restrained by the boundaries to estimate the pixels in the same position of all bands synchronously. Experimental results show satisfactory performance of the proposed system and its significance on HSI completion.	algorithm;horizontal situation indicator;inpainting;mathematical morphology;pixel;stripes;support vector machine	Yidan Teng;Ye Zhang;Yushi Chen;Chunli Ti	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025596	support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics	Robotics	46.30482927588653	-67.23516129573584	86321
7fade0eb05388da9c8c9681c2b9891588e785278	random walks for feature-preserving mesh denoising	metodo cuadrado menor;iterative method;concepcion asistida;filtering;methode moindre carre;computer aided design;filtrage;mise a jour;algoritmo busqueda;least squares method;weighted averaging;algorithme recherche;speed of convergence;generation maille;filtrado;search algorithm;gradiente;gradient;probabilistic approach;curva gauss;conjugate gradient method;reduccion ruido;convergence speed;metodo iterativo;actualizacion;conjugate gradient;metodo gradiente conjugado;methode iterative;random walk;enfoque probabilista;approche probabiliste;noise reduction;least square;mesh denoising;reduction bruit;feature preservation;methode moyenne;loi normale;conception assistee;velocidad convergencia;preservation;mesh smoothing;rapport signal bruit;relacion senal ruido;marcha aleatoria;methode gradient conjugue;signal to noise ratio;mesh generation;vitesse convergence;preservacion;averaging method;gaussian distribution;marche aleatoire;metodo medio;updating;variance;variancia;qa76 computer software	An approach to mesh denoising based on the concept of random walks is examined. The proposed method consists of two stages: face normal filtering, followed by vertex position updating to integrate the denoised face normals in a least-squares manner. Face normal filtering is performed by weighted averaging of normals in a neighbourhood. A novel approach to determining weights is to compute the probability of arriving at each neighbour following a fixed-length random walk of a virtual particle starting at a given face of the mesh. The probability of the particle stepping from its current face to some neighbouring face is a function of the angle between the two face normals, based on a Gaussian distribution whose variance is adaptively adjusted to enhance the feature-preserving property of the algorithm. The vertex position updating procedure uses the conjugate gradient algorithm for speed of convergence. Analysis and experiments show that random walks of different step lengths yield similar denoising results. Our experiments show that, in fact, iterative application of a one-step random walk in a progressive manner effectively preserves detailed features while denoising the mesh very well. This approach is faster than many other feature-preserving mesh denoising algorithms.	algorithm;analysis of algorithms;bilateral filter;conjugate gradient method;edge detection;expectation propagation;experiment;geometry processing;gradient descent;interactivity;iteration;iterative method;least squares;median filter;noise reduction;radio frequency;rate of convergence;requirement;sparse matrix;stepping level;whole earth 'lectronic link	Xianfang Sun;Paul L. Rosin;Ralph R. Martin;Frank C. Langbein	2008	Computer Aided Geometric Design	10.1016/j.cagd.2007.12.008	computer aided design;calculus;mathematics;geometry;conjugate gradient method;least squares;statistics	Vision	50.114267745730224	-73.48950135517427	86598
f60804c347df06ed3e378b26bb1bca1109c139a0	um método baseado em mineração de grafos para segmentação e contagem de clusters de máximos locais em imagens digitais		Faustino, Geisa Martins; Gattass, Marcelo (Advisor) ; Lucena, Carlos J. P. de (Co-Advisor) . A Graph-mining Based Method for Segmentation and Counting of Local Maximum Clusters in Digital Images. Rio de Janeiro, 2011. 79p. DSc Thesis — Department of Informática, Pontifı́cia Universidade Católica do Rio de Janeiro. A grayscale image can be viewed as a topological surface and this way, objects of interests may appear as peaks (sharp mountains), domes (smooth hills) or valleys (Vor U-shaped). Generally, the dome top presents more than one local maximum. Thus, it can be characterized by a local maximum cluster. Segmenting objects individually in images where they appear partially or totally fused is a problem which frequently may not be solved by a watershed segmentation or a basic morphological processing of images. Other issue is counting similar objects in images segmented beforehand. Counting them manually is a tedious and time-consuming task, and its subjective nature can lead to a wide variation in the results. This work presents a new method for segmenting and counting of local maximum clusters in digital images through a graph-based approach. Using the luminance information, the image is represented by a region adjacency graph and a graph-mining algorithm is applied to segment the clusters. Finally, according to image characteristics, a graph-clustering algorithm can be added to the process to improve the final result. The object counting step is a direct result from the mining algorithm and the clustering algorithm, when the latter is applied. The proposed method is tolerant to variations in object size and shape and can easily be parameterized to handle different image groups resulting from distinct objects. Tests made on a database with 262 images, composed of photographs of objects (group 1) and embryonic stem cells under fluorescence microscopy images (group 2), attest the effectiveness and quality of the proposed method as for segmentation and counting purpose. The images form group 1 processed by our method were checked by the author and those ones from group 2 by the specialists from the Institute of Biomedical Sciences at UFRJ. For these images we obtained an average F-measure of 85.33% and 90.88%, respectively. Finally, a comparative study with the widely used watershed algorithm was done. The watershed achieved an average F-measure of 74.02% e 78.28% for groups 1 and 2, respectively, against 85.33% e 91.60% obtained by our method.		Geisa Martins Faustino	2011				Vision	43.62019263274324	-69.3360950539935	86613
6d7bd49685b1db543927e0450d061fabe6907d61	extracting surfaces from fuzzy 3d-ultrasound data	mathematical morphology;multi resolution analysis;clinical application;volume rendering;low pass;ultrasound imaging;3d ultrasound;morphology;3d model;region of interest;on the fly;multiresolution analysis	Rendering 3D models from 3D-ultrasonic data is a complicated task due to the noisy, fuzzy nature of ultrasound imaging containing a lot of artifacts, speckle etc. In the method presented in this paper we first apply several filtering techniques (low-pass, mathematical morphology, multi-resolution analysis) to separate the areas of low coherency containing mostly noise and speckle from those of useful information. Our novel BLTP filtering can be applied at interactive times on-the-fly under user control & feed-back. Goal of this processing is to create a ’region-of-interest’ (ROI) mask, whereas the data itself remains unaltered. Secondly,we examine several alternatives to the original Levoy contouring method. Finally we introduce an improved surface-extraction volume rendering procedure, applied on the original data within the ROI areas for visualizing high quality images within a few seconds on a normal workstation, or even on a PC, thus making the complete system suitable for routine clinical applications. CR Descriptors: General Terms: Algorithms. I.3.3 [Computer Graphics]: Picture/image generation; I.3.8 [Computer Graphics]: Applications; I.4.3 [Image Processing]: Enhancement, Smoothing, Filtering; I.4.6 [Image Processing]: Segmentation, Edge and Feature Detection, Pixel Classification; J.3 [Life and Medical Sciences]. Additional	3d computer graphics;3d modeling;algorithm;display resolution;feature detection (computer vision);feedback;glossary of computer graphics;image processing;low-pass filter;mathematical morphology;medical ultrasound;multiresolution analysis;pixel;region of interest;smoothing;user interface;volume rendering;workstation	Georgios Sakas;Stefan Walter	1995		10.1145/218380.218504	multiresolution analysis;computer vision;simulation;mathematical morphology;morphology;low-pass filter;computer science;volume rendering;computer graphics (images);region of interest	Graphics	44.84392051548106	-71.64497731126468	86823
d8b2d05de28c473af9584ce744ad661fdd05dd41	segmentation of subcortical structures and the hippocampus in brain mri using graph-cuts and subject-specific a-priori information	silicon;subcortical structures;graph cuts subcortical structures segmentation hippocampus magnetic resonance imaging brain mri multi atlas segmentation subject specific probabilistic atlas markov random field based energy function a priori information;brain;medical image processing biomedical mri brain image segmentation markov processes;image segmentation;structural mr images;mrf;adni;subcortical structures segmentation;training;hippocampus;hippocampus magnetic resonance imaging image segmentation gaussian distribution brain alzheimer s disease computer vision educational institutions neuroimaging magnetic resonance;indexing terms;markov random field;energy function;multi atlas segmentation;brain modeling;mr imaging;image edge detection;magnetic resonance;graph cut;medical image processing;magnetic resonance imaging;mrf structural mr images atlas based segmentation graph cuts subcortical structures;subject specific probabilistic atlas;mri;brain imaging;markov random field based energy function;a priori information;markov processes;probabilistic logic;atlas based segmentation;graph cuts;biomedical mri	We propose a general framework for segmentation of subcortical structures and the hippocampus in magnetic resonance brain images based on multi-atlas label propagation and graph cuts. The label maps obtained from multi-atlas segmentation are used to build a subject-specific probabilistic atlas of a structure of interest. From this atlas and an intensity model estimated from the unseen image, a Markov random field-based energy function is defined and optimized via graph cuts. Compared to a previously proposed approach, our method does not rely on manual training of the intensity model and is applied to five subcortical structures and the hippocampus. We used this approach to segment the hippocampus on 60 images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and achieved an average overlap (Dice coefficient) of 0.86 with the manually delineated reference segmentations.	alzheimer's disease neuroimaging initiative;cut (graph theory);map;markov chain;markov random field;mathematical optimization;resonance;software propagation;sørensen–dice coefficient	Robin Wolz;Paul Aljabar;Rolf A. Heckemann;Alexander Hammers;Daniel Rueckert	2009	2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2009.5193086	computer vision;radiology;cut;medicine;computer science;artificial intelligence;magnetic resonance imaging;machine learning;mathematics	Vision	42.67776965376981	-77.49273135895083	86892
b4fcb6165efb75db373fa51e4846c7072ef40102	automatic bone boundary detection in hand radiographs by using modified level set method and diffusion filter	noise reduction automatic bone boundary detection modified level set method diffusion filter rheumatoid arthritis chronic inflammatory joint disease bone destruction distinctive pattern joint destruction distinctive pattern ra diagnosis hand bone radiograph analysis level set parameters curve evolution process modified speed function image boundary;diffusion filter hand bones radiograph boundary detection modified level set method;set theory;medical image processing;bone;diseases;image denoising;bones level set joints diagnostic radiography image segmentation arthritis;set theory bone diagnostic radiography diseases filtering theory image denoising medical image processing object detection;filtering theory;diagnostic radiography;object detection	RA (Rheumatoid Arthritis) is a chronic inflammatory joint disease characterized by a distinctive pattern of bone and joint destruction. To give an RA diagnosis, a hand radiograph is taken and analyzed. Hand bone radiograph analysis starts with the detection of the boundary of bones. It is, however, an extremely exhausting and time consuming task for radiologists, not only because of the complexity, but also because of the precision required for a correct diagnosis. Automatic bone boundary detection is thus required. The Level Set Method has been widely used in boundary detection. However, the convergence and stability of the level set are strongly affected by the speed function and the parameters of the level set, which often leads to either a complete breakdown or a premature termination of the curve evolution process, resulting in unsatisfactory results. In this paper, we propose a modified speed function of the level set for bone boundary detection in hand radiographs. And in order to preserve the boundary of an image and to reduce noise, we further apply diffusion filter to substitute Gaussian Filter in the standard Level Set Method. Evaluating the experiments using a particular set of hand bones radiographs, the proposed method worked well for almost all of the images that we used.	experiment;pixel;premature convergence;radiography;radiology;robustness (computer science);the mask;traffic collision avoidance system	Syaiful Anam;Eiji Uchino;Hideaki Misawa;Noriaki Suetake	2013	2013 IEEE 6th International Workshop on Computational Intelligence and Applications (IWCIA)	10.1109/IWCIA.2013.6624782	computer vision;mathematics;set theory	Vision	44.85027001461672	-73.63556595126617	86963
169a6cf4372b34651e1d0554e9cbe00de83d66c2	improved image decompression for reduced transform coding artifacts	wavelet analysis;decompression;traitement signal;optimisation;descompresion;perceived quality;image coding;probability;image processing;signal estimation;pressure relief;video compression;compresion senal;procesamiento imagen;gaussian markov random field;transform coding;maximum likelihood estimation;probabilistic models image decompression reduced transform coding artifacts reconstructed image quality stochastic model map estimate transform coefficient partition cell image estimation technique nongaussian markov random field image model convex constrained optimization problem iterative solution gradient projection method scalar quantization vector quantization subband wavelet transform;traitement image;compression signal;reduccion ruido;algorithme;wavelet transforms;iterative methods;algorithm;reconstruction image;scalar quantization;wavelet transform;vector quantization;image compression;mathematical models;reconstruccion imagen;maximum likelihood estimation transform coding vector quantisation optimisation image reconstruction image coding iterative methods markov processes wavelet transforms source coding probability;image reconstruction;signal processing;noise reduction;signal compression;estimacion senal;random processes;reduction bruit;algorithms;vector quantizer;markov processes;stochastic model;iterative solution;vector quantisation;gradient projection method;constrained optimization problem;procesamiento senal;estimation signal;image modeling;modelo estocastico;transform coding image reconstruction quantization image coding bit rate degradation stochastic processes markov random fields constraint optimization discrete cosine transforms;modele stochastique;source coding;algoritmo	-The perceived quality of images reconstructed from low bit rate compression is severely degraded by the appearance of transform coding artifacts. This paper proposes a method for producing higher quality reconstructed images based on a stochastic model for the image data. Quantization (scalar or vector) partitions the transform coefficient space and maps all points in a partition cell to a representative reconstruction point, usually taken as the centroid of the cell. The proposed image estimation technique selects the reconstruction point within the quantization partition cell which results in a reconstructed image which best fits a non-Gaussian Marker random field (MRF) image model. This approach results in a convex constrained optimization problem which can be solved iteratively. At each iteration, the gradient projection method is used to update the estimate based on the image model. In the transform domain, the resulting coefficient reconstruction points are projected to the particular quantization partition cells defined by the compressed image. Experimental results will be shown for images compressed using scalar quantization of block DCT and using vector quantization of subband wavelet transform. The proposed image decompression provides a reconstructed image with reduced visibility of transform coding artifacts and superior perceived quality.	coefficient;constrained optimization;constraint (mathematics);data compression;discrete cosine transform;fits;gradient;image compression;iteration;map;markov random field;mathematical optimization;optimization problem;quantization (signal processing);transform coding;vector quantization;wavelet transform	Thomas P. O'Rourke;Robert L. Stevenson	1995	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.475891	computer vision;mathematical optimization;image processing;computer science;signal processing;pattern recognition;mathematics;fractal transform;quantization;top-hat transform;vector quantization;statistics;wavelet transform	Vision	53.057601460788135	-68.0297298607577	87030
0812a0eac90f7b72a6bfcbbcc3939c4bb419fad6	accurate real-time neural disparity map estimation with fpga	disparity space image dsi;fpga;disparity map;neural network	We propose in this paper a new method for real-time dense disparity map computing using a stereo pair of rectified images. Based on the neural network and Disparity Space Image (DSI) data structure, the disparity map computing consists of two main steps: initial disparity map estimation by combining the neuronal network and the DSI structure, and its refinement. Four improvements are introduced so that an accurate and fast result will be reached. The first one concerns the proposition of a new strategy in order to optimize the computation time of the initial disparity map. In the second one, a specific treatment is proposed in order to obtain more accurate disparity for the neighboring pixels to boundaries. The third one, it concerns the pixel similarity measure for matching score computation and it consists of using in addition to the traditional pixel intensities, the magnitude and orientation of the gradients providing more accuracy. Finally, the processing time of the method has been decreased consequently to our implementation of some critical steps on FPGAs. Experimental results on real datasets are conducted and a comparative evaluation of the obtained results relative to the state-of-art methods is presented. & 2011 Elsevier Ltd. All rights reserved.	artificial neural network;autonomous robot;binocular disparity;computation;data structure;field-programmable gate array;genetic algorithm;gradient;image resolution;mobile robot;pixel;real-time clock;refinement (computing);robotic mapping;scope (computer science);similarity measure;time complexity	Nadia Baha Touzene;Slimane Larabi	2012	Pattern Recognition	10.1016/j.patcog.2011.08.005	computer vision;computer science;theoretical computer science;machine learning;artificial neural network;field-programmable gate array	Robotics	46.877811390171985	-68.60632988303753	87184
c056c41f86d0ce2929a9507cfd913d42f8b2d5cd	boundary and medial shape analysis of the hippocampus in schizophrenia	control group;spherical harmonic;shape analysis;statistical shape analysis;medical image analysis;schizophrenia;brain morphometry;medial shape description;shape description	Statistical shape analysis has become of increasing interest to the neuroimaging community due to its potential to precisely locate morphological changes and thus potentially discriminate between healthy and pathological structures. This paper describes a combined boundary and medial shape analysis based on two different shape descriptions applied to a study of the hippocampus shape abnormalities in schizophrenia. The first shape description is the sampled boundary implied by the spherical harmonic SPHARM description. The second one is the medial shape description called M-rep. Both descriptions are sampled descriptions with inherent point correspondence. Their shape analysis is based on computing differences from an average template structure analyzed using standard group mean difference tests. The results of the global and local shape analysis in the presented hippocampus study exhibit the same patterns for the boundary and the medial analysis. The results strongly suggest that the normalized hippocampal shape of the schizophrenic group is different from the control group, most significantly as a deformation difference in the tail region.	medial graph;shape analysis (digital geometry)	Martin Styner;Jeffrey A. Lieberman;Dimitrios Pantazis;Guido Gerig	2004	Medical Image Analysis	10.1016/j.media.2004.06.004	computer vision;brain morphometry;shape analysis;schizophrenia;mathematics;geometry	Vision	42.38656376275298	-78.8981346219797	87263
68b4cab49bf667c736bb1ba1c7e7d3948510ff47	registration of pathological images		This paper proposes an approach to improve atlas-to-image registration accuracy with large pathologies. Instead of directly registering an atlas to a pathological image, the method learns a mapping from the pathological image to a quasi-normal image, for which more accurate registration is possible. Specifically, the method uses a deep variational convolutional encoder-decoder network to learn the mapping. Furthermore, the method estimates local mapping uncertainty through network inference statistics and uses those estimates to down-weight the image registration similarity measure in areas of high uncertainty. The performance of the method is quantified using synthetic brain tumor images and images from the brain tumor segmentation challenge (BRATS 2015).	atlases;brain neoplasms;brain simulation;convolutional code;decoder device component;encoder device component;estimated;image registration;inference;similarity measure;synthetic intelligence;variational principle;registration - actclass	Xiao Yang;Xu Han;Eunbyung Park;Stephen R. Aylward;Roland Kwitt;Marc Niethammer	2016	Simulation and synthesis in medical imaging : first International Workshop, SASHIMI 2016, held in conjunction with MICCAI 2016, Athens, Greece, October 21, 2016, Proceedings. SASHIMI (Workshop)	10.1007/978-3-319-46630-9_10	pathological;computer science;pattern recognition;artificial intelligence;computer vision;similarity measure;image registration;inference	Vision	43.68019871363753	-78.25705178379009	87382
d6c3836c15373f31028cf643d19076c27541bdc7	image segmentation using local spectral histograms and linear regression	linear regression;texture segmentation;spectral histogram	We present a novel method for segmenting images with texture and nontexture regions. Local spectral histograms are feature vectors consisting of histograms of chosen filter responses, which capture both texture and nontexture information. Based on the observation that the local spectral histogram of a pixel location can be approximated through a linear combination of the representative features weighted by the area coverage of each feature, we formulate the segmentation problem as a multivariate linear regression, where the solution is obtained by least squares estimation. Moreover, we propose an algorithm to automatically identify representative features corresponding to different homogeneous regions, and show that the number of representative features can be determined by examining the effective rank of a feature matrix. We present segmentation results on different types of images, and our comparison with other methods shows that the proposed method gives more accurate results. 2011 Elsevier B.V. All rights reserved.	adaptive filter;approximation algorithm;autonomous robot;experiment;general linear model;image segmentation;least squares;mcgurk effect;pixel;smoothing;texture mapping	Jiangye Yuan;DeLiang Wang;Rongxing Li	2012	Pattern Recognition Letters	10.1016/j.patrec.2011.12.003	computer vision;computer science;linear regression;machine learning;pattern recognition;mathematics	Vision	47.89417509600591	-68.96027232007086	87743
a1f19158c71631968b3ca1dc8f1435ff4340766c	total variation for image denoising based on a novel smart edge detector: an application to medical images	computer tomography;medical images;total variation;image denoising;edge detector	In medical imaging applications, diagnosis relies essentially on good quality images. Edges play a crucial role in identifying features useful to reach accurate conclusions. However, noise can compromise this task as it degrades image information by altering important features and adding new artifacts rendering images non-diagnosable. In this paper, we propose a novel denoising technique based on the total variation method with an emphasis on edge preservation. Image denoising techniques such as the Rudin–Osher–Fatemi model which are guided by gradient regularizer are generally accompanied with staircasing effect and loss of details. To overcome these issues, our technique incorporates in the model functional, a novel edge detector derived from fuzzy complement, non-local mean filter and structure tensor. This procedure offers more control over the regularization, allowing more denoising in smooth regions and less denoising when processing edge regions. Experimental results on synthetic images demonstrate the ability of the proposed edge detector to determine edges with high accuracy. Furthermore, denoising experiments conducted on CT scan images and comparison with other denoising methods show the outperformance of the proposed denoising method.		Ahmed Ben Said;Rachid Hadjidj;Sebti Foufou	2018	Journal of Mathematical Imaging and Vision	10.1007/s10851-018-0829-6	rendering (computer graphics);artificial intelligence;mathematics;computer vision;structure tensor;computed tomography;noise reduction;median filter;medical imaging;detector;compromise	Vision	47.522923292159945	-73.78848591994768	87846
01cf7f43c92b3fae2b24d48a89902688a0437e7b	meta-heuristic moth swarm algorithm for multilevel thresholding image segmentation		Multilevel thresholding is a very important image processing technique in the field of image segmentation. However, the computational complexity of determining the optimal threshold grows exponentially with increasing thresholds. To overcome this drawback, in this paper, we propose a multi-threshold image segmentation method based on the moth swarm algorithm. The meta-heuristic algorithm uses Kapur’s entropy method to optimize the thresholds for eight standard test images. When compared with other state-of-the-art evolutionary algorithms, the proposed method proved to be robust and effective according to numerical experimental results and image segmentation results. This indicates the high performance of the method for the segmentation of digital images.	computational complexity theory;digital image;evolutionary algorithm;heuristic (computer science);image processing;image segmentation;numerical analysis;standard test image;swarm;thresholding (image processing)	Yongquan Zhou;Xiao Yang;Ying Ling;Jinzhong Zhang	2018	Multimedia Tools and Applications	10.1007/s11042-018-5637-x	computer vision;digital image;image processing;evolutionary algorithm;image segmentation;computer science;computational complexity theory;artificial intelligence;thresholding;pattern recognition;algorithm;heuristic;segmentation	Vision	42.78276414068407	-68.7711777218886	88020
0cbfd0030d4010ef480af91f3de914f0525a52e6	statistical shape model of nested structures based on the level set		We propose a method for constructing a multi-shape statistical shape model (SSM) for nested structures such that each is a subset or superset of another. The proposed method has potential application to any pair of shapes with an inclusive relationship. These types of shapes are often found in anatomy such as the brain and ventricle. Most existing multi-shape SSMs can be used to describe these nested shapes; however, none of them guarantees a correct inclusive relationship. The basic concept of the proposed method is to describe nested shapes by applying different thresholds to a single continuous real-valued function in an image space. We demonstrate that there exists a one-to-one mapping from an arbitrary pair of nested shapes to this type of function. We also demonstrate that this method can be easily extended to represent three or more nested structures. We demonstrate the effectiveness of proposed SSM using brain and ventricle volumes obtained from particular stages of human embryos. The performance of the SSM was evaluated in terms of generalization and specificity ability. Additionally, we measured leakage criteria to assess the ability to preserve inclusive relationships. A quantitative comparison of our SSM with conventional multi-shape SSMs demonstrates the superiority of the proposed method.		Atsushi Saito;Masaki Tsujikawa;Tetsuya Takakuwa;Shigehito Yamada;Akinobu Shimizu	2017		10.1007/978-3-319-66182-7_20	pattern recognition;artificial intelligence;level set;computer science;existential quantification;subset and superset	Vision	42.955696655835766	-78.7244343229621	88102
55d35055334902f0d849a0930c5700736d37c4f4	multimodality bayesian algorithm for image reconstruction in positron emission tomography: a tissue composition model	nuclear magnetic resonance imaging;bayes estimation;segmented mri images multimodality bayesian algorithm pet image reconstruction tissue composition model anatomical information incorporation nuclear medicine tissue composition assignment maximum a posteriori estimation tissue type medical diagnostic imaging full physical model spatial smoothing;tissu;medical imagery;image segmentation;bayes methods;simulation;biomedical nmr;pet imaging;simulacion;estimation a posteriori;tomocentelleografia;bayesian methods image reconstruction positron emission tomography pixel reconstruction algorithms image segmentation testing brain modeling smoothing methods magnetic resonance;segmentation;modele physique;indexing terms;anatomia;qualite image;positron emission tomography;a posteriori estimation;imageria rmn;tejido;reconstruction image;physiological models image reconstruction image segmentation biomedical nmr bayes methods positron emission tomography medical image processing;estimacion bayes;mr imaging;algorithms alzheimer disease bayes theorem brain computer simulation humans image processing computer assisted magnetic resonance imaging tomography emission computed;estimacion a posteriori;reconstruccion imagen;exploration radioisotopique;magnetic resonance;image reconstruction;medical image processing;emission tomography;image quality;positron;imagerie medicale;modelo fisico;tissue;map estimation;radionuclide study;imagerie rmn;calidad imagen;imageneria medical;physical model;positon;anatomie;exploracion radioisotopica;physiological models;anatomy;segmentacion;tomoscintigraphie;estimation bayes	The use of anatomical information to improve the quality of reconstructed images in positron emission tomography (PET) has been extensively studied. A common strategy has been to include spatial smoothing within boundaries defined from the anatomical data. The authors present an alternative method for the incorporation of anatomical information into PET image reconstruction, in which they use segmented magnetic resonance (MR) images to assign tissue composition to PET image pixels. The authors model the image as a sum of activities for each tissue type, weighted by the assigned tissue composition. The reconstruction is performed as a maximum a posteriori (MAP) estimation of the activities of each tissue type. Two prior functions, defined for tissue-type activities, are considered. The algorithm is tested in realistic simulations employing a full physical model of the PET scanner.	ct scan;histocompatibility testing;iterative reconstruction;multimodal imaging;personnameuse - assigned;pixel;polyethylene terephthalate;positron-emission tomography;positrons;resonance;scanning systems;simulation;smoothing (statistical technique);x-ray computed tomography;algorithm	Srikath Sastry;Richard E. Carson	1997	IEEE Transactions on Medical Imaging	10.1109/42.650872	iterative reconstruction;image quality;computer vision;index term;radiology;medicine;physical model;computer science;positron;magnetic resonance imaging;image segmentation;nuclear medicine;segmentation;medical physics	Vision	47.3777182811501	-79.37347668098309	88150
e9507a914eb3b6deb58babb5f85781ee63a5f141	hypergraph-based saliency map generation with potential region-of-interest approximation and validation	databases;graph theory;sensors;modeling and simulation;edge detection;approximation theory;distance measurement;computational modeling;image representation;feature extraction;journal magazine article	A novel saliency model is proposed in this paper to automatically process images in the similar way as the human visual system which focuses on conspicuous regions that catch human beings’ attention. The model combines a hypergraph representation and a partitioning process with potential region-of-interest (p-ROI) approximation and validation. Experimental results demonstrate that the proposed method shows considerable improvement in the performance of saliency map generation. © 2012 SPIE and IS&T. [DOI: 10.1117/1.JEI.21.1.013012]	approximation;bottom-up parsing;data validation;human visual system model;region of interest;top-down and bottom-up design	Zhen Liang;Hong Fu;Zheru Chi;David Dagan Feng	2012	J. Electronic Imaging	10.1117/1.JEI.21.1.013012	computer vision;edge detection;feature extraction;computer science;sensor;graph theory;machine learning;pattern recognition;modeling and simulation;computational model;approximation theory	Robotics	42.93667082018853	-66.5723795645834	88279
8b6b00e32688131cf8dacb083f7142b8e5418f5d	convex image segmentation model based on local and global intensity fitting energy and split bregman method		We propose a convex image segmentation model in a variational level set formulation. Both the local information and the global information are taken into consideration to get better segmentation results.We first propose a globally convex energy functional to combine the local and global intensity fitting terms. The proposed energy functional is then modified by adding an edge detector to force the active contour to the boundary more easily. We then apply the split Bregman method to minimize the proposed energy functional efficiently. By using a weight function that varies with location of the image, the proposed model can balance the weights between the local and global fitting terms dynamically. We have applied the proposed model to synthetic and real images with desirable results. Comparison with other models also demonstrates the accuracy and superiority of the proposed model.		Yunyun Yang;Boying Wu	2012	J. Applied Mathematics	10.1155/2012/692589	econometrics;mathematical optimization;mathematics;geometry	Vision	50.570571635916096	-71.22735214116334	88435
5fd3bd788d87e00c5691fbed16578ad4432fbfbd	automatic image thumbnailing based on fast visual saliency detection	saliency detection;thumbnailing;visual attention	Image retargeting has seen many applications in areas such as content adaptation for small displays and thumbnailing for image database browsing. Most retargeting methods, however, are too expensive computationally to achieve fast performance on common desktop systems. This work addresses the problem of fast automatic thumbnailing for image browsing. A simple approach of automatic thresholding saliency maps and cropping using bounding box extraction is presented. Eight of the fastest saliency detectors in the literature and three automatic thresholding methods are assessed using precision, recall, F-score and execution time on the MSRA1K dataset. The results show that the approach is computationally efficient and adequate for fast automatic image thumbnailing. In particular, saliency detection with difference to random color samples (RS) thresholded by Rosin's method achieved the best trade-off between execution time and F-score.	algorithmic efficiency;content adaptation;desktop computer;f1 score;fastest;image viewer;map;minimum bounding box;retargeting;run time (program lifecycle phase);seam carving;sensor;thresholding (image processing);thumbnail	Maiko M. I. Lie;Hugo Vieira Neto;Gustavo B. Borba;Humberto R. Gamba	2016		10.1145/2976796.2988190	computer vision;computer science;pattern recognition;computer graphics (images)	Vision	40.1998660130524	-66.24514882069919	88561
6f84a9925e8efc5a4d7708dbcbaad79604d08ab9	automated snake initialization for the segmentation of the prostate in ultrasound images	analisis imagen;modelizacion;prostata;vision ordenador;medical imagery;image acoustique;acoustic image;image segmentation;image processing;ultrasound;edge detection;ultrasonic imaging;procesamiento imagen;automatisation;imagen acustica;seed points;inicializacion;automatizacion;traitement image;automatic generation;formation image ultrason;transmision asincronica;computer vision;deteccion contorno;formacion imagen ultrasonico;modelisation;detection contour;ultrasound imaging;initial snake;medical image processing;segmentation image;imagineria medica;imagerie medicale;pattern recognition;asynchronous transmission;image analysis;vision ordinateur;transmission asynchrone;reconnaissance forme;reconocimiento patron;nathematical morphology;user interaction;modeling;analyse image;initialization;prostate;initialisation;active contour model;automation	Segmentation is a crucial task in medical image processing. Snakes or Active Contour Models (ACM) are valuable tools to segment images. However, they need a good initialization, which is usually provided manually by an expert. In order to achieve a reliable automation of prostate segmentation in ultrasound images, morphological techniques have been used in this work to automatically generate the initial snake. The accuracy of the proposed approach is verified by testing several images. The automated segmentation of the prostate can be done in the majority of the cases without user interaction.		Shahryar Rahnamayan;Hamid R. Tizhoosh;Magdy M. A. Salama	2005		10.1007/11559573_113	computer vision;initialization;image analysis;systems modeling;edge detection;image processing;computer science;automation;asynchronous communication;ultrasound;active contour model;image segmentation;scale-space segmentation;computer graphics (images)	Vision	46.148688136172055	-78.7893109273504	88612
254d59ab9cac1921687d2c0313b8f6fc66541dfb	a new mura defect inspection way for tft-lcd using level set method	local lightness variation;tft lcd;vision defects computer vision gabor filters image segmentation liquid crystal displays thin film transistors;inspection level set image segmentation active contours image edge detection machine vision gabor filters shape brightness image reconstruction;active contour;image segmentation;typical vision defect;level set;semu method tft lcd thin film transistors mura defect inspection level set method typical vision defect local lightness variation blurry contour machine vision inspection gabor filters image segmentation chan vese model;liquid crystal displays;mura defect inspection;semu method;gabor filters;active contours;defect inspection;computer vision;gabor filter;mura;vision defects;image edge detection;machine vision;mura active contour defect inspection level set;blurry contour;thin film transistors;level set method;machine vision inspection;active contour model;chan vese model	Mura is a typical vision defect of LCD panel, appearing as local lightness variation with low contrast and blurry contour. This letter presents a new machine vision inspection way for Mura defect based on the level set method. First, a set of real Gabor filters are applied to eliminating the global textured backgrounds. Then, the level set method is employed for image segmentation with a new region-based active contours model, which is an improvement of the Chan-Vese's model so that it more suitable to the segmentation of Mura. Using some results from the level set based segmentation, the defects are quantified based on the SEMU method. Experiments show that the proposed method has better performance for Mura detection and quantification.	experiment;gabor filter;image segmentation;machine vision;modified uniformly redundant array;software bug;software defect indicator;thin-film-transistor liquid-crystal display	Xin Bi;Chungang Zhuang;Han Ding	2009	IEEE Signal Processing Letters	10.1109/LSP.2009.2014113	computer vision;machine vision;computer science;active contour model;computer graphics (images)	Vision	47.05391331140356	-66.19341086931671	88637
c34d46a98a7cea29b295d66356dffd0f9101c3c0	3-d visual feedback for automated sorting of cells with ultra-low proportion under dark field		Study of cellular behaviors, especially the ultra-rare cell type, can aid in the accuracy of clinic diagnoses as well as the development of bioresearch engineering, thus the importance of isolating them from heterogeneous mixtures. However, current methods may fail in purity, versatility or cause contamination to cell targets, which is fatal drawback to rare cells. To address this issue, we propose a versatile method to automatically select and capture fluorescent stained target cells with high purity and recovery rate, through developing a novel 3D image processing algorithm under dark field. With the automated pick-and-place strategies, the micro-robotic system achieves cell screening even in an environment with ultra-sparse cells. In the proposed visual method, Markov Random Field (MRF) separation is adapted into the fluorescent environment to attain real-time planar location of micropipette and target cells. A reformative method derived from Depth from Defocus (DFD) is brought up to acquire 3D information. The basic system for this method mainly consists of a camera mounted on motorized fluorescent microscope and a micromanipulator for cell capture. The fluorescent label help to screen out most of the undesired cells while also bring extra constraints and requisition to our visual method. Finally, experiments of collecting 3 T3 cells are performed to verify the feasibility and validity of the designed method, achieving average 98% purity and 80% recovery rate within the time limits. This study indicates that proposed visual processing method can not only provides reliable location feedback for micro-manipulation in rare cell sorting, but also can be easily extended to satisfy other automated micro-robotics manipulation.	algorithm;boosting (machine learning);cell (microprocessor);computation;data flow diagram;experiment;image processing;markov chain;markov random field;microbotics;pure function;real-time clock;robot;robotics;smt placement equipment;sorting;sparse matrix;system image	J. L. Tan;Huaping Wang;Qing Shi;Zhiqiang Zheng;Juan Cui;Tao Sun;Qiang Huang;Toshio Fukuda	2018	2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2018.8525529	image processing;computer vision;visual processing;cell sorting;dark field microscopy;artificial intelligence;image segmentation;micromanipulator;planar;sorting;computer science	Robotics	39.51982053545188	-71.58154563976917	88791
a402d6b7ff5dd0d66fede811be54446f9ddf7342	layer-based representation of polyhedrons for point containment tests	solid representation computational geometry polyhedron point containment;point in polyhedron tests;performance evaluation;time complexity;algorithms computer graphics image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted;computational geometry;binary space partitioning;sequential analysis;solid representation;polyhedron;testing;layer based representation;shape;bsp based inclusion layer based representation point in polyhedron tests binary space partitioning;binary space partition;organizing;image representation;bsp based inclusion;sun;robustness;binary search;life estimation;computer science;testing robustness performance evaluation life estimation laboratories computer science sun sequential analysis organizing shape;point containment	This paper presents the layer-based representation of polyhedrons and its use for point-in-polyhedron tests. In the representation, the facets and edges of a polyhedron are sequentially arranged, and so, the binary search algorithm is efficiently used to speed up inclusion tests. In comparison with conventional representation for polyhedrons, the layer-based representation that we propose greatly reduces the storage requirement because it represents much information implicitly though it still has a storage complexity O(n). It is simple to implement and robust for inclusion tests because many singularities are erased in constructing the layer-based representation. By incorporating an octree structure for organizing polyhedrons, our approach can run at a speed comparable with Binary space partitioning (BSP)-based inclusion tests and, at the same time, greatly reduce storage and preprocessing time in treating large polyhedrons. We have developed an efficient solution for point-in-polyhedron tests, with the time complexity varying between O(n) and O(logn), depending on the polyhedron shape and the constructed representation, and less than O(log3n) in most cases. The time complexity of preprocess is between O(n) and O(n2), varying with polyhedrons, where n is the edge number of a polyhedron.	3d film;binary search algorithm;binary space partitioning;octree;organizing (structure);polyhedron;preprocessor;requirement;structure of articular surface of bone;time complexity;trees (plant);anatomical layer	Wencheng Wang;Jing Li;Hanqiu Sun;Enhua Wu	2008	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2007.70407	time complexity;binary space partitioning;combinatorics;computational geometry;shape;computer science;theoretical computer science;sequential analysis;mathematics;geometry;software testing;algorithm;robustness;polyhedron;binary search algorithm	Visualization	45.48162752680544	-71.33454336159166	89077
dbca10779dae6c7e79961d16a71aa0902d435f07	skeleton extraction of a specified object in the gray image based on geometric features		In this paper, we present a two-stage method to extract the accurate and smooth skeleton of a specified object in the gray image based on geometric characteristics of the boundary. In the first stage, according to the statistical intensity disparity between the sample points and the object, an energy function is constructed, and then a novel segmentation model is proposed to extract any specified objects in the gray image by the variational method. In the second stage, on the basis of the segmentation, an improved skeleton extraction algorithm is given in virtue of the shortest path connection approach. Some examples show the robustness and insensitivity of the presented algorithm to the perturbation and noise, respectively.		Zhihui Yang;Fangfang Guo;Ping Dong	2012		10.1007/978-3-642-34038-3_23	computer vision;morphological skeleton	Robotics	48.12235959949652	-69.68371063117573	89182
3c0d121ce1db15e7a70d508153e7329dcece7779	based on multi-directional multi-scale weighted morphology fod images enhance algorithm	multi directional multi scale weighted morphology;airport runway;multi directional multi scale weighted morphology airport runway fod enhance;fod denoising method multidirectional multiscale weighted morphology fod image enhancement algorithm foreign object debris airport security;fod;mathematical morphology image denoising image enhancement;enhance	The existence of the airport runway foreign object debris (FOD) threats airport security, in order to reduce the influence of economic and person's security is caused by the FOD, and FOD debris monitoring is very important. A new FOD de-noising method is proposed based on multi-directional multi-scale weighted morphology in this paper. Experimental results prove that the proposed method can eliminate FOD noise and fast operation. And it can keep the detail of the images. Subsequent FOD images detection and recognition is very good.	airport security;algorithm;galaxy morphological classification;mathematical morphology	Yanying Guo;Zhigang Liu;Lihui Jiang	2015	2015 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2015.7340894	runway;foreign object damage	Robotics	40.0727662781988	-66.5803143184231	89232
c165539c4a41cdb135e0e3920a0fc31fb1462187	adaptive diffusion flow for parametric active contours	vectors convergence edge detection gradient methods image restoration laplace equations;gradient vector flow;laplacian functional;convergence;active contour;image segmentation;force image edge detection noise active contours image segmentation convergence harmonic analysis;edge preserving;edge detection;adaptive diffusion flow;image restoration;active contours;noise robustness;force;concavity convergence adaptive diffusion flow parametric active contours gradient vector flow image restoration laplacian functional noise robustness edge preserving;laplace equations;vectors;image edge detection;concavity convergence;gradient methods;image segmentation adaptive diffusion flow gradient vector flow active contours;parametric active contours;diffusion process;noise;harmonic analysis	This paper proposes a novel external force for active contours, called adaptive diffusion flow (ADF). We reconsider the generative mechanism of gradient vector flow (GVF) diffusion process from the perspective of image restoration, and exploit a harmonic hyper surface minimal function to substitute smoothness energy term of GVF for alleviating the possible leakage problem. Meanwhile, a ∞- laplacian functional is incorporated in the ADF framework to ensure that the vector flow diffuses mainly along normal direction in homogenous regions of an image. Experiments on synthetic and real images demonstrate the good properties of the ADF snake, including noise robustness, weak edge preserving, and concavity convergence.	active contour model;circuit restoration;concave function;experiment;gradient;image restoration;image segmentation;normal (geometry);spectral leakage;synthetic intelligence	Yuwei Wu;Yunde Jia;Yuanquan Wang	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.683	image restoration;computer vision;mathematical optimization;edge detection;topology;convergence;computer science;noise;diffusion process;harmonic analysis;active contour model;mathematics;image segmentation;force	Vision	51.15228697438057	-71.59937130399733	89329
727df2121042e7c87b1c3e0299a58eab743d36a0	cyclic continuous max-flow: a third paradigm in generating local phase shift maps in mri		Sensitivity to phase deviations in MRI forms the basis of a variety of techniques, including magnetic susceptibility weighted imaging and chemical shift imaging. Current phase processing techniques fall into two families: those which process the complex image data with magnitude and phase coupled, and phase unwrapping-based techniques that first linearize the phase topology across the image. However, issues, such as low signal and the existence of phase poles, can lead both methods to experience error. Cyclic continuous max-flow (CCMF) phase processing uses primal-dual-variational optimization over a cylindrical manifold, which represent the inherent topology of phase images, increasing its robustness to these issues. CCMF represents a third distinct paradigm in phase processing, being the only technique equipped with the inherent topology of phase. CCMF is robust and efficient with at least comparable accuracy as the prior paradigms.	anatomy, regional;calculus of variations;dual;flow cytometry;in-phase and quadrature components;instantaneous phase;mathematical optimization;maximum flow problem;microtubule-associated proteins;programming paradigm;manifold	John S. H. Baxter;Zahra Hosseini;Terry M. Peters;Maria Drangova	2018	IEEE Transactions on Medical Imaging	10.1109/TMI.2017.2766922	manifold;phase (waves);maximum flow problem;mathematics;mathematical optimization;robustness (computer science);magnitude (mathematics);magnetic susceptibility;linear programming	Vision	47.140210647398774	-77.15889787580103	89410
0f088fff04f9a0a862450276d0993037f9697250	group average difference: a termination criterion for active contour	femur;journal article;image enhancement;image interpretation computer assisted;drntu engineering chemical engineering;tibia;humerus;magnetic resonance imaging;algorithms;pattern recognition automated	This paper presents a termination criterion for active contour that does not involve alteration of the energy functional. The criterion is based on the area difference of the contour during evolution. In this criterion, the evolution of the contour terminates when the area difference fluctuates around a constant. The termination criterion is tested using parametric gradient vector flow active contour with contour resampling and normal force selection. The usefulness of the criterion is shown through its trend, speed, accuracy, shape insensitivity, and insensitivity to contour resampling. The metric used in the proposed criterion demonstrated a steadily decreasing trend. For automatic implementation in which different shapes need to be segmented, the proposed criterion demonstrated almost 50% and 60% total time reduction while achieving similar accuracy as compared with the pixel movement-based method in the segmentation of synthetic and real medical images, respectively. Our results also show that the proposed termination criterion is insensitive to shape variation and contour resampling. The criterion also possesses potential to be used for other kinds of snakes.	active contour model;contour line;eisenstein's criterion;generalized anxiety disorder;gradient;pixel;segmentation action;snakes;synthetic intelligence;terminate (software);biologic segmentation	Tong Kuan Chuah;Jun Hong Lim;Chueh-Loo Poh	2011	Journal of Digital Imaging	10.1007/s10278-011-9405-y	computer vision;radiology;medicine;artificial intelligence;magnetic resonance imaging;mathematics	Vision	45.209409249248374	-73.54639936296005	89585
174c0bbff322696634a2231032f6adc29150d3fe	self-similarity weighted mutual information: a new nonrigid image registration metric	nonlocal means;mutual information;ultrasound and mri registration;α mutual information;nonrigid registration;α	Mutual information (MI) has been widely used as a similarity measure for rigid registration of multi-modal and uni-modal medical images. However, robust application of MI to deformable registration is challenging mainly because rich structural information, which are critical cues for successful deformable registration, are not incorporated into MI. We propose a self-similarity weighted graph-based implementation of α-mutual information (α-MI) for nonrigid image registration. We use a self-similarity measure that uses local structural information and is invariant to rotation and to local affine intensity distortions, and therefore the new Self Similarity α-MI (SeSaMI) metric inherits these properties and is robust against signal nonstationarity and intensity distortions. We have used SeSaMI as the similarity measure in a regularized cost function with B-spline deformation field to achieve nonrigid registration. Since the gradient of SeSaMI can be derived analytically, the cost function can be efficiently optimized using stochastic gradient descent methods. We show that SeSaMI produces a robust and smooth cost function and outperforms the state of the art statistical based similarity metrics in simulation and using data from image-guided neurosurgery.		Hassan Rivaz;Zahra Karimaghaloo;D. Louis Collins	2014	Medical image analysis	10.1016/j.media.2013.12.003	computer vision;mathematical optimization;pattern recognition;mathematics;mutual information;statistics	Vision	47.26021747083293	-76.21719941342606	90010
97d4dd5b14aaa85e1a43cdcf1c35b546b3c86a20	reconstructing the three-dimensional surface of a branching and merging biological structure from a stack of coplanar contours	branching biological structure;neuroanatomical datasets;biology computing;image reconstruction surface reconstruction merging bifurcation surface treatment blood vessels;biological tissues;brain;3d modeling;biological structure 3d surface reconstruction;optical microscopy biological techniques biological tissues biology computing brain image reconstruction neurophysiology;cost function;neuroanatomical datasets biological structure 3d surface reconstruction branching biological structure merging biological structure coplanar contour stack correspondence problem connection constraints contour association cost function correspondence graph branching problem bifurcation cascade;bifurcation;coplanar contour stack;correspondence problem;surface reconstruction;three dimensional;surface treatment;3d model;merging biological structure;contour association cost function;image reconstruction;correspondence graph;merging;neuroanatomy surface reconstruction histology 3d modeling;neuroanatomy;neurophysiology;biological techniques;histology;optical microscopy;blood vessels;branching problem;bifurcation cascade;connection constraints	We address here two important problems when reconstructing the three-dimensional surface of a biological structure from a stack of section contours. The correspondence problem, which aims at associating contours on adjacent sections, is solved using connection constraints and a contour association cost function to build a correspondence graph. The branching problem, which aims to reconstruct the surface between one contour and several ones on the adjacent section, is decomposed as a cascade of bifurcations at linearly interpolated contour locations between sections. Our method restores connections even for distant contours and produces anatomically relevant branching reconstructions. The validity of this approach is illustrated on two neuroanatomical datasets.	contour line;correspondence problem;linear interpolation;loss function	Jasmine Burguet;Philippe Mailly;Yves Maurin;Philippe Andrey	2011	2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2011.5872479	iterative reconstruction;three-dimensional space;computer vision;topology;medicine;surface reconstruction;neuroanatomy;optical microscope;mathematics;geometry;histology;correspondence problem;neurophysiology	Vision	45.80454250729899	-77.63534149519187	90021
8423ebeb92e2453dfe033ff899b6f942a8f96406	a method to reconstruct activation wavefronts without isotropy assumptions using a level sets approach	level set approach;level set;inverse problem;level set method	We report on an investigation into using a Level Sets based method to reconstruct activation wavefronts at each time instant from measured potentials on the body surface. The potential map on the epicardium is approximated by a two level image and the inverse problem is solved by evolving a boundary, starting from an initial region, such that a filtered residual error is minimized. The advantage of this method over standard activation-based solutions is that no isotropy assumptions are required. We discuss modifications of the Level Sets method used to improve accuracy, and show the promise of this method via simulation results using recorded canine epicardial data.		Felipe Calderero;Alireza Ghodrati;Dana H. Brooks;Gilead Tadmor;Robert S. MacLeod	2005		10.1007/11494621_20	mathematical optimization;mathematical analysis;mathematics;geometry;level set method	NLP	51.19780082651209	-79.15789091632101	90136
a2eddb9f41b8d66e4c901d012d37ede2b3079e4c	robust optimization using disturbance for image registration	robust optimization;image registration;success rate;mutual information;multi resolution;optimal algorithm;similarity measure	This paper exploits the different properties between the local neighborhood of global optimum and those of local optima in image registration optimization. Namely, a global optimum has a larger capture neighborhood, in which from any location a monotonic path exists to reach this optimum, than any other local optima. With these properties, we propose a simple and computationally efficient technique using transformation disturbance to assist an optimization algorithm to avoid local optima, and hence to achieve a robust optimization. We demonstrate our method on 3D rigid registrations by using mutual information as similarity measure, and we adopt quaternions to represent rotations for the purpose of the unique and order-independent expression. Randomized registration experiments on four clinical CT and MR-T1 datasets show that the proposed method consistently gives much higher success rates than the conventional multi-resolution mutual information based method. The accuracy of our method is also high.	algorithmic efficiency;experiment;global optimization;image registration;local optimum;mathematical optimization;maxima and minima;mutual information;powell's method;randomized algorithm;robust optimization;similarity measure;simplex algorithm	Rui Gan;Albert C. S. Chung	2006		10.1007/11784012_34	computer vision;local optimum;mathematical optimization;robust optimization;image registration;machine learning;mathematics;mutual information;statistics	Vision	48.00727429430824	-74.9622500374555	90450
07850a1d06d0a8dff1d1adac1de7b9638007854d	tumor segmentation in brain mri using a fuzzy approach with class center priors	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	This paper proposes a new fuzzy approach for the automatic segmentation of normal and pathological brain magnetic resonance imaging (MRI) volumetric datasets. The proposed approach reformulates the popular fuzzy c-means (FCM) algorithm to take into account any available information about the class center. The uncertainty in this information is also modeled. This information serves to regularize the clusters produced by the FCM algorithm thus boosting its performance under noisy and unexpected data acquisition conditions. In addition, it also speeds up the convergence process of the algorithm. Experiments using simulated and real, both normal and pathological, MRI volumes of the human brain show that the proposed approach has considerable better segmentation accuracy, robustness against noise, and faster response compared with several well-known fuzzy and non-fuzzy techniques reported in the literature.	algorithm;data acquisition;fuzzy clustering;fuzzy cognitive map;fuzzy logic;resonance	Moumen T. El-Melegy;Hashim Mokhtar	2014	EURASIP J. Image and Video Processing	10.1186/1687-5281-2014-21	computer vision;computer science;archaeology;machine learning;pattern recognition;biometrics	Vision	43.35556932639621	-72.7628492456245	90795
722d54dd79f139ec45b23296839b2393cd768c8e	shadow-based building detection and segmentation in high-resolution remote sensing image	remote sensing image;shadow detection;lda;superpixel;building detection	This paper proposes an effective method to extract buildings in high-resolution remote sensing images based on shadow detection. Firstly, a superpixel segmentation algorithm called SLIC is introduced to split the input image into homogeneous patches. LDA-based color features of the patches are extracted for detecting shadow regions. According to the positions of the shadows, an adaptive strategy for seed location and regional growth is developed to accomplish the coarse detection of buildings. Finally, buildings are extracted accurately using Level Set segmentation algorithm. The experimental results prove that the proposed method is applicable in various complicated situations and is more robust and precise as compared with other competing algorithms		Dongyue Chen;Shibo Shang;Chengdong Wu	2014	Journal of Multimedia	10.4304/jmm.9.1.181-188	computer vision	Vision	46.1040098295233	-67.00150627906923	90992
c36580084891002519a0181d1f8224836b449faa	space-variant smoothing in median-regularized reconstruction for transmission tomography			ct scan;smoothing;tomography	Ji-Eun Jung;Soo-Jin Lee	2017		10.2352/ISSN.2470-1173.2017.17.COIMG-446	iterative reconstruction;algorithm;transmission tomography;smoothing;tomographic reconstruction;computer science	Vision	52.997591451010216	-80.156696966501	91839
b29719dfba1a41f89d0519f7363df595b29d3b0d	adaptive-threshold region merging via path scanning	silicon;image segmentation image resolution;merging image segmentation measurement silicon bismuth image resolution clustering algorithms;image segmentation;measurement;image resolution;bismuth;human segmentation adaptive threshold region merging algorithm path scanning image segmentation techniques region homogeneity semigreedy criterion segmentation resolution control relative performance indicator;merging;clustering algorithms	Region merging algorithms commonly produce results that are seen to be far below the current commonly accepted state-of-the-art image segmentation techniques. The main challenging problem is the selection of an appropriate and computationally efficient method to control resolution and region homogeneity. In this paper we present a region merging algorithm that includes a semi-greedy criterion and an adaptive threshold to control segmentation resolution. In addition we present a new relative performance indicator that compares algorithm performance across many metrics against the results from human segmentation. Qualitative (visual) comparison demonstrates that our method produces results that outperform existing leading techniques.	algorithmic efficiency;dvd region code;greedy algorithm;image segmentation;semiconductor industry	Gaurav Gupta;Alexandra Psarrou	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.173	image texture;computer vision;range segmentation;image resolution;computer science;machine learning;segmentation-based object categorization;bismuth;data mining;region growing;image segmentation;cluster analysis;silicon;minimum spanning tree-based segmentation;scale-space segmentation;measurement	Vision	44.45987837809507	-70.54133338564642	92354
4ee30bbc5e3044c031c977c0eebe9468e6134fee	morphons: paint on priors and elastic canvas for segmentation and registration	modelizacion;viscosity;anisotropie;image acoustique;anisotropia;medical and health sciences;acoustic image;filter bank;image processing;banc filtre;data interpretation;ultrasound;medicin och halsovetenskap;ultrasonic imaging;anisotropy;prior information;procesamiento imagen;imagen acustica;segmentation;traitement image;formation image ultrason;paint;formacion imagen ultrasonico;registro imagen;modelisation;brightness;viscosidad;stopping criterion;informacion a priori;brillance;interpreteur;recalage image;image registration;banco filtro;peinture;image sequence;filtro adaptable;secuencia imagen;viscosite;interpreter;filtre adaptatif;pintura;estimacion adaptativa;modeling;brillantez;interprete;deformable model;information a priori;adaptive filter;segmentacion;adaptive estimation;sequence image;estimation adaptative	This paper presents a new robust approach for registration and segmentation. Segmentation as well as registration is attained by morphing of an N -dimensional model, the Morphon, onto the N dimensional data. The approach is general and can, in fact, be said to encompass much of the deformable model ideas that have evolved over the years. However, in contrast to commonly used models, a distinguishing feature of the Morphon approach is that it allows an intuitive interface for specifying prior information, hence the expression paint on priors. In this way it is simple to design Morphons for specific situations. The priors determine the behavior of the Morphon and can be seen as local data interpreters and response generators. There are three different kinds of priors: material parameter fields (elasticity, viscosity, anisotropy etc.), context fields (brightness, hue, scale, phase, anisotropy, certainty etc.) and global programs (filter banks, estimation procedures, adaptive mechanisms etc.). The morphing is performed using a dense displacement field. The field is updated iteratively until a stop criterion is met. Both the material parameter and context fields are addressed via the present displacement field. In each iteration the neighborhood operators are applied, using both data and the displaced parameter fields, and an incremental displacement field is computed. An example of the performance is given using a 2D ultrasound heart image sequence where the purpose is to segment the heart wall. This is a difficult task even for trained specialists yet the Morphon generated segmentation is highly robust. Further it is demonstrated how the Morphon approach can be used to register the individual images. This is accomplished by first finding the displacement field that aligns the morphon model with the heart wall structure in each image separately and then using the displacement field differences to align the images.	align (company);displacement mapping;elasticity (data store);filter bank;iteration;morphing	Hans Knutsson;Mats T. Andersson	2005		10.1007/11499145_31	computer vision;simulation;image processing;computer science;artificial intelligence;anisotropy	Vision	53.29602821701637	-67.96763099212427	92677
8ffdf8c1c954b39175afb4f54493324274b1a9b1	feature-based registration of medical images: estimation and validation of the pose accuracy	mahalanobis distance;geometric feature;medical image;gradient descent;least square	We provide in this article a generic framework for pose estimation from geometric features. We propose more particularly two algorithms: a gradient descent on the Riemannian least squares distance and on the Mahalanobis distance. For each method, we provide a way to compute the uncertainty of the resulting transformation. The analysis and comparison of the algorithms show their advantages and drawbacks and point out the very good prediction on the transformation accuracy. An application in medical image analysis validates the uncertainty estimation on real data and demonstrates that, using adapted and rigorous tools, we can detect very small modiications in medical images. We believe that these algorithms could be easily embedded in many applications and provide a thorough basis for computing many image statistics.	algorithm;embedded system;gradient descent;image analysis;least squares;medical image computing;medical imaging;motion estimation;scene statistics	Xavier Pennec;Charles R. G. Guttmann;Jean-Philippe Thirion	1998		10.1007/BFb0056300	gradient descent;computer vision;computer science;mahalanobis distance;machine learning;pattern recognition;mathematics;least squares;statistics	Vision	46.52151759155998	-76.66837120594684	92998
37ba51527cb45d2cd3e0b5d94a74844c13d383c7	unsupervised texture segmentation using active contours driven by the chernoff gradient flow	active contours unsupervised texture image segmentation chernoff gradient flow textural region geometry shape operator beltrami framework probability density function total variation norm;minimization;active contour;probability;image segmentation;manifolds;kernel density estimation;probability density function;active contours image segmentation tensile stress probability density function shape measurement geometry filters laboratories hospitals biomedical measurements;geometry;computational geometry;total variational;unsupervised segmentation;active contours;texture segmentation;chernoff distance;image texture;density functional theory;shape;kernel density estimate;mathematical model;gradient methods;probability computational geometry density functional theory gradient methods image segmentation image texture;texture region descriptor;total variation;chernoff disatnce;total variational texture region descriptor chernoff disatnce active contours kernel density estimation;gradient flow;active contour model	We present a new unsupervised segmentation of textural images based on integration of a texture descriptor in the formulation of active contour. The proposed texture descriptor intrinsically describes the geometry of textural regions using the shape operator defined in Beltrami framework. We use the Chernoff distance to define an active contours model which discriminates textures by maximizing the distance between the probability density functions which leads to distinguish textural objects of interest and background described by texture descriptor. We prove the existence of a solution to the new formulated active contours based segmentation model and we propose a fast and easy algorithm based on the dual formulation of the Total Variation norm. Finally, we show results on challenging images to illustrate accurate segmentations that are possible.	active contour model;algorithm;chernoff bound;gradient descent;unsupervised learning	Foued Derraz;Abdelmalik Taleb-Ahmed;Nacim Betrouni;Azeddine Chikh;Antonio Pinti;Fethi Bereksi-Reguig	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413423	kernel density estimation;computer vision;topology;computational geometry;pattern recognition;active contour model;mathematics;statistics	Vision	49.582550343362435	-71.3803390871618	93017
7b66e1991aac67651eddebf59a32173ccc09a5a2	automatic robust threshold finding aided by fuzzy information granulation	brain;white matter;image segmentation;image matching;biomedical nmr;canonical atlas images fuzzy information granulation automatic robust threshold finding human brain mr image segmentation gray matter white matter cerebrospinal fluid gray level image histogram fuzzy matching technique brain region segmentation experiment brain region extraction;mr imaging;feature extraction;medical image processing;image matching brain image segmentation feature extraction medical image processing biomedical nmr diagnostic radiography fuzzy systems;gray matter;robustness humans histograms image segmentation fuzzy sets hair head nose forehead brain modeling;human brain;fuzzy systems;diagnostic radiography;cerebrospinal fluid	This paper presents a robust automatic threshold finding method for the human brain MR image segmentation. The method is based on fuzzy information granulation shown by Zadeh (see Abstract of BISC Seminar, 1996). The human brain MR image consists of several parts; the gray matter, white matter, cerebrospinal fluid and so on. By treating their parts as the fuzzy granules in the gray level histogram of the image and developing a fuzzy matching technique, we can find the required thresholds and can segment the brain region from the MR image. An experiment is done on 50 gray level histograms of the human brain MR volumes. To evaluate our method, we extract the brain region using the obtained thresholds. A comparison of the obtained region with canonical atlas images shows that our method find the thresholds of the gray matter and white matter correctly.		Syoji Kobashi;Naotake Kamiura;Yutaka Hata;Makoto Ishikawa	1997		10.1109/ICIP.1997.648020	computer vision;feature extraction;computer science;artificial intelligence;machine learning;image segmentation;fuzzy control system	HCI	42.06016174852337	-73.86159424659304	93068
037e556a8abc768171d2a88a572fed3034c94cee	active contours driven by local rayleigh distribution fitting energy for ultrasound image segmentation			curve fitting;image segmentation;rayleigh–ritz method	Hui Bi;Yibo Jiang;Hui Li;Xuan Sha;Yi Wang	2018	IEICE Transactions		pattern recognition;computer vision;artificial intelligence;image segmentation;computer science;rayleigh distribution;ultrasound	Vision	44.42683292023604	-74.1019291762564	93219
677ba3b7054310fbe5e062e36ffc5d1741aa6def	twister segment morphological filtering. a new method for live zebrafish embryos confocal images processing	image segmentation;image processing;cell membranes twister segment morphological filtering live zebrafish embryos confocal images processing noise removal confocal laser scanning microscopy;microscopy biomedical image processing image restoration morphological operations;microscopy;morphological operation;image restoration;indexing terms;embryos;morphological operations;medical image processing cellular biophysics filtering theory image denoising image segmentation;medical image processing;biomedical image processing;cell membrane;image segmentation embryo image processing image reconstruction computational efficiency microscopy biomedical imaging information filtering information filters biomembranes;confocal laser scanning microscopy;image denoising;cellular biophysics;filtering theory	We propose an extension of the classical morphological filtering based on openings by line segment structuring elements. It consists in filtering a 3D+time image with the opening by all the possible rotations of a segment in the 4D space, given an initial segment and a rotation angle. The method has been applied to remove noise from confocal laser scanning microscopy images of live zebrafish embryos engineered to fluorescently label all their cell membranes.	mathematical morphology;twister	Miguel A. Luengo-Oroz;Emmanuel Faure;Benoit Lombardot;Rosario Sance;Paul Bourgine;Nadine Peyriéras;Andrés Santos	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379813	image restoration;embryo;computer vision;index term;image processing;computer science;microscopy;confocal laser scanning microscopy;image segmentation;computer graphics (images)	Robotics	44.8782576773585	-71.17323744080706	93301
af122304fc86d168119fb80820fde3838fc56d4c	active contours driven by region-scalable fitting and optimized laplacian of gaussian energy for image segmentation	laplacian of gaussian;image segmentation;active contours;level set method;region scalable fitting	It had been known that the famous region scalable-fitting model can segment the images with intensity inhomogeneity effectively, but it largely depends on the position of initial contour. In this paper, an active contour model which combines region-scalable fitting energy and optimized Laplacian of Gaussian (LoG) energy is proposed for image segmentation. We first present a LoG energy term optimized by an energy functional which can smooth the homogeneous regions and enhance edge information at the same time. Then, we integrate the optimized LoG energy term with the region-scalable fitting energy term which makes use of local region information to drive the curve towards the boundaries. With the addition of LoG term, the proposed model is insensitive to the positions of initial contour and realizes an accurate segmentation result. Experiments on some synthetic and real images have proved that the proposed model not only has a good robustness of initialization, but also has a higher segmentation accuracy and efficiency than other major region-based models.	active contour model;blob detection;contour line;experiment;image segmentation;mathematical optimization;reasonable server faces;scalability;synthetic intelligence	Keyan Ding;Linfang Xiao;Guirong Weng	2017	Signal Processing	10.1016/j.sigpro.2016.12.021	computer vision;mathematical optimization;blob detection;computer science;pattern recognition;mathematics;image segmentation;scale-space segmentation;level set method	Vision	49.87544413346256	-71.18206004553886	93314
91bfd39705efaa4dcb2fe704609847614bd2dcff	an algorithm using projection onto subspace of prior distributions for long-wavelength sound wave ct	computerized axial tomography;tomodensitometria;medical imagery;image processing;bioacoustics;computed tomography;etude theorique;genie biomedical;algorithme marquardt;algorithms densitometry humans mathematics tomography x ray computed ultrasonics;regularization method;acoustic tomography;simulation;prior information;procesamiento imagen;prior distribution;computerised tomography inverse problems acoustic tomography bioacoustics image reconstruction medical image processing;echography;simulacion;indexing terms;problema inverso;ley a priori;onde acoustique;traitement image;algorithme;algorithm;subspace;reconstruction image;computed tomography inverse problems reconstruction algorithms conductivity shape measurement cost function acoustical engineering computational modeling surface waves surface reconstruction;long wavelength;tomodensitometrie;informacion a priori;exploration ultrason;medical diagnostic imaging prior distributions subspace subspace regularization method projection long wavelength sound wave ct reconstruction algorithm prior information marquardt reconstruction algorithms;biomedical engineering;inverse problem;reconstruccion imagen;image reconstruction;medical image processing;estudio teorico;computerised tomography;imagineria medica;imagerie medicale;sous espace;acoustic wave;exploracion ultrasonido;ingenieria biomedica;theoretical study;reconstruction algorithm;nonlinear inverse problem;algorithme spmm;probleme inverse;information a priori;ecografia;echographie;loi a priori;sonography;inverse problems;onda acustica;algoritmo	The stationary long-wavelength sound wave computed tomography is a nonlinear inverse problem that requires the use of prior information of the object. However, the prior assumptions that are usually used in similar inverse problems are more or less inappropriate. Here, a new reconstruction algorithm using the prior information is proposed and compared with subspace regularization method and Marquardt reconstruction algorithms. The simulation shows that the proposed algorithm can give a better reconstructed result whether the actual distribution is compatible or incompatible with the prior distributions.	algorithm;ct scan;electromagnetically induced transparency;iteration;levenberg–marquardt algorithm;nonlinear system;simulation;software incompatibility;stationary process;tomography;tomography, emission-computed;x-linked emery-dreifuss muscular dystrophy;wavelength	Juzhong Dong;Yuukou Horita;Tadakuni Murai	2001	IEEE Transactions on Medical Imaging	10.1109/42.932743	computer vision;radiology;image processing;computer science;inverse problem;mathematics;optics	Vision	52.78261417917573	-78.86294646269432	93376
5e9b56d18ce7e0b5232de7ababd53d3525cb2a65	tagged volume rendering of the heart	active contour;volume rendering;fast marching;transfer function;coronary artery	We present a novel system for 3-D visualisation of the heart and coronary arteries. Binary tags (generated offline) are combined with value-gradient transfer functions (specified online) allowing for interactive visualisation, while relaxing the offline segmentation criteria. The arteries are roughly segmented using a Hessian-based line filter and the pericardial cavity using a Fast Marching active contour. A comparison of different contour initialisations reveals that simple geometric shapes (such as spheres or extruded polygons) produce suitable results.	active contour model;arterial system;body cavities;contour line;coronary artery;fast fourier transform;gradient;heart diseases;hessian;interactive visualization;interactivity;myocardial ischemia;online and offline;pericardial cavity;promotion (action);receiver operator characteristics;relaxation;sensitivity and specificity;tracer;transfer function;volume rendering;web application;biologic segmentation	Daniel Mueller;Anthony J. Maeder;Peter J. O'Shea	2007	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-540-75757-3_24	computer vision;computer science;fast marching method;active contour model;transfer function;volume rendering;computer graphics (images)	Visualization	40.286686973208255	-79.17168027626074	93466
78b3c3c64d404beb13ce9ba526b86b9725cd2acb	matlab graphic user interface for image segmentation using markov random fields and entropy estimation with parallel processing	computational time graphic user interface gui image segmentation markov random fields parallel processing nonparametric entropy estimation probabilistic techniques mrf single visual environment matlab software;mathematics computing;image segmentation;image segmentation matlab computational modeling mathematical model probabilistic logic artificial intelligence brain modeling;parallel processing computational complexity entropy graphical user interfaces image segmentation markov processes mathematics computing;graphical user interfaces;computational complexity;entropy;markov processes;parallel processing image segmentation markov random fields entropy estimation graphic user interface;parallel processing	In this work it is presented, described and tested a new Matlab Graphic User Interface (GUI) for image segmentation of degraded images using two probabilistic techniques, Markov random fields (MRF) and nonparametric entropy estimation. This GUI was created in order to integrate a series of steps needed for the segmentation process into a single visual environment to allow an easier handling of input images and saving of results. It is also used a powerful utility of the Matlab software concerning to parallel processing, with the aim of reduce the computational time because of the high time consumption of this kind of algorithms. Results show a very satisfactory performance of this tool, allowing us to make this task easier and faster.	algorithm;entropy estimation;graphical user interface;image segmentation;matlab;markov chain;markov random field;parallel computing;time complexity	Osvaldo Gutierrez Mata;Alejandro Serna Dominguez;José Ismael de la Rosa Vargas;Jesús Villa Hernández;Efrén González	2014	2014 International Conference on Electronics, Communications and Computers (CONIELECOMP)	10.1109/CONIELECOMP.2014.6808596	parallel processing;computer vision;entropy;maximum-entropy markov model;computer science;theoretical computer science;operating system;machine learning;segmentation-based object categorization;graphical user interface;image segmentation;markov process;scale-space segmentation;computational complexity theory;algorithm;statistics	Robotics	51.35960542336713	-69.05589739855505	93617
343ea690709766b3e5b2ea3fbadb83d30774f691	wspm or how to obtain statistical parametric maps using shift-invariant wavelet processing	discrete wavelet transforms;integrated approach;spm2 software;shift invariant wavelet processing;brain;statistical parametric map;discrete wavelet transform;brain activity detection;performance evaluation;spatial domain;type i error;redundant discrete wavelet transform;statistical test;testing;spm2 software wspm statistical parametric maps shift invariant wavelet processing brain activity detection fmri data spatial discrete wavelet transform statistical test wavelet coefficients spatial domain strong type i error control redundant discrete wavelet transform shift invariant detection scheme;fmri data;shift invariant detection scheme;spatial discrete wavelet transform;statistical analysis;statistical parametric maps;wspm;error correction;medical image processing;discrete wavelet transforms wavelet domain brain performance evaluation testing wavelet coefficients statistical analysis error correction scanning probe microscopy software tools;software tools;statistical testing;brain activation;scanning probe microscopy;wavelet domain;strong type i error control;shift invariant;wavelet coefficients;biomedical mri;statistical testing biomedical mri brain discrete wavelet transforms medical image processing	"""Recently, we have proposed a new framework for detecting brain activity from fMRI data, which is based on the spatial discrete wavelet transform. The standard wavelet-based approach performs a statistical test in the wavelet domain, and therefore fails to provide a rigorous statistical interpretation in the spatial domain. The new framework provides an """"integrated"""" approach: the data is processed in the wavelet domain (by thresholding wavelet coefficients), and a suitable statistical testing procedure is applied afterwards in the spatial domain. This method is based on conservative assumptions only and has a strong type-I error control by construction. At the same time, it has a sensitivity comparable to that of SPM. Here, we discuss the extension of our algorithm to the redundant discrete wavelet transform, which provides a shift-invariant detection scheme. The key features of our technique are illustrated with experimental results. An implementation of our framework is available as a toolbox (WSPM) for the SPM2 software"""	algorithm;coefficient;discrete wavelet transform;electroencephalography;ensemble interpretation;error detection and correction;sensor;strong and weak typing;super paper mario;thresholding (image processing)	Dimitri Van De Ville;Thierry Blu;Michael Unser	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1661472	wavelet;computer vision;statistical hypothesis testing;second-generation wavelet transform;continuous wavelet transform;theoretical computer science;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;statistics;wavelet transform	Robotics	50.581757522323436	-76.84432143129321	93652
1fc3344da80a9b2269217b2b0c07cff509921775	the developing human connectome project: a minimal processing pipeline for neonatal cortical surface reconstruction	cortical surface reconstruction;developing human connectome project;neonatal mri;pipeline;segmentation;dhcp	The Developing Human Connectome Project (dHCP) seeks to create the first 4-dimensional connectome of early life. Understanding this connectome in detail may provide insights into normal as well as abnormal patterns of brain development. Following established best practices adopted by the WU-MINN Human Connectome Project (HCP), and pioneered by FreeSurfer, the project utilises cortical surface-based processing pipelines. In this paper, we propose a fully automated processing pipeline for the structural Magnetic Resonance Imaging (MRI) of the developing neonatal brain. This proposed pipeline consists of a refined framework for cortical and sub-cortical volume segmentation, cortical surface extraction, and cortical surface inflation, which has been specifically designed to address considerable differences between adult and neonatal brains, as imaged using MRI. Using the proposed pipeline our results demonstrate that images collected from 465 subjects ranging from 28 to 45 weeks post-menstrual age (PMA) can be processed fully automatically; generating cortical surface models that are topologically correct, and correspond well with manual evaluations of tissue boundaries in 85% of cases. Results improve on state-of-the-art neonatal tissue segmentation models and significant errors were found in only 2% of cases, where these corresponded to subjects with high motion. Downstream, these surfaces will enhance comparisons of functional and diffusion MRI datasets, supporting the modelling of emerging patterns of brain connectivity.	best practice;downstream (software development);entity name part qualifier - adopted;evaluation;freesurfer;human connectome project;magnetic resonance imaging;menstruation;pipeline (computing);post-traumatic stress disorder;biologic segmentation;brain development;phpmyadmin	Antonios Makropoulos;Emma C. Robinson;Andreas Schuh;Robert Wright;Sean P. Fitzgibbon;Jelena Bozek;Serena J. Counsell;Johannes Steinweg;Katy Vecchiato;Jonathan Passerat-Palmbach;Gregor Lenz;Filippo Mortari;Tencho Tenev;Eugene P. Duff;Matteo Bastiani;Lucilio Cordero-Grande;Emer J. Hughes;Nora Tusor	2018	NeuroImage	10.1016/j.neuroimage.2018.01.054	cognitive psychology;human connectome project;computer vision;connectome;diffusion mri;artificial intelligence;ranging;computer science;bioinformatics	Vision	41.605941808724154	-80.06896819361927	93878
50374c9c378ff5c71fa22c9581094b4ea3329949	ranked k-means clustering for terahertz image segmentation	terahertz wave imaging image sampling image segmentation pattern clustering;simple random sampling segmentation terahertz imaging k means ranked set sampling;observed data clustering ranked k means clustering technique terahertz image segmentation k means version center initialization ranked set sampling design k means technique;image segmentation sociology statistics imaging linear programming indexes clustering algorithms	It is known that k-means clustering is especially sensitive to initial starting centers. In this paper, we propose an original version of k-means for the segmentation of Terahertz images, called ranked-k-means, which is essentially less sensitive to the initialization of the centers. We present the ranked set sampling design and explain how to reformulate the k-means technique under the ranked sample to estimate the expected centers as well as the clustering of the observed data. Our clustering approach is tested on various Terahertz images. Experimental results show that k-means based on the ranked sample is more efficient than other clustering techniques.	cluster analysis;image segmentation;k-means clustering;sampling (signal processing)	Mohamed Walid Ayech;Djemel Ziou	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351636	computer vision;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;cluster analysis;scale-space segmentation	Robotics	42.55567664011651	-66.87790462719853	93955
a3ed03f54f64910eae2cf2aadd2142be048fd60f	object detection in watershed partitioned gray-level images	image segmentation;power method;watershed transform;image representation;image analysis;region growing;object detection	Gray-level image segmentation is the first task for any image analysis process, and is necessary to distinguish the objects of interest from the background. Segmentation is a complex task, especially when the gray-level distribution along the image is such that sets of pixels characterized by a given gray-level are interpreted by a human observer as belonging to the foreground in certain parts of the image, and to the background in other parts, depending on the local context. It very seldom happens that the background is characterized by an almost uniform gray-level. Thus, in the majority of cases, segmentation cannot be achieved by simply thresholding the image, i.e., by assigning all pixels with gray-level lower than a given threshold to the background and all remaining pixels to the foreground. One of the most often adopted segmentation techniques is based on a preliminary partition of the input gray-level image into regions, homogeneous with respect to a given property, to successively classify the obtained regions in two classes (foreground and background). In this paper, we follow this approach and present a powerful method to discriminate regions in a partition of a gray-level image obtained by using the watershed transformation. The basic idea underlying the classification is that for a wide class of graylevel images, e.g., a number of biological images, the boundary between the foreground and the background is perceived where locally maximal changes in gray-level occur through the image. Our classification procedure works well even starting from a standard watershed partition, i.e., without resorting to seed selection and region growing. However, we will also briefly discuss new criteria to be used when applying digging and flooding techniques in the framework of watershed transformation, so as to produce a less fragmented partition of the image. By using the so obtained partition of the gray-level image, the successive classification is facilitated and the quality of the obtained results is improved. Some hints regarding the use of multi-scale image representation to reduce the computational load will also be introduced.	object detection;watershed (image processing)	Maria Frucci;Gabriella Sanniti di Baja	2007		10.1007/978-3-540-76300-0_10	image texture;image restoration;computer vision;feature detection;range segmentation;background subtraction;orientation;binary image;machine learning;segmentation-based object categorization;free boundary condition;pattern recognition;image subtraction;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Vision	45.26154448678449	-66.64344603406215	93957
1cd156cb51cfc30a72577d05f162c96a94d34207	incorporating domain knowledge into medical image clustering	objet;cluster algorithm;object clustering;analisis numerico;explotacion minera;cluster;matematicas aplicadas;aplicacion;measurement;mathematiques appliquees;amas;mining;connaissance;exploitation miniere;metodo imagen;object;image classification;image;conocimiento;data mining;image clustering;similitude;analyse numerique;symetrie;symmetry;algorithme;domain knowledge;algorithm;knowledge;domaine;numerical analysis;medical image;imagen;medida;clustering;region of interest;image method;classification image;region;similarity;image mining;brain imaging;domains;monton;mesure;similitud;simetria;applied mathematics;application;methode image;objeto;domain specificity;algoritmo	Image mining is more than just an extension of data mining to image domain but an interdisciplinary endeavor. Very few people have systematically investigated this field. Clustering medical images is an important part in domain-specific application image mining because there are several technical aspects which make this problem challenging. In this paper, we firstly quantify the domain knowledge about brain image (especially the brain symmetry), and then incorporate this quantified measurement into the clustering algorithm. Our algorithm contains two parts: (1) clustering regions of interest (ROI) detected from brain image; (2) clustering images based on the similarity of ROI. We apply the method to cluster brain images and present results to demonstrate its usefulness and effectiveness.	cluster analysis	Haiwei Pan;Jianzhong Li;Wei Zhang	2007	Applied Mathematics and Computation	10.1016/j.amc.2006.06.083	correlation clustering;mathematical analysis;mining;region;similarity;fuzzy clustering;numerical analysis;artificial intelligence;object;similitude;image;consensus clustering;data mining;mathematics;knowledge;symmetry;cluster analysis;domain knowledge;algorithm;measurement;neuroimaging;cluster;clustering high-dimensional data;region of interest	Vision	39.893182913155535	-69.30699196712467	94076
89b90cd1b92fa3d6bf54d933f766ed9a380d7712	fast geodesic regression for population-based image analysis		Geodesic regression on images enables studies of brain development and degeneration, disease progression, and tumor growth. The high-dimensional nature of image data presents significant computational challenges for the current regression approaches and prohibits large scale studies. In this paper, we present a fast geodesic regression method that dramatically decreases the computational cost of the inference procedure while maintaining prediction accuracy. We employ an efficient low dimensional representation of diffeomorphic transformations derived from the image data and characterize the regressed trajectory in the space of diffeomorphisms by its initial conditions, i.e., an initial image template and an initial velocity field computed as a weighted average of pairwise diffeomorphic image registration results. This construction is achieved by using a first-order approximation of pairwise distances between images. We demonstrate the efficiency of our model on a set of 3D brain MRI scans from the OASIS dataset and show that it is dramatically faster than the state-of-the-art regression methods while producing equally good regression results on the large subject cohort.	clinical use template;color gradient;computation;computational complexity theory;distance;hepatolenticular degeneration;image analysis;image registration;inference;initial condition;mri scans;neoplasms;order of approximation;progressive disease;silo (dataset);velocity (software development);brain development;cell transformation	Yi Hong;Polina Golland;Miaomiao Zhang	2017	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-319-66182-7_37	vector field;computer science;artificial intelligence;pattern recognition;geodesic;regression;pairwise comparison;image registration;inference;trajectory;population	Vision	44.443150141667886	-79.04098879076956	94122
61b40f78610906e4dd8a0c25195c77cf377bd55e	automatic multi-model-based segmentation of the left atrium in cardiac mri scans	single model;accurate segmentation result;left atrium;correct model;cardiac mri;model-based segmentation approach;automatic model selection;fitting model;accurate segmentation;different model;automatic multi-model-based segmentation;average segmentation result	Model-based segmentation approaches have been proven to produce very accurate segmentation results while simultaneously providing an anatomic labeling for the segmented structures. However, variations of the anatomy, as they are often encountered e.g. on the drainage pattern of the pulmonary veins to the left atrium, cannot be represented by a single model. Automatic model selection extends the model-based segmentation approach to handling significant variational anatomies without user interaction. Using models for the three most common anatomical variations of the left atrium, we propose a method that uses an estimation of the local fit of different models to select the best fitting model automatically. Our approach employs the support vector machine for the automatic model selection. The method was evaluated on 42 very accurate segmentations of MRI scans using three different models. The correct model was chosen in 88.1% of the cases. In a second experiment, reflecting average segmentation results, the model corresponding to the clinical classification was automatically found in 78.0% of the cases.	anatomic structures;handling (psychology);heart atrium;left atrial structure;mri scans;model selection;pulmonary veins;scanning;science of anatomy;support vector machine;variational principle;x-ray microtomography;biologic segmentation	Dominik Kutra;Axel Saalbach;Helko Lehmann;Alexandra Groth;Sebastian P. M. Dries;Martin W. Krüger;Olaf Dössel;Jürgen Weese	2012	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-33418-4_1	computer vision;scale-space segmentation;anatomy	Vision	41.934631744975654	-79.2021271758698	94165
bdeaddc755f11cdec19ce61b94c7341fb3a819a8	a model based contour searching method	model based contour searching method;spline;brain;image segmentation;edge detection;search method;model based approach;biomedical imaging;deformable models;mr brain images;mri model based contour searching method biomedical contour image regionalized a posteriori probability model mr brain images deformable contour method first order approximation a posteriori probability model medical diagnostic imaging;wavelet transforms;deformable contour method;first order;shape;a posteriori probability model;medical image processing;biomedical contour image;fourier transforms;mri;brain imaging;modelling edge detection medical image processing brain biomedical mri;probability model;wavelet domain;regionalized a posteriori probability model;image segmentation deformable models biomedical imaging fourier transforms spline wavelet transforms biomedical computing brain shape wavelet domain;biomedical computing;first order approximation;medical diagnostic imaging;biomedical mri	A two-step model based approach to a contour extraction problem is developed to provide a solution to more challenging contour extraction problenzs of bionzedical iniages. A biomedical contour image is initially processed by a deforniable contour method to obtain a first order approximation of the contour. The two-step model includes a linked contour model and a posteriori probability niodel. Initially, the output contour from the deformable contour method is matched against the linked contour model for both model detection and corresponding landmark contour points identification. Segments obtained f iom these landmarks are matched for errors. Larger error segments are then passed on to a regionalized a posteriori probability model for further fine tuning to obtain a final result. Experiments on both MR brain images are most encouraging.	contour line;experiment;order of approximation	Yingjie Tang;Lei He;Xun Wang;William G. Wee	2000		10.1109/BIBE.2000.889627	medical imaging;spline;fourier transform;computer vision;speech recognition;edge detection;shape;computer science;magnetic resonance imaging;pattern recognition;first-order logic;active contour model;image segmentation;orders of approximation;neuroimaging;wavelet transform	Vision	43.91394809105887	-76.19935420892482	94274
aee5dea499c0183379d3eb303c84b78e88c3de01	geodesic saliency propagation for image salient region detection	object detection differential geometry image segmentation;image segmentation;saliency map;saliency propagation;differential geometry;saliency detection;geodesic distance;saliency map saliency detection geodesic distance saliency propagation;visual comparisons image salient region detection geodesic saliency propagation method salient objects detection saliency energy coarse saliency map global contrast harris convex hull superpixels presegmented image image preprocessing geodesic distance objects local connectivity uniform saliency map rendering background suppression;object detection	This paper proposes a novel geodesic saliency propagation method where detected salient objects may be isolated from both the background and other clutter by adding global considerations in the detection process. The method transmits saliency energy from a coarse saliency map to all image parts rather than from image boundaries in conventional cases. The coarse saliency map is computed using the combination of global contrast and Harris convex hull. Superpixels from pre-segmented image are used as pre-processing to further enhance the efficiency. The proposed propagation is geodesic distance assisted and retains the local connectivity of objects. It is capable of rendering a uniform saliency map while suppressing the background, leading to salient objects being popped out. Experiments were conducted on a benchmark dataset, visual comparisons and performance evaluations with 9 existing methods have shown that the proposed method is robust and achieves the state-of-the-art performance.	benchmark (computing);clutter;convex hull;distance (graph theory);experiment;harris affine region detector;preprocessor;software propagation	Keren Fu;Chen Gong;Irene Y. H. Gu;Jie Yang	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738675	differential geometry;computer vision;geodesic;computer science;kadir–brady saliency detector;machine learning;pattern recognition;mathematics;image segmentation	Vision	46.87415638035387	-66.71504223316306	94341
aae4012399d0a1955138587e67bd76883b81f154	multi-scale vessel boundary detection	contrast enhanced;front propagation;mean shift;smoothing parameter;multiple scales;mr imaging;cross sectional area;cross section;boundary detection	In this paper, we present a robust and accurate method for the segmentation of cross-sectional boundaries of vessels found in contrast-enhanced (CE) CTA and MRA images. The proposed algorithm first detects the edges along 1D rays in multiple scales by using mean-shift analysis. Second, edges from different scales are accurately and efficiently combined by using the properties of mean-shift clustering. Third, boundaries of vessel cross-sections are obtained by using local and global perceptual edge grouping and elliptical shape verification. The proposed algorithm is stable to (i) the case where the vessel is surrounded by other vessels or other high contrast structures, (iii) contrast variations in vessel boundary, and (iii) variations in the vessel size and shape. The accuracy of the algorithm is shown on several MRA and CTA examples.	algorithm;cluster analysis;cross-sectional data;mean shift;sensor	Hüseyin Tek;Alper Ayvaci;Dorin Comaniciu	2005		10.1007/11569541_39	computer vision;mathematical optimization;mathematics;cross section;geometry	Vision	46.321645710644944	-72.33445509784883	94507
069510adddea0d5da9b8b384fb1caa5c3936e271	superpixels via pseudo-boolean optimization	minimisation;benchmarking;pseudo boolean function;image segmentation;image processing;boolean functions;edge detection;smoothing method;minimisation image segmentation;bottles;superpixels;processing time;conference paper;shape optimization;smoothing methods;shape;image edge detection;image color analysis;strips image color analysis smoothing methods image segmentation image edge detection shape optimization;algorithms;pseudo boolean optimization;optimization;strips;pseudo boolean;superpixel segmentation pseudo boolean optimization pseudo boolean function minimisation superpixel creation;keywords benchmark datasets	We propose an algorithm for creating superpixels. The major step in our algorithm is simply minimizing two pseudo-Boolean functions. The processing time of our algorithm on images of moderate size is only half a second. Experiments on a benchmark dataset show that our method produces superpixels of comparable quality with existing algorithms. Last but not least, the speed of our algorithm is independent of the number of superpixels, which is usually the bottle-neck for the traditional algorithms of superpixel creation.	algorithm;benchmark (computing);experiment;iterative method;mathematical optimization;pseudo-boolean function	Yuhang Zhang;Richard I. Hartley;John Mashford;Stewart Burn	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126393	computer vision;minimisation;strips;edge detection;image processing;shape;computer science;shape optimization;machine learning;pattern recognition;mathematics;image segmentation;boolean function;benchmarking	Vision	45.64824033838304	-69.71108936740019	94611
08b81a2b84efcc78e56ad05d22b06416d2eb5786	knowledge based fuzzy information fusion applied to classification of abnormal brain tissues from mri	radiology;fuzzy classification;brain;information model;fuzzy set;tissues;three dimensions;proton density;proton density feature images;fuzzy relation;tumours;magnetic resonance imaging neoplasms humans fuzzy sets image sequences radiology data mining medical services biomedical equipment image analysis;image classification;data mining;fuzzy set theory;magnetic resonance image;fuzzy sets;multispectral magnetic resonance images;three dimension space;fuzzy information fusion method;image classification brain tumours biomedical mri sensor fusion fuzzy set theory feature extraction radiology medical image processing;mr imaging;medical services;data matching;tumor area;feature extraction;medical image processing;magnetic resonance imaging;radiology fuzzy information fusion method tissues human brain three dimension space multispectral magnetic resonance images proton density feature images data matching fuzzy classification tumor area;image analysis;information modelling;humans;information fusion;sensor fusion;neoplasms;brain tissue;human brain;biomedical equipment;biomedical mri;image sequences;knowledge base	A fuzzy information fusion method is proposed in this paper. It can automatically classify abnormal tissues in human brain in a three dimension space from multispectral magnetic resonance images such as T1-weighted, T2-weighted and proton density feature images. It consists of four steps: data matching, information modelling, information fusion and fuzzy classification. Several fuzzy set definitions are proposed to describe the specific observation universal. The fuzzy information models of tumor area in human brain and the particular fuzzy relations that contribute to information fusion and classification are also established. Three MR image sequences of a patient are utilized as an example to show the method performances. The results are appreciated by experts in radiology.		Weibei Dou;Su Ruan;Qingmin Liao;Daniel Bloyet;Jean-Marc Constans	2003		10.1109/ISSPA.2003.1224795	computer vision;computer science;magnetic resonance imaging;machine learning;pattern recognition;data mining;fuzzy set	Vision	41.63644941426847	-72.94982506514923	94670
27bcc980ea110652194a2f556e6cadad3f8765cb	nonrigid mammogram registration using mutual information	databases;cost function;mammogram registration;radial basis function;mutual information;mammography;nonrigid registration;rigid registration;similarity measure;thin plate spline	Of the papers dealing with the task of mammogram registration, the majority deal with the task by matching corresponding control-points derived from anatomical landmark points. One of the caveats encountered when using pure point-matching techniques is their reliance on accurately extracted anatomical features-points. This paper proposes an innovative approach to matching mammograms which combines the use of a similarity-measure and a point-based spatial transformation. Mutual information is a cost-function used to determine the degree of similarity between the two mammograms. An initial rigid registration is performed to remove global differences and bring the mammograms into approximate alignment. The mammograms are then subdivided into smaller regions and each of the corresponding subimages is matched independently using mutual information. The centroids of each of the matched subimages are then used as corresponding control-point pairs in association with the Thin-Plate Spline radial basis function. The resulting spatial transformation generates a nonrigid match of the mammograms. The technique is illustrated by matching mammograms from the MIAS mammogram database. An experimental comparison is made between mutual information incorporating purely rigid behavior, and that incorporating a more nonrigid behavior. The effectiveness of the registration process is evaluated using image differences.	approximation algorithm;image registration;landmark point;loss function;mutual information;radial (radio);radial basis function;thin plate spline	Michael A. Wirth;Jay Narhan;Derek W. S. Gray	2002		10.1117/12.467198	computer vision;computer science;pattern recognition;data mining	Vision	43.174639535110764	-77.91481980129426	94719
8cba2ae57a57af307403e191552c9da2edb4c384	phase- and gvf-based level set segmentation of ultrasonic breast tumors		Automatically extracting breast tumor boundaries in ultrasound images is a difficult task due to the speckle noise, the low image contrast, the variance in shapes, and the local changes of image intensity. In this paper, an improved edge-based active contour model in a variational level set formulation is proposed for semi-automatically capturing ultrasonic breast tumor boundaries. First, we apply the phase asymmetry approach to enhance the edges, and then we define a new edge stopping function, which can increase the robustness to the intensity inhomogeneities. To extend the capture range of the method and provide good convergence to boundary concavities, we use the phase information to obtain an improved edge map, which can be used to calculate the gradient vector flow GVF . Combining the edge stopping term and the improved GVF in the level set framework, the proposed method can robustly cope with noise, and it can extract the low contrast and/or concave boundaries well. Experiments on breast ultrasound images show that the proposed method outperforms the state-of-art methods.		Liang Gao;Xiaoyun Liu;Wufan Chen	2012	J. Applied Mathematics	10.1155/2012/810805	mathematical optimization;machine learning;mathematics	Vision	45.86065441283274	-72.94532838429735	94741
d5f8ecddb4eaf7b83fedef49a0a8348bb79258f1	new approximating gaussian elastic body splines for landmark-based registration of medical images	energy function;mr imaging;medical image;image registration;approximation scheme;human brain	We introduce a new approximation scheme for landmark-based elastic image registration using Gaussian elastic body splines (GEBS). The scheme is based on an extended energy functional related to the Navier equation under Gaussian forces and allows to individually weight the landmarks according to their localization uncertainties. We demonstrate the applicability of the registration scheme based on 3D synthetic image data as well as 2D MR images of the human brain. From the experiments it turns out that the new approximating GEBS approach achieves more accurate registration results in comparison to previously proposed interpolating GEBS as well as TPS.	amazon elastic compute cloud (ec2);spline (mathematics)	Stefan Wörz;Karl Rohr	2006		10.1007/3-540-32137-3_42	computer vision;mathematical optimization;image registration;theoretical computer science;mathematics	Vision	47.22590302057929	-76.60212476981094	95014
415360644f8581159894c89175ff393a4871d9d9	a real-time multisensory image segmentation algorithm with an application to visual and x-ray inspection	image segmentation;x ray imaging;curve evolution;real time;fusion rule;real time application;x rays	A new multisensory image segmentation algorithm is presented. In this algorithm, the images from different sensors are segmented in a sequential manner using curve evolution methods. There are no fusion rules involved and no controlling weights to adjust. It is effective in eliminating errors in single modality segmentation, and is fast enough for segmentation in real-time applications. The algorithm is applied to real-time fan bone detection in deboned poultry meat based on visual and x-ray images. Results show that the fusion-based inspection algorithm is efficient, accurate, and robust to registration errors.	algorithm;image segmentation;modality (human–computer interaction);radiography;real-time clock;real-time transcription;real-time web;sensor	Yuhua Ding;George J. Vachtsevanos;Anthony J. Yezzi;Wayne Daley;Bonnie S. Heck-Ferri	2003		10.1007/3-540-36592-3_19	computer vision;computer science;machine learning;segmentation-based object categorization;image segmentation;scale-space segmentation;computer graphics (images)	Vision	41.72404008598213	-71.91393297964068	95060
71b8c13a5f3af09b96c592ad17874c6fc7f0da0a	normalized gaussian distance graph cuts for image segmentation	normalized gaussian distance;image segmentation;normalized cuts;会议论文;spectral clustering;normalized gaussian distance image segmentation spectral clustering normalized cuts kernel k means;kernel k means;spectral analysis gaussian processes graph theory image segmentation pattern clustering;multimedia communication;synthetic data sets fast image segmentation method normalized gaussian distance graph cuts kernel k means normalized cuts spectral clustering efficiency choosing weight avoidance weighted graph cut approach real world images	This paper presents a novel, fast image segmentation method based on normalized Gaussian distance on nodes in conjunction with normalized graph cuts. We review the equivalence between kernel k-means and normalized cuts. Then we extend the framework of efficient spectral clustering and avoid choosing weights in the weighted graph cuts approach. Experiments on synthetic data sets and real-world images demonstrate that the proposed method is effective and accurate.	cluster analysis;cut (graph theory);experiment;image segmentation;k-means clustering;segmentation-based object categorization;spectral clustering;synthetic data;turing completeness	Chengcai Leng;Wei Xu;L. Irene Cheng;Zhihui Xiong;Anup Basu	2015	2015 IEEE International Symposium on Multimedia (ISM)	10.1109/ISM.2015.36	mathematical optimization;computer science;grabcut;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;spectral clustering	Vision	44.86251706330879	-68.17685373301688	95233
183313efd579277e047f04eee058e53a4b1e7863	echocardiographic contour extraction with local and global priors through boosting and level sets	endocardium;spatiotemporal filter;heart;image segmentation;level set;wall motion;boosting level set shape image segmentation data mining spatiotemporal phenomena spatial filters computer vision noise shaping noise level;boosting method;echocardiographic contour extraction;temporal information;shape based global prior;boosting;shape;local features;spatiotemporal filter echocardiographic contour extraction endocardium epicardium boosting method level set method shape based global prior;feature extraction;medical image processing;pixel;medical image processing echocardiography feature extraction;echocardiography;epicardium;level set method	Extracting endocardium and epicardium from echocardiographic images is a challenging task because of large amounts of noise, signal drop-out, unrelated structures, and unseen wall parts. This paper introduces a new technique that automatically extracts cardiac borders by incorporating local and global priors through boosting and level set methods. The shape-based global prior is incorporated into the system by regularly re-initializing the level set surface under the influence of the expert detected contours. The local priors with image and temporal information are learned through boosting. The proposed system has many advantages. First, boosting encodes the knowledge about the image information and the temporal cardiac wall motion effectively by using spatiotemporal filters. Second, the local priors can use any features from the images including different filters and intensity profiles. Furthermore, other hard constraints like local shape, texture, distance, etc. can be added to local features effectively. The system is validated on echocardiograms and the results are found to be promising.	apache axis;boosting (machine learning);contour line;electron hole;image analysis;medical image computing;medical imaging;top-down and bottom-up design	Ayse Betül Oktay;Yusuf Sinan Akgül	2009	2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2009.5204349	computer vision;feature extraction;shape;computer science;level set;machine learning;pattern recognition;mathematics;image segmentation;endocardium;boosting;heart;level set method;pixel	Vision	45.22199948765716	-72.64172406032529	95347
a0277cbba80acc051e3abeb112880896f85f053f	license plate detection in an open environment by density-based boundary clustering	image processing;neural networks;sensors;matrices;pattern recognition	Due to the variation of background, illumination, and view point, license plate detection in an open environment is challenging. We propose a detection method by boundary clustering. To start with, a boundary map is obtained through Canny edge detector and removal of unwanted horizontal background edges. Second, boundaries are classified into different clusters by a density-based approach. In the approach, the density of each boundary is defined by the total gradient intensity of its neighboring and reachable boundaries. Also, the cluster centers and the number of them are determined automatically according to a minimum-distance principle. At last, a set of horizontal candidate regions with accurately located borders are extracted for classification. The classifier is trained on the histogram of oriented gradient feature by a linear support vector machine model. Experiments on three public datasets including images captured under different scenarios demonstrate that the proposed method outperforms several state-of-the-art methods in detection accuracy and its performance in efficiency is also comparable. © 2017 SPIE and IS&T [DOI: 10.1117/1.JEI.26.3.033017]	canny edge detector;cluster analysis;computation;edge detection;effective method;experiment;gradient;interference (communication);logo;reachability;support vector machine	Jiangmin Tian;Guoyou Wang;Jianguo Liu;Yuanchun Xia	2017	J. Electronic Imaging	10.1117/1.JEI.26.3.033017	computer vision;image processing;computer science;sensor;machine learning;pattern recognition;matrix	Vision	46.01707927481303	-66.89551487434481	95360
c4602b3a8931e216abfe6943837153053b4b89ea	automated labeling of neurites in fluorescence microscopy images	fully automated neurite analysis;neurite lengths;fluorescence;labeling fluorescence neurons image analysis bioinformatics biomedical imaging humans large scale systems optical microscopy image resolution;image resolution;manual tracing;fluorescence microscopy images;nervous system;microscopy;biomedical imaging;optical microscopy biomedical optical imaging fluorescence medical image processing neurophysiology;neurite centerline extraction;intricate nervous processes;large scale neurite analysis;large scale;biological activity;fluorescence microscopy;medical image processing;biomedical image processing;image analysis;humans;neurons;neurophysiology;biomedical optical imaging;manual tracing automated neurite labeling fluorescence microscopy images intricate nervous processes image analysis fully automated neurite analysis neurite centerline extraction neurite lengths large scale neurite analysis;automated neurite labeling;optical microscopy;labeling;large scale systems;bioinformatics	To investigate the intricate nervous processes involved in many biological activities by image analysis, accurate and reproducible labeling and measurement of neurites is a prerequisite. We have developed a fully automated neurite analysis method to assist this task. Unlike most of the previous reported manual or semiautomatic methods, our approach is fully automated. Single and connected centerlines along neurites are extracted. The computerized method can also output branching and end points. Due to its multi-scale nature, both thick and thin neurites are simultaneously detected. With the accurate and fully automated extraction of neurite centerlines and measurement of neurite lengths, the proposed method, which greatly reduces human labor and improves efficiency, can serve as a candidate tool for large-scale neurite analysis beyond the capability of manual tracing methods	bioinformatics;communication endpoint;emoticon;image analysis;semantic role labeling;semiconductor industry	Guanglei Xiong;Xiaobo Zhou;Liang Ji;Alexei Degterev;Stephen T. C. Wong	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1624971	fluorescence microscope;computer vision;labeling theory;image analysis;image resolution;medicine;fluorescence;computer science;microscopy;biological activity;nanotechnology;optical microscope;optics;nervous system;neurophysiology	Embedded	39.885986245362496	-77.90231887509552	95435
a03611ea5c44e84234bcfbb2043c2f551023330f	an automatic detection and segmentation algorithm of video multiple moving targets for computer vision	spatial connectivity rate;edge detection;adaptive clustering;perpendicular segmentation	In this paper an automatic detection and segmentation algorithm of video multiple moving targets is proposed for the problem of computer vision in intelligent monitoring system. The algorithm improved the adaptive clustering by defining the pixel spatial connectivity rate. We design the perpendicular split method, initial cluster adaptive splitting and merging self-organizing the iterative clustering segmentation algorithm. It improved the active contour model to complete the edge detection. Experimental results show that multiple moving targets segmentation results are consistent with the human visual judgment, take use of space connectivity information improves the accuracy of clustering segmentation, take use of sparse matrix block operation for active contour model that improves multiple moving targets edge detection result, comparison and analysis the experimental results show that the proposed algorithm is feasible, rapid and effective.	active contour model;algorithm;cluster analysis;computer vision;edge detection;iterative method;organizing (structure);pixel;self-organization;sparse matrix	Kun Zhang;Cuirong Wang;Shaoheng Li	2015		10.1145/2808492.2808540	computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	45.64875271878235	-68.71843441224766	95461
468c9d37e77f155008c7d3f929399207da573ba8	clustering by optimum path forest and its application to automatic gm/wm classification in mr-t1 images of the brain	brain;image foresting transform;phantoms;large dataset;classification tree analysis tree graphs imaging phantoms personal communication networks clustering algorithms image segmentation biomedical image processing probability density function robustness clustering methods;white matter optimum path forest mr t1 images brain optimum path trees phantom automatic gm wm classification gray matter;mean shift;image classification;mr imaging;graph cut;medical image processing;mr image segmentation medical image processing image foresting transform improved mean shift algorithm graph cut measures;phantoms biomedical mri brain image classification medical image processing;biomedical mri	A new approach to identify clusters as trees of an optimum- path forest has been presented. We are extending the method for large datasets with application to automatic GM/WM classification in MR-T1 images of the brain. The method is computed for a few randomly selected voxels, such that GM and WM define two optimum-path trees. The remaining voxels are classified incrementally, by identifying which tree would contain each voxel if it were part of the forest. Our method produces accurate results on phantom and real images, similarly to those obtained by the state-of-the-art, does not rely on templates, and takes less than 1.5 minute on modern PCs.	algorithm;cluster analysis;experiment;feature selection;imaging phantom;phantom reference;randomness;the forest;voxel	Fabio A. M. Cappabianco;Alexandre X. Falcão;Leonardo M. Rocha	2008	2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2008.4541024	computer vision;contextual image classification;cut;mean-shift;computer science;machine learning;pattern recognition;mathematics	Vision	41.88579624883599	-76.51607849413992	95471
0fb388f38c0db11ff65a44a638122ca100018cbc	a unified framework for clustering and quantitative analysis of white matter fiber tracts	sensitivity and specificity;brain;white matter;imaging three dimensional;tract based quantitative analysis;gamma mixture model;prior information;spatial variation;models biological;image enhancement;white matter fiber tracts;cluster analysis;image interpretation computer assisted;expectation maximization;mixture model;likelihood functions;clustering;reproducibility of results;quantitative analysis;models statistical;artificial intelligence;algorithms;pattern recognition automated;humans;curve matching;em algorithm;diffusion tensor mri;nerve fibers myelinated;diffusion magnetic resonance imaging	We present a novel approach for joint clustering and point-by-point mapping of white matter fiber pathways. Knowledge of the point correspondence along the fiber pathways is not only necessary for accurate clustering of the trajectories into fiber bundles, but also crucial for any tract-oriented quantitative analysis. We employ an expectation-maximization (EM) algorithm to cluster the trajectories in a gamma mixture model context. The result of clustering is the probabilistic assignment of the fiber trajectories to each cluster, an estimate of the cluster parameters, i.e. spatial mean and variance, and point correspondences. The fiber bundles are modeled by the mean trajectory and its spatial variation. Point-by-point correspondence of the trajectories within a bundle is obtained by constructing a distance map and a label map from each cluster center at every iteration of the EM algorithm. This offers a time-efficient alternative to pairwise curve matching of all trajectories with respect to each cluster center. The proposed method has the potential to benefit from an anatomical atlas of fiber tracts by incorporating it as prior information in the EM algorithm. The algorithm is also capable of handling outliers in a principled way. The presented results confirm the efficiency and effectiveness of the proposed framework for quantitative analysis of diffusion tensor MRI.	atlases;blast e-value;cluster analysis;distance transform;ephrin type-b receptor 1, human;expectation–maximization algorithm;gamma distribution;handling (psychology);iteration;matching;mixture model;optical fiber;population parameter;providing (action);residential gateway;snord41 gene;snord54 gene;sample variance;sampling - surgical action;similarity measure;sourceforge;tissue fiber;tract (literature);unified framework;united states national institutes of health;white matter;benefit;statistical cluster;unu	Mahnaz Maddah;W. Eric L. Grimson;Simon K. Warfield;William M. Wells	2008	Medical image analysis	10.1016/j.media.2007.10.003	k-medians clustering;expectation–maximization algorithm;computer science;machine learning;pattern recognition;mathematics;cluster analysis;statistics	ML	44.423141762627324	-77.82553049174086	95491
6d8c2660d727db3e7fa0a58040093ff4599c8476	probabilistic minimal path for automated esophagus segmentation	shortest path;segmentation;catheter ablation;esophagus;ellipse fitting	This paper introduces a probabilistic shortest path approach to extract the esophagus from CT images. In this modality, the absence of strong discriminative features in the observed image make the problem ill-posed without the introduction of additional knowledge constraining the problem. The solution presented in this paper relies on learning and integrating contextual information. The idea is to model spatial dependency between the structure of interest and neighboring organs that may be easier to extract. Observing that the left atrium (LA) and the aorta are such candidates for the esophagus, we propose to learn the esophagus location with respect to these two organs. This dependence is learned from a set of training images where all three structures have been segmented. Each training esophagus is registered to a reference image according to a warping that maps exactly the reference organs. From the registered esophagi, we define the probability of the esophagus centerline relative to the aorta and LA. To extract a new centerline, a probabilistic criterion is defined from a Bayesian formulation that combines the prior information with the image data. Given a new image, the aorta and LA are first segmented and registered to the reference shapes and then, the optimal esophagus centerline is obtained with a shortest path algorithm. Finally, relying on the extracted centerline, coupled ellipse fittings allow a robust detection of the esophagus outer boundary.	bayesian approaches to brain function;dijkstra's algorithm;map;modality (human–computer interaction);shortest path problem;well-posed problem	Mikaël Rousson;Ying Bai;Chenyang Xu;Frank Sauer	2006		10.1117/12.653657	computer vision;mathematical optimization;mathematics;scale-space segmentation;anatomy	Vision	42.38009398299063	-79.31563170045891	95515
c39dc98992e66d17a2c2702dcfe7d71e0de35269	longitudinal image registration with non-uniform appearance change	sensitivity and specificity;animals;brain;imaging three dimensional;aging;image enhancement;image interpretation computer assisted;haplorhini;magnetic resonance imaging;reproducibility of results;algorithms;pattern recognition automated;longitudinal studies;subtraction technique	Longitudinal imaging studies are frequently used to investigate temporal changes in brain morphology. Image intensity may also change over time, for example when studying brain maturation. However, such intensity changes are not accounted for in image similarity measures for standard image registration methods. Hence, (i) local similarity measures, (ii) methods estimating intensity transformations between images, and (iii) metamorphosis approaches have been developed to either achieve robustness with respect to intensity changes or to simultaneously capture spatial and intensity changes. For these methods, longitudinal intensity changes are not explicitly modeled and images are treated as independent static samples. Here, we propose a model-based image similarity measure for longitudinal image registration in the presence of spatially non-uniform intensity change.	biological metamorphosis;estimated;galaxy morphological classification;image registration;mathematical morphology;medical imaging;mutual information;similarity measure;cell transformation;registration - actclass	Istvan Csapo;Bradley C. Davis;Yundi Shi;Mar Sanchez;Martin Styner;Marc Niethammer	2012	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-33454-2_35	ageing;computer vision;radiology;medicine;computer science;magnetic resonance imaging	Vision	43.181628581884105	-79.42738371229883	95726
1fb228c721cee70327a0057f623d08f56cabad7e	piecewise-smooth image segmentation models with $$l^1$$ l 1 data-fidelity terms		In this article, we propose a class of piecewise-smooth image segmentation models in a variational framework. The models involve \(L^1\) data fidelity measures and assume that an image can be approximated by the sum of a piecewise-constant function and a smooth function. The smooth function models intensity inhomogeneity, and the \(L^1\) data-fitting terms enable to segment images with low contrast or outliers such as impulsive noise. The regions to be segmented are represented as smooth functions, almost binary functions, instead of the Heaviside expression of level set functions. The existence of minimizers of our main model is shown. Furthermore, we design fast and efficient optimization algorithms based on the augmented Lagrangian method and present a partial convergence result. Numerical results validate the effectiveness of the proposed models compared with other state-of-the-art methods.	image segmentation	Miyoun Jung	2017	J. Sci. Comput.	10.1007/s10915-016-0280-z	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;calculus;mathematics;statistics	AI	52.84088707428011	-71.26548581018017	95922
247e8156571be36b644c521b73e146431335083e	large deformation diffeomorphic registration of diffusion-weighted images with explicit orientation optimization.	sensitivity and specificity;female;brain;male;image enhancement;image interpretation computer assisted;adult;reproducibility of results;artificial intelligence;algorithms;pattern recognition automated;humans;subtraction technique;diffusion magnetic resonance imaging	We seek to compute a diffeomorphic map between a pair of diffusion-weighted images under large deformation. Unlike existing techniques, our method allows any diffusion model to be fitted after registration for subsequent multifaceted analysis. This is achieved by directly aligning the diffusion-weighted images using a large deformation diffeomorphic registration framework formulated from an optimal control perspective. Our algorithm seeks the optimal coordinate mapping by simultaneously considering structural alignment, local fiber reorientation, and deformation regularization. Our algorithm also incorporates a multi-kernel strategy to concurrently register anatomical structures of different scales. We demonstrate the efficacy of our approach using in vivo data and report on detailed qualitative and quantitative results in comparison with several different registration strategies.	anatomic structures;kernel;numerous;optimal control;seizures;tissue fiber;video-in video-out;algorithm;registration - actclass	Pei Zhang;Marc Niethammer;Dinggang Shen;Pew-Thian Yap	2013	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-40763-5_4	computer vision;simulation;computer science;artificial intelligence;machine learning	Vision	43.277600836799394	-79.89756250129362	95971
9a547deb7762428b952cb3bf293d6278f3017b5b	deriving 3d shape properties by using backward wavelet remesher		It is important to determine 3D shape properties of a population of 3D mesh models in biomedical imaging issues. In contrast to conventional 3D shape analysis techniques focusing on applications like shape matching and shape retrieval, we propose in this paper a strategy capable to collect statistical information of multiple triangular mesh models. Our method operates in a coarse-to-fine fashion based on wavelet synthesis. Hence, its analysis result can be invariant against the triangular tiling of the input mesh model. This characteristic enables us to make a comparison among multiple mesh models simultaneously. The experiment results show that our method can extract 3D shape components of each observation scale and is efficient in estimating an average shape and visualizing 3D shape variability of multiple triangular mesh models.	medical imaging;shape analysis (digital geometry);shape context;spatial variability;tiling window manager;wavelet	Hao-Chiang Shao;Wen-Liang Hwang	2017	2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2017.8308630	wavelet;algorithm;wavelet transform;triangle mesh;shape analysis (digital geometry);wavelet packet decomposition;mathematical analysis;population;polygon mesh;invariant (mathematics);computer science	Visualization	46.79758090916727	-75.47552181155736	96005
6afaf4e01c535e4ea2540e4961d76e4cc341ccf0	denoising in magnetic resonance imaging: theory, algorithms and applications		In the context of medical image processing, denoising is widely considered as one of most fundamental postprocessing tasks. In this field, the non-local means (NLM) filter demonstrated to be a robust and performing approach respect to the previous state-of-art denoising methods. As the filtering strength must be tuned to obtain an optimized and customized restoring process, the estimation of image noise variance is an important issue. Althought in clinical practice noise estimation is performed on background (no signal area) of magnitude MR images, in case of parallel MR imaging (pMRI) techniques noise estimation from the image background produces biased results due to spatially varying noise distribution of the pMRI images. A novel NLM approach based on local noise estimation is introduced (hereafter indicated as SVN-NLM).#N##N#As second task, since the susceptibility-weighted imaging (SWI) suffers from reduced SNR due to the high resolution required to obtain a proper contrast generation, a novel pipeline (Multicomponent-Imaginary-Real-SWI, hereafter MIR-SWI) to obtain susceptibility-weighted images with higher SNR and improved conspicuity is proposed. In this context, the application of a denoising filter is non-trivial as the distributions of magnitude and phase noise may introduce biases during image restoration. Taking advantage of the potential multispectral nature of MR images, the multicomponent approach of the MIR-SWI approach performs better than a component-by-component image restoration method.#N##N#Finally, a new strategy to address the computational demand of the NLM filter is investigated. Due to high computational complexity of the NLM denoising filter, in literature several 2D NLM implementations on Graphic Processor Unit (GPU) architectures were proposed. Here a fully 3D NLM implementation on a multi-GPU architecture is presented and its high scalability is suggested.	algorithm;noise reduction;resonance	Pasquale Borrelli	2015			computer vision;electronic engineering;simulation;computer science	Theory	50.915927746537186	-78.32674653583614	96285
b5aa59085ebd6b0a23e0941efc2ab10efb7474bc	spectral–spatial classification and shape features for urban road centerline extraction	geophysical image processing;image segmentation;image fusion;image classification;roads;feature extraction;remote sensing;期刊论文	This letter presents a two-step method for urban main road extraction from high-resolution remotely sensed imagery by integrating spectral-spatial classification and shape features. In the first step, spectral-spatial classification segments the imagery into two classes, i.e., the road class and the nonroad class, using path openings and closings. The local homogeneity of the gray values obtained by local Geary's C is then fused with the road class. In the second step, the road class is refined by using shape features. The experimental results indicated that the proposed method was able to achieve a comparatively good performance in urban main road extraction.	image resolution;statistical classification;stellar classification	Wenzhong Shi;Zelang Miao;Qunming Wang;Hua Zhang	2014	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2013.2279034	computer vision;contextual image classification;feature extraction;computer science;pattern recognition;image segmentation;image fusion;remote sensing	Vision	45.96281563517126	-67.11941109446563	96488
10bebbd8dd7c6a4ceb312dcf1ef070cb0f0ac1f0	automatic vertebral identification using surface-based registration	analisis imagen;computerized axial tomography;tomodensitometria;radiodiagnostic;medical imagery;informatica biomedical;biomedical data processing;spine;lombaire;computed tomography;analyse surface;automatic system;objet test;extraction forme;informatique biomedicale;hombre;colonne vertebrale;radiodiagnostico;tomodensitometrie;sistema automatico;extraccion forma;vertebral column;analisis superficie;systeme osteoarticulaire;sistema osteoarticular;human;imagerie medicale;systeme automatique;lumbar;osteoarticular system;image analysis;imageneria medical;radiodiagnosis;analyse image;objeto prueba;test object;pattern extraction;columna vertebral;homme;surface analysis	This work introduces an enhancement to currently existing methods of intra-operative vertebral registration by allowing the portion of the spinal column surface that correctly matches a set of physical vertebral points to be automatically selected from several possible choices. Automatic selection is made possible by the shape variations that exist among lumbar vertebrae. In our experiments, we register vertebral points representing physical space to spinal column surfaces extracted from computed tomography images. The vertebral points are taken from the posterior elements of a single vertebra to represent the region of surgical interest. The surface is extracted using an improved version of the fully automatic marching cubes algorithm, which results in a triangulated surface that contains multiple vertebrae. We find the correct portion of the surface by registering the set of physical points to multiple surface areas, including all vertebral surfaces that potentially match the physical point set. We then compute the standard deviation of the surface error for the set of points registered to each vertebral surface that is a possible match, and the registration that corresponds to the lowest standard deviation designates the correct match. We have performed our current experiments on two plastic spine phantoms and one patient.© (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Jeannette L. Herring;Benoit M. Dawant	2000		10.1117/12.387701	geography;engineering drawing;surgery;cartography	Vision	45.743927625378085	-79.46675936349826	96574
f5b5b64d2068e72fe0fac4f709f26044bc5bac22	extraction of vascular network in 3d images	real vascular images;medical diagnostic imaging vascular network extraction hyper surface real vascular images synthetic vascular images medical vascular imaging therapy planning ir sup 4 thin network volumic image crest points surgery spiral ct mri;vascular network extraction;anatomical structure;image processing;3d imaging;biomedical nmr;biomedical imaging;synthetic vascular images;data mining;three dimensional;volumic image;hyper surface;surface treatment;shape;thin network;therapy planning;feature extraction;medical image processing;crest points;mri;computerised tomography;surgery;pattern recognition;biomedical nmr medical image processing feature extraction computerised tomography;space technology;intelligent networks;medical vascular imaging;intelligent networks biomedical imaging shape surface treatment space technology anatomical structure image retrieval data mining laboratories pattern recognition;spiral ct;medical diagnostic imaging;ir 4;image retrieval	Thin network extraction from three dimensional (3D) images is a new issue in image processing. It is of major importance in medical vascular imaging for diagnostic, therapy planning and surgery. In this paper, we develop a framework for automatic vascular network extraction from the volumic image. The approach consists in treating the 3D image as a hyper-surface of R4. It is shown that the crest points of this hyper-surface correspond to the center line of the thin network in the image. Promising results are shown on synthetic and real vascular images.	image processing;stereoscopy;synthetic intelligence	Véronique Prinet;Olivier Monga;Songde Ma	1996		10.1109/ICIP.1996.560491	stereoscopy;three-dimensional space;computer vision;intelligent network;image processing;feature extraction;image retrieval;shape;computer science;space technology	Vision	41.82585857637892	-75.24972652135644	96645
ba45684c66c5cfc2f4ac27db58c35ab374ee76b1	accurate inverse consistent non-rigid image registration and its application on automatic re-contouring	energy function;non rigid image registration;maximum likelihood estimate;region of interest;additive operator splitting;similarity measure	This paper provides a novel algorithm for invertible nonrigidimage registration. The proposed model minimizes two energy functionalscoupled by a natural inverse consistent constraint. Both of theenergy functionals for forward and backward deformation fields consista smoothness measure of the deformation field, and a similarity measurebetween the deformed image and the one to be matched. In this proposedmodel the similarity measure is based on maximum likelihood estimationof the residue image. To enhance algorithm efficiency, the Additive OperatorSplitting (AOS) scheme is used in solving the minimization problem.The inverse consistent deformation field can be applied to automatic recontouringto get an accurate delineation of Regions Of Interest(ROIs).The experimental results on synthetic images and 3D prostate data indicatethe effectiveness of the proposed method in inverse consistency andautomatic re-contouring.	image registration	Qingguo Zeng;Yunmei Chen	2008		10.1007/978-3-540-79450-9_28	computer vision;mathematical optimization;mathematics;maximum likelihood;statistics;region of interest	Vision	46.64147829844582	-77.13994972804042	96799
3d8569337ec0184ff0ae693799b185f7bf5bbf24	a novel trilateral filter based adaptive support weight method for stereo matching		The performance of local stereo matching algorithm highly depends on the support window selected, in which the cost are aggregated. A variety of cost aggregation approaches (proposed before 2008) were comprehensively analyzed in [7] and these approaches attempt to seek an optimal support window for each pixel by changing the window size, shape and center offset. The ideal optimal window should satisfy the rule that all pixels in this window lie on the same disparity with the center pixel. Recent years have witnessed a great deal of attention focused on the Adaptive Support Weight (ASW) based methods [1, 4, 6, 8], proposed firstly by Yoon and Kweon in [8]. The ASW methods assign an adaptive weight to each pixel of the support window, depending on how it is likely to lie on the same disparity with the center pixel. Essentially, the assignment of an adaptive weight amounts to changing the support window in terms of size, shape or center offset. In ASW methods, the weight function is very important, because it directly decides the support window. The weight function proposed in [8] is based on bilateral filter. Following this pioneering work, various weight function were proposed, including in particular the segmented bilateral filter weight function [6], the geodesic weight function [1], the guided filter weight function [4]. Thus, which weight function is the most accurate one? Recently, Hosni et al. [2] carried out a comprehensive comparative study to fairly evaluate various weight functions while fixing the preprocessing, matching cost function and post-processing. Their conclusion is that both bilateral filter weight function [8] and guided filter weight function [4] are the best, since bilateral filter weight function performs better on the average rank while guided filter weight function produces a lower average error. We revisit the bilateral filter weight function [8], which obeys two rules that, given a support pixel, (1) if its color is similar to the center pixel’s and (2) if it is spatially close to the center pixel, it is likely to lie on the same disparity with the center pixel. Therefore, the bilateral filter weight function consists of two parts, color similarity term and spatial proximity term, defined as:	algorithm;bilateral filter;binocular disparity;computer stereo vision;loss function;pixel;preprocessor;video post-processing;weight function;window function	Dongming Chen;Mohsen Ardabilian;Liming Chen	2013		10.5244/C.27.96	computer vision	Vision	45.89227771542854	-67.65171766385343	96941
c7076f762dfab8cf7ef4b2b40f98816c70b3bbff	greylevel edge thinning: a new method	image analysis;edge thinning	A comparison is made of the performance of two greylevel thinning algorithm for simple images which contain feature primitives from which more complexes scenes are built. A quantitative assessment of performance is made and a new method proposed.	edge detection;thinning	K. Paler;Josef Kittler	1983	Pattern Recognition Letters	10.1016/0167-8655(83)90079-X	computer vision;image analysis;computer science;computer graphics (images)	Vision	43.604136821601564	-69.19799996176097	97058
fb277247711f8dd274f1a936ebc78954426859f8	a target model construction algorithm for robust real-time mean-shift tracking	health research;uk clinical guidelines;biological patents;asymmetric kernel;europe pubmed central;citation search;mean shift;uk phd theses thesis;object tracking;life sciences;background clutter;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Mean-shift tracking has gained more interests, nowadays, aided by its feasibility of real-time and reliable tracker implementation. In order to reduce background clutter interference to mean-shift object tracking, this paper proposes a novel indicator function generation method. The proposed method takes advantage of two 'a priori' knowledge elements, which are inherent to a kernel support for initializing a target model. Based on the assured background labels, a gradient-based label propagation is performed, resulting in a number of objects differentiated from the background. Then the proposed region growing scheme picks up one largest target object near the center of the kernel support. The grown object region constitutes the proposed indicator function and this allows an exact target model construction for robust mean-shift tracking. Simulation results demonstrate the proposed exact target model could significantly enhance the robustness as well as the accuracy of mean-shift object tracking.	algorithm;characteristic function (convex analysis);clutter;computation;computational complexity theory;gain;gradient;increment;interference (communication);kernel;largest;mean shift;mean squared error;physical object;real-time clock;real-time transcription;region growing;segmentation action;simulation;software propagation	Yoo-Joo Choi;Yong-goo Kim	2014		10.3390/s141120736	simulation;mean-shift;telecommunications;computer science;bioinformatics;engineering;electrical engineering;data science;video tracking;data mining;nanotechnology;statistics	Vision	49.03971396629711	-76.27305122787904	97086
4f3c86ca3ea229d517d30e975d2f9f4410d2ec88	parabolic movement primitives and cortical states: merging optimality with geometric invariance	geometric invariants;building block;hidden markov model;differential geometry;motor system;unsupervised segmentation;state dependence;arm movement;affine transformation;equi affine geometry;neural representation;power law;hidden markov modeling;cortical neurons;neuronal activity;drawing primitives	Previous studies have suggested that several types of rules govern the generation of complex arm movements. One class of rules consists of optimizing an objective function (e.g., maximizing motion smoothness). Another class consists of geometric and kinematic constraints, for instance the coupling between speed and curvature during drawing movements as expressed by the two-thirds power law. It has also been suggested that complex movements are composed of simpler elements or primitives. However, the ability to unify the different rules has remained an open problem. We address this issue by identifying movement paths whose generation according to the two-thirds power law yields maximally smooth trajectories. Using equi-affine differential geometry we derive a mathematical condition which these paths must obey. Among all possible solutions only parabolic paths minimize hand jerk, obey the two-thirds power law and are invariant under equi-affine transformations (which preserve the fit to the two-thirds power law). Affine transformations can be used to generate any parabolic stroke from an arbitrary parabolic template, and a few parabolic strokes may be concatenated to compactly form a complex path. To test the possibility that parabolic elements are used to generate planar movements, we analyze monkeys’ scribbling trajectories. Practiced scribbles are well approximated by long parabolic strokes. Of the motor cortical neurons recorded during scribbling more were related to equi-affine than to Euclidean speed. Unsupervised segmentation of simulta- neously recorded multiple neuron activity yields states related to distinct parabolic elements. We thus suggest that the cortical representation of movements is state-dependent and that parabolic elements are building blocks used by the motor system to generate complex movements.	approximation algorithm;cerebrovascular accident;clinical use template;concatenation;coupling (computer programming);language primitive;loss function;mathematics;moore's law;movement;neuron;neurons;optimization problem;parabolic antenna;rule (guideline);sheffer stroke;stroke, lacunar;cell transformation	Felix Polyakov;Eran Stark;Rotem Drori;Moshe Abeles;Tamar Flash	2008	Biological Cybernetics	10.1007/s00422-008-0287-0	psychology;power law;combinatorics;discrete mathematics;neuroscience;machine learning;motor system;control theory;affine transformation;mathematics;geometry;premovement neuronal activity;hidden markov model	ML	47.35062859256547	-75.73179324494922	97095
23d472987bf2e70091f7f5da38cda6d0e1e08be5	integrating atlas and graph cut methods for right ventricle blood-pool segmentation from cardiac cine mri	heart;image segmentation;gold;blood;magnetic resonance imaging;cardiovascular magnetic resonance imaging	Segmentation of right ventricle from cardiac MRI images can be used to build pre-operative anatomical heart models to precisely identify regions of interest during minimally invasive therapy. Furthermore, many functional parameters of right heart such as right ventricular volume, ejection fraction, myocardial mass and thickness can also be assessed from the segmented images. To obtain an accurate and computationally efficient segmentation of right ventricle from cardiac cine MRI, we propose a segmentation algorithm formulated as an energy minimization problem in a graph. Shape prior obtained by propagating label from an average atlas using affine registration is incorporated into the graph framework to overcome problems in ill-defined image regions. The optimal segmentation corresponding to the labeling with minimum energy configuration of the graph is obtained via graph-cuts and is iteratively refined to produce the final right ventricle blood pool segmentation. We quantitatively compare the segmentation results obtained from our algorithm to the provided gold-standard expert manual segmentation for 16 cine-MRI datasets available through the MICCAI 2012 Cardiac MR Right Ventricle Segmentation Challenge according to several similarity metrics, including Dice coefficient, Jaccard coefficient, Hausdorff distance, and Mean absolute distance error.	cut (graph theory);graph cuts in computer vision	Shusil Dangi;Cristian A. Linte	2017		10.1117/12.2256013	gold;magnetic resonance imaging;image segmentation;heart;medical physics	Vision	41.0670387620752	-79.50054276598178	97137
1672f50dc1664f70e5a300ab6714b4b3ff6bf355	dart: a practical reconstruction algorithm for discrete tomography	μct data dart discrete tomography iterative reconstruction algorithm discrete algebraic reconstruction technique noisy projection;discrete tomography;reconstruction algorithms;prior knowledge;segmentation;noise measurement;image reconstruction;pixel;algorithms computer simulation databases factual humans image processing computer assisted models theoretical phantoms imaging x ray microtomography;image reconstruction pixel reconstruction algorithms equations noise noise measurement;reconstruction algorithm;noise;prior knowledge discrete tomography image reconstruction segmentation	In this paper, we present an iterative reconstruction algorithm for discrete tomography, called discrete algebraic reconstruction technique (DART). DART can be applied if the scanned object is known to consist of only a few different compositions, each corresponding to a constant gray value in the reconstruction. Prior knowledge of the gray values for each of the compositions is exploited to steer the current reconstruction towards a reconstruction that contains only these gray values. Based on experiments with both simulated CT data and experimental μCT data, it is shown that DART is capable of computing more accurate reconstructions from a small number of projection images, or from a small angular range, than alternative methods. It is also shown that DART can deal effectively with noisy projection data and that the algorithm is robust with respect to errors in the estimation of the gray values.	algebraic reconstruction technique;algorithm;angularjs;composition;computation (action);dart, device (physical object);discrete tomography;experiment;grayscale;heuristic (computer science);heuristics;iterative method;iterative reconstruction;nv network;pixel;population parameter;reconstruction conjecture;scanning;segmentation action;silo (dataset);tomographic reconstruction	Kees Joost Batenburg;Jan Sijbers	2011	IEEE Transactions on Image Processing	10.1109/TIP.2011.2131661	iterative reconstruction;computer vision;computer science;noise measurement;noise;machine learning;algebraic reconstruction technique;segmentation;pixel;computer graphics (images)	Vision	53.41686557954949	-77.22467433217399	97389
9c48b03043519d22a333af621681720b44c814bf	a novel automatic concrete surface crack identification using isotropic undecimated wavelet transform	image processing;savitzky golay filter automatic concrete surface crack identification isotropic undecimated wavelet transform automatic crack detection image processing concrete structures high resolution photographs elongated dark objects concrete surface images cross section pixel intensity profiles;wavelet transform;structural engineering computing;crack detection;wavelet transforms image edge detection concrete surface cracks surface treatment image segmentation;savitzky golay filter;proceedings paper;thinning algorithm crack detection wavelet transform savitzky golay filter;surface cracks concrete crack detection image processing structural engineering computing;surface cracks;concrete;thinning algorithm	Traditionally, concrete surface cracks are manually measured and recorded by experienced inspectors who observe cracks with their naked eye. This task is very costly, time-consuming and dangerous. Some automatic crack detection techniques utilizing image processing have been proposed. However, it is very difficult to automatically identify cracks sufficiently fast and accurate. This paper presents a novel automatic algorithm for effectively identifying cracks in concrete structures, which can be applied to high resolution photographs. Firstly, as cracks can be considered as elongated dark objects in concrete surface images, all dark objects are roughly identified using Isotropic Undecimated Wavelet Transform (IUWT) and morphological image processing. Secondly, all previous detected dark objects are classified into crack and non-crack objects by analyzing their cross section pixel intensity profiles, which are computed perpendicularly across their centre-lines in the original input image, using a novel method based on Savitzky-Golay filter. Therefore, both local and global changes in crack width can be readily quantified. Experiments show that our method can automatically detect true cracks in concrete surface images using a unique set of parameters.	algorithm;cross section (geometry);experiment;image processing;image resolution;mathematical morphology;pixel;smoothing;stationary wavelet transform	Hoang-Nam Nguyen;Tai-Yan Kam;Pi-Ying Cheng	2012	2012 International Symposium on Intelligent Signal Processing and Communications Systems	10.1109/ISPACS.2012.6473594	structural engineering;computer vision;engineering;forensic engineering	Graphics	41.26631956619601	-69.68056097629176	97769
224c0eb5d247fccef4810a273d892448ac561ae3	parameterisation invariant statistical shape models	paper novel theory;covariance matrix;shape modelling;curve parameterisations;continuous curve;shape variation;major problem;finite sample;parameterisation invariant statistical shape;earlier formulation;art algorithmfor automatic shape;image processing;scalar product;statistical analysis	In this paper novel theory to automate shape modelling is described. The main idea is to develop a theory that is intrinsically defined for curves, as opposed to a finite sample of points along the curves. The major problem here is to define shape variation in a way that is invariant to curve parametrisations. Instead of representing continuous curves using landmarks, the problem is treated analytically and numerical approximations are introduced at the latest stage. The problem is solved by calculating the covariance matrix of the shapes using a scalar product that is invariant to global reparametrisations. An algorithm for implementing the ideas is proposed and compared to a state of the an algorithm for automatic shape modelling. The problems with instability in earlier formulations are solved and the resulting models are of higher quality.	3d modeling;algorithm;approximation;cluster analysis;hoc (programming language);instability;landmark point;mathematical optimization;numerical analysis;shape optimization	Johan Karlsson;Anders Ericsson;Kalle Åström	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1333696	covariance matrix;mathematical optimization;combinatorics;dot product;image processing;mathematics;statistics	Vision	49.50225425634856	-71.87422905666821	97838
dd0d68ffba3379dd2a89693db7308b10879b5572	sparse representations for range data restoration	algorithms image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval pattern recognition automated reproducibility of results sensitivity and specificity;range data;image restoration sparse representations range data restoration denoising problem occlusion restoration dictionary learning;image resolution;image restoration;three dimensional;noise measurement;range data resolution enhancement;shape;three dimensional displays shape dictionaries image restoration noise image resolution noise measurement;three dimensional displays;sparse modeling occlusion restoration range data denoising range data resolution enhancement;image registration;resolution enhancement;dictionaries;sparse modeling;image denoising;learning artificial intelligence;learning artificial intelligence image denoising image registration;occlusion restoration;sparse representation;noise;range data denoising	In this paper, the problem of denoising and occlusion restoration of 3-D range data based on dictionary learning and sparse representation methods is explored. We apply these techniques after converting the noisy 3-D surface into one or more images. We present experimental results on the proposed approaches.	circuit restoration;dictionary [publication type];isdb;interpolation imputation technique;machine learning;noise reduction;obstruction;part dosing unit;scanner device component;sparse approximation;sparse matrix;structured light	Mona Mahmoudi;Guillermo Sapiro	2012	IEEE Transactions on Image Processing	10.1109/TIP.2012.2185940	image restoration;three-dimensional space;computer vision;image resolution;k-svd;shape;computer science;noise measurement;noise;image registration;machine learning;pattern recognition;sparse approximation;mathematics	Vision	53.133543473920085	-77.18075436265256	97969
c8fcca8639b53da71b07a8cf116fde43a945009e	segmentation algorithms for abdominal computerized tomography scans	local algorithm;image segmentation;computed tomography;iterative algorithms;edge detection;biomedical imaging;physics computing;lesions;computerized tomography;abdomen;image analysis;humans;abdomen computed tomography image segmentation biomedical imaging medical diagnostic imaging iterative algorithms image analysis humans physics computing lesions;medical diagnostic imaging	Two approaches to organ detection in ab dominal computerized tomography scans, one local and one global, have been developed. The first involves a boundary-delineating algorithm which operates homogeneously on entire images and coordinates the use of multiple local criteria for advanced edge detection. The second involves an iterative, adaptive boundary-delineating algorithm suitable for single organ detection and amenable to the incorporation of organ-specific knowledge. Regional results of the first (global) algorithm can initialize the second (local) algorithm. Algorithms were tested on mathematical phantoms prior to application on clinical patient data.	algorithm;ct scan;tomography	Peter G. Selfridge;Judith M. S. Prewitt;Charles R. Dyer;Sanjay Ranade	1979		10.1109/CMPSAC.1979.762560	iterative reconstruction;computer vision;image analysis;edge detection;computer science;image segmentation	Theory	41.609992349015194	-77.06456925630492	98194
ab985cd4917e2349906007b0030df3d3e3ddcc0b	improved quantification of mri relaxation rates using bayesian estimation	bayesian framework;bayesian algorithm;estimation theory;kernel;image distortion;fourier reconstruction;gradient echo mri;gibbs sampler;monte carlo methods magnetic resonance imaging parameter estimation;estimation method;bayes methods;bayesian methods;qualitative analysis;least squares approximation;free induction decay;magnetic resonance image;free induction decay rate;distortion;estimation;medical image processing bayes methods biomedical mri distortion estimation theory image reconstruction;image reconstruction;medical image processing;magnetic resonance imaging bayesian methods laboratories magnetic analysis signal to noise ratio magnetic materials parkinson s disease pixel distortion measurement biological system modeling;magnetic resonance imaging;monte carlo method;pixel;bayesian estimator;gradient echo;gibbs sampler magnetic resonance imaging gradient echo mri bayesian algorithm free induction decay rate t 2 estimation fourier reconstruction image distortion simple signal model signal to noise ratio;parameter estimation;t 2 estimation;signal to noise ratio;simple signal model;monte carlo methods;biomedical mri	Traditional magnetic resonance imaging (MRI) studies are based on image contrast and qualitative analysis. However, there is an increasing interest in quantifying the physical parameters of the object such as the free induction decay rate, T*2 . In this paper, a new Bayesian algorithm is proposed for the estimation of T*2 from gradient echo MRI scans. Current estimation methods use a simple signal model based on Fourier reconstruction which imposes a trade-off between the signal-to-noise ratio (SNR) and image distortion, and results in estimation bias. The proposed algorithm uses a Gibbs sampler in a Bayesian framework to account for image distortion allowing data samples to be acquired with increased SNR, improving the estimation accuracy. Estimation results on simulated objects and in vivo experimental data demonstrate the effectiveness of the algorithm.	algorithm;distortion;gibbs sampling;gradient;linear programming relaxation;resonance;sampling (signal processing);signal-to-noise ratio;video-in video-out	Kelvin J. Layton;Mark R. Morelande;Leigh A. Johnston;Peter Mark Farrell;William Moran	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495694	econometrics;mathematical optimization;magnetic resonance imaging;mathematics;estimation theory;statistics;monte carlo method	Vision	52.5755241889196	-76.62378471267724	98277
10817112ee212effb793476d3c3871b0cd9bdf87	a probabilistic approach for color correction in image mosaicking applications	histograms;local color palette mapping functions probabilistic approach color correction algorithm image mosaicking application geometrical registration photometrical registration photometrical disparities correction image segmentation mean shift region fusion algorithm local joint image histograms truncated gaussian maximum likelihood estimation procedure;image segmentation;measurement;transfer functions;joints;image color analysis;maximum likelihood estimation feature extraction gaussian processes image colour analysis image registration image segmentation;probabilistic logic;image color analysis probabilistic logic image segmentation histograms transfer functions joints measurement;color transfer color correction image mosaicking	Image mosaicking applications require both geometrical and photometrical registrations between the images that compose the mosaic. This paper proposes a probabilistic color correction algorithm for correcting the photometrical disparities. First, the image to be color corrected is segmented into several regions using mean shift. Then, connected regions are extracted using a region fusion algorithm. Local joint image histograms of each region are modeled as collections of truncated Gaussians using a maximum likelihood estimation procedure. Then, local color palette mapping functions are computed using these sets of Gaussians. The color correction is performed by applying those functions to all the regions of the image. An extensive comparison with ten other state of the art color correction algorithms is presented, using two different image pair data sets. Results show that the proposed approach obtains the best average scores in both data sets and evaluation metrics and is also the most robust to failures.	algorithm;ana (programming language);collections (publication);color space;evaluation function;extraction;handling (psychology);image fusion;image histogram;image segmentation;image stitching;kernel density estimation;large;mathematics;mean shift;mosaic - computer software;palette (computing);peak signal-to-noise ratio;performance;preprocessor;silo (dataset);biologic segmentation	Miguel Oliveira;Angel Domingo Sappa;Vítor M. F. Santos	2015	IEEE Transactions on Image Processing	10.1109/TIP.2014.2375642	demosaicing;color histogram;image texture;computer vision;feature detection;hsl and hsv;color image;image gradient;binary image;computer science;machine learning;pattern recognition;histogram;mathematics;color balance;transfer function;image segmentation;probabilistic logic;histogram equalization;measurement;statistics	Vision	44.67723802215173	-68.66789647415945	99039
5184bddbd62c100cbf9692fde7554b801d5c12a4	aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks		Despite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (epistemic) and image-based (aleatoric) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks. We additionally propose a test-time augmentation-based aleatoric uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based aleatoric uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.	artificial neural network;baseline (configuration management);convolutional neural network;dropout (neural networks);experiment;image segmentation;monte carlo method;resonance;simulation	Guotai Wang;Wenqi Li;Michael Aertsen;Jan Deprest;Sébastien Ourselin;Tom Vercauteren	2018	CoRR		convolutional neural network;image segmentation;pixel;aleatoric music;monte carlo method;artificial intelligence;mathematics;pattern recognition;segmentation	ML	43.73171886627659	-77.9130589445553	99090
08fea4eb97a2b2b2bec808ea98ec409acdeee618	a multi-objectively-optimized graph-based segmentation method for breast ultrasound image	breast tumors;breast tumor ultrasound image segmentation graph based segmentation algorithm particle swarm optimization multi objective optimization;image segmentation;ultrasonic imaging;particle swarm optimisation biological tissues biomedical ultrasonics graph theory image segmentation mammography medical image processing;image segmentation linear programming breast tumors ultrasonic imaging robustness;linear programming;robustness;rgb performance multi objectively optimized robust graph based segmentation method breast ultrasound image medical image segmentation computer aided diagnosis system system performance segmentation algorithm parameter automatically optimized robust graph based segmentation method rgb parameter optimization particle swarm optimization algorithm pso algorithm single objectively optimized paorgb method global optimization moorgb method	Segmentation of medical image, as the most essential and important step in the computer-aided diagnosis system, can greatly influence the system performance. Better segmentation to a great extent means better performance. Among many proposed segmentation algorithms, graph-based segmentation has become a hot one in the past few years because of the simple structure and rich theories. After the robust graph-based segmentation method (RGB) was introduced in 2010, a parameter-automatically-optimized robust graph-based segmentation method (PAORGB) was presented in 2013 as well, to optimize the two key parameters of RGB utilizing the particle swarm optimization algorithm (PSO). However, single-objectively-optimized PAORGB cannot well guarantee the global optimization. Therefore, this paper continues the work of PAORGB and proposes a multi-objectively-optimized robust graph-based segmentation method (MOORGB) to further improve the performance of RGB. Experimental results have shown that MOORGB can get better segmentation results from breast ultrasound images compared to PAORGB.	algorithm;computation;computer performance;experiment;global optimization;gradient;mathematical optimization;parallel computing;particle swarm optimization;real-time computing;real-time transcription;time complexity	Qiangzhi Zhang;Xia Zhao;Qinghua Huang	2014	2014 7th International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2014.7002754	computer vision;mathematical optimization;computer science;linear programming;machine learning;segmentation-based object categorization;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;robustness;connected-component labeling	Robotics	42.76736304026197	-72.87388970358234	99264
73ec5021856ec480b6b06ce7000e9f233d8f7705	current derivative estimation of non-stationary processes based on metrical information		Demand for estimation of derivatives has arisen in a range of some applied problems. One of the possible approaches to estimating derivatives is to approximate measurement data. The problem of real-time estimation of de-rivatives is investigated. A variation method of obtaining recurrent smoothing splines is proposed for estimation of derivatives. A distinguishing feature of the described method is recurrence of spline coefficients with respect to its segments and locality about measured values inside the segment. Influence of smoothing spline parameters on efficiency of such estimations is studied. Comparative analysis of experimental results is performed.	stationary process	Elena Kochegurova;Ekaterina Gorokhova	2015		10.1007/978-3-319-24306-1_50	computer science;smoothing spline;artificial intelligence;mathematical optimization;machine learning;locality;spline (mathematics)	EDA	49.08316371467083	-73.76291760785885	99331
bff936108495214ce684047286738476eeb53af3	robust linear registration of ct images using random regression forests	databases;posterior probability;random forests;energy function;medical image analysis;regression;posterior distribution;machine learning;image registration;random regression;medical imaging;registration;mutual information;failure rate;algorithms;ct images;local minima;kullback leibler	Global linear registration is a necessary first step for many different tasks in medical image analysis. Comparing longitudinal studies 1 , cross-modality fusion 2 , and many other applications depend heavily on the success of the automatic registration. The robustness and efficiency of this step is crucial as it affects all subsequent operations. Most common techniques cast the linear registration problem as the minimization of a global energy function based on the image intensities. Although these algorithms have proved useful, their robustness in fully automated scenarios is still an open question. In fact, the optimization step often gets caught in local minima yielding unsatisfactory results. Recent algorithms constrain the space of registration parameters by exploiting implicit or explicit organ segmentations, thus increasing robustness 4,5 . In this work we propose a novel robust algorithm for automatic global linear image registration. Our method uses random regression forests to estimate posterior probability distributions for the locations of anatomical structures – represented as axis aligned bounding boxes 6 . These posterior distributions are later integrated in a global linear registration algorithm. The biggest advantage of our algorithm is that it does not require pre-defined segmentations or regions. Yet it yields robust registration results. We compare the robustness of our algorithm with that of the state of the art Elastix toolbox 7 . Validation is performed via 1464 pair-wise registrations in a database of very diverse 3D CT images. We show that our method decreases the “failure” rate of the global linear registration from 12.5% (Elastix) to only 1.9%.	algorithm;apache axis;ct scan;elastix;image analysis;image registration;mathematical optimization;maxima and minima;medical image computing;medical imaging;modality (human–computer interaction)	Ender Konukoglu;Antonio Criminisi;Sayan D. Pathak;Duncan P. Robertson;Steve White;David R. Haynor;Khan M. Siddiqui	2011		10.1117/12.878085	medical imaging;computer vision;machine learning;pattern recognition;posterior probability	Vision	42.46100035712433	-78.45239975877972	99664
3dda036f4635c30cd76550847e00c33e786803ac	a novel extension to fuzzy connectivity for body composition analysis: applications in thigh, brain, and whole body tissue segmentation		Magnetic resonance imaging (MRI) is the non-invasive modality of choice for body tissue composition analysis due to its excellent soft tissue contrast and lack of ionizing radiation. However, quantification of body composition requires an accurate segmentation of fat, muscle and other tissues from MR images, which remains a challenging goal due to the intensity overlap between them. In this study, we propose a fully automated, data-driven image segmentation platform that addresses multiple difficulties in segmenting MR images such as varying inhomogeneity, non-standardness, and noise, while producing high-quality definition of different tissues. In contrast to most approaches in the literature, we perform segmentation operation by combining three different MRI contrasts and a novel segmentation tool which takes into account variability in the data. The proposed system, based on a novel affinity definition within the fuzzy connectivity (FC) image segmentation family, prevents the need for user intervention and reparametrization of the segmentation algorithms. In order to make the whole system fully automated, we adapt an affinity propagation clustering algorithm to roughly identify tissue regions and image background. We perform a thorough evaluation of the proposed algorithm's individual steps as well as comparison with several approaches from the literature for the main application of muscle/fat separation. Furthermore, whole-body tissue composition and brain tissue delineation were conducted to show the generalization ability of the proposed system. This new automated platform outperforms other state-of-the-art segmentation approaches both in accuracy and efficiency.	addresses (publication format);affinity analysis;affinity propagation;body composition;body tissue;cluster analysis;entity class - imaging modality;fatty acid glycerol esters;file allocation table;functional connectivity magnetic resonance imaging;generalization (psychology);image segmentation;ionizing radiation;lewy body disease;modality (human–computer interaction);muscle;numerous;physical object;platelet glycoprotein 4, human;processor affinity;quantitation;sarcoma;scanning;semiconductor industry;software propagation;spatial variability;algorithm;biologic segmentation;soft tissue;statistical cluster	Nastasja Van Wyk;Sarfaraz Hussein;Aydogan Savran;Rita Rastogi Kalyani;David Reiter;Chee W. Chia;Kenneth W. Fishbein;Richard G. S. Spencer;Luigi Ferrucci;Ulas Bagci	2018	IEEE transactions on bio-medical engineering	10.1109/TBME.2018.2866764	fuzzy logic;market segmentation;computer vision;artificial intelligence;cluster analysis;image segmentation;affinity propagation;segmentation;magnetic resonance imaging;computer science	Vision	40.06307459934789	-77.84360406618914	99673
cd1356907a8e9d00ffe98e518d68916497729e89	confidence maps and confidence intervals for near infrared images in breast cancer	glandula mamaria patologia;metodo estadistico;ir spectroscopic tomography near infrared images breast cancer confidence intervals confidence maps statistical tests noisy images interpretation medical diagnostic imaging phantom based images simulated images;intervalo confianza;cancer;tumor maligno;infrared imaging breast cancer testing pixel image generation infrared spectra spectroscopy tomography covariance matrix image reconstruction;objet test;mammary gland;simulation;diagnostico;statistical test;matrice covariance;simulacion;statistical method;breast cancer diagnosis;matriz covariancia;indexing terms;statistical hypothesis testing;glandula mamaria;reconstruction image;near infrared;confidence interval;image generation;espectrometria ir;statistical analysis;infrared spectrometry;reconstruccion imagen;infrared imaging;methode statistique;image reconstruction;medical image processing;spectrometrie ir;intervalle confiance;tomographie;mammary gland diseases;tumeur maligne;glande mammaire;infrared spectroscopy;mammography;tomografia;diagnosis;breast neoplasms computer simulation confidence intervals diagnosis differential female humans image processing computer assisted phantoms imaging spectroscopy near infrared tomography;objeto prueba;test object;tomography;statistical analysis infrared imaging cancer mammography infrared spectroscopy image reconstruction medical image processing;near infrared imaging;breast cancer;glande mammaire pathologie;malignant tumor;covariance matrix;diagnostic	Extends basic concepts of statistical hypothesis testing and confidence intervals to images generated by a new procedure for near infrared spectroscopic tomography being developed for use in breast cancer diagnosis. By estimating the covariance matrix of the pixels of an image from data used in the image reconstruction process, confidence maps for statistical tests on individual pixels and confidence intervals for entire images are displayed as an aid to research and clinical personnel interpreting possibly noisy images. The methods are applied to simulated and phantom-based images.	breast carcinoma;ct scan;confidence intervals;estimated;imaging phantom;iterative reconstruction;map;phantoms, imaging;pixel;statistical test;tomography	Tor D. Tosteson;Brian W. Pogue;Eugene Demidenko;Troy O. McBride;Keith D. Paulsen	1999	IEEE Transactions on Medical Imaging	10.1109/42.819328	infrared spectroscopy;computer vision;statistical hypothesis testing;radiology;medicine;mathematics;tomography;nuclear medicine;statistics;cancer	Vision	52.50406062938654	-79.17087207564776	99740
01be6453b13dde9f163feb812f1ea86bb6ec0e98	flux invariants for shape	object recognition;edge detection;euclidean distance;computer vision flux invariant jordan curve gradient vector field euclidean distance function 2d shape boundary divergence theorem region area medial point hamilton jacobi skeletonization algorithm circular neighborhood average outward flux measure object angle skeletal point boundary curve geometric quantity outward flux limit value euclidean invariant shape description boundary reconstruction numerical simulation shape representation;computer vision;euclidean distance skeleton shape measurement numerical simulation computer vision computer science mathematics area measurement image reconstruction biomedical imaging;feature extraction;image reconstruction;hamilton jacobi;shape description;curve fitting;vector field;image thinning;jacobian matrices;computer vision edge detection image thinning jacobian matrices curve fitting image reconstruction feature extraction object recognition	We consider the average outward flux through a Jordan curve of the gradient vector field of the Euclidean distance function to the boundary of a 2D shape. Using an alternate form of the divergence theorem, we show that in the limit as the area of the region enclosed by such a curve shrinks to zero, this measure has very different behaviours at medial points than at non-medial ones, providing a theoretical justification for its use in the Hamilton-Jacobi skeletonization algorithm of [7]. We then specialize to the case of shrinking circular neighborhoods and show that the average outward flux measure also reveals the object angle at skeletal points. Hence, formulae for obtaining the boundary curves, their curvatures, and other geometric quantities of interest, can be written in terms of the average outward flux limit values at skeletal points. Thus this measure can be viewed as a Euclidean invariant for shape description: it can be used to both detect the skeleton from the Euclidean distance function, as well as to explicitly reconstruct the boundary from it. We illustrate our results with several numerical simulations.	computer simulation;computer vision;euclidean distance;experiment;flux qubit;gradient;interpolation;jacobi method;medial graph;nelder–mead method;numerical analysis;numerical method;pattern recognition;pixel;sampling (signal processing);simplex algorithm;smoothing;spectral flux	Pavel Dimitrov;James N. Damon;Kaleem Siddiqi	2003		10.1109/CVPR.2003.1211439	iterative reconstruction;computer vision;mathematical optimization;vector field;edge detection;topology;feature extraction;computer science;cognitive neuroscience of visual object recognition;euclidean distance;mathematics;geometry;euclidean distance matrix;curve fitting	Vision	48.65406178071459	-70.12910595386349	99780
6e2cca7ccb4d2f97d1f9f4a7326bced62809e704	semi-supervised segmentation fusion of multi-spectral and aerial images	image segmentation training optimization approximation algorithms algorithm design and analysis clustering algorithms computer aided instruction;convex optimization problem semisupervised segmentation fusion algorithm multispectral images aerial images consensus distributed learning unsupervised segmentation fusion usf segmentation outputs np problem computational complexity semisupervision sssf pixels co occurrence;learning artificial intelligence convex programming geophysical image processing image fusion image segmentation;stochastic optimization segmentation clustering fusion consensus	A Semi-supervised Segmentation Fusion algorithm is proposed using consensus and distributed learning. The aim of Unsupervised Segmentation Fusion (USF) is to achieve a consensus among different segmentation outputs obtained from different segmentation algorithms by computing an approximate solution to the NP problem with less computational complexity. Semi-supervision is incorporated in USF using a new algorithm called Semi-supervised Segmentation Fusion (SSSF). In SSSF, side information about the co-occurrence of pixels in the same or different segments is formulated as the constraints of a convex optimization problem. The results of the experiments employed on artificial and real-world benchmark multi-spectral and aerial images show that the proposed algorithms perform better than the individual state-of-the art segmentation algorithms.	aerial photography;approximation algorithm;benchmark (computing);computational complexity theory;consensus (computer science);convex optimization;experiment;mathematical optimization;optimization problem;pixel;semi-supervised learning;semiconductor industry	Mete Ozay	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.659	computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Vision	50.898202230611595	-72.661538484392	99845
bdf6302ded893f6a842eb5ade79a3de8e739b255	new fcm segmentation approach based on multi-resolution analysis			fuzzy cognitive map;multiresolution analysis	Yaghmorasan Benzian;Nacéra Benamrane	2018	IJFSA	10.4018/IJFSA.2018100105	computer vision;computer science;segmentation;artificial intelligence	Vision	41.95560420943992	-70.94862934086498	99879
c610480f86b8197717cb61d670330ca3a4d4a110	ellipse detection with hard c-regression models and random initializations	ellipse detection;pattern clustering;hcrm;dell precision t5400;regression analysis edge detection pattern clustering random processes;local circles;edge detection;computer model;prototypes;cluster prototypes;shell clustering methods partition data sets;regression model;hard c regression models;shell shape clusters;random initializations;image edge detection computational modeling mathematical model transforms feature extraction prototypes switches;computational modeling;image edge detection;clustering;feature extraction;ellipses;random processes;transforms;mathematical model;defuzzified switching regression models;switching regression;regression analysis;switches;random initializations ellipse detection clustering switching regression;dell precision t5400 ellipse detection hard c regression models random initializations shell clustering methods partition data sets shell shape clusters local circles ellipses cluster prototypes hcrm defuzzified switching regression models	Shell clustering methods partition data sets into several shell-shape clusters by extracting local circles or ellipses as prototypes of clusters. This paper proposes hard c-regression models (HCRMs) for shell clustering. The procedure is a defuzzified switching regression models. HCRMs successfully detect ellipses by using random initializations. We report the performance using 20 data sets each of which consists of two ellipses. The detection time on average is 14 milliseconds on DELL PRECISION T5400 3.16GHz.	cluster analysis;defuzzification;dell precision	Hidetomo Ichihashi;Li Chieu Lam;Katsuhiro Honda;Akira Notsu	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007377	stochastic process;computer science;machine learning;pattern recognition;mathematics;regression analysis;statistics	Robotics	47.174993277268726	-71.81887028699056	100079
fbd316345b6c8cc4ce16047996f190697dfc05a9	free-form registration using mutual information and curvature regularization	distance measure;viscous fluid;non rigid registration;brain imaging;mutual information;sum of squared difference;similarity measure	In this paper we present a novel 3-D free-form non-rigid registration algorithm which combines the mutual information similarity measure with a particular curvature based regularizer, which has been demonstrated to produce very satisfactory results in conjunction with the sum of squared differences distance measure. The method is evaluated for inter-subject MR brain image registration using simulated deformations and compared with a scheme that applies the same similarity measure but with a viscous fluid regularizer.	algorithm;image registration;matrix regularization;mutual information;similarity measure	Emiliano D'Agostino;Jan Modersitzki;Frederik Maes;Dirk Vandermeulen;Bernd Fischer;Paul Suetens	2003		10.1007/978-3-540-39701-4_2	mathematical optimization;viscous liquid;topology;mathematics;geometry;mutual information;statistics;neuroimaging	Vision	45.676116444114875	-77.65679104154559	100405
421dd38140dae0dc2ee50749ff2b9be5275227ad	level set segmentation with shape and appearance models using affine moment descriptors	variational approach;level set segmentation;gray level;efficient narrow band technique;shape prior;training shape;appearance model;affine transformation;ct image;affine moment descriptors;boundary pixel;geometric property;active contour	We propose a level set based variational approach that incorporates shape priors into edge-based and region-based models. The evolution of the active contour depends on local and global information. It has been implemented using an efficient narrow band technique. For each boundary pixel we calculate its dynamic according to its gray level, the neighborhood and geometric properties established by training shapes. We also propose a criterion for shape aligning based on affine transformation using an image normalization procedure. Finally, we illustrate the benefits of the our approach on the liver segmentation from CT images.	active contour model;ct scan;grayscale;image moment;mathematical optimization;pixel;variational principle	Carlos Platero;María C. Tobar;Javier Sanguino;José Manuel Poncela;Olga Velasco	2011		10.1007/978-3-642-21257-4_14	active shape model;computer vision;mathematical optimization;mathematics;geometry;affine shape adaptation	Vision	49.48302503451802	-71.30539815298971	100499
e87fca24a61602980356c0ee3eda1ef9ec93dd90	automatic segmentation of brain tumor image based on region growing with co-constraint		Image segmentation remains an ongoing challenge in medical image processing research. Owing to brain tumor’s inhomogeneous structure and blurred boundary, the segmentation of brain tumor image is not always ideal. Therefore, we propose a novel region growing model that enables to segment the brain tumor image accurately and automatically. The model mainly improves the selection of seed points and the growth rules. Using the method of fusion information with multimodal MRI images is described to select the seed point automatically, which makes the segmentation algorithm more robust. Furthermore, in order to mostly remain the local feature and the boundary information of brain tumor, a spatial texture feature is constructed in this study. Based on the above model, an automatic brain tumor image segmentation algorithm is established, which uses the region growing with the Co-constraint of intensity and spatial texture. In terms of performance evaluation, the proposed method not only outperforms other segmentation algorithms in the accuracy of results, but also has lower computational cost. This is undoubtedly a worthy method of brain tumor image segmentation.		Siming Cui;Xuanjing Shen;Yingda Lyu	2019		10.1007/978-3-030-05710-7_50	artificial intelligence;image processing;computer vision;pattern recognition;computer science;image segmentation;segmentation;region growing;brain tumor	Vision	42.43752952008937	-73.44353665229059	100521
1f3bf1f0c4a14ef1540b9bde240a45229d49d8e3	the registration and atlas construction of noisy brain computer tomography images based on free form deformation technique	deformacja;algorytm;brain;computer tomography;mozg;algorithm;deformation;tomografia komputerowa			Tomasz Hachaj	2008	Bio-Algorithms and Med-Systems		computer vision;computer science;nuclear medicine;computer graphics (images)	Vision	45.88754784729456	-78.44799337814067	100650
3973c74b1dde12a4b898427ff2e18981fdedf340	region matching in the temporal study of mammograms using integral invariant scale-space	g740 computer vision;cad;shape analysis;integral invariants;temporal study;region matching;breast cancer	Our aim is to compare two mammograms (left-right, temporal) in an unsupervised manner. To this end, we propose a novel region matching algorithm (RMA) for mammograms based upon the non-emergence and non-enhancement of maxima and the causality principle of integral invariant scale space (in a limited sense). The algorithm has several advantages over commonly used methods for comparing segmented regions as shapes. First, it gives improved key-points alignment for optimal shape correspondence. Second, it identifies new growths and complete/partial occlusion in corresponding regions by dividing the segmented region into sub-regions based upon the extrema that persist over all scales. Third, the algorithm does not depend upon the spatial locations of mammographic features and eliminates the need for registration to identify salient changes over time. Finally, the algorithm is fast to compute and requires no human intervention.	scale space	Faraz Janan;Michael Brady	2012		10.1007/978-3-642-31271-7_23	computer vision;mathematical optimization;discrete mathematics;mathematics	Vision	46.65712512309522	-72.87792556774826	100821
1af95d65745947c8866727714f2cc454ac3285fa	optimal filter-bank design for multiple texture discrimination	optimal solution;closed form optimal solution;filtering;image recognition;filter bank;image processing;band pass filters;filter bank finite impulse response filter feature extraction data mining energy measurement filtering;optimal filtering;finite impulse response filter;filter response energy measure;gabor filters;texture features;data mining;image texture;free parameters;smoothing methods;finite impulse response;energy measurement;feature extraction;fir filter;optimal design;optimal filter bank design;fir filters;multiple texture discrimination;filtering theory fir filters band pass filters circuit optimisation feature extraction image texture;texture filtering;multi texture optimal filter bank design;circuit optimisation;closed form optimal solution multiple texture discrimination optimal filter bank design texture features feature extraction filter response energy measure texture filtering multi texture optimal filter bank design free parameters finite impulse response fir filters;filtering theory	An important category of texture features are fea4 m , n 4 w ( k l ) p: & tures extracted by filters together with a filter response Filter Nonlinearity Smoothing image energy measure. Most approaches t o texture filtering Local ener iy function use filter banks which are selected by some heuristic criteria and which are n o t optimal with respect t o the given data. S o m e approaches t o optimal filter design have been presented. A few approaches t o multi-texture optimal filter bank design have also been presented, but they are all restricted t o very small sets of free parameters. I n this paper a new technique f o r multiple texture optimal design of a bank of general f ini te impulse response (FIR) filters is presented. A closed f o r m optimal solution i s obtained by modeling the feature extraction for the textures.	feature extraction;filter bank;filter design;finite impulse response;heuristic;infinite impulse response;nonlinear system;optimal design;smoothing;texture filtering	Trygve Randen;John Håkon Husøy	1997		10.1109/ICIP.1997.638722	adaptive filter;computer vision;image processing;computer science;finite impulse response;pattern recognition;filter bank;filter design	Robotics	50.62496044105205	-66.83014501470709	100996
630633ba4674d08f492489e5468e3a21ee05f87c	<ce:italic>dr-tamas</ce:italic>: diffeomorphic registration for tensor accurate alignment of anatomical structures		In this work, we propose DR-TAMAS (Diffeomorphic Registration for Tensor Accurate alignMent of Anatomical Structures), a novel framework for intersubject registration of Diffusion Tensor Imaging (DTI) data sets. This framework is optimized for brain data and its main goal is to achieve an accurate alignment of all brain structures, including white matter (WM), gray matter (GM), and spaces containing cerebrospinal fluid (CSF). Currently most DTI-based spatial normalization algorithms emphasize alignment of anisotropic structures. While some diffusion-derived metrics, such as diffusion anisotropy and tensor eigenvector orientation, are highly informative for proper alignment of WM, other tensor metrics such as the trace or mean diffusivity (MD) are fundamental for a proper alignment of GM and CSF boundaries. Moreover, it is desirable to include information from structural MRI data, e.g., T1-weighted or T2-weighted images, which are usually available together with the diffusion data. The fundamental property of DR-TAMAS is to achieve global anatomical accuracy by incorporating in its cost function the most informative metrics locally. Another important feature of DR-TAMAS is a symmetric time-varying velocity-based transformation model, which enables it to account for potentially large anatomical variability in healthy subjects and patients. The performance of DR-TAMAS is evaluated with several data sets and compared with other widely-used diffeomorphic image registration techniques employing both full tensor information and/or DTI-derived scalar maps. Our results show that the proposed method has excellent overall performance in the entire brain, while being equivalent to the best existing methods in WM.	alignment;anatomic structures;cerebrospinal fluid;dr-dos;diffusion tensor imaging;gray matter;image registration;information;loss function;microtubule-associated proteins;patients;spatial variability;velocity (software development);white matter;algorithm;registration - actclass	M. Okan Irfanoglu;Amritha Nayak;Jeffrey Jenkins;Elizabeth B. Hutchinson;Neda Sadeghi;Cibu Thomas;Carlo Pierpaoli	2016	NeuroImage	10.1016/j.neuroimage.2016.02.066	computer vision;mathematical optimization;mathematics;geometry	Vision	43.825910900770914	-79.0693181864924	101181
08282cbdd51b92dc94931395cd20dd6b72f11c0b	vessel scale-selection using mrf optimization	graph theory;vesselness filter vessel scale selection mrf optimization feature detection standard scale selection algorithm spatial regularization graph labeling problem markov random field multilabel optimization vascular structures medical images;vessel scale selection;optimisation;feature detection;vascular structures;markov random field multilabel optimization;medical images;vesselness filter;computer vision biomedical imaging image analysis pixel detectors detection algorithms markov random fields filters retina image databases;markov random field;medical image;graph labeling problem;image edge detection;retina;feature extraction;medical image processing;pixel;standard scale selection algorithm;optimization;markov processes;spatial regularization;retinal imaging;optimisation blood vessels feature extraction graph theory markov processes medical image processing;mrf optimization;blood vessels;labeling;noise	Many feature detection algorithms rely on the choice of scale. In this paper, we complement standard scale-selection algorithms with spatial regularization. To this end, we formulate scale-selection as a graph labeling problem and employ Markov random field multi-label optimization. We focus on detecting the scales of vascular structures in medical images. We compare the detected vessel scales using our method to those obtained using the selection approach of the well-known vesselness filter (Frangi et al 1998). We propose and discuss two different approaches for evaluating the goodness of scale-selection. Our results on 40 images from the Digital Retinal Images for Vessel Extraction (DRIVE) database show an average reduction in these error measurements by more than 15%.	algorithm;feature detection (computer vision);feature detection (web development);graph labeling;markov chain;markov random field;mathematical optimization;medical imaging;multi-label classification;sensor;whole earth 'lectronic link	Hengameh Mirzaalian;Ghassan Hamarneh	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540051	computer vision;labeling theory;feature extraction;computer science;noise;graph theory;machine learning;pattern recognition;feature detection;mathematics;markov process;pixel	Vision	44.72767160368017	-68.41780920328235	101460
91e78e7181afc204e968eb4d132f985474f44a86	cytoplasm contour approximation based on color fuzzy sets and color gradient	fuzzy set;color histogram;region segmentation;medical image;color gradient;color histograms;cell contour;color image segmentation;fuzzy characterization	Here we propose a method for contour detection of cells on medical images. The problem that arises in such images is that cells' color is very similar to the background, because the cytoplasm is translucent and sometimes overlapped with other cells, making it difficult to properly segment the cells. To cope with these drawbacks, given a cell center, we use hue and saturation histograms for defining the fuzzy sets associated with cells relevant colors, and compute the membership degree of the pixels around the center to these fuzzy sets. Then we approach the color gradient (module and argument) of pixels near the contour points, and use both the membership degrees and the gradient information to drive the deformation of the region borders towards the contour of the cell, so obtaining the cell region segmentation.	approximation;color gradient;contour line;fuzzy set	Santiago Romaní;Belén Prados-Suárez;Pilar Sobrevilla;Eduard Montseny	2010		10.1007/978-3-642-14049-5_66	color gradient;color histogram;computer vision;color image;computer science;artificial intelligence;pattern recognition;mathematics;color balance;fuzzy set	Robotics	42.646900486003666	-67.67868828411397	101585
9f331144e5f28483a86795f07fcbec01917450fc	parametric active contour for boundary estimation of weld defects in radiographic testing	radiographic testing;welds nondestructive testing radiography;weld defects;parametric active contour;active contour;boundary estimation;welds;snake initialization;welding;active contours;snake initialization parametric active contour boundary estimation weld defects radiographic testing snake evolution;force;radiography;snake evolution;image edge detection;mathematical model;active contours welding radiography deformable models image segmentation manufacturing industries nondestructive testing radiation detectors quality assurance code standards;nondestructive testing	In this paper we present a new approach to deal with the defects contours estimation problem in radiographic images using parametric active contours. In this approach we exploit the performance of the GVF as external force and enhance it by joining to it external adaptive pressure forces which bring speed to the snake evolution and less sensitivity to the snake initialization and provides capability of tracking the concavities.	active contour model;color gradient;experiment;radiography;synthetic intelligence;whole earth 'lectronic link	Aicha-Baya Goumeidane;Mohammed Khamadja;Christophe Odet	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555552	computer vision;radiographic testing;radiography;nondestructive testing;mathematical model;active contour model;force;welding	Vision	41.685263613624464	-75.70897256430845	101898
de79180e6970d3fc726ef2a219d6d1082dc304c4	automated generation of directed graphs from vascular segmentations	feature recognition;segmentation;medical imaging	Automated feature extraction from medical images is an important task in imaging informatics. We describe a graph-based technique for automatically identifying vascular substructures within a vascular tree segmentation. We illustrate our technique using vascular segmentations from computed tomography pulmonary angiography images. The segmentations were acquired in a semi-automated fashion using existing segmentation tools. A 3D parallel thinning algorithm was used to generate the vascular skeleton and then graph-based techniques were used to transform the skeleton to a directed graph with bifurcations and endpoints as nodes in the graph. Machine-learning classifiers were used to automatically prune false vascular structures from the directed graph. Semantic labeling of portions of the graph with pulmonary anatomy (pulmonary trunk and left and right pulmonary arteries) was achieved with high accuracy (percent correct⩾0.97). Least-squares cubic splines of the centerline paths between nodes were computed and were used to extract morphological features of the vascular tree. The graphs were used to automatically obtain diameter measurements that had high correlation (r⩾0.77) with manual measurements made from the same arteries.	algorithm;anatomic structures;arterial system;ct scan;diameter (qualifier value);directed graph;feature extraction;graph - visual representation;idiopathic pulmonary fibrosis;image segmentation;imaging informatics;informatics (discipline);least squares;machine learning;semiconductor industry;spline (mathematics);thinning;x-ray computed tomography;angiogram;biologic segmentation	Brian E. Chapman;Holly Berty;Stuart L. Schulthies	2015	Journal of biomedical informatics	10.1016/j.jbi.2015.07.002	medical imaging;feature recognition;computer vision;computer science;pattern recognition;segmentation;genetics	Vision	39.689974158332625	-78.90720254545491	102007
6d7d9a9264949ccbcb401dca255e42ab980c1d84	reconstruction of patient-specific 3d bone model from biplanar x-ray images and point distribution models	image morphing;image reconstruction bones x ray imaging image edge detection pixel image segmentation shape measurement image converters energy measurement data mining;x ray imaging;edge detection;solid modelling bone diagnostic radiography edge detection feature extraction image morphing image reconstruction image registration medical image processing physiological models;indexing terms;shape deformation;three dimensional;bones;3d model;feature extraction;image reconstruction;medical image processing;image registration;cadaveric dry bones biplanar two dimensional x ray image reconstruction patient specific 3d bone model point distribution models regularized morphing shape deformation proximal femur image to model correspondence building method edge pixels detection apparent contour extraction image registration;bone;x ray imaging bones image reconstruction;physiological models;3d reconstruction;diagnostic radiography;solid modelling;qualitative evaluation;point distribution model	Reconstruction of patient-specific three-dimensional (3D) bone model from biplanar two-dimensional (2D) X-ray images and point distribution models (PDM) is discussed. We present a stable and accurate approach combining regularized morphing and shape deformation, and show its application to reconstruction of proximal femur. A novel image-to-model correspondence building method using directly the edge pixels detected from the 2D images and the apparent contour extracted from the 3D model is proposed to convert a 2D/3D reconstruction problem to a 3D/3D one, whose solutions are well studied. Quantitative and qualitative evaluation results on eleven cadaveric dry bones are given which indicate the validity of our approach.	morphing;pixel;point distribution model;polygonal modeling;radiography;reconstruction conjecture	Guoyan Zheng	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312539	3d reconstruction;iterative reconstruction;three-dimensional space;point distribution model;computer vision;edge detection;index term;feature extraction;computer science;image registration;geometry	Vision	45.56786779484042	-77.16123642144333	102026
791e4eb0b6154ef56506e807a66c85ed05935a91	3d registration using a new implementation of the icp algorithm based on a comprehensive lookup matrix: application to medical imaging	pulse noise;gaussian noise;iterative method;evaluation performance;medical imagery;point matching;algorithm performance;performance evaluation;aplicacion medical;implementation;efficient algorithm;evaluacion prestacion;impulse noise;ruido gaussiano;medical data;bruit impulsion;3d registration;metodo iterativo;algorithme;algorithm;surface registration;medical image;resultado algoritmo;methode iterative;robustesse;bruit gaussien;performance algorithme;imagineria medica;imagerie medicale;iterative closest point;robustness;medical application;implementacion;ruido impulso;icp algorithm;robustez;application medicale;algoritmo	The iterative closest point (ICP) algorithm is an efficient algorithm for robust rigid registration of 3D data. Results provided by the algorithm are highly dependent upon the step of finding corresponding pairs between the two sets of 3D data before registration. In this paper, a look up matrix is introduced in the point matching step to enhance the overall ICP performance. Convergence properties and robustness are evaluated in the presence of Gaussian and impulsive noise, and under different data set sizes. The new algorithm has been evaluated on 3D medical data. It has been applied successfully to register closed surfaces acquired using different medical imaging modalities.	algorithm;iterative closest point;iterative method;lookup table;medical imaging	Ahmad Almhdie;Christophe Léger;Mohamed A. Deriche;Roger Lédée	2007	Pattern Recognition Letters	10.1016/j.patrec.2007.03.005	gaussian noise;computer vision;impulse noise;computer science;artificial intelligence;theoretical computer science;iterative method;implementation;algorithm;iterative closest point;robustness	Vision	48.77028679463433	-75.75177917540384	102085
524d71fbf1e6b638330acc263b473507042cebb5	maximum-likelihood expectation-maximization reconstruction of sinograms with arbitrary noise distribution using nec-transformations	institutional repositories;clinical data;convergence;fedora;aplicacion medical;systeme nerveux central;maximum likelihood;noise equivalent counts shifting;maximization;helium;image converters;acoustic noise image reconstruction positron emission tomography convergence reconstruction algorithms attenuation nuclear medicine image converters helium noise robustness;sinogramme;simulation;maximum vraisemblance;reconstruction algorithms;hombre;simulacion;tomography emission computed;tomocentelleografia;attenuation;algorithm convergence;encefalo;noise equivalent count;image processing computer assisted;noise robustness;positron emission tomography;vital;artifacts;convergence speed;algorithme;signal processing computer assisted;algorithm;reconstruction image;sistema nervioso central;nec transformations;expectation maximization;encephale;reconstruccion imagen;exploration radioisotopique;likelihood functions;sinograms;image reconstruction;medical image processing;acoustic noise;emission tomography;simple pixel by pixel methods;noise image reconstruction positron emission tomography medical image processing;human;positron;suboptimal results;lower likelihood value maximum likelihood expectation maximization reconstruction sinograms simple pixel by pixel methods arbitrary noise distribution nec transformations poisson distributed data noise equivalent counts scaling noise equivalent counts shifting nuclear medicine general purpose nonnegative reconstruction algorithm clinical data medical diagnostic imaging suboptimal results streak artifacts algorithm convergence;general purpose nonnegative reconstruction algorithm;radionuclide study;expectation;velocidad convergencia;algorithms;evaluation;nuclear medicine;medical application;streak artifacts;humans;rapport signal bruit;relacion senal ruido;maximum likelihood expectation maximization reconstruction;evaluacion;reconstruction algorithm;vtls;signal to noise ratio;maximum likelihood expectation maximization;positon	The maximum-likelihood (ML) expectation-maximization (EM) [ML-EM] algorithm is being widely used for image reconstruction in positron emission tomography. The algorithm is strictly valid if the data are Poisson distributed. However, it is also often applied to processed sinograms that do not meet this requirement. This may sometimes lead to suboptimal results: streak artifacts appear and the algorithm converges toward a lower likelihood value. As a remedy, the authors propose two simple pixel-by-pixel methods [noise equivalent counts (NEC)-scaling and NEC-shifting] in order to transform arbitrary sinogram noise into noise which is approximately Poisson distributed (the first and second moments of the distribution match those of the Poisson distribution). The convergence speed associated with both transformation methods is compared, and the NEC-scaling method is validated with both simulations and clinical data. These new methods extend the ML-EM algorithm to a general purpose nonnegative reconstruction algorithm.	blast e-value;expectation–maximization algorithm;image scaling;iterative reconstruction;morphologic artifacts;pixel;platelet count measurement;positron-emission tomography;positrons;simulation;sinogram display;cell transformation	Johan Nuyts;Christian Michel;Patrick Dupont	2001	IEEE Transactions on Medical Imaging	10.1109/42.925290	computer simulation;computer vision;mathematical optimization;expectation–maximization algorithm;computer science;mathematics;statistics	Visualization	53.552385376475755	-75.83394536101933	102086
734423784cd24af3fda1953b9f568491cfaddad1	automated image registration by maximization of a region similarity metric	rigid body	This paper presents a robust algorithm for automated registration of images related by rigid-body transformations. This algorithm uses a new region-based similarity metric , which enables accurate registration of images of large contrast diierences. Region segmentation required by the metric is accomplished using a multiscale segmentation algorithm , and minimization of this metric is done using the Powell direction set method. Experimental results are presented to demonstrate that the algorithm is eeective for aligning images from single or multiple imaging modalities without the use of any du-cial markers.	expectation–maximization algorithm;image registration;image segmentation;powell's method	Zhi-Pei Liang;Hao Pan;Richard L. Magin;Narendra Ahuja;Thomas S. Huang	1997	Int. J. Imaging Systems and Technology	10.1002/(SICI)1098-1098(1997)8:6%3C513::AID-IMA3%3E3.0.CO;2-D	computer vision;mathematical optimization;rigid body;mathematics;geometry	Vision	45.27980513405545	-75.79124618991324	102120
8575a44e299d2e070801895505c6e0982f8ae6d9	evaluation of radiomic features stability when deformable image registration is applied		Radiomic features are currently being evaluated as potential imaging biomarkers. Deformable image registration (DIR) is now routinely applied in many medical imaging applications. Usually, DIR is applied in one of two ways: a) mapping the surface of a contoured volume, or b) mapping the image intensities. This study investigated radiomic feature stability when DIR is applied in these two ways using four dimensional computed tomography (4DCT) data. DIR was applied between the inspiration and expiration phases of 4DCT datasets. Radiomic features were extracted from (1) the expiration phases of 25 lung cancer 4DCT datasets within the contoured tumor volumes, (2) the inspiration phases with the mapped tumor volumes, and (3) the inspiration phases deformed to the corresponding expiration phases of the original contoured volumes. The mean variation and the concordance correlation coefficient (CCC) between these 3 sets of features were analyzed. Many features were found unstable (mean variation > 50% or CCC < 0.5) when DIR was applied in either way. Caution is needed in radiomic feature applications when DIR is necessary.	ct scan;categorization;coefficient;concordance (publishing);contour line;control theory;emoticon;image registration;medical imaging;software propagation;tomography	Kuei-Ting Chou;Kujtim Latifi;Eduardo G. Moros;Vladimir Feygelman;Tzung-Chi Huang;Thomas J. Dilling;Bradford Perez;Geoffrey G. Zhang	2018		10.5220/0006694301530158	machine learning;artificial intelligence;computer vision;computer science;image registration	Vision	40.18746512794541	-79.76333039336912	102199
0c7c308f54d20ff058ed98bc6b21a00345099405	the research of automatic registering detection of rotary screen printing machine based on meanshift and fast block-matching algorithm	meanshift;rotary screen printing;matching;registering detection;harris	With the register detection problem, the color textile image segmentation algorithm based on MeanShift and the block matching algorithm based on Harris corner detection were proposed. The extended MeanShift algorithm was used to segment the textile image, and then different color regions were extracted from the segmented standard image; then the feature points were detected by Harris operator, and with these feature points as the centers, the standard matching blocks were selected; finally the best matching block in the dealt target image were found to calculate register errors. The experiment results show that MeanShift algorithm is not sensitive to the texture of the fabric image, so the algorithm is good for the segmentation of it. And the matching accuracy and speed are improved, because the feature block-matching algorithm combines the point pattern matching with the feature matching algorithm. The research in this paper will lay the foundation for the closed-loop control of online register detection.	block-matching algorithm;control theory;corner detection;harris affine region detector;image segmentation;pattern matching;printing;rotary woofer	Junfeng Jing;Guangyan Li;Pengfei Li	2012	JCP	10.4304/jcp.7.6.1369-1376	matching;computer vision;speech recognition;computer science	Vision	39.31710057164481	-67.31747913191506	102400
0d02e851da9bf9614b440470a27f3ed55c78d790	automatic extraction of brain surface and mid-sagittal plane from pet images applying deformable models	raclopride brain images;deformable surface;pet imaging;fdg brain images;segmentation;brain imaging;deformable model;brain surface extraction	In this study, we propose and evaluate new methods for automatic extraction of the brain surface and the mid-sagittal plane from functional positron emission tomography (PET) images. Designing methods for these segmentation tasks is challenging because the spatial distribution of intensity values in a PET image depends on the applied radiopharmaceutical and the contrast to noise ratio in a PET image is typically low. We extracted the brain surface with a deformable model which is based on a global optimization algorithm. The global optimization allows reliable automation of the extraction task. Based on the extracted brain surface, the mid-sagittal plane was determined. The method was tested with the image of the Hoffman brain phantom (FDG) and the images from the brain studies with the FDG (17 images) and the C11-Raclopride tracers (4 images). In addition to the brain surfaces, we applied the deformable model for extraction of the coarse cortical structure based on the tracer uptake from FDG-PET brain images. The proposed segmentation methods provide a promising direction for automatic processing and analysis of PET brain images.	ct scan;computational human phantom;contrast resolution;contrast-to-noise ratio;extraction;fdg-positron emission tomography;fluorodeoxyglucose f18;functional discourse grammar;global optimization;mathematical optimization;phantoms, imaging;polyethylene terephthalate;positron-emission tomography;positrons;radiopharmaceuticals;sagittal plane;tracer;x-ray computed tomography;algorithm;biologic segmentation	Jouni M. Mykkänen;Jussi Tohka;Jouni Luoma;Ulla Ruotsalainen	2005	Computer methods and programs in biomedicine	10.1016/j.cmpb.2005.03.003	computer vision;nuclear medicine;segmentation;neuroimaging;medical physics	Vision	44.98433777447646	-76.46972503967963	102746
3687d5cf55366416717d94bb23f32f10b3a63dd2	a statistical image fusion scheme for multi focus applications	transformation ondelette;modelo markov oculto;analisis estadistico;image processing;modelo markov;maximum likelihood;modele markov cache;hidden markov model;image fusion;modele markov variable cachee;maximum vraisemblance;procesamiento imagen;image multiple;mesure niveau;imagen multiple;intelligence artificielle;data fusion;probabilistic approach;transformacion hough;traitement image;multiple image;haute frequence;markov model;hidden markov models;statistical analysis;enfoque probabilista;approche probabiliste;fusion donnee;analyse statistique;level measurement;artificial intelligence;hough transformation;hough transform;transformation hough;inteligencia artificial;transformacion ondita;modele markov;fusion datos;alta frecuencia;high frequency;maxima verosimilitud;wavelet transformation;medicion nivel;variance;variancia	In this paper, we propose a statistical scheme to judge the activity level measurement (ALM) that is based on wavelet-domain hidden Markov model (WD-HMM) and maximum likelihood (MLK). The source images are firstly decomposed by the wavelets and only the coefficients in the high frequency (HH) are utilized. Considering the shift-variance of wavelets, the merged image is obtained from the source images directly. The regions of each source image are obtained by the Hough transform (HT) and their ALM are decided by the ALM of their coefficients in HH according to MLK. Finally, two multi focus images are merged by our new framework. The fusion results show the high ability of our scheme in preserving edge information and avoiding shift-variant.	cobham's thesis;coefficient;hidden markov model;hough transform;image fusion;markov chain;wavelet	Zehua Liao;S. X. Hu;W. F. Chen;Yuan Yan Tang;T. Z. Huang	2005		10.1007/11739685_115	hough transform;speech recognition;computer science;artificial intelligence;mathematics;hidden markov model;statistics	Vision	53.18396086038169	-67.08095354099474	102779
710ed5c7b9f005bfb21249402b1a0699210bc904	on segmentation of images having multi-regions using gaussian type radial basis kernel in fuzzy sets framework		Abstract Segmentation of images having multi-objects with intensity inhomogeneity and noise is always challenging. In this paper, we propose a new model for segmentation of images having multi-objects with varying intensity. In the proposed model we develop a novel kernel metric which is based on generalized averages. To ensure its applicability in noisy images we use Gaussian type radial basis kernel. To speed up the convergence and to get global optima of the proposed model, we express energy functional of our model in fuzzy Pseudo level set formulation. The proposed model works well in images having multi-objects with intensity inhomogeneity and noise. Our proposed model also works very well in images having maximum, minimum or average intensity background. Instead of length term we use Gaussian smoothing for regularization of Pseudo level set (fuzzy membership function). Experimental results show better performance of the proposed model over existing state of the art models qualitatively and quantitatively (Jaccard similarity).	fuzzy set;kernel (operating system);radial (radio)	Noor Badshah;Ali Ahmad	2018	Appl. Soft Comput.	10.1016/j.asoc.2017.12.029	mathematical optimization;mathematics;kernel (linear algebra);jaccard index;fuzzy logic;level set;fuzzy set;gaussian;membership function;gaussian blur	ML	50.49331636319903	-71.15846175474088	102938
08d3311b73469b8b78c190f0d1c6f86eaac7c72d	estimating the usefulness of preprocessing in colour image segmentation.				Henryk Palus	2004			segmentation-based object categorization;image segmentation;scale-space segmentation	Vision	41.81657090916635	-71.13418799105112	102984
22c1eac72c9a8b6674bd42ef37692a7ae6f8bb95	a genetic algorithm for image segmentation	eigenvalues and eigenfunctions;global optimization problem;image recognition;image segmentation;image matching;genetic algorithms image segmentation surface fitting visual system layout image recognition shape deformable models eigenvalues and eigenfunctions humans;surface fitting;deformable models;layout;genetics;shape;image matching image segmentation genetic algorithms;real images genetic algorithm image segmentation global optimization problem fitness function image similarity;genetic algorithm;genetic algorithms;global optimization;humans;real images;visual system;fitness function;image similarity	The paper describes a new algorithm for image segmentation, It is based on a genetic approach that allow us to consider the segmentation problem as a global optimization problem (GOP). For this purpose afitness function, based on the similarity between images, has been dejned. The similarity is function of both the intensity and spatial position of pixels. Preliminary result, obtained using real images, show a good performance of the segmentation algorithm.	genetic algorithm;global optimization;image segmentation;is functions;mathematical optimization;optimization problem;pixel	Giosuè Lo Bosco	2001		10.1109/ICIAP.2001.957019	computer vision;mathematical optimization;range segmentation;genetic algorithm;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;global optimization	Vision	43.95683676737095	-66.71261875843976	103057
b3103c9813c24265319fcf1e09fa7b54ee419b98	chestwall segmentation in 3d breast ultrasound using a deformable volume model	ultrasound;prior knowledge;article letter to editor;false positive;volume modeling	A deformable volume segmentation method is proposed to detect the breast parenchyma in frontal scanned 3D whole breast ultrasound. Deformable volumes are a viable alternative to the deformable surface paradigm in noisy images with poorly defined object boundaries. A deformable ultrasound volume model was developed containing breast, rib, intercostal space and thoracic shadowing. Using prior knowledge about grey value statistics and shape the parameterized model deforms by optimization to match an ultrasound scan. Additionally a rib shadow enhancement filter was developed based on a Hessian sheet detector. An ROC chestwall detection study on 88 multi-center scans (20 non-visible chestwalls) showed a significant accuracy which improved strongly using the sheet detector. The results show the potential of our methodology to extract breast parenchyma which could help reduce false positives in subsequent computer aided lesion detection.	bone structure of rib;chest;detectors;hessian;mathematical optimization;medical ultrasound;numerous;parenchyma;programming paradigm;receiver operator characteristics;scanning;shadow and highlight enhancement;shadowing (histology);ultrasonography;ultrasonography, mammary;biologic segmentation	Henkjan J. Huisman;Nico Karssemeijer	2007	Information processing in medical imaging : proceedings of the ... conference	10.1007/978-3-540-73273-0_21	computer vision;radiology;medicine;pathology;type i and type ii errors;ultrasound	Vision	39.61875573283792	-78.99617414168638	103395
36db98fc353457d13210fc8fa210bf269aaed921	a fast and accurate segmentation method for medical images			medical imaging	Jiatao Wu;Yong Li;Yun Peng;Chunxiao Fan	2017		10.2352/ISSN.2470-1173.2017.2.VIPC-404	computer vision;scale-space segmentation;image segmentation;computer science;segmentation;artificial intelligence	Vision	41.732137075162754	-71.71040830929577	103416
4fc6fc4676474767c14cfae4c625426cbe4e2442	enforcing monotonous shape growth or shrinkage in video segmentation		We propose a new method based on graph cuts for joint segmentation of monotonously growing or shrinking shapes in time series of noisy images. By introducing directed infinite links connecting pixels at the same spatial locations in successive image frames, we impose shape growth/shrinkage constraint in graph cuts. Minimization of energy computed on the resulting graph of the image sequence yields globally optimal segmentation. We validate the proposed approach on two applications: segmentation of melting sea ice floes from a time series of multimodal satellite images and segmentation of a growing brain tumor from sequences of 3D multimodal medical scans. In the latter application, we impose an additional inter-sequences inclusion constraint by adding directed infinite links between pixels of dependent image structures.	3d computer graphics;cut (graph theory);graph cuts in computer vision;linear scale;manifold regularization;mathematical optimization;matrix regularization;maxima and minima;multimodal interaction;pixel;time series;unified model;weatherstar	Yuliya Tarabalka;Guillaume Charpiat;Ludovic Brucker;Bjoern H. Menze	2013		10.5244/C.27.27	computer vision;mathematical optimization;machine learning;segmentation-based object categorization;mathematics;image segmentation;scale-space segmentation	Vision	51.19353801346148	-71.3817240763022	103434
5f52ba7e9e8a349bb3e6efe9330e88e26ab5d6b3	stochastic information gradient algorithm with generalized gaussian distribution model	stochastic information gradient sig algorithm;least mean p power lmp;generalized gaussian density ggd;minimum error entropy criterion mee	This paper presents a parameterized version of the stochastic information gradient (SIG) algorithm, in which the error distribution is modeled by generalized Gaussian density (GGD), with location, shape, and dispersion parameters. Compared with the kernel-based SIG (SIGKernel) algorithm, the GGD-based SIG (SIG-GGD) algorithm does not involve kernel width selection. If the error is zero-mean, the SIG-GGD algorithm will become the least mean p-power (LMP) algorithm with adaptive order and variable step-size. Due to its well matched density estimation and automatic switching capability, the proposed algorithm is favorably in line with existing algorithms.	algorithm;gradient;kernel (operating system);kernel density estimation;simulation	Badong Chen;José Carlos Príncipe;Jinchun Hu;Yu Zhu	2012	Journal of Circuits, Systems, and Computers	10.1142/S0218126612500065	mathematical optimization;machine learning;mathematics;statistics	ML	51.65503080667389	-70.1713263512419	103449
4b703f822fc64bb16bc55e09035b2b047dfd436d	bayesian blind separation and deconvolution of dynamic image sequences using sparsity priors	probabilistic and statistical methods blind source separation computer aided detection and diagnosis functional imaging;kernel convolution blind source separation mathematical model estimation vectors bayes methods;radioisotope imaging bayes methods biological organs biological tissues blind source separation convolution deconvolution image segmentation mean square error methods medical image processing;bayesian blind separation matlab mean absolute errors mean square errors tissue separation dynamic renal scintigraphy sparse convolution kernel sparse source image variational bayes method organ retention function organ impulse response source specific kernel probabilistic model blind source separation method sparsity priors dynamic image sequence deconvolution	A common problem of imaging 3-D objects into image plane is superposition of the projected structures. In dynamic imaging, projection overlaps of organs and tissues complicate extraction of signals specific to individual structures with different dynamics. The problem manifests itself also in dynamic tomography as tissue mixtures are present in voxels. Separation of signals specific to dynamic structures belongs to the category of blind source separation. It is an underdetermined problem with many possible solutions. Existing separation methods select the solution that best matches their additional assumptions on the source model. We propose a novel blind source separation method based on probabilistic model of dynamic image sequences assuming each source dynamics as convolution of an input function and a source specific kernel (modeling organ impulse response or retention function). These assumptions are formalized as a Bayesian model with hierarchical prior and solved by the Variational Bayes method. The proposed prior distribution assigns higher probability to sparse source images and sparse convolution kernels. We show that the results of separation are relevant to selected tasks of dynamic renal scintigraphy. Accuracy of tissue separation with simulated and clinical data provided by the proposed method outperformed accuracy of previously developed methods measured by the mean square and mean absolute errors of estimation of simulated sources and the sources separated by an expert physician. MATLAB implementation of the algorithm is available for download.	algorithm;appendiceal neoplasms;appendix;bayesian network;bayesian programming;blind signal separation;body tissue;convolution;deconvolution;download;dynamic data;dynamic imaging;extraction;image plane;matlab;mean squared error;noise shaping;open-source software;organ;patients;physical object;population parameter;projections and predictions;quantum superposition;radionuclide imaging;solutions;source separation;sparse matrix;statistical model;trix (operating system);the matrix;tomography;variable (computer science);variational principle;visually impaired persons;voxel;mixture	Ondrej Tichy;Václav Smídl	2015	IEEE Transactions on Medical Imaging	10.1109/TMI.2014.2352791	computer science;machine learning;pattern recognition;blind signal separation;blind deconvolution;statistics	Vision	47.48741397620161	-79.98243114934306	103469
b08c62359fd5f3ddbf9d50caa37f8b0c3811baa2	mutual information preconditioning improves structure learning of bayesian networks from medical databases	databases;structure learning;belief networks;bayesian network;medical information systems belief networks learning artificial intelligence matrix algebra medical diagnostic computing;uncertainty;bayesian methods;standard search and score approach;matrix algebra;maximum relevance minimum redundancy selection strategy;packaging;treatment outcome;square matrix binarization;network topology;bn structure learning algorithm;medical diagnostic tools;medical datasets;physiology;algorithms artificial intelligence bayes theorem databases factual humans medical informatics medical records systems computerized;mutual information preconditioning;medical information systems;mi binary matrix;structural learning;treatment outcome prediction;mutual information mi;mutual information bayesian methods databases medical diagnostic imaging medical treatment councils physiology packaging uncertainty network topology;medical datasets mutual information preconditioning bayesian networks bn structure learning algorithm medical databases medical diagnostic tools treatment outcome prediction standard search and score approach redundant network structures square matrix binarization maximum relevance minimum redundancy selection strategy mi binary matrix subsequent greedy search procedure;structural learning bayesian network bn biomedical data mutual information mi;councils;mutual information;redundant network structures;subsequent greedy search procedure;bayesian network bn;network structure;learning artificial intelligence;medical treatment;biomedical data;medical databases;medical diagnostic computing;medical diagnosis;medical diagnostic imaging;bayesian networks	Bayesian networks (BNs) represent one of the most successful tools for medical diagnosis, selection of the optimal treatment, and prediction of the treatment outcome. In this paper, we present an algorithm for BN structure learning, which is a variation of the standard search-and-score approach. The proposed algorithm overcomes the creation of redundant network structures that may include nonsignificant connections between variables. In particular, the algorithm finds what relationships between the variables must be prevented, by exploiting the binarization of a square matrix containing the mutual information (MI) among all pairs of variables. Two different binarization methods are implemented. The first one is based on the maximum relevance minimum redundancy selection strategy. The second one uses a threshold. The MI binary matrix is exploited as a preconditioning step for the subsequent greedy search procedure that optimizes the network score, reducing the number of possible search paths in the greedy search. Our algorithm has been tested on two different medical datasets and compared against the standard search-and-score algorithm as implemented in the DEAL package.	bayesian network;binary image;complex network;deal;greedy algorithm;hospital admission;inference;large;mutual information;numerical analysis;preconditioner;relevance;topological skeleton;z-score	Antonella Meloni;Andrea Ripoli;Vincenzo Positano;Luigi Landini	2009	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2009.2026273	computer science;artificial intelligence;machine learning;pattern recognition;bayesian network;data mining;statistics	ML	42.18221635030673	-76.38179428757086	103657
cd111305d0ec884ea09af8a006abdc87088ff55c	variational level set approach for automatic correction of multiplicative and additive intensity inhomogeneities in brain mr images	brain;image segmentation;computational geometry;brain tissue segmentation variational level set approach automatic intensity inhomogeneity correction multiplicative intensity inhomogeneities additive intensity inhomogeneities brain mr images retrospective correction brain magnetic resonance images mr image degradation model bias field estimation receptor signal intensity linear image degradation model intensity inhomogeneity estimation image segmentation process;brain databases factual female humans image enhancement magnetic resonance imaging male models theoretical;medical image processing biomedical mri brain computational geometry image segmentation;medical image processing;nonhomogeneous media additives image segmentation degradation brain modeling magnetic resonance imaging biomedical imaging;biomedical mri	Retrospective correction of intensity inhomogeneities in magnetic resonance images of the brain is an essential pre-processing step before any sophisticated image analysis task can be performed. A popular choice when defining the degradation model in MR images is to use multiplicative intensity inhomogeneities that slowly varying across the image domain; this approach has been extensively used for bias field estimation. However, such a multiplicative model is often insufficient given that some of the most dominant physical causes of intensity inhomogeneities in MRI (such as nonuniform excitation strength) have a non-linear relationship with the receptor signal intensity. In this study, we consider a linear image degradation model with multiplicative and additive intensity inhomogeneity components. We propose a variational level sets approach that combines estimation of intensity inhomogeneity components during the image segmentation process. The evaluation of proposed approach on real MR image datasets demonstrate accurate estimation of multiplicative and additive intensity inhomogeneities improving brain tissue segmentation.	calculus of variations;elegant degradation;estimated;excitation;image analysis;image segmentation;nonlinear system;preprocessor;resonance;segmentation action;utility functions on indivisible goods;variational principle;biologic segmentation	Nishant Verma;Matthew C. Cowperthwaite;Mia K. Markey	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6345880	computer vision;computational geometry;computer science;image segmentation;nuclear magnetic resonance;scale-space segmentation;medical physics	Vision	44.459154521887186	-79.75430460755864	103754
8fb20c54a6ceb72c588498d858d4474e622b40d1	image segmentation and selective smoothing based on variational framework	second order;median filtering;image segmentation;probability density function;biomedical imaging;image smoothing;probability distribution;variational framework;fourth order pde model;active contour model	This paper addresses the segmentation and smoothing problems in biomedical imaging under variational framework. In order to get better results, this paper proposes a new segmentation and selective smoothing algorithm. This paper has the following three contributions. First, a new statistical active contour model (SACM) is introduced for noisy image segmentation. SACM is proposed to solve the problem in fast edge integration (FEI) method, which takes advantages of both edge-based and region-based active contour model but only considers the mean information inside and outside of the evolution curve. In SACM, a new statistical term for considering the probability distribution density of regions and a unified variational framework are proposed for construction of different segmentation models with different probability density functions. Moreover, a penalized term is also introduced in the proposed model as internal energy in order to avoid the time consuming re-initialization process. Second, a new symmetric fourth-order PDE denoising algorithm is developed to avoid the blocky effects in second-order PDE model, while preserving edges. Third, in each stage of segmentation process, different denoising algorithms (or different parameters in the same denoising model) can be employed for different sub-regions independently, so that better segmentation and smoothing results can be obtained. Compared with existing methods, our method is more flexible, robust to noise, computationally efficient and produces better results.	active contour model;algorithm;algorithmic efficiency;call of duty: black ops;code;computation;entity–relationship model;ibm notes;image noise;image processing;image segmentation;mathematical optimization;medical imaging;noise reduction;numerical analysis;smoothing;variational method (quantum mechanics);variational principle;video denoising	Bo Chen;Pong C. Yuen;Jianhuang Lai;Wensheng Chen	2009	Signal Processing Systems	10.1007/s11265-008-0248-9	medical imaging;probability distribution;median filter;computer vision;mathematical optimization;probability density function;computer science;machine learning;segmentation-based object categorization;active contour model;mathematics;image segmentation;scale-space segmentation;second-order logic;statistics	Vision	51.163516663008004	-71.25037256342625	104152
3115f95e231213443bf13534a94970ce4294ea96	an effective approach for automatic lv segmentation based on gmm and asm		In this paper, we propose a novel approach for automatic left ventricle LV segmentation in cardiac magnetic resonance images CMRI. This algorithm incorporates three key techniques: 1 the mid-ventricular coarse segmentation based on Gaussian mixture model GMM; 2 the mid-slice endo-/epi-cardial initialization based on geometric transformation; 3 the myocardium tracking based on active shape models ASM. Experiment results tested on a standard database demonstrate the effectiveness and competitiveness of the proposed method.	automatic summarization;google map maker;logical volume management	Yurun Ma;Deyuan Wang;Yide Ma;Ruoming Lei;Min Dong;Kemin Wang;Li Wang	2016		10.1007/978-3-319-46672-9_74	geometric transformation;active shape model;mixture model;computer science;pattern recognition;initialization;scale-space segmentation;computer vision;artificial intelligence;segmentation	NLP	43.309401591220414	-74.11784554109276	104271
6ec4e5d56cf8ba8f4a3fc15986d9ec9373c45e22	fast tensor image morphing for elastic registration	optimal solution;sensitivity and specificity;deformable registration;brain;high dimensionality;degree of freedom;correspondence problem;image enhancement;image interpretation computer assisted;reproducibility of results;artificial intelligence;algorithms;pattern recognition automated;humans;subtraction technique;topology preservation;local minima;thin plate spline;elasticity imaging techniques	We propose a novel algorithm, called Fast Tensor Image Morphing for Elastic Registration or F-TIMER. F-TIMER leverages multiscale tensor regional distributions and local boundaries for hierarchically driving deformable matching of tensor image volumes. Registration is achieved by aligning a set of automatically determined structural landmarks, via solving a soft correspondence problem. Based on the estimated correspondences, thin-plate splines are employed to generate a smooth, topology preserving, and dense transformation, and to avoid arbitrary mapping of non-landmark voxels. To mitigate the problem of local minima, which is common in the estimation of high dimensional transformations, we employ a hierarchical strategy where a small subset of voxels with more distinctive attribute vectors are first deployed as landmarks to estimate a relatively robust low-degrees-of-freedom transformation. As the registration progresses, an increasing number of voxels are permitted to participate in refining the correspondence matching. A scheme as such allows less conservative progression of the correspondence matching towards the optimal solution, and hence results in a faster matching speed. Results indicate that better accuracy can be achieved by F-TIMER, compared with other deformable registration algorithms, with significantly reduced computation time cost of 4-14 folds.	555 timer ic;algorithm;anatomy, regional;assignment problem;color gradient;computation;correspondence problem;matching;mathematical optimization;maxima and minima;morphing;programmable interval timer;sarcoma;subgroup;thin plate spline;time complexity;voxel;cell transformation;registration - actclass	Pew-Thian Yap;Guorong Wu;Hongtu Zhu;Weili Lin;Dinggang Shen	2009	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-04268-3_89	computer vision;mathematical optimization;computer science;artificial intelligence;machine learning;maxima and minima;mathematics;geometry;thin plate spline;degrees of freedom;correspondence problem	Vision	44.98325931354768	-75.98192956132714	104452
150dbb7d37026e9d0500ec887f913061928ff57e	automatic determination of the appropriate number of clusters for multispectral image data	multispectral image;clustering;co occurrence		multispectral image	Kitti Koonsanit;Chuleerat Jaruskulchai	2012	IEICE Transactions		multispectral image;computer vision;co-occurrence;computer science;machine learning;pattern recognition;multispectral pattern recognition;cluster analysis	Vision	41.744712129554415	-66.76893663729297	104816
971bd7926198c155f48c381c17c7e952fc04a45a	regional flux analysis for discovering and quantifying anatomical changes: an application to the brain morphometry in alzheimer's disease		In this study we introduce the regional flux analysis, a novel approach to deformation based morphometry based on the Helmholtz decomposition of deformations parameterized by stationary velocity fields. We use the scalar pressure map associated to the irrotational component of the deformation to discover the critical regions of volume change. These regions are used to consistently quantify the associated measure of volume change by the probabilistic integration of the flux of the longitudinal deformations across the boundaries. The presented framework unifies voxel-based and regional approaches, and robustly describes the volume changes at both group-wise and subject-specific level as a spatial process governed by consistently defined regions. Our experiments on the large cohorts of the ADNI dataset show that the regional flux analysis is a powerful and flexible instrument for the study of Alzheimer's disease in a wide range of scenarios: cross-sectional deformation based morphometry, longitudinal discovery and quantification of group-wise volume changes, and statistically powered and robust quantification of hippocampal and ventricular atrophy.	alzheimer's disease;atrophic;cross-sectional data;experiment;flux;morphometric analysis;morphometrics;power (psychology);quantitation;silo (dataset);spinocerebellar ataxia type 2;stationary process;velocity (software development);voxel	Marco Lorenzi;Nicholas Ayache;Xavier Pennec	2015	NeuroImage	10.1016/j.neuroimage.2015.04.051	bioinformatics;artificial intelligence	Vision	43.8256826001893	-79.0316450851993	104861
261d674170c808873eea4b568089df1996c30ee0	automatic construction of 3d statistical deformation models using non-rigid registration	mr imaging;statistical analysis;non rigid registration;deformable model;active shape model	In this paper we introduce the concept of statistical deformation models (SDM) which allow the construction of average models of the anatomy and their variability. SDMs are built by performing a statistical analysis of the deformations required to map anatomical features in one subject into the corresponding features in another subject. The concept of SDMs is similar to active shape models (ASM) which capture statistical information about shapes across a population but offers several new advantages over ASMs: Firstly, SDMs can be constructed directly from images such as MR or CT without the need for segmentation which is usually a prerequisite for the construction of active shape models. Instead a non-rigid registration algorithm is used to compute the deformations required to establish correspondences between the reference subject and the subjects in the population class under investigation. Secondly, SDMs allow the construction of an atlas of the average anatomy as well as its variability across a population of subjects. Finally, SDMs take the 3D nature of the underlying anatomy into account by analysing dense 3D deformation fields rather than only the 2D surface shape of anatomical structures. We demonstrate the applicability of this new framework to MR images of the brain and show results for the construction of anatomical models from 25 different subjects.	active shape model;algorithm;automatic taxonomy construction;spatial variability	Daniel Rueckert;Alejandro F. Frangi;Julia A. Schnabel	2001		10.1007/3-540-45468-3_10	active shape model;computer vision;simulation;computer science;mathematics;statistics	Vision	43.73757731689064	-78.3917964062236	104884
cb7b0ee30dc82934963f69d0d5d5c8f929575d0d	the analysis of saliency processes and its application to grouping cues design	shape humans image segmentation image retrieval image edge detection length measurement random variables image analysis application software computer science;bayesian considerations saliency process analysis grouping cues design image retrieval object shapes salient structure detection perceptual organization saliency algorithms image measurements;perceptual organization;image retrieval	Retrieving images using object shapes usually requires to specify the major (or salient) structures in the image. Detecting salient structures is a basic task in perceptual organization. Saliency algorithms depend on grouping cues and mark edge-points with some saliency measure, which typically grows with the length and smoothness of the curve on which these edge-points lie. We consider a particular saliency mechanism [20] and focus on one aspect of its design: how different image measurements should be combined to get an effective grouping cue, leading to better specifications of the salient structures. We discuss 5 different combination methods, based, respectively, on a common heuristic, on Bayesian considerations (two versions), on optimizing a figure-ground separation measure using recent analysis results, and on incorporation of global information using bootstrap. Three of these cues are refinements of older methods and two are completely new. The advantage of the later methods is demonstrated.	algorithm;booting;heuristic	Roman Golubchyck;Michael Lindenbaum	2007	2007 International Workshop on Content-Based Multimedia Indexing	10.1109/CBMI.2007.385387	computer vision;image retrieval;computer science;machine learning;pattern recognition	Vision	47.92869780789616	-70.77940008289792	105203
272cbe761d4dbdb79b88aaeda5b19067b23f3835	digital image compression based on non-stationary and inhomogeneous multiresolution analysis	scale function;image coding;data compression;cost function;digital images image coding image resolution image analysis multiresolution analysis wavelet analysis filters wavelet packets tensile stress decoding;digital filters data compression image coding adaptive codes wavelet transforms transform coding;transform coding;adaptive codes;wavelet decomposition;wavelet transforms;tensor product;image compression;digital filters;digital image;multiresolution analysis;minimizing cost functions digital image compression inhomogeneous multiresolution analyses nonstationary multiresolution analyses adaptive wavelet based image compression methods scaling functions tensor product	Adaptive wavelet-based image compression methods are introduced. Contrasting to the classical wavelet decomposition scheme one can use different wavelet and scaling functions at every scale this leads to nonstationary multiresolution analysis. An inhomogeneous multiresolution analysis is obtained by using different functions for the two directions in the tensor product of the bidimensional multiresolution analysis, furthermore these two methods are combined. The freedom in using different functions is exploited for image compression in an adaptive way. That special non-stationary and/or inhomogeneous multiresolution analysis is built out of the functions in a given library that is best suited for compression of a given image. This is done by minimizing cost-functions a t every scale-level.	digital image;image compression;image scaling;multiresolution analysis;stationary process;wavelet	Andreas Uhl	1994		10.1109/ICIP.1994.413823	data compression;multiresolution analysis;tensor product;computer vision;mathematical optimization;transform coding;digital filter;image compression;computer science;theoretical computer science;mathematics;digital image;algorithm;wavelet transform	Vision	53.33122485343987	-67.59912988120458	105367
bcb710fb8d633b346385a618ce7c15044e7cb369	regional curvature analysis in the left ventricle of the heart using hotelling t2 metric	aha standard;3d object;myocardium scintigraphy;hotelling t2 metric;surface curvature;heart s left ventricle	This paper proposes a method for local deformation analysis of the heart's left ventricle (LV) which is of an utmost interest in characterizing the myocardium disease extent and severity. Our method is based on regional curvature variation calculation using the Hotelling T2 two samples difference metric. Our approach is validated on real data obtained from myocardial scintigraphy imaging technique. The curvature variation is calculated at stress and rest. Experimental results demonstrate the effectiveness and the validity of our approach.	logical volume management;ultrasparc t2	Rim Ayari;Asma Ben Abdallah;Mohamed Bedoui Hedi;Faouzi Ghorbel	2014	JCP	10.4304/jcp.9.7.1572-1576	mathematical optimization;topology;mathematics;geometry	Vision	42.26091443528254	-79.71563960966012	105428
33417b5fa74359a1a1a58831637c0fb5f749a188	a comparative study of individual and ensemble majority vote cdna microarray image segmentation schemes, originating from a spot-adjustable based restoration framework	fuzzy c mean;image segmentation;restoration;segmentation;gene expression;seeded region growing;majority vote segmentation;noise reduction;gaussian kernel;cdna microarray;unsupervised classification;image analysis;majority voting	The aim of this study was to comparatively evaluate the performances of various segmentation algorithms, in conjunction with a noise reduction step, for gene expression levels intensity extraction in cDNA microarray images. Different segmentation algorithms, based on histogram and unsupervised classification methods, which have never been previously employed in microarray image analysis, were employed either individually or in ensemble majority vote structures for separating spot-images from background pixels. The performances of segmentation algorithms or ensemble structures were evaluated by assessing the validity and reproducibility of gene expression levels extraction in simulated and real cDNA microarray images. By processing high quality simulated images, the highest segmentation accuracy was achieved by an ensemble structure (Histogram Concavity, Gaussian Kernelized Fuzzy-C-Means, Seeded Region Growing). Optimum performance in terms of processing time and segmentation precision for low quality simulated and replicated real cDNA microarray images was attained by the Histogram Concavity algorithm.	algorithm;circuit restoration;dna microarray;dna, complementary;display resolution;gene expression;histogram;image analysis;image segmentation;kernel method;noise reduction;normal statistical distribution;performance;pixel;region growing;unsupervised learning;biologic segmentation	Antonis Daskalakis;Dimitris Glotsos;Spiros A. Kostopoulos;Dionisis A. Cavouras;George Nikiforidis	2009	Computer methods and programs in biomedicine	10.1016/j.cmpb.2009.01.007	majority rule;computer vision;image analysis;gene expression;computer science;machine learning;segmentation-based object categorization;pattern recognition;noise reduction;data mining;region growing;image segmentation;scale-space segmentation;segmentation;gaussian function	Vision	40.56085066888658	-72.40995142284092	105544
5e15f00cac54429107f7950fe963d106a99c6e7d	genetic algorithm-based interactive segmentation of 3d medical images	image tridimensionnelle;vision ordenador;medical imagery;structure anatomique;aplicacion medical;contour;segmentation;resonancia magnetica;algoritmo genetico;computer vision;medical image;magnetic resonance;filter;image sequence;tomographie;computer aid;imagerie medicale;algorithme genetique;tridimensional image;genetic algorithm;asistencia ordenador;vision ordinateur;medical application;contorno;imageneria medical;elastic contour;tomografia;resonance magnetique;tomography;assistance ordinateur;segmentacion;imagen tridimensional;interactive segmentation;application medicale	This article describes a method for evolving adaptive procedures for the contour-based segmentation of anatomical structures in 3D medical data sets. With this method, the user first manually traces one or more 2D contours of an anatomical structure of interest on parallel planes arbitrarily cutting the data set. Such contours are then used as training examples for a genetic algorithm to evolve a contour detector. By applying the detector to the rest of the image sequence it is possible to obtain a full segmentation of the structure. The same detector can then be used to segment other image sequences of the same sort. Segmentation is driven by a contour-tracking strategy that relies on an elastic-contour model whose parameters are also optimized by the genetic algorithm. We report results obtained on a software-generated phantom and on real tomographic images of different sorts. q 1999 Elsevier Science B.V. All rights reserved.	computation;genetic algorithm;mathematical optimization;particle filter;phantom reference;sensor;software framework;software release life cycle;time complexity;tomography;tracing (software)	Stefano Cagnoni;A. B. Dobrzeniecki;Riccardo Poli;J. C. Yanch	1999	Image Vision Comput.	10.1016/S0262-8856(98)00166-8	computer vision;genetic algorithm;filter;computer science;artificial intelligence;segmentation-based object categorization;tomography;image segmentation;scale-space segmentation;segmentation	Vision	46.10495024333244	-78.97223949952533	105578
8b4c4e7e12b7d2dcc5658db9e91ed8556378c161	lignes de partage des eaux discrètes : théorie et application à la segmentation d'images cardiaques. (discrete watersheds: theory and applications to cardiac image segmentation)		This paper deals with mathematical properties of watersheds in weighted graphs linked to region merging methods, as used in image analysis. In a graph, a cleft (or a binary watershed) is a set of vertices that cannot be reduced, by point removal, without changing the number of regions (connected components) of its complement. To obtain a watershed adapted to morphological region merging, it has been shown that one has to use the topological thinnings introduced by M. Couprie and G. Bertrand. Unfortunately, topological thinnings do not always produce thin clefts. Therefore, we introduce a new transformation on vertex weighted graphs, called C-watershed, that always produces a cleft. We present the class of perfect fusion graphs, for which any two neighboring regions can be merged, while preserving all other regions, by removing from the cleft the points adjacent to both. An important theorem of this paper states that, on these graphs, the C-watersheds are topological thinnings and the corresponding divides are thin clefts. We propose a linear-time immersion-like monotone algorithm to compute C-watersheds on perfect fusion graphs, whereas, in general, a linear-time topological thinning algorithm does not exist. Finally, we derive some characterizations of perfect fusion graphs based on thinness properties of both C-watersheds and topological watersheds.		Jean Cousty	2007				Vision	44.40993643091514	-69.29630658655219	105712
eaea7e601466cde6dba9b4e3b8aea58d84490f78	formulation of four katsevich algorithms in native geometry	native cylindrical detector geometry;computed tomography;computed tomography ct;wide cone beam aperture;cone beam reconstruction;computed tomography filtering algorithms image reconstruction biomedical imaging detectors computational geometry object detection equations apertures reconstruction algorithms;cone beam;four exact helical katsevich algorithms;algorithms imaging three dimensional information storage and retrieval numerical analysis computer assisted radiographic image enhancement radiographic image interpretation computer assisted reproducibility of results sensitivity and specificity signal processing computer assisted tomography spiral computed;computed tomography scanners;efficient implementation;image reconstruction;medical image processing;exact algorithm;computerised tomography;biomedical image processing;exact algorithms;medical image processing computerised tomography image reconstruction;shift invariant filtering computed tomography ct cone beam reconstruction exact algorithms;shift invariant;wide cone beam aperture four exact helical katsevich algorithms native cylindrical detector geometry computed tomography scanners;shift invariant filtering	We derive formulations of the four exact helical Katsevich algorithms in the native cylindrical detector geometry, which allow efficient implementation in modern computed tomography scanners with wide cone beam aperture. Also, we discuss some aspects of numerical implementation	algorithm;ct scan;detectors;numerical analysis;x-ray computed tomography	Alexander Katsevich;Katsuyuki Taguchi;Alexander A. Zamyatin	2006	IEEE Transactions on Medical Imaging	10.1109/TMI.2006.876159	iterative reconstruction;computer vision;radiology;medicine;mathematics;optics;computed tomography;shift-invariant system;medical physics	Visualization	51.9783560775036	-78.74729525784338	105786
c75e493589f3c4510e5442ed6c88ae0c51317b48	clutter-free volume rendering for magnetic resonance angiography using fuzzy connectedness			clutter;resonance;volume rendering	Bowen L. Rice;Jayaram K. Udupa	2000	Int. J. Imaging Systems and Technology	10.1002/(SICI)1098-1098(2000)11:1%3C62::AID-IMA7%3E3.0.CO;2-6	radiology;computer science	Visualization	45.10509492631436	-74.83775737061556	106111
39cd2798d5c571de9e84d1278bcd47498eda5ce6	automatic hippocampus localization in histological images using differential evolution-based deformable models	levenberg marquardt;automatic localization;differential evolution;scatter search;global continuous optimization;hippocampus;histological images;deformable models;simulated annealing;particle swarm optimization;genetic algorithms;hippocampus localization	9 In this paper, the localization of structures in biomedical images is con10 sidered as a multimodal global continuous optimization problem and solved 11 by means of soft computing techniques. We have developed an automatic 12 method aimed at localizing the hippocampus in histological images, after 13 discoveries indicating the relevance of structural changes of this region as 14 early biomarkers for Alzheimer’s disease and epilepsy. The localization is 15 achieved by searching the parameters of an empirically-derived deformable 16 model of the hippocampus which maximize its overlap with the corresponding 17 anatomical structure in histological brain images. The comparison between 18 six real-parameter optimization techniques (Levenberg-Marquardt, Differen19 tial Evolution, Simulated Annealing, Genetic Algorithms, Particle Swarm 20 Optimization and Scatter Search) shows that Differential Evolution signif21 icantly outperforms the other techniques in this task, providing successful 22 localizations in 90.9% and 93.0% of two test sets of real and synthetic im23 ∗Corresponding Author. Tel. +39 0521 905785 Email addresses: pmesejo@ce.unipr.it (Pablo Mesejo), rob_ugo@ce.unipr.it (Roberto Ugolotti), ferdinando.dicunto@unito.it (Ferdinando Di Cunto), mario.giacobini@unito.it (Mario Giacobini), cagnoni@ce.unipr.it (Stefano Cagnoni) Preprint submitted to Pattern Recognition Letters October 8, 2012 ages, respectively. 24	continuous optimization;dhrystone;differential evolution;email;genetic algorithm;levenberg–marquardt algorithm;mathematical optimization;multimodal interaction;optimization problem;particle swarm optimization;pattern recognition letters;relevance;simulated annealing;soft computing	Pablo Mesejo;Roberto Ugolotti;Ferdinando Di Cunto;Mario Giacobini;Stefano Cagnoni	2013	Pattern Recognition Letters	10.1016/j.patrec.2012.10.012	differential evolution;mathematical optimization;genetic algorithm;levenberg–marquardt algorithm;simulated annealing;computer science;artificial intelligence;machine learning;hippocampus;mathematics;particle swarm optimization	Vision	43.78531868570715	-71.96499788905037	106249
1a1a5f78aa9777983c6f1f4f44c509892d3c92bb	vector (self) snakes: a geometric framework for color, texture, and multiscale image segmentation	image processing algorithms;minimal weighted distance curves;geometric active contours;partial differential equation;electric shock;active contour;image segmentation;image processing;filtering theory image segmentation image texture edge detection partial differential equations;vector self snakes;edge detection;level set;filters;deforming curve;shock filters;active contours;active contours image segmentation partial differential equations extraterrestrial measurements stability image processing anisotropic magnetoresistance electric shock filters image decomposition;anisotropic diffusion;multiscale image segmentation;vector valued images;2d geometric active contours;image texture;stability;anisotropic diffusion filters;texture images;uniform regions;riemannian space;partial differential equations;geodesics;object boundaries;anisotropic magnetoresistance;geometric framework;curve flow;image decompositions geometric framework color image segmentation image texture multiscale image segmentation vector self snakes partial differential equations vector valued images 2d geometric active contours object boundaries geodesics minimal weighted distance curves riemannian space image components curve flow deforming curve uniform regions image processing algorithms anisotropic diffusion filters shock filters texture images;vector data;image decomposition;image components;extraterrestrial measurements;filtering theory;color image segmentation;image decompositions	A partial differential equations (PDEs) based geometric framework for segmentation of vector-valued images is described. The first component of this approach is based on two dimensional geometric active contours deforming from their initial position towards objects in the image. The boundaries of these objects are then obtained as geodesics or minimal weighted distance curves in a Riemannian space. The metric in this space is given by a definition of edges in vector-valued images, incorporating information from all the image components. The curve flow corresponding to these active contours holds formal existence, uniqueness, stability, and correctness results. Then, embedding the deforming curve as the level-set of the image, that is, deforming each one of the image components level-sets according to these active contours, a system of coupled PDEs is obtained. This system deforms the image towards uniform regions, obtaining a simplified (or segmented) image. The flow is related to a number of PDEs based image processing algorithms as anisotropic diffusion and shock filters. The technique is applicable to color and texture images, as well as to vector data obtained from general image decompositions.	image segmentation;texture mapping	Guillermo Sapiro	1996		10.1109/ICIP.1996.559624	image texture;computer vision;topology;image processing;computer science;mathematics;geometry;image segmentation;anisotropic diffusion;partial differential equation	Vision	49.400526787724566	-66.70820868561437	106264
1ec00c8c6088f4e495b57f8bd7e0eda4c3f20655	automated lung segmentation and smoothing techniques for inclusion of juxtapleural nodules and pulmonary vessels on chest ct images	curvature threshold;pulmonary vessel;lung segmentation;fuzzy c means clustering;juxtapleural nodule	Segmentation of the lung is often performed as an important preprocessing step for quantitative analysis of chest computed tomography (CT) imaging. However, the presence of juxtapleural nodules and pulmonary vessels, image noise or artifacts, and individual anatomical variety make lung segmentation very complex. To address these problems, a fast and fully automatic scheme based on iterative weighted averaging and adaptive curvature threshold is proposed in this study to facilitate accurate lung segmentation for inclusion of juxtapleural nodules and pulmonary vessels and ensure the smoothness of the lung boundary. Our segmentation scheme consists of four main stages: image preprocessing, thorax extraction, lung identification and lung contour correction. The aim of preprocessing stage is to encourage intra-region smoothing and preserve the inter-region edge of CT images. In the thorax extraction stage, the artifacts external to the patient’s body are discarded. Then, a fuzzy-c-means clustering method is used in the thorax region and all lung parenchyma is identified according to fuzzy membership value and connectivity. Finally, the segmented lung contour is smoothed and corrected with iterative weighted averaging and adaptive curvature threshold on each axis slice, respectively. Our method was validated on 20 datasets of chest CT scans containing 65 juxtapleural nodules. Experimental results show that our method can re-include all juxtapleural nodules and achieve an average volume overlap ratio of 95.81 ± 0.89% and an average mean absolute border distance of 0.63 ± 0.09 mm compared with the manually segmented results. The average processing time for segmenting one slice was 2.56 s, which is over 20 times faster than manual segmentation. © 2014 Elsevier Ltd. All rights reserved.	apache axis;ct scan;cluster analysis;image noise;iterative method;preprocessor;smoothing;tomography	Shengjun Zhou;Yuanzhi Cheng;Shinichi Tamura	2014	Biomed. Signal Proc. and Control	10.1016/j.bspc.2014.03.010	computer vision;radiology;medicine;pathology	Vision	39.34561594717193	-80.08602997981515	106454
23002d631403e5f914325658f201a6d3da8bc12f	diffeomorphic registration with self-adaptive spatial regularization for the segmentation of non-human primate brains		Cerebral aging has been linked to structural and functional changes in the brain throughout life. Here, we study the marmoset, a small non-human primate, in order to get insights into the mechanisms of brain aging in normal and pathological conditions. Imaging the brain of small animals with techniques such as MRI, quickly becomes a challenging task when compared with human brain imaging. Very often, a simple pre-processing step such as brain extraction cannot be achieved with classical tools. In this paper, we propose a diffeomorphic registration algorithm, which makes use of learned constraints to propagate the manual segmentation of a marmoset brain template to other MR images of marmoset brains. The main methological contribution of our paper is to explore a new strategy to automatically tune the spatial regularization of the deformations. Results show that we obtain a robust segmentation of the brain, even for images with a low contrast.	brain;callithrix;clinical use template;matrix regularization;preprocessor;primates;algorithm;biologic segmentation;registration - actclass	Laurent Risser;Lionel Dolius;Caroline Fonta;Muriel Mescam	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6945164	computer vision	Vision	42.28627852796996	-78.61997684524626	106599
ad7432aedf74d5ba5060ac86268733161d5edd6a	automatic recovery of the left ventricular blood pool in cardiac cine mr images	left ventricular;magnetic resonance image;left ventricle;mr imaging;fourier analysis;connected component	We present a method for automatic localization and rough segmentation of the left ventricle blood pool in cardiac cine magnetic resonance images. The method first detects the whole heart using time-based Fourier analysis. It then segments the left ventricle blood pool by grouping connected components across slices using isoperimetric clustering. The system was tested on 253 datasets and failed in only 2 cases.		Marie-Pierre Jolly	2008	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-540-85988-8_14	connected component;radiology;magnetic resonance imaging;mathematics;fourier analysis;anatomy;cardiology	Robotics	39.357054621732416	-79.42755902672117	106661
3d66a57b36589258a202b7e7522a0e3e772b30ec	steerable pyramid for contrast enhancement and directional structures detection	cancer;filtering theory;image enhancement;mammography;medical image processing;tumours;breast cancer detection;breast cancer diagnosis;contrast enhancement;directional structure detection;linear spicule radiating pattern;malignant density;mammograms;multiscale hiftable method;orientation shiftable method;oriented contrast enhancement;oriented filters;recursive multiscale transform;steerable filters;steerable pyramid;stellate masses;tumors	The object of this work is the application of steerable filters for the diagnosis of breast cancer. Because of the very specific geometry of malignant densities, we intend to apply a multiscale and orientation shiftable method for the detection of breast cancer in mammograms. Stellate masses have an irregular appearance and are frequently surrounded by a radiating pattern of linear spicules. Multiscale approaches using oriented filters have proved to be efficient to detect such stellate patterns. We first use the fact that tumors are always brighter than the surrounding tissues to locate suspicious regions. The detection is then based on steerable filters, which can be steered to any orientation fixed by the user, and are synthesized using a limited number of basis filters. These filters are used in a recursive multi-scale transform: the steerable pyramid. Using the steerable pyramid, oriented contrast enhancement is performed. The resulting enhanced image is then analyzed to estimate whether a stellate mass is present or not.	image editing;pixel;pyramid (image processing);quantization (signal processing);radiology;recursion;steerable filter	Florence Denis;Franck Davignon;Atilla Baskurt	2002	2002 11th European Signal Processing Conference		computer vision;electronic engineering;engineering;optics	Vision	40.06808678036441	-74.9866207262701	106769
5eba23982b9cf6b310c78bb8aa9d1a005588fbac	conditional entropy maximization for pet image reconstruction using adaptive mesh model	clinical data;delaunay triangulation;adaptive mesh;adaptive mesh model;pet;pet imaging;positron emission tomography;image reconstruction;conditional entropy;adaptive smoothing;conditional entropy maximization	Iterative image reconstruction algorithms have been widely used in the field of positron emission tomography (PET). However, such algorithms are sensitive to noise artifacts so that the reconstruction begins to degrade when the number of iterations is high. In this paper, we propose a new algorithm to reconstruct an image from the PET emission projection data by using the conditional entropy maximization and the adaptive mesh model. In a traditional tomography reconstruction method, the reconstructed image is directly computed in the pixel domain. Unlike this kind of methods, the proposed approach is performed by estimating the nodal values from the observed projection data in a mesh domain. In our method, the initial Delaunay triangulation mesh is generated from a set of randomly selected pixel points, and it is then modified according to the pixel intensity value of the estimated image at each iteration step in which the conditional entropy maximization is used. The advantage of using the adaptive mesh model for image reconstruction is that it provides a natural spatially adaptive smoothness mechanism. In experiments using the synthetic and clinical data, it is found that the proposed algorithm is more robust to noise compared to the common pixel-based MLEM algorithm and mesh-based MLEM with a fixed mesh structure.	ct scan;conditional entropy;delaunay triangulation;entropy maximization;estimated;experiment;iteration;iterative method;iterative reconstruction;morphologic artifacts;pixel;polyethylene terephthalate;positron-emission tomography;positrons;randomness;synthetic intelligence;tomography, emission-computed, single-photon	Hongqing Zhu;Huazhong Shu;Jian Zhou;Xiubing Dai;Limin Luo	2007	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1016/j.compmedimag.2007.01.001	iterative reconstruction;computer vision;mathematical optimization;radiology;medicine;delaunay triangulation;machine learning;mathematics;pet;conditional entropy;statistics	Vision	52.8693940042635	-75.53218366171382	106913
e0d1ceddc97735e3e06005f90acf8361ab272c32	modeling surface from a single grayscale image	automatic surface model acquisition;signal detection image processing;image processing;two dimensions;intensity constraint;signal detection;needle map;2d cellular automata system;single grayscale image;gray scale surface topography surface reconstruction needles image reconstruction photometry solid modeling brightness computer vision cost function;2d cellular automata system single grayscale image automatic surface model acquisition image gradient field intensity constraint needle map;surface model;cellular automata;image gradient field	In this paper we show how a system for performing automatic surface model acquisition from a single grayscale image can be designed in two steps. Firstly, surface normals are parallelly and gradually adjusted by a procedure which includes three constraints: smooth constraint ensures the recovered normals are smooth and integrable, intensity gradient constraint ensures the recovered normals are consistent with the image gradient field and intensity constraint guarantees the recovered intensity is equal to the input image. Secondly, the surface is recovered from needle map using a two-dimension cellular automata system. The experiment results demonstrate this approach is practicable.	automata theory;cellular automaton;grayscale;image gradient;normal (geometry)	Bin Xu;Lixin Tang;Hanmin Shi	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284991	computer vision;feature detection;two-dimensional space;u-matrix;image processing;computer science;algorithm;detection theory;computer graphics (images)	Robotics	50.22700489271156	-66.24099981449461	106990
16324adb8a50587941ad2cfe4765c4cc5b6d4ad0	efficient multi-scale patch-based segmentation		The objective of this paper is to devise an efficient and accurate patch-based method for image segmentation. The method presented in this paper builds on the work of Wu et al. [14] with the introduction of a compact multi-scale feature representation and heuristics to speed up the process. A smaller patch representation along with hierarchical pruning allowed the inclusion of more prior knowledge, resulting in a more accurate segmentation. We also propose an intuitive way of optimizing the search strategy to find similar voxel, making the method computationally efficient. An additional approach at improving the speed was explored with the integration of our method with Optimised PatchMatch [11]. The proposed method was validated using the 100 hippocampus images with ground truth segmentation from ADNI-1 (mean DSC = 0.892) and the MICCAI SATA segmentation challenge dataset (mean	algorithmic efficiency;computation;ground truth;heuristic (computer science);image segmentation;mathematical optimization;patchmatch;serial ata;the 100;time complexity;voxel	Abinash Pant;David Rivest-Hénault;Pierrick Bourgeat	2015		10.1007/978-3-319-28194-0_25	segmentation-based object categorization;image segmentation;scale-space segmentation	Vision	44.55443249658831	-74.47416723260953	107380
e8b98cba27869e6c933252b1fd1562ad04e5ae65	hierarchical key frame extraction for wireless capsule endoscopy video based on the saliency map	saliency map;hierarchical key frames extraction;wireless capsule endoscopy;mean shift clustering method;information entropy;wce;adapted weights	Wireless capsule endoscopy (WCE) is a non-invasive imaging technique to examine the entire small intestine. This paper presents a novel hierarchical key frame extraction algorithm based on the saliency map to automatically select a small number of key informative frames in the WCE video to assist the diagnostic process. We start from calculating the saliency map and extracting the inside colour, texture and shape features. We then compute the image information entropies with adapted weights on three features; calculate the differences of entropies in the successive frames and extract the first level key frames to divide the video into multi-segments. Finally, the mean shift clustering method is applied on each segment to obtain the final key frames. The proposed technique achieves the fidelity of 95.07% and the compression ratio of 87.9% on average, validating that the proposed scheme is highly promising for the key frame extraction in the WCE video.	algorithm;cluster analysis;execution unit;feature extraction;feature integration theory;fidelity of quantum states;information;interference (communication);key frame;maxima and minima;mean shift;preprocessor;region of interest;texture mapping;video clip	Yixuan Yuan;Max Q.-H. Meng	2014	IJMA	10.1504/IJMA.2014.066383	computer vision;speech recognition;computer science;communication	Vision	41.50254285731863	-73.04064368385139	107382
cbc385c926082120999bc35ba8c2c139b3b92fa4	diffeomorphic point set registration using non-stationary mixture models	biological patents;biomedical journals;text mining;gaussian processes;europe pubmed central;gaussian mixture model nonstationary mixture model diffeomorphic point set registration nonlinear registration anatomical structure general nonstationary kernel point shape point set similarity calculated deformation nonrigid transform symmetric diffeomorphism space synthetic dataset human dataset fiber bundle registration lung airways registration;citation search;citation networks;transforms deformation gaussian processes image registration lung medical image processing;lung;deformation;research articles;abstracts;medical image processing;image registration;open access;shape transforms kernel ellipsoids anatomical structure statistics atmospheric measurements;transforms;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	This paper investigates a diffeomorphic point-set registration based on non-stationary mixture models. The goal is to improve the non-linear registration of anatomical structures by representing each point as a general non-stationary kernel that provides information about the shape of that point. Our framework generalizes work done by others that use stationary models. We achieve this by integrating the shape at each point when calculating the point-set similarity and transforming it according to the calculated deformation. We also restrict the non-rigid transform to the space of symmetric diffeomorphisms. Our algorithm is validated in synthetic and human datasets in two different applications: fiber bundle and lung airways registration. Our results shows that nonstationary mixture models are superior to Gaussian mixture models and methods that do not take into account the shape of each point.	anatomic structures;kernel;mixture model;muscle rigidity;nonlinear system;normal statistical distribution;point set registration;stationary process;structure of parenchyma of lung;synthetic intelligence;tissue fiber;algorithm;registration - actclass	Demian Wassermann;James C. Ross;George R. Washko;Carl-Fredrik Westin;Raúl San José Estépar	2013	2013 IEEE 10th International Symposium on Biomedical Imaging	10.1109/ISBI.2013.6556656	point set registration;computer vision;mathematical optimization;text mining;computer science;image registration;pattern recognition;gaussian process;mathematics;deformation	Vision	47.322754737743416	-77.22709655243655	107659
82d0d3a4101ee64365673976ce915b16ad9bae7c	3d active shape model segmentation with nonlinear shape priors	asm segmentation;linear model;nonlinear case;linear shape model;linear ssm;nonlinear ssms;active shape model segmentation;linear combination;segmentation algorithm;nonlinear asm segmentation;nonlinear ssm;nonlinear shape prior	The Active Shape Model (ASM) is a segmentation algorithm which uses a Statistical Shape Model (SSM) to constrain segmentations to 'plausible' shapes. This makes it possible to robustly segment organs with low contrast to adjacent structures. The standard SSM assumes that shapes are Gaussian distributed, which implies that unseen shapes can be expressed by linear combinations of the training shapes. Although this assumption does not always hold true, and several nonlinear SSMs have been proposed in the literature, virtually all applications in medical imaging use the linear SSM. In this work, we investigate 3D ASM segmentation with a nonlinear SSM based on Kernel PCA. We show that a recently published energy minimization approach for constraining shapes with a linear shape model extends to the nonlinear case, and overcomes shortcomings of previously published approaches. Our approach for nonlinear ASM segmentation is applied to vertebra segmentation and evaluated against the linear model.	active shape model;algorithm;bone structure of spine;energy minimization;kernel principal component analysis;linear model;medical imaging;nonlinear system;normal statistical distribution;organ;seizures, scoliosis, and macrocephaly syndrome;scientific publication;statistical shape analysis;biologic segmentation	Matthias Kirschner;Meike Becker;Stefan Wesarg	2011	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-23629-7_60	active shape model;computer vision;machine learning;shape analysis;mathematics	Vision	44.0910157832794	-78.31908983008255	107705
fe8a0c27b7d993ec7c0f82a330e6dc6b02a74cce	conformal and other image warpings for reading with field defect	high resolution;retina;facial recognition systems;visual field;vision;image warping	Certain visual functions, such as reading, are dependent on the high resolution capability of the central visual field. When that area becomes dysfunctional (the lesion is called a scotoma), the tasks become difficult or impossible. We have proposed an image warping prosthesis, in which the structure of the image that would otherwise be unseen owing to the scotoma is moved outward and onto portions of the retina that still function. Previously we used normally sighted volunteers with fixated foveation, synthetic scotomas, a limited form of image warping, and externally controlled reading saccades. Their reading rate showed improvement in a significant number of instances. In the next stage, we are prepared to use volunteers with actual, not synthesized, scotomas; to allow them to foveate where they will; to extend the forms of image warpings they may select from; and control the motion of text themselves. The results will be used to design realistic prostheses. Different warpings may better help other visual tasks such as facial recognition. Some of the image warpings designed for reading are shown here and our rationale for considering them are given.	design rationale;facial recognition system;image warping;software bug;synthetic intelligence	Richard D. Juday;R. Shane Barton;Curtis D. Johnson;David Loshin	1994		10.1117/12.179299	image warping;vision;computer vision;image resolution;computer science;optics;computer graphics (images)	HCI	48.55921286745298	-74.17246189330854	107779
33cd2791ac2e754a461657f05b53be3e42325457	characterization of patient-specific biventricular mechanics in heart failure with preserved ejection fraction: hyperelastic warping	wrapping;heart;computational modeling;magnetic resonance;echocardiography;diseases;strain	Heart failure with preserved ejection fraction (HFPEF) is considered as a major public health problem. Traditionally, HFPEF is diagnosed based on a “normal” EF, but the studies have explored the potential role of left ventricular mechanics. Furthermore, right ventricular mechanics and bi-ventricular interaction in HFPEF is currently not well understood. In this study, we aim to develop a framework using a hyperelastic warping approach to quantify bi-ventricular and septum strains from cardiac magnetic resonance (CMR) images. Whole heart models were reconstructed in HFPEF, HF with reduced EF (HFREF) and normal control patients, and a Laplace-Dirichlet Rule-Based (LDRB) algorithm was employed to assign circumferential orientation. The LV circumferential strain was 10.56% in normal control, and decreased to 5.90% in HFPEF and 1.66% in HFREF. Interestingly, the RV circumferential strain was 7.29% in normal control, but increased to 8.93% in HFPEF, and decreased to 2.16% in HFREF. The septum circumferential strain was comparable between HFPEF and normal control. Heart failure with preserved ejection fraction demonstrated augmented right ventricular strain and comparable septum strain to maintain its “normal” ejection fraction. This might unveil a new mechanism of bi-ventricular interaction and compensation in heart failure with preserved ejection fraction.	biologic preservation;circumference;ejection fraction (procedure);entity framework;geo warping;heart failure;logical volume management;mechanics;patients;resonance;right heart strain;septum - general anatomical term;ventricular fibrillation;algorithm	Hua Zou;Xiaodan Zhao;Xi Ce;Lik Chuan Lee;Martin Genet;Yi Su;Ru San Tan;Liang Zhong	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591640	structural engineering;radiology;magnetic resonance imaging;biological engineering;strain;computational model;heart;cardiology	Visualization	40.745015826172285	-80.16769508931205	108031
064f1f58b735faa0decab85280585ecf46894faa	cardiac modeling using active appearance models and morphological operators	pulmonary vein;heart;morphological operation;veins;active appearance model;blood;magnetic resonance imaging;right atrium;modeling;right ventricular;volume data	We present an approach for fast reconstructing of cardiac myocardium and blood masses of a patient’s heart from morphological image data, acquired either MRI or CT, in order to estimate numerically the spread of electrical excitation in the patient’s atria and ventricles. The approach can be divided into two main steps. During the first step the ventricular and atrial blood masses are extracted employing Active Appearance Models (AAM). The left and right ventricular blood masses are segmented automatically after providing the positions of the apex cordis and the base of the heart. Because of the complex geometry of the atria the segmentation process of the atrial blood masses requires more information as the ventricular blood mass segmentation process of the ventricles. We divided, for this reason, the left and right atrium into three divisions of appearance. This proved sufficient for the 2D AAM model to extract the target blood masses. The base of the heart, the left upper and left lower pulmonary vein from its first up to its last appearance in the image stack, and the right upper and lower pulmonary vein have to be marked. After separating the volume data into these divisions the 2D AAM search procedure extracts the blood masses which are the main input for the second and last step in the myocardium extraction pipeline. This step uses morphologically-based operations in order to extract the ventricular and atrial myocardium either directly by detecting the myocardium in the volume block or by reconstructing the myocardium using mean model information, in case the algorithm fails to detect the myocardium.	active appearance model;algorithm;apex (geometry);mathematical morphology;numerical analysis;sensor	Bernhard Pfeifer;Friedrich Hanser;Michael Seger;Christoph Hintermüller;Robert Modre-Osprian;Gerald Fischer;Hannes Mühlthaler;Thomas Trieb;Bernhard Tilg	2005		10.1117/12.591903	engineering;anatomy;surgery;cardiology	Vision	39.25358188400409	-79.62177352842785	108260
92882d822d21221247015887e777bf69408e57d9	image segmentation based on adaptive fuzzy-c-means clustering	pattern clustering;fuzzy c mean;adaptive fuzzy c means clustering;image segmentation;pattern clustering fuzzy set theory image segmentation;adaptive distance image segmentation fuzzy clustering fcm;fuzzy set theory;noise measurement;fuzzy clustering;artificial neural networks;image edge detection;clustering method;pixel;clustering algorithms;fuzzy c means clustering;adaptive distance image segmentation adaptive fuzzy c means clustering;adaptive distance;pixel image segmentation image edge detection noise clustering algorithms noise measurement artificial neural networks;fcm;noise	The clustering method “Fuzzy-C-Means” (FCM) is widely used in image segmentation. However, the major drawback of this method is its sensitivity to the noise. In this paper, we propose a variant of this method which aims at resolving this problem. Our approach is based on an adaptive distance which is calculated according to the spatial position of the pixel in the image. The obtained results have shown a significant improvement of our approach performance compared to the standard version of the FCM, especially regarding the robustness face to noise and the accuracy of the edges between regions.	cluster analysis;fuzzy cognitive map;image segmentation;pixel;synthetic intelligence	Mohamed Walid Ayech;Karim Kalti;Béchir el Ayeb	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.564	computer vision;fuzzy clustering;computer science;noise measurement;noise;machine learning;segmentation-based object categorization;pattern recognition;fuzzy set;image segmentation;cluster analysis;scale-space segmentation;artificial neural network;pixel	Vision	43.64640189231929	-71.57830553068067	108308
c5c1af68bebd3211f1eb1a72385233b2ed304dc0	flux maximizing geometric flows	geometric active contours;image segmentation active contours blood vessels biomedical imaging computer vision statistics shape image analysis angiography image motion analysis;image motion analysis;image segmentation;gradient flows;angiography images;curves;reliable region statistics;geometric active contour models;computational geometry;shape analysis;biomedical imaging;active contours;divergence and flux;computer vision;angiography;shape;computational geometry image segmentation computer vision blood vessels medical image processing;medical image processing;blood vessel segmentation;statistics;flux maximizing geometric flows;image analysis flux maximizing geometric flows geometric active contour models image segmentation curves surfaces intensity contrast reliable region statistics gradient flows vector field angiography images blood vessels computer vision;image analysis;surfaces;vector field;blood vessels;intensity contrast;geometric flow	Several geometric active contour models have been proposed for segmentation in computer vision and image analysis. The essential idea is to evolve a curve (in 2D) or a surface (in 3D) under constraints from image forces so that it clings to features of interest in an intensity image. Recent variations on this theme take into account properties of enclosed regions and allow for multiple curves or surfaces to be simultaneously represented. However, it is still unclear how to apply these techniques to images of narrow elongated structures, such as blood vessels, where intensity contrast may be low and reliable region statistics cannot be computed. To address this problem, we derive the gradient flows which maximize the rate of increase of flux of an appropriate vector field through a curve (in 2D) or a surface (in 3D). The key idea is to exploit the direction of the vector field along with its magnitude. The calculations lead to a simple and elegant interpretation which is essentially parameter free and has the same form in both dimensions. We illustrate its advantages with several level-set-based segmentations of 2D and 3D angiography images of blood vessels.		Alexander Vasilevskiy;Kaleem Siddiqi	2002	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/TPAMI.2002.1114849	computer vision;geometric flow;vector field;image analysis;computational geometry;shape;computer science;shape analysis;mathematics;geometry;image segmentation;surface;computer graphics (images)	Vision	49.01694435350306	-71.0264268428972	108321
d6fa5e83eb1ed400eb7ecc45c4c2005fc5e3ae07	a fuzzy shape descriptor and inference by fuzzy relaxation with application to description of bones contours at hand radiographs	fuzzy parsing;contour;shape descriptor;hand radiographs;fuzzy shape descriptor	Generalization of string languages describing shapes in order to apply them to analyze a contour of bones in hand radiographs is proposed in this paper. An algorithm to construct a fuzzy shape descriptor is introduced. Next, basing on the fuzzy descriptor, a univocal description by fuzzy interference is realized. In prospects this method will be used to erosion detection of hand bones visible in hand radiographs.	linear programming relaxation;radiography	Marzena Bielecka;Marek Skomorowski;Bartosz Zielinski	2009		10.1007/978-3-642-04921-7_48	computer vision;artificial intelligence;pattern recognition;mathematics	Vision	41.39326264891541	-69.58937381078385	108447
445a96b7ed958e646c6612a0f778ded6618f7cb6	a hierarchical statistical modeling approach for the unsupervised 3d reconstruction of the scoliotic spine	minimisation;postero anterior;optimal method;computerised tomography image reconstruction medical image processing neurophysiology bone orthopaedics minimisation radiography;indexing terms;orthopaedics;ct scan;statistical model;radiography;energy function;experience report;stochastic optimization;first order;medical image;image reconstruction;medical image processing;bone;markov process;computerised tomography;genetic algorithm;ct scan technique unsupervised 3d reconstruction priori hierarchical global knowledge rough geometric template rigid admissible deformations spine crude registration statistical modal analysis pathological deformations scoliotic vertebra population coarse to fine 3d reconstruction procedure double energy function minimization problems stochastic optimization algorithm biplanar radiographic images;shape modeling;neurophysiology;modal analysis;geometric structure;3d reconstruction;deformable models solid modeling pathology spine principal component analysis eigenvalues and eigenfunctions radiography stochastic processes parameter estimation covariance matrix	In this paper, we propose a new and accurate 3D reconstruction technique for the scoliotic spine from a pair planar and conventional radiographic images (postero-anterior and lateral). The proposed model uses a priori hierarchical global knowledge, both on the geometric structure of the whole spine and of each vertebra. More precisely, it relies on the specification of two 3D templates. The first, a rough geometric template on which rigid admissible deformations are defined, is used to ensure a crude registration of the whole spine. 3D reconstruction is then refined for each vertebra, by a template on which non-linear admissible global deformations are modeled, with statistical modal analysis of the pathological deformations observed on a representative scoliotic vertebra population. This unsupervised coarse-to-fine 3D reconstruction procedure is stated as a double energy function minimization problems efficiently solved with a stochastic optimization algorithm. The proposed method, tested on several pairs of biplanar radiographic images with scoliotic deformities, is comparable in terms of accuracy with the classical CT-scan technique while being unsupervised and requiring a lower amount of radiation for the patient.	3d reconstruction;algorithm;ct scan;dendritic spine;lateral thinking;mathematical optimization;modal logic;nonlinear system;radiography;statistical model;stochastic optimization	Said Benameur;Max Mignotte;Stefan Parent;Hubert Labelle;Wafa Skalli;Jacques A. de Guise	2003		10.1109/ICIP.2003.1247023	3d reconstruction;iterative reconstruction;statistical model;computer vision;minimisation;mathematical optimization;genetic algorithm;radiography;index term;computer science;stochastic optimization;modal analysis;first-order logic;mathematics;markov process;neurophysiology;statistics	Vision	43.11018758278925	-77.24673703933833	108658
e23e19ae96246b039f0ba13a9829bc2f8084605e	segmentation of regions in jpeg compressed medical images	image coding;image segmentation;image resolution;data compression;rule based;k means;image classification;adaptive k means clustering algorithm jpeg compressed medical image image segmentation feature extraction dct coefficient discrete cosine transform inverse transform fisher discriminant k mean technique image pixel clustering feature vector;feature vector;medical image;discrete cosine transforms;image classification image segmentation data compression image coding feature extraction discrete cosine transforms image resolution;feature extraction;image segmentation transform coding image coding biomedical imaging discrete cosine transforms clustering algorithms feature extraction pixel medical diagnostic imaging classification algorithms;k means clustering	A novel algorithm for the segmentation of medical images using features derived directly from JPEG compressed domain is proposed in this paper. The algorithm uses features extracted from DCT coefficients without its inverse transform and the rule based fisher discriminant K-means (FDK) technique for clustering image pixels based on derived feature vectors. In this study, we extract features for each 2/spl times/2 DCT block of compressed image. The extracted feature vector is used by an extended version of the adaptive K-means clustering algorithm for the classification of image pixels.	algorithm;cluster analysis;coefficient;discrete cosine transform;discriminant;feature vector;jpeg;k-means clustering;pixel;synapomorphy	Pramod K. Singh	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421865	rule-based system;image texture;computer vision;feature detection;range segmentation;binary image;image processing;feature extraction;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation;quantization;top-hat transform;feature;k-means clustering	Vision	41.99905204595233	-66.76133112396504	108690
21ae00b5da9af6bcc8646e02367d454f5f1b7490	image segmentation as regularized clustering: a fully global curve evolution method	minimisation;pattern clustering;minimisation image segmentation pattern clustering;image segmentation;curve evolution;level set;image segmentation level set partitioning algorithms equations image converters digital images computer vision numerical stability convergence councils;energy function;euler lagrange;natural vectorial image image segmentation image data regularized clustering global curve evolution method euler lagrange curve evolution equation synthetic vectorial image	The purpose of this study is to investigate image segmentation from the viewpoint of image data regularized clustering. From this viewpoint, segmentation into a fixed but arbitrary number N of regions is stated as the simultaneous minimization of N - 1 energy functional, each involving a single region and its complement. The resulting Euler-Lagrange curve evolution equations yield a partition at convergence provided the curves are initialized so as to define an arbitrary partition of the image domain. The method is implemented via level sets, and results are shown on synthetic and natural vectorial images.	cluster analysis;euler;euler–lagrange equation;image segmentation;synthetic intelligence	Carlos Vázquez;Amar Mitiche;Ismail Ben Ayed	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421861	image texture;computer vision;minimisation;mathematical optimization;feature detection;computer science;level set;segmentation-based object categorization;mathematics;geometry;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;statistics	Robotics	50.116315887199384	-70.97780954943356	108720
8f5768bb515bf11a7a4b037fab8b3312aeb6d3b3	intensity-based image matching for regular firearms	revolvers;image matching;pistols;digitized image matching;fast fourier transformation fft;differential matching;histogram comparison	This paper reports the digitized image matching of intensity-based signals generated by the POLIVIEW imaging system for regular firearms. The POLIVIEW imaging system transforms nonlinearized models into linearized models by applying a differential matching method. This paper also discusses the performance of the computer program for determining the peaks of the histograms generated and their threshold segmentation for matching. This method can be used to generate corroborative evidence in solving crimes. © 2002 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 12, 68–72, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/ima.10011	computer program;digital image;emoticon;fm broadcasting;image registration;john d. wiley;memory segmentation	S. Sridhara Murthy;Chandan Mazumdar;M. S. Rao;A. K. Pal	2002	Int. J. Imaging Systems and Technology	10.1002/ima.10011	computer science;theoretical computer science;algorithm;computer graphics (images)	Vision	39.618029189986764	-70.89030757418128	109031
77dce984c7eca52eef39b5d967c150dce4cb4f1a	abdominal multi-organ segmentation of ct images based on hierarchical spatial modeling of organ interrelations	statistical shape model;statistical shape prediction;probabilistic atlas	The automated segmentation of multiple organs in CT data of the upper abdomen is addressed. In order to explicitly incorporate the spatial interrelations among organs, we propose a method for finding and representing the interrelations based on canonical correlation analysis. Furthermore, methods are developed for constructing and utilizing the statistical atlas in which inter-organ constraints are explicitly incorporated to improve accuracy of multi-organ segmentation. The proposed methods were tested to perform segmentation of seven abdominal organs (liver, spleen, kidneys, pancreas, gallbladder and inferior vena cava) from contrast-enhanced CT datasets and was compared to a previous approach. 28 datasets acquired at two institutions were used for the validation. Significant accuracy improvement was observed for the segmentation of pancreas and gallbladder while there was no accuracy reduction for any organ.	ct scan	Toshiyuki Okada;Marius George Linguraru;Yasuhide Yoshida;Masatoshi Hori;Ronald M. Summers;Yen-Wei Chen;Noriyuki Tomiyama;Yoshinobu Sato	2011		10.1007/978-3-642-28557-8_22	radiology;pathology;anatomy	Vision	41.93187747282449	-79.33069048866463	109196
3a6c0dcca45331bf1bea59c8cf1664879a077b4e	a region extraction approach to blood smear segmentation	laboratory investigations;comptage;automatic instrument;examen laboratoire;biologie clinique;algorithme;algorithm;blood;image analysis;digital processing;metering;clinical biology;appareil automatique;sang;analyse image;traitement numerique	A major problem in the development of systems for automated differential blood count of leucocytes of the peripheral blood is the correct segmentation of the cell scene. Standard preparation techniques of blood smears have been optimized for the visual inspection of the cell scene. Variations in the preparation have a negligible effect on the scene evaluation by visual analysis but do pose major problems on an automated analysis. Therefore a new algorithm for the segmentation of microscopic cell scenes of leucocytes has been developed which uses, in addition to local properties of the scene, global information about neighborhood relations and shape of the scene components for the improvement of the segmentation. The algorithm is based on a region extraction approach and a subsequent labeling procedure. A priori knowledge about shape and neighborhood relations is entered by a relaxation process.	smear campaign	G. Haussmann;Claus-E. Liedtke	1984	Computer Vision, Graphics, and Image Processing	10.1016/0734-189X(84)90099-9	computer vision;image analysis;computer science;artificial intelligence;scale-space segmentation;metering mode;algorithm	Vision	45.53684110879482	-79.29888058357051	109212
a405dc2b8f61d6134cdb147d061a6ad38c7f02be	point-based rigid registration: clinical validation of theory	nuclear magnetic resonance imaging;ciblage;computerized axial tomography;tomodensitometria;radiodiagnostic;medical imagery;erreur;closed form solution;rigid body;etude theorique;validacion;etude experimentale;estudio comparativo;simulacion numerica;hombre;skull;qualite image;imageria rmn;etude comparative;accuracy;radiodiagnostico;tomodensitometrie;blancado;precision;targeting;independent and identically distributed;receiver operating characteristic curves;roc analysis;image quality;simulation numerique;human;comparative study;estudio teorico;imagerie medicale;validation;imagerie rmn;numerical simulations;calidad imagen;imageneria medical;root mean square;error;radiodiagnosis;theoretical study;metodo roc;methode roc;rigid registration;estudio experimental;homme;numerical simulation	This paper presents the first comparison between theoretical estimates and clinically observed values for registration accuracy in point-based rigid-body registration. Rigid-body registration is appropriate for applications in which relatively rigid parts of the body are involved. In some such applications rigid-body registration is accomplished by aligning two sets of discrete points. In neurosurgical guidance, for example, the points are found by localizing the centroids of fiducial markers. We have previously provided two fundamental theoretical results on the relationship between localization error and registration error in rigid-body, point-based registration and have justified these results by showing them to be close to those given by numerical simulations. Rigid-body, point-based registration is accomplished by finding a rigid-body transformation that aligns pairs of homologous 'fiducial' points. The imprecision in locating a fiducial point is known as the 'fiducial localization error' (FLE). Fiducial points may be centroids of attached markers, which tend to have small, equal FLEs, or salient points in segmented anatomic features, whose FLEs tend to be larger and more varied. Alignment is achieved by minimizing the 'fiducial registration error' (FRE), which is the root mean square distance between homologous fiducials after registration. Closed form solutions for the rigid transformation that minimizes FRE have been known since 1966. A more critical and direct measure of registration error is the 'target registration error' (TRE), which is the distance between homologous points other than the centroids of fiducials. This error measure has been investigated by numerical simulation for many years, and we presented at this meeting in 1998 the first derivation of theoretical estimates of TRE. In that paper we showed that these estimates agree well with our simulations and those of others. We made the simplifying assumption in both the derivations and the simulations that the FLE is isotropic, i.e. that it has the same distribution in each coordinate direction. In the present work, we extend the validation beyond simulated data sets to clinically acquired head images from a set of 86 patients. We use the actual localizations of skull-implanted, visible fiducial markers in the images to compare the observed TRE values with those estimated by theory. This approach provides a clinically relevant estimate of the usefulness of the theoretical predictions. We also make a comparison between the observed TRE values and those given by numerical simulation: this allows us to determine whether the assumptions we use for the derivation of our results are good ones in practice. Although the distributions of observed and theoretical values appear different, ROC analysis shows that the theoretical values are good predictors of the observed ones. This gives some validity to the assumptions we make governing the point- based registration process (e.g., that the FLE is isotropic, and that it is independent and identically distributed at each fiducial point), and shows that our theory has practical use in a clinical setting.© (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Jay B. West;J. Michael Fitzpatrick	2000		10.1117/12.387697	simulation;mathematics;engineering drawing;cartography	Robotics	45.966199210707785	-80.09849073743415	109217
9d8d02bc7b3d5b1ee568ef30eb6018a2fd417bfe	a quantitative analysis of 3-d coronary modeling from two or more projection images	phantoms;cardiology;confidence region;algorithms animals artificial intelligence computer simulation coronary angiography image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval models anatomic models cardiovascular numerical analysis computer assisted pattern recognition automated phantoms imaging reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique swine;euclidean distance;3 d quantification angiography multiprojection 3 d modeling;three dimensional;medical image processing;quantitative analysis;cross sectional area;medical image processing cardiology physiological models blood vessels diagnostic radiography phantoms;ground truth;euclidean distance 3 d coronary modeling coronary arteries multiple calibrated two dimensional angiographic projections multiprojection modeling automated centerline determination automated width determination phantom;coronary artery;physiological models;diagnostic radiography;blood vessels;image analysis arteries image reconstruction angiography two dimensional displays shape image segmentation associate members imaging phantoms data analysis	A method is introduced to examine the geometrical accuracy of the three-dimensional (3-D) representation of coronary arteries from multiple (two and more) calibrated two-dimensional (2-D) angiographic projections. When involving more then two projections, (multiprojection modeling) a novel procedure is presented that consists of fully automated centerline and width determination in all available projections based on the information provided by the semi-automated centerline detection in two initial calibrated projections. The accuracy of the 3-D coronary modeling approach is determined by a quantitative examination of the 3-D centerline point position and the 3-D cross sectional area of the reconstructed objects. The measurements are based on the analysis of calibrated phantom and calibrated coronary 2-D projection data. From this analysis a confidence region (/spl alpha//spl deg//spl ap/[35/spl deg/-145/spl deg/]) for the angular distance of two initial projection images is determined for which the modeling procedure is sufficiently accurate for the applied system. Within this angular border range the centerline position error is less then 0.8 mm, in terms of the Euclidean distance to a predefined ground truth. When involving more projections using our new procedure, experiments show that when the initial pair of projection images has an angular distance in the range /spl alpha//spl deg//spl ap/[35/spl deg/-145/spl deg/], the centerlines in all other projections (/spl gamma/=0/spl deg/-180/spl deg/) were indicated very precisely without any additional centering procedure. When involving additional projection images in the modeling procedure a more realistic shape of the structure can be provided. In case of the concave segment, however, the involvement of multiple projections does not necessarily provide a more realistic shape of the reconstructed structure.	3d computer graphics;angularjs;arterial system;calibration (statistics);concave function;cross section (geometry);euclidean distance;experiment;ground truth;phantom reference;phantoms, imaging;physical object;projection defense mechanism;projections and predictions;semiconductor industry;width	Babak Movassaghi;Volker Rasche;Michael Grass;Max A. Viergever;Wiro J. Niessen	2004	IEEE Transactions on Medical Imaging	10.1109/TMI.2004.837340	three-dimensional space;computer vision;ground truth;computer science;quantitative analysis;confidence region;euclidean distance;mathematics;cross section	Vision	44.690322896266146	-79.90086799738802	109446
b37de8f51049e42a210683c74b2bfae93bf18a82	nonrigid registration of 3-d multichannel microscopy images of cell nuclei	filtrage gauss;image tridimensionnelle;optimisation;canal multiple;intensity based nonrigid registration approach;microscopie;image segmentation;image resolution;optimizacion;intensity based nonrigid registration approach 3d multichannel microscopy images cell nuclei intensity based registration scheme gaussian filter adaptive step length optimisation 2 d cell like synthetic images 3 d phantom images;fluorescence microscopy images;optical filters;objet test;relacion convergencia;gaussian filtering;microscopy;taux convergence;convergence rate;segmentation;3d multichannel microscopy images;analyse multiresolution;filtrado gaussiano;deformacion elastica;elastic deformations;multiple channel;algorithme;algorithm;cell nuclei;filtering algorithms;2 d cell like synthetic images;3 d phantom images;fluorescence microscopy;intensity based registration scheme;elastic deformation;medical image processing;image registration;deformation elastique;algorithms cell nucleus image enhancement image interpretation computer assisted imaging three dimensional microscopy numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique;transforms;adaptive step length optimisation;cell nuclei shape variation;gaussian filter;tridimensional image;optimisation cellular biophysics filtering theory image registration image resolution image segmentation medical image processing;optimization;nonrigid registration;microscopy bioinformatics biological cells genomics humans fluorescence image registration image segmentation filters convergence;multiresolution analysis;objeto prueba;test object;3 d multichannel images cell nuclei shape variation elastic deformations fluorescence microscopy images nonrigid registration;cellular biophysics;3 d multichannel images;segmentacion;microscopia;filtering theory;analisis multiresolucion;imagen tridimensional;bioinformatics;algoritmo	We present an intensity-based nonrigid registration approach for the normalization of 3-D multichannel microscopy images of cell nuclei. A main problem with cell nuclei images is that the intensity structure of different nuclei differs very much; thus, an intensity-based registration scheme cannot be used directly. Instead, we first perform a segmentation of the images from the cell nucleus channel, smooth the resulting images by a Gaussian filter, and then apply an intensity-based registration algorithm. The obtained transformation is applied to the images from the nucleus channel as well as to the images from the other channels. To improve the convergence rate of the algorithm, we propose an adaptive step length optimization scheme and also employ a multiresolution scheme. Our approach has been successfully applied using 2-D cell-like synthetic images, 3-D phantom images as well as 3-D multichannel microscopy images representing different chromosome territories and gene regions. We also describe an extension of our approach, which is applied for the registration of (4-D) image series of moving cell nuclei.	30 nm chromatin fiber;cell nucleus;computational human phantom;convergence (action);experiment;hela cells;image registration;large;mpeg multichannel;mathematical optimization;normal statistical distribution;optical fiber;phantoms, imaging;rate of convergence;segmentation action;series - set of composite instances;synthetic intelligence;algorithm;registration - actclass	Siwei Yang;Daniela Köhler;Kathrin Teller;Thomas Cremer;Patricia Le Baccon;Edith Heard;Roland Eils;Karl Rohr	2008	IEEE Transactions on Image Processing	10.1109/TIP.2008.918017	multiresolution analysis;fluorescence microscope;computer vision;image resolution;computer science;image registration;microscopy;optical filter;mathematics;image segmentation;rate of convergence;gaussian filter;segmentation;algorithm;deformation;computer graphics (images)	Vision	51.72656634214024	-72.54341332764511	109473
7b9ecfc51c724d8f54fdb20d95a91750eb67ee83	active contour model for simultaneous mr image segmentation and denoising	image segmentation;perona malik diffusion equation;期刊论文;image denoising;nadaraya watson estimator;local image information	In this paper, a new region-based active contour model is proposed for magnetic resonance image segmentation and denoising based on the global minimization framework and level set evolution. A new region fitting energy based on Nadaraya-Watson estimator and local image information is defined to enforce the curve evolution. By this improved region fitting term, the images with noise and intensity un-uniformity can be segmented and denoised. Inspired by the Perona-Malik diffusion equation, an edge-preserving regularization term is defined through the duality formulation to penalize the length of region boundaries. By this new regularization term, the edge information is utilized to improve the contour@?s ability of capturing the edge and remaining smooth during the evolution. The energy functional of the proposed model is minimized by an efficient dual algorithm avoiding the inefficiency of the gradient descent method. Experiments on medical images demonstrate the proposed model provides a hybrid way to perform image segmentation and image denoising simultaneously.	active contour model;circuit complexity;cluster analysis;diffusing update algorithm;image segmentation;noise reduction;watson (computer)	Qi Ge;Liang Xiao;Zhihui Wei	2013	Digital Signal Processing	10.1016/j.dsp.2012.12.015	image texture;computer vision;mathematical optimization;computer science;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation;anisotropic diffusion;non-local means	Vision	51.024240294236975	-71.5016531698393	109661
011291fa78adb0fcdfa72b0c5a0167a5a1d7ece0	2d-3d rigid registration of x-ray fluoroscopy and ct images using mutual information and sparsely sampled histogram estimators	orthopedic surgery;histograms;pre operative volumetric dataset registration;lumbar spine;optimisation;stochastic gradient ascent strategy;instruments;computed tomography;probability density;bone image registration medical image processing stereo image processing computerised tomography optimisation diagnostic radiography orthopaedics;x ray imaging;maximization;medical instrument location verification;patient position verification;x ray fluoroscopy;plastic lumbar spine segment;probability density measure estimation;biomedical imaging;skull;x ray imaging computed tomography mutual information histograms skull plastics biomedical imaging instruments orthopedic surgery neurosurgery;intra operative 2d images;orthopaedics;plastics;sparsely sampled histogram estimators;mutual information based registration algorithm;plastic skull;2d 3d rigid registration;real skull;medical image processing;image registration;stereo image processing;bone;computerised tomography;stochastic gradient;gradient estimate;real skull pre operative volumetric dataset registration intra operative 2d images 2d 3d rigid registration x ray fluoroscopy ct images sparsely sampled histogram estimators patient position verification medical instrument location verification mutual information based registration algorithm stochastic gradient ascent strategy probability density measure estimation image intensities maximization plastic skull plastic lumbar spine segment;mutual information;image intensities;neurosurgery;ct images;diagnostic radiography;x rays	The registration of pre-operative volumetric datasets to intra-operative two-dimensional images provides an improved way of verifying patient position and medical instrument location. In applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. We propose a mutual information-based registration algorithm which establishes the proper alignment via a stochastic gradient ascent strategy. Our main contribution lies in estimating probability density measures of image intensities with a sparse histogramming method which could lead to potential speedup over existing registration procedures and deriving the gradient estimates required by the maximization procedure. Experimental results are presented on fluoroscopy and CT datasets of a real skull, and on a CT-derived dataset of a real skull, a plastic skull and a plastic lumbar spine segment.	ct scan;expectation–maximization algorithm;gradient descent;mutual information;run time (program lifecycle phase);similarity measure;sparse matrix;speedup;times ascent;verification and validation	Lilla Zöllei;W. Eric L. Grimson;Alexander Norbash;William M. Wells	2001		10.1109/CVPR.2001.991032	computer vision;probability density function;orthopedic surgery;image registration;plastic;histogram;mutual information;statistics	Vision	42.74961239053883	-80.01980098888718	109745
410b156aaf5d96febb2f6e81854ff36a47ba709f	ranks for pairs of spatial fields via metric based on grayscale morphological distances	similarity degree;matrix algebra image classification image matching;mathematical morphology;image suprema;dilation distance;measurement;erosion distance;morphological distances;gray scale indexes measurement spatial databases silicon compounds equations shape;image matching;cloud top temperature;spatial field;grayscale morphological distance;spatial interaction;image classification;gisci;matrix algebra;gray scale;indexes;erosion;shape;silicon compounds;image size specification;median based distance;spatial databases;grayscale images;spatial fields;image infima;spatial interaction dilation erosion gisci mathematical morphology morphological distances spatial fields;cloud top temperature spatial field grayscale morphological distance grayscale images image size specification image infima image suprema similarity degree dilation distance erosion distance median based distance parameter specific interaction matrices;parameter specific interaction matrices;dilation	Based on a set of morphological distances computed between the grayscale images (spatial fields) of similar size specifications, the ratios of selected morphological distances, and the ratios of areas of infima and suprema of grayscale images, a new metric to quantify the degree of similarity between the grayscale images is proposed. We denote the two spatial fields (grayscale images), respectively, with f<sup>i</sup> and f<sup>j</sup>, and the infima and suprema of these spatial fields with (fi ∧ f<sup>j</sup>) and (f<sup>i</sup> ⋁ f<sup>j</sup>). The three morphology-based distances include: 1) dilation distance d( f<sup>i</sup>, f<sup>j</sup>); 2) erosion distance e(f<sup>i</sup>, f<sup>j</sup>); and 3) median-based distance MN(f<sup>i</sup>, f<sup>j</sup>). By employing these parameters, which play vital role in construction of parameter-specific interaction matrices, we provide a metric to designate every possible pair of images that can be considered out of a database consisting of a huge number of images. We demonstrate the whole approach on: 1) synthetic spatial fields; 2) a set of 12 similar-sized grayscale images representing cloud-top temperatures of a specific region for 12 different time instants; and 3) four spatial elevation fields to rank possible pairs of images.	categorization;classification;computation;computer vision;dilation (morphology);distance;grayscale color map;image registration;mathematical morphology;morphological parsing;pathological dilatation;population parameter;snap25 wt allele;specification;structuring element;synthetic data;test set;thresholding (image processing);registration - actclass	B. S. Daya Sagar;Sin Liang Lim	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2390135	computer vision;discrete mathematics;computer science;mathematics;geometry;grayscale	Vision	43.84660644884979	-69.24366040643962	109837
02e3da5b8c7684340deae28e44b9b9b270053ec6	computerised segmentation of suspicious lesions in the digital mammograms	segmentation;wavelet transforms;breast masses;mammograms	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;anisotropic diffusion;associate-o-matic;francis;image segmentation;nl (complexity);preprocessor;primary source;robustness (computer science);watershed (image processing);wavelet transform	Zainul Abdin Jaffery;Zaheeruddin;Laxman Singh	2017	CMBBE: Imaging & Visualization	10.1080/21681163.2014.982304	computer vision;speech recognition;pattern recognition;mathematics;scale-space segmentation	Robotics	41.495678219993835	-71.59919638018782	109884
1309337ac66c697d1c419528b44b7be19b6d4fd0	edge aware anisotropic diffusion for 3d scalar data	material boundaries;de noising;volume rendering edge aware anisotropic diffusion model 3d scalar field data material boundaries fine tubular structures directional second derivative gradient magnitude based technique denoising capabilities;curvature scale space;edge detection;volume rendering;rendering computer graphics edge detection gradient methods image denoising;gradient magnitude based technique;edge aware anisotropic diffusion model;pde;fine tubular structures;anisotropic diffusion;denoising capabilities;principle curvatures;scale space;three dimensional displays;noise reduction;directional second derivative;principle curvatures anisotropic diffusion pde de noising scale space;anisotropic magnetoresistance;mathematical model;scalar field;gradient methods;mathematical model three dimensional displays equations anisotropic magnetoresistance noise noise reduction rendering computer graphics;3d scalar field data;image denoising;rendering computer graphics;diffusion model;noise	In this paper we present a novel anisotropic diffusion model targeted for 3D scalar field data. Our model preserves material boundaries as well as fine tubular structures while noise is smoothed out. One of the major novelties is the use of the directional second derivative to define material boundaries instead of the gradient magnitude for thresholding. This results in a diffusion model that has much lower sensitivity to the diffusion parameter and smoothes material boundaries consistently compared to gradient magnitude based techniques. We empirically analyze the stability and convergence of the proposed diffusion and demonstrate its de-noising capabilities for both analytic and real data. We also discuss applications in the context of volume rendering.	algorithm;anisotropic diffusion;biologic preservation;edge-preserving smoothing;geometric median;gradient;imagery;isosurface;noise reduction;non-local means;photon mapping;photons;population parameter;preprocessor;requirement;scalar processor;scientific visualization;scott continuity;silo (dataset);smoothing (statistical technique);synthetic intelligence;thresholding (image processing);transfer function;volume rendering	Zahid Hossain;Torsten Möller	2010	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2010.147	magnetoresistance;computer vision;scalar field;mathematical optimization;scale space;edge detection;computer science;noise;noise reduction;diffusion;mathematical model;mathematics;geometry;anisotropic diffusion;volume rendering	Visualization	51.00627103919616	-75.91690288272046	109902
a0daecce2c20c2d269d6822806ff9a4e0b079268	embedding diffusion in variational bayes: a technique for segmenting images	model selection;variational bayes;image segmentation;diffusion equation	In this paper, we discuss how image segmentation can be handled by using Bayesian learning and inference. In particular variational techniques relying on free energy minimization will be introduced. It will be shown how to embed a spatial diffusion process on segmentation labels within the Variational Bayes learning procedure so as to enforce spatial constraints among labels.	energy minimization;image segmentation;variational principle	Giuseppe Boccignone;Paolo Napoletano;Mario Ferraro	2008	IJPRAI	10.1142/S0218001408006533	diffusion equation;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;image segmentation;scale-space segmentation;model selection;statistics	Vision	51.063702195488496	-71.01406545037614	109964
95905d928b3eb4feab881189090413e4d780960e	a vectorial image soft segmentation method based on neighborhood weighted gaussian mixture model	image segmentation;optimal method;image classification;segmentation;3d registration;gaussian mixture model;vectorial image segmentation;probability distribution;soft segmentation;expectation maximization algorithm;neighborhood;3d 3d registration;partial volume effect	The CT uroscan consists of three to four time-spaced acquisitions of the same patient. After registration of these acquisitions, the data forms a volume in which each voxel contains a vector of elements corresponding to the information of the CT uroscan acquisitions. In this paper we will present a segmentation tool in order to differentiate the anatomical structures within the vectorial volume. Because of the partial volume effect (PVE), soft segmentation is better suited because it allows regions or classes to overlap. Gaussian mixture model is often used in statistical classifier to realize soft segmentation by getting classes probability distributions. But this model relies only on the intensity distributions, which will lead a misclassification on the boundaries and on inhomogeneous regions with noise. In order to solve this problem, a neighborhood weighted Gaussian mixture model is proposed in this paper. Expectation maximization algorithm is used as optimization method. The experiments demonstrate that the proposed method can get a better classification result and is less affected by the noise.	anatomic structures;ct scan;class;expectation–maximization algorithm;experiment;mathematical optimization;mixture model;normal statistical distribution;patients;segmentation action;statistical classification;voxel;biologic segmentation	Hui Tang;Jean-Louis Dillenseger;Xu Dong Bao;Limin Luo	2009	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1016/j.compmedimag.2009.07.001	probability distribution;computer vision;contextual image classification;expectation–maximization algorithm;computer science;machine learning;segmentation-based object categorization;pattern recognition;mixture model;mathematics;region growing;image segmentation;scale-space segmentation;segmentation;partial volume;statistics	ML	43.01439718693168	-75.47434612111736	110117
1d29d5b91bcf501179cc57fc3c3e2e4e42bf168c	illumination invariant optical flow using neighborhood descriptors	bilateral filtering;neighborhood descriptors;image registration;endoscopic image mosaicing;optical flow	Total variational (TV) methods using l-norm are efficient approaches for optical flow determination. This contribution presents a multi-resolution TV-l approach using a data-term based on neighborhood descriptors and a weighted non-local regularizer. The proposed algorithm is robust to illumination changes. The benchmarking of the proposed algorithm is done with three reference databases (Middlebury, KITTI and MPI Sintel). On these databases, the proposed approach exhibits an optimal compromise between robustness, accuracy and computation speed. Numerous tests performed both on complicated data of the reference databases and on challenging endoscopic images acquired under three different modalities demonstrate the robustness and accuracy of the method against the presence of large or small displacements, weak texture information, varying illumination conditions and modality changes. © 2015 Elsevier Inc. All rights reserved.	algorithm;computation;database;modality (human–computer interaction);moore neighborhood;optical flow;variational principle	Sharib Ali;Christian Daul;Ernest Galbrun;Walter Blondel	2016	Computer Vision and Image Understanding	10.1016/j.cviu.2015.12.003	computer vision;computer science;image registration;theoretical computer science;machine learning;optical flow;mathematics;bilateral filter	Vision	46.46152489762681	-72.34216056280296	110181
070739774b92a76eed1dff07d903a7bd58b839c2	use of a ct statistical deformation model for multi-modal pelvic bone segmentation	computerized axial tomography;tomodensitometria;radiodiagnostic;image guidance;medical imagery;differential evolution;image segmentation;shape model;hueso;automatic segmentation;os;multi modal;segmentation;statistical model;radiodiagnostico;tomodensitometrie;deformation;non rigid registration;principal component analysis;magnetic resonance imaging;registration;bone;mri;surgery;modele statistique;imagineria medica;imagerie medicale;modelo estadistico;shape modeling;radiodiagnosis;deformable model;deformacion	We present a segmentation algorithm using a statistical deformation model constructed from CT data of adult male pelves coupled to MRI appearance data. The algorithm allows the semi-automatic segmentation of bone for a limited population of MRI data sets. Our application is pelvic bone delineation from pre-operative MRI for image guided pelvic surgery. Specifically, we are developing image guidance for prostatectomies using the daVinci telemanipulator. Hence the use of male pelves only. The algorithm takes advantage of the high contrast of bone in CT data, allowing a robust shape model to be constructed relatively easily. This shape model can then be applied to a population of MRI data sets using a single data set that contains both CT and MRI data. The model is constructed automatically using fluid based non-rigid registration between a set of CT training images, followed by principal component analysis. MRI appearance data is imported using CT and MRI data from the same patient. Registration optimisation is performed using differential evolution. Based on our limited validation to date, the algorithm may outperform segmentation using non-rigid registration between MRI images without the use of shape data. The mean surface registration error achieved was 1.74 mm. The algorithm shows promise for use in segmentation of pelvic bone from MRI, though further refinement and validation is required. We envisage that the algorithm presented could be extended to allow the rapid creation of application specific models in various imaging modalities using a shape model based on CT data.	algorithm;ct scan;differential evolution;iterative closest point;mathematical optimization;modal logic;principal component analysis;refinement (computing);remote manipulator;semiconductor industry;texas instruments davinci	Stephen A. Thompson;Graeme P. Penney;Damien Buie;Prokar Dasgupta;Dave Hawkes	2008		10.1117/12.770254	differential evolution;statistical model;computer vision;real-time mri;image registration;magnetic resonance imaging;multimodal interaction;image segmentation;segmentation;deformation;principal component analysis	Vision	43.111880183054836	-79.77538879548463	110189
de77746c7af999504fc15ee59fad7527d06207af	a discrete mrf framework for integrated multi-atlas registration and segmentation	multi-atlas segmentation;medical imaging;markov random fields;discrete optimization	Multi-atlas segmentation has emerged in recent years as a simple yet powerful approach in medical image segmentation. It commonly comprises two steps: (1) a series of pairwise registrations that establish correspondences between a query image and a number of atlases, and (2) the fusion of the available segmentation hypotheses towards labeling objects of interest. In this paper, we introduce a novel approach that solves simultaneously for the underlying segmentation labels and the multi-atlas registration. The proposed approach is formulated as a pairwise Markov Random Field, where registration and segmentation nodes are coupled towards simultaneously recovering all atlas deformations and labeling the query image. The coupling is achieved by promoting the consistency between selected deformed atlas segmentations and the estimated query segmentation. Additional membership fields are estimated, determining the participation of each atlas in labeling each voxel. Inference is performed by using a sequential relaxation scheme. The proposed approach is validated on the IBSR dataset and is compared against standard post-registration label fusion strategies. Promising results demonstrate the potential of our method.	image segmentation;linear programming relaxation;markov chain;markov random field;voxel	Stavros Alchatzidis;Aristeidis Sotiras;Evangelia I. Zacharaki;Nikos Paragios	2016	International Journal of Computer Vision	10.1007/s11263-016-0925-2	computer vision;segmentation-based object categorization;pattern recognition;data mining;mathematics;image segmentation;scale-space segmentation	Vision	44.52640470002614	-75.20663002041498	110350
08d26e3a45873f1b4c046e34f563e01397c9db67	evaluation of two cortical fraction estimation algorithms for the calculation of dynamic magnetic resonance renograms	metodo cuadrado menor;simulation ordinateur;nuclear magnetic resonance imaging;gd dtpa;contrast enhancement;substraccion imagen;rein;methode moindre carre;contrast enhanced;renal cortex;high resolution;urinary system;imagineria rmn;mr;aplicacion medical;image processing;least squares method;caracteristica dinamica;least square error;paramagnetic contrast agent;computer model;appareil urinaire;procesamiento imagen;hombre;tecnica medida;estimation algorithm;traitement image;magnetic resonance image;algorithme;feasibility;corticale renale;algorithm;corteza renal;rinon;renal function;aparato urinario;magnetic resonance;region of interest;human;technique mesure;caracteristique dynamique;agente contraste paramagnetico;renograms;gradient echo;imagerie rmn;medical application;simulacion computadora;agent contraste paramagnetique;dynamic characteristic;image subtraction;dynamic mri;diagnosis;measurement technique;computer simulation;kidney;partial volume effect;homme;application medicale;algoritmo;soustraction image	With the high resolution of dynamic magnetic resonance imaging (MRI) scans it is possible to measure cortical renograms directly, but due to partial volume effects this is impossible for medullary renograms. With weighted subtraction of the cortical renogram from a mixed renogram it becomes possible to extract the medullary renogram. For this subtraction the fraction of cortical tissue, present in the region of interest in which the mixed renogram is determined, has to be calculated. We have evaluated two algorithms for calculation of the cortical fraction. Both algorithms use the fact that during an interval after the start of the cortical enhancement no medullary enhancement occurs. One algorithm calculates the ratio between the slopes of both enhancement curves. The other is based on minimising the medullary signal values using a least squares error (LSE) method. Using a computer model of the renograms and measurements on real patients we analysed the accuracy of both methods and determined the best parameters for each.	algorithm;computer simulation;least squares;magnetic resonance imaging;numerical weather prediction;patients;region of interest;scanning;slope	E. L. W. Giele;J. A. de Priester;Johannes A. Blom;J. A. den Boer;J. M. A. van Engelshoven;Arie Hasman	2002	Computer methods and programs in biomedicine	10.1016/S0169-2607(01)00119-5	computer simulation;radiology;image resolution;computer science;magnetic resonance imaging;image subtraction;dynamic contrast-enhanced mri;mathematics;renal function;urinary system;nuclear medicine;least squares;partial volume;region of interest	ML	46.20364162443908	-79.64574551432094	110414
62466a2d0229ace2503fd9652d3de90b8f314395	interactive segmentation: a scalable superpixel-based method		This paper addresses the problem of interactive multiclass segmentation of images. We propose a fast and efficient new interactive segmentation method called superpixel α fusion (SαF). From a few strokes drawn by a user over an image, this method extracts relevant semantic objects. To get a fast calculation and an accurate segmentation, SαF uses superpixel oversegmentation and support vector machine classification. We compare SαF with competing algorithms by evaluating its performances on reference benchmarks. We also suggest four new datasets to evaluate the scalability of interactive segmentation methods, using images from some thousand to several million pixels. We conclude with two applications of SαF.	scalability	Bérengère Mathieu;Alain Crouzil;Jean-Baptiste Puel	2017	J. Electronic Imaging	10.1117/1.JEI.26.6.061606	support vector machine;computer science;pattern recognition;artificial intelligence;pixel;computer vision;scalability;segmentation;image editing;scale-space segmentation	Vision	41.67198938353008	-70.28403463088611	110662
64189ed03a95745cc5db3b0c4e9d07c7ac18d255	a gaussian-mixture-based image segmentation algorithm	gaussian mixture;algorithm performance;image segmentation;image processing;seuil;estimation method;etude experimentale;edge detection;echantillonnage;laplacian;procesamiento imagen;threshold;selective sampling;segmentation;traitement image;gray scale;deteccion contorno;sampling;algorithme;algorithm;detection contour;laplacien;laplaciano;gaussian mixture model;resultado algoritmo;performance algorithme;estimacion parametro;umbral;parameter estimation;estimation parametre;muestreo;echelle gris;estudio experimental;segmentacion;escala gris;algoritmo	This paper focuses on the formulation, development, and evaluation of an autonomous segmentation algorithm which can segment targets in a wide class of highly degraded images. A segmentation algorithm based on a Gaussian-mixture model of a two-class image is selected because it has the potential for effective segmentation provided that the histogram of the image approximates a Gaussian mixture and the parameters of the model can be estimated accurately. A selective sampling approach based on the Laplacian of the image is developed to transform the histogram of any image into an approximation of a Gaussian mixture and a new estimation method which uses information derived from the tails of the mixture density is formulated to estimate the model parameters. The resulting selective-sampling-Gaussian-mixture parameter-estimation segmentation algorithm is tested and evaluated on a set of real degraded target images and the results show that the algorithm is able to accurately segment diverse images.	algorithm;image segmentation	Lalit Gupta;Thotsapon Sortrakul	1998	Pattern Recognition	10.1016/S0031-3203(97)00045-9	image texture;sampling;computer vision;laplace operator;range segmentation;edge detection;image processing;computer science;segmentation-based object categorization;mixture model;mathematics;region growing;image segmentation;estimation theory;minimum spanning tree-based segmentation;scale-space segmentation;segmentation;grayscale;statistics;image histogram	Vision	51.04010360470565	-68.46238731049006	110766
27a61c68224838987ddaae9ec5c82b40633fba61	a new statistical detector for dwt-based additive image watermarking using the gauss–hermite expansion	libre mercado;discrete wavelet transforms;filigranage numerique;protection information;digital watermarking;detectors;watermarking;filtering;evaluation performance;metodo estadistico;decision thresholding;filtrage;probability density function pdf;dwt;additive white gaussian noise;image coding;discrete wavelet transform;probability;image segmentation;performance evaluation;hermite interpolation;receiver operating characteristics;gaussian processes;receiver operator characteristic;funcion densidad probabilidad;probability density function;ondelette;interpolation hermite;evaluacion prestacion;signal analysis;filtrado;false alarm rate;analisis de senal;statistical method;transformation ondelette discrete;awgn;detectors discrete wavelet transforms watermarking gaussian processes probability density function parameter estimation testing performance evaluation noise robustness additive white noise;testing;bruit blanc gaussien additif;hermite expansion;noise robustness;higher order;marche concurrentiel;fonction densite probabilite;interpolacion hermite;statistical analysis;watermarking discrete wavelet transforms gaussian processes image coding image segmentation probability statistical analysis;proteccion informacion;methode statistique;statistical detector;information protection;gauss hermite expansion;filigrana digital;estadistica orden superior;receiver operating characteristics statistical detector dwt discrete wavelet transforms additive image watermarking gauss hermite expansion pdf probability density function parameter estimation decision thresholding additive white gaussian noise filtering theory;generalized gaussian;wavelet coefficient gauss hermite gh image watermarking probability density function pdf receiver operating characteristics statistical detector;statistique ordre superieur;gauss hermite gh;additive white noise;pdf;image watermarking;open market;parameter estimation;higher order statistic;taux fausse alarme;porcentaje falsa alarma;wavelets	Traditional statistical detectors of the discrete wavelet transform (DWT)-based image watermarking use probability density functions (PDFs) that show inadequate matching with the empirical PDF of image coefficients in view o f the fact that they use a fixed number of parameters. Hence, the decision values obtained from the estimated thresholds of these detectors provide substandard detection performance. In this paper, a new detector is proposed for the DWT-based additive image watermarking, wherein a PDF based on the Gauss-Hermite expansion is used, in view of the fact that this PDF provides a better statistical match to the empirical PDF by utilizing an appropriate number of parameters estimated from higher-order moments of the image coefficients. The decision threshold and the receiver operating characteristics are derived for the proposed detector. Experimental results on test images demonstrate that the proposed watermark detector performs better than other standard detectors such as the Gaussian and generalized Gaussian (GG), in terms of the probabilities of detection and false alarm as well as the efficacy. It is also shown that detection performance of the proposed detector is more robust than the competitive GG detector in the case of compression, additive white Gaussian noise, filtering, or geometric attack.	additive model;additive white gaussian noise;coefficient;compression;computer performance;detector device component;detectors;digital watermarking;discrete wavelet transform;distortion;epilepsy, generalized;filter (signal processing);gadu-gadu;gauss;gaussian blur;gauss–hermite quadrature;monumenta germaniae historica;normal statistical distribution;portable document format;probability;receiver operator characteristics;utility functions on indivisible goods;pegvisomant	S. M. Mahbubur Rahman;M. Omair Ahmad;M. N. Shanmukha Swamy	2009	IEEE Transactions on Image Processing	10.1109/TIP.2009.2021313	additive white gaussian noise;econometrics;speech recognition;digital watermarking;computer science;signal processing;mathematics;receiver operating characteristic;statistics	Vision	52.719975768886165	-66.68831001587898	110836
8957c7d6993ebb746c371779c788d39897af27d1	apoptosis detection for adherent cell populations in time-lapse phase-contrast microscopy images	time lapse phase contrast microscopy;apoptosis detection;microscopy image restoration;event detection in videos	The detection of apoptosis, or programmed cell death, is important to understand the underlying mechanism of cell development. At present, apoptosis detection resorts to fluorescence or colorimetric assays, which may affect cell behavior and thus not allow long-term monitoring of intact cells. In this work, we present an image analysis method to detect apoptosis in time-lapse phase-contrast microscopy, which is nondestructive imaging. The method first detects candidates for apoptotic cells based on the optical principle of phase-contrast microscopy in connection with the properties of apoptotic cells. The temporal behavior of each candidate is then examined in its neighboring frames in order to determine if the candidate is indeed an apoptotic cell. When applied to three C2C12 myoblastic stem cell populations, which contain more than 1000 apoptosis, the method achieved around 90% accuracy in terms of average precision and recall.	apoptosis;cell death;cell development;fluorescence;frame (physical object);image analysis;information retrieval;microscopy, phase-contrast;one thousand;population;precision and recall;stem cells	Seungil Huh;Dai Fei Elmer Ker;Hang Su;Takeo Kanade	2012	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-33415-3_41	computer vision	Visualization	39.60612203336747	-73.44418360476608	110837
053e37b37d16049e686827986eaad4459b493452	multi-structure network shape analysis via normal surface momentum maps	shape analysis;statistical model;mr imaging;surface deformation;momentum map;open access;large deformation diffeomorphic metric mapping;diffeomorphisms;young adult;anatomical variation;surface momentum maps;random field	We present a shape analysis pipeline for the assessment of anatomical variations in subcortical networks in MR images. The shape analysis pipeline injects the global shape properties of the CFA subcortical template into the subcortical parcellations generated from FreeSurfer via large deformation diffeomorphic metric mapping (LDDMM). Examples are shown for this injection in several subcortical structures whose raw MR images were sampled from the database of Open Access Series of Imaging Studies (OASIS). The shape analysis is performed on random field representation of the template surface momentum maps that encode the shape variation of subcortical structure targets of each individual subject relative to the template. The momentum maps have the optimum property that they are supported only on the boundary of the subcortical structures with the direction normal to the subcortical nuclei boundary thereby reducing the dimension of shape variation significantly. A two-level statistical model was built on these momentum maps to assess anatomical connectivity among the subcortical structures on the basis of similar surface deformation (compression or expansion). Results in the study of healthy aging on the hippocampus-amygdala network indicate the anatomical connectivity between the basolateral complex of the amygdala and the subiculum of the hippocampus on the basis of shape compression in healthy elders relative to young adults.	amygdaloid structure;clinical use template;compression;connected_to relation;elder extract;encode (action);freesurfer;healthy aging;large deformation diffeomorphic metric mapping;map;shape analysis (digital geometry);statistical model	Anqi Qiu;Michael I. Miller	2008	NeuroImage	10.1016/j.neuroimage.2008.04.257	statistical model;computer vision;random field;topology;young adult;shape analysis;mathematics;geometry;statistics	Vision	42.95561551846896	-79.87696434135755	111384
9ead205654069ccccbbc098762c88dfd0256e0d7	least biased target selection in probabilistic atlas construction	target selection;multidimensional scaling;medical image segmentation;distance matrix	Probabilistic atlas has broad applications in medical image segmentation and registration. The most common problem building a probabilistic atlas is picking a target image upon which to map the rest of the training images. Here we present a method to choose a target image that is the closest to the mean geometry of the population under consideration as determined by bending energy. Our approach is based on forming a distance matrix based on bending energies of all pair-wise registrations and performing multidimensional scaling (MDS) on the distance matrix.	atlases;bending - changing basic body position;cervical atlas;decompression sickness;distance matrix;energy, physics;genetic selection;image registration;image scaling;image segmentation;medical records systems, computerized;test scaling;multidimensional scaling	Hyunjin Park;Peyton H. Bland;Alfred O. Hero;Charles R. Meyer	2005	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/11566489_52	computer vision;distance matrix;multidimensional scaling;computer science;machine learning;pattern recognition;mathematics	Vision	42.733393850210156	-78.26108755710257	111832
dbc49aa0e7f997fde52f457a3eefa2650c7a719c	useaq: ultra-fast superpixel extraction via adaptive sampling from quantized regions		We present a novel and highly efficient superpixel extraction method called ultra-fast superpixel extraction via adaptive sampling from quantized regions (USEAQ) to generate regular and compact superpixels in an image. To reduce the computational cost of iterative optimization procedures adopted in most recent approaches, the proposed USEAQ for superpixel generation works in a one-pass fashion. It first performs joint spatial and color quantizations and groups pixels into regions. It then takes into account the variations between regions, and adaptively samples one or a few superpixel candidates for each region. It finally employs maximum a posteriori estimation to assign pixels to the most spatially consistent and perceptually similar superpixels. It turns out that the proposed USEAQ is quite efficient, and the extracted superpixels can precisely adhere to boundaries of objects. Experimental results show that USEAQ achieves better or equivalent performance compared with the state-of-the-art superpixel extraction approaches in terms of boundary recall, undersegmentation error, achievable segmentation accuracy, the average miss rate, average undersegmentation error, and average unexplained variation, and it is significantly faster than these approaches. The source code of USEAQ is available at https://github.com/nchucvml/USEAQ.	adaptive sampling;algorithm;algorithmic efficiency;benchmark (computing);entity name part qualifier - adopted;extraction;image segmentation;iterative method;manifold regularization;mathematical optimization;physical object;pixel;refinement (computing);relevance;resultant;sampling (signal processing);sampling - surgical action;silo (dataset);smart battery;source code;time complexity;biologic segmentation	Chun-Rong Huang;Weiwei Wang;Wei-An Wang;Szu-Yu Lin;Yen-Yu Lin	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2848548	iterative method;computer vision;source code;adaptive sampling;maximum a posteriori estimation;image segmentation;pixel;mathematics;pattern recognition;artificial intelligence;quantization (physics);segmentation	Vision	51.767114241423656	-70.24960298380348	111865
19dad4d803b2f10f89f277dd54c9e88a977ca4df	snake model-based automatic segmentation of the left ventricle from cardiac mr images	selective smoothing direction gradient vector flow snake model;myocardium;automatic localization;shape constraints;gradient vector flow;image segmentation;muscle biomedical mri cardiology image segmentation medical image processing;cardiology;automatic segmentation;active contours;snake model based automatic segmentation;left ventricle;mr imaging;shape;image edge detection;medical image processing;pixel;image segmentation shape image edge detection smoothing methods myocardium active contours level set noise robustness anisotropic magnetoresistance information technology;elliptic shape constraint;shape priors;elliptic shape constraint snake model based automatic segmentation image segmentation left ventricle myocardium cardiac mr images selective smoothing direction gradient vector flow snake model automatic localization cardiac endocardium contour;cardiac endocardium contour;cardiac mr images;muscle;biomedical mri;left ventricle myocardium	An approach based on selective smoothing direction gradient vector flow (SSDGVF) snake model incorporating shape prior is proposed to segment the left ventricle from cardiac MR images in this paper. The originalities of the presented method include SSDGVF algorithm, automatic localization of the cardiac endocardium contour, and elliptic shape constraint. This novel approach can overcome the unexpected local minimum, and conquer the weak boundary leakage in tracking the boundaries of the left ventricle myocardium. Validation is performed on a set of 21 cardiac MR images, and satisfactory segmentation results are obtained. Keywords-snake; GVF; smooth selective; cardiac MR images	active contour model;algorithm;emoticon;gradient;logical volume management;maxima and minima;smoothing;spectral leakage;stemming	Yuwei Wu;Yuanquan Wang;Kun Lu	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5305142	computer vision;mathematical optimization;muscle;shape;computer science;bioinformatics;mathematics;image segmentation;pixel;anatomy	Vision	41.70013653451008	-76.29901108460356	111915
daa93238dc84b76eda0445009d3134b783a5c186	detection of transient signals in lung sounds: local approach using a markovian tree with frequency selectivity	hidden markov tree;generalized gaussian distributions;biomedical signals;lung sounds;signatures extraction;transient signals detection	We deal in this paper with the extraction of multiresolution statistical signatures for the characterization of transient signals in strongly noisy contexts. These short-time signals have sharp and highly variable frequency components. The time/frequency window to choose for our analysis is then a major issue. We have chosen the Wavelet Packet Transform (WPT) due to its ability to provide multiple windows analysis with different time/frequency resolutions. We propose a new oriented Hidden Markov Tree (HMT) dedicated to the tree structure of the WPT, which offers promising statistical characterization of time/frequency variations in a signal, by exploiting several time/frequency resolutions. This model is exploited in a Bayesian context for the segmentation of signals containing transient components. The chosen data likelihood is a Generalized Gaussian Distributions (GGD), well suited for the modeling of Wavelet Packet Coefficients (WPC) distributions. We demonstrate the efficiency of our method on synthetic signals with several Signal to Noise Ratio (SNR). Our application domain is related to biomedical signals, and more specifically for the detection of uprising abnormalities in pulmonary sounds. This original method shows remarkable ability to detect such sounds, which are usually buried in the normal lung noise and are often very difficult to perceive with the human earing.	selectivity (electronic)	Steven Le Cam;Christophe Collet;Fabien Salzenstein	2011	Signal Processing Systems	10.1007/s11265-010-0529-y	speech recognition;pattern recognition;statistics	ML	51.20816288566992	-74.65336198522556	112123
2e12a22c1f0c9b2ee2b64c9767cc0ad78e6e00e4	characteristic quantities of microvascular structures in clsm volume datasets	confocal laser scanning microscopy clsm;topology;image processing;microscopy;microvascular structures;automated topological quantification;microscopy in vitro muscles recruitment image segmentation volume measurement anisotropic magnetoresistance morphology network topology tissue engineering;morphology;laser applications in medicine;characteristic quantities;angiogenesis;medical image processing;clsm volume datasets;topology angiogenesis anisotropic skeletonization characteristic quantities compactness confocal laser scanning microscopy clsm morphology;biomedical image processing;image processing microvascular structures clsm volume datasets automated morphological quantification automated topological quantification confocal laser scanning microscopy;compactness;confocal laser scanning microscopy;anisotropic skeletonization;automated morphological quantification;biomedical optical imaging;optical microscopy biomedical optical imaging blood vessels laser applications in medicine medical image processing;optical microscopy;algorithms animals artificial intelligence databases factual humans image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval microcirculation microscopy confocal pattern recognition automated reproducibility of results sensitivity and specificity;blood vessels;tissue engineering	A method for fully automated morphological and topological quantification of microvascular structures in confocal laser scanning microscopy (CLSM) volume datasets is presented. Several characteristic morphological and topological quantities are calculated in a series of image-processing steps and can be used to compare single components as well as whole networks of microvascular structures to each other. The effect of the individual image-processing steps is illustrated and characteristic quantities of measured volume datasets are presented and discussed.	image processing;microscopy, confocal;quantitation;quantity;radionuclide imaging	Kirsten Winter;Lars H.-W. Metz;Jens-Peer Kuska;Bernhard Frerich	2007	IEEE Transactions on Medical Imaging	10.1109/TMI.2007.900379	computer vision;morphology;image processing;microscopy;confocal laser scanning microscopy;nanotechnology;optical microscope;mathematics;tissue engineering;compact space	Visualization	40.71340200950356	-78.18599894108254	112362
7c8a1e974f3a6eabb9e79e1b2cdb747d9223195e	multiple-target tracking of 3d fluorescent objects based on simulated annealing	3d fluorescent objects;multiframe object correspondence;evolutionary events;tyrosine phosphatase;fluorescent lar ptp;fluorescence;image segmentation;cancer;simulated annealing algorithm;biological system modeling;tumours;microscopy;tumours biomedical optical imaging biomembrane transport cancer enzymes fluorescence medical image processing optical microscopy simulated annealing;tumor progression multiple target tracking 3d fluorescent objects simulated annealing 3d time video microscopy np hard problem multiframe object correspondence feature correspondence evolutionary events carcinoma nbt ii cells fluorescent lar ptp tyrosine phosphatase;simulated annealing;biomembranes;multiple target tracking;carcinoma nbt ii cells;np hard problem;enzymes;proteins;medical image processing;3d time video microscopy;probability distribution;biomedical image processing;amino acids;feature correspondence;fluorescence simulated annealing object detection biological system modeling probability distribution proteins image segmentation np hard problem amino acids neoplasms;biomedical optical imaging;neoplasms;biomembrane transport;tumor progression;optical microscopy;object detection	"""This paper presents a framework for tracking fluorescent objects imaged with 3D+time video-microscopy. The proposed technique solves the NP-hard problem of the multi-frame object correspondence. For this purpose, we use the simulated annealing algorithm for its flexibility for feature correspondence and its ability to give results in a very short time. Our approach takes into account events like """"split"""", """"merge"""", """"birth"""" and """"death"""". These """"evolutionary events"""" are motivated by biological and physical considerations. We demonstrate the performance of the proposed algorithm on images of carcinoma NBT-II cells expressing fluorescent LAR-PTP, a tyrosine phosphatase possibly involved in tumor progression"""	algorithm;color gradient;simulated annealing	Victor Racine;Ariane Hertzog;Jacqueline Jouanneau;Jean Salamero;Charles Kervrann;Jean-Baptiste Sibarita	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1625094	computer vision;medicine;simulated annealing;computer science;bioinformatics;microscopy;mathematics;optics	Vision	39.741020270214364	-73.87118516129681	112386
a7bf3fdf190745acb35fa2785b99040f812af4a6	dense multi-modal registration with structural integrity using non-local gradients		In this work, the challenging problem of dense non-rigid registration [NRR] for multi-modal data is addressed. We look at a class of differentiable metrics based on weighted L2 distance of non-local image gradients. For intensity dependent choice of weights, the metric is seen to give enhanced multi-modal capability than using just gradients. In a variational dense deformation setting, the metric is coupled with non-local regularization to make the framework feature based. The above combination maintains the visual quality of the registered image, and gives a good correspondence for features of similar geometry under the challenges of noise, large motion, and presence of small structures. We also address computational speed ups of the energy minimization using an approximation scheme. The proposed approach is demonstrated on synthetic and medical data, and results are quantitatively compared with MI based, diffeomorphic NRR.	approximation;energy minimization;image gradient;image noise;modal logic;nl (complexity);robustness (computer science);structural integrity and failure;synthetic data;synthetic intelligence;variational principle	Sheshadri R. Thiruvenkadam	2013			artificial intelligence;computer vision;deformation (mechanics);computer science;differentiable function;energy minimization;similarity (geometry);regularization (mathematics);diffeomorphism	Vision	52.27998771822119	-72.55427766404283	112565
0a39497d1a5b43141739cf2293d6890dfa702f38	improved image segmentation with a modified bayesian classifier	multidimensional classifiers image segmentation modified bayesian classifier texture segmentation mixture probability density;bayesian classifier;probability;image segmentation;filter bank;probability density;bayes methods;image classification;texture segmentation;image texture;modified bayesian classifier;multidimensional classifiers;image segmentation bayesian methods filter bank multidimensional systems image edge detection statistics degradation bandwidth welding surface texture;probability bayes methods image classification image segmentation image texture;mixture probability density	A method for improving texture segmentation results by slightly modifying the decision surfaces of a Bayesian classifier is presented. Although a Bayesian classifier provides optimum classification within homogeneous regions, it does not necessarily provide accurate localization of region boundaries. In the proposed method, a modified classifier is formed by using a mixture probability density. This approach has the advantage that it is easily implemented in multidimensional classifiers such as those used in classifying the vector output of a filter bank. Experimental results demonstrate improved texture segmentation using the proposed classifier	bayesian network;decision boundary;filter bank;image segmentation;naive bayes classifier	Thomas P. Weldon	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660438	image texture;computer vision;bayes classifier;contextual image classification;probability density function;naive bayes classifier;computer science;machine learning;segmentation-based object categorization;pattern recognition;probability;filter bank;mathematics;region growing;image segmentation;scale-space segmentation;statistics	Robotics	50.37116940151706	-67.17820256358398	112752
c3d5097055c62cf8f050ec56abacbcc01490deeb	independent component analysis-based defect detection in patterned liquid crystal display surfaces	surface inspection;liquid crystal display;tft lcd panels;independent component analysis;automatic detection;machine vision;image reconstruction;thin film transistor;independent component;defect detection	In this paper, we propose a machine vision approach for automatic detection of micro defects in periodically patterned surfaces and, especially, aim at thin film transistor liquid crystal display (TFT-LCD) panels. The proposed method is based on an image reconstruction scheme using independent component analysis (ICA). ICA is first applied to a faultless training image to determine the de-mixing matrix and the corresponding independent components (ICs). The ICs representing the global structure of the training image are then identified and the associated row vectors of those ICs in the de-mixing matrix are replaced with a de-mixing row representing the least structured region of the training image. The reformed de-mixing matrix is then used to reconstruct the TFT-LCD image under inspection. The resulting image can effectively remove the global structural pattern and preserve only local anomalies. A number of micro defects in different TFT-LCD panel surfaces are evaluated with the proposed method. The experiments show that the proposed method can well detect various ill-defined defects in periodically patterned surfaces.	independent component analysis;liquid-crystal display;software bug	Chi-Jie Lu;Du-ming Tsai	2008	Image Vision Comput.	10.1016/j.imavis.2007.10.007	iterative reconstruction;independent component analysis;computer vision;thin-film transistor;machine vision;computer science;machine learning;liquid-crystal display	Robotics	48.12418071441205	-66.46911898655144	112830
6e11d5309a6323443f60c00a1053cb464311868f	a kd curvature based corner detector	corner detection;feature detection;corner strength;curvature	Due to the low efficiency of curvature calculation in corner detection algorithms, we propose a new corner detection technique for discrete curvature estimation based on KD curvature. Firstly, KD curvature is redefined and computed for each point on the curve after edge extraction and smoothing. Then, nonmaximum suppression is used for obtaining candidate corner sets. Finally, the refined corner sets are retained with false and unstable corners removed. In addition, we introduce corner strength, as a new concept, for controlling detection precision. Our experimental results show that the proposed method outperforms existing detectors in both computational efficiency and flexibility of corner detection. Moreover, the average repeatability rate and local error rate are raised about 20 percent respectively. & 2015 Elsevier B.V. All rights reserved.	algorithm;computation;control theory;corner detection;definition;digital image;image noise;repeatability;sensor;smoothing;zero suppression	Suting Chen;Hao Meng;Chuang Zhang;Changshu Liu	2016	Neurocomputing	10.1016/j.neucom.2015.01.102	corner detection;topology;computer science;feature detection;mathematics;geometry;curvature	Vision	39.57626687659268	-66.29482715888771	112860
2f66bbf96261c202548f6e0999603aa51388bce4	axonal bouton modeling, detection and distribution analysis for the study of neural circuit organization and plasticity	neuroanatomy axonal bouton modeling bouton distribution analysis neural circuit organization and plasticity light microscopy confocal microscopy two photon microscopy neuron;two photon microscopy;probability;image resolution;bouton distribution analysis;circuits nerve fibers microscopy neurons neuroscience data mining biomedical engineering probability image resolution neurotransmitters;neural nets;neural synapses;neural circuit dynamics;presynaptic specialization;automated labeled neuron detection;neural circuit organization;microscopy;nerve fibers;light microscopy;indexing terms;axonal bouton modeling;data mining;neuroscience;plasticity;neural circuit dynamics axonal bouton modeling neural circuit organization plasticity automated labeled neuron detection bouton distribution analysis presynaptic specialization neural synapses;biomedical engineering;neural circuit organization and plasticity;position estimation;plasticity biomedical optical imaging neural nets neurophysiology optical microscopy;neurotransmitters;probability of false alarm;circuits;geometric model;neuroanatomy;neurons;neurophysiology;biomedical optical imaging;confocal microscopy;neuron;optical microscopy;probability of detection;normalized cross correlation	We propose a novel method for axonal bouton modeling and automated detection in populations of labeled neurons, as well as bouton distribution analysis for the study of neural circuit organization and plasticity. Since axonal boutons are the presynaptic specializations of neural synapses, their locations can be used to determine the organization of neural circuitry, and in time-lapse studies, neural circuit dynamics. We propose simple geometric models for axonal boutons that account for variations in size, position, rotation and curvature of the axon in the vicinity of the bouton. We then use the normalized cross-correlation between the model and image data as a test statistic for bouton detection and position estimation. Thus, the problem is cast as a statistical detection problem where we can tune the algorithm parameters to maximize the probability of detection for a given probability of false alarm. For example, we can detect 81% of boutons with 9% false alarm from noisy, out of focus, images. We also present a novel method to characterize the orientation and elongation of a distribution of labeled boutons and we demonstrate its performance by applying it to a labeled data set.	algorithm;artificial neural network;cross-correlation;electronic circuit;population	Christina A. Hallock;Inci Ozgunes;Ramamurthy Bhagavatula;Gustavo K. Rohde;Justin C. Crowley;Christina E. Onorato;Abhay Mavalankar;Amina Chebira;Chuen Hwa Tan;Markus Püschel;Jelena Kovacevic	2008	2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2008.4540958	electronic circuit;plasticity;two-photon excitation microscopy;index term;image resolution;computer science;microscopy;artificial intelligence;geometric modeling;confocal laser scanning microscopy;cross-correlation;probability;statistical power;optical microscope;mathematics;optics;anatomy	ML	41.053913729878616	-74.92884812902814	112979
738ff30f53ad87ffe7f8bd4a3898ec84302e5c1c	non-parametric higher-order random fields for image segmentation		Models defined using higher-order potentials are becoming increasingly popular in computer vision. However, the exact representation of a general higher-order potential defined over many variables is computationally unfeasible. This has led prior works to adopt parametric potentials that can be compactly represented. This paper proposes a non-parametric higher-order model for image labeling problems that uses a patch-based representation of its potentials. We use the transformation scheme of [11, 25] to convert the higher-order potentials to a pair-wise form that can be handled using traditional inference algorithms. This representation is able to capture structure, geometrical and topological information of labels from training data and to provide more precise segmentations. Other tasks such as image denoising and reconstruction are also possible. We evaluate our method on denoising and segmentation problems with synthetic and real images.	algorithm;computer vision;experiment;image segmentation;noise reduction;patch (computing);pixel;sparse approximation;sparse matrix;synthetic intelligence;unary operation	Pablo Márquez-Neila;Pushmeet Kohli;Carsten Rother;Luis Baumela	2014		10.1007/978-3-319-10599-4_18	computer vision;theoretical computer science;machine learning;mathematics;statistics	Vision	52.818376302665826	-70.61306616952287	113098
8e36e7c5ab0a54340d9fbecb15875d6c489117ff	boundary aware reconstruction of scalar fields	kernel regression;boundary conditions;volume rendering;reconstruction;computer and information science;pattern classification data visualisation;image classification;data modeling;natural sciences;image reconstruction boundary conditions data modeling image classification rendering computer graphics probabilistic logic data visualization;image reconstruction;signal processing;data visualization;datavetenskap datalogi;boundary aware reconstruction multivariate data scalability tf dimensionality derivative expression tf design transfer function transitional regions material specific reconstructions classifier information data classification data reconstruction visualization scalar field;computer science;probabilistic logic;data och informationsvetenskap;rendering computer graphics	In visualization, the combined role of data reconstruction and its classification plays a crucial role. In this paper we propose a novel approach that improves classification of different materials and their boundaries by combining information from the classifiers at the reconstruction stage. Our approach estimates the targeted materials' local support before performing multiple material-specific reconstructions that prevent much of the misclassification traditionally associated with transitional regions and transfer function (TF) design. With respect to previously published methods our approach offers a number of improvements and advantages. For one, it does not rely on TFs acting on derivative expressions, therefore it is less sensitive to noisy data and the classification of a single material does not depend on specialized TF widgets or specifying regions in a multidimensional TF. Additionally, improved classification is attained without increasing TF dimensionality, which promotes scalability to multivariate data. These aspects are also key in maintaining low interaction complexity. The results are simple-to-achieve visualizations that better comply with the user's understanding of discrete features within the studied object.	classification;color mapping;data model;estimated;gradient;imagery;kernel;morphologic artifacts;overhead (computing);probabilistic turing machine;rendering (computer graphics);s-expression;sampling - surgical action;scalability;scalar processor;scientific publication;semiconductor industry;sensitivity and specificity;signal reconstruction;signal-to-noise ratio;small;tf–idf;transfer function	Stefan Lindholm;Daniel Jönsson;Charles D. Hansen;Anders Ynnerman	2014	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2014.2346351	iterative reconstruction;data modeling;kernel regression;computer vision;contextual image classification;boundary value problem;computer science;theoretical computer science;signal processing;data mining;probabilistic logic;volume rendering;data visualization;statistics	Visualization	51.41472838216714	-75.22149920029035	113125
d2f9331a8a834975fec33dc8ebacf7a79d3466f8	rapid identification of neuronal structures in electronic microscope image using novel combined multi-scale image features	electronic microscopy;membrane;neuron segmentation	This paper proposes an incremental scheme to identify the neurons in electronic microscope image. It first computes the probability of each pixel being cell membrane and then determines the image areas of the neurons. Our contributions also include novel combined multi-scale image features that are computationally efficient and provide strong discrimination ability for differentiating membrane pixels from other pixels. Experiment results show that the proposed system offers much higher speed than the compared state-of-the-art methods while produces comparably high accuracy.	algorithm;algorithmic efficiency;kullback–leibler divergence;microsoft research;neuron;pixel;rand index;watershed (image processing)	Jingwen Zhao;Hui Li;Meng Duan;Shuohong Wang;Yan Qiu Chen	2017	Neurocomputing	10.1016/j.neucom.2016.12.006	computer vision;membrane	ML	41.3105516338455	-71.00463170315355	113311
6ed3f25370dbcd253009adc412f3e86e0007d254	efficient multimodal registration using least-squares	fft;image registration;least squares;multimodal registration;linear approximation;cost function;least square;remote sensing;global optimization;magnetic resonance image;mutual information	Multimodal image registration is a difficult problem in both medical imaging and remote sensing. The least-squares cost function has generally been overlooked for multimodal registration problems due to an underlying assumption that the two images being registered must have corresponding intensities. More recently, methods that employ the least-squares cost function have been developed to efficiently evaluate the globally optimal shift and intensity remapping simultaneously. However, these methods estimate the translation and not the rotation. In this paper we propose a method for using the least-squares cost function efficiently for multimodal registration. By modeling rotation using a linear approximation, we find the globally optimal translation and intensity remapping, and locally optimal rotation angle. In a series of experiments based on registering PD-, T1-, and T2-weighted magnetic resonance images, our method performs better than mutual information.	experiment;image registration;least squares;linear approximation;local optimum;loss function;maxima and minima;medical imaging;multimodal interaction;multimodal learning;mutual information;resonance	Maja Omanovic;Jeff Orchard	2006			global optimization;linear approximation;mutual information;machine learning;least squares;image registration;computer vision;computer science;artificial intelligence	Vision	48.86018044033856	-75.09439488392731	113345
c00ef47f36a9ac24cc530119a91db97bba938c17	rf inhomogeneity correction algorithm in magnetic resonance imaging	gabor transform;magnetic resonance image;mr imaging	MR images usually present grey level inhomogeneities which are a problem of significant importance. Eliminating these inhomogeneities is not an easy problem and has been studied and discussed in several previous publications. Most of those approaches are based on segmentation processes. The algorithm presented in this paper has the advantage that it does not involve any segmentation step. Instead, a interpolating polynomial model based on a Gabor transform was used to construct a filter that can be used in order to correct these inhomogeneities. The results obtained are really good and show that the grey-level inhomogeneities can be corrected without segmentation.	algorithm;grayscale;polynomial interpolation;radio frequency;resonance	Juan Antonio Hernández Tamames;Martha L. Mora;Emanuele Schiavi;Pablo Toharia	2004		10.1007/978-3-540-30547-7_1	shinnar-le roux algorithm;k-space;magnetic resonance microscopy;nuclear magnetic resonance;physics of magnetic resonance imaging	Vision	45.78847857891965	-74.46004926304738	113621
13e360573a133892c62cefb135fea78476d659e2	texture classification and segmentation using simultaneous autoregressive random model	image recognition;least squares approximations;image segmentation;texture classification;least square method;image texture;medical image processing;parameter estimation biomedical ultrasonics image recognition image segmentation image texture least squares approximations medical image processing;human b scan images texture classification texture segmentation simultaneous autoregressive random model least squares method parameter estimation;parameter estimation;biomedical ultrasonics;image segmentation parameter estimation statistics liver image edge detection image classification maximum likelihood estimation pixel random variables gaussian noise	The simultaneous autoregressive (SAR) model is used to describe texture. The authors also propose using the least-squares method to estimate six SAR parameters. Based on the SAR model and the parameter estimation method, experiments have been done to classify and segment images of various natural textures and human B-scan images. Excellent results have been obtained. >	autoregressive model	Mengyang Liao;Jiamei Qin;Yanni Tan	1992		10.1109/CBMS.1992.244923	image texture;computer vision;feature detection;computer science;machine learning;segmentation-based object categorization;pattern recognition;region growing;image segmentation;estimation theory;scale-space segmentation;texture compression;least squares	Vision	48.25676960774743	-72.81141145731225	113780
8176fd7516b82020bef5bc2927ef245b0b9d4418	narrow band to image registration in the insight toolkit	image processing;level set approach;compact representation;image registration;data structure	  This paper introduces the new concept of narrow-band to image registration. Narrow-banding is a common technique used in the  solution of level set approaches to image processing. For our application, the narrow-band describes the shape of an object  by using a data structure containing the signed distance values at a small band of neighboring pixels. This compact representation  of an object is well suited for performing registration against a standard image as well as against another narrow-band. The  novel technique was implemented in the registration framework of the NLM Insight Toolkit (ITK). This implementation illustrates  the great advantage of a modular framework structure that allows researchers to concentrate in the interesting aspects of  a new algorithm by building on an existing set of predefined components for providing the rest of standard functionalities  that are required.    	itk;image registration	Lydia Ng;Luis Ibáñez	2003		10.1007/978-3-540-39701-4_29	computer vision;feature detection;data structure;image processing;computer science;image registration;theoretical computer science;digital image processing;computer graphics (images)	Vision	44.84761960281994	-71.53736244479182	113970
efe21ca966ecaf35873046c2449cd83251f435d1	inhomogeneity correction for magnetic resonance images with fuzzy c-mean algorithm	brain;image segmentation;magnetism;tissues;quantitative analysis;pathology	Abstract: Segmentation of magnetic resonance (MR) images plays an important role in quantitative analysis of brain tissue morphology and pathology. However, the inherent effect of image-intensity inhomogeneity renders a challenging problem and must be considered in any segmentation method. For example, the adaptive fuzzy c-mean (AFCM) image segmentation algorithm proposed by Pham and Prince can provide very good results in the presence of the inhomogeneity effect under the condition of low noise levels. Their results deteriorate quickly as the noise level goes up. In this paper, we present a new fuzzy segmentation algorithm to improve the noise performance of the AFCM algorithm. It achieves accurate segmentation in the presence of inhomogeneity effect and high noise levels by incorporating the spatial neighborhood information into the objective function. This new algorithm was tested by both simulated experimental and real clinical MR images. The results demonstrated the improved performance of this new algorithm over the AFCM in the clinical environment where the inhomogeneity and noise levels are commonly encountered.	algorithm;image segmentation;loss function;mathematical morphology;noise (electronics);optimization problem;prince;rendering (computer graphics);resonance	Xiang Li;Lihong Li;Hongbing Lu;Dongqing Chen;Zhengrong Liang	2003		10.1117/12.481375	computer vision;simulation;geography;artificial intelligence;scale-space segmentation	Vision	43.50650174502037	-73.29484245503627	114008
97ba0c9f0b2ad1826380254fb4fbc6707abf6a99	a rule-based approach to hand x-ray image segmentation	image features;image formation;x ray imaging;rule based	Here we present a rule-based approach which allows the localization and segmentation of several hand bones in a radiograph. The approach lies on extracting and interpreting different types of information: the image, the domain, the image formation and its processing techniques. We study several interpretations of the image features (edge, valley and ridge points) obtained from a facet model and we apply them to X-ray images. The variety and type of the information available leads us to implemement the technique mentioned above in a rule-based segmentation system.		Petia Radeva	1993		10.1007/3-540-57233-3_86	rule-based system;image texture;computer vision;feature detection;image gradient;binary image;image processing;computer science;segmentation-based object categorization;region growing;image segmentation;scale-space segmentation;image formation;feature	Vision	41.370327327399416	-69.64472492752702	114088
2a720f3380ea440421d294f43d582d4a0b5fcbca	anisotropic laplace-beltrami eigenmaps: bridging reeb graphs and skeletons	eigenvalues and eigenfunctions;graph theory;biological patents;topology;corpus callosum anisotropic laplace beltrami eigenmap reeb graphs robust topology simply connected surfaces eigenfunctions anisotropic laplace beltrami operator flux based weight function intrinsic geometry surface geometry neuroanatomical structures cingulate gyrus;robust topology;biomedical journals;text mining;europe pubmed central;anisotropic laplace beltrami eigenmap;citation search;level set;geometry;computational geometry;cingulate gyrus;biomedical imaging;citation networks;skeleton;mathematical operators;surface geometry;distance measurement;intrinsic geometry;shape;corpus callosum;anisotropic laplace beltrami operator;reeb graphs;research articles;neuroanatomical structures;abstracts;neuroimaging;open access;anisotropic magnetoresistance;life sciences;clinical guidelines;robustness;weight function;full text;flux based weight function;anisotropic magnetoresistance skeleton topology shape biomedical imaging robustness eigenvalues and eigenfunctions geometry biomedical computing neuroimaging;simply connected surfaces;rest apis;orcids;europe pmc;biomedical computing;biomedical research;mathematical operators computational geometry eigenvalues and eigenfunctions graph theory;laplace beltrami operator;eigenfunctions;bioinformatics;literature search	In this paper we propose a novel approach of computing skeletons of robust topology for simply connected surfaces with boundary by constructing Reeb graphs from the eigen-functions of an anisotropic Laplace-Beltrami operator. Our work brings together the idea of Reeb graphs and skeletons by incorporating a flux-based weight function into the Laplace-Beltrami operator. Based on the intrinsic geometry of the surface, the resulting Reeb graph is pose independent and captures the global profile of surface geometry. Our algorithm is very efficient and it only takes several seconds to compute on neuroanatomical structures such as the cingulate gyrus and corpus callosum. In our experiments, we show that the Reeb graphs serve well as an approximate skeleton with consistent topology while following the main body of conventional skeletons quite accurately.	anatomy, regional;approximation algorithm;body of uterus;bridging (networking);computation (action);corpus callosum;eigen (c++ library);experiment;graph - visual representation;gyrus cinguli;morphometric analysis;morphometrics;shape analysis (digital geometry);skeleton;weight function;width	Yonggang Shi;Rongjie Lai;Sheila Krishna;Nancy L. Sicotte;Ivo D. Dinov;Arthur W. Toga	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4563018	magnetoresistance;combinatorics;text mining;weight function;topology;computational geometry;shape;level set;graph theory;mathematics;geometry;laplace–beltrami operator;eigenfunction;skeleton;neuroimaging;robustness	Vision	46.91622497657195	-76.66454206630533	114168
5ccfe266e5b4ea0a420ef1f00bc00bec9e87c868	an automatic segmentation method of the spinal canal from clinical mr images based on an attention model and an active contour model	unsupervised learning;international organizations;dice similarity index;clinical mr images;active contour;image segmentation;irrigation;saliency map;unsupervised image segmentation method;spinal cord;spinal canal;automatic segmentation;level set;unsupervised segmentation;dice similarity index image segmentation method spinal canal clinical mr images active contour model unsupervised image segmentation method feature extraction sagittal plane saliency driven attention model patient data analysis;active contours;segmentation;unsupervised learning biomedical mri data analysis feature extraction image segmentation medical image processing neurophysiology;indexes;data analysis;mr imaging;computational modeling;spinal cord injury;sagittal plane;patient data analysis;magnetic resonance;image segmentation spinal cord irrigation active contours indexes magnetic resonance imaging computational modeling;feature extraction;medical image processing;magnetic resonance imaging;level set spinal canal segmentation saliency map active contour;inflammatory disease;saliency driven attention model;image segmentation method;neurophysiology;experience base;active contour model;similarity index;biomedical mri	The spinal cord is a vital organ that serves as the only communication link between the brain and the various parts of the body. It is vulnerable to traumatic spinal cord injury and various diseases such as tumors, infections, inflammatory diseases and degenerative diseases. The exact segmentation and localization of the spinal cord are essential to effective clinical management of such conditions. In recent years, due to the advances in imaging technology, the structure of internal organs and tissues can be captured accurately, and various abnormalities are diagnosed based on scanned images. In this paper, we present an unsupervised segmentation method that automatically extracts the spinal canal in the sagittal plane of magnetic resonance (MR) images. This segmentation method based on a novel saliency-driven attention model and a standard active contour model requires no human intervention and no training. Experiments based on 60 patients' data show that this procedure performs segmentation robustly, achieving the Dice's similarity index of 0.71 between the segmentation by our model and reference segmentation, as compared to the Dice's similarity index of 0.90 between two observers.	active contour model;experiment;image scanner;imaging technology;resonance	Jaehan Koh;Peter D. Scott;Vipin Chaudhary;Gurmeet Dhillon	2011	2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2011.5872677	computer vision;radiology;computer science;magnetic resonance imaging;machine learning;pattern recognition;active contour model	Vision	39.274015146746954	-77.84943620957	114323
924be9f6f9335232e311f38286759ea39dda26a8	multimodal medical image registration using a novel implementation of the icp algorithm	cardiology;image registration;iterative methods;matrix algebra;medical image processing;3d data registration;3d free form closed surface registration;cicp;comprehensive icp algorithm;global rigid transformation;heart;iterative closest point;left ventricle;look up matrix;medical diagnosis;medical imaging modality;medical treatment;multimodal medical image registration;parameter estimation;estimation;biomedical imaging	Image registration is a valuable technique for medical diagnosis and treatment. In this paper, we present an enhanced implementation of the popular iterative closest point (ICP) algorithm developed for the registration of 3D free-form closed surfaces. The main step of the ICP consists of finding the closest points between data sets which are then used to estimate the parameters of the global rigid transformation. We propose a new technique based on the use of a look up matrix for finding the best correspondence pairs. The algorithm, called Comprehensive ICP (CICP) algorithm, is then successfully applied for the registration of 3D data of the left ventricle of the heart, acquired using two different medical imaging modalities.	distance matrix;experiment;genetic algorithm;image registration;iterative closest point;iterative method;logical volume management;lookup table;medical imaging;multimodal interaction;proximity problems	Ahmad Almhdie;Christophe Léger;Mohamed A. Deriche;Roger Lédée	2007	2007 15th European Signal Processing Conference		point set registration;computer vision;mathematical optimization;computer science;image registration;theoretical computer science;iterative closest point	Vision	46.152205530812786	-77.18868641568271	114389
a7075491df17633f22776a5ed86999f6d75ddc4c	segmentation and shape extraction of 3d boundary meshes		In this report we present the state of the art on segmentation, or partitioning techniques used on boundary meshes. Recently, these have become a part of many mesh and object manipulation algorithms in computer graphics. We formulation the segmentation problem as an optimization problem and identify two primarily distinct types of mesh segmentation, namelyparts segmentation and patch segmentation. We classify previous segmentation solutions according to the different segmentation goals, the optimization criteria and features used, and the various algorithmic techniques employed. We also present generic algorithms for the major techniques of segmentation.	3d printing	Ariel Shamir	2006		10.2312/egst.20061056	computer vision;mathematical optimization;computer science;segmentation-based object categorization;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;engineering drawing	Robotics	46.66136704524014	-71.47831054474041	114612
1edab0df3df1b2c844da16f4c32f6502dfd214dd	interactive binary active contours for prostate contour delineation	manuals;bhattacharyya distance;image segmentation;shape image segmentation active contours tv manuals deformable models training;training;geometry;active contours;deformable models;supine prostate segmentation interactive binary active contours prostate contour delineation interactive segmentation framework mr images finsler active contours fac statistical shape user feedback prostate shape cost functional local image statistics geometric shape prior knowledge statistical shape prior knowledge clinical data sets prone prostate segmentation;shape;statistical analysis;medical image processing;shape prior;total variation;statistical analysis biomedical mri geometry image segmentation interactive systems medical image processing;tv;total variation active contours bhattacharyya distance user interaction shape prior;user interaction;interactive systems;biomedical mri	We present a new interactive segmentation framework to delineate the prostate from MR images. We first explicitly address the segmentation problem based on fast globally Finsler Active Contours (FAC) by incorporating both statistical and geometric shape prior knowledge. In doing so, we are able to exploit the more global aspects of segmentation by incorporating user feedback in segmentation process. In addition, once the prostate shape has been segmented, a cost functional is designed to incorporate both the local image statistics as user feedback and the learned shape prior. We provide experimental results, which include several challenging clinical data sets, to highlight the algorithm's capability of robustly handling supine/prone prostate segmentation.	algorithm;fly-by-wire;image segmentation;mathematical optimization;scene statistics	Foued Derraz;Laurent Peyrodie;Abdelmalik Taleb-Ahmed;Gérard Forzy	2012	2012 IEEE 12th International Conference on Bioinformatics & Bioengineering (BIBE)	10.1109/BIBE.2012.6399727	computer vision;bhattacharyya distance;shape;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation;total variation	Vision	43.38738409705913	-77.41570078099056	114670
6ff00a152ed8f3df1eb5dbfefd4dbfee773233f2	three-dimensional active surface approach to lymph node segmentation	analisis imagen;computerized axial tomography;tomodensitometria;energia superficie;radiodiagnostic;medical imagery;surface energy;sistema activo;modelo 3 dimensiones;analyse surface;3d imaging;modele 3 dimensions;analisis forma;diagnostico;three dimensional model;segmentation;estimacion a priori;three dimensional;systeme actif;active system;a priori estimation;radiodiagnostico;tomodensitometrie;analisis superficie;surface model;imagerie medicale;estimation a priori;energie surface;ganglion lymphatique;ganglio linfatico;image analysis;pattern analysis;imageneria medical;radiodiagnosis;diagnosis;user interaction;analyse image;segmentacion;analyse forme;lymph node;surface analysis;x rays;diagnostic	A three-dimensional active surface model has been suggested as a possible solution to the difficult problem of segmenting lymph nodes in CT x-rays. In this paper, a computationally simple active surface, or balloon, is proposed which requires minimal user interaction or a priori shape knowledge. A structure is proposed for the balloon model in which each balloon point is guaranteed a fixed number of neighbors, and a method is provided for adding points to the model while still maintaining that structural regularity. Equations are provided for deriving surface energy from 3D shape and 3D image data. Minimal user interaction is required, with only a single point somewhere inside the node needed to initialize the algorithm. The balloon naturally inflates to find the correct surface due to a unique candidate point selection algorithm that is biased in favor of outward moves. Preliminary results show the model to be successful in finding boundaries in synthetic 3-D images.© (1999) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		David M. Honea;Wesley E. Snyder	1999		10.1117/12.348493	geography;telecommunications;artificial intelligence;cartography	Vision	46.00184654812468	-79.07246732215972	114720
d09d0915cf69ddb684c343c5a818023f8b986b17	threshold selection using second derivatives of the gray scale image	image recognition;image segmentation;histograms pixel colored noise image analysis image color analysis laboratories convolution iterative methods production facilities feature extraction;input population pixel intensity character recognition gray scale image bilevel image curvature blurred image threshold selection partial histogram extrema values low contrast images textured backgrounds dot matrices;optical character recognition;image segmentation image recognition optical character recognition	It is known that when a bilevel image is blurred, the intensity of the original pixels is related with the sign of the curvature of the pixels of the blurred image. A technique for threshold selection is presented where a partial histogram is constructed solely from the pixels where curvature achieves extrema values. The method is most suitable when low-contrast images with textured backgrounds (but not sparse dot matrices) are a large fraction of the input population. >		Theodosios Pavlidis	1993		10.1109/ICDAR.1993.395733	color histogram;image texture;image restoration;computer vision;feature detection;speech recognition;image resolution;color image;binary image;image processing;computer science;free boundary condition;pattern recognition;image segmentation;pixel connectivity;optical character recognition;anisotropic diffusion;non-local means;image histogram	Robotics	48.54733660128979	-66.95194009623415	114723
d9ab31e1e3e83370887cc38356bb8c6769be1abc	reduction of noise and image artifacts in computed tomography by nonlinear filtration of projection images	gaussian noise;nonlinear diffusion;computed tomography;image resolution;nonlinear filter;anisotropic diffusion;computing systems;cross section;filtered backprojection;diffusion;tomography;gaussian distribution;nonlinear filtering	This study investigates the efficacy of filtering two-dimensional (2D) projection images of Computer Tomography (CT) by the nonlinear diffusion filtration in removing the statistical noise prior to reconstruction. The projection images of Shepp-Logan head phantom were degraded by Gaussian noise. The variance of the Gaussian distribution was adaptively changed depending on the intensity at a given pixel in the projection image. The corrupted projection images were then filtered using the nonlinear anisotropic diffusion filter. The filtered projections as well as original noisy projections were reconstructed using filtered backprojection (FBP) with Ram-Lak filter and/or Hanning window. The ensemble variance was computed for each pixel on a slice. The nonlinear filtering of projection images improved the SNR substantially, on the order of fourfold, in these synthetic images. The comparison of intensity profiles across a cross-sectional slice indicated that the filtering did not result in any significant loss of image resolution.© (2001) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	ct scan;nonlinear system;tomography;visual artifact	Omer Demirkaya	2001		10.1117/12.430964	iterative reconstruction;gaussian noise;computer vision;mathematical optimization;mathematics;optics;anisotropic diffusion	Vision	52.83255903094754	-75.45171159186476	114873
36e0326f6b1afa06a7bbdadf7b100512a1808b87	analysis of hdaf for interpolation and noise suppression in volume rendering		In this paper, we evaluate the HDAF (Hermite Distributed Approximating Functionals) family of interpolation and derivative functions, with respect to their accuracy for reliable volume rendering, and compare them with other interpolation and derivative estimation filters. We utilize several different evaluation methods, both analytical and experimental. The former includes the order of decay of the global error, the local spatial error, and the behavior of the filters in the frequency domain. In the experimental part, visualizations of both synthetic and medical data are produced and studied. We show that the HDAFs exhibit superior behavior if the volumetric data are distorted by high frequency noise, and perform well under noise free conditions. This due to their ability to adjust the range of recovered frequencies.	interpolation;volume rendering;zero suppression	K. Anderson;Ioannis A. Kakadiaris;Emmanuel Papadakis;Donald Kouri;David K. Hoffman	2003				Visualization	51.12260801432666	-77.95002184355339	115052
b34e6f2d7c669796a6b4380ed3e0979fd1b75a7c	a nurbs-based technique for the segmentation of medical images	magnetic resonance	Extractingthehumanbrain frommagneticresonancehead scans is difficult becauseof its highly convoluted and nonuniformgeometry. A technique basedon Non-Uniform Rational B-Splines(NURBS)and energy minimising deformable modelsto extract the brain surfaceaccurately fromMRheadscansis presented. Theweightingparameter that comeswith theNURBSdefinitionis exploredto attract the surfaceinto the regionsshowinghigh curvature. The weightat each control point is adjustedautomaticallyaccording to thecurvature propertiesof theevolvingsurface. Thisprocessfacilitatesa deformablesurfacewith increased local flexibility thatadaptsto complex geometricalfeatures of thebrain. Theresultsshowthattheproposedmodelis capableof capturingthe correct brain surfacewith a higher accuracythantheexistingtechniques.	control point (mathematics);maxima and minima;non-uniform rational b-spline;personalbrain	Ravinda G. N. Meegama;Jagath C. Rajapakse	2002			computer science;computer vision;artificial intelligence;pattern recognition;brain surface;curvature;non-uniform rational b-spline;segmentation;weighting;control point	Vision	45.3236823021138	-75.71834170272983	115076
802df697cb51e6777192d49956922581fb034088	a novel intelligent approach to anchorage measurement using electron microscopy	image segmentation coating techniques electron microscopy packaging inspection fuzzy logic quality control data acquisition;fuzzy reasoning;image segmentation;image processing;electron microscopy packaging biomedical imaging quality control seals inspection fuzzy reasoning fusion power generation image generation image processing;fuzzy logic based quantifier intelligent approach anchorage measurement electron microscopy medical packaging industry visual inspection procedure image processing technique fuzzy reasoning approach quality control system image thresholding image analysis data acquisition;packaging;inspection;fuzzy logic;electron microscopy;visual inspection;coating techniques;image processing techniques;decision process;quality control;data acquisition	There are several key attributes within the medical packaging industry, such as, anchorage, seal strength etc, which characterize the quality of the medical packaging product. This paper presents a new approach to determine the anchorage of the packaging material. The conventional approach to anchorage measurement involves a manual visual inspection procedure, which is very subjective and has led to inconsistencies in quality control. A new proposed method to improve this procedure involves image-processing technique, which eliminates operator dependencies. A fuzzy reasoning approach is then used to quantify the results generated from image processing which replaces the human decision process. This fusion of these techniques results in a more automated systematic approach to anchorage measurement. Experimental studies confirm that this novel approach leads to a more robust and improved quality control system for the medical packaging company.		Karol Warne;Girijesh Prasad;Nazmul H. Siddique;Liam P. Maguire	2003		10.1109/ICSMC.2003.1244482	fuzzy logic;computer vision;packaging and labeling;quality control;inspection;image processing;computer science;image segmentation;data acquisition;electron microscope;visual inspection	Robotics	40.44332495512841	-70.70823594122926	115388
c30a78567075ae9432ddac343d730158d6409b8a	improved mumford-shah functional for coupled edge-preserving regularization and image segmentation	signal image and speech processing;mumford shah functional;evaluation performance;image segmentation;performance evaluation;image processing;funcion no lineal;complexite calcul;relacion convergencia;evaluacion prestacion;procesamiento imagen;non linear function;taux convergence;convergence rate;traitement image;algorithme;algorithm;minimizacion costo;complejidad computacion;quantum information technology spintronics;minimisation cout;cost minimization;computational complexity;ecuacion difusion;segmentation image;fonction non lineaire;diffusion equation;equation diffusion;algoritmo	An improved Mumford-Shah functional for coupled edge-preserving regularization and image segmentation is presented. A nonlinear smooth constraint function is introduced that can induce edge-preserving regularization thus also facilitate the coupled image segmentation. The formulation of the functional is considered from the level set perspective, so that explicit boundary contours and edge-preserving regularization are both addressed naturally. To reduce computational cost, a modified additive operator splitting (AOS) algorithm is developed to address diffusion equations defined on irregular domains and multi-initial scheme is used to speed up the convergence rate. Experimental results by our approach are provided and compared with that of MumfordShah functional and other edge-preserving approach, and the results show the effectiveness of the proposed method.		Hongmei Zhang;Mingxi Wan	2006	EURASIP J. Adv. Sig. Proc.	10.1155/ASP/2006/37129	regularization perspectives on support vector machines;diffusion equation;mathematical optimization;image processing;computer science;calculus;mathematics;geometry;image segmentation;rate of convergence;scale-space segmentation;computational complexity theory;algorithm	Vision	53.49312479160506	-71.37691847761995	115555
24085974a6ef93bbde3f2ceffc2362c99b3d5289	fusion of anatomical and functional images using parallel saliency features		Abstract An efficient method is proposed for fusion of anatomical and functional images by constructing the fused image through the combination of parallel saliency features in a multi-scale domain. First, the anatomical and functional images are decomposed into a series of smooth layers and detail layers at different scales by the average filter. Second, the parallel saliency features of both sharp edge and color detail are extracted to obtain the saliency maps. The edge saliency weighted map aims to preserve the high-spatial-resolution structural information using the Canny edge detection operator, while the color saliency weighted map extracts the high-intensity color detail using the context-aware operator. Finally, the fused image is reconstructed by the fused smooth layers and the fused detail layers using saliency maps. We demonstrate the application of the proposed method to a medical problem: Alzheimeru0027s disease. Experimental results show that the proposed method for fusion of MRI-CBV and SPECT-Tc images and fusion of MRI-T1 and PET-FDG images successfully presents the pleasing fused medical images with high-spatial-resolution anatomical structural boundaries and high-intensity color detail.		Jiao Du;Weisheng Li;Bin Xiao	2018	Inf. Sci.	10.1016/j.ins.2017.12.008	machine learning;mathematics;canny edge detector;operator (computer programming);fusion;computer vision;salience (neuroscience);artificial intelligence	AI	47.15799559446606	-70.51068152368123	115712
1f3a0a810da003f825b6c8febd54ecab90877ae4	fast computation of mutual information in a variational image registration approach	fast fourier transform;image registration;mutual information	In this paper we present a novel method for computing the Mutual Information of images using recently developed non-equidistant fast Fourier-transform (NFFT) techniques. Standard approaches suffer from the problem that some sort of quantization is needed in order to apply a fast equidistant FFT-technique. For the new method no quantization is necessary which on one hand leads to an improved registration accuracy and on the other hand to a straight forward implementation. The evaluation is done for MR brain registration as well as for synthetic examples.	computation;fast fourier transform;image registration;mutual information;quantization (signal processing);synthetic intelligence;variational principle	Stefan Heldmann;Oliver Mahnke;Daniel Potts;Jan Modersitzki;Bernd Fischer	2004		10.1007/978-3-642-18536-6_92	computer vision;mathematical optimization;image registration;phase correlation	Vision	49.250936718948246	-75.63727957493401	115966
0accd1c2326a0330ba36252ae8635c98fe9da3da	texture synthesis and compression using gaussian-markov random field models	methode moindre carre;synthese;least squares method;neighbourhood;picture processing;synthesis;quartier voisinage;picture processing encoding markov processes parameter estimation;texture analysis;markov model;compresion;image analysis;mathematical model computational modeling boundary conditions vectors plastics cybernetics complexity theory;periodic boundary conditions parameter estimation texture synthesis texture compression picture processing gaussian markov random field models coding neighboring observations;markov processes;parameter estimation;modele markov;compression;encoding;analyse image;analyse texture	The usefulness is illustrated of two-dimensional (2-D) Gaussian-Markov random field (MRF) models for coding of textures. The MRF models used are noncausal; the mean of observation y(s) at position s is written as a linear weighted sum of neighboring observations surrounding s in all directions. The method of least squares is used to obtain estimates of the model parameters. The model is then used with periodic boundary conditions to regenerate the original texture. Results obtained indicate that this method could be used to compress textures with low bit rates.	least squares;markov chain;markov random field;periodic boundary conditions;texture synthesis;weight function	Rama Chellappa;Sangit Chatterjee;R. Bagdazian	1985	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1985.6313361	neighbourhood;combinatorics;image analysis;computer science;machine learning;mathematics;markov process;markov model;estimation theory;least squares;compression;encoding;statistics	Vision	52.17353338127124	-68.47832195675672	116177
6d32ac948e7f3034c83115eaab634e792ed44e9d	automatic visual defect detection using texture prior and low-rank representation		Automatic surface detection for quality control has largely employed image processing techniques, for example in steel and fabric defect inspection. There are rising demands in the quality control industry for defective image analysis to fulfill its vital role in visual inspection. In this paper, we introduce an unsupervised method using a low-rank representation based on texture prior for detection of defects on natural surfaces and formulate the detection process as a novel weighted low-rank reconstruction model. The first step of the proposed method estimates the texture prior to a given image by constructing a texture prior map where higher values indicate a higher probability of abnormality. The second step of the proposed method detects the defect via low-rank decomposition with the help of the texture prior. Experiments on synthetic and real images show that the proposed method is superior in terms of detection accuracy and competitive in computational efficiency with respect to the state-of-the-art methods in surface defect detection research. This contribution is of particular interest for manufacturers (e.g., steel and fabric) for which defect detection largely relies on manual inspection.	computation;experiment;image analysis;image processing;low-rank approximation;software bug;synthetic intelligence;unsupervised learning;visual inspection	Qizi Huangpeng;Hong Zhang;Xiangrong Zeng;Wenwei Huang	2018	IEEE Access	10.1109/ACCESS.2018.2852663	distributed computing;image processing;visualization;computer science;real image;feature extraction;visual inspection;computer vision;artificial intelligence	Vision	48.283587963207495	-66.53520755673442	116341
5f5d565c48f5b757fe437da4dd8c3548914a5c91	an instability problem of region growing segmentation algorithms and its set median solution	image segmentation;region growing	The region growing paradigm is a well known technique for image segmentation. In the first part of this work, the robustness of region growing algorithms is studied. It is shown that within a small parameter range, which leads to good segmentation results in the majority of cases, bad segmentation results may occur. Furthermore the influence of noise on segmentation results is studied. In fact, instability is a problem of region growing methods and reasons for its occurrence are discussed. In the second part of the work, a solution for this problem based on the set median concept is proposed. The set median is adopted to combine image ensembles and stability is achieved. Experimental results illustrate the performance of our approach.	algorithm;instability;region growing	Lucas Franek;Xiaoyi Jiang	2009		10.1007/978-3-642-10520-3_70	computer vision;mathematical optimization;computer science;machine learning;segmentation-based object categorization;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Vision	44.201517915862865	-71.62025711063599	116533
0b41ac0f0e4525f79d2f6b6afedf9bceca36aa93	particle smoothing in continuous time: a fast approach via density estimation	ecuacion estocastica;forward backward;nuclear magnetic resonance imaging;filtre particule;traitement signal;continuous time;discretisation;equation differentielle;stochastic equation;medical imagery;metodo monte carlo;imagineria rmn;stochastic process;mathematical model biological system modeling approximation methods signal processing algorithms numerical models stochastic processes smoothing methods;estimacion densidad;transition density;methode noyau;approximation method;convolution;smoothing method;stochastic processes approximation theory biomedical mri convolution differential equations particle filtering numerical methods smoothing methods;equation euler;estimation densite;analyse fonctionnelle;sequential monte carlo particle smoothing continuous time density estimation state space model transition density nonlinear model stochastic differential equation kernel density approximation magnetic resonance imaging deconvolution task density estimation;simulacion numerica;state space models continuous time density estimation particle filter sequential monte carlo smoothing;functional mag netic resonance imaging;biological system modeling;discretization;methode monte carlo;differential equation;kernel density;temps continu;discretizacion;non linear model;tiempo continuo;stochastic differential equation;modele non lineaire;filtro particulas;methode runge kutta;equation stochastique;metodo runge kutta;desconvolucion;ecuacion diferencial;approximation theory;functional imaging;density estimation;accuracy;modelo no lineal;kernel density approximation;smoothing methods;precision;stochastic processes;numerical model;state space method;functional analysis;methode espace etat;particle filter;smoothing;signal processing;magnetic resonance imaging;monte carlo method;deconvolution task;metodo nucleo;simulation numerique;ecuacion euler;imagineria medica;imagerie medicale;mathematical model;deconvolution;kernel method;imagerie rmn;approximation methods;differential equations	We consider the particle smoothing problem for state-space models where the transition density is not available in closed form, in particular for continuous-time, nonlinear models expressed via stochastic differential equations (SDEs). Conventional forward-backward and two-filter smoothers for the particle filter require a closed-form transition density, with the linear-Gaussian Euler-Maruyama discretization usually applied to the SDEs to achieve this. We develop a pair of variants using kernel density approximations to relieve the dependence, and in doing so enable use of faster and more accurate discretization schemes such as Runge-Kutta. In addition, the new methods admit arbitrary proposal distributions, providing an avenue to deal with degeneracy issues. Experimental results on a functional magnetic resonance imaging (fMRI) deconvolution task demonstrate comparable accuracy and significantly improved runtime over conventional techniques.	approximation;arcsde;deconvolution;degeneracy (graph theory);discretization;euler;euler–maruyama method;kernel (operating system);markov chain;nonlinear system;open dynamics engine;particle filter;resonance;runge–kutta methods;smoothing;state space	Lawrence Murray;Amos J. Storkey	2011	IEEE Transactions on Signal Processing	10.1109/TSP.2010.2096418	stochastic process;econometrics;mathematical optimization;runge–kutta methods;particle filter;calculus;discretization;mathematics;accuracy and precision;differential equation;statistics	Visualization	52.507052908435085	-72.02615926491785	116716
ef69c312dd4c682a8e00e9b4be7d6b52fa76acfd	oriented relative fuzzy connectedness: theory, algorithms, and its applications in hybrid image segmentation methods	signal image and speech processing;biometrics;interdisciplinar;pattern recognition;image processing and computer vision	Anatomical structures and tissues are often hard to be segmented in medical images due to their poorly defined boundaries, i.e., low contrast in relation to other nearby false boundaries. The specification of the boundary polarity can help alleviate a part of this problem. In this work, we discuss how to incorporate this property in the relative fuzzy connectedness (RFC) framework. We include a theoretical proof of the optimality of the new algorithm, named oriented relative fuzzy connectedness (ORFC), in terms of an oriented energy function subject to the seed constraints, and show its usage to devise powerful hybrid image segmentation methods. The methods are evaluated using medical images of MRI and CT of the human brain and thoracic studies.	algorithm;image segmentation;mathematical optimization;medical imaging	Hans Ccacyahuillca Bejar;Paulo André Vechiatto Miranda	2015	EURASIP J. Image and Video Processing	10.1186/s13640-015-0067-4	computer vision;computer science;theoretical computer science;archaeology;machine learning;pattern recognition;biometrics	Vision	43.9228442537709	-73.5539569221968	116916
e3fd464c73a189b944632aae771b591dc0673efb	edge detection in ventriculograms using support vector machine classifiers and deformable models	shape estimation;support vector machines;edge detection;optimal method;deformable models;left ventricle;anatomical landmarks;support vector machine;energy value;deformable model	In this paper a left ventricle (LV) contour detection method is described. The method works from an approximate contour defined by anatomical landmarks extracted using Support Vector Machine (SVM) classifiers. The LV contour approximation is used as an initialization step for the deformable model algorithm. An optimization method based on a gradient descend algorithm is used to obtain the optimal contour that provides a minimum energy value. Both classifier and edge detection method performances have been validated. The error is determined as the difference between the shape estimated by the algorithm and the shape traced by an expert.	edge detection;support vector machine	Antonio Bravo;Miguel A. Vera;Rubén Medina	2007		10.1007/978-3-540-76725-1_82	support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics	Vision	44.31564188523718	-76.64740289676132	116982
fda0e7e559ed5d717b136c15fb6c4dfbd76aa0f9	stepwise recovery of arc segmentation in complex line environments	document analysis;consistency checking;graphics recognition	Accurate arc segmentation, essential for high-level engineering drawing understanding, is very difficult due to noise, clutter, tangency, and intersections with other geometry objects. We present an application of a generic methodology for recognition of multicomponent graphic objects in engineering drawings to the segmentation of circular arcs. The underlying mechanism is a sequential stepwise recovery of components that are segmented as wire fragments during the sparse-pixel vectorization process and meet a set of continuity conditions. Proper threshold selection and consistent checking of co-circularity of the assumed arc pieces result in an accurate arc segmentation method. Experimental results are presented, evaluated by an objective protocol, and discussed.	arcs (computing);algorithm;automatic vectorization;clutter;engineering drawing;experiment;high- and low-level;pixel;programming paradigm;provider backbone bridge traffic engineering;real life;scott continuity;sparse matrix;stepwise regression	Dov Dori;Wenyin Liu	1998	International Journal on Document Analysis and Recognition	10.1007/s100320050007	computer vision;computer science;machine learning;scale-space segmentation;algorithm	Vision	46.958415768874126	-71.02593517217862	117028
749a1c55aaea18dcdb3e31c9c3e2461281ff3b1f	improving image segmentation quality through effective region merging using a hierarchical social metaheuristic	image segmentation;hierarchical social algorithm;evolutionary metaheuristics;object segmentation;watershed transform;graph partitioning;weighted graph;graph based segmentation;watershed;region merging;normalized cut	This paper proposes a new evolutionary region merging method in order to efficiently improve segmentation quality results. Our approach starts from an oversegmented image, which is obtained by applying a standard morphological watershed transformation on the original image. Next, each resulting region is represented by its centroid. The oversegmented image is described by a simplified undirected weighted graph, where each node represents one region and weighted edges measure the dissimilarity between pairs of regions (adjacent and non-adjacent) according to their intensities, spatial locations and original sizes. Finally, the resulting graph is iteratively partitioned in a hierarchical fashion into two subgraphs, corresponding to the two most significant components of the actual image, until a termination condition is met. This graph-partitioning task is solved by a variant of the min-cut problem (normalized cut) using a Hierarchical Social (HS) metaheuristic. We have efficiently applied the proposed approach to brightness segmentation on different standard test images, with good visual and objective segmentation quality results.	graph (discrete mathematics);graph partition;image segmentation;maxima and minima;metaheuristic;minimum cut;standard test image;watershed (image processing)	Abraham Duarte;Miguel Ángel Sánchez Vidales;Felipe Fernández;Antonio S. Montemayor	2006	Pattern Recognition Letters	10.1016/j.patrec.2005.07.022	mathematical optimization;range segmentation;watershed;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;connected-component labeling	Vision	45.616055761365146	-68.776754678087	117044
e2627f998ac34cd39e74f61d7868d5fb2939ab31	shape prior based on statistical map for active contour segmentation	lts5;active contour;image segmentation;edge detection;statistical analysis edge detection image segmentation medical image processing shape recognition;shape active contours image segmentation biomedical imaging object detection level set principal component analysis signal processing laboratories oncology;training;level set;prior distribution;shape recognition;active contours;prior knowledge;indexing terms;energy function;medical image analysis;shape;statistical analysis;medical image;statistical map;medical image processing;principal component analysis;active contour segmentation;medical image analysis statistical map active contour segmentation prior distribution statistical shape;shape priors;active contour segmentation statistical shape;muscles;statistical shape	We propose a new method for performing active contour segmentation based on the statistical prior knowledge of the object to detect. From a binary training set of objects, a statistical map describes the possible shapes of the object by computing the probability for each point to belong to the object. This statistical map is treated as a prior distribution and an energy functional is defined such that the object reaches the most probable shape knowing the model. The optimization is done in the level-set framework. Results on both synthetic and medical images are shown.	active contour model;contour line;mathematical optimization;synthetic intelligence;test set	Nawal Houhou;Alia Lemkaddem;Valerie Duay;Abdelkarim Allal;Jean-Philippe Thiran	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712247	active shape model;computer vision;active appearance model;prior probability;edge detection;index term;shape;computer science;level set;machine learning;pattern recognition;active contour model;mathematics;image segmentation;statistics;principal component analysis	Vision	43.57940276565289	-76.46219539189771	117236
5d6b89ca9ef2251b3a2eb7def0ac5e57958e311c	automatic centerline extraction of irregular tubular structures using probability volumes from multiphoton imaging	shortest path;3d imaging;level set;eikonal equation	In this paper, we present a general framework for extracting 3D centerlines from volumetric datasets. Unlike the majority of previous approaches, we do not require a prior segmentation of the volume nor we do assume any particular tubular shape. Centerline extraction is performed using a morphology-guided level set model. Our approach consists of: i) learning the structural patterns of a tubular-like object, and ii) estimating the centerline of a tubular object as the path with minimal cost with respect to outward flux in gray level images. Such shortest path is found by solving the Eikonal equation. We compare the performance of our method with existing approaches in synthetic, CT, and multiphoton 3D images, obtaining substantial improvements, especially in the case of irregular tubular objects.	estimated;grayscale;ibm notes;mathematical morphology;nephrogenic systemic fibrosis;short;shortest path problem;structural pattern;synthetic intelligence;united states national institutes of health;biologic segmentation	Alberto Santamaría-Pang;C. M. Colbert;Peter Saggau;Ioannis A. Kakadiaris	2007	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-540-75759-7_59	stereoscopy;computer vision;mathematical optimization;eikonal equation;level set;mathematics;geometry;shortest path problem	Vision	40.5749655500511	-79.25570754439086	117428
7f1e5a51f907e6053a0db54b661db97fbb34c035	efa-bmfm: a multi-criteria framework for the fusion of colour image segmentation		Considering the recent progress in the development of practical applications in the field of image processing, it is increasingly important to develop new, efficient and more reliable algorithms to solve an image segmentation problem. To this end, various fusion-based segmentation approaches which use consensus clustering, and which are based on the optimization of a single criterion, have been proposed. One of the greatest challenges with these approaches is to select the best fusion criterion, which gives the best performance for the image segmentation model. In this paper, we propose a new fusion model of image segmentation based on multi-objective optimization, which aims to overcome the limitation and bias caused by a single criterion, and to provide a final improved segmentation. To address the ill-posedness for the search of the best criterion, the proposed fusion model combines two conflicting and complementary criteria for segmentation fusion, namely, the region-based variation of information (VoI) criterion and the contour-based F-measure (precision-recall) criterion using an entropy-based confidence weighting factor. To optimize our energy-based model, we propose an extended local optimization procedure based on superpixels and derived from the iterative conditional mode (ICM) algorithm. This new multi-objective median partition-based approach, which relies on the fusion of inaccurate, quick and spatial clustering results, has emerged as an appealing alternative to the use of traditional segmentation fusion models which exist in the literature. We perform experiments using the Berkeley database with manual ground truth segmentations, and the results clearly show the feasibility and efficiency of the proposed methodology.	color image;exploratory factor analysis;image segmentation	Lazhar Khelifi;Max Mignotte	2017	Information Fusion	10.1016/j.inffus.2017.03.001	artificial intelligence;machine learning;image processing;consensus clustering;scale-space segmentation;local search (optimization);pattern recognition;cluster analysis;multi-objective optimization;image segmentation;segmentation-based object categorization;computer science	Vision	44.071829454417895	-70.76645156635564	117479
87083f85519a388a7bd1b23ec80db0ed593ba883	three-dimensional echocardiography based evaluation of right ventricular remodeling in patients with pressure overload		Although 3D echocardiography (3DE) allows imaging of right ventricular (RV) morphology, regional RV remodeling has not been evaluated using 3DE. We developed a technique to quantify regional RV shape and tested its ability to characterize RV shape in normal subjects and in patients with RV pressure overload. Transthoracic 3DE images were acquired in 54 subjects (39 patients with pulmonary artery hypertension, PAH, and 15 normal controls, NL). 3D RV surfaces were reconstructed at end-diastole and end-systole (ED, ES) and analyzed using custom software to calculate 3D mean curvature of the inflow and outflow tracts, apex and body (both divided into free-wall and septum). Septal segments in NLs were characterized by concavity (curvature<;O) in ED and slight convexity (curvature>O) in ES. Conversely, the septum remained convex throughout the cardiac cycle in P AH. In the NL group, the body free-wall transitioned from a convex surface to a more flattened surface during contraction, while the convexity of the apex free-wall increased. In contrast, in PAH, both RV free-wall segments remained equally convex throughout the cardiac cycle. Curvature analysis using 3DE allows quantitative evaluation of RV remodeling, which could be used to track diferential changes in regional RV shape, as a way to assess disease progression or regression.	apex (geometry);color gradient;concave function;mathematical morphology;nl (complexity)	Francesco Maffessanti;Karima Addetia;Megan Yamat;Lynn Weinert;Roberto M. Lang;Victor Mor-Avi	2015	2015 Computing in Cardiology Conference (CinC)	10.1109/CIC.2015.7411134	cardiology	HCI	40.853667818780984	-80.20248275760753	117904
5d81a98948cfb6b6a04be131996d254efc8b4376	reconstruction of 3-d binary tree-like structures from three mutually orthogonal projections	metodo cuadrado menor;filtrage morphologique;proyeccion ortogonal;x ray images;filtering;methode moindre carre;mathematical morphology;filtrage;least squares approximations;medical imagery;binarized images;3d coronary arterial structures;image processing;least squares method;x ray imaging;3d binary tree like structures;iterative algorithms;3d imaging;cardiology;filtrado;3d tree structure;three view reconstruction;procesamiento imagen;structure arborescente 3d;least square method;lagrange multiplier;tree data structures;indexing terms;backprojection;blood vessel;traitement image;gray scale;conjugate gradient algorithm;filtering and prediction theory;minimum voxel representation algorithm;noise free conditions;filtering and prediction theory image reconstruction mathematical morphology conjugate gradient methods least squares approximations medical image processing cardiology diagnostic radiography;conjugate gradient;imaging phantoms;optical imaging;reconstruction trois vue;morphological filtering;image reconstruction;medical image processing;tree structure;3d binary representation;imagerie medicale;orthogonal projection;medical diagnostic imaging 3d binary tree like structures mutually orthogonal projections 3d binary representation binarized images backprojection lagrange multiplier algorithm conjugate gradient algorithm minimum voxel representation algorithm noise free conditions 3d coronary arterial structures x ray images human chest phantom 3d imaging blood vessels;humans;human chest phantom;imageneria medical;mutually orthogonal projections;lagrange multiplier algorithm;coronary artery;conjugate gradient methods;lagrangian functions;projection orthogonale;diagnostic radiography;blood vessels;iterative algorithms image reconstruction lagrangian functions tree data structures gray scale x ray imaging humans imaging phantoms optical imaging blood vessels;medical diagnostic imaging;binary tree	A method is developed to generate the 3D binary representation for a tree-like object from three mutually orthogonal projections. This is done by first backprojecting the binarized images from three directions and then iteratively removing artifacts in the backprojection. Three different algorithms have been developed: the Lagrange multiplier algorithm, the conjugate gradient algorithm, and the minimum-voxel representation algorithm (MRA). The performance of these algorithms under noise-free conditions is evaluated using mathematically projected images of a 3D tree structure. While all three algorithms are capable of producing a relatively accurate reconstruction, the MRA is superior not only because it requires the least amount of computation but also because it uses binary instead of gray-scale information in the input images. Reconstruction of 3D coronary arterial structures using MRA is further verified with X-ray images of a human chest phantom and shows a satisfactory performance. The result of this study should be valuable for 3D imaging of blood vessels. >	binary tree	Ying Sun;Iching Liu;John K. Grady	1994	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.276123	computer vision;mathematical optimization;mathematical morphology;image processing;computer science;mathematics;geometry;least squares	Vision	46.03637718580506	-79.7809410362303	118172
17fad0eba373a3e740f160edcc7f1b422c004f39	shock filters for character image enhancement and peeling	image recognition;degradation;electric shock;image processing;filters;image restoration;discontinuous coefficient;image enhancement;shape;indexing;electric shock filters image enhancement image restoration image recognition character recognition image processing degradation shape indexing;hyperbolic partial differential equation;character recognition	In this paper, we propose a generalized shock model for the enhancement and restoration of degraded gray-level character images. This model is a quasi-linear hyperbolic partial differential equation with discontinuous coefficients, based on a recently-published work. The model was extended to peeling character shapes with a uniform and centered width of value K. These processing tools may significantly improve the subsequent stages of the character image recognition task. Both models have been tested on a set of gray-level character images of varying quality from the CEDAR database, and experimental results have proved the efficiency and effectiveness of such modeling.	binary image;circuit restoration;coefficient;computer vision;electronic data processing;image editing;image processing;mesa;particle filter;pixel	Mohamed Cheriet	2003		10.1109/ICDAR.2003.1227857	image restoration;computer vision;search engine indexing;feature detection;speech recognition;degradation;binary image;image processing;shape;hyperbolic partial differential equation;computer science;pattern recognition	Vision	48.70309525423438	-66.8179784181488	118459
38c6a36987cb59ba8e07d4c9bfaf8cb7545cd1ac	topology constraint graph-based model for non-small-cell lung tumor segmentation from pet volumes	dice similarity coefficient topology constraint graph based model non small cell lung tumor segmentation pet volumes graph based segmentation model non small cell lung carcinoma nsclc topological information roi topological skeleton tree model tumor delineation heterogeneous density distribution manual tumor segmentation;topology pet graph based segmentation;tumours cancer image segmentation lung medical image processing positron emission tomography trees mathematics;tumors positron emission tomography lungs image segmentation mathematical model cancer equations	We propose a graph-based segmentation model that takes into account both topological information and intensity similarity to segment non-small-cell lung carcinoma (NSCLC) from PET volumes. The proposed model estimates the probabilities of each voxel belonging to the given foreground and background labels. The topological information is derived from our region of interest topological skeleton tree model (ROI-TKT) to better capture the inclusion and nesting information. Topological information contributes to the separation of tumors from adjacent lesions with similar intensities and the delineation of tumors with heterogeneous density distributions. The experimental evaluation on 20 PET volumes with manual tumor segmentation results from two doctors demonstrated that our method outperformed two other graph-based segmentation methods as measured by the Dice similarity coefficient (DSC).	coefficient;constraint graph;polyethylene terephthalate;region of interest;topological skeleton;voxel	Hui Cui;Xiuying Wang;Jianlong Zhou;Michael J. Fulham;Stefan Eberl;David Dagan Feng	2014	2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2014.6868101	computer vision;mathematics;nuclear medicine;medical physics	Vision	41.18902286656837	-77.61475596355866	118519
ae7bc4ea99dd7a407ef00f596161bbd9067fb9b7	possibilistic fuzzy local information c-means for sonar image segmentation		Side-look synthetic aperture sonar (SAS) can produce very high quality images of the sea-floor. When viewing this imagery, a human observer can often easily identify various sea-floor textures such as sand ripple, hard-packed sand, sea grass and rock. In this paper, we present the Possibilistic Fuzzy Local Information C-Means (PFLICM) approach to segment SAS imagery into sea-floor regions that exhibit these various natural textures. The proposed PFLICM method incorporates fuzzy and possibilistic clustering methods and leverages (local) spatial information to perform soft segmentation. Results are shown on several SAS scenes and compared to alternative segmentation approaches.	algorithm;aperture (software);cluster analysis;display resolution;image segmentation;ripple effect;sas;sonar (symantec);synthetic intelligence	Alina Zare;Nicholas Young;Daniel Suen;Thomas Nabelek;Aquila Galusha;James Keller	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8285358	fuzzy logic;pattern recognition;artificial intelligence;sonar;machine learning;computer science;cluster analysis;scale-space segmentation;segmentation-based object categorization;segmentation;computer vision;synthetic aperture sonar;image segmentation	Vision	43.91681118308272	-71.12681076847402	118536
44b4988af5bc5f7292dbbfe4e3611302b24fd0ac	regularized total least squares approach for nonconvolutional linear inverse problems	metodo cuadrado menor;modelizacion;regularized least squares;methode moindre carre;least squares approximations;medical imagery;restauration image;linear inverse problem;image processing;least squares method;optical tomography;simulation;procesamiento imagen;simulacion;image restoration;indexing terms;problema inverso;traitement image;experimental result;modelisation;restauracion imagen;reconstruction image;conjugate gradient;total least square;linear operator;inverse problem;reconstruccion imagen;image reconstruction;tomographie;imagerie medicale;resultado experimental;imageneria medical;tomografia;resultat experimental;conjugate gradient methods;modeling;optical tomography regularized total least squares nonconvolutional linear inverse problems rtls estimate linear operator rayleigh quotient formulation rq formulation tls problem regularization smooth solution conjugate gradient algorithm perturbation equation;rayleigh quotient;probleme inverse;tomography;least squares methods inverse problems tomography geophysics computing image reconstruction least squares approximation equations biomedical optical imaging image restoration optical noise;conjugate gradient methods inverse problems optical tomography least squares approximations image reconstruction;inverse problems	In this correspondence, a solution is developed for the regularized total least squares (RTLS) estimate in linear inverse problems where the linear operator is nonconvolutional. Our approach is based on a Rayleigh quotient (RQ) formulation of the TLS problem, and we accomplish regularization by modifying the RQ function to enforce a smooth solution. A conjugate gradient algorithm is used to minimize the modified RQ function. As an example, the proposed approach has been applied to the perturbation equation encountered in optical tomography. Simulation results show that this method provides more stable and accurate solutions than the regularized least squares and a previously reported total least squares approach, also based on the RQ formulation.	algorithm;conjugate gradient method;difference quotient;immunostimulating conjugate (antigen);matrix regularization;møller–plesset perturbation theory;rayleigh–ritz method;real-time locating system;simulation;solutions;tomography;tomography, optical;total least squares	Wenwu Zhu;Yao Wang;Nikolas P. Galatsanos;Jun Zhang	1999	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.799895	generalized least squares;total least squares;computer vision;mathematical optimization;computer science;inverse problem;calculus;mathematics;geometry;tomography;non-linear least squares;least squares	Vision	53.02246309140731	-78.71628110083816	118538
c6b9af23ce68480fa06a4ecdd63d549becffc7fd	a discrete region competition approach incorporating weak edge enhancement for ultrasound image segmentation	edge enhancement;early vision;speckle;speckle noise;statistical model;region segmentation;ultrasound imaging;ultrasound image segmentation;article	Ultrasound images are inherently difficult to analyze due to their echo texture, speckle noise and weak edges. Taking into account these characteristics, we present a new region-based approach for ultrasound image segmentation. It is composed of two primary algorithms, discrete region competition and weak edge enhancement. The discrete region competition features four techniques, region competition, statistical modeling of speckle, early vision modeling, and discrete concepts. In addition, to prevent regions from leaking out of the desired area across weak edges, edges located on the slowly varying slope are enhanced according to their position on the slope and the length of the slope. This new approach has been implemented and verified on clinical ultrasound images. 2002 Elsevier Science B.V. All rights reserved.	algorithm;distance transform;edge enhancement;grayscale;image segmentation;statistical model	Chung-Ming Chen;Henry Horng-Shing Lu;Yao-Lin Chen	2003	Pattern Recognition Letters	10.1016/S0167-8655(02)00175-7	speckle pattern;speckle noise;statistical model;computer vision;simulation;computer science;mathematics;edge enhancement;statistics	Vision	45.44652398733379	-72.82730619398706	118583
09d3b338e6497048488f4b2c45b820c73e14c951	using multiscale product for ecg characterization	multiscale product;multi-scale product;r wave corresponds;wavelet coefficients signal;multiscale wavelet analysis;ecg characterization;pointwise product;modulus maximum line;r wave;wavelet coefficient;prototype wavelet;modulus maxima line	This paper introduces a new method for R wave’s locations using the multiscale wavelet analysis, that is based on Mallat’s and Hwang’s approach for singularity detection via local maxima of the wavelet coefficients signals. Using a first derivative Gaussian function as prototype wavelet, we apply the pointwise product of the wavelet coefficients (PWCs) over some successive scales, in order to enhance the peak amplitude of the modulus maxima line and to reduce noise. The R wave corresponds to two modulus maximum lines with opposite signs (min-max) of multi-scale product. The proposed algorithm does not include regularity analysis but only amplitude-based criteria. We evaluated the algorithm on two manually annotated databases, such as MIT-BIH Arrhythmia and QT.	algorithm;bounding interval hierarchy;coefficient;database;maxima and minima;modulus of continuity;prototype;sampling (signal processing);sensitivity and specificity;wavelet	Rym Besrour;Zied Lachiri;Noureddine Ellouze	2009	J. Electrical and Computer Engineering	10.1155/2009/209395	mathematical optimization;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;statistics	DB	51.601791189047546	-66.88415849044624	118592
74e63ced92952abbeca409ba0feff661564a22a0	multiple visual objects segmentation based on adaptive otsu and improved drlse		Aiming at the problem that contours always are fractured and not precise in the segmentation of multiple visual objects, Distance Regularized Level Set Evolution(DRLSE) is used. Considering the characteristics of background difference image, an adaptive Otsu is proposed to segment difference image. In order to take full advantage of temporal information in videos, frame difference and background difference are introduced into energy function of DRLSE. Firstly, an adaptive Otsu and asymmetric morphological filtering based method are used to obtain better initial contours. Secondly, initial contours are evolved with improved DRLSE that frame difference and background difference are integrated in as priori knowledge, which can avoid that objects contours are evolved to background edge, and reduce over segmentation. The experimental results show that the contours of multiple video targets can be obtained more precisely and rapidly with the method proposed in this paper than with the existing methods.	otsu's method;visual objects	Yaochi Zhao;Zhuhua Hu;Yong Bai;Xingzi Liu;Xiyang Liu	2016		10.1007/978-3-319-42297-8_65	otsu's method	Vision	46.87321994559654	-67.67782932179801	118710
616aeed3bc9a1d84e86f379f2fa62d581287d51a	bowstring-based dual-threshold computation method for adaptive canny edge detector	gradient magnitude histogram gmh canny edge detector adaptive dual threshold computation unsupervised hysteresis thresholding;image segmentation;otsu method bowstring based dual threshold computation method adaptive canny edge detector gradient magnitude histogram gmh adaptive acquisition low high threshold unimodal hysteresis thresholding adaptive searching receiver operating characteristics roc curve evaluation method;edge detection;gradient methods;search problems edge detection gradient methods image segmentation;search problems;image edge detection detectors hysteresis histograms testing shape adaptive systems	This paper proposed a novel dual-threshold computation method of Canny edge detector based on gradient magnitude histogram (GMH), targeting with the adaptive acquisition of low-/high-threshold for unimodal hysteresis thresholding. With the introduction of the bowstring concept, which accurately measures the tendency of the GMH on the whole, the dual-threshold computation is implemented by adaptive-searching two tangent points with transitional characteristics. This skillful algorithm of the dual-threshold computation method is further evaluated by using the receiver operating characteristics (ROC) curve evaluation method. The detailed comparison to the Otsu's method is presented and demonstrates the reliability and robust performance of the proposed dual-threshold computation method.	algorithm;canny edge detector;computation;computational complexity theory;edge detection;gradient;hysteresis;otsu's method;parameter (computer programming);receiver operating characteristic;thresholding (image processing)	Xiangdong Liu;Yin Yu;Bing Liu;Zhen Li	2013	2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013)	10.1109/IVCNZ.2013.6726985	computer vision;mathematical optimization;edge detection;image gradient;computer science;deriche edge detector;pattern recognition;balanced histogram thresholding;mathematics;image segmentation;canny edge detector	Robotics	43.28647819650437	-68.18399609659262	118811
9677bedd60d6ea66ba638038618381ac1b7b37b9	fully automatic identification of ac and pc landmarks on brain mri using scene analysis	nuclear magnetic resonance imaging;evaluation performance;medical imagery;brain;metodo paso a paso;step by step method;stereotaxic surgery;performance evaluation;aplicacion medical;systeme nerveux central;analisis estructural;limitation;evaluacion prestacion;biomedical nmr;interhemispheric commissure;pet imaging;hombre;medical diagnostic imaging fully automatic identification pc landmarks ac landmarks brain mri scene analysis midsagittal plane talairach space definition common coordinate systems functional positron emission tomography images neuroanatomy studies grey level histogram interhemispheric plane anatomical structures magnetic resonance imaging;cirugia estereotaxica;imagen nivel gris;commissure interhemispherique;encefalo;indexing terms;analisis automatico;positron emission tomography;imageria rmn;posterior commissure;sistema nervioso central;medical image processing feature extraction biomedical nmr brain;automatic analysis;algorithms automatic data processing brain humans image processing computer assisted magnetic resonance imaging neuroanatomy observer variation pattern recognition automated reproducibility of results skull stereotaxic techniques superior colliculi tomography emission computed;comisura interhemisferica;encephale;limitacion;systeme talairach;feature extraction;medical image processing;chirurgie;chirurgie stereotaxique;image niveau gris;human;methode pas a pas;brain structure;surgery;imagerie medicale;analyse automatique;magnetic resonance imaging image analysis brain positron emission tomography anatomical structure robustness computed tomography neurosurgery skull histograms;cirugia;imagerie rmn;medical application;imageneria medical;analyse structurale;structural analysis;grey level image;central nervous system;coordinate system;homme;scene analysis;application medicale;brain vertebrata	Describes a method for identification of brain structures from MRI data sets. The bulk of the paper concerns an automatic system for finding the anterior and posterior commissures [(AC) and (PC)] in the midsagittal plane. These landmarks are key for the definition of the Talairach space, commonly used in stereotactic neurosurgery, in the definition of common coordinate systems for the pooling of functional positron emission tomography (PET) images and for neuroanatomy studies. The process works according to a step-by-step procedure: it first analyzes the skull limits. A grey-level histogram is then calculated and allows an automated selection of thresholds. Then, the interhemispheric plane is detected. Following an advanced scene analysis in the midsagittal plane for anatomical structures, the AC and the PC are identified. Experimentally, with a set of 200 patients, the process never failed. Its performances and limits are comparable to that of neuroanatomy experts. Those results are due to a high degree of robustness at each step of the program.	anatomic structures;automatic identification and data capture;commissure;experiment;histogram;neuroanatomy;patients;performance;positron-emission tomography;positrons;science of neurosurgery;stereotactic imaging	Libero Verard;P. Allain;J. M. Travere;J. C. Baron;D. Bloyer	1997	IEEE Transactions on Medical Imaging	10.1109/42.640751	computer vision;index term;feature extraction;computer science;central nervous system;coordinate system;mathematics;structural analysis;nuclear medicine	Vision	45.60490995516838	-79.32857777814742	118911
19bc40ef3038de847ef055e4032684913c4a7de5	development of a point-based shape representation of arbitrary three-dimensional medical objects suitable for statistical shape modeling	analisis componente principal;medical imagery;representation tridimensionnelle;analisis forma;simulation;simulacion;anatomia;statistical model;statistical analysis;triangulacion;principal component analysis;computer aid;modele statistique;imagerie medicale;analyse composante principale;asistencia ordenador;three dimensional representation;modelo estadistico;pattern analysis;triangulation;imageneria medical;anatomie;modeling;anatomy;assistance ordinateur;representacion tridimensional;analyse forme	ABSTRACT A novel method that allows the development of surface point based three-dimensional statistical shape models is presented.Fourier decomposition and multiple two-dimensional contours 2,3 have previously been proposed for the development ofstatistical shape models of three-dimensional medical objects. Unlike Fourier decomposition the presented method can beapplied to shapes of arbitrary topology. Furthermore, the method described here results in a true three-dimensional shapemodel, independent, for example, from slice orientations of contour images.Given a set of medical objects, a statistical shape model can be obtained by Principal Component Analysis. This techniquerequires that a set of complex shaped objects is represented as a set of vectors that on the one hand uniquely determine theshapes of the objects and on the other hand are suited for a statistical analysis. The correspondence between the vectorcomponents and the respective shape features has to be the same for all shape parameter vectors to be considered. Wepresent a novel approach to the correspondence problem for complex three-dimensional objects. The underlying idea is todevelop a template shape and to fit this template to all objects to be analyzed. Although we used surface triangulation torepresent the shape the method can easily be adapted to work with other representations. The method is successfully appliedto obtain a statistical shape model for the lumbar vertabrae.Keywords: 3D, statistical, shape model, point distribution model, medical imaging, anatomical objects, triangulation	statistical model	Nils Krahnstoever;Cristian Lorenz	1999		10.1117/12.348618	active shape model;point distribution model;computer vision;shape analysis;mathematics;geometry;engineering drawing	Vision	45.459416834431046	-79.08070341568157	118963
8fa0c1457b444402e9d4c4880be09e51a993271f	pq-space based 2d/3d registration for endoscope tracking	shape from shading;ucl;discovery;theses;conference proceedings;3d registration;electro magnetic;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;pose estimation;ucl research	This paper presents a new pq-space based 2D/3D registration method for camera pose estimation for endoscope tracking. The proposed technique involves the extraction of surface normals for each pixel of the video images by using a linear local shape-from-shading algorithm derived from the unique camera/lighting constrains of the endoscopes. We illustrate how to use the derived pq-space distribution to match to that of the 3D tomographic model, and demonstrate the accuracy of the proposed method by using an electro-magnetic tracker and a specially constructed airway phantom. Comparison to existing intensity-based techniques has also been made, which highlights the major strength of the proposed method in its robustness against illumination and tissue deformation.	algorithm;imaging phantom;normal (geometry);photometric stereo;pixel;shading	Fani Deligianni;Adrian James Chung;Guang-Zhong Yang	2003		10.1007/978-3-540-39899-8_39	computer vision;simulation;pose;photometric stereo;electromagnetism;computer science;computer graphics (images)	Vision	46.345002255984824	-75.40522190919494	119243
134ee2d2fd5db70b4b2fb8b172cf26185ab6b1f1	articulated and generalized gaussian kernel correlation for human pose estimation	kernel;skeleton;pose estimation gaussian processes image representation object tracking;computational modeling;shape;kinect kernel correlation sum of gaussians sog articulated pose estimation human pose tracking hand pose tracking shape modeling depth sensor;kernel correlation shape computational modeling robustness skeleton;robustness;benchmark depth data sets articulated gaussian kernel correlation generalized gaussian kernel correlation human pose estimation unified gkc representation articulated gkc kinematic skeleton multivariate sog template subject specific shape modeling articulated pose estimation sequential pose tracking algorithm visibility regularization term intersection penalty regularization term pose continuity regularization term;correlation	In this paper, we propose an articulated and generalized Gaussian kernel correlation (GKC)-based framework for human pose estimation. We first derive a unified GKC representation that generalizes the previous sum of Gaussians (SoG)-based methods for the similarity measure between a template and an observation both of which are represented by various SoG variants. Then, we develop an articulated GKC (AGKC) by integrating a kinematic skeleton in a multivariate SoG template that supports subject-specific shape modeling and articulated pose estimation for both the full body and the hands. We further propose a sequential (body/hand) pose tracking algorithm by incorporating three regularization terms in the AGKC function, including visibility, intersection penalty, and pose continuity. Our tracking algorithm is simple yet effective and computationally efficient. We evaluate our algorithm on two benchmark depth data sets. The experimental results are promising and competitive when compared with the state-of-the-art algorithms.	3d pose estimation;algorithm;algorithmic efficiency;apricot kernel oil;articulated body pose estimation;benchmark (computing);clamping (graphics);clinical use template;correlation study;cxcl1 protein, mouse;exptime;embedding;gaussian blur;integrated circuit layout design protection;intersection of set of elements;kernel (operating system);kernel density estimation;mbnl1 gene;mathematical optimization;normal statistical distribution;radon;scott continuity;similarity measure;tree structure;algorithm	Meng Ding;Guoliang Fan	2016	IEEE Transactions on Image Processing	10.1109/TIP.2015.2507445	computer vision;mathematical optimization;kernel;3d pose estimation;shape;computer science;pattern recognition;mathematics;computational model;skeleton;correlation;robustness	Vision	47.37707483150341	-76.57359081409388	119425
50b42584503ba785ffa233fbd6977a1c9979f7c2	bayesian region growing and mrf-based minimization for texture and colour segmentation	minimisation;image colour segmentation;graph theory;topology;filtering;pattern clustering;unsupervised feature classification;independent flooding algorithm;unsupervised clustering;image segmentation;filter bank;markov random field model;bayes methods;k means;bayesian methods;image classification;gabor filters;markov random field model image texture image colour segmentation bayesian region growing method unsupervised feature classification block based unsupervised clustering statistical criteria independent flooding algorithm graph cuts algorithm;image texture;bayesian region growing method;optimization problem;graph cuts algorithm;shape;image colour analysis;graph cut;feature extraction;statistical criteria;clustering algorithms;pattern clustering bayes methods feature extraction graph theory image classification image colour analysis image segmentation image texture markov processes minimisation;informatics;markov processes;region growing;bayesian methods image segmentation topology filter bank gabor filters clustering algorithms shape filtering feature extraction informatics;block based unsupervised clustering	We propose a generic, unsupervised feature classification and image segmentation framework, where only the number of classes is assumed as known. Image segmentation is treated as an optimization problem. The framework involves block-based unsupervised clustering using k-means, followed by region growing in spatial domain. High confidence statistical criteria are used to compute a map of initial labelled pixels. A new region growing algorithm is introduced, which is named Independent Flooding Algorithm and computes a height per label for each one of the unlabeled pixels, using Bayesian dissimilarity criteria. Finally, a MRF model is used to incorporate the local pixel interactions of label heights and a graph cuts algorithm performs the final labelling by minimizing the underlying energy. Segmentation results using texture, intensity and color features are presented.	cluster analysis;color;cut (graph theory);flooding algorithm;image segmentation;interaction;k-means clustering;markov random field;mathematical optimization;optimization problem;pixel;region growing;statistical classification;texture mapping;unsupervised learning	Ilias Grinias;Nikos Komodakis;Georgios Tziritas	2007	Eighth International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS '07)	10.1109/WIAMIS.2007.26	filter;image texture;optimization problem;computer vision;minimisation;contextual image classification;cut;feature extraction;bayesian probability;shape;computer science;graph theory;machine learning;segmentation-based object categorization;pattern recognition;filter bank;mathematics;region growing;image segmentation;markov process;cluster analysis;scale-space segmentation;informatics;k-means clustering	Vision	44.792418049811396	-68.11178885135119	119473
b5e5b1e4932f9711f13bf103fa1345ea4d63bd60	active contours driven by local likelihood image fitting energy for image segmentation	image segmentation;local likelihood image fitting energy;variational method;level set method	Accurate image segmentation is an essential step in image analysis and understanding, where active contour models play an important part. Due to the noise, low contrast and intensity inhomogeneity in images, many segmentation algorithms suffer from limited accuracy. This paper presents a novel region-based active contour model for image segmentation by using the variational level set formulation. In this model, we construct the local likelihood image fitting (LLIF) energy functional by describing the neighboring intensities with local Gaussian distributions. The means and variances of local intensities in the LLIF energy functional are spatially varying functions, which can be iteratively estimated during an energy minimization process to guide the contour evolving toward object boundaries. To address diverse image segmentation needs, we also expand this model to the multiphase level set, multi-scale Gaussian kernels and narrowband formulations. The proposed model has been compared with several state-of-the-art active contour models on images with different modalities. Our results indicate that the proposed LLIF model achieves superior performance in image segmentation.	active contour model;computational complexity theory;convex optimization;image segmentation;mathematical optimization;sensor;synthetic data;titcoin;variational principle	Zexuan Ji;Yong Xia;Quan-Sen Sun;Guo Cao;Qiang Chen	2015	Inf. Sci.	10.1016/j.ins.2015.01.006	image texture;computer vision;mathematical optimization;range segmentation;computer science;variational method;segmentation-based object categorization;pattern recognition;active contour model;mathematics;image segmentation;scale-space segmentation;level set method	Vision	50.17861465329807	-71.33298912286911	119720
2081681c130881862b44376b266b5da44aa43da0	automatic threshold determination of centroid-linkage region growing by mpeg-7 dominant color descriptors	histograms;image segmentation;video signal processing;video sequences mpeg 7 descriptors dominant color descriptors automatic threshold determination centroid linkage region growing image segmentation video segmentation color histogram clustering;speech;color histogram;video sequences;video segmentation;color histogram clustering;mpeg 7 standard image segmentation image color analysis histograms clustering algorithms robustness video sequences navigation speech multimedia databases;navigation;automatic threshold determination;mpeg 7 descriptors;dominant color descriptors;image sequences image colour analysis image segmentation video signal processing;image color analysis;image colour analysis;adaptive method;multimedia databases;clustering algorithms;robustness;centroid linkage region growing;dominant color descriptor;mpeg 7 standard;region growing;image sequences	The emerging MPEG-7 standard embodies a visual descriptor that will be associated with the dominant colors of an image. In this contribution, a threshold adaptation method for region based image and video segmentation that takes the advantage of the MPEG7 dominant color descriptor is presented. This method enables assignment of region growing parameters without any low-level processing. In the standard, the dominant colors proposed to be extracted by clustering of color histograms. This property is used to determine color homogeneity that is formulated into Lorentzianbased color distance norm and corresponding thresholds. The proposed algorithm is compared with other region growing algorithms, and results show that the threshold adaptation performs faster and more robust.	algorithm;cluster analysis;color;high- and low-level;linkage (software);mpeg-7;region growing;visual descriptor	Fatih Murat Porikli	2002		10.1109/ICIP.2002.1038144	color histogram;computer vision;navigation;color quantization;color depth;computer science;speech;machine learning;pattern recognition;histogram;mathematics;region growing;color balance;image segmentation;cluster analysis;robustness;computer graphics (images)	Vision	43.81498379209592	-66.61634808676517	119914
324a0b9919914f6520858c6b5308713b6a68b18a	sparse appearance learning based automatic coronary sinus segmentation in cta		Interventional cardiologists are often challenged by a high degree of variability in the coronary venous anatomy during coronary sinus cannulation and left ventricular epicardial lead placement for cardiac resynchronization therapy (CRT), making it important to have a precise and fully-automatic segmentation solution for detecting the coronary sinus. A few approaches have been proposed for automatic segmentation of tubular structures utilizing various vesselness measurements. Although working well on contrasted coronary arteries, these methods fail in segmenting the coronary sinus that has almost no contrast in computed tomography angiography (CTA) data, making it difficult to distinguish from surrounding tissues. In this work we propose a multiscale sparse appearance learning based method for estimating vesselness towards automatically extracting the centerlines. Instead of modeling the subtle discrimination at the low-level intensity, we leverage the flexibility of sparse representation to model the inherent spatial coherence of vessel/background appearance and derive a vesselness measurement. After centerline extraction, the coronary sinus lumen is segmented using a learning based boundary detector and Markov random field (MRF) based optimal surface extraction. Quantitative evaluation on a large cardiac CTA dataset (consisting of 204 3D volumes) demonstrates the superior accuracy of the proposed method in both centerline extraction and lumen segmentation, compared to the state-of-the-art.	anatomic structures;blood vessel tissue;body tissue;ct scan;cardiac resynchronization therapy;cathode ray tube;coherence (physics);computed tomography angiography;coronary artery vasospasm;coronary sinus structure;detectors;estimated;extraction;heart rate variability;high- and low-level;machine learning;markov chain;markov random field;model-driven architecture;nasal sinus;sensor;silo (dataset);sinus - general anatomical term;sparse approximation;sparse matrix;structure of lumen of body system;ventricular remodeling;x-ray computed tomography;angiogram;biologic segmentation	Shiyang Lu;Xiaojie Huang;Zhiyong Wang;Yefeng Zheng	2014	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-319-10404-1_97	computer vision;radiology	Vision	41.89985308506236	-79.47962311910344	119991
751ef25c97502931e13e8982b30b0883cec6b9c5	segmentation of fuzzy images: a novel and fast two-step pseudo map method	map;em;mcmc;fuzzy images;graph cut;normalized graph cut;welding	This paper presents a new two-step pseudo maximum a posteriori (MAP) segmentation method for the Markov random field (MRF)-modeled image because the exact MAP estimation is hard to implement due to intractable complexity. The expectation maximization (EM) and Markov Chain Monte Carlo (MCMC) methods are adopted to estimate the parameters for the MRF model due to their comparatively good performance. Although the image segmentation algorithms via graph cuts have become very popular nowadays, our proposed algorithm still performs significantly better in automatic identification and segmentation of fuzzy images than them, which is shown by the quantitative results on synthesized images. In practical applications, the proposed two-step pseudo MAP method is superior in segmenting the fuzzy laser images reflected from the weld pool surfaces during the P-GMAW welding process.	automatic identification and data capture;computational complexity theory;cut (graph theory);expectation–maximization algorithm;image segmentation;markov chain monte carlo;markov random field;monte carlo method	Zhen Zhou Wang;Yuming Zhang	2012	Machine Vision and Applications	10.1007/s00138-012-0441-5	computer vision;mathematical optimization;machine learning;mathematics;image segmentation;scale-space segmentation	Vision	49.593226285073946	-69.0707826720733	120265
516e6585d1942231b083ec8c1be8bfd74fe7a310	optimal graph search based segmentation of airway tree double surfaces across bifurcations	image sampling;female;image segmentation;phantoms;bifurcation;middle aged;adult aged algorithms bronchi female humans image processing computer assisted male middle aged phantoms imaging reproducibility of results tomography x ray computed;male;bronchi;image processing computer assisted;graphs;lung;phantoms bifurcation computerised tomography diagnostic radiography diseases graphs image sampling image segmentation lung medical image processing;multiple surfaces airway trees bifurcations complex topology graph search image segmentation;medical image processing;adult;image segmentation bifurcation topology computed tomography search problems lungs surface treatment;reproducibility of results;phantoms imaging;computerised tomography;diseases;carina noncarina regions optimal graph search based segmentation airway tree double surfaces bifurcations bronchial tree structure volumetric x ray computed tomography data sets phenotypes lung diseases chronic obstructive pulmonary diseases asthma logismos input 3d image segmentation tree topology image sampling voxel columns computerised tomography scanned double wall physical phantom mean signed error;algorithms;humans;diagnostic radiography;aged;tomography x ray computed	Identification of both the luminal and the wall areas of the bronchial tree structure from volumetric X-ray computed tomography (CT) data sets is of critical importance in distinguishing important phenotypes within numerous major lung diseases including chronic obstructive pulmonary diseases (COPD) and asthma. However, accurate assessment of the inner and outer airway wall surfaces of a complete 3-D tree structure is difficult due to their complex nature, particularly around the branch areas. In this paper, we extend a graph search based technique (LOGISMOS) to simultaneously identify multiple inter-related surfaces of branching airway trees. We first perform a presegmentation of the input 3-D image to obtain basic information about the tree topology. The presegmented image is resampled along judiciously determined paths to produce a set of vectors of voxels (called voxel columns). The resampling process utilizes medial axes to ensure that voxel columns of appropriate lengths and directions are used to capture the object surfaces without interference. A geometric graph is constructed whose edges connect voxels in the resampled voxel columns and enforce validity of the smoothness and separation constraints on the sought surfaces. Cost functions with directional information are employed to distinguish inner and outer walls. The assessment of wall thickness measurement on a CT-scanned double-wall physical phantom (patterned after an in vivo imaged human airway tree) achieved highly accurate results on the entire 3-D tree. The observed mean signed error of wall thickness ranged from -0.09 ±0.24 mm to 0.07 ±0.23 mm in bifurcating/nonbifurcating areas. The mean unsigned errors were 0.16±0.12 mm to 0.20±0.11 mm. When the airway wall surface was partitioned into meaningful subregions, the airway wall thickness accuracy was the same in most tested bifurcation/nonbifurcation and carina/noncarina regions (p=NS). Once validated on phantoms, our method was applied to human in vivo volumetric CT data to demonstrate relationships of airway wall thickness as a function of luminal dimension and airway tree generation. Wall thickness differences between the bifurcation/nonbifurcation regions were statistically significant (p <; 0.05) for tree generations 6, 7, 8, and 9. In carina/noncarina regions, the wall thickness was statistically different in generations 1, 4, 5, 6, 7, and 8.	anatomic bifurcation;anatomy, regional;bifurcation theory;bronchial tree;ct pulmonary angiogram;chronic obstructive airway disease;column (database);diagnostic radiologic examination;geometric graph theory;graph - visual representation;graph traversal;imaging phantom;interference (communication);lung diseases, obstructive;lung diseases;mathematical morphology;maxima and minima;medial graph;phantoms, imaging;phenobarbital;physical object;quantitation;scanning;segmentation action;signature;structure of carina;thickness (graph theory);tomography, spiral computed;tree network;tree structure;trees (plant);video-in video-out;voxel;walls of a building;x-ray computed tomography	Xiaomin Liu;Danny Ziyi Chen;Merryn H. Tawhai;Xiaodong Wu;Eric A. Hoffman;Milan Sonka	2013	IEEE Transactions on Medical Imaging	10.1109/TMI.2012.2223760	computer vision;pathology;computer science;mathematics;geometry;image segmentation;graph;algorithm	Visualization	41.059745499268075	-78.79668339152478	120296
6805f63a95885a59979a0e44c168e00d6ab37fc5	graph-based foreground extraction in extended color space	signal processing algorithms image segmentation pixel robustness brightness color video sequences tree graphs extraterrestrial measurements current measurement;image segmentation;distance measure;video signal processing;color space;minimum spanning tree graph based foreground extraction extended color space semantic foreground region extraction color video sequence static backgrounds distance measure background subtraction graph based region segmentation method;trees mathematics;minimum spanning tree background subtraction graph algorithm;region segmentation;distance measurement;image edge detection;image color analysis;image colour analysis;feature extraction;pixel;video signal processing distance measurement feature extraction image colour analysis image segmentation image sequences trees mathematics;background subtraction;classification algorithms;minimum spanning tree;high definition video;graph algorithm;cameras;image sequences	We propose a region-based method to extract semantic foreground regions from color video sequences with static backgrounds. First, we introduce a new distance measure for background subtraction which is robust against shadows. Then the foreground region is extracted with a graph-based region segmentation method considering background difference and spatial homogeneity. For efficient computation, the graph structure is optimized by the minimum spanning tree before segmentation. The main contribution is that the proposed algorithm improves on conventional approaches especially in strong shadow regions and does not require manual initialization. We have verified through experiments and comparison to state of the art methods that the proposed algorithm works well with various cameras and environment.	algorithm;background subtraction;color space;computation;experiment;file spanning;minimum spanning tree	Hansung Kim;Adrian Hilton	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414370	statistical classification;computer vision;background subtraction;feature extraction;computer science;minimum spanning tree;machine learning;pattern recognition;mathematics;image segmentation;color space;pixel	Vision	44.19994205112645	-66.95781364264151	120302
2e55da8dd99419c07f878e94430459c4070f86cd	singularities of principal direction fields from 3-d images	gaussian processes;noise robustness;gaussian processes feature extraction noise robustness standards development tiles;standards development;feature extraction;tiles	Generic singularities can provide position-independent information about the qualitative shape of surfaces. The authors determine the singularities of the principal direction fields of a surface (its umbilic points) from a computation of the index of the fields. The authors present examples both for 3-D synthetic images to which noise has been added and for clinical magnetic resonance images		Peter T. Sander;Steven W. Zucker	1988		10.1109/CCV.1988.590048	computer vision;feature extraction;computer science;machine learning;pattern recognition;gaussian process;mathematics;statistics	Vision	46.22982029598853	-75.65999476054503	120317
ad70193e33b56ee0e20a2710b31c74295286e5c6	segmentation of echocardiographic data. multiresolution 2d and 3d algorithm based on grey level statistics	bayesian framework;region segmentation	  We propose in this paper a robust adaptive region segmentation algorithm of dirty images in a Bayesian framework. A multiresolution  implementation of the algorithm is performed using a wavelets basis. The algorithm can process both 2D and 3D data. In this  work we focus on the adaptive character of the algorithm and we discuss how global and local statistics can be taken into  account in the segmentation process. Results of segmentation performed on echocardiographic sequences (2D+T) and an evaluation  of the performance of the proposed algorithm are presented.    		Djamal Boukerroui;Olivier Basset;Atilla Baskurt;J. Alison Noble	1999		10.1007/10704282_56	computer vision;computer science;segmentation-based object categorization;pattern recognition;data mining;image segmentation;scale-space segmentation	Visualization	42.94849146719999	-72.25457619797378	120467
6d97994f33c3ff8e4c1bb45bac34b57dcc131999	automatic and robust single-camera specular highlight removal in cardiac images	image color analysis heart image edge detection wavelet transforms surgery tracking;surgery biomedical optical imaging edge detection image denoising medical image processing speckle;medical imaging;adaptive windowing automatic specular highlight removal single camera specular highlight removal cardiac images computer assisted beating heart surgeries heart motion specular highlight detection hybrid color attributes wavelet based edge projection dynamic search based inpainting;cardiac images;conference lecture	In computer-assisted beating heart surgeries, accurate tracking of the heart's motion is of huge importance and there is a continuous need to eliminate any source of error that might disturb the tracking process. One source of error is the specular reflection that appears on the glossy surface of the heart. In this paper, we propose a robust solution for the detection and removal of specular highlights. A hybrid color attributes and wavelet based edge projection approach is applied to accurately identify the affected regions. These regions are then recovered using a dynamic search-based inpainting with adaptive windowing. Experimental results demonstrate the precision and efficiency of the proposed method. Moreover, it has a real-time performance and can be generalized to various other applications.	choice behavior;detectors;edge detection;emoticon;excretory function;experiment;heart failure;inpainting;numerous;real-time clock;sensor;specular highlight;time complexity;wavelet analysis;wavelet transform	Samar M. Alsaleh;Angelica I. Avilés;Pilar Sobrevilla;Alicia Casals;James K. Hahn	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7318452	medical imaging;computer vision;radiology;medicine;engineering;optics;computer graphics (images)	Vision	39.75199500614091	-75.99388792611822	120675
1af8e14f8e4715012bee262fa4d7222bbea5164b	integrated graph cuts for brain mri segmentation	iterated conditional mode;optimal method;information integration;graph cut;medical image segmentation	Brain MRI segmentation remains a challenging problem in spite of numerous existing techniques. To overcome the inherent difficulties associated with this segmentation problem, we present a new method of information integration in a graph based framework. In addition to image intensity, tissue priors and local boundary information are integrated into the edge weight metrics in the graph. Furthermore, inhomogeneity correction is incorporated by adaptively adjusting the edge weights according to the intermediate inhomogeneity estimation. In the validation experiments of simulated brain MRIs, the proposed method outperformed a segmentation method based on iterated conditional modes (ICM), which is a commonly used optimization method in medical image segmentation. In the experiments of real neonatal brain MRIs, the results of the proposed method have good overlap with the manual segmentations by human experts.	brain simulation;cut (graph theory);experiment;graph - visual representation;image segmentation;iterated conditional modes;iteration;mathematical optimization;medical image;weight;biologic segmentation	Zhuang Song;Nicholas J. Tustison;Brian B. Avants;James C. Gee	2006	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/11866763_102	computer vision;cut;computer science;information integration;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	43.94277598252141	-73.46298156300112	120983
618118d23d0a4254f64d6fd789986f18c886f05b	multidimensional co-segmentation of longitudinal brain mri ensembles in the presence of a neurodegenerative process	brain atrophies;level sets method;longitudinal data;mouse brain mri;multi-region co-segmentation;neurodegenerative conditions	MRI Segmentation of a pathological brain poses a significant challenge, as the available anatomical priors that provide top-down information to aid segmentation are inadequate in the presence of abnormalities. This problem is further complicated for longitudinal data capturing impaired brain development or neurodegenerative conditions, since the dynamic of brain atrophies has to be considered as well. For these cases, the absence of compatible annotated training examples renders the commonly used multi-atlas or machine-learning approaches impractical. We present a novel segmentation approach that accounts for the lack of labeled data via multi-region multi-subject co-segmentation (MMCoSeg) of longitudinal MRI sequences. The underlying, unknown anatomy is learned throughout an iterative process, in which the segmentation of a region is supported both by the segmentation of the neighboring regions, which share common boundaries, and by the segmentation of corresponding regions, in the other jointly segmented images. A 4D multi-region atlas that models the spatio-temporal deformations and can be adapted to different subjects undergoing similar degeneration processes is reconstructed concurrently. An inducible mouse model of p25 accumulation (the CK-p25 mouse) that displays key pathological hallmarks of Alzheimer disease (AD) is used as a gold-standard to test the proposed algorithm by providing a conditional control of rapid neurodegeneration. Applying the MMCoSeg to a cohort of CK-p25 mice and littermate controls yields promising segmentation results that demonstrate high compatibility with expertise manual annotations. An extensive comparative analysis with respect to current well-established, atlas-based segmentation methods highlights the advantage of the proposed approach, which provides accurate segmentation of longitudinal brain MRIs in pathological conditions, where only very few annotated examples are available.	alzheimer's disease;anatomic structures;atlases;atrophic;automatic identification and data capture;cervical atlas;congenital abnormality;cross-sectional data;database;ethinyl estradiol;gentamicins;ground truth;immunoprecipitation;inference;iteration;machine learning;medical device incompatibility problem;musculoskeletal diseases;numerous;outlines (document);pro tools;project 25;projection-slice theorem;qualitative comparative analysis;rendering (computer graphics);sampling - surgical action;silo (dataset);status epilepticus;super paper mario;texture atlas;top-down and bottom-up design;tracer;tree accumulation;unified framework;acre;algorithm;biologic segmentation;brain development;nervous system disorder	Shiri Gordon;Irit Dolgopyat;Itamar Kahn;Tammy Riklin-Raviv	2018	NeuroImage	10.1016/j.neuroimage.2018.04.039	labeled data;neurodegeneration;cognitive psychology;alzheimer's disease;segmentation;artificial intelligence;psychology;pattern recognition	Vision	42.24525224358612	-79.43537808751181	121004
aa3b262ce587afb5fb4d6a736ee7aae29d29ca91	geometric variability of the scoliotic spine using statistics on articulated shape models	orthopedic surgery;frechet mean;spine shape variability;rigid transforms;statistics shape solid modeling geometry deformable models spine orthopedic surgery hospitals diagnostic radiography statistical analysis;covariance analysis;spine;rigid transformation vector;geometry;vector space;hospitals;biomechanics;anatomical variability;covariance;deformable models;statistical shape analysis;indexing terms;orthopaedics;transforms biomechanics bone covariance analysis deformation diseases neurophysiology orthopaedics patient treatment physiological models;shape deformation;lumbar vertebrae;articulated shape models;statistical shape analysis anatomical variability orthopaedic treatment radiograph rigid transformations scoliosis spine;shape;statistical analysis;cotrel dubousset corrective surgery;scoliotic spine geometric variability;deformation;boston brace patients;solid modeling;intervertebral poses;transforms;bone;statistics;diseases;patient treatment;spine shape deformations;shape modeling;neurophysiology;rigid transformations;orthopaedic treatment;radiograph;scoliosis;statistical shape analysis scoliotic spine geometric variability articulated shape models spine shape variability spine shape deformations rigid transformation vector frechet mean covariance cotrel dubousset corrective surgery boston brace patients intervertebral poses lumbar vertebrae statistics orthopaedic treatment radiograph;physiological models;algorithms computer simulation humans image interpretation computer assisted models anatomic models biological reproducibility of results scoliosis sensitivity and specificity spine;diagnostic radiography;coordinate system	This paper introduces a method to analyze the variability of the spine shape and of the spine shape deformations using articulated shape models. The spine shape was expressed as a vector of relative poses between local coordinate systems of neighboring vertebrae. Spine shape deformations were then modeled by a vector of rigid transformations that transforms one spine shape into another. Because rigid transformations do not naturally belong to a vector space, conventional mean and covariance could not be applied. The Frechet mean and a generalized covariance were used instead. The spine shapes of a group of 295 scoliotic patients were quantitatively analyzed as well as the spine shape deformations associated with the Cotrel-Dubousset corrective surgery (33 patients), the Boston brace (39 patients), and the scoliosis progression without treatment (26 patients). The variability of intervertebral poses was found to be inhomogeneous (lumbar vertebrae were more variable than the thoracic ones) and anisotropic (with maximal rotational variability around the coronal axis and maximal translational variability along the axial direction). Finally, brace and surgery were found to have a significant effect on the Frechet mean and on the generalized covariance in specific spine regions where treatments modified the spine shape.	appendix;axis vertebra;bone structure of lumbar vertebra;bone structure of spine;braces-orthopedic appliances;calculus of variations;chest;classification;color gradient;congenital abnormality;dendritic spine;deploy;epr paradox;genetic translation process;hl7publishingsubsection <operations>;heart rate variability;instrumentation (attribute);knee;local coordinates;map;maximal set;motion;muscle rigidity;musculoskeletal diseases;optic axis of a crystal;patients;scoliosis, unspecified;shape context;smoothing;software deployment;spatial variability;time complexity;unfolding (dsp implementation);cell transformation;exponential;manifold	Jonathan Boisvert;Farida Cheriet;Xavier Pennec;Hubert Labelle;Nicholas Ayache	2008	IEEE Transactions on Medical Imaging	10.1109/TMI.2007.911474	radiography;rigid transformation;index term;spine;analysis of covariance;vector space;orthopedic surgery;shape;covariance;coordinate system;mathematics;solid modeling;deformation;surgery;statistics	Vision	43.88510810304684	-79.16373221802078	121072
1a94063db10f4f5a80e42b57b599a8fbca9b2a31	fast trust region for segmentation	optimisation;bhattacharyya distance;convergence;optimisation computer vision convergence image segmentation iterative methods;image segmentation;standards;shape prior constraint;high order energies;linear approximation;general iterative approach;segmentation;segment size;computational modeling linear approximation image segmentation standards optimization shape;computer vision;kl divergence;iterative methods;gradient descent technique;fast trust region approach;trust region;computational modeling;shape;trust region segmentation high order energies optimization;segmentation energy optimization;target shape moments;ftr approach;optimization;nonlinear approximation model;appearance distribution;volume constraint;convergence general iterative approach gradient descent technique nonlinear approximation model constrained optimization algorithm fast trust region approach ftr approach segmentation energy optimization kl divergence bhattacharyya distance appearance distribution volume constraint segment size shape prior constraint target shape moments;constrained optimization algorithm	Trust region is a well-known general iterative approach to optimization which offers many advantages over standard gradient descent techniques. In particular, it allows more accurate nonlinear approximation models. In each iteration this approach computes a global optimum of a suitable approximation model within a fixed radius around the current solution, a.k.a. trust region. In general, this approach can be used only when some efficient constrained optimization algorithm is available for the selected non-linear (more accurate) approximation model. In this paper we propose a Fast Trust Region (FTR) approach for optimization of segmentation energies with non-linear regional terms, which are known to be challenging for existing algorithms. These energies include, but are not limited to, KL divergence and Bhattacharyya distance between the observed and the target appearance distributions, volume constraint on segment size, and shape prior constraint in a form of L2 distance from target shape moments. Our method is 1-2 orders of magnitude faster than the existing state-of-the-art methods while converging to comparable or better solutions.	algorithm;approximation;constrained optimization;extension (mac os);fitness approximation;global optimization;gradient descent;iteration;kullback–leibler divergence;lagrange multiplier;levenberg–marquardt algorithm;line search;mathematical optimization;maxima and minima;nonlinear system;trust region	Lena Gorelick;Frank R. Schmidt;Yuri Boykov	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.224	computer vision;bhattacharyya distance;mathematical optimization;convergence;shape;computer science;machine learning;pattern recognition;mathematics;iterative method;trust region;image segmentation;kullback–leibler divergence;computational model;segmentation;linear approximation	Vision	50.550119320546166	-72.09037522787115	121197
dc1477ce9e079da7e04fcd541845307c9388da1d	multi-scale and non local mean based filter for positron emission tomography imaging denoising	animals;positron emission tomography filtering algorithms signal to noise ratio wavelet transforms animals noise reduction;tissue tracer concentrations multiscale nonlocal mean based filter positron emission tomography imaging denoising dynamic positron emission tomography dynamic pet functional imaging modality tracer kinetic 18f fluorodeoxyglucose tracer 18f fdg tracer local myocardium metabolic rate glucose parameter estimation nonlocal mean algorithm discrete curvelet transform;positron emission tomography;wavelet transforms;filtering algorithms;noise reduction;sugar biological tissues curvelet transforms image denoising image filtering medical image processing parameter estimation positron emission tomography;signal to noise ratio;wavelet pet denoising nlm curvelet	Dynamic Positron Emission Tomography (PET) is a functional imaging modality which provides information about tracer kinetic in a specific target. In the last three decades, the [18F]-fluorodeoxyglucose ([18F]-FDG) tracer has been widely used by many institutions to measure the local myocardium metabolic rate for glucose. The analysis of the dynamic measurements requires, often, parameters estimation in which the PET data is noisy. In this paper, we propose a systematic methodology to reduce noise in PET data based on the combination of an extension of Non Local Means algorithm and the Discrete Curvelet Transform. The methodology was applied to a small animal model study of the heart, where both the input function and the tissue tracer concentrations at each time were derived from de-noised images. Experimental results revealed a significant improvement in SNR and the spatial distribution of the tracer.	algorithm;ct scan;curvelet;estimation theory;functional imaging;noise reduction;polyethylene terephthalate;signal-to-noise ratio;tomography	Hajer Jomaa;Rostom Mabrouk;Frédéric Morain-Nicolier;Nawres Khlifa	2016	2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2016.7523063	computer vision;mathematics;nuclear medicine;medical physics	Robotics	50.18925153380839	-77.65244813296005	121200
bc755dfca9431a98f6c15546ee453f8c2b10258e	fast localised active contour for inhomogeneous image segmentation	modified speed function;modified speed function inhomogeneous image segmentation localised active contour framework computational complexity speed function;inhomogeneous image segmentation;computational complexity;speed function;image segmentation computational complexity;localised active contour framework	The localised active contour framework has been widely used for image segmentation because it provides reliable results for inhomogeneous images. However, its computational complexity remains an issue. In this study, the authors introduce a fast algorithm based on the localised active contour framework. A key concept of the proposed algorithm is its consideration of the curve evolution based on the speed function only at active points that change across time, rather than at all points in a narrow band. This approach reduces computational time in the localised active contour. The authors additionally propose a modified speed function to address inhomogeneous image segmentation. The experimental results demonstrate significant advantages of the proposed method over existing methods, both in terms of computational efficiency and segmentation accuracy, for homogeneous and inhomogeneous images.	active contour model;image segmentation	Toan Duc Bui;Chunsoo Ahn;Jitae Shin	2016	IET Image Processing	10.1049/iet-ipr.2015.0489	computer vision;mathematical optimization;computer science;machine learning;segmentation-based object categorization;mathematics;scale-space segmentation;computational complexity theory;algorithm	Vision	48.697900426559954	-71.09973137203606	121421
faea6899d0be52b3bd3a423e3c102290090cef8d	multicue pde-based segmentation of ultrasound carotid artery using flooding technique	partial differential equation;flooding;segmentation;pde;morphological filtering;feature extraction;watershed	One of the successful applications of image processing techniques is in the area of medical imaging. There exists no general segmentation algorithm in image processing which can work efficiently for all images. A technique for the segmentation of an ultrasound carotid artery is presented in this paper. This paper solves the segmentation problem of carotid artery by attempting to incorporate cues such as intensity contrast, region size and texture in the segmentation procedure and derive improved results compared to using individual cues separately. It proposes efficient simplification operators and feature extraction schemes, capable of quantifying important characteristics like geometrical complexity, rate of change in local contrast variations and orientation. This helps the final segmentation result. The main technique used in this project is the modelling and partial differential equation (PDE) formulation of watershed transform in order to satisfy various flooding criteria.		N. Santhiyakumari;M. Madheswaran	2011	IJMEI	10.1504/IJMEI.2011.042871	computer vision;mathematical optimization;mathematical morphology;watershed;feature extraction;computer science;flooding;machine learning;segmentation-based object categorization;image segmentation;scale-space segmentation;segmentation;partial differential equation	Vision	45.288336647072256	-73.0669186703746	121454
02661621f0d660c5abffa5c83ca18194c7dc405a	nonrigid point matching of chinese characters for robot writing		Point matching is a key step in robot writing-learning process. There are three major challenges that weakened most of the existing point matching algorithms to match points for Chinese characters, including nonlinearly deformation, connected strokes and geometrically dispersive. We propose a novel algorithm based on constrained global energy function (CGE) in the matching process to cope with the abovementioned challenges in this paper. We utilized a global spatial distribution energy function (EF) to evaluate relationship among point sets. Then we could solve problems with point registration by minimizing the energy function. To evaluate the matching results of energy function, we defined a vector that describe local spatial information to optimize the algorithm presentation. In addition, to avoid divergence, we designed an operator based on sigmoid function, using literation numbers as inputs to constrain the vector. We have conducted experiments on an extensive human handwriting database, and our algorithm performed competitively against the state-of-the-art point matching algorithm in terms of accuracy and stability.	advanced power management;algorithm;dispersive partial differential equation;entity framework;experiment;graphics;mathematical optimization;nonlinear system;rate of convergence;robot;sigmoid function	Bocheng Zhao;Minghao Yang;Hang Pan;Qingjie Zhu;Jianhua Tao	2017	2017 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2017.8324509	spatial distribution;control theory;point set registration;divergence;operator (computer programming);deformation (mechanics);feature extraction;engineering;mathematical optimization;sigmoid function;spatial analysis	Robotics	48.576949300911956	-71.9070150598081	121573
b88c409a08f2059c5d45a5d964d2327f1c6235d9	shape-based tracking of left ventricular wall motion	pared;nuclear magnetic resonance imaging;analisis imagen;paroi;animals;imaging modalities;wall;magnetic resonance tagging;algorithms animals dogs echocardiography heart humans image processing computer assisted magnetic resonance imaging models cardiovascular myocardial contraction ventricular function left;medical imagery;informatica biomedical;diagnostic imaging;biomedical data processing;image motion analysis;movimiento;heart;global minimum;image segmentation;computed tomography;x ray imaging;linear filter;initial boundary segmentation;shape based strategy;genie biomedical;cardiac imaging;dogs;angiocardiography;cardiology;analisis cuantitativo;appareil circulatoire pathologie;biomedical nmr;informatique biomedicale;tecnica medida;echography;two dimensional displays;wall motion;left ventricular;optimization functional;gold standard;indexing terms;motion;motion computation;image processing computer assisted;filtro lineal;left ventricle;three dimensional;linear filtering;local segment matching;point by point basis;trajectory estimates;algorithme;imageria rmn;algorithm;aparato circulatorio patologia;gold standard markers;composite flow field;temporal sequence;smoothing methods;optical imaging;contrast x ray angiogram images;tracking magnetic resonance imaging image sequences image segmentation image motion analysis computed tomography heart two dimensional displays optical imaging x ray imaging;three dimensional problem;biomedical engineering;left ventricular endocardial wall;filtre lineaire;ventricule gauche;analyse quantitative;exploration radioisotopique;cardiovascular disease;flow field;medical image processing;magnetic resonance imaging;mouvement;computer aid;imagerie medicale;smoothing methods medical image processing image sequences image segmentation angiocardiography biomedical nmr cardiology;technique mesure;radionuclide study;smoothness term;echocardiography;quantitative analysis;nonrigid nonuniform motion;myocardial contraction;shape based tracking	An approach for tracking and quantifying the nonrigid, nonuniform motion of the left ventricular (LV) endocardial wall from two-dimensional (2-D) cardiac image sequences, on a point-by-point basis over the entire cardiac cycle, is presented. Given a set of boundaries, motion computation involves first matching local segments on one contour to segments on the next contour in the sequence using a shape-based strategy. Results from the match process are incorporated with a smoothness term into an optimization functional. The global minimum of this functional is found, resulting in a smooth flow field that is consistent with the match data. The computation is performed for all pairs of frames in the temporal sequence and equally sampled points on one contour are tracked throughout the sequence, resulting in a composite flow field over the entire sequence. Two perspectives on characterizing the optimization functional are presented which result in a tradeoff resolved by the confidence in the initial boundary segmentation. Experimental results for contours derived from diagnostic image sequences of three different imaging modalities are presented. A comparison of trajectory estimates with trajectories of gold-standard markers implanted in the LV wall are presented for validation. The results of this comparison confirm that although cardiac motion is a three-dimensional (3-D) problem, two-dimensional (2-D) analysis provides a rich testing ground for algorithm development.	3d computer graphics;computation (action);contour line;endocardium;estimated;frame (physical object);implants;logical volume management;matching;mathematical optimization;maxima and minima;sampling - surgical action;wall of left ventricle;wall of ventricle;algorithm;biologic segmentation	John C. McEachen;James S. Duncan	1997	IEEE Transactions on Medical Imaging	10.1109/42.585761	computer vision;image analysis;radiology;medicine;computer science;magnetic resonance imaging;linear filter;mathematics;geometry;nuclear medicine	Vision	45.10186574462963	-79.56719591187344	121637
35e3f0d7998936eb22d58d950e8c111d7e101926	an image segmentation method for maize disease based on iga-pcnn	image segmentation;maize melanoma powder disease;genetic algorithm;pulse coupled neural network	The image segmentation of plant diseases is one of the critical technical aspects of digital image processing technology for Disease Recognition. This paper proposes an improved pulse coupled neural network based on an improved genetic algorithm. An objective evaluation function is defined based on linear weighted function with maximum Shannon entropy and minimum cross-entropy. Through adaptive adjustment of crossover probability and mutation probability, we optimized the parameters of pulse coupled neural network based on the improve genetic algorithm. The improved network is used to segment the color images of Maize melanoma powder disease in RGB color subspaces. Then combined with the results by color image merger strategy, we can get the terminal results of target area. The experimental results show that this method could segment the disease regions better and set complexity parameters simplier.	image segmentation;in-game advertising;pulse-coupled networks	Wen Changji;Yu Helong	2013		10.1007/978-3-642-39065-4_72	computer vision;genetic algorithm;computer science;artificial intelligence;machine learning;image segmentation	Vision	41.990177986676855	-67.98808434062181	121844
3020d3e574cd7f9fdff7eeb89f72af3f4407ff32	signal and image approximation using interval wavelet transform	transformation ondelette;wavelet transforms approximation theory data compression filtering theory image coding;traitement signal;filtering;filtrage;image coding;image processing;data compression;ondelette;signal analysis;filtrado;procesamiento imagen;analisis de senal;indexing terms;wavelets on the interval boundary filters image approximation;traitement image;interval wavelet transform;boundary filters;wavelet transforms;approximation theory;wavelet synthesis;codage image;wavelet transforms wavelet analysis wavelet coefficients filtering image coding signal synthesis filters image reconstruction dictionaries extrapolation;compression image;wavelet transform;wavelets on the interval;image compression;signal processing;image upscaling;algorithms data compression image enhancement image interpretation computer assisted numerical analysis computer assisted reproducibility of results sensitivity and specificity signal processing computer assisted;compresion dato;transformacion ondita;image upscaling image approximation signal approximation interval wavelet transform wavelet synthesis wavelet coefficients filtering image compression;procesamiento senal;wavelets;wavelet coefficients;filtering theory;analyse signal;wavelet transformation;compression donnee;signal approximation;compresion imagen;image approximation	In signal approximation, classical wavelet synthesis are known to produce Gibbs-like phenomenon around discontinuities when wavelet coefficients in the cone of influence of the discontinuities are quantized. By analyzing a function in a piecewise manner, filtering across discontinuities can be avoided. Using this principle, the interval wavelet transform can generate sparser representations in the vicinity of discontinuities than classical wavelet transforms. This work introduces two new constructions of interval wavelets and shows how they can be used for image compression and upscaling	approximation;coefficient;image compression;interval arithmetic;wavelet transform	Wei Siong Lee;Ashraf A. Kassim	2007	IEEE Transactions on Image Processing	10.1109/TIP.2006.884950	wavelet;computer vision;mathematical optimization;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;computer science;signal processing;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	Visualization	53.17491053268703	-66.86999858382005	121967
00e5d626bc7e038294653eeb67ba38ace425f955	neural network analysis of minerva scene analysis benchmark	image segmentation;image processing;neural networks;neural nets;neural networks image analysis roads classification tree analysis layout computer science windows benchmark testing image segmentation wire;texture extraction algorithms;texture extraction algorithms neural network analysis minerva scene analysis benchmark object identification natural scenes image processing image classification natural images feature extraction methods region of interest identification image segmentation algorithms;image classification;natural images;neural network classifier;layout;windows;region of interest identification;image texture;wire;feature extraction methods;image segmentation algorithms;roads;feature extraction;region of interest;neural network analysis;image analysis;minerva scene analysis benchmark;cross validation;classification tree analysis;computer science;neural nets natural scenes image classification image segmentation feature extraction image texture;object identification;benchmark testing;natural scenes;neural network;scene analysis	Scene analysis is an important area of research with the aim of identifying objects and their relationships in natural scenes. MINERVA benchmark has been recently introduced in this area for testing different image processing and classification schemes. In this paper we present results on the classification of eight natural objects in the complete set of 448 natural images using neural networks. An exhaustive set of experiments with this benchmark has been conducted using four different segmentation methods and five texture-based feature extraction methods. The results in this paper show the performance of a neural network classifier on a ten fold cross-validation task. On the basis of the results produced, we are able to rank how well different image segmentation algorithms are suited to the task of region of interest identification in these images, and we also see how well texture extraction algorithms rank on the basis of classification results.	algorithm;artificial neural network;benchmark (computing);cross-validation (statistics);experiment;feature extraction;grayscale;image processing;image segmentation;performance;region of interest;statistical classification;texture filtering	Markos Markou;Sameer Singh;Mona Sharma	2001		10.1109/ICIAP.2001.957020	image texture;layout;benchmark;computer vision;contextual image classification;feature extraction;computer science;machine learning;pattern recognition;image segmentation;artificial neural network;cross-validation;region of interest	ML	39.745635221727895	-67.63133632796156	122223
3f2f62d7ccd463f27bce8b38fc9accd7d6883272	ray-based and graph-based methods for fiber bundle boundary estimation	diffusion tensor images;three dimensional;pattern recognition;corticospinal tract;human brain	—Diffusion Tensor Imaging (DTI) provides the possibility of estimating the location and course of eloquent structures in the human brain. Knowledge about this is of high importance for preoperative planning of neurosurgical interventions and for intraoperative guidance by neuronavigation in order to minimize postoperative neurological deficits. Therefore, the segmentation of these structures as closed, three-dimensional object is necessary. In this contribution, two methods for fiber bundle segmenta-tion between two defined regions are compared using software phantoms (abstract model and anatomical phantom modeling the right corticospinal tract). One method uses evaluation points from sampled rays as candidates for boundary points, the other method sets up a directed and weighted (depending on a scalar measure) graph and performs a min-cut for optimal segmentation results. Comparison is done by using the Dice Similarity Coefficient (DSC), a measure for spatial overlap of different segmentation results.	coefficient;maxima and minima;minimum cut;phantom reference;tract (literature)	Miriam H. A. Bauer;Jan Egger;Daniela Kuhnt;Sebastiano Barbieri;Jan Klein;Horst K. Hahn;Bernd Freisleben;Christopher Nimsky	2011	CoRR		three-dimensional space;computer vision;mathematical optimization;computer science;machine learning;pattern recognition;mathematics	Vision	41.96954444929508	-79.75211163248903	122286
05d308b08d179ca8a04f45aac9613e8066381584	knn matting	image segmentation;image matching;image colour analysis;feature extraction;conjugate gradient methods	We are interested in a general alpha matting approach for the simultaneous extraction of multiple image layers; each layer may have disjoint segments for material matting not limited to foreground mattes typical of natural image matting. The estimated alphas also satisfy the summation constraint. Our approach does not assume the local colorline model, does not need sophisticated sampling strategies, and generalizes well to any color or feature space in any dimensions. Our matting technique, aptly called KNN matting, capitalizes on the nonlocal principle by usingK nearest neighbors (KNN) in matching nonlocal neighborhoods, and contributes a simple and fast algorithm giving competitive results with sparse user markups. KNN matting has a closed-form solution that can leverage on the preconditioned conjugate gradient method to produce an efficient implementation. Experimental evaluation on benchmark datasets indicates that our matting results are comparable to or of higher quality than state of the art methods.	benchmark (computing);cluster analysis;code;color space;conjugate gradient method;dimensions;experiment;feature vector;immunostimulating conjugate (antigen);k-nearest neighbors algorithm;matlab;nonlocal lagrangian;pixel;quantum nonlocality;sampling (signal processing);sampling - surgical action;sparse matrix;summation (document);anatomical layer;statistical cluster	Qifeng Chen;Dingzeyu Li;Chi-Keung Tang	2013	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/TPAMI.2013.18	computer vision;feature extraction;computer science;machine learning;pattern recognition;mathematics;image segmentation	Vision	50.040600256915724	-71.77369676670261	122444
454ca5266a6a47db06be0dd8a1faed88f9ad2de3	automatic whole brain mri segmentation of the developing neonatal brain	paediatrics biomedical mri brain image segmentation medical image processing;atlas based techniques automatic whole brain mri segmentation neonatal brain magnetic resonance imaging anatomical segmentations brain shape brain appearance intensity based segmentation;partial volume expectation maximization hierarchical modelling image segmentation model averaging neonatal brain mri;image segmentation pediatrics brain models manuals magnetic resonance imaging	Magnetic resonance (MR) imaging is increasingly being used to assess brain growth and development in infants. Such studies are often based on quantitative analysis of anatomical segmentations of brain MR images. However, the large changes in brain shape and appearance associated with development, the lower signal to noise ratio and partial volume effects in the neonatal brain present challenges for automatic segmentation of neonatal MR imaging data. In this study, we propose a framework for accurate intensity-based segmentation of the developing neonatal brain, from the early preterm period to term-equivalent age, into 50 brain regions. We present a novel segmentation algorithm that models the intensities across the whole brain by introducing a structural hierarchy and anatomical constraints. The proposed method is compared to standard atlas-based techniques and improves label overlaps with respect to manual reference segmentations. We demonstrate that the proposed technique achieves highly accurate results and is very robust across a wide range of gestational ages, from 24 weeks gestational age to term-equivalent age.	atlases;gestational age;resonance;segmentation action;signal-to-noise ratio;algorithm;biologic segmentation	Antonios Makropoulos;Ioannis S. Gousias;Christian Ledig;Paul Aljabar;Ahmed Serag;Joseph V. Hajnal;A. David Edwards;Serena J. Counsell;Daniel Rueckert	2014	IEEE Transactions on Medical Imaging	10.1109/TMI.2014.2322280	computer vision;medical physics	Vision	41.90959954818572	-78.6884443702515	122479
9c51c1928753016225d05cb6830c7e77766f08ab	hierarchical markov random field modeling for mammographic structure segmentation using multiple spatial and intensity image resolutions	digital radiography;hierarchical system;female;hierarchical structure;radiodiagnostic;medical imagery;image processing;tissues;image resolution;resolution spatiale;modelo markov;resolucion espacial;mastografia;analisis estructural;iterated conditional mode;radiografia numerica;skin;mammary gland;systeme hierarchise;procesamiento imagen;hombre;segmentation;blood vessel;traitement image;breast;markov random field;glandula mamaria;sistema jerarquizado;radiodiagnostico;digital mammography;markov model;campo aleatorio;hembra;mammographie;radiographie numerique;spatial distribution;human;imagerie medicale;glande mammaire;imageneria medical;radiodiagnosis;femelle;modele markov;mammography;analyse structurale;structural analysis;modeling;image modeling;segmentacion;blood vessels;champ aleatoire;bits per pixel;homme;random field;spatial resolution	A hierarchical Markov random field (MRF) model for mammographic structure segmentation using multiple spatial and intensity image resolutions is proposed. The general image model is formed by a sequence of representations at different spatial and intensity scales. Through the hierarchical structure of the MRF model, components at different local spatial resolutions are used to condition the corresponding intensity resolution and the spatial distribution of the intensity components. As a first step, only breast skin edge and non fat breast parenchyma (Cooper's ligaments, blood vessels and fibroglandular tissue) have been included into the model and implemented. Three basic priors for the local spatial intensity distribution (texture) are defined. An iterated conditional mode (ICM) optimization procedure is implemented, the lower resolution representations are used sequentially to form the initial image configurations for the ICM procedure. The proposed approach was tested using 100 digitized mammograms (at a resolution of 100 microns and 12 bits per pixel). The mammograms are from three different views and different breast parenchyma densities. Results for breast skin edge and breast parenchyma were obtained and evaluated visually. For all cases, the location of the three possible structures (skin, parenchyma and background) was identified correctly.© (1999) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	markov chain;markov random field	Rene Vargas-Voracek;Carey E. Floyd	1999		10.1117/12.348570	computer vision;simulation;geography;cartography	Vision	47.29624102195742	-79.1938204488657	123064
6235d4080834752e5135443720ae5174b7dd1241	photon-limited image denoising by inference on multiscale models	photonics;poisson process;poisson noise;hidden markov tree;image denoising hidden markov models parameter estimation bayesian methods robustness gunshot detection systems expectation maximization algorithms degradation fluctuations pixel;rate ratio;beta mixture rate ratio density image reconstruction photon limited image denoising poisson process poisson intensities expectation maximization algorithm maximum likelihood estimation rate ratio density parameters scale recursive hidden markov tree model quadtree image partitioning dirichlet mixture rate ratio;image segmentation;shot noise;binary image;quadtree image partitioning;bayesian inference;multiscale representation;expectation maximization algorithm photon limited imaging poisson bayesian inference hidden markov tree;random variables;indexing terms;maximum likelihood estimation;statistical model;photon limited imaging;noise measurement;multiscale modeling;indexes;maximum likelihood estimate;hidden markov models;estimation;stochastic processes;expectation maximization;poisson intensities;photon limited image denoising;image reconstruction;expectation maximization algorithm;rate ratio density parameters;beta mixture rate ratio density;image denoising;scale recursive hidden markov tree model;stochastic processes expectation maximisation algorithm hidden markov models image denoising image reconstruction image segmentation maximum likelihood estimation quadtrees;em algorithm;quadtrees;dirichlet mixture rate ratio;noise;poisson;expectation maximisation algorithm	We present an improved statistical model of Poisson processes, with applications to photon-limited imaging. We build on previous work, adopting a multiscale representation of the Poisson process in which the ratios of the underlying Poisson intensities (rates) in adjacent scales are modeled as mixtures of conjugate parametric distributions. Our main novel contributions are (1) a rigorous and robust regularized expectation-maximization (EM) algorithm for maximum-likelihood estimation of the rate-ratio density parameters directly from the observed Poisson data (counts); (2) extension of the method to work under a scale-recursive hidden Markov tree model (HMT) which couples the mixture label assignments in consecutive scales, thus modeling inter-scale coefficient dependencies in the vicinity of edges; and (3) exploration of a fully 2-D quad-tree image partitioning, involving Dirichlet-mixture rate-ratio densities, instead of the conventional separable binary image partitioning involving Beta-mixture rate-ratio densities. Experimental intensity estimation results on standard images with artificially simulated Poisson noise and photon-limited images with real shot noise demonstrate the effectiveness of the proposed approach.	binary image;binary space partitioning;coefficient;expectation–maximization algorithm;hidden markov model;markov chain;multiscale modeling;noise reduction;noise shaping;quadtree;recursion;shot noise;statistical model	Stamatios Lefkimmiatis;George Papandreou;Petros Maragos	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712259	stochastic process;mathematical optimization;expectation–maximization algorithm;computer science;shot noise;pattern recognition;mathematics;hidden markov model;statistics	Vision	52.63250776388638	-74.71614392269049	123128
c51b1a5fb5b24d90ea40ad2ef8fa37d16fe0d8f6	trus image segmentation driven by narrow band contrast pattern using shape space embedded level sets	level sets;active contours;prostate segmentation;narrow band contrast pattern;shape prior;transrectal ultrasound images	Prostate segmentation in transrectal ultrasound (TRUS) images is highly desired in many clinical applications. However, manual segmentation is difficult, time consuming and irreproducible. In this paper, we present a novel automatic approach using narrow band contrast pattern to segment prostates in TRUS images. Implicit representation of the segmenting level sets curve is firstly trained via principal component analysis, which also constraints the shape of prostate into a linear subspace. Then the model evolves to segment the prostate by maximizing the contrast in a narrow band near the segmenting curve. Many experimental results demonstrate the performance of the proposed algorithm, whose favorableness is validated by comparing to the state-of-the-art algorithms. Especially, the shape of prostate segmented by our algorithm is close to the one manually obtained by expert, and the mean absolute distance is only 1.07±0.77mm, which is quite promising.	embedded system;image segmentation	Pengfei Wu;Yiguang Liu;Yongzhong Li;Liping Cao	2012		10.1007/978-3-642-36669-7_42	computer vision;pattern recognition;mathematics;optics	Vision	40.531998921756866	-78.41326043131339	123169
c6f1036a3b6ba90fda7c749880569f1053e20487	automatic analysis of 2d foam sequences: application to the characterization of aqueous proteins foams stability	mathematical morphology;macroscopic imaging;foams stability;scale space;food products;image sequence;industrial application;image analysis;granulometric analysis;tracking;physical properties	Aqueous foams occur in several industrial applications including food production. One of the challenging issues in foams study is to evaluate the air bubbles stability. This problem is addressed in the present work using sequences of 2D macroscopic images of aqueous foams carried out in a glass column. Despite commercial softwares that are currently used in microscopy, the automatic analysis of foam sequences is still an unsolved problem. It can be related to two fundamental problems in the field of image analysis: the granulometric analysis and the automatic tracking of objects in image sequences. Rather than an adaptation of existing granulometric or tracking methods, the proposed foam analyzer consists in a new algorithm based on a double tracking of the foam bubbles: a scale-space tracking, where the scale parameter is the size, and a temporal tracking. It ensures a complete description of any bubble through the time and the detection of any bubbles interaction so that the foam’s structure is finally entirely and very precisely described. The proposed foam analyzer has been validated by comparing the characteristics computed by the automatic method to those that can be predicted via the foams physical properties. The proposed foam analyzer was exploited in three applied research works already published. 2008 Published by Elsevier B.V. C 39	algorithm;ana (programming language);electromagnetically induced grating;experiment;granulometry (morphology);heart rate variability;image analysis;maxima and minima;scale space;weak measurement;eric	Sabrina Rami-Shojaei;Corinne Vachier;Christophe Schmitt	2009	Image Vision Comput.	10.1016/j.imavis.2008.10.004	computer vision;scale space;image analysis;simulation;mathematical morphology;computer science;tracking;physical property	Vision	40.62891925208621	-73.69845653904052	123321
055a40bcadc8ea87e8821ac5ecee399d1f16dfb8	deformable models for random small-size objects: case of lung nodules in ct tomography	tumours cancer computerised tomography diagnostic radiography lung medical image processing;deformable models active tensor models active shape models traditional active appearance models standard mechanism human chest small size lung nodules appearance models randomness effect image analysis image modeling computerised tomography random small size objects;cancer;biological system modeling abstracts lungs medical services databases;tumours;lung;medical image processing;computerised tomography;diagnostic radiography	Deformable models are common in image modeling and analysis. Random objects provide major challenges as shapes and appearances are hard to quantify; hence, formulation of deformable models are much harder to construct and validate. In this paper, we examine the effect of randomness on building the shape and appearance models for small-size lung nodules (≤ 1cm) which appear in computed tomography (CT) of the human chest. We devise an approach for annotation, which lends a standard mechanism for building traditional active appearance (AAM), active shape (ASM) and active tensor models (ATM). We illustrate the effectiveness of AAM for nodule detection.	atm turbo;active appearance model;adaboost;automatic acoustic management;ct scan;categorization;cross-correlation;ensemble kalman filter;jones calculus;randomness;tomography	Amal A. Farag;James H. Graham;Aly A. Farag	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637818	cancer	Vision	41.43945924505721	-74.76147542795692	123491
4132349559a09bb947c87d34bed6c08b4f6be4d6	temporally sequenced intelligent block-matching and motion-segmentation using locally coupled networks	locally coupled network motion segmentation temporally sequenced intelligent block matching multilayer pulse coupled neural networks motion estimation optic flow image segmentation;intelligent networks image motion analysis motion estimation image segmentation optical sensors optical noise multi layer neural network motion analysis computer vision neural networks;image segmentation;neural nets;neural nets image segmentation motion estimation;video analysis;motion estimation;motion segmentation;synchronization;motion vector;block matching;optical flow;motion based segmentation;locally coupled networks;pcnn;pulse coupled neural network	Motion-based segmentation is a very important capability for computer vision and video analysis. It depends fundamentally on the system's ability to estimate optic flow using temporally proximate image frames. This is often done using block-matching. However, block-matching is sensitive to the presence of observational noise, which is inevitable in real images. Also, images often include regions of homogeneous intensity, where block-matching is problematic. A better method in this case is to estimate motion at the region level. In the approach described in this paper, we have attempted to address the noise-sensitivity and texture-insufficiency problems using a two-pathway system. The pixel-level pathway is a multilayer pulse-coupled neural network (PCNN)-like locally coupled network used to correct outliers in the block-matching motion estimates and produce improved estimates in regions with sufficient texture. In contrast, the region-level pathway is used to estimate the motion for regions with little intensity variation. In this pathway, a PCNN network first partitions intensity images into homogeneous regions, and a motion vector is then determined for the whole region. The optic flows from both pathways are fused together based on the estimated intensity variation. The fused optic flow is then segmented by a one-layer PCNN network. Results on synthetic and real images are presented to demonstrate that the accuracy of segmentation is improved significantly by taking advantage of the complementary strengths and weaknesses of the two pathways.	artificial neural network;attempt;computer vision;convergence insufficiency;estimated;eye;frame (physical object);gene regulatory network;matching;neural network simulation;optic flow;pixel;pulmonary valve insufficiency;pulse-coupled networks;synthetic intelligence;video content analysis;weakness;biologic segmentation;negative regulation of phospholipase c-activating g-protein coupled receptor signaling pathway	Xiaofu Zhang;Ali A. Minai	2004	IEEE Transactions on Neural Networks	10.1109/TNN.2004.832817	synchronization;computer vision;simulation;computer science;machine learning;motion estimation;optical flow;image segmentation;artificial neural network	Vision	39.589722621977685	-73.1507044280847	123498
9b06c3c7fd7a90ad2d8779eb39eaf635f9b504a6	effects of anatomical asymmetry in spatial priors on model-based segmentation of the brain mri: a validation study	maximum likelihood;prior information;mr imaging;tissue classification;brain tissue;validation studies	This paper examines the effect of bilateral anatomical asymmetry of spatial priors on the final tissue classification based on maximum-likelihood (ML) estimates of model parameters, in a modelbased intensity driven brain tissue segmentation algorithm from (possibly multispectral) MR images. The asymmetry inherent in the spatial priors is enforced on the segmentation routine by laterally flipping the priors during the initialization stage. The influence of asymmetry on the final classification is examined by making the priors subject-specific using non-rigid warping, by reducing the strength of the prior information, and by a combination of both. Our results, both qualitative and quantitative, indicate that reducing the prior strength alone does not have any significant impact on the segmentation performance, but when used in conjunction with the subject-specific priors, helps to remove the misclassifications due to the influence of the asymmetric priors.	algorithm;bilateral filter;image warping;multispectral image	Siddharth Srivastava;Frederik Maes;Dirk Vandermeulen;Wim Van Paesschen;Patrick Dupont;Paul Suetens	2004		10.1007/978-3-540-30135-6_40	computer vision;machine learning;pattern recognition;mathematics;maximum likelihood;statistics	Vision	43.30655295166342	-78.0164565585196	123865
16e2666b748c332b17df2f74eb078553ea9b423c	compressed sensing of diffusion mri data using spatial regularization and positivity constraints	diffusion mri;numerical analysis biodiffusion biomedical mri image reconstruction medical image processing;proximity operations compressed sensing diffusion mri data spatial regularization positivity constraints time efficient reconstruction water diffusion diffusion encoding gradients high angular resolution diffusion imaging signals numerical solution variable splitting;spherical ridgelets;compressed sensing;image resolution;numerical solution;biodiffusion;variable splitting;diffusion encoding gradients;high angular resolution diffusion imaging signals;diffusion mri data;proximity operations;arrays;numerical analysis;estimation;image reconstruction magnetic resonance imaging compressed sensing image resolution arrays estimation;image reconstruction;medical image processing;region of interest;magnetic resonance imaging;total variation;hardi;compressed sensing diffusion mri hardi sparse representations spherical ridgelets total variation;spatial regularization;high angular resolution diffusion imaging;sparse representation;sparse representations;positivity constraints;time efficient reconstruction;biomedical mri;water diffusion	In recent years, there has been an ever increasing number of works reporting the successful application of the theory of compressed sensing (CS) to the problem of time-efficient reconstruction of MRI scans. The CS theory seems to be particularly advantageous in application to diffusion MRI (dMRI), where, for the same region of interest, a number of MRI scans need to be acquired in order to assess the strength of water diffusion along different spatial directions. In this paper, we propose a CS-based reconstruction method which allows a substantial reduction in the number of diffusion encoding gradients required for reliable estimation of high angular resolution diffusion imaging signals. Specifically, the method performs a CS-based reconstruction in the diffusion domain subject to two additional constraints, namely: 1) the diffusion signals have to be spatially regular, and 2) the diffusion signals have to be non-negative valued. Additionally, we detail an efficient numerical solution based on variable splitting and proximity operations, which can be used to perform the proposed reconstruction. The paper is concluded with experimental results which support the practical value of our methodology.	angularjs;compressed sensing;gradient;numerical partial differential equations;region of interest;variable splitting	Sudipto Dolui;Oleg V. Michailovich;Yogesh Rathi	2011	2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2011.5872708	iterative reconstruction;diffusion mri;computer vision;mathematical optimization;estimation;radiology;image resolution;numerical analysis;computer science;magnetic resonance imaging;sparse approximation;mathematics;nuclear magnetic resonance;compressed sensing;total variation;region of interest	Vision	50.0305003950852	-79.73240697207986	123929
30a79d7b3f9cb0407dbc9cfbd55b2dd21fa87866	sector-based optic cup segmentation with intensity and blood vessel priors	bending;origa light database sector based optic cup segmentation blood vessel prior sector based method intensity based cup segmentation blood vessel based refinement statistical deformable model vessel free image optic disk vessel bending vessel boundary nasal side dice coefficient glaucoma diagnosis;image segmentation;vision defects bending biomechanics biomedical optical imaging blood vessels diseases image segmentation medical image processing;biomechanics;vision defects;medical image processing;algorithms databases factual diagnostic techniques ophthalmological glaucoma humans image processing computer assisted models statistical optic disk retinal vessels;diseases;biomedical optical imaging;image segmentation testing deformable models retina;blood vessels	The optic cup segmentation is critical for automated cup-to-disk ratio measurement, and hence computer-aided diagnosis of glaucoma. In this paper, we propose a novel sector-based method for optic cup segmentation. The method comprises two parts: intensity-based cup segmentation with shape constraints and blood vessel-based refinement. The initial estimation of the cup is obtained by applying a statistical deformable model on the vessel free image. At the same time, blood vessels within the optic disk are extracted, after which vessel bendings and vessel boundaries in the nasal side are located. Subsequently, these key points in the blood vessels are used to fine tune the cup. The algorithm is evaluated on 650 fundus images from the ORIGA-light database. Experimental results show that the Dice coefficient for the optic cup segmentation can be as high as 0.83, which outperforms other existing methods. The results demonstrate good potential for the proposed method to be used in automated optic cup segmentation and glaucoma diagnosis.	blood vessel tissue;computer assisted diagnosis;extraction;eye;glaucoma;level of measurement;optic disk;refinement (computing);structure of optic cup;sørensen–dice coefficient;algorithm;biologic segmentation	Fengshou Yin;Jiang Liu;Damon Wing Kee Wong;Ngan Meng Tan;Jun Cheng;Ching Yu Cheng;Yih Chung Tham;Tien Yin Wong	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346214	computer vision;bending;computer science;engineering;biomechanics;image segmentation;optics;physiology;engineering drawing	Vision	40.07852587193995	-79.05796036259169	124163
2fe88300edcff8b8e32c7c305edcaa2e1a694a94	a semi-automatic solitary pulmonary nodule volume measurement algorithm on low-dose ct images	cancer detection;lung cancer;solitary pulmonary nodule;manuals;radiology;low dose;protocols;computed tomography;cancer;lungs;lung cancer semiautomatic geometric solitary pulmonary nodule volume measurement algorithm low dose ct image ldct image computer assisted method spn core parenchymal area partial volume area computerised radiology indeterminate nodule growth rate evaluation;parenchymal area;computational geometry;data mining;medical image processing cancer computational geometry computerised tomography diagnostic radiography lung;lung;computer networks;ldct image;low dose ct image;geometry method low dose ct images solitary pulmonary nodule volume measurement;partial volume;medical image processing;voltage;growth rate;computerised radiology;geometry method;computerised tomography;tumors;software algorithms;low dose ct images;volume measurement;indeterminate nodule growth rate evaluation;partial volume area;spn core;computer assisted method;volume measurement computed tomography lungs computer networks in vivo cancer detection protocols voltage optimization methods software algorithms;semiautomatic geometric solitary pulmonary nodule volume measurement algorithm;in vivo;diagnostic radiography;partial volume effect;optimization methods	The computer-assisted methods for measuring and tracking nodule volumes have the potential to improve precision for indicating of malignancy for indeterminate nodules. In this paper, we propose a semi-automatic geometric solitary pulmonary nodule (SPN) volume measurement algorithm for calculating the precise volume of indeterminate SPNs with low-dose CT (LDCT) images. The algorithm divided the SPN volume into three parts: the SPN core, the parenchymal area, and the partial volume area. Then we calculated the volume with a geometry method and corrected the volume for partial volume effects with the partial volume area. The proposed method has been compared with the manual volume measurement of nodules by radiologists using two sets CT images in vivo. The result shows that the method is more objective and can evaluate the indeterminate nodules growth rate effectively using LDCT images.	algorithm;ct pulmonary angiogram;ct scan;indeterminacy in concurrent computation;radiology;semiconductor industry;substitution-permutation network;video-in video-out	Guodong Zhang;Donghong Sun;Hong Zhao;Zhezhu Li	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.326	computational geometry;computer science;partial volume	Visualization	39.791922415065635	-79.28632652148707	124276
34b515d8aa35b301df8ab14b06ba8f46408dd71a	morphological operations on matrix-valued images	mathematical morphology;shape analysis;morphological operation;rotation invariance;image processing methods;stress tensor	The output of modern imaging techniques such as diffusion tensor MRI or the physical measurement of anisotropic behaviour in materials such as the stress-tensor consists of tensor-valued data. Hence adequate image processing methods for shape analysis, skeletonisation, denoising and segmentation are in demand. The goal of this paper is to extend the morphological operations of dilation, erosion, opening and closing to the matrix-valued setting. We show that naive approaches such as componentwise application of scalar morphological operations are unsatisfactory, since they violate elementary requirements such as invariance under rotation. This lead us to study an analytic and a geometric alternative which are rotation invariant. Both methods introduce novel non-component-wise definitions of a supremum and an infimum of a finite set of matrices. The resulting morphological operations incorporate information from all matrix channels simultaneously and preserve positive definiteness of the matrix field. Their properties and their performance are illustrated by experiments on diffusion tensor MRI data.	closing (morphology);density matrix;dilation (morphology);erosion (morphology);experiment;image processing;mathematical morphology;maxima and minima;maximal set;noise reduction;opening (morphology);requirement;shape analysis (digital geometry);the matrix	Bernhard Burgeth;Martin Welk;Christian Feddern;Joachim Weickert	2004		10.1007/978-3-540-24673-2_13	computer vision;morphological skeleton;mathematical morphology;computer science;morphological gradient;pattern recognition;shape analysis;cauchy stress tensor;mathematics;geometry	Vision	53.42302838313137	-70.76149227949806	124595
20318122fde0bfb6a820359d98ed1312de5940f7	detecting text in natural scenes with stroke width transform	stroke width transform;image segmentation;filter bank;image processing;colored noise;training;natural images natural scenes stroke width transform image operator text detection image pixel;geometry;text analysis;natural images;layout;image operator;computer vision;text analysis image processing;optical character recognition software;engines;image edge detection;roads;data dependence;image color analysis;pixel;transforms;robustness;text detection;image pixel;natural scenes;layout optical character recognition software pixel computer vision filter bank engines image segmentation colored noise geometry robustness	We present a novel image operator that seeks to find the value of stroke width for each image pixel, and demonstrate its use on the task of text detection in natural images. The suggested operator is local and data dependent, which makes it fast and robust enough to eliminate the need for multi-scale computation or scanning windows. Extensive testing shows that the suggested scheme outperforms the latest published algorithms. Its simplicity allows the algorithm to detect texts in many fonts and languages.	algorithm;computation;feature (computer vision);local variable;microsoft windows;optical character recognition;pixel;standard widget toolkit	Boris Epshtein;Eyal Ofek;Yonatan Wexler	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540041	layout;computer vision;speech recognition;colors of noise;image processing;computer science;filter bank;image segmentation;pixel;robustness;computer graphics (images)	Vision	39.51408398164917	-66.53246069041376	124845
ee49c3f17cad559717ad4db554ec273a2387916b	regularized fieldmap estimation in mri	estimation theory;image distortion;off resonance frequency;estimation method;phase noise;nonlinear convergent algorithms regularized fieldmap estimation mri echo planar imaging spiral scans field inhomogeneity effects correction image distortion image blurring off resonance frequency smooth functions;regularization method;maximum likelihood estimation;noise measurement;field inhomogeneity effects correction;image blurring;nonlinear distortion;mr imaging;smoothing methods;estimation;resonant frequency;phase estimation;image reconstruction;medical image processing;magnetic resonance imaging;spirals;magnetic resonance imaging phase estimation maximum likelihood estimation image reconstruction phase noise smoothing methods noise measurement spirals nonlinear distortion frequency;mri;echo planar imaging;biomedical image processing;regularized fieldmap estimation;frequency;smooth functions;article;nonlinear convergent algorithms;biomedical mri;spiral scans;medical image processing biomedical mri estimation theory	In fast MR imaging with long readout times, such as echo-planar imaging (EPI) and spiral scans, it is important to correct for the effects of field inhomogeneity to reduce image distortion and blurring. Such corrections require an accurate field map, a map of the off-resonance frequency at each voxel. Standard fieldmap estimation methods yield noisy fieldmaps, particularly in image regions having low spin density. This paper describes regularized methods for fieldmap estimation. These methods exploit the fact that fieldmaps are smooth functions. Efficient convergent algorithms are given even though the problem is highly nonlinear. Results show that the proposed regularized methods significantly improve the quality of fieldmap estimates relative to conventional unregularized methods	algorithm;ct scan;distortion;kernel density estimation;nonlinear system;resonance;voxel	Jeffrey A. Fessler;Desmond Teck Beng Yeo;Douglas C. Noll	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1625014	iterative reconstruction;computer vision;mathematical optimization;nonlinear distortion;estimation;radiology;resonance;noise measurement;magnetic resonance imaging;frequency;mathematics;maximum likelihood;estimation theory;phase noise;statistics;spiral	Vision	52.55140506258612	-76.8074447825994	124859
e1a1efadc0f34fbd9e2909a513a139d24b4b433e	relaxed conditional hierarchical statistical shape model of multiple organs	multiple organs;shape computational modeling training solid modeling covariance matrices principal component analysis vectors;solid modelling computerised tomography liver medical computing principal component analysis;conditional statistical shape model;liver;level set;medical computing;hierarchical ssm relaxed conditional hierarchical statistical shape model multiple organs shape extraction pose parameter extraction training label dataset organ pose model principal component analysis organ principal score concatenation vector pca based statistical modeling relaxation scheme spleen modeling gallbladder modeling liver conditional organ generalization specificity abdominal ct volumes voxels;principal component analysis;computerised tomography;level set conditional statistical shape model hierarchical model multiple organs;hierarchical model;solid modelling	This paper proposes a relaxed conditional hierarchical statistical shape model (SSM) of multiple organs. After extracting shape and pose parameters from the training label dataset of multiple organs, the shape model and the pose model of each organ are constructed by principal component analysis (PCA). Subsequently, the principal scores of all organs are concatenated into a vector, and the vectors computed from the training dataset are forwarded to the PCA-based statistical modeling of the multiple organs under conditions of their neighboring organs. A relaxation scheme is introduced, to take into account errors in the conditions. This study focuses on modeling of a spleen and a gallbladder given a liver as a conditional organ. The performance of the model was evaluated with the measures of generalization and specificity, which were computed by three-fold cross-validation using labels of 27 abdominal CT volumes with the size of 170 × 170 × 110 voxels and a resolution of 1.8809 mm/voxel. Compared with a hierarchical SSM without conditions, generalization and specificity were improved from 0.488 to 0.506 and from 0.319 to 0.328 on average, respectively. In addition, the proposed relaxed conditional hierarchical SSM outperformed a hierarchical SSM with hard conditions. The performance indices were improved by 0.040 and 0.010 for generalization and specificity, respectively.	concatenation;cross-validation (statistics);linear programming relaxation;principal component analysis;sensitivity and specificity;shape context;statistical model;statistical shape analysis;test case;voxel	Reito Oshima;Atsushi Saito;Hidefumi Watanabe;Akinobu Shimizu;Shigeru Nawano	2013	2013 First International Symposium on Computing and Networking	10.1109/CANDAR.2013.50	computer vision;computer science;machine learning;pattern recognition	Vision	42.20708495829077	-79.07470280282146	124895
3301a1d0f9cc6c0ab24cf863bb573bb68ebe508f	combining image entropy with the pulse coupled neural network in edge detection	detectors;strictly bounded function;image segmentation;neural nets;edge detection;natural images;gray scale;pulse coupled neural network edge detection image entropy;image edge detection;local image entropy;pixel;image intensity;image entropy;entropy;edge pixels;neural nets edge detection;mathematical properties;pixel grayscale values;pulse coupled neural network;image edge detection entropy pixel gray scale image segmentation detectors;edge pixels pulse coupled neural network edge detection pixel grayscale values strictly bounded function local image entropy image intensity mathematical properties	We propose a simple and effective approach for edge detection using the image entropy defined on pixel grayscale values instead of the histogram. A strictly bounded function of local image entropy is designed for identifying abrupt changes of image intensity across edges. Mathematical properties of this function are analyzed to validate its applicability in the edge detection task. Edge pixels are segmented using a Pulse Coupled Neural Network in which the connectivity prior of edge pixels is used. Experimental results demonstrate that our method can robustly detect edges in synthetic as well as natural images.	artificial neural network;edge detection;grayscale;pixel;synthetic intelligence	Jiansheng Chen;Jinping He;Guangda Su	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653182	computer vision;entropy;detector;edge detection;computer science;machine learning;pattern recognition;mathematics;image segmentation;canny edge detector;artificial neural network;pixel;grayscale	Vision	46.59247427250208	-66.5676480121963	125009
129a9b90e56eb75c9ddcd07f0979afbb0168c23a	accurate cell segmentation in microscopy images using membrane patterns		MOTIVATION Identifying cells in an image (cell segmentation) is essential for quantitative single-cell biology via optical microscopy. Although a plethora of segmentation methods exists, accurate segmentation is challenging and usually requires problem-specific tailoring of algorithms. In addition, most current segmentation algorithms rely on a few basic approaches that use the gradient field of the image to detect cell boundaries. However, many microscopy protocols can generate images with characteristic intensity profiles at the cell membrane. This has not yet been algorithmically exploited to establish more general segmentation methods.   RESULTS We present an automatic cell segmentation method that decodes the information across the cell membrane and guarantees optimal detection of the cell boundaries on a per-cell basis. Graph cuts account for the information of the cell boundaries through directional cross-correlations, and they automatically incorporate spatial constraints. The method accurately segments images of various cell types grown in dense cultures that are acquired with different microscopy techniques. In quantitative benchmarks and comparisons with established methods on synthetic and real images, we demonstrate significantly improved segmentation performance despite cell-shape irregularity, cell-to-cell variability and image noise. As a proof of concept, we monitor the internalization of green fluorescent protein-tagged plasma membrane transporters in single yeast cells.   AVAILABILITY AND IMPLEMENTATION Matlab code and examples are available at http://www.csb.ethz.ch/tools/cellSegmPackage.zip.	cell signaling;cross-correlation;cut (graph theory);cytology;gradient;graph - visual representation;green fluorescent proteins;image noise;incised wound;matlab;membrane transport proteins;plasma active;plasma membrane;protocols documentation;segmentation action;spatial variability;synthetic intelligence;tissue membrane;tracer;algorithm;biologic segmentation	Sotiris Dimopoulos;Christian E. Mayer;Fabian Rudolf;Jörg Stelling	2014	Bioinformatics	10.1093/bioinformatics/btu302	computer vision;computer science;bioinformatics;theoretical computer science;segmentation-based object categorization;scale-space segmentation	Comp.	39.80652968982992	-73.34084444041665	125069
225ebebaed8e922e07dddd9f8e1afce3660ce44c	variational inference of the fiber orientation density using diffusion mr imaging	gaussian process model;constant diffusivity assumption;confidence band;white matter;forward model;mr imaging;inverse problem;complex system;variational inference;reproducing kernel hilbert space;fourier analysis;cerebral cortex;diffusion process;long range;gaussian process;diffusion mr imaging;high angular resolution;high frequency;density functional;smoothing spline;human brain;spherical deconvolution;laplace beltrami operator;diffusion weighted	Diffusion MR imaging has enabled the in vivo exploration of the connectional architecture in human brain. This method particularly reveals the complex system of long-range nerve fibers that integrate the functionally distinct areas of the cerebral cortex. Since the fibers are not directly observed but the diffusion process of water molecules in the underlying material, a forward model is established that maps the microgeometry of nervous tissue onto the diffusion-weighted signals. This article proposes the spherical deconvolution of the fiber orientation density in a reproducing kernel Hilbert space, thereby generalizing previous approaches that perform a truncated Fourier analysis on the sphere. The specified inverse problem is solved within a smoothing spline framework which preserves the characteristic properties of a density function, namely its normalization and non-negativity. A Gaussian process model allows the specification of confidence bands for the estimated fiber orientation density and the rigorous selection of the hyperparameters, here the high-frequency content in the density function and the noise variance of the MR observations. In addition, we weaken the constant diffusivity assumption frequently made in the spherical convolution methodology. The novel approach, which uncovers the fiber orientation field of white matter, is demonstrated with diffusion-weighted data sets featuring high angular resolution.	angularjs;bands;cerebral cortex;complex system;convolution;deconvolution;fourier analysis;gaussian process;hilbert space;inference;kernel;map;negativity (quantum mechanics);nerve fibers;nerve tissue;normal statistical distribution;process modeling;sample variance;smoothing (statistical technique);smoothing spline;specification;tissue fiber;variational principle;video-in video-out;white matter	Enrico Kaden;Alfred Anwander;Thomas R. Knösche	2008	NeuroImage	10.1016/j.neuroimage.2008.06.004	mathematical optimization;complex systems;mathematical analysis;smoothing spline;inverse problem;diffusion process;high frequency;reproducing kernel hilbert space;gaussian process;mathematics;laplace–beltrami operator;fourier analysis;confidence and prediction bands;statistics	ML	49.67747857441271	-79.14864681013088	125699
83573880572707ea77b0ce4cb505bbaf38bb97af	empirical evaluation of dissimilarity measures for color and texture	performance evaluation;dissimilarity measure;random sampling;unsupervised segmentation;texture features;large scale;computer experiment;classification image;ground truth;empirical evaluation	This paper empirically compares nine families of image dissimilarity measures that are based on distributions of color and texture features summarizing over 1000 CPU hours of computational experiments. Ground truth is collected via a novel random sampling scheme for color, and by an image partitioning method for texture. Quantitative performance evaluations are given for classification, image retrieval, and segmentation tasks, and for a wide variety of dissimilarity measure parameters. It is demonstrated how the selection of a measure, based on large scale evaluation, substantially improves the quality of classification, retrieval, and unsupervised segmentation of color and texture images.	image texture	Yossi Rubner;Jan Puzicha;Carlo Tomasi;Joachim M. Buhmann	2001	Computer Vision and Image Understanding	10.1006/cviu.2001.0934	image texture;sampling;computer vision;computer experiment;ground truth;machine learning;pattern recognition;mathematics;image segmentation	Vision	40.116918793148336	-69.03601252171039	125777
1c7b4d7972e4ee271ff7d980235f2e8cca628723	detecting osteoporosis from dental radiographs using active shape models	dual energy x ray absorptiometry;dentistry;active shape model osteoporosis dentistry radiography automatic testing brain modeling bones minerals hip shape measurement;image segmentation;dental radiographs;indexing terms;inferior mandibular cortex;active shape models;bone mineral density;bone;computerised tomography;osteoporosis;dental panoramic tomograms;diseases;dual energy x ray absorptiometry osteoporosis dental radiographs active shape models bone mineral density dental panoramic tomograms inferior mandibular cortex;physiological models;active shape model;diagnostic radiography;physiological models bone computerised tomography dentistry diagnostic radiography diseases	We describe a novel method of estimating reduced bone mineral density (BMD) from dental panoramic tomograms (DPTs) which show the entire mandible. Careful expert width measurement of the inferior mandibular cortex has been shown to be predictive of BMD in hip and spine osteopenia and osteoporosis. We have implemented a method of automatic measurement of the width by active shape model search, using as training data 132 DPTs of female subjects and tested on a test data set of 606 DPTs of female subjects whose BMD has been established by dual-energy X-ray absorptiometry. We demonstrate that the sensitivity of the automatic method to osteoporosis is equivalent to that of expert manual measurement.	active shape model;dual-energy x-ray absorptiometry;image stitching;radiography;test data;tomography	Philip D. Allen;James Graham;Damian J. J. Farnell;Elizabeth J. Marjanovic;Judith Adams;Reinhilde Jacobs;Kety Nicopolou-Karayianni;Christina Lindh;Paul F. van der Stelt;Keith Horner;Hugh Devlin	2007	2007 4th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2007.357087	active shape model;computer vision;index term;radiology;pathology;computer science;image segmentation;dentistry	Vision	41.30845446466291	-78.51919036627675	125795
d3a6032396bd37389595600d12bd46b355fb5160	segmentation of touching cells using gradient flow tracking	cell segmentation;image segmentation;3d imaging;molecular imaging;microscopy;biomedical imaging;deformable models;segmentation;indexing terms;voxel;c elegant segmentation gradient flow tracking voxel touching cells zebrafish;touching cells;shape;c elegant;zebrafish;elastic deformation transformation;medical image processing;gradient flow tracking;clustering algorithms;gradient methods;robustness;vector field;image segmentation deformable models robustness microscopy clustering algorithms shape bioinformatics biomedical imaging automation molecular imaging;cellular biophysics;gradient flow;medical image processing cellular biophysics gradient methods image segmentation;bioinformatics;automation	The development of automated and robust computational algorithms for 3D cell segmentation remains challenging in situations where the cells are touching each other or connected together. In this paper, we present a novel automated method that aims to tackle the aforementioned challenges in the segmentation of clustered or connected 3D cells. We first diffuse the gradient vector field with an elastic deformable transform. Then, a gradient flow tracking procedure is performed for each voxel to find the corresponding center that the point flows to. All points that flow to the same center are grouped as a region. In this way, the boundaries between touching cells are formed naturally, and the touched cells are divided. Evaluation results from 3D image data of zebrafish and C. Elegant indicate good performance of the proposed method	algorithm;bioinformatics;computation;gradient;voxel	Gang Li;Tianming Liu;Jingxin Nie;Lei Guo;Stephen T. C. Wong	2007	2007 4th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2007.356792	stereoscopy;computer vision;vector field;zebrafish information network genome database;index term;radiology;shape;computer science;microscopy;theoretical computer science;automation;image segmentation;molecular imaging;cluster analysis;balanced flow;voxel;segmentation;robustness;computer graphics (images)	Visualization	46.599089592170294	-71.84869351434791	125874
61897c21dae8cbbd7d3de7892ac8df10bead9ba9	curvature prior for mrf-based segmentation and shape inpainting	image segmentation;learning model;higher order;linear functionals;maximum a posterior;compact representation;image reconstruction;pattern recognition;higher order functions;shape priors	Most image labeling problems such as segmentation and image reconstruction are fundamentally ill-posed and suffer from ambiguities and noise. Higher order image priors encode high level structural dependencies between pixels and are key to overcoming these problems. However, these priors in general lead to computationally intractable models. This paper addresses the problem of discovering compact representations of higher order priors which allow efficient inference. We propose a framework for solving this problem which uses a recently proposed representation of higher order functions where they are encoded as lower envelopes of linear functions. Maximum a Posterior inference on our learned models reduces to minimizing a pairwise function of discrete variables, which can be done approximately using standard methods. Although this is a primarily theoretical paper, we also demonstrate the practical effectiveness of our framework on the problem of learning a shape prior for image segmentation and reconstruction. We show that our framework can learn a compact representation that approximates a prior that encourages low curvature shapes. We evaluate the approximation accuracy, discuss properties of the trained model, and show various results for shape inpainting and image segmentation.	approximation;computational complexity theory;encode;grammar-based code;high-level programming language;higher-order function;image segmentation;inpainting;iterative reconstruction;linear function;markov random field;pixel;well-posed problem	Alexander Shekhovtsov;Pushmeet Kohli;Carsten Rother	2012		10.1007/978-3-642-32717-9_5	iterative reconstruction;computer vision;mathematical optimization;higher-order logic;computer science;machine learning;pattern recognition;mathematics;image segmentation;higher-order function	Vision	53.10501759447047	-70.56148109032013	125907
601b4c788475040ca26d35b60ff2a788ba3be542	convergence analysis of active contours	second order;convergence analysis;snakes;active contour;image segmentation;active contours;iterative algorithm;object tracking	Active contours are very useful tools in image segmentation and object tracking in video sequences. The practical implementations are built with an iterative algorithm based on a second order system defined in the spatial domain, where the elasticity and rigidity are the static parameters for its characterization and mass and damping are the dynamic parameters. In the process, the contour is influenced by external and internal forces varying its shape adaptively. The number of iterations required by the contour to delineate the objects is determined by these forces, by its initialization and by the coefficients of the second order system. This paper analyzes the convergence of active contours using the frequency based formulation and shows that the convergence depends on the dynamic parameters of the second order system and the distance between nodes of the contour attracted by the external forces.		Rafael Verdú;Juan Morales-Sánchez;Luis Weruaga	2008	Image Vision Comput.	10.1016/j.imavis.2007.12.003	computer vision;mathematical optimization;computer science;active contour model;control theory;mathematics;image segmentation	Vision	48.25297995958869	-71.50478729526588	125914
0e346bb3df91e579b1b2561a1b9bec662afa6d9b	feature tracking by multi-frame relaxation	multi-frame relaxation;inter-frame relaxation;feature tracking;line detection;feature detection;surface structure;statistical model;probability density	This paper describes a novel feature tracking method. It is based on an interframe relaxation technique. This method combines intra-frame and inter-frame constraints on the behaviour of acceptable contour structure. The intra-frame information is represented by both a dictionary of local contour structure and a statistical model of the response of a set of directional feature detection operators. The inter-frame ingredient represents the novel modelling component; it is encapsulated by an implicit model of the underlying surface structure of 3D feature points. The model is represented in terms of a series of unimodal probability densities whose single parameter is the interframe distance. The initial probabilities in our relaxation scheme effectively combine distributions describing the statistical uncertainties in the position and feature characteristics of multiframe contours; these probabilities are refined in the light of the dictionary to produce consistent contours. We present an experimental evaluation of the resulting feature detection method on cranial MRI data. Here the method significantly outperforms its single frame counterpart in term of its ability to extract noise-free and smooth feature contours.	dictionary;feature detection (computer vision);feature detection (web development);intra-frame coding;lagrangian relaxation;linear programming relaxation;motion estimation;statistical model	Nigel G. Sharp;Edwin R. Hancock	1994		10.5244/C.8.40	probability distribution;titration;statistical model;probability density function;systems modeling;computer science;artificial intelligence;relaxation;feature detection;mathematics;transformational grammar;feature;statistics	Vision	51.10186112261485	-70.0182910301153	126039
4d56d44a1a9380b2c63a3a58cfed6a79e712faaf	agreement between the white matter connectivity based on the tensor-based morphometry and the volumetric white matter parcellations based on diffusion tensor imaging	health research;uk clinical guidelines;biological patents;novel computational framework;white matter connectivity;brain;t1 weighted magnetic resonance imaging;white matter;diffusion tensor images;image processing;europe pubmed central;biodiffusion;white matter atlas;citation search;structural connectivity;biomedical imaging;volumetric white matter parcellations;magnetic resonance image;correlation diffusion tensor imaging biomedical imaging educational institutions brain;uk phd theses thesis;medical image processing;jacobian determinants;brain network;image processing white matter connectivity tensor based morphometry volumetric white matter parcellations diffusion tensor imaging novel computational framework t1 weighted magnetic resonance imaging jacobian determinants;tensor based morphometry;life sciences;white matter atlas structural connectivity brain network tensor based morphometry;correlation;diffusion tensor imaging;jacobian matrices;uk research reports;medical journals;europe pmc;biomedical research;biomedical mri;bioinformatics;medical image processing biodiffusion biomedical mri brain jacobian matrices	We are interested in investigating white matter connectivity using a novel computational framework that does not use diffusion tensor imaging (DTI) but only uses T1-weighted magnetic resonance imaging. The proposed method relies on correlating Jacobian determinants across different voxels based on the tensor-based morphometry (TBM) framework. In this paper, we show agreement between the TBM-based white matter connectivity and the DTI-based white matter atlas. As an application, altered white matter connectivity in a clinical population is determined.	cervical atlas;diffusion tensor imaging;jacobian matrix and determinant;magnetic resonance imaging;morphometric analysis;morphometrics;mucin 5ac;tobramycin;voxel;white matter	Seung-Goo Kim;Brian B. Avants;Hyekyoung Lee;James C. Gee;Moo K. Chung;Richard J. Davidson;Jamie L. Hanson;Seth D. Pollak	2012	2012 9th IEEE International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2012.6235479	diffusion mri;radiology;medicine;image processing;magnetic resonance imaging;nuclear magnetic resonance;nuclear medicine;correlation	Vision	43.52672862164538	-79.08183943038749	126044
2856e364b0ac7fe38ff30111cf1230c80c35b62d	multiscale analysis of digital segments by intersection of 2d digital lines	complexity theory;image segmentation;dsl;image resolution;multiscale analysis;geometry;digital straight segment;shape recognition;digital segment;scale space;shape;shape recognition image resolution image segmentation;2d digital line;decision support systems;pattern recognition;multiresolution;digital shape contour multiscale analysis digital segment 2d digital line pattern recognition continuous scale space theory multiresolution digital straight segment;decision support systems dsl shape complexity theory geometry equations pattern recognition;continuous scale space theory;digital shape contour	A theory for the multiscale analysis of digital shapes would be very interesting for the pattern recognition community, giving a digital equivalent of the continuous scale-space theory. We focus here on providing analytical formulae of the multiresolution of Digital Straight Segments (DSS), which is a fundamental tool for describing digital shape contours.	pattern recognition;scale space	Mouhammad Said;Jacques-Olivier Lachaud;Fabien Feschet	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.996	computer vision;scale space;digital subscriber line;image resolution;decision support system;shape;computer science;pattern recognition;mathematics;geometry;image segmentation	Vision	49.427611041851186	-66.50004713195771	126072
28cce56d36d1064a0303c78c17d7bbab477f656b	anscombe meets hough: noise variance stablization via parametric model estimation		In this work we pose the parameter estimation of the Poisson-Gaussian noise model as a parametric model estimation problem. We first take patches from the image/video to analyze and treat variance stabilization transforms, e.g., the classical Generalized Anscombe transform, as a parametric model, which we fit to the patches using the Hough transform. This algorithm allows to successfully estimate the noise parameters, is computationally efficient, and is fully parallelizable. We present an application to calcium imaging data, where the estimated parameters are used to enhance state-of-the-art processing pipelines.	algorithm;algorithmic efficiency;estimation theory;hough transform;parametric model;pipeline (computing);quantum decoherence	Mariano Tepper;Andrea Giovannucci;Eftychios A. Pnevmatikakis	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461938	parallelizable manifold;mathematical optimization;parametric model;pattern recognition;artificial intelligence;parametric statistics;estimation theory;anscombe transform;computer science;hough transform	Vision	52.27869953247235	-76.64013831757009	126447
fee684bab3bbbf538a744b5c7efa2d3d6acab6c4	a refined algorithm for multisensor image registration based on pixel migration	multisensor;deteccion multiespectral;image processing;detection multispectrale;multispectral image registration;genetic algorithm refined algorithm multisensor image registration pixel migration squared gradient magnitude surface;genetic algorithms image registration sensor fusion;search space;multispectral image registration genetic algorithm;procesamiento imagen;algoritmo genetico;traitement image;registro imagen;recalage image;algorithms artificial intelligence computer simulation image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval models biological models statistical pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique transducers;image registration;multispectral images;image registration pixel image sensors image representation signal processing algorithms minimally invasive surgery ultrasonic imaging magnetic resonance imaging brightness image converters;sum of squares;estimacion parametro;algorithme genetique;genetic algorithm;genetic algorithms;global optimization;parameter estimation;estimation parametre;sensor fusion;capteur multiple;multispectral detection	Multimodality image registration via pixel migration is a powerful approach. However, it suffers from a serious problem - the global maximum on the sum of squared gradient magnitude (SSG) surface does not correspond to the correct solution of registration. To solve the problem, we partition the search space into feasible and infeasible regions. The genetic algorithm (global optimizer) is used to obtain a good initial estimate of registration parameters and followed by a fast refining with Powell's approach (local optimizer). The experimental results demonstrate that the use of this modified pixel migration algorithm on multisensor image registration is very effective.	genetic algorithm;gradient;image registration;mathematical optimization;maxima and minima;mental suffering;multimodal imaging;pixel;powell's method;sodium stibogluconate;symbolic stream generator;registration - actclass	J. Yao;K. L. Goh	2006	IEEE Transactions on Image Processing	10.1109/TIP.2006.873451	computer vision;mathematical optimization;genetic algorithm;image processing;computer science;machine learning;mathematics;global optimization	Vision	52.02384988138097	-77.93147827523254	126631
fd02257193adbdbab35362111532100bcc438c5c	grey level thresholding using second-order statistics	segmentation;threshold selection;unimodal images;co occurance matrices;second order statistics	This letter describes algorithms for global thresholding of grey-tone images which use second-order grey level statistics. Two measures of interaction between classes of intensity levels are defined on simple co-occurance matrices and are used to evaluate and select thresholds. One of these measures is seen to be independent of the grey level histogram and effective in selecting thresholds for images with unimodal grey level distributions. The algorithms are also used for multithresholding without modifications. Grey level thresholding for the purpose of image segmentation is essentially a classification pro­ blem. The intensity (grey) levels are to be subdivid­ ed into bands so as to provide classes of intensity levels corresponding to regions of similar attribute. Various threshold selection techniques have been derived based on the grey level histogram or 'improved' versions of such histograms (using edge strength information) (Weszka (1978), Pal et al. (to appear». These techniques follow the simple heuristic of threshold selection at the minimum between histogram peaks (valleys). Weszka and Rosenfeld (1978) suggested a cost function based on the joint probability matrices of grey levels which can be used for threshold evaluation and selection. Unlike grey level histograms, such co­ occurrence matrices (or grey tone spatial dependency matrices (Haralick et al. (1973») con­ tain information about the spatial relationship bet­ ween the intensity levels and can therefore be the basis of more meaningful criteria for grey level classification. However, the 'business' measure of Weszka and Rosenfeld (1978) as a function of the threshold level is essentially an improved histogram and in this respect is similar to other methods aimed at using second-order statistics to define improved histograms. This paper describes two 'interaction measures' for the selection of thresholds based on similar second-order grey level statistics as those mention­ ed above. The measures are defined on simple joint frequency matrices for grey levels occurring in horizontal and vertical nearest neighbour relative positions. The relationship between grey levels at these relative displacements is here referred to as 'intensity transition' and the corresponding co­ occurance matrix is referred to as a transition matrix. The interaction measures are defined to represent the 'cost' of a threshold in terms of the probabilities of transition between the intensity classes which the threshold defines. Therefore the optimum threshold is chosen so as to minimise the interaction measures. One of the measures is similar to the 'business'	algorithm;grayscale;heuristic;image segmentation;loss function;louis rosenfeld;robert haralick;stochastic matrix;thresholding (image processing)	Farzin Deravi;Sankar K. Pal	1983	Pattern Recognition Letters	10.1016/0167-8655(83)90080-6	machine learning;pattern recognition;mathematics;thresholding;segmentation;statistics	Vision	43.49946428247858	-67.26875132729305	126928
6e5426cf6f9e4bae62cb3ffb1096bd8b2d8bdd10	local regularity-based image denoising	wavelet analysis;image denoising noise reduction additive noise image segmentation image analysis minimax techniques wavelet analysis low pass filters functional analysis radar imaging;image recovery image denoising local holder regularity additive noise white noise besov space wavelet analysis sar image denoising asymptotically minimax;image restoration;wavelet transforms;minimax techniques;radar imaging;minimax techniques image denoising image restoration wavelet transforms white noise radar imaging;image denoising;point of view;white noise;besov space	We present an approach for image denoising based on the analysis of the local Ḧolder regularity. The method takes the point of view that denoising may be performed by increasing the H ölder regularity at each point. Under the assumption that the noise is additive and white, we show that our procedure is asymptotically minimax, provided the original signal belongs to a ball in some Besov space. Such a scheme is well adapted to the case where the image to be recovered is itself very irregular, e.g. nowhere differentiable with rapidly varying local regularity. The method is implemented through a wavelet analysis. We show an application to SAR image denoising where this technique yields good results compared to other algorithms.	algorithm;minimax;noise reduction;utility functions on indivisible goods;wavelet	Pierrick Legrand;Jacques Lévy Véhel	2003		10.1109/ICIP.2003.1247260	besov space;wavelet;image restoration;computer vision;mathematical optimization;mathematical analysis;mathematics;white noise;radar imaging;non-local means;video denoising;statistics;wavelet transform	ML	53.496214935929686	-67.82632692317733	126944
8e225be4620909ff31690a524096be92b0bfc991	an algorithmic overview of surface registration techniques for medical imaging	appearance;rigid body;surface representation;medical image;surface model;registration;feature;global optimization;free form surface;literature survey	This paper presents a literature survey of automatic 3D surface registration techniques emphasizing the mathematical and algorithmic underpinnings of the subject. The relevance of surface registration to medical imaging is that there is much useful anatomical information in the form of collected surface points which originate from complimentary modalities and which must be reconciled. Surface registration can be roughly partitioned into three issues: choice of transformation, elaboration of surface representation and similarity criterion, and matching and global optimization. The first issue concerns the assumptions made about the nature of relationships between the two modalities, e.g. whether a rigid-body assumption applies, and if not, what type and how general a relation optimally maps one modality onto the other. The second issue determines what type of information we extract from the 3D surfaces, which typically characterizes their local or global shape, and how we organize this information into a representation of the surface which will lead to improved efficiency and robustness in the last stage. The last issue pertains to how we exploit this information to estimate the transformation which best aligns local primitives in a globally consistent manner or which maximizes a measure of the similarity in global shape of two surfaces. Within this framework, this paper discusses in detail each surface registration issue and reviews the state-of-the-art among existing techniques.	algorithm;alignment;ct scan;categories;categorization;choose (action);computation (action);computational complexity theory;contribution;distance transform;drop measurement;estimated;finite difference;foundations;global optimization;iterative closest point;iterative method;kalman filter;matching;map;mathematical optimization;mathematics;medical imaging;modality (human–computer interaction);muscle rigidity;musculoskeletal diseases;numerical analysis;online and offline;polygonal modeling;polynomial;preprocessor;relevance;review [publication type];sparse matrix;spline (mathematics);stimulation (motivation);succession;surface acoustic wave device component;z, rat strain;cell transformation;registration - actclass;soft tissue	Michel A. Audette;Frank P. Ferrie;Terry M. Peters	2000	Medical image analysis	10.1016/S1361-8415(00)00014-1	computer vision;mathematical optimization;rigid body;feature;computer science;artificial intelligence;machine learning;mathematics;global optimization	Vision	45.39101632626838	-76.14797918609288	126993
f49cecc305eb63ee94bc3556e1929b3cad941917	a device enhancing and denoising algorithm for x-ray cardiac fluoroscopy	clinical data;filtering;feature detection;x ray imaging;cardiology;clinical data denoising algorithm x ray cardiac fluoroscopy x ray interventional imaging x ray dose image denoising fluoroscopic image sequences interventional cardiology spatial temporal filtering feature detection;temporal filtering;noise reduction filtering x ray imaging motion detection image sequences filters algorithm design and analysis optical imaging cardiology computer vision;feature extraction;medical image processing;noise reduction;pixel;image sequence;image denoising;synthetic data;algorithm design and analysis;diagnostic radiography;medical image processing cardiology diagnostic radiography feature extraction image denoising image sequences;noise;image sequences;x rays	In X-ray interventional imaging, improved denoising enables lower X-ray dose and better visualization, resulting in increased confidence in the therapeutic act. This article focuses on the denoising of fluoroscopic image sequences in interventional cardiology. The challenge with these images is the need to preserve fine details in terms of size and contrast, representing the tools manipulated by the operator. These tools superimpose over an anatomical background. This context drives us to propose an algorithm based on spatial-temporal filtering conditioned by a feature of interest map. Based on the confidence in the feature detection our algorithm will either filter, preserve or enhance the image content. Our method is fast and compares very favorably with state of the art methods both quantitatively on synthetic data and perceptually on clinical data.	algorithm;algorithmic efficiency;edge detection;experiment;feature detection (computer vision);feature detection (web development);noise reduction;numerical analysis;synthetic data;thresholding (image processing);wavelet	Vincent Bismuth;Régis Vaillant	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761642	filter;algorithm design;computer vision;feature extraction;computer science;noise;noise reduction;feature detection;pixel;synthetic data	Vision	49.82487613613725	-77.4574569778658	127232
5a5ae4015e3f452c6eaa036f86136b884c151593	medical image fusion by combining parallel features on multi-scale local extrema scheme		Two efficient image fusion algorithms are proposed for constructing a fused image through combining parallel features on multi-scale local extrema scheme. Firstly, the source image is decomposed into a series of smoothed and detailed images at different scales by local extrema scheme. Secondly, the parallel features of edge and color are extracted to get the saliency maps. The edge saliency weighted map aims to preserve the structural information using Canny edge detection operator; Meanwhile, the color saliency weighted map works for extracting the color and luminance information by context-aware operator. Thirdly, the average and weighted average schemes are used as the fusion rules for grouping the coefficients of weighted maps obtained from smoothed and detailed images. Finally, the fused image is reconstructed by the fused smoothed and the fused detailed images. Experimental results demonstrate that the proposed algorithms show the best performances among the other fusion methods in the domain of MRI-CT and MRI-PET fusion.	image fusion;maxima and minima	Jiao Du;Weisheng Li;Bin Xiao;Qamar Nawaz	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.09.008	computer vision;machine learning;pattern recognition	Vision	47.20898014022585	-70.37877153126603	127374
64ac2d657c47f41cb00716c002ffe30f0021eccf	how are siblings similar? how similar are siblings? large-scale imaging genetics using local image features	twin data local invariant image features genetics 3d sift rank head mri;feature extraction artificial neural networks manifolds magnetic resonance imaging transforms genetics geometry;probability biomedical mri brain feature extraction image registration medical image processing neurophysiology;genetics;3d sift rank;head mri;local invariant image features;manifold proximity large scale imaging genetics local image features 3d medical image data image manifold proximity graph bags of features local invariant feature extraction pairwise geodesic distance jaccard distance metric bof overlap nearest neighbor feature correspondences computational framework registration error genetic proximity t1 weighted head mri nontwin siblings probability bof distance frontal cortices;twin data	A novel method is developed for analyzing large sets of 3D medical image data. The image manifold is approximated as a proximity graph between bags-of-features (BoFs), i.e. order-less sets of local invariant features extracted from images. The pairwise geodesic distance between images (or BoFs) is approximated locally by the Jaccard distance metric, which measures BoF overlap based on sets of nearest neighbor (NN) feature correspondences computed across a data set. An efficient computational framework allows the method to scale gracefully with the amount of data. Registration error associated with NN correspondences is shown to decrease with increasing numbers of images N. Experiments demonstrate strong links between manifold and genetic proximity, from T1-weighted head MRI of 511 twin and non-twin siblings. Siblings are automatically identified with high probability based on BoF distance. NN correspondences are more numerous in the frontal cortices of twin vs. non-twin siblings. An erroneous duplicate subject is identified based on manifold proximity.	approximation algorithm;distance (graph theory);experiment;jaccard index;medical imaging;with high probability	Matthew Toews;William M. Wells	2016	2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2016.7493398	computer vision;machine learning;pattern recognition;mathematics	Vision	42.538311624582924	-77.12633458780357	127417
c8ea9f3da097b9306c03477fc9c0d1bc03ac0beb	a semi-automatic method to determine electrode positions and labels from gel artifacts in eeg/fmri-studies	point matching;hungarian algorithm;article letter to editor;eeg fmri;electrode positions	"""The analysis of simultaneous EEG and fMRI data is generally based on the extraction of regressors of interest from the EEG, which are correlated to the fMRI data in a general linear model setting. In more advanced approaches, the spatial information of EEG is also exploited by assuming underlying dipole models. In this study, we present a semi automatic and efficient method to determine electrode positions from electrode gel artifacts, facilitating the integration of EEG and fMRI in future EEG/fMRI data models. In order to visualize all electrode artifacts simultaneously in a single view, a surface rendering of the structural MRI is made using a skin triangular mesh model as reference surface, which is expanded to a """"pancake view"""". Then the electrodes are determined with a simple mouse click for each electrode. Using the geometry of the skin surface and its transformation to the pancake view, the 3D coordinates of the electrodes are reconstructed in the MRI coordinate frame. The electrode labels are attached to the electrode positions by fitting a template grid of the electrode cap in which the labels are known. The correspondence problem between template and sample electrodes is solved by minimizing a cost function over rotations, shifts and scalings of the template grid. The crucial step here is to use the solution of the so-called """"Hungarian algorithm"""" as a cost function, which makes it possible to identify the electrode artifacts in arbitrary order. The template electrode grid has to be constructed only once for each cap configuration. In our implementation of this method, the whole procedure can be performed within 15 min including import of MRI, surface reconstruction and transformation, electrode identification and fitting to template. The method is robust in the sense that an electrode template created for one subject can be used without identification errors for another subject for whom the same EEG cap was used. Furthermore, the method appears to be robust against spurious or missing artifacts. We therefore consider the proposed method as a useful and reliable tool within the larger toolbox required for the analysis of co-registered EEG/fMRI data."""	artifact (error);clinical use template;correspondence problem;data model;electroencephalography;event (computing);general linear model;hungarian algorithm;hungarian language;ion-selective electrodes;large;loss function;maxima and minima;morphologic artifacts;reference surface;registration;robustness (computer science);semiconductor industry;electrode;fmri	Jan C. de Munck;Petra J. van Houdt;Ruud M. Verdaasdonk;Pauly P. W. Ossenblok	2012	NeuroImage	10.1016/j.neuroimage.2011.07.021	eeg-fmri;computer science;artificial intelligence;machine learning;hungarian algorithm	Vision	47.06879716783448	-78.16831046699735	127562
296d5b4a65be1b55bf22f8d56597ba99a45d8689	denoising of natural stochastic colored-textures based on fractional brownian motion model	natural image statistics texture enhancement natural stochastic textures denoising color images;textural image component natural stochastic colored texture denoising fractional brownian motion model image processing color nst image structure analysis gaussian model maximum a posteriori scheme map scheme color nst patch wise denoising boosting algorithm image denoising cartoon type component;image color analysis noise reduction correlation noise measurement estimation boosting stochastic processes;maximum likelihood estimation brownian motion gaussian processes image colour analysis image denoising image texture	Denoising of Natural Stochastic Colored-Textures (color NST) is of special interest in image processing. Existing algorithms produce over-smoothed images with sharp edges, and do not restore the fine textural color details. We analyze the structure of color NST images and propose a simple model. This model is Gaussian and has a low number of parameters that can be estimated efficiently. A maximum-a-posteriori (MAP) scheme is proposed for patch-wise denoising of color NST. The denoised images exhibit better restored textural details compared to existing algorithms. A boosting algorithm is proposed for denoising of complex images containing both cartoon-type and textural image components.	algorithm;boosting (machine learning);brownian motion;image processing;network security toolkit;noise reduction;smoothing	Ido Zachevsky;Yehoshua Y. Zeevi	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7350963	image texture;computer vision;mathematical optimization;pattern recognition;mathematics;non-local means	Vision	52.84267343517776	-68.61600536747643	127623
ca328ddb3e5fd00d77be1aec53d68640f9bc7f4b	a statistical multiscale framework for poisson inverse problems	poisson process;astronomical energy spectral analysis statistical multiscale framework poisson inverse problems linear inverse problems multiscale analysis recursive partitioning intensity multiscale factorization prior probability distribution maximum likelihood solution prior belief smoothness unknown intensity expectation maximization algorithm maximum a posteriori estimate closed form expression emission computed tomography;recursive partitioning;image reconstruction inverse problems spectral analysis astronomical techniques emission tomography medical image processing bayes methods poisson distribution iterative methods maximum likelihood estimation;bayes methods;multiscale analysis;indexing terms;maximum likelihood estimation;statistical model;iterative methods;poisson processes;inverse problem;image reconstruction;medical image processing;emission tomography;probability distribution;imaging;spectral analysis;astronomical techniques;poisson distribution;inverse problems	"""This paper describes a statistical multiscale modeling and analysis framework for linear inverse problems involving Poisson data. The framework itself is founded upon a multiscale analysis associated with recursive partitioning of the underlying intensity, a corresponding multiscale factorization of the likelihood (induced by this analysis), and a choice of prior probability distribution made to match this factorization by modeling the """"splits"""" in the underlying partition. The class of priors used here has the interesting feature that the """"noninformative"""" member yields the traditional maximum-likelihood solution; other choices are made to reflect prior belief as to the smoothness of the unknown intensity. Adopting the expectation-maximization (EM) algorithm for use in computing the maximum a posteriori (MAP) estimate corresponding to our model, we find that our model permits remarkably simple, closed-form expressions for the EM update equations. The behavior of our EM algorithm is examined, and it is shown that convergence to the global MAP estimate can be guaranteed. Applications in emission computed tomography and astronomical energy spectral analysis demonstrate the potential of the new approach."""		Robert D. Nowak;Eric D. Kolaczyk	2000	IEEE Trans. Information Theory	10.1109/18.857793	medical imaging;econometrics;mathematical optimization;inverse problem;mathematics;statistics	Robotics	52.72100623053696	-74.59846344818276	127953
3afcd83b9304d45252cb066130843008a0724217	accurate registration of dynamic contrast-enhanced breast mr images with robust estimation and linear programming	dynamic programming robustness linear programming breast neoplasms deformable models image registration magnetic resonance imaging radiology spline pixel;spline;robust estimator;cancer;biological organs;tumours;breast;breast imaging;dynamic contrast enhanced;mr imaging;estimation;medical image processing;image registration;tumors;linear programming;linear program;robustness;global optimization;optimization;tumours biological organs biomedical mri cancer image registration linear programming mammography medical image processing;tumor expanding accurate registration dynamic contrast enhanced mri breast tumor shrinking image similarity robust estimation global optimization linear programming;mammography;similarity measure;biomedical mri;image similarity	Accurate registration of the dynamic contrast-enhanced (DCE) MR breast images is a well-known difficult problem. It is because the breast motion is non-rigid and the intensity variations of tumor between pre-contrast and post-contrast images can cause the unexpected tumor shrinking/expanding effect in the registration process. To obtain accurate registration, we propose two techniques: a novel image similarity measure based on the robust estimation and a new global optimization technique by reformulating the registration problem as solving a linear programming. The novel similarity measure can help to handle the shrinking/expanding problem while the global optimization technique offers more accurate estimation of the breast motion.	contrast ratio;global optimization;linear programming;mathematical optimization;similarity measure	Yuanjie Zheng;Andrew D. A. Maidment;James C. Gee	2010	2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2010.5490290	spline;robust statistics;computer vision;mathematical optimization;estimation;computer science;linear programming;image registration;mathematics;robustness;cancer;medical physics	Vision	45.748745206252494	-76.56581826110768	128208
2001b3d73b04d4afce901049bbf9ed3cf6f9ef79	two-dimensional arma modeling for breast cancer detection and classification	biological organs;biomedical ultrasonics;cancer;gynaecology;image classification;least squares approximations;medical image processing;regression analysis;tumours;2d moving average parameters;wold decomposition theorem;benign lesion;breast cancer detection;cancer classification;cancerous tumor;computer aided diagnosis paradigms;malignant lesion;mathematical tractability;one-dimensional time-series;statistical inference;two-dimensional arma modeling;two-dimensional autoregressive-moving-average model;two-stage yule-walker least-squares algorithm;ultrasound breast images;vector representations;breast cancer;k-means algorithm;two-dimensional arma models;random field;time series;moving average;k means algorithm;least square;solid modeling;arma model;parameter estimation;spatial correlation;mathematical model;ultrasound;k means	Computer aided diagnosis (CAD) paradigms have gained currency for discriminating malignant from benign lesions in ultrasound breast images. But even the most sophisticated investigators often rely on one-dimensional representations of the image in terms of its scanlines. Such vector representations are convenient because of the mathematical tractability of one-dimensional time-series. However, they fail to take into account the spatial correlations between the pixels, which is crucial in tumor detection and classification in breast images. In this paper, we propose a CAD system for tumor detection and classification (cancerous v.s. benign) in ultrasound breast images based on a two-dimensional Auto-Regressive-Moving-Average (ARMA) model of the breast image. First, we show, using the Wold decomposition theorem, that ultrasound breast images can be accurately modeled by two-dimensional ARMA random fields. As in the 1D case, the 2D ARMA parameter estimation problem is much more difficult than its 2D AR counterpart, due to the non-linearity in estimating the 2D moving average (MA) parameters. We propose to estimate the 2D ARMA parameters using a two-stage Yule-Walker Least-Squares algorithm. The estimated parameters are then used as the basis for statistical inference and biophysical interpretation of the breast image. We evaluate the performance of the 2D ARMA vector features in real ultrasound images using a k-means classifier. Our results suggest that the proposed CAD system based on a two-dimensional ARMA model leads to parameters that can accurately segment the ultrasound breast image into three regions: healthy tissue, benign tumor, and cancerous tumor. Moreover, the specificity and sensitivity of the proposed two-dimensional CAD system is superior to its one-dimensional homologue.	algorithm;amiga walker;computer-aided design;estimation theory;k-means clustering;least squares;nonlinear system;pixel;scan line;sensitivity and specificity;time series	Nidhal Bouaynaya;Jerzy S. Zielinski;Dan Schonfeld	2010	2010 International Conference on Signal Processing and Communications (SPCOM)		speech recognition;machine learning;mathematics;statistics;k-means clustering	Robotics	43.04674501033145	-75.7085445814063	128239
aa0c37ac9cbe4d363da70a63479569338d1f2e12	3d ct spine data segmentation and analysis of vertebrae bone lesions	lesions three dimensional displays computed tomography image segmentation biomedical imaging bones;3d intensity modeling 3d ct spine data segmentation 3d ct spine data analysis vertebrae bone lesions bone lesion classification human spine bayesian approach markov random fields heavily distorted vertebrae;image segmentation;bayes methods;image classification;data analysis;medical image processing;random processes;bone;computerised tomography;markov processes;random processes bayes methods bone computerised tomography data analysis image classification image segmentation markov processes medical image processing	A method is presented aiming at detecting and classifying bone lesions in 3D CT data of human spine, via Bayesian approach utilizing Markov random fields. A developed algorithm for necessary segmentation of individual possibly heavily distorted vertebrae based on 3D intensity modeling of vertebra types is presented as well.	algorithm;bone structure of spine;ct scan;illness (finding);markov chain;markov random field;sensor;statistical classification;biologic segmentation;bone lesion	Roman Peter;Milos Malinsky;Petr Ourednicek;Jirí Jan	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610016	stochastic process;computer vision;contextual image classification;radiology;pathology;computer science;mathematics;image segmentation;markov process;data analysis;statistics	Robotics	43.06472893503662	-76.29530444362726	128299
60ccd07e339e212ef97c8db1c9185a81f72ff6da	spinal canal centerline extraction in mri		The centerline of the spinal canal holds interesting information which can be used for tasks such as segmenting the spinal canal or to track the progression of spinal deformities. We propose a method that extracts the centerline of the canal by a shortest path search in 4D, whereby dimensions correspond to 3D canal location and canal width. Our method requires only minimal user interaction in the form of two seed points to extract the centerline over the whole length of the spinal canal. We reconstruct the shortest path using a second-order fast marching scheme on a vesselness-based energy. Our approach was evaluated on an MRI dataset of 103 subjects with encouraging results and proved viable with a variety of different parameterizations.	color gradient;fast marching method;ground truth;maxima and minima;shortest path problem;visual analytics;voxel	Stephan Fensky;Fabian P Held;Marko Rak;Klaus D. Tönnies	2015			spinal canal;radiology;computer science	Vision	40.830856323311764	-77.81857719764851	128354
13aa5a1179a1a3ec94c2872033f81962e8b019d6	a color-texture segmentation method to extract tree image in complex scene	image recognition;lab color space;mathematical morphology;image segmentation;colored noise;color space;image segmentation layout colored noise data mining image recognition feature extraction pixel noise reduction anisotropic filters color;two dimensions;color;color feature;image segmentation lab color space color feature texture feature color tree image;texture segmentation;anisotropic filters;layout;texture features;data mining;image texture;color segmentation;image colour analysis;feature extraction;noise reduction;pixel;mathematical morphology methods image color texture segmentation tree image extraction tree image recognition feature extraction image pixels anisotropic filter lab color space;gray level co occurrence matrix;color tree image;mathematical morphology feature extraction filtering theory image colour analysis image recognition image segmentation image texture;filtering theory;color image;texture feature	In order to provide basic data for tree image recognition and feature extraction, this paper studies the tree image segmentation in complex background. Based on the visual characteristics differences of the tree and the surrounding objects, the trees from different backgrounds are separated into a single set of tree image pixels. This paper proposes a segmentation method to extract object based on color and texture features of color tree images. Firstly, reduce noise of the color tree image by the use of anisotropic filter. Then, we select Lab color space as the space for image segmentation. And the color image of RGB space is transformed into Lab space. Next, due to the negative end of a-channel reflects the color feature of trees, the L, a, and b channels are split. Green is the main feature of tree images, so segmentation by two-dimension OTSU of automatic threshold in a-channel. Based on the color segmentation result, and the texture differences between the background image and the object tree, we extract the object tree by the gray level co-occurrence matrix for texture segmentation. Finally, the segmentation result is corrected by mathematical morphology methods. This method not only segmentation speed is faster, and without human participation, but also the segmentation result is ideal when there are not green plants which are so close to the object tree in the tree image background.	anisotropic filtering;co-occurrence matrix;color image;color space;computer vision;document-term matrix;feature extraction;grayscale;image segmentation;mathematical morphology;object-based language;otsu's method;pixel	Xiaosong Wang;Xinyuan Huang;Hui Fu	2010	2010 International Conference on Machine Vision and Human-machine Interface	10.1109/MVHI.2010.138	color histogram;image texture;layout;computer vision;range segmentation;mathematical morphology;colors of noise;color image;feature extraction;computer science;machine learning;segmentation-based object categorization;pattern recognition;noise reduction;region growing;image segmentation;color space;minimum spanning tree-based segmentation;scale-space segmentation;pixel	Vision	44.127673597610034	-66.98408890955847	128807
9111d31158f54c48889cf1abeed197994eec6cd1	a multi-agent system for mri brain segmentation	nuclear magnetic resonance imaging;analisis imagen;medical imagery;informatica biomedical;multiagent system;biomedical data processing;multi agent system;systeme nerveux central;performance;informatique biomedicale;prior information;hombre;segmentation;encefalo;region segmentation;imageria rmn;sistema nervioso central;a priori knowledge;informacion a priori;automatic detection;encephale;human;imagerie medicale;brain imaging;image analysis;imagerie rmn;imageneria medical;rendimiento;sistema multiagente;analyse image;information a priori;segmentacion;central nervous system;systeme multiagent;homme;brain vertebrata	In this paper we present an original approach for the segmentation of MRI brain images which is based on a cooperation between low-level and high-level approches. MRI brain images are very difficult to segment mainly due to the presence of inhomogeneities within tissues and also due to the high anatomical variability of the brain topology between individuals. In order to tackle these difficulties, we have developped a method whose characteristics are : (i) the use of a priori knowledge essentially anatomical and model-based ; (ii) a multi-agent system (MAS) for low-level region segmentation ; (iii) a cooperation between a priori knowledge and low-level segmentation to guide and constrain the segmentation processes. These characteristics allow to produce an automatic detection of the main tissues of the brain. The method is validated with phantoms and real images through comparisons with another widely used approach (SPM).	high- and low-level;multi-agent system;spatial variability;super paper mario	Laurence Germond;Michel Dojat;Christopher J. Taylor;Catherine Garbay	1999		10.1007/3-540-48720-4_47	computer vision;image analysis;a priori and a posteriori;performance;computer science;artificial intelligence;central nervous system;scale-space segmentation;segmentation	Vision	46.14109195830403	-78.96260117695624	128877
538140e5ebe59314733721aadd58eb2e516b77ab	individual tree detection from multi-view satellite images		Individual tree detection is critical in forest monitoring and inventory. In this paper, we propose a novel method to use multi-view satellite images to detect individual trees and delineate their crowns. As compared to previous methods that only use image information, we generate the DSM from the multi-view high-resolution satellite images and combine it with the spectral information to detect the trees. Firstly, the vegetation areas are extracted to remove the non-vegetation objects while terrain areas are extracted to help estimate the tree height. Then, we utilize top-hat morphological operation to efficiently find the local maximal points as treetops and further refine them by checking their heights and doing non-maximum suppression. Finally, we use a revised superpixel segmentation algorithm to delineate the tree crowns which considered both 2D spectral and 3D structure similarities. To effectively assess the performance, we rigorously match and evaluate the detected and reference trees in a one-to-one relationship. A quantitative evaluation at three different sites shows that the proposed method is able to detect individual trees at different regions with high accuracy.	algorithm;free viewpoint television;image resolution;maximal set;one-to-one (data model);tree (data structure);zero suppression	Changlin Xiao;Rongjun Qin;Xu Huang;Jiaqiang Li	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518040	terrain;computer vision;vegetation;artificial intelligence;computer science	Vision	46.173728301158015	-66.94195871597613	128878
3a0f0880172256854d3c4a659ad464db7b0b4db3	robust color edge detection through tensor voting	saliency map color edge detection tensor voting robust perceptual grouping salient information extraction noisy data voting process region uniformity perceptual color difference;robustness image edge detection tensile stress voting colored noise color detectors eigenvalues and eigenfunctions intelligent robots computer vision;tensile stress;colored noise;saliency map;edge detection;noisy data;color;tensor voting;ciede2000 image edge analysis tensor voting cielab;ciede2000;perceptual grouping;noise measurement;image edge detection;image color analysis;image colour analysis;cielab;feature extraction;pixel;tensors edge detection feature extraction image colour analysis;image edge analysis;tensors	This paper presents a new method for color edge detection based on the tensor voting framework, a robust perceptual grouping technique used to extract salient information from noisy data. The tensor voting framework is adapted to encode color information via tensors in order to propagate them into a neighborhood through a voting process specifically designed for color edge detection by taking into account perceptual color differences, region uniformity and edginess according to a set of intuitive perceptual criteria. Perceptual color differences are estimated by means of an optimized version of the CIEDE2000 formula, while uniformity and edginess are estimated by means of saliency maps obtained from the tensor voting process. Experiments show that the proposed algorithm is more robust and has a similar performance in precision when compared with the state-of-the-art.	algorithm;circuit complexity;encode;edge detection;map;signal-to-noise ratio	Rodrigo Moreno;Miguel Angel García;Domenec Puig;Carme Julià	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414337	computer vision;colors of noise;edge detection;feature extraction;computer science;noise measurement;machine learning;pattern recognition;mathematics;pixel	Vision	44.26366186140437	-66.58830889142934	128929
a8fbe1b30f7c6d8aa774f153966ad21d51ea79f8	alignment and morphing for the boundary curves of anatomical organs	tracking method;anatomical organ;variational registration method;deformable boundary curve;intermediate boundary curve;total distortion;biological organ;relative distortion;minimum distortion;corresponding point	In this paper, we develop a tracking method for the deformable boundary curves of biological organs using variational registration method. We first define the relative distortion of a pair of curves using curvatures of curves. This minimum distortion aligns corresponding points of a pair of curves. Then, we derive the mean of curves as the curve which minimises the total distortion of a collection of shapes. We compute the intermediate boundary curve of a pair of curves as the mean of these curves.	morphing	Keiko Morita;Atsushi Imiya;Tomoya Sakai;Hidekaka Hontan;Yoshitaka Masutani	2012		10.1007/978-3-642-34166-3_50	mathematical optimization;geometric design;topology;differential geometry of curves;mathematics;geometry;family of curves	Vision	46.363582443448244	-76.16468012069632	128981
f0189ad3a8f91b002c8849a575f86d587adc4976	locally monotonic models for image and video processing	image segmentation;multidimensional systems signal processing algorithms signal analysis digital images automotive engineering vehicles smoothing methods noise reduction rendering computer graphics image enhancement;video coding image segmentation image enhancement;video processing;video coding;image enhancement;video coding locally monotonic models video processing image processing model definitions video frame input local monotonicity image smoothing denoising scale space generation image enhancement image segmentation	Definitions of locally monotonic images are introduced. The model definitions are complemented with algorithms that compute locally monotonic versions of a given image or video frame input. The property of local monotonicity provides a useful vehicle for image smoothing and denoising. Local monotonicity is also useful for scale space generation, wherein the degree of local monotonicity is the scale parameter. Currently, the property of local monotonicity is well defined for the 1-D case, but is not well defined for images or video. In this paper, models for multidimensional local monotonicity that extend the 1-D definition are rendered. Regression-based and diffusion-based processing methods are prescribed that yield meaningful locally monotonic images. The definitions and associated algorithms are applicable to image enhancement and a variety of multiscale tasks such as image segmentation and video coding.	video processing	Scott T. Acton;Alfredo Restrepo	1999		10.1109/ICIP.1999.822932	image texture;computer vision;feature detection;image processing;computer science;theoretical computer science;digital image processing;mathematics;multimedia;video processing;image segmentation;scale-space segmentation;video post-processing;video denoising	Vision	52.813073331755504	-66.72828955756806	129255
b6c726faa64cfa1c2a8a0675736cb7952466651d	low-level processing of polsar images with binary partition trees	speckle;image segmentation;speckle noise;segmentation;polsar;binary partition tree;speckle filtering low level polsar image processing binary partition trees graph cuts image segmentation image classification object detection pruning;covariance matrices;graph cut;conference report;merging;optimization;trees mathematics geophysical image processing image classification image segmentation radar imaging radar polarimetry remote sensing by radar synthetic aperture radar;segmentation binary partition tree polsar graph cut speckle noise;speckle merging noise covariance matrices image segmentation context optimization;context;noise	This paper discusses the interest of Binary Partition Trees (BPTs) and the usefulness of graph cuts for low-level processing of PolSAR images. BPTs group pixels to form homogeneous regions, which are hierarchically structured by inclusion in a tree. They provide multiple resolutions of description and easy access to subsets of regions. Once constructed, BPTs can be used for many applications including filtering, segmentation, classification and object detection. Many processing strategies consist in populating the tree with a specific feature and in applying a graph-cut called pruning. Different graph-cuts are discussed and analyzed in the context of PolSAR images for speckle filtering and segmentation.	accessibility;cut (graph theory);high- and low-level;image segmentation;object detection;partition type;pixel;population;statistical classification	Philippe Salembier;Samuel Foucher;Carlos López-Martínez	2014	2014 IEEE Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2014.6946602	speckle pattern;speckle noise;computer vision;cut;computer science;noise;machine learning;pattern recognition;mathematics;image segmentation;segmentation;physics	Vision	44.706528097148436	-68.05595048243865	129317
3b8b1e4dd5412bbc4a9ee4ba5ed7ef5d4545fb74	multichannel image identification and restoration using continuous spatial domain modeling	domain model;ar model;image colour analysis maximum likelihood estimation image restoration autoregressive processes optical noise;degradation;image processing;colored noise;minerals;optical noise;color;multichannel image identification;cross spectral components;performance;out of focus blur;restoration;image restoration;maximum likelihood estimation;uniform linear motion;autoregressive model;noisy color images;petroleum;maximum likelihood estimate;spatial components;autoregressive processes;image colour analysis;out of focus blur multichannel image identification restoration continuous spatial domain modeling image processing maximum likelihood estimation autoregressive model blur cross spectral components spatial components performance noisy color images uniform linear motion;image restoration maximum likelihood estimation image processing colored noise color degradation;continuous spatial domain modeling;blur;wiener filter;computer science;color image	In this paper, a novel identification technique for multichannel image processing is presented. Using the maximum likelihood estimation (ML) approach, the image is represented as an autoregressive (AR) model and blur is described as a continuous spatial domain model. Such a formulation overcomes some major limitations encountered in other ML methods. Moreover, cross-spectral and spatial components are incorporated in the multichannel modeling. It is shown that by incorporating those components, the overall performance is improved significantly. Also, experimental results show that blur extent can be optimally identified from noisy color images that are degraded by uniform linear motion or out-of-focus blurs.	circuit restoration	U. A. Al-Suwailem;J. Keller	1997		10.1109/ICIP.1997.638809	computer vision;speech recognition;image processing;pattern recognition;mathematics;maximum likelihood;autoregressive model;statistics	Vision	53.7133549290201	-66.26677157477496	129333
3b6fb9b7709409a050db788d2d577f03977b7d04	optimizing non-local means for denoising low dose ct	filtering;low dose;low dose ct;psnr;image processing;computed tomography;weighted averaging;image processing non local means optimal parameters low dose ct denoising;radiation induced cancer;ct imaging;non local means;nonlocal means;noise reduction computed tomography pixel information filtering information filters image reconstruction imaging phantoms signal to noise ratio psnr biomedical imaging;smoothing methods;image reconstruction;medical image processing;low dose ct image denoising;noise reduction;peak signal to noise ratio;pixel;computerised tomography;low dose images;nonlocal means low dose ct image denoising ct imaging radiation induced cancer low dose ct diagnostic quality signal to noise ratio low dose images;optimal parameters;image denoising;denoising;signal to noise ratio;medical image processing computerised tomography diagnostic radiography image denoising;low dose ct diagnostic quality;diagnostic radiography	Due to the rapid increase in use of CT imaging and the recently-heightened awareness of radiation-induced cancer, improving the diagnostic quality of low dose CT has become increasingly important. One potential method is to increase the signal-to-noise ratio of low dose images through denoising. Non-local means is a promising approach; however, it has many potentially adjustable parameters and application-specific areas of improvement. The filter uses a weighted average of similar regions to denoise each image pixel. Though the classic formulation uses only patches from the image being filtered, these patches can, in principle, be drawn from other images. In CT images, patches can be drawn from neighboring slices. We used that potential to increase the peak signal-to-noise ratio (PSNR) by over 4 dB when denoising low dose phantom CT images, and quantitatively demonstrated the filter's sensitivity to adjustment of each of its parameters.	ct scan;noise reduction;non-local means;optimizing compiler;peak signal-to-noise ratio;phantom reference;pixel;potential method	Zachary Kelm;Daniel J. Blezek;Brian J. Bartholmai;Bradley James Erickson	2009	2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2009.5193134	computer vision;radiology;peak signal-to-noise ratio;image processing;computer science;noise reduction;mathematics;medical physics	Vision	47.56803956578184	-74.13629186889688	129557
37acf4acc50868b5b9a3c97deacee375572b2197	visualization of brain microstructure through spherical harmonics illumination of high fidelity spatio-angular fields	neurophysiology biodiffusion biological tissues biomedical mri biomedical optical imaging brain data visualisation injuries medical image processing;lighting harmonic analysis tensile stress brain modeling diffusion tensor imaging biomedical image processing rendering computer graphics;tensile stress;brain modeling;diffusion kurtosis imaging;biomedical image processing;spatio angular fields;lighting;spherical harmonics lighting functions brain microstructure visualization spherical harmonics illumination high fidelity spatio angular fields diffusion kurtosis imaging medical imaging community non gaussian property water diffusion biological tissues traditional diffusion tensor imaging microstructural characteristics neural tissues gray matter mild traumatic brain injury dki datasets glyph based visualization techniques dti dataset visualization orientation occlusion higher fidelity dki data;tensor fields;diffusion tensor imaging;rendering computer graphics;spherical harmonics fields;harmonic analysis	Diffusion kurtosis imaging (DKI) is gaining rapid adoption in the medical imaging community due to its ability to measure the non-Gaussian property of water diffusion in biological tissues. Compared to traditional diffusion tensor imaging (DTI), DKI can provide additional details about the underlying microstructural characteristics of the neural tissues. It has shown promising results in studies on changes in gray matter and mild traumatic brain injury where DTI is often found to be inadequate. The DKI dataset, which has high-fidelity spatio-angular fields, is difficult to visualize. Glyph-based visualization techniques are commonly used for visualization of DTI datasets; however, due to the rapid changes in orientation, lighting, and occlusion, visually analyzing the much more higher fidelity DKI data is a challenge. In this paper, we provide a systematic way to manage, analyze, and visualize high-fidelity spatio-angular fields from DKI datasets, by using spherical harmonics lighting functions to facilitate insights into the brain microstructure.	angularjs;approximation algorithm;basis function;brain injuries;data point;diffusion kurtosis imaging;diffusion tensor imaging;glyph;gray matter;imagery;legendre wavelet;medical imaging;nerve tissue;normal statistical distribution;silo (dataset);spherical basis;traumatic brain injury;nervous system disorder	Sujal Bista;Jiachen Zhuo;Rao P. Gullapalli;Amitabh Varshney	2014	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2014.2346411	diffusion mri;computer vision;tensor field;computer science;harmonic analysis;lighting;mathematics;stress;computer graphics (images)	Visualization	48.45337169503486	-78.60712794791255	130261
8baba9da1fab0ea6c80db9784ed11e23d5b4ff9e	fourier cross-sectional profile for vessel detection on retinal images	feature detection;edge detection;asymmetry and symmetry;blood vessel;vessel detection;retinal images;generalized gaussian;cross section;cross sectional profile;retinal imaging;medial axis	Retinal blood vessels are important objects in ophthalmologic images. In spite of many attempts for vessel detection, it appears that existing methodologies are based on edge detection or modeling of vessel cross-sectional profiles in intensity. The application of these methodologies is hampered by the presence of a wide range of retinal vessels. In this paper we define a universal representation for upward and downward vessel cross-sectional profiles with varying boundary sharpness. This expression is used to define a new scheme of vessel detection based on symmetry and asymmetry in the Fourier domain. Phase congruency is utilized for measuring symmetry and asymmetry so that our scheme is invariant to vessel brightness variations. We have performed experiments on fluorescein images and color fundus images to show the efficiency of the proposed algorithm technique. We also have performed a width measurement study, using an optimal medial axis skeletonization scheme as a post-processing step, to compare the technique with the generalized Gaussian profile modeling. The new algorithm technique is promising for automated vessel detection where optimizing profile models is difficult and preserving vessel width information is necessary.	apache axis;axis vertebra;blood vessel tissue;cross-sectional data;edge detection;experiment;fluorescein;medial graph;normal statistical distribution;ophthalmology specialty;phase congruency;structure of blood vessel of retina;video post-processing;algorithm;brightness;width	Tao Zhu	2010	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1016/j.compmedimag.2009.09.004	computer vision;edge detection;medial axis;computer science;feature detection;mathematics;cross section;optics	Vision	40.15182701904631	-75.8934058401371	130282
d0892bfe66f0815595c774f8777bbbf35c950852	automatic detection of coronary stenosis in x-ray angiography through spatio-temporal tracking	spatiotemporal phenomena blood vessels cardiovascular system diagnostic radiography image sequences medical image processing object detection object tracking;spatio temporal analysis x ray angiography automatic stenosis detection;arteries image segmentation angiography x ray imaging junctions robustness;automatic detection stenosis detection single frame analysis stenosis locations minima image frame isolated artery segment arterial width surface coronary artery tree angiography sequences vessel detection large intensity gradients surrounding tissue x ray angiography data spatiotemporal tracking coronary stenosis	Automatic detection of coronary stenosis in X-ray angiography data is a challenging problem. The low contrast between vessels and surrounding tissue, as well as large intensity gradients within the image, make detection of vessels and stenoses difficult. In this paper we exploit the spatiotemporal nature of the angiography sequences to present a robust method for automatically isolating the coronary artery tree. An arterial width surface is formed for each isolated artery segment by calculating the width along a segment and tracking the segment in each image frame over time. A persistent minima of this surface then corresponds to a stenosis in the artery. Results of testing on a variety of stenosis locations in various coronary arteries are presented and compared to stenosis detected from single frame analysis. This method is able to detect the presence of stenosis in an artery segment with a sensitivity of 86% and a specificity of 97% on 16 patients with a total of 20 image runs. This is the first fully automatic method for stenosis detection in X-ray angiography.	gradient;maxima and minima;persistent homology;sensitivity and specificity	Colin B. Compas;Tanveer F. Syeda-Mahmood;Patrick McNeillie;David Beymer	2014	2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2014.6868115	computer vision;radiology;cardiology	Vision	40.360718153467	-78.81043967667335	130306
1d8008233cb89bd83f58fcdb814d5209b34cd5cb	fabric defect detection based on biological vision modeling		Fabric defect detection plays a key role in the quality control of textiles. Existing fabric defect detection methods adopt traditional pattern recognition methods; however, these methods lack adaptability and present poor detection performance. Because biological vision system has the ability to quickly locate salient objects, we propose a novel fabric defect detection algorithm based on biological vision modeling by simulating the mechanism of biological visual perception. First, a distinct, efficient, and robust feature descriptor from the biological modeling of P ganglion cells, which was proposed in our previous work, is adopted to improve the representation of fabric images with complex textures. To account for the low-rank and sparsity characteristics of biological vision, the low-rank representation (LRR) technique is adopted to model biological visual saliency, and it can decompose the fabric image into backgrounds and salient defect objects. Meanwhile, dictionary learning and Laplacian regularization are integrated into the LRR model as follows: 1) dictionary learning is used to denoise the saliency map; and 2) Laplacian regularization enlarges the gaps between defective regions and the background. Finally, the linearized accelerated direction method with adaptive penalty is adopted to solve the proposed model. The experimental results emphasize that the proposed algorithm has good detection performance for plain or twill fabrics with simple textures as well as for patterned fabrics with complex textures. Moreover, the proposed method is superior to the state-of-the-art methods in terms of its adaptability and detection efficiency.	algorithm;angular defect;dictionary;laplacian matrix;low-rank approximation;machine learning;matrix regularization;noise reduction;pattern recognition;simulation;software bug;sparse matrix;visual descriptor	Chunlei Li;Guangshuai Gao;Zhoufeng Liu;Miao Yu;Di Huang	2018	IEEE Access	10.1109/ACCESS.2018.2841055	computer vision;adaptability;visualization;machine vision;distributed computing;salience (neuroscience);saliency map;computer science;laplace operator;visual perception;artificial intelligence	Vision	48.378904868862655	-66.53295178328236	130415
b38efcd924eaea887570297c10501964ce913360	enhanced semi-automated method to identify the endo-cardium and epi-cardium borders		We present two semi-automatic solution methods for the three dimensional (3D) segmentation of cavity and myocardium from a 3D cardiac multislice CT (MSCT) data. The main framework of the segmentation algorithms is based on random walks, in which the novelty lies in a seeds-selection method composed of region growing technique and morphological operation to locate and identify the cavity andmyocardium of the left ventricle (LV). In the first solution, a semi-automatic segmentation approach (Method_1) is suggested to extract the epi-cardium and endo-cardium boundaries of LV of the heart. This proposed solution depends on the use of the normal random walker algorithm. In the second solution, a semiautomatic segmentation approach (Method_2) based on improved randomwalks is proposed. Segmentation is donewithin the framework of toboggan algorithm in combination with a random walk based technique.The twoproposedsemi-automatic segmentationmethodseither based on the normal random walker or the improved random walker algorithms utilized six-connected lattice topology and a conjugate gradient method to promote the segmentation performance of the 3D data. The two semi-automatic solution methods were evaluated using 20 cardiac MSCT datasets. Semi-automatic generated contours were compared to expert contours. ForMethod_1, 83.4%of epi-cardial contours and74.7%of endo-cardial contours hadamaximumerror of 5 mm along 95% of the contour arc length. For Method_2, those numbers were 94.3% (epi-cardium) and 92.3% (endo-cardium), respectively. Volume regression analysis revealed good linear correlations between manual and semiautomatic volumes, r ≥ 0.99. © 2012 SPIE and IS&T. [DOI: 10.1117/1.JEI.21.2.023024]	active appearance model;amiga walker;bland's rule;ct scan;conjugate gradient method;electronic filter topology;linear equation;logical volume management;mad;mathematical morphology;particle filter;point-to-point protocol;random walker algorithm;region growing;semiconductor industry;sparse matrix;system of linear equations	Osama S. Faragallah	2012	J. Electronic Imaging	10.1117/1.JEI.21.2.023024	computer vision;artificial intelligence;random walk;scale-space segmentation;regression analysis;pattern recognition;image segmentation;conjugate gradient method;random walker algorithm;multislice;region growing;mathematics	Vision	44.14473441384915	-74.27921341490429	130585
dd4515aa49cf374eac37ba599dbaf424aadc6192	fabric defect detection using local contrast deviations	quality assurance;local contrast deviation lcd;image segmentation;local binary pattern;fabric defect detection;defect detection;environmental factor	Defect inspection is a vital step for quality assurance in fabric production. The development of a fully automated fabric defect detection system requires robust and efficient fabric defect detection algorithms. The inspection of real fabric defects is particularly challenging due to delicate features of defects complicated by variations in weave textures and changes in environmental factors (e.g., illumination, noise, etc.). Based on characteristics of fabric structure, an approach of using local contrast deviation (LCD) is proposed for fabric defect detection in this paper. LCD is a parameter used to describe features of the contrast difference in four directions between the analyzed image and a defect-free image of the same fabric, and is used with a bilevel threshold function for defect segmentation. The validation tests on the developed algorithms were performed with fabric images from TILDA’s Textile Texture Database and captured by a line-scan camera on an inspection machine. The experimental results show that the proposed method has robustness and simplicity as opposed to the approach of using modified local binary patterns (LBP).	algorithm;liquid-crystal display;local binary patterns;software bug	Meihong Shi;Rong Fu;Yong Guo;Shixian Bai;Bugao Xu	2010	Multimedia Tools and Applications	10.1007/s11042-010-0472-8	quality assurance;computer vision;local binary patterns;computer science;machine learning;image segmentation	SE	41.77625056324934	-68.45422050236436	130710
37d71d509b3a9881cbd1775f4b26bf1278bb2e5c	adaptive pre-filtering for retinal vessel detection in hrt images				Xiaoyun Yang;Philip J. Morrow;Bryan W. Scotney	2003			filter (signal processing);computer vision;retinal;artificial intelligence;computer science	Vision	41.47350935881444	-71.39382489150962	130734
b555cb8d92d970e25e963a0e99bb9c4e94483d14	adaptive local threshold with shape information and its application to object segmentation	kernel;shape object segmentation image segmentation algorithm design and analysis petroleum digital images robots biomimetics image analysis lighting;image segmentation;pattern classification image segmentation learning artificial intelligence;support vector machines;maximum likelihood;training;oil sand images adaptive local threshold selection object segmentation local threshold segmentation algorithm digital images shape information image segmentation intensity analysis supervised classifier shape attribute distributions ground truth images;object segmentation;petroleum;shape;pattern classification;ground truth;oil sand;digital image;learning artificial intelligence;algorithm design and analysis	This paper presents a novel local threshold segmentation algorithm for digital images incorporating shape information. In image segmentation, most of local threshold algorithms are only based on intensity analysis. In many applications where an image contains objects with a similar shape, besides the intensity information, prior known shape attributes could be exploited to improve the segmentation. The goal of this work is to design a local threshold algorithm that includes shape information to enhance the segmentation quality. The algorithm can be divided into two steps: adaptively selecting local threshold based on maximum likelihood, and then removing unwanted segmented fragments by a supervised classifier. Shape attribute distributions are learned from typical objects in ground truth images. Local threshold for each object in an image to be segmented is chosen to maximize probabilities of these shape attributes according to learned distributions. After local thresholds are picked, the algorithm applies a supervised classifier trained by shape features to reject unwanted fragments. Experiments on oil sand images have shown that the proposed algorithm has superior performance to local threshold approaches based on intensity information in terms of segmentation quality.	algorithm;digital image;ground truth;image segmentation;machine learning;supervised learning	Jichuan Shi;Hong Zhang	2009	2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2009.5420762	active shape model;support vector machine;algorithm design;computer vision;kernel;ground truth;shape;computer science;machine learning;segmentation-based object categorization;oil sands;pattern recognition;mathematics;maximum likelihood;image segmentation;scale-space segmentation;petroleum;digital image	Vision	44.36652831355633	-67.6471171432232	130754
a377c011eb1ec244f6f94efec858c8835514fd7f	tree segmentation from an image	energy function;optimization problem;3d reconstruction	Tree is a very common object in nature, thus its segmentation provides quite important information for scene 3D reconstruction. In this paper, we present an algorithm for segmenting trees from a complex scene. The trunk and leaf regions of a tree can be individually identified and the trunk structure of the tree can also be extracted. The proposed algorithm is mainly composed of a preliminary image segmentation, a trunk structure extraction, and a leaf regions identification process. We model the extraction of trunk structure as an optimization problem, where an energy function is formulated according to the color, position, and orientation of the segmented regions. We propose an algorithm to minimize this energy function and thus extract the trunk structure of the tree. After obtaining the trunk structure, the leaf regions can then be easily identified by finding those leaf regions located above the trunk regions. This algorithm has been tested on some real images and the results indicated that our algorithm performed well for these images.	3d reconstruction;algorithm;color;image segmentation;mathematical optimization;optimization problem;structure from motion	Chin-Hung Teng;Yung-Sheng Chen;Wen-Hsing Hsu	2005			3d reconstruction;optimization problem;computer vision;mathematical optimization;computer science;mathematics	Vision	44.18359236840812	-66.62093081486984	130994
3ca288b51b314df08559b6630ca725da81152f2d	fuzzy-logic based information fusion for image segmentation	fuzzy rule merging algorithm fuzzy logic based information fusion image segmentation information fusion mechanism fuzzy clustering fuzzy inference mechanism;image segmentation;inference mechanisms;fuzzy logic;fuzzy clustering;fuzzy rule base;fuzzy inference;membership function;merging;merging fuzzy logic image segmentation inference mechanisms;information fusion;majority voting;image segmentation clustering algorithms fuzzy logic data mining inference mechanisms merging fuses aggregates voting inference algorithms	This work presents an information fusion mechanism for image segmentation using multiple cues. Initially, a fuzzy clustering of each cue space is performed and corresponding membership functions are produced on the image coordinates space. The latter include complementary as well as redundant information. A fuzzy inference mechanism is developed, which exploits these characteristics and fuses the membership functions. The produced aggregate membership functions represent objects, which bear combinations of the properties specified by the cues. The segmented image results after post-processing and defuzzification, which involves majority voting. A fuzzy rule based merging algorithm is finally proposed for reducing possible oversegmentation. Experimental results have been included to illustrate the steps and the efficiency of the algorithm.	aggregate data;algorithm;cluster analysis;defuzzification;fuzzy clustering;fuzzy logic;fuzzy rule;image segmentation;membership function (mathematics);video post-processing	Niki Aifanti;Anastasios Delopoulos	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530279	fuzzy logic;majority rule;membership function;defuzzification;fuzzy clustering;adaptive neuro fuzzy inference system;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;image segmentation;fuzzy associative matrix;scale-space segmentation;fuzzy set operations;fuzzy control system	Robotics	43.26792213843634	-66.90888823826482	131191
90910299ca52fc916fcf24c620adc743e7db7623	segmentation of thin section images for grain size analysis using region competition and edge-weighted region merging	thin section image;segmentation;classification;minimum description length;region competition;region merging	Microscopic thin section images are a major source of information on physical properties, crystallization processes, and the evolution of rocks. Extracting the boundaries of grains is of special interest for estimating the volumetric structure of sandstone. To deal with large datasets and to relieve the geologist from a manual analysis of images, automated methods are needed for the segmentation task. This paper evaluates the region competition framework, which also includes region merging. The procedure minimizes an energy functional based on the Minimum Description Length (MDL) principle. To overcome some known drawbacks of current algorithms, we present an extension of MDL-based region merging by integrating edge information between adjacent regions. In addition, we introduce a modified implementation for region competition for overcoming computational complexities when dealing with multiple competing regions. Commonly used methods are based on solving differential equations for describing the movement of boundaries, whereas our approach implements a simple updating scheme. Furthermore, we propose intensity features for reducing the amount of data. They are derived by comparing theoretical values obtained from a model function describing the intensity inside uniaxial crystals with measured data. Error, standard deviation, and phase shift between the model and intensity measurements preserve sufficient information for a proper segmentation. Additionally, identified objects are classified into quartz grains, anhydrite, and reaction fringes by these features. This grouping is, in turn, used to improve the segmentation process further. We illustrate the benefits of this approach by four samples of microscopic thin sections and quantify them in a comparison of a segmentation result and a manually obtained one. & 2014 Elsevier Ltd. All rights reserved.	algorithm;computation;information source;mdl (programming language);minimum description length;thin plate spline	Matthias Jungmann;Hansgeorg Pape;Peter Wißkirchen;Christoph Clauser;Thomas Berlage	2014	Computers & Geosciences	10.1016/j.cageo.2014.07.002	computer vision;minimum description length;biological classification;computer science;machine learning;scale-space segmentation;segmentation;statistics	Vision	47.24250550867816	-67.73903039942556	132437
610dc1cd04890488ff977f186b979a27d9698701	continuous wavelet transform applied to removing the fluctuating background in near-infrared spectra	continuous wavelet transform;near infrared	A novel method based on continuous wavelet transform (CWT) was proposed as a preprocessing tool for the near-infrared (NIR) spectra. Due to the property of the vanishing moments of the wavelet, the fluctuating background of the NIR spectra can be successfully removed through convolution of the spectra with an appropriate wavelet function. The vanishing moments of a wavelet and the scale parameter are two key factors that govern the result of the background elimination. The result of its application to both the simulated spectra and the NIR spectra of tobacco samples demonstrates that CWT is a competitive tool for removing fluctuating background in spectra.	complex wavelet transform;continuous wavelet;convolution;excretory function;population parameter;preprocessor;rca spectra 70;spectroscopy, near-infrared	Chaoxiong Ma;Xueguang Shao	2004	Journal of chemical information and computer sciences	10.1021/ci034211+	wavelet;continuous wavelet transform;stationary wavelet transform;discrete wavelet transform;second-generation wavelet transform;harmonic wavelet transform;analytical chemistry;wavelet packet decomposition;convolution;mathematics	Vision	51.830202490593116	-66.99749088584792	132582
d4758cbab72cd663554db05a8287f09109249ae4	parcellation of the thalamus using diffusion tensor images and a multi-object geometric deformable model	health research;uk clinical guidelines;biological patents;brain;europe pubmed central;dti;citation search;5d knutsson space;medical diagnostics;thalamic parcellation;uk phd theses thesis;multiple object geometric deformable model;life sciences;diffusion tensor imaging;relays;uk research reports;medical journals;diffusion;diseases and disorders;europe pmc;biomedical research;bioinformatics	The thalamus is a sub-cortical gray matter structure that relays signals between the cerebral cortex and midbrain. It can be parcellated into the thalamic nuclei which project to different cortical regions. The ability to automatically parcellate the thalamic nuclei could lead to enhanced diagnosis or prognosis in patients with some brain disease. Previous works have used diffusion tensor images (DTI) to parcellate the thalamus, using either tensor similarity or cortical connectivity as information driving the parcellation. In this paper, we propose a method that uses the diffusion tensors in a different way than previous works to guide a multiple object geometric deformable model (MGDM) for parcellation. The primary eigenvector (PEV) is used to indicate the homogeneity of fiber orientations. To remove the ambiguity due to the fact that the PEV is an orientation, we map the PEV into a 5D space known as the Knutsson space. An edge map is then generated from the 5D vector to show divisions between regions of aligned PEV's. The generalized gradient vector flow (GGVF) calculated from the edge map drives the evolution of the boundary of each nucleus. Region based force, balloon force, and curvature force are also employed to refine the boundaries. Experiments have been carried out on five real subjects. Quantitative measures show that the automated parcellation agrees with the manual delineation of an expert under a published protocol.	alignment;brain diseases;cell nucleus;cerebral cortex;diffusion tensor imaging;distance;epilepsy, generalized;experiment;forecast of outcome;gradient;gray matter;intrinsic drive;large;matthews correlation coefficient;midbrain structure;neoplasms, unknown primary;numerous;patients;relay;scientific publication;sørensen–dice coefficient;tensor fascia lata;thalamic nuclei;thalamic structure;tissue fiber;quantitative	Chuyang Ye;John A. Bogovic;Sarah H. Ying;Jerry L. Prince	2013	Proceedings of SPIE--the International Society for Optical Engineering	10.1117/12.2006119	diffusion mri;computer vision;artificial intelligence;diffusion;physics	Vision	41.348436853296676	-78.85862746517907	132622
32f46162d6295cee7c4f93e3645c2b2c6110e9e0	expectation-maximization algorithm with local adaptivity	image segmentation;62p99;local adaptivity;posterior probability;local adaptation;gaussian mixture model;68u10;expectation maximization algorithm;68w01	We develop an expectation-maximization algorithm with local adaptivity for image segmentation and classification. The key idea of our approach is to combine global statistics extracted from the Gaussian mixture model or other proper statistical models with local statistics and geometrical information, such as local probability distribution, orientation, and anisotropy. The combined information is used to design an adaptive local classification strategy that improves the robustness of the algorithm and also keeps fine features in the image. The proposed methodology is flexible and can be easily generalized to deal with other inferred information/quantities and statistical methods/models.	authorization;exptime;expectation–maximization algorithm;google map maker;image segmentation;mixture model;pixel;statistical model	Shingyu Leung;Gang Liang;Knut Sølna;Hongkai Zhao	2009	SIAM J. Imaging Sciences	10.1137/080731530	mathematical optimization;expectation–maximization algorithm;computer science;machine learning;pattern recognition;mixture model;mathematics;image segmentation;posterior probability;statistics	ML	50.37596277798645	-70.05461213732585	132685
00d5fbd3e6b5a174a5185a0547c285fd5a55a71b	optical flow estimation		This chapter provides a tutorial introduction to gradientbased optical flow estimation. We discuss least-squares and robust estimators, iterative coarse-to-fine refinement, different forms of parametric motion models, different conservation assumptions, probabilistic formulations, and robust mixture models.	iterative method;least squares;mixture model;optical flow;refinement (computing)	David J. Fleet;Y. Weiss	2006		10.1007/0-387-28831-7_15	econometrics;mathematical optimization;computer science;statistics	Vision	52.67312786781208	-71.89652618067696	132815
4f0bb2a26f5478a5fb30a329e82b2b9175e68fce	segmentation using the edge strength function as a shape prior within a local deformation model	estimation theory;image segmentation;shape deformable models image segmentation feature extraction data mining mathematical model mathematics noise shaping image resolution scholarships;edge detection;prior based image segmentation;level set;deformable models;computational modeling;shape;image edge detection;image segmentation edge detection estimation theory feature extraction;feature extraction;registration;mathematical model;map estimation;object boundary edge strength function local deformation model image segmentation prior guided segmentation shape feature extraction local deformation estimation;variational method;shape priors;variational methods prior based image segmentation registration;deformable model;variational methods	This paper presents a new image segmentation framework which employs a shape prior in the form of an edge strength function to introduce a higher-level influence on the segmentation process. We formulate segmentation as the minimization of three coupled functionals, respectively, defining three processes: prior-guided segmentation, shape feature extraction and local deformation estimation. Particularly, the shape feature extraction process is in charge of estimating an edge strength function from the evolving object region. The local deformation estimation process uses this function to determine a meaningful correspondence between a given prior and the evolving object region, and the deformation map estimated in return supervises the segmentation by enforcing the evolving object boundary towards the prior shape.	feature extraction;image segmentation	Erkut Erdem;Sibel Tari;Luminita A. Vese	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414504	computer vision;edge detection;feature extraction;shape;computer science;variational method;level set;machine learning;segmentation-based object categorization;pattern recognition;mathematical model;mathematics;image segmentation;estimation theory;scale-space segmentation;computational model;statistics	Vision	49.867337407692595	-71.81281602655473	132986
ff6d165ab47b458cac86741d40fced7781d6cea7	a new multi-criteria fusion model for color textured image segmentation	image segmentation;image color analysis;linear programming;optimization;entropy;gain measurement	Fusion of image segmentations using consensus clustering and based on the optimization of a single criterion (commonly called the median partition based approach) may bias and limit the performance of an image segmentation model. To address this issue, we propose, in this paper, a new fusion model of image segmentation based on multi-objective optimization which aims to avoid the bias caused by a single criterion and to achieve a final improved segmentation. The proposed fusion model combines two conflicting and complementary segmentation criteria, namely; the region-based variation of information (VoI) criterion and the contour-based F-Measure (precision-recall) criterion with an entropy-based confidence weighting factor. To optimize our energy-based model we use an optimization procedure derived from the iterative conditional modes (ICM) algorithm. The experimental results on the Berkeley database with manual ground truth segmentations clearly show the effectiveness and the robustness of our multi-objective median partition based approach.	algorithm;berkeley db;cluster analysis;consensus clustering;f1 score;ground truth;image segmentation;iterated conditional modes;iterative method;mathematical optimization;multi-objective optimization;variation of information	Lazhar Khelifi;Max Mignotte	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532825	image texture;computer vision;entropy;computer science;linear programming;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	44.128808565761545	-70.7557331207043	133026
61159b74a2ce4e27e38145f8ae8e96a51cbdb398	mutual information matching in multiresolution contexts	high resolution;computed tomography;multiresolution matching;pet imaging;low resolution;positron emission tomography;normalized mutual information;mr imaging;magnetic resonance;image registration;brain imaging;mutual information;multimodality registration;brain images	Image registration methods based on maximization of mutual information have shown promising results for matching of 3D multimodal brain images. This paper discusses the effects of multiresolution approaches to rigid registration based on mutual information, aiming for an acceleration of the matching process while maintaining the accuracy and robustness of the method. Both standard mutual information and a normalized version are considered. The behaviour of mutual information matching in a multiresolution scheme is examined for pairs of high resolution magnetic resonance (MR) and computed tomography (CT) images and for low resolution MR images paired with either positron emission tomography (PET) images or low resolution CT images. Two methods of downscaling the images are compared: equidistant sampling and Gaussian blurring followed by equidistant sampling. The experiments show that a multiresolution approach to mutual information matching is an appropriate method for images of high (sampling) resolution, achieving an average acceleration of a factor of almost 2. For images of lower resolution the multiresolution method is not recommended. The little difference observed between matching with standard or normalized mutual information seems to indicate a preference for the normalized measure. Gaussian blurring of the images before registration does not improve the performance of the multiresolution method. q 2001 Elsevier Science B.V. All rights reserved.	ct scan;downscaling;electron tomography;expectation–maximization algorithm;experiment;image registration;image resolution;multimodal interaction;multiresolution analysis;mutual information;resonance;sampling (signal processing)	Josien P. W. Pluim;J. B. Antoine Maintz;Max A. Viergever	2001	Image Vision Comput.	10.1016/S0262-8856(00)00054-8	computer vision;image resolution;magnetic resonance imaging;mathematics;computed tomography;neuroimaging	Vision	49.72977022435136	-76.9912697670209	133097
0bd500e013b3fb6daf9082714255bba7862d4de4	cardiac localization in topograms using hierarchical models	image resolution;cardiology;lungs heart shape active appearance model training robustness adaptation models;hierarchical systems;cardiac localization active appearance models hierarchical pyramids automated scan planning;surface topography;medical image processing;computerised tomography;surface topography cardiology computerised tomography diagnostic radiography hierarchical systems image resolution medical image processing physiological models;physiological models;2d ct topograms cardiac localization hierarchical models medical imaging two dimensional localizer images high resolution scan planning two dimensional projections overlapping tissue roi delineation active appearance model human heart single global aam approach;diagnostic radiography	A vast number of medical imaging protocols identify anatomical regions of interest (ROI) from two dimensional (2D) localizer images to aid high resolution scan planning. These localizer scans are typically two dimensional projections of three dimensional data and as such have lower image detail due to overlapping tissue. The problem is further complicated by large variations in shape, size, appearance and the high occurrence of anomalies in the human anatomy. Manual ROI delineation is time consuming and error prone. To combat these issues we develop a hierarchical multi-object active appearance model (AAM) framework that is both robust to inaccuracies in model initialization yet sufficiently flexible to handle the large diversity of the human body. The method was successfully applied to automatically determine the extents of the human heart in 99 2D CT topograms yielding significant improvement in accuracy over a single global AAM approach.	active appearance model;bayesian network;ct scan;cognitive dimensions of notations;image resolution;medical imaging;region of interest	Qi Song;V. Srikrishnan;Bipul Das;Roshni R. Bhagalia	2013	2013 IEEE 10th International Symposium on Biomedical Imaging	10.1109/ISBI.2013.6556423	computer vision;radiology;image resolution;medicine;computer science;computer graphics (images)	Vision	41.716210136552135	-79.81075879687478	133126
6467c2eb3c35b5f8bc1b0c8f9bd177127d5f2c34	a medial axis based thinning strategy and structural feature extraction of character images	image segmentation;structural feature extraction;structural feature;skeleton segment;medial axis based thinning strategy;extrapolation;shape deformation;junctions;skeleton;concavity and convexity;concavity and convexity medial axis thinning skeleton segment structural feature;stroke segmentation;stroke segmentation medial axis based thinning strategy structural feature extraction character images shape deformation skeletonization;shape;feature extraction;medical image processing;pixel;skeleton pixel image segmentation feature extraction shape junctions extrapolation;medical image processing character recognition feature extraction image segmentation image thinning;skeletonization;image thinning;character recognition;medial axis;thinning;character images	The thinning methodology is novel in terms of its ability to incorporate character shape specific knowledge while constructing the thinned skeleton. But removal of spurious strokes or shape deformation in thinning is a difficult problem. In this paper, we have proposed a novel medial-axis based thinning strategy used for performing skeletonization of noisy character images. The proposed algorithm produces segmented strokes in vector form as a by-product. Hence further stroke segmentation is not required. Experiment is done on printed English, Bengali, Hindi, and Tamil characters and we obtain less spurious branches compared to other thinning methods without any post processing. We have concluded with a proposed methodology to extract structural features from thinned character images. This feature set improves the performance of existing OCR for Indian languages.	algorithm;apache axis;feature extraction;medial graph;optical character recognition;printing;thinning	Soumen Bag;Gaurav Harit	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5654311	skeletonization;computer vision;medial axis;feature extraction;shape;computer science;pattern recognition;image segmentation;extrapolation;skeleton;pixel	Robotics	44.800657959537205	-71.23129329345477	133342
e7adb5566c6d22fd3add098930d4ed00f86e2404	computer-assisted detection of pulmonary embolism	aide diagnostic;faux positif;image tridimensionnelle;computerized axial tomography;tomodensitometria;radiodiagnostic;medical imagery;informatica biomedical;biomedical data processing;respiratory disease;appareil respiratoire pathologie;image processing;tomodensitometrie spiralee;vaso sanguineo patologia;volume rendering;appareil circulatoire pathologie;informatique biomedicale;procesamiento imagen;hombre;aparato respiratorio patologia;segmentation;traitement image;falso positivo;angiography;aparato circulatorio patologia;radiodiagnostico;tomodensitometrie;cardiovascular disease;human;vascular disease;imagerie medicale;embolie pulmonaire;tridimensional image;computing systems;imageneria medical;radiodiagnosis;false positive;angiographie;angiografia;pulmonary embolism;segmentacion;diagnostic aid;imagen tridimensional;embolia pulmonar;homme;ayuda diagnostica;helical computerized tomography;vaisseau sanguin pathologie	In this study, a computerized method for detection of pulmonary embolism in spiral CT angiography was developed. The method is based on segmentation of pulmonary vessels to limit the search space for thrombi. Several three-dimensional image features such as local contrast, seco nd derivatives, and distance to the vessel wall were employed for detection of thrombi and for elimination of false positives. Volume rendering was applied for display of detection results. Preliminary results based on several clinical data show the potential of our method.	ct scan;computed tomography angiography;volume rendering	Yoshitaka Masutani;Heber MacMahon;Kunio Doi	2000		10.1117/12.387760	radiology;medicine;pathology;surgery	Vision	45.90947718779287	-79.32320735776156	133459
f308ff65e616479b8a7d03201f47eb7fb323a113	color image segmentation on region growing and multi-scale clustering	cluster algorithm;spatial process;image segmentation;color space;noise suppression;algorithms;region growing;color image segmentation	This paper presents a color image segmentation method by combining region growing and color clustering algorithms. This method considers the both color and location information in a transformed color space. After multi-scale clustering (MSC), it does a spatial processing - region growing. MSC can perform better in conquering the over-segment problem than equal distance clustering. Compared with the previous methods only depending on MSC, the region growing can enhance the ability of noise suppression. This method inherits the idea that operates clustering first and then carries out spatial processing. Both clustering algorithm and spatial processing algorithm are improved, so this method (the two algorithms) can obtain more satisfactory results.© (2011) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	cluster analysis;color image;image segmentation;region growing	Zong-pu Jia;Weixing Wang;Junding Sun;Tai-wen Wei	2011		10.1117/12.872089	color histogram;correlation clustering;computer vision;canopy clustering algorithm;machine learning;segmentation-based object categorization;region growing;image segmentation;cluster analysis;color space;computer graphics (images)	Vision	44.806967718575166	-67.90132285936897	133926
ca645b0ae86402eea10af9fc94937c82b19939c4	an evolutionary learning approach to self-configuring image pipelines in the context of carbon fiber fault detection		Carbon fiber reinforced plastics (CFRP) play a key role for the production of leightweight structures. Simultaneously, online quality inspection of CFRP becomes more important, especially for environments with high safety standards. In this context, vision systems aim to find defects of different shape, size, contour and orientation. Little effort, however, has been made in detecting defect areas in images taken from the surface of carbon fibers. A common approach for segmenting filament defects are edge detection and thresholding. With every change of material and process adjustments, the filter parameters have to be adapted. In this paper, we propose a cartesian genetic programming (CGP) approach to semi-automatically select the best parameters. This strategy saves time for parameter identification while at the same time increases precision. A test run on randomly selected samples shows how the approach can substantially improve detection reliability.	algorithm;edge detection;evolutionary computation;fitness function;futures studies;genetic programming;image processing;mathematical optimization;optical fiber;pipeline (computing);randomness;region of interest;semiconductor industry;sensor;software bug;thresholding (image processing)	Andreas Margraf;Anthony Stein;Leonhard Engstler;Steffen Geinitz;Jörg Hähner	2017	2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2017.0-165	machine learning;image processing;market segmentation;computer vision;computer science;genetic programming;fault detection and isolation;thresholding;carbon fibers;edge detection;artificial intelligence	Robotics	43.19566849720154	-70.24813716617162	134080
c239023f6e2cbefd46d5e9c625b671a70136d8b2	functionally informed cortex based alignment: an integrated approach for whole-cortex macro-anatomical and roi-based functional alignment	group alignment;group statistics;roi analysis;spatial normalisation;fmri	Due to anatomical variability across subjects many brain mapping experiments have analysis focused on a few particular regions of interest so as to circumvent the problem of sub-optimal statistics resulting from the lack of anatomical correspondence across subjects. Since the topographic distribution of experimental effects across the cortex is also often of interest, two separate analyses are often conducted, one on the regions of interest alone, as well as a separate 'whole brain' analysis with sub-optimal spatial correspondence across brains. In this paper we present a new group alignment procedure which incorporates, from each subject, both macro-anatomical (curvature) information and functional information from standard localizer experiments. After specifying appropriate parameters to weight anatomical and functional alignment forces, we were able to create a group cortical reconstruction which was well aligned in terms of both anatomical and functional areas. We observed an increase in the overlap of functional areas as well as an improvement in group statistics following this integrated alignment procedure. We propose that, using this alignment scheme, two separate analyses may not be necessary as both analyses can be integrated into a single procedure. After an integrated structural and functional alignment one is able to carry out a whole brain analysis with improved statistical sensitivity due to the reduction in spatial variation in the location of functional regions of interest which fCBA accomplishes. Furthermore, regions in the vicinity of localised and aligned regions-of-interest will also benefit from the integrated alignment.	alignment;brain mapping;experiment;region of interest;spatial variability;topography	Martin A. Frost;Rainer Goebel	2013	NeuroImage	10.1016/j.neuroimage.2013.07.056	computer vision;bioinformatics	ML	43.30826353400247	-79.38393168852377	134086
4d2c9047cf35a441f96571e452cf957d7e0ab7f2	correspondence free 3d statistical shape model fitting to sparse x-ray projections	femur;pilot study;3d modeling;statistical shape model;2d 3d reconstruction;dynamic program;computer programming;biplane x ray;field of view;correspondence free;3d shape reconstruction;ssm;3d reconstruction;x rays	In this paper we address the problem of 3D shape reconstruction from sparse X-ray projections. We present a correspondence free method to fit a statistical shape model to two X-ray projections, and illustrate its performance in 3D shape reconstruction of the femur. The method alternates between 2D segmentation and 3D shaoe reconstruction, where 2D segmentation is guided by dynamic programming along the model projection on the X-ray plane. 3D reconstruction is based on the iterative minimization of the 3D distance between a set of support points and the back-projected silhouette with respect to the pose and model parameters. We show robustness of the reconstruction on simulated X-ray projection data of the femur, varying the field of view; and in a pilot study on cadaveric femora.	3d reconstruction;curve fitting;dynamic programming;iterative method;sparse matrix;statistical shape analysis	Nóra Baka;Wiro J. Niessen;Bart L. Kaptein;Theo van Walsum;Luca Ferrarini;Johan H. C. Reiber;Boudewijn P. F. Lelieveldt	2010		10.1117/12.840935	3d reconstruction;active shape model;computer vision;field of view;computer programming;optics;physics	Vision	46.91601707810651	-78.07661034878869	134239
12043502d4ec3a75b5e7d9ace34dcafd3fb4cbdc	estimation of diffusion properties in crossing fiber bundles	radiology;levenberg marquardt;estimation theory;optimisation;biological tissues;brain;cramer rao analysis;crossing white matter structures;white matter;tensile stress;diffusion weighted magnetic resonance imaging;diffusion orientation;maximum likelihood;dual tensor model;biodiffusion;brain computer simulation diffusion tensor imaging humans models neurological models statistical monte carlo method nerve fibers normal distribution reproducibility of results signal processing computer assisted;cramer rao lower bound;biomedical imaging;tensile stress shape anisotropic magnetoresistance biomedical imaging parameter estimation magnetic analysis magnetic resonance imaging radiology monte carlo methods estimation theory;optimisation biodiffusion biological tissues biomedical mri brain maximum likelihood estimation monte carlo methods neurophysiology;maximum likelihood estimation;universiteitsbibliotheek;diffusion properties;axial diffusion;shape;isotropic fraction;levenberg marquardt optimization;magnetic resonance imaging;diffusion weighting parameters;magnetic analysis;anisotropic magnetoresistance;comparative study;optimization;fractional anisotropy;crossing fiber bundles;dw mri;monte carlo simulations cramer rao analysis diffusion properties diffusion weighted magnetic resonance imaging dual tensor model;neurophysiology;rician noise model;parameter estimation;heterogeneous tissue;monte carlo simulation;monte carlo simulations;maximum likelihood function;diffusion shape;monte carlo methods;lower bound;heterogeneous tissue diffusion properties crossing fiber bundles optimization dual tensor model diffusion weighting parameters diffusion shape diffusion orientation monte carlo simulations cramer rao lower bound isotropic fraction axial diffusion fractional anisotropy levenberg marquardt optimization maximum likelihood function rician noise model crossing white matter structures diffusion weighted magnetic resonance imaging dw mri;biomedical mri;diffusion weighted	There is an ongoing debate on how to model diffusivity in fiber crossings. We propose an optimization framework for the selection of a dual tensor model and the set of diffusion weighting parameters b, such that both the diffusion shape and orientation parameters can be precisely as well as accurately estimated. For that, we have adopted the Cramér-Rao lower bound (CRLB) on the variance of the model parameters, and performed Monte Carlo simulations. We have found that the axial diffusion λ|| needs to be constrained, while an isotropic fraction can be modeled by a single parameter fiso. Under these circumstances, the Fractional Anisotropy (FA) of both tensors can theoretically be independently estimated with a precision of 9% (at SNR=25 ). Levenberg-Marquardt optimization of the Maximum Likelihood function with a Rician noise model approached this precision while the bias was insignificant. A two-element b-vector b = [ 1.0 amp; 3.5 ] · 103 mm-2 s was found to be sufficient for estimating parameters of heterogeneous tissue with low error. This has allowed us to estimate consistent FA-profiles along crossing tracts. This work defines fundamental limits for comparative studies to correctly analyze crossing white matter structures.	aoc2 gene;ampicillin;cloud fraction;dual;entity name part qualifier - adopted;estimated;estimation theory;fractional anisotropy;levenberg–marquardt algorithm;mathematical optimization;mean squared error;monte carlo method;population parameter;sample variance;simulation;tissue fiber;white matter	Matthan W. A. Caan;Ganesh Khedoe;Dirk H. J. Poot;Arjan Jan den Dekker;Sílvia Delgado Olabarriaga;Cornelis A. Grimbergen;Lucas J. van Vliet;Frans Vos	2010	IEEE Transactions on Medical Imaging	10.1109/TMI.2010.2049577	mathematical optimization;magnetic resonance imaging;mathematics;maximum likelihood;nuclear magnetic resonance;neurophysiology;statistics;monte carlo method	Vision	48.73498854225337	-79.54741886797753	134318
7680b36107c00ece2b5569eaec2ebdd777683112	shape and topology constrained image segmentation with stochastic models			image segmentation;stochastic process	Thomas Zöller	2005			stochastic modelling;computer vision;active shape model;segmentation-based object categorization;artificial intelligence;mathematics;mathematical optimization;scale-space segmentation;image segmentation	Vision	50.403425583030426	-71.02404367578558	134433
4c2552c7d0216ee1cff03b03ac6c388cc52cf501	a new model for image segmentation	image segmentation;variational method image segmentation region based approach mumford shah model segmentation curve level set method image intensity distribution step edges roof edges partial differential equations;partial differential equations image segmentation;difference equation;partial differential equations;partial different equations;variational method;image segmentation image edge detection robustness linear approximation object detection level set differential equations markov random fields partial differential equations computer science;level set method;high frequency;variational method image segmentation partial different equations	As a region-based approach, the Mumford-Shah (MS) model is a robust image segmentation technique. However, the solution of the MS model is not trivial. Although some alternative approaches have been presented, these methods are either inefficient or applicable only to some special cases. We present a new model which consists of two terms, the length of the segmentation curve and the high-frequency component in the regions. Because only one variable needs to be solved, the method of solution is very efficient. Using the level set method, the approach can segment objects with complicated image intensity distribution without any approximations. In addition, the new model can segment both step and roof edges.	approximation algorithm;image segmentation;linear approximation;mumford–shah functional	Xiaojun Du;Tien D. Bui	2008	IEEE Signal Processing Letters	10.1109/LSP.2007.913625	mathematical optimization;mathematical analysis;discrete mathematics;scale space;computer science;variational method;segmentation-based object categorization;high frequency;mathematics;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;differential equation;partial differential equation;level set method;numerical partial differential equations	Vision	51.09447781360089	-71.11591181694916	134577
8d1b8c7e0ea9bdf86b7c3e983a8c777eaea6f488	level sets-based image segmentation approach using statistical shape priors		Abstract A robust 3-D segmentation technique incorporated with the level sets concept and based on both shape and intensity constraints is introduced. A partial differential equation (PDE) is derived to describe the evolution of the level set contours. This PDE does not contain weighting parameters that need to be tuned, which overcomes the drawbacks of other PDE approaches. The shape information is collected from a set of co-aligned manually segmented contours of the training data. A promising statistical approach is used to get the distribution of the intensity gray values. The introduced statistical approach is built by modeling the empirical PDF (normalized histogram of occurrences) for the intensity level distribution with a linear combination of Gaussians (LCG) incorporating both negative and positive components. An Expectation-Maximization (EM) algorithm is modified to deal with the LCGs, and we also proposed an EM-based sequential technique to acquire a close initial LCG approximation for the modified EM algorithm to start with. The PDF of the intensity levels is incorporated in the speed function of the moving level set to specify the evolution direction. Experimental results show how accurately the approach is in segmenting various types of 2-D and 3-D datasets comprising medical images.	image segmentation	Ahmed ElTanboly;Mohammed Ghazal;Hassan Hajjdiab;Ahmed Shalaby;Andy Switala;Ali M. Mahmoud;Prasanna Sahoo;Magdi El-Azab;Ayman El-Baz	2019	Applied Mathematics and Computation	10.1016/j.amc.2018.05.064	mathematical optimization;level set;expectation–maximization algorithm;image segmentation;normalization (statistics);linear combination;prior probability;mathematics;histogram;weighting	Vision	49.635429957871914	-69.17908950542952	134782
95db9c81dd6da823fbf41df6912df363b44e97bd	improved conformal metrics for 3d geometric deformable models in medical images	3d segmentation;conformal metric;segmentation;medical image;partial volume;geometric deformable model;computing systems;medical application;energy minimization;scale selection;image modeling;deformable model	The Geometric Deformable Model (GDM) is a useful segmentation method that combines the energy minimization concepts of physically deformable models and the flexible topology of implicit deformable models in a mathematically well-defined framework. The key aspect of the method is the measurement of length and area using a conformal metric derived from the image. This conformal metric, usually a monotonicly decreasing function of the gradient, defines a Riemannian space in which the surface evolves. The success of the GDM for 3D segmentation in medical applications is directly related to the definition of the conformal metric. Like all deformable models, the GDM is susceptible to poor initialization, varying contrast, partial volume, and noise. This paper addresses these difficulties via the definition of the conformal metric and describes a new method for computing the metric in 3D. This method, referred to as a confidence-based mapping, incorporates a new 3D scale selection mechanism and an a-priori image model. A comparison of the confidence-based approach and previous formulations of the conformal metric is presented using computer phantoms. A preliminary application in two clinical examples is given.© (2001) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Christopher L. Wyatt;Yaorong Ge;David J. Vining	2001		10.1117/12.431107	computer vision;mathematical optimization;energy minimization;segmentation;partial volume	Vision	43.65970531746535	-78.2088378603281	134828
aa942ed29ca95764f97d752b1566cfb83191db16	robust spatially constrained fuzzy c-means algorithm for brain mr image segmentation	image segmentation;magnetic resonance imaging;fuzzy c means;intensity inhomogeneity;spatial information	Objective#R##N#Accurate brain tissue segmentation from magnetic resonance (MR) images is an essential step in quantitative brain image analysis, and hence has attracted extensive research attention. However, due to the existence of noise and intensity inhomogeneity in brain MR images, many segmentation algorithms suffer from limited robustness to outliers, over-smoothness for segmentations and limited segmentation accuracy for image details. To further improve the accuracy for brain MR image segmentation, a robust spatially constrained fuzzy c-means (RSCFCM) algorithm is proposed in this paper.#R##N#Method#R##N#Firstly, a novel spatial factor is proposed to overcome the impact of noise in the images. By incorporating the spatial information amongst neighborhood pixels, the proposed spatial factor is constructed based on the posterior probabilities and prior probabilities, and takes the spatial direction into account. It plays a role as linear filters for smoothing and restoring images corrupted by noise. Therefore, the proposed spatial factor is fast and easy to implement, and can preserve more details. Secondly, the negative log-posterior is utilized as dissimilarity function by taking the prior probabilities into account, which can further improve the ability to identify the class for each pixel. Finally, to overcome the impact of intensity inhomogeneity, we approximate the bias field at the pixel-by-pixel level by using a linear combination of orthogonal polynomials. The fuzzy objective function is then integrated with the bias field estimation model to overcome the intensity inhomogeneity in the image and segment the brain MR images simultaneously.#R##N#Results#R##N#To demonstrate the performances of the proposed algorithm for the images with/without skull stripping, the first group of experiments is carried out in clinical 3T-weighted brain MR images which contain quite serious intensity inhomogeneity and noise. Then we quantitatively compare our algorithm to state-of-the-art segmentation approaches by using Jaccard similarity on benchmark images obtained from IBSR and BrainWeb with different level of noise and intensity inhomogeneity. The comparison results demonstrate that the proposed algorithm can produce higher accuracy segmentation and has stronger ability of denoising, especially in the area with abundant textures and details.#R##N#Conclusion#R##N#In this paper, the RSCFCM algorithm is proposed by utilizing the negative log-posterior as the dissimilarity function, introducing a novel factor and integrating the bias field estimation model into the fuzzy objective function. This algorithm successfully overcomes the drawbacks of existing FCM-type clustering schemes and EM-type mixture models. Our statistical results (mean and standard deviation of Jaccard similarity for each tissue) on both synthetic and clinical images show that the proposed algorithm can overcome the difficulties caused by noise and bias fields, and is capable of improving over 5% segmentation accuracy comparing with several state-of-the-art algorithms.	algorithm;image segmentation	Zexuan Ji;Jinyao Liu;Guo Cao;Quan-Sen Sun;Qiang Chen	2014	Pattern Recognition	10.1016/j.patcog.2014.01.017	computer vision;computer science;artificial intelligence;magnetic resonance imaging;machine learning;segmentation-based object categorization;mathematics;spatial analysis;image segmentation;scale-space segmentation	Vision	44.10554980331557	-72.52891294051841	134831
80521ee0dd9dfee315a393d616e27614fef49e64	segmentation of medical images based on homogram thresholding	mr imaging;medical image	Homogram, or histogram based on homogeneity is employed in our algorithm. Histogram thresholding is a classical and efficient method for the segmentation of various images, especially of CT images. However, MR images are difficultly segmented via this method; as the gray levels of their pixels are too similar to distinguish. The regular histogram of a MR image is usually plain, thus the peaks and valleys of the histogram are hard to find and locate precisely. We proposed a new definition of homogeneity for which a series of sub-images are employed to compute. Therefore, both local and global information are taken in accounted. Then the image is updated with the homogeneity weighted original and average gray levels. The more homogeneous the pixel is, the closer the updated gray level is to the average. The new histogram is calculated based on the updated image. It is much steeper than the regular one. Some indiscernible peaks in the regular histogram can be recognized easily from the new histogram. Therefore a simple but agile peakfinding approach is able to determine objects to segment and corresponding thresholds exactly. Segmentation via thresholding is feasible now even in MR images. Moreover, our algorithm remains speedy even though the accuracy of segmentation advances.	agile software development;algorithm;ct scan;grayscale;pixel;thresholding (image processing)	Xingfei Ge;Jie Tian;Fuping Zhu	2003		10.1117/12.480866	computer vision;histogram matching;pattern recognition;balanced histogram thresholding;mathematics;region growing;image segmentation;adaptive histogram equalization;image histogram;computer graphics (images)	Vision	45.04518204529169	-70.31017584104531	134869
ffcae98122ee37272944425fee334816e6a751da	second order variational optic flow estimation	second order;optical flow estimation;taylor expansion;motion vector field;energy function;variational approach;image sequence;displacement vector field	In this paper we present a variational approach to accurately estimate the motion vector field in a image sequence introducing a second order Taylor expansion of the flow in the energy function to be minimized. This feature allows us to simultaneously obtain, in addition, an estimation of the partial derivatives of the motion vector field. The performance of our approach is illustrated with the estimation of the displacement vector field on the well known Yosemite sequence and compared to other techniques from the state of the art.	calculus of variations;displacement mapping;mathematical optimization;optical flow;synthetic data;variational principle;video post-processing;whole earth 'lectronic link	Luis Álvarez;Carlos A. Castaño-Moraga;Miguel García;Karl Krissian;Luis Mazorra;Agustín Salgado de la Nuez;Javier Sánchez Pérez	2007		10.1007/978-3-540-75867-9_81	mathematical optimization;taylor series;vector potential;calculus;mathematics;geometry;motion field;second-order logic	Vision	52.70266553231837	-71.7062715052518	134967
e57d51f1331a58da42dc97d1060b3f3a6fa5e234	intervertebral disc segmentation in mr images using anisotropic oriented flux	intervertebral disc;spine;magnetic resonance images;segmentation;anisotropic oriented flux	This study proposes an unsupervised intervertebral disc segmentation system based on middle sagittal spine MR scans. The proposed system employs the novel anisotropic oriented flux detection scheme which helps distinguish the discs from the neighboring structures with similar intensity, recognize ambiguous disc boundaries, and handle the shape and intensity variation of the discs. Based on minimal user interaction, the proposed system begins with vertebral body tracking to infer the information regarding the positions and orientations of the target intervertebral discs. The information is employed in a set of image descriptors, which jointly constitute an energy functional describing the desired disc segmentation result. The energy functional is minimized by a level set based active contour model to perform disc segmentation. The proposed segmentation system is evaluated using a database consisting of 455 intervertebral discs extracted from 69 middle sagittal slices. It is demonstrated that the proposed method is capable of delivering accurate results for intervertebral disc segmentation.	active contour model;bone structure of spine;extraction;flux;inference;intervertebral disk displacement;intervertebral disc structure;sagittal plane;segmentation action;visual descriptor;biologic segmentation	Max W. K. Law;KengYeow Tay;Andrew E. Leung;Gregory J. Garvin;Shuo Li	2013	Medical image analysis	10.1016/j.media.2012.06.006	computer vision;spine;segmentation;anatomy	Vision	40.85940422479081	-78.33489022298895	135003
69b7b6a81abed2dded903b58917ef4f40e7b3c30	a semi-automated technique for internal jugular vein segmentation in ultrasound images using active contours	manuals;image segmentation;ultrasonic imaging;veins;active contours;blood;videos	The assessment of the blood volume is crucial for the management of many acute and chronic diseases. Recent studies have shown that circulating blood volume correlates with the cross-sectional area (CSA) of the internal jugular vein (IJV) estimated from ultrasound imagery. In this paper, a semi-automatic segmentation algorithm is proposed using a combination of region growing and active contour techniques to provide fast and accurate segmentation of IJV ultrasound videos. The algorithm is applied to track and segment the IJV across a range of image qualities, shapes and temporal variation. The experimental results show that the algorithm performs well compared to expert manual segmentation and outperforms several published algorithms incorporating speckle tracking.	active contour model;algorithm;cross-sectional data;region growing;semiconductor industry	Ebrahim Karami;Mohamed S. Shehata;P. McGuire;Andrew J J Smith	2016	2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)	10.1109/BHI.2016.7455865	computer vision;radiology;medicine;pathology;scale-space segmentation	Vision	39.27579144327322	-79.94552468463037	135139
470098123f5486166fe3603d8d30e7ac67e99c07	using a level set approach for image segmentation under interpolation conditions	image segmentation;level set approach;lagrange multiplier;variational problem;hilbert space;energy minimization;geometric constraints;level set method;deformable model	In this paper, we propose a new 2D segmentation model including geometric constraints, namely interpolation conditions, to detect objects in a given image. We propose to apply the deformable models to an explicit function using the level set approach (Osher and Sethian [24]); so, we avoid the classical problem of parameterization of both segmentation representation and interpolation conditions. Furthermore, we allow this representation to have topological changes. A problem of energy minimization on a closed subspace of a Hilbert space is defined and introducing Lagrange multipliers enables us to formulate the corresponding variational problem with interpolation conditions. Thus the explicit function evolves, while minimizing the energy and it stops evolving when the desired outlines of the object to detect are reached. The stopping term, as in the classical deformable models, is related to the gradient of the image. Numerical results are given.	calculus of variations;energy minimization;gradient;hilbert space;image segmentation;interpolation;lagrange multiplier;loss function;numerical method	Carole Le Guyader;Dominique Apprato;Christian Gout	2004	Numerical Algorithms	10.1007/s11075-004-3631-z	mathematical optimization;mathematical analysis;calculus;mathematics;geometry;image segmentation;lagrange multiplier;energy minimization;level set method;hilbert space	Vision	52.11829141532008	-71.22601344758421	135231
e5a67f7146419b17015b2d00cdce81c58b40762e	an effective local regional model based on salient fitting for image segmentation		Intensity inhomogeneity often occurs in real-world images, and inevitably leads to many difficulties for accurate image segmentation. Although a lot of level set methods have been proposed to solve the problem of intensity inhomogeneity, they are often unavailable for some images with severe intensity inhomogeneity. In this paper, we propose a novel level set based segmentation model to effectively segment those images with severe intensity inhomogeneity, which is named as Local Salient Fitting (LSF) model. In LSF, we firstly transform original image into the new modality in which the object and background regions can be discriminated easily. Specially, we propose a weight factor based on local intensity variation to highlight the local region contrast of image. Meanwhile, the variation degree of local region is also computed to extract the distribution information of intensity variation. Then, since the new modality embodies the distribution information of intensity variation, we utilize the idea of fitting region distribution information to construct the salient fitting term. Finally, the salient fitting term is incorporated into the level set method to segment intensity inhomogeneous images. Furthermore, the combined effect of highlighted local region contrast and statistical distribution information of intensity variation will enhance the robustness of our method. Experiments conducted on synthetic and real images clearly demonstrate the efficiency and robustness of the proposed LSF model.	curve fitting;experiment;image segmentation;lsf;modality (human–computer interaction);robustness (computer science);synthetic intelligence	Hai Min;Jingting Lu;Wei Jia;Yang Zhao;Yue-Tong Luo	2018	Neurocomputing	10.1016/j.neucom.2018.05.070	salient;level set;robustness (computer science);local-regional;artificial intelligence;image segmentation;real image;pattern recognition;mathematics;segmentation;level set method	Vision	44.7002840779688	-72.44766496507029	135322
53d9e2d2ebd304e027c72385703634a4690547d2	quantum edge detection based on shannon entropy for medical images		In this paper, a new edge detection algorithm for medical images based on quantum computation concept is presented. The proposed method is performed through two stages: the first one enhances the image according to the quantum states' superposition property, which determine the adequate quantum pixels' values. The second one extracts edges based on Shannon entropy. The algorithm is tested on synthetic and real medical images, and the obtained results are compared to others' classical edge detection methods. It can be seen that the approach is promising.	algorithm;computation;display resolution;edge detection;entropy (information theory);formal grammar;high-level programming language;image editing;image processing;medical imaging;pixel;quantum computing;quantum state;quantum superposition;sensor;shannon (unit);superposition principle;synthetic intelligence	Abdelilah El Amraoui;Lhoussaine Masmoudi;Hamid Ez-Zahraouy;Youssef El Amraoui	2016	2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2016.7945739	entropy (information theory);quantum phase estimation algorithm;computer science;canny edge detector;real-time computing;superposition principle;quantum state;edge detection;quantum;quantum computer;computer vision;pattern recognition;artificial intelligence	Vision	51.13125644313106	-67.50824532274002	135494
cb8de51dbe3d5d93b08298e28abdab713cb2e3b5	a robust image segmentation method using hierarchical color clustering	color clustering;image segmentation;gaussian mixture model	A robust and fast image segmentation method is provided based on color distribution. In this paper, unlike the widely used training and optimizing process, an image is recursively divided only according to the color differences, a whole region is divided into two regions that have the most difference Gaussian Mixture Model distribution in color space, and the image is segmented into separate and continuous regions after a few iterations. Morphological post process is introduced to get correct segment results with closed and semantical boundary. At last of this paper, experimental comparison is provided on different hardware platform. A robust image segment result is got with high calculation performance according to contract experiments on different platforms including normal PC and high performance server with GPU support.	cluster analysis;color space;experiment;graphics processing unit;image segmentation;iteration;mixture model;recursion;server (computing)	Jijun He;Jinjin Zheng;Yutang Guo;Yuan Shen	2016		10.1145/3028842.3028843	color histogram;image texture;computer vision;binary image;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;scale-space segmentation	Vision	44.738935238361464	-67.8668892422665	135689
c0d4afcc269febecb16a9291cc58b3c419f11e2b	part-to-whole registration of histology and mri using shape elements		Image registration between histology and magnetic resonance imaging (MRI) is a challenging task due to differences in structural content and contrast. Too thick and wide specimens cannot be processed all at once and must be cut into smaller pieces. This dramatically increases the complexity of the problem, since each piece should be individually and manually pre-aligned. To the best of our knowledge, no automatic method can reliably locate such piece of tissue within its respective whole in the MRI slice, and align it without any prior information. We propose here a novel automatic approach to the joint problem of multi-modal registration between histology and MRI, when only a fraction of tissue is available from histology. The approach relies on the representation of images using their level lines so as to reach contrast invariance. Shape elements obtained via the extraction of bitangents are encoded in a projective-invariant manner, which permits the identification of common pieces of curves between two images. We evaluated the approach on human brain histology and compared resulting alignments against manually annotated ground truths. Considering the complexity of the brain folding patterns, preliminary results are promising and suggest the use of characteristic and meaningful shape elements for improved robustness and efficiency.	algorithmic efficiency;align (company);fréchet distance;image registration;keyboard shortcut;line level;medical imaging;modal logic;multimodal interaction;refinement (computing);resonance	Jonas Pichat;Juan Eugenio Iglesias;Sotiris Nousias;Tarek Yousry;Sébastien Ourselin;Marc Modat	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.21	robustness (computer science);artificial intelligence;computer vision;histology;fréchet distance;image registration;pattern recognition;computer science;magnetic resonance imaging	Vision	42.26473003691039	-78.27854655939836	135828
1e820b7416fa93a2591a76b9d263a5c15631886e	bayesian spatiotemporal segmentation of combined pet-ct data using a bivariate poisson mixture model	tumours bayes methods cancer computerised tomography image representation image segmentation lung markov processes medical image processing mixture models positron emission tomography spatiotemporal phenomena;positron emission tomography computed tomography image segmentation data models tumors bayes methods lungs;bivariate poisson distribution multimodality data fusion 4 d segmentation pet ct;temporal series bayesian spatiotemporal segmentation combined pet ct data unsupervised algorithm joint 4d pet ct image segmentation bivariate poisson mixture model bimodal data representation voxel labelling joint parameter estimation generalized four dimensional potts markov random field mrf spatio temporal coherence representation 4d registered pet ct data lung cancer patient tissue segmentation tumor identification	This paper presents an unsupervised algorithm for the joint segmentation of 4-D PET-CT images. The proposed method is based on a bivariate-Poisson mixture model to represent the bimodal data. A Bayesian framework is developed to label the voxels as well as jointly estimate the parameters of the mixture model. A generalized four-dimensional Potts-Markov Random Field (MRF) has been incorporated into the method to represent the spatio-temporal coherence of the mixture components. The method is successfully applied to 4-D registered PET-CT data of a patient with lung cancer. Results show that the proposed model fits accurately the data and allows the segmentation of different tissues and the identification of tumors in temporal series.	algorithm;bayesian network;bivariate data;ct scan;coherence (physics);fits;markov chain;markov random field;mixture model;polyethylene terephthalate;potts model;unsupervised learning;voxel	Zacharie Irace;Hadj Batatia	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		computer vision;pattern recognition;mathematics;scale-space segmentation;statistics	ML	47.40231690242221	-79.59467749683058	135916
1e2d76a05ca4b135d2661b9001c2cfdd1da6ac8b	neural network approach to background modeling for video object segmentation	bayes estimation;unsupervised learning;modelizacion;subtraction;belief networks;diseno circuito;arquitectura red;background modeling;bayesian classifier;image motion analysis;probability;image segmentation;analisis estadistico;image processing;neural networks;illumination;information security;neural nets;illumination change;video signal processing;unsupervised bayesian classifier;surveillance;surveillance neural network background modeling video object segmentation background subtraction unsupervised bayesian classifier natural scene sequence illumination change complex background motion probability statistical analysis;luminance;circuit design;sustraccion;procesamiento imagen;soustraction;bayesian methods;video processing;video sequences;automated surveillance;layout;probabilistic approach;architecture reseau;classification;traitement image;neural networks object segmentation layout lighting low pass filters statistics surveillance video sequences information security bayesian methods;modelisation;object segmentation;estimacion bayes;complex background motion;natural scene sequence;vigilancia;video object segmentation;senal video;statistical analysis;signal video;monitoring;object oriented;enfoque probabilista;approche probabiliste;video processing automated surveillance background subtraction neural networks nns object segmentation;analyse statistique;segmentation image;background subtraction;scene naturelle;statistics;algorithms artificial intelligence bayes theorem cluster analysis colorimetry computer graphics computer simulation data interpretation statistical feedback image enhancement image processing computer assisted information storage and retrieval lighting models statistical neural networks computer numerical analysis computer assisted pattern recognition automated photogrammetry signal processing computer assisted software subtraction technique video recording;video signal;oriente objet	This paper presents a novel background modeling and subtraction approach for video object segmentation. A neural network (NN) architecture is proposed to form an unsupervised Bayesian classifier for this application domain. The constructed classifier efficiently handles the segmentation in natural-scene sequences with complex background motion and changes in illumination. The weights of the proposed NN serve as a model of the background and are temporally updated to reflect the observed statistics of background. The segmentation performance of the proposed NN is qualitatively and quantitatively examined and compared to two extant probabilistic object segmentation algorithms, based on a previously published test pool containing diverse surveillance-related sequences. The proposed algorithm is parallelized on a subpixel level and designed to enable efficient hardware implementation.	acclimatization;algorithm;application domain;artificial neural network;authorization;bayesian network;biological neural networks;ieee xplore;image segmentation;lattice boltzmann methods;lithium;naive bayes classifier;parallel computing;physical object;pixel;relevance;rule (guideline);scientific publication;shadow paging;temporal logic;unsupervised learning;weight;zero suppression;biologic segmentation;width	Dubravko Culibrk;Oge Marques;Daniel Socek;Hari Kalva;Borko Furht	2007	IEEE Transactions on Neural Networks	10.1109/TNN.2007.896861	computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;lighting;scale-space segmentation;artificial neural network	Vision	51.32278796774875	-68.38416679947767	136014
87b66b68345f4e2c238c0f3af835a710d7afc538	size-adapted segmentation of individual mammographic microcalcifications	point estimation;active contour;image segmentation;computer aided diagnosis;pleomorphic clusters size adapted segmentation mammographic microcalcification computer aided diagnosis system automated segmentation method polar transformed active contours continuous wavelet representation shape adaptivity scale space representation region growing method pixel aggregation;scale space representation;pleomorphic clusters;wavelet transforms;computer aided diagnosis system;physics;accuracy;laplace equations;scale space;pixel aggregation;shape;wavelet transforms image segmentation mammography medical image processing;medical image processing;pixel;active rays;region growing method;shape adaptivity;size adapted segmentation;size adapted scale space analysis active rays continuous wavelet transform microcalcification segmentation region growing;continuous wavelet transform;mammography;size adapted scale space analysis;continuous wavelet transforms medical diagnostic imaging physics shape hospitals wavelet transforms morphology mammography active contours system performance;region growing;microcalcification segmentation;mammographic microcalcification;quantitative evaluation;polar transformed active contours;automated segmentation method;continuous wavelet transforms;continuous wavelet representation	Accurate Microcalcification (MC) segmentation is a crucial first step in morphology based computer aided diagnosis systems for microcalcifications in mammography. In this article we present an automated segmentation method of individual MCs adaptive to both size and shape variations. Size is estimated by active rays (polar-transformed active contours) on continuous wavelet representation while shape adaptivity is achieved by a subsequent region growing step. Following MC seed point annotation, contour point estimates are obtained by implementing active rays on an analytic scale-space representation in a coarse-to-fine strategy. Initial coarsest scale is automatically defined by analyzing MC responses across scales. A region growing method is used to delineate the final MC contour curve, with pixel aggregation constrained by the MC contour point estimates. The segmentation accuracy of the proposed method was quantitatively evaluated by means of area overlap by comparing automatically derived borders with manually traced ones provided by an expert radiologist. The proposed method achieved an area overlap of 0.68plusmn0.13 on a dataset of 67 individual microcalcifications, originating from pleomorphic clusters.	continuous wavelet;contour line;galaxy morphological classification;pixel;radiology;region growing;scale space	Nikolaos Arikidis;Anna Karahaliou;Spyros Skiadopoulos;Panayiotis Korfiatis;Eleni A. Likaki;George Panayiotakis;Lena Costaridou	2008	2008 8th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2008.4696833	computer vision;scale space;continuous wavelet transform;shape;computer science;point estimation;pattern recognition;active contour model;mathematics;accuracy and precision;region growing;image segmentation;pixel;wavelet transform;computer graphics (images)	Vision	41.48849735024676	-75.66356024693633	136175
292b5f656ede7fc1eaf30b190b30b4ffff47841b	evaluation of the effect of doubling atlases using midsagittal plane on multi-atlas based segmentation of brain structures	detectors;brain;market research;image segmentation;measurement;image edge detection;correlation	Normal human brain exhibits approximately bi-fold symmetry with respect to its midsagittal plane (MSP). The objective of this work is to investigate the effect of doubling atlases (i.e., reference images) used in multi-atlas fusion methods by exploiting the inherent bilateral symmetry of human brain. To this end, we perform automated segmentation of 15 subcortical structures using Local Weighted Voting (LWV) fusion method with varying number of atlases. We consider three specific scenarios for atlases while performing fusion: (i) fusion with original OASIS atlases, (ii) with atlases obtained by flipping the original atlases based on their MSP, and (iii) with both original and flipped atlases. Evaluations are performed on the publicly available OASIS dataset of 20 normal human brain MR images. One of the key findings of this study is that when the number of atlases available for fusion is less than 10, fusion by combining both the original and flipped atlases provided more accurate segmentations than using only the original atlases, or only the flipped atlases.	atlases;bilateral filter;diffusion weighted imaging;doubling;evaluation;exhibits as topic;max;numerous;silo (dataset);biologic segmentation	Subrahmanyam Gorthi	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591618	market research;computer vision;detector;computer science;artificial intelligence;mathematics;image segmentation;engineering drawing;correlation;measurement	Vision	41.58630192291238	-79.5629051954888	136508
1b5d136f75e66983f2b1d55ae5883fcee8404519	registration of multiple temporally related point sets using a novel variant of the coherent point drift algorithm: application to coronary tree matching	expectation maximization algorithms	We present a novel algorithm for the registration of multiple temporally related point sets. Although our algorithm is derived in a general setting, our primary motivating application is coronary tree matching in multi-phase cardiac spiral CT. Our algorithm builds upon the fast, outlier-resistant Coherent Point Drift (CPD) algorithm, but incorporates temporal consistency constraints between the point sets, resulting in spatiotemporally smooth displacement fields. We preserve the speed and robustness of the CPD algorithm by using the technique of separable surrogates within an EM (Expectation-Maximization) optimization framework, while still minimizing a global registration cost function employing both spatial and temporal regularization. We demonstrate the superiority of our novel temporally consistent group-wise CPD algorithm over a straightforward pair-wise approach employing the original CPD algorithm, using coronary trees derived from both simulated and real cardiac CT data. In all the tested configurations and datasets, our method presents lower average error between tree landmarks compared to the pairwise method. In the worst case, the difference is around few micrometers but in the better case, our method divides by two the error from the pairwise method. This improvement is especially important for a dataset with numerous outliers. With a fixed set of parameter that has been tuned automatically, our algorithm yields better results than the original CPD algorithm which shows the capacity to register without a priori information on an unknown dataset.	best, worst and average case;ct scan;coherence (physics);coherent;collaborative product development;computed tomography of the heart;displacement mapping;expectation–maximization algorithm;loss function;mathematical optimization;matrix regularization;motion compensation;point set registration;surrogates;temporal logic;terms of service	Séverine Habert;Parmeshwar Khurd;Christophe Chefd'Hotel	2013		10.1117/12.2004764	mathematical optimization;machine learning;data mining;physics	Vision	47.82931569604844	-79.06905877901255	136826
19301ba4817f48859ead8d664cca64f4b249e636	cardio-thoracic ratio measurement using non-linear least square approximation and local minimum		This paper presents a method to automatically measure cardio-thoracic ratio (CTR) from a chest radiographic images using non-linear least square approximation and local minimum. The proposed method consists of initial boundary point identification, cardiac diameter measurement, thoracic diameter measurement and cardio-thoracic ratio measurement. First, the initial boundary points used to approximate the region of thoracic cavity are identified using general human anatomy features. Then the non-linear least square approximation and local minimum are used to detect the heart boundary. Finally, the thoracic cage boundary is detected and the cardio-thoracic ratio can be measured. The proposed method is tested on a set of 255 chest radiographs. The experimental results are evaluated using correlation test between two sets of numerical measurement which are measured by our proposed method and by the radiologists. The evaluation reveals that the correlation result on CTR is about 78%.	approximation algorithm;level of measurement;maxima and minima;nonlinear system;numerical analysis;numerical method;radiography;radiology	Wasin Poncheewin;Monravee Tumkosit;Rajalida Lipikorn	2015	JCP		mathematical analysis;artificial intelligence;radiography;boundary (topology);pattern recognition;ratio measurement;thoracic cavity;nonlinear system;rib cage;computer science;least squares	Robotics	40.16556792607755	-77.47188731532881	136876
27d97a731a510c8ec48737dbb8a640ca202569ed	teichmüller shape descriptor and its application to alzheimer’s disease study	conformal welding;shape descriptor;shape analysis;teichmuller space	We propose a novel method to apply Teichmüller space theory to study the signature of a family of nonintersecting closed 3D curves on a general genus zero closed surface. Our algorithm provides an efficient method to encode both global surface and local contour shape information. The signature—Teichmüller shape descriptor—is computed by surface Ricci flow method, which is equivalent to solving an elliptic partial differential equation on surfaces and is numerically stable. We propose to apply the new signature to analyze abnormalities in brain cortical morphometry. Experimental results with 3D MRI data from Alzheimer’s disease neuroimaging initiative (ADNI) dataset [152 healthy control subjects versus 169 Alzheimer’s disease (AD) patients] demonstrate the effectiveness of our method and illustrate its potential as a novel surface-based cortical morphometry measurement in AD research.	algorithm;alzheimer's disease neuroimaging initiative;encode;morphometrics;numerical stability	Wei Zeng;Rui Shi;Yalin Wang;Shing-Tung Yau;Xianfeng Gu	2012	International Journal of Computer Vision	10.1007/s11263-012-0586-8	computer vision;teichmüller space;mathematical analysis;topology;computer science;shape analysis;mathematics;geometry	Vision	46.66519958882298	-77.23803855949103	137103
c92958fb5a693840fd24b69796613b985516d2a0	geometrically guided fuzzy c-means clustering for multivariate image segmentation	pattern clustering;fuzzy c mean;unsupervised clustering;image segmentation;geometry;unsupervised segmentation;fuzzy set theory;fuzzy set theory image segmentation pattern clustering geometry;fuzzy c means clustering;homogeneous regions geometrically guided fuzzy c means clustering multivariate image segmentation unsupervised clustering technique unsupervised segmentation pixel geometrical relationship semi supervised fcm technique geometrically guided fcm gg fcm;image segmentation spatial filters clustering algorithms fuzzy sets error correction lapping shape prototypes	Fuzzy C-means (FCM) clustering is an unsupervised clustering technique and is often used for the unsupervised segmentation of multivariate images. The segmentation of the image in meaningful regions with FCM is based on spectral information only. The geometrical relationship between neighbouring pixels is not used. In this paper, a semisupervised FCM technique is used to add geometrical information during clustering. The local neighbourhood of each pixel determines the condition of each pixel, which guides the clustering process. Segmentation experiments with the Geometrically Guided FCM (GG-FCM) show improved segmentation above traditional FCM such as more homogeneous regions and less spurious pixels.	cluster analysis;experiment;fuzzy cognitive map;gadu-gadu;image segmentation;pixel;semi-supervised learning;unsupervised learning	J. C. Noordam;W. H. A. M. Van den Broek;Lutgarde M. C. Buydens	2000		10.1109/ICPR.2000.905376	correlation clustering;computer vision;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;segmentation-based object categorization;pattern recognition;mathematics;geometry;fuzzy set;image segmentation;cluster analysis;scale-space segmentation	Vision	43.481093655156634	-70.88621004026125	137448
68af2ecc9fb8faf9e98937a5c9755c2cb61b385b	segmentation and quantitative evaluation of brain mri data with a multiphase 3d implicit deformable model	brain;active contour;white matter;tissues;level set;false negative;three dimensional;biomedical engineering;magnetic resonance imaging;image quality;numerical computation;gray matter;brain imaging;a priori information;false positive;brain tissue;deformable model;quantitative evaluation;cerebrospinal fluid	Segmentation of three-dimensional anatomical brain images into tissue classes has applications in both clinical and research settings. This paper presents the implementation and quantitative evaluation of a four-phase three-dimensional active contour implemented with a level set framework for automated segmentation of brain MRIs. The segmentation algorithm performs an optimal partitioning of three-dimensional data based on homogeneity measures that naturally evolves to the extraction of different tissue types in the brain. Random seed initialization was used to speed up numerical computation and avoid the need for a priori information. This random initialization ensures robustness of the method to variation of user expertise, biased a priori information and errors in input information that could be influenced by variations in image quality. Experimentation on three MRI brain data sets showed that an optimal partitioning successfully labeled regions that accurately identified white matter, gray matter and cerebrospinal fluid in the ventricles. Quantitative evaluation of the segmentation was performed with comparison to manually labeled data and computed false positive and false negative assignments of voxels for the three organs. We report high accuracy for the two comparison cases. These results demonstrate the efficiency and flexibility of this segmentation framework to perform the challenging task of automatically extracting brain tissue volume contours.		Elsa D. Angelini;Ting Song;Brett D. Mensh;Andrew F. Laine	2004		10.1117/12.535860	computer vision;pathology;computer science;artificial intelligence;segmentation-based object categorization;image segmentation;scale-space segmentation	Vision	40.04990872446837	-77.74231507712578	137512
3e4a8cf51a139dc36c1684f6c3d47e3e09f18012	segmentation of thalamic nuclei based on tensorial morphological gradient of diffusion tensor fields	diffusion mri;animals;mathematical morphology;brain;image segmentation;tensile stress;diffusion tensor images;biodiffusion;dti;segmentation;tensile stress image segmentation diffusion tensor imaging magnetic resonance imaging clustering algorithms partitioning algorithms humans surface morphology relays phased arrays;magnetic resonance image;surface morphology;watershed transform;medical image processing;magnetic resonance imaging;watershed transform diffusion tensor imaging segmentation tensorial morphological gradient;transforms;mri;clustering algorithms;thalamic nuclei;tensorial morphological gradient;humans;diffusion tensor imaging;relays;mathematical morphology thalamic nuclei tensorial morphological gradient segmentation magnetic resonance imaging mri diffusion tensor imaging dti diffusion mri watershed transform;ensorialt morphological gradient;medical image processing biodiffusion biomedical mri brain image segmentation;diffusion tensor;partitioning algorithms;biomedical mri;phased arrays	Although thalamic nuclei are not directly visible on conventional anatomical magnetic resonance images (MRI), it is possible to observe differences between the nuclei using diffusion tensor imaging (DTI), because of their distinct fiber orientation. This work presents a method to segment the various nuclei of human thalamus using diffusion MRI. Our approach is to use the watershed transform and other concepts from mathematical morphology to segment the nuclei. However, to segment structures using the tensor data produced with DTI (as opposed to scalar images) the concept of a tensorial morphological gradient (TMG) needs to be introduced. Based on the TMG, segmentation of the nuclei of the thalamus was successful using the watershed transform. Our segmentation is consistent with a histological atlas. Since the proposed method, as opposed to the majority of the DTI-based segmentation methods, does not require manual seed and/or surface placement, its results are highly repeatable.	atlas autocode;mathematical morphology;morphological gradient;random seed;resonance;tmg (language);watershed (image processing)	Letícia Rittner;Roberto de Alencar Lotufo;Jennifer S. W. Campbell;G. Bruce Pike	2010	2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2010.5490203	diffusion mri;computer vision;radiology;medicine;computer science;magnetic resonance imaging;mathematics;nuclear magnetic resonance;nuclear medicine	Vision	41.27380141012927	-78.16073530385961	137591
7bd1c8dc685b0cf66fa10806710988b66717de99	a multiscale dynamic programming procedure for boundary detection in ultrasonic artery images	carotid artery common costs and cost analysis femoral artery humans image processing computer assisted ultrasonography;analisis imagen;dynamic programming;optimal solution;image features;carotid;informatica biomedical;programacion dinamica;biomedical data processing;publikationer;cost function;methode echelle multiple;ultrasound;edge detection;informatique biomedicale;hombre;konferensbidrag;metodo escala multiple;dynamic program;indexing terms;carotida;funcion coste;deteccion contorno;detection contour;circulatory system;exploration ultrason;human factors;weighted sums;medical image processing;human;programmation dynamique;artiklar;rapporter;dynamic programming arteries humans cost function ultrasonic variables measurement councils anthropometry heuristic algorithms robustness ultrasonic imaging;dynamic program ming;fonction cout;image analysis;global optimization;exploracion ultrasonido;multiscale method;sistema difuso;systeme flou;carotide;medical diagnostic imaging cost function fuzzy expression forms vessel interfaces operator intervention global optimality subjectiveness clinical environment human expert tracings geometrical characteristics coarse scale image fine scale image overall analysis time manual procedure;appareil circulatoire;boundary detection;geometric constraints;analyse image;aparato circulatorio;biomedical ultrasonics;fuzzy system;blood vessels;sonography;homme;biomedical ultrasonics dynamic programming edge detection blood vessels medical image processing	Ultrasonic measurements of human carotid and femoral artery walls are conventionally obtained by manually tracing interfaces between tissue layers. The drawbacks of this method are the interobserver variability and inefficiency. Here, the authors present a new automated method which reduces these problems. By applying a multiscale dynamic programming (DP) algorithm, approximate vessel wall positions are first estimated in a coarse-scale image, which then guide the detection of the boundaries in a fine-scale image. In both cases, DP is used for finding a global optimum for a cost function. The cost function is a weighted sum of terms, in fuzzy expression forms, representing image features and geometrical characteristics of the vessel interfaces. The weights are adjusted by a training procedure using human expert tracings. Operator interventions, if needed, also take effect under the framework of global optimality. This reduces the amount of human intervention and, hence, variability due to subjectiveness. By incorporating human knowledge and experience, the algorithm becomes more robust. A thorough evaluation of the method in the clinical environment shows that interobserver variability is evidently decreased and so is the overall analysis time. The authors conclude that the automated procedure can replace the manual procedure and leads to an improved performance.	approximation algorithm;blood vessel tissue;davis–putnam algorithm;dynamic programming;global optimization;heart rate variability;inter-rater reliability;loss function;spatial variability;structure of femoral artery;ultrasonics (sound);walls of a building;weight function;anatomical layer	Quan Liang;Inger Wendelhag;John Wikstrand;Tomas Gustavsson	2000	IEEE Transactions on Medical Imaging	10.1109/42.836372	computer vision;image analysis;simulation;edge detection;index term;computer science;artificial intelligence;dynamic programming;circulatory system;ultrasound;feature;global optimization	Vision	40.91875489887298	-78.7734579163893	137615
251050460e5402c13c3fe841f86b958316b08cc1	machine vision based automatic separation of touching convex shaped objects	corner detection;computer vision;curvature analysis;fourier approximation;automatic separation	An automatic separation procedure of touching objects based on their curvature values was developed. Plastic bottles were used as model geometries. Using elliptic Fourier descriptors (EFDs), the image contours were smoothed avoiding the occurrence of local pseudo-corners resulted from the image acquisition inefficiencies. Then, nodal points were determined by evaluating the curvature along the smoothed boundary of the images. Nodal points are those points at which the curvature fell below a threshold. In situations where multiple nodal points were found, the 'nearest-neighbor' and 'critical radial distance' criteria were used to draw the segmentation lines. The algorithm was tested to separate bottles of different sizes and shapes with different touching scenarios and showed a success rate of more than 99%. At the current stage of development, the algorithm can effectively separate objects with smooth boundaries and is thus suitable for separating mechanical objects with well-defined geometries. Further work is being done to make it robust enough to handle biological entities as well as to deal with scenarios where one or more objects are surrounded by a multitude of objects.	machine vision	H. K. Mebatsion;J. Paliwal	2012	Computers in Industry	10.1016/j.compind.2012.05.005	corner detection;computer vision;computer science;engineering drawing	Vision	46.955707057365416	-67.26444115009348	137660
29d40b3d4365ddf1bc56ad6b9a2331ba3e511f76	automatic extraction of femur contours from hip x-ray images	active contour;x ray imaging;model based approach;mr imaging;medical image;computer analysis	Extraction of bone contours from x-ray images is an important first step in computer analysis of medical images. It is more complex than the segmentation of CT and MR images because the regions delineated by bone contours are highly nonuniform in intensity and texture. Classical segmentation algorithms based on homogeneity criteria are not applicable. This paper presents a model-based approach for automatically extracting femur contours from hip x-ray images. The method works by first detecting prominent features, followed by registration of the model to the x-ray image according to these features. Then the model is refined using active contour algorithm to get the accurate result. Experiments show that this method can extract the contours of femurs with regular shapes, despite variations in size, shape and orientation.	active contour model;algorithm;contour line;distortion;experiment;medical imaging;radiography;sensor	Ying Chen;Xianhe Ee;Wee Kheng Leow;Tet Sen Howe	2005		10.1007/11569541_21	computer vision;computer science;active contour model;computer graphics (images)	Vision	41.42814678007548	-75.82862025689325	137748
c741f1e0838c11459c2cf8ef0b0bf806d9ec2956	a fully automatic method for segmenting retinal artery walls in adaptive optics images	mathematical morphology;retina imaging;active contour model;adaptive optics	Adaptive optics imaging of the retina has recently proven its capability to image micrometric structures such as blood vessels, involved in common ocular diseases. In this paper, we propose an approach for automatically segmenting the walls of retinal arteries in the images acquired with this technology. The walls are modeled as four curves approximately parallel to a previously detected reference line located near the vessel center (axial reflection). These curves are first initialized using a tracking procedure and then more accurately positioned using an active contour model embedding a parallelism constraint. We consider both healthy and pathological subjects in the same framework and show that the proposed method applies in all cases. Extensive experiments are also proposed, by analyzing the robustness of the axial reflections detection, the influence of the tracking parameters as well as the performance of the tracking and the active contour model. Noticeably, the results show a good robustness for detecting axial reflections and a moderate influence of the tracking parameters. Compared to a naive initialization, the active contour model coupled with the tracking also offers faster convergence and better accuracy while keeping an overall error smaller or very near the inter-physicians error. © 2015 Elsevier B.V. All rights reserved.	active contour model;experiment;parallel computing;reflection (computer graphics);robustness (computer science);sensor	Nicolas Lermé;Florence Rossant;Isabelle Bloch;Michel Pâques;Edouard Koch;Jonathan Benesty	2016	Pattern Recognition Letters	10.1016/j.patrec.2015.10.011	computer vision;mathematical morphology;computer science;active contour model;adaptive optics	Vision	41.33414482504082	-76.4225577796919	137959
1c4d948a0b035fd66b4663c7d1ad36ef6da6b22a	morphology for matrix data: ordering versus pde-based approach	matrix valued images;mathematical morphology;image processing;positive semidefinite matrix;nonlinear partial differential equation;shape analysis;morphological operation;satisfiability;symmetric matrices;rotation invariance;erosion;nonlinear equation;digital image;nonlinear system;continuous dependence;dilation;diffusion tensor mri;loewner ordering;partial order	Matrix fields are becoming increasingly important in digital imaging. In order to perform shape analysis, enhancement or segmentation of such matrix fields, appropriate image processing tools must be developed. This paper extends fundamental morphological operations to the setting of matrices, in the literature sometimes referred to as tensors despite the fact that matrices are only rank two tensors. The goal of this paper is to introduce and explore two approaches to mathematical morphology for matrix-valued data: One is based on a partial ordering, the other utilises nonlinear partial differential equations (PDEs). We start by presenting definitions for the maximum and minimum of a set of symmetric matrices since these notions are the cornerstones of the morphological operations. Our first approach is based on the Loewner ordering for symmetric matrices, and is in contrast to the unsatisfactory component-wise techniques. The notions of maximum and minimum deduced from the Loewner ordering satisfy desirable properties such as rotation invariance, preservation of positive semidefiniteness, and continuous dependence on the input data. These properties are also shared by the dilation and erosion processes governed by a novel nonlinear system of PDEs we are proposing for our second approach to morphology on matrix data. These PDEs are a suitable counterpart of the nonlinear equations known from scalar continuous-scale morphology. Both approaches incorporate information simultaneously from all matrix channels rather than treating them independently. In experiments on artificial and real medical positive semidefinite matrix-valued images we contrast the resulting notions of erosion, dilation, opening, closing, top hats, morphological derivatives, and shock filters stemming from these two alternatives. Using a ball shaped structuring element we illustrate the properties and performance of our orderingor PDE-driven morphological operators for matrix-valued data.	closing (morphology);concatenation;digital imaging;dilation (morphology);emoticon;erosion (morphology);experiment;image processing;laplacian matrix;mathematical morphology;maxima and minima;nonlinear system;numerical analysis;numerical method;numerical partial differential equations;opening (morphology);reflections of signals on conducting lines;scott continuity;shape analysis (digital geometry);simulation;stemming;structuring element;the matrix	Bernhard Burgeth;Andrés Bruhn;Stephan Didas;Joachim Weickert;Martin Welk	2007	Image Vision Comput.	10.1016/j.imavis.2006.06.002	partially ordered set;computer vision;mathematical optimization;mathematical analysis;discrete mathematics;mathematical morphology;erosion;nonlinear system;computer science;shape analysis;mathematics;positive-definite matrix;dilation;digital image;symmetric matrix;satisfiability	Vision	53.55864293196913	-70.71463087644563	138079
1e2944d3aa13e6a73b4df4882b18c751d7a6a1af	manifold parametrization of the left ventricle for a statistical modelling of its complete anatomy	frechet mean;heart;lv anatomic atlases;differential geometry;statistical modelling;statistical models;statistical model;left ventricle;activity pattern;distortion;mechanics;manifold parametrization;statistical modeling;diffusion tensor imaging;point distribution models;modeling;diffusion;diffusion tensor	Distortion of Left Ventricle (LV) external anatomy is related to some dysfunctions, such as hypertrophy. The architecture of myocardial fibers determines LV electromechanical activation patterns as well as mechanics. Thus, their joined modelling would allow the design of specific interventions (such as peacemaker implantation and LV remodelling) and therapies (such as resynchronization). On one hand, accurate modelling of external anatomy requires either a dense sampling or a continuous infinite dimensional approach, which requires non-Euclidean statistics. On the other hand, computation of fiber models requires statistics on Riemannian spaces. Most approaches compute separate statistical models for external anatomy and fibers architecture. In this work we propose a general mathematical framework based on differential geometry concepts for computing a statistical model including, both, external and fiber anatomy. Our framework provides a continuous approach to external anatomy supporting standard statistics. We also provide a straightforward formula for the computation of the Riemannian fiber statistics. We have applied our methodology to the computation of complete anatomical atlas of canine hearts from diffusion tensor studies. The orientation of fibers over the average external geometry agrees with the segmental description of orientations reported in the literature.	angularjs;b-spline;circuit complexity;computation;distortion;ion implantation;logical volume management;new product development;population;sampling (signal processing);software metric;statistical model	Debora Gil;Jaume Garcia-Barnes;Aura Hernández-Sabaté;Enric Martí	2010		10.1117/12.844480	statistical model;differential geometry;mathematical optimization	DB	43.68202506423563	-79.61412763367176	138502
cd0889103442acfe77afde3420062a52cae9d63c	a generic fuzzy rule based image segmentation algorithm	generic fuzzy rules;fuzzy c mean;image segmentation;fuzzy rules;possibilistic c means;fuzzy clustering;fuzzy rule base;membership function;spatial relationships;quantitative evaluation;spatial information	Fuzzy rule based image segmentation techniques tend in general, to be application dependent with the structure of the membership functions being predefined and in certain cases, the corresponding parameters being manually determined. The net result is that the overall performance of the segmentation technique is very sensitive to parameter value selections. This paper addresses these issues by introducing a generic fuzzy rule based image segmentation (GFRIS) algorithm, which is both application independent and exploits inter-pixel spatial relationships. The GFRIS algorithm automatically approximates both the key weighting factor and threshold value in the definitions of the fuzzy rule and neighbourhood system, respectively. A quantitative evaluation is presented between the segmentation results obtained using GFRIS and the popular fuzzy c-means (FCM) and possibilistic c-means (PCM) algorithms. The results demonstrate that GFRIS exhibits a considerable improvement in performance compared to both FCM and PCM, for many different image types.	algorithm;fuzzy rule;image segmentation	Gour C. Karmakar;Laurence Dooley	2002	Pattern Recognition Letters	10.1016/S0167-8655(02)00069-7	spatial relation;membership function;defuzzification;fuzzy clustering;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;spatial analysis;image segmentation;scale-space segmentation;fuzzy set operations	Vision	42.947023910858775	-71.50020771571856	138865
d06e59aa288de36670908201081c3f3b933de6ca	probabilistic complex phase representation objective function for multimodal image registration	probabilistic;complex;thesis or dissertation;volumetric;phase;image;multi scale;computer;objective;doctoral thesis;medical;registration;multimodal;vision	An interesting problem in computer vision is that of image registration, which plays an important role in many vision-based recognition and motion analysis applications. Of particular interest among data registration problems are multimodal image registration problems, where the image data sets are acquired using different imaging modalities. There are several important issues that make real-world multimodal registration a difficult problem to solve. First, images are often characterized by illumination and contrast non-uniformities. Such image non-uniformities result in local minima along the convergence plane that make it difficult for local optimization schemes to converge to the correct solution. Second, realworld images are often contaminated with signal noise, making the extraction of meaningful features for comparison purposes difficult to accomplish. Third, feature space differences make performing direct comparisons between the different data sets with a reasonable level of accuracy a challenging problem. Finally, solving the multimodal registration problem can be computationally expensive for large images. This thesis presents a probabilistic complex phase representation (PCPR) objective function for registering images acquired using different imaging modalities. A probabilistic multi-scale approach is introduced to create image representations based on local phase relationships extracted using complex wavelets. An objective function is introduced for assessing the alignment between the images based on a Geman-McClure error distribution model between the probabilistic complex phase representations of the images. Experimental results show that the proposed PCPR objective function can provide improved registration accuracies when compared to existing objective functions.	analysis of algorithms;computer vision;converge;feature vector;image registration;loss function;mathematical optimization;maxima and minima;multimodal interaction;multimodal learning;noise (electronics);optimization problem;wavelet	Alexander Wong	2010			computer vision;computer science;artificial intelligence;machine learning	Vision	48.913458420404325	-74.8949961649162	138878
92de20b8028760e3dd6e0b22feadd6f7448f30fc	3d mapping of airway wall thickening in asthma with msct: a level set approach	databases;computed tomography;tissues;chronic obstructive pulmonary disease;lung imaging;diseases and disorders	Abstract. Assessing the airway wall thickness in multi slice comput ed tomography (MSCT) as image marker for airway disease phenotyping such asthma and CO PD is a current trend and challenge for the scientific community working in lung imaging. This paper addresses the same problem from a different point of view: considering the expected wall thickness-to-lumen-radius ratio for a normal subject as known and constant throughout the whole airway tree, the aim is to build up a 3D map of airway wall regions of larger thickness and to define an overall score able to highlight a pathological status. In this respect, the local dimension (caliber) of the previously segmented airway lumen is obtained on each point by exploiting the granulometry morphological operator. A level set function is defined based on this caliber information and on the expected wall thickness ratio, which allows obtaining a good estimate of the airway wall throughout all segmented lumen generations. Next, the vascular (or mediastinal dense tissue) contact regions are automatically detected and excluded from analysis. For the remaining airway wall border points, the real wall thickness is estimated based on the tissue density analysis in the airway radial direction; thick wall points are highlighted on a 3D representation of the airways and several quantification scores are defined. The proposed approach is fully automatic and was evaluated (proof of concept) on a patient selection coming from different databases including mild, severe asthmatics and normal cases. This pre liminary evaluation confirms the discriminative power of the proposed approach regarding different phenotypes and is cu rrently extending to larger cohorts. Keywords: airway wall thickening, level set, lumen caliber, mo rphological granulometry, wall thickness ratio, asthma 1. Introduction Airway remodeling (lumen narrowing and/or wall thickening) is a well-known indicator of the pathologic process in asthma and its quantitative evaluation in multi slice computed tomography (MSCT) is seen as a key marker for disease phenotyping and follow-up. The current wall-thickness quantification methods in the literature are focusing on an accurate segmentation of the airway wall, most of them performing on a 2D cross-s ectional basis, with some recent work on fully-3D approaches. Airway quantification in cross-section planes orthogonal to the bronchus axis better allowed the estimation of eventual heterogeneity in the airway response between lung lobes. It remains however limited to a set of sampling locations and might thus miss information on localized airway remodeling; in addition, measurements at ai rway subdivisions cannot be obtained. Several approaches for in-plane segmentation of the airway wall cross section have been developed and we refer in this respect to the works in [1] based on full-width at half maximum criterion, [2] using contour detection by the Laplacian of the Gaussian, [3] where airway wall measurements were obtained via dynamic programming, [4, 5] based on phase congruency and [6] exploiting active contour segmentation. Recently, fully-3D segmentation and quantification approaches of the airway wall were proposed to overcome the drawbacks of the 2D (cross-sectional) me thods. One class of approaches used ac tive surface models with auto-collision management [7, 8] to segment the inner and outer borders of the airway wall. Similarly, [9 ] proposed a surface evolution	ct scan	Catalin I. Fetita;Pierre-Yves Brillet;Ruth Hartley;Philippe A. Grenier;Christopher Brightling	2014		10.1117/12.2042997	computed tomography	Vision	40.8989067082085	-79.90071517752313	139133
b10dbf3fbe71d514651e6601799ff6e0777af2bd	comparison of error bounds for non-parametric dominant point detection	digital edge curves error bounds nonparametric dominant point detection slope estimation rdp method digital error bound third error bound line segment length curve variety termination condition;edge detection;digitization dominant point detection nonparametric non heuristic error bound comparison;computational geometry;decision support systems pattern recognition estimation approximation methods educational institutions thumb;edge detection computational geometry	This paper compares three error bounds that can be used to make dominant point detection methods non-parametric. The error bounds are based on the error in slope estimation due to digitization. However, each bound is derived from a different approach. This results into different natures of the three methods and different values. The error bounds can be incorporated in non-parametric framework for dominant point detection. Here, the impact of these error bounds is studied in the context of the non-parametric version of the widely used RDP method of dominant point detection. It is seen that the digital error bound (the third error bound), which depends on both the length and the slope of the line segment, provides the most balanced dominant point detection results for a variety of curves. This analysis is useful for optimal choice of error bound or termination condition in dominant point detection methods.	american and british english spelling differences;estimation theory;remote desktop protocol	Dilip K. Prasad;Hiok Chai Quek	2013	2013 9th International Conference on Information, Communications & Signal Processing	10.1109/ICICS.2013.6782883	computer vision;mathematical optimization;combinatorics;edge detection;computational geometry;computer science;mathematics;statistics	Robotics	48.68320105319196	-72.84519694551278	139215
7a63ecea1609c1a0c40c2181a6ef8f9afd5cd468	automated threshold detection using a pyramid data structure	1d image;optimal solution;histograms;image segmentation;noise computerised pattern recognition computerised picture processing data structures;computerised pattern recognition;smoothing computerised picture processing pattern recognition automated threshold detection pyramid data structure noisy histogram 1d image;smoothing methods;biomedical engineering;shape;smoothing;data structures;noise reduction;magnetic resonance imaging;pattern recognition;computerised picture processing;pyramid data structure;noise shaping;magnetic noise;data structures histograms image segmentation smoothing methods shape noise shaping magnetic resonance imaging noise reduction biomedical engineering magnetic noise;point of view;data structure;noisy histogram;automated threshold detection;noise	An approach to detect the thresholds automatically in a noisy histogram is discussed. The histogram is considered as a 1D image. Then, a histogram pyramid is built for smoothing the noisy histogram and eliminating the small noise peaks and valleys. The threshold is first found at the higher level of the histogram pyramid and then mapped back to the successive levels. The results show that the thresholds obtained are close to the optimal solution in terms of the statistical point of view and are of reasonable values in some complicated and noisy histograms. >	data structure	Ting Jiang;Michael B. Merickel;Edward A. Parrish	1988		10.1109/ICPR.1988.28329	computer vision;speech recognition;noise shaping;data structure;shape;computer science;noise;histogram matching;pattern recognition;noise reduction;balanced histogram thresholding;histogram;image segmentation;adaptive histogram equalization;histogram equalization;smoothing;image histogram	Vision	44.94428362596631	-70.91864622815987	139219
dc87f4f848df50f44496adcf650c02cf8dcafc17	evaluating hierarchical graph-based segmentation	graph theory;image segmentation humans pattern recognition inspection tree graphs informatics image processing bridges brightness pixel;image segmentation;hierarchical graph based segmentation;image segmentation graph theory;region size variation;region size variation hierarchical graph based segmentation human visual inspection;human visual inspection	Using real world images, two hierarchical graph-based segmentation methods are evaluated with respect to segmentations produced by humans. Global and local consistency measures do not show big differences between the two representative methods although human visual inspection of the results show advantages for one method. To a certain extent this subjective impression is captured by the new criteria of 'region size variation'	local consistency;visual inspection	Yll Haxhimusa;Adrian Ion;Walter G. Kropatsch	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.511	computer vision;range segmentation;computer science;graph theory;machine learning;segmentation-based object categorization;pattern recognition;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;connected-component labeling	Robotics	43.14825962874839	-66.88964610114593	139262
c380365c4cd2efd7587f93cc221f485f6a29de5f	multi-scale crest line extraction based on half gaussian kernels	detector filters multiscale crest line extraction half gaussian kernels image processing second order filters rotating gaussian semifilters ridge anisotropic detector valley anisotropic detector;image processing feature extraction filtering theory gaussian processes;kernel noise detectors indexes biomedical imaging image edge detection smoothing methods;steerable filter crest lines multi scale	Crest line extraction has always been a challenging task in image processing and its applications. It is possible to detect ridges and valleys in images using second order filters. In order to estimate crest lines of variable widths, a multi-scale analysis of the image is required. In this paper we propose a new ridge/valley detection method in images based on the difference of rotating Gaussian semi filters adapted in a multi-scale process. Due to the directional filters, we obtain a new ridge/valley anisotropic detector enabling very precise detection of ridge/valley of varied widths. Moreover, as the detector filters compute the two directions of crest lines, even highly bended crest lines are correctly extracted. Numerical comparisons with other oriented Gaussian filters and results on real images validate the interest of this method.	gaussian blur;image processing;kadir–brady saliency detector;numerical method;ridge detection;semiconductor industry;uncanny valley	Baptiste Magnier;Arezki Aberkane;Philippe Borianne;Philippe Montesinos;Christophe Jourdan	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854575	computer vision;gaussian blur;gaussian filter	Robotics	48.0628210145438	-67.59592601172149	139782
4a88e71ed1b1820f742c2590bd31a07bb7061659	automatic segmentation of brain tissues and mr bias field correction using a cigital brain atlas	parametric model;automatic segmentation;psi_mic;brain atlas;parameter estimation;brain tissue	This paper proposes a method for fully automatic segmentation of brain tissues and MR bias field correction using a digital brain atlas. We have extended the EM segmentation algorithm, including an explicit parametric model of the bias field. The algorithm interleaves classification with parameter estimation, yielding better results at every iteration. The method can handle multi-channel data and slice-per-slice constant offsets, and is fully automatic due to the use of a digital brain atlas.	algorithm;brain atlas;estimation theory;iteration;parametric model	Koenraad Van Leemput;Frederik Maes;Dirk Vandermeulen;Paul Suetens	1998		10.1007/BFb0056312	computer vision;speech recognition;parametric model;computer science;bioinformatics;mathematics;estimation theory;statistics	NLP	52.3166366417076	-76.88427278174755	139829
82d2abf608bbe01b57799a1758e12dc5e2d2e568	direct skeleton extraction using river-lake algorithm	computers;rivers;single pixel width;lakes;skeleton shape iterative methods lakes rivers computers transforms;skeleton extraction thinning fast non iterative;non iterative;set theory;skeleton;fast;iterative methods;skeleton extraction;shape;feature extraction;skeletonization algorithms;transforms;direct skeleton extraction;vertical set;skeletonization problem;high speed process;high speed process direct skeleton extraction river lake algorithm skeletonization algorithms skeletonization problem vertical set horizontal set single pixel width;river lake algorithm;image thinning;iteration method;horizontal set;high speed;thinning;set theory feature extraction image thinning	About 200 skeletonization algorithms have been developed since 1950. It shows that there is a large variety of the problem which arises from the combination of the criteria used whether in its accuracy (size and shape) or speed. In this paper, a new idea to enrich skeletonization problem by reversing the approach is proposed. This new idea has an advantage in the algorithm speed. The proposed approach collects points which are assumed as the center point in vertical and horizontal set which every vertical point is connected to horizontal set through projection. The result of the skeleton extraction has single-pixel width which represents starting shape and is conducted in high speed process.	algorithm;pixel;reversing: secrets of reverse engineering;topological skeleton	Iping Supriana;Peb Ruswono Aryan	2011	Proceedings of the 2011 International Conference on Electrical Engineering and Informatics	10.1109/ICEEI.2011.6021850	computer vision;computer science;iterative method;engineering drawing	Robotics	45.08976176077741	-66.19230342556075	140086
fdd72e73d7324d7e6c7dea11dc5c74c4fb68fa8d	remote sensing data analysis based on hierarchical image approximation with previously computed segments		  In this report, a method for remote sensing data segmentation is proposed. For image approximation, the segments with minimal  difference between adjusted threshold parameter and segment variance or entropy are selected. The generation of image hierarchical  segmentation based on intensity gradient analysis and computation of object segments accounting for variance (entropy) are  considered. For the algorithm generalization, the method of sequential image approximations within prescribed standard deviation  values by means of image segments computed in merging/splitting technique is proposed. The peculiarities of data structures  for optimization of computations are discussed. The comparison with known methods is outlined.    	approximation	Philipp Galiano;Mikhail V. Kharinov;Viktor Kuzenny	2011		10.1007/978-3-642-19766-6_10	computer vision;geography;pattern recognition;remote sensing	Robotics	42.86892766529734	-70.17503632122944	140317
44b1155d22927fabf1bcda3956dd78dee0c631a1	analysis of the direct fourier method for computer tomography	metodo directo;computerized axial tomography;tomodensitometria;radiodiagnostic;medical imagery;metodo matematico;interpolation;mathematical method;transformacion radon;computed tomography;medical diagnostic imaging direct fourier method x ray projections function reconstruction cartesian coordinates polynomial interpolation sinc functions gaussians kaiser bessel functions numerical experiments basis functions resampling;complexite calcul;bessel function;estudio comparativo;performance;simulation;polynomial interpolation;radon transformation;tomography design for manufacture interpolation fourier transforms image reconstruction image quality polynomials gaussian approximation two dimensional displays x ray imaging;simulacion;polynomials;splines mathematics;etude comparative;reconstruction image;radiodiagnostico;complejidad computacion;tomodensitometrie;reconstruccion imagen;fourier transformation;algorithms fourier analysis humans phantoms imaging reproducibility of results tomography x ray computed;computational complexity;image reconstruction;medical image processing;polynomials interpolation computerised tomography image reconstruction medical image processing splines mathematics;transformation fourier;comparative study;computerised tomography;methode mathematique;imagerie medicale;transformation radon;imageneria medical;radiodiagnosis;rendimiento;numerical experiment;methode directe;direct method;fourier method;transformacion fourier;x rays	The authors develop a direct Fourier method (DFM) for reconstructing a function from its X-ray projections. They introduce a framework that can be used to get a quantitative comparison between different choices of basis functions in the step of resampling from polar to Cartesian coordinates. They use the framework to compare polynomial interpolation, approximated sinc-functions, Gaussians, splines, and Kaiser-Bessel functions. The resulting algorithm is very fast, requiring 12.5 N/sup 2/ log, N+49 N/sup 2/ flops. Numerical experiments show it to be efficient.	approximation algorithm;basis function;bessel filter;ct scan;cartesian closed category;choice behavior;experiment;flops;interpolation imputation technique;numerical linear algebra;polynomial interpolation;projections and predictions;sinc function;spline (mathematics);tomography;deferoxamine mesylate	Johan Waldén	2000	IEEE Transactions on Medical Imaging	10.1109/42.845179	iterative reconstruction;direct method;fourier transform;radiology;performance;interpolation;bessel function;polynomial interpolation;comparative research;calculus;mathematics;geometry;computed tomography;computational complexity theory;polynomial	Vision	52.838249683198406	-78.99949151326403	140339
be125753f6e5f818830bfd88ee6723f78891dcd5	combining anatomical manifold information via diffeomorphic metric mappings for studying cortical thinning of the cingulate gyrus in schizophrenia	gaussian random field;cingulate gyrus;large deformation diffeomorphic metric mapping lddmm;schizophrenia;structure and function;the laplace beltrami operator;large deformation diffeomorphic metric mapping;statistical inference;cortical thickness;laplace beltrami operator;coordinate system;hypothesis test	Spatial normalization is a crucial step in assessing patterns of neuroanatomical structure and function associated with health and disease. Errors that occur during spatial normalization can influence hypothesis testing due to the dimensionalities of mapping algorithms and anatomical manifolds (landmarks, curves, surfaces, volumes) used to drive the mapping algorithms. The primary aim of this paper is to improve statistical inference using multiple anatomical manifolds and large deformation diffeomorphic metric mapping (LDDMM) algorithms. We propose that combining information generated by the various manifolds and algorithms improves the reliability of hypothesis testing. We used this unified approach to assess variation in the thickness of the cingulate gyrus in subjects with schizophrenia and healthy comparison subjects. Three different LDDMM algorithms for mapping landmarks, curves and triangulated meshes were used to transform thickness maps of the cingulate surfaces into an atlas coordinate system. We then tested for group differences by combining the information from the three types of anatomical manifolds and LDDMM mapping algorithms. The unified approach provided reliable statistical results and eliminated ambiguous results due to surface mismatches. Subjects with schizophrenia had non-uniform cortical thinning over the left and right cingulate gyri, especially in the anterior portion, as compared to healthy comparison subjects.	cervical atlas;gyrus cinguli;inference;large deformation diffeomorphic metric mapping;map;schizophrenia;thickness (graph theory);thinning;algorithm;manifold	Anqi Qiu;Laurent Younes;Lei Wang;J. Tilak Ratnanather;Sarah K. Gillepsie;Gillian Kaplan;John G. Csernansky;Michael I. Miller	2007	NeuroImage	10.1016/j.neuroimage.2007.05.007	gaussian random field;computer vision;statistical inference;topology;coordinate system;schizophrenia;mathematics;geometry;statistics	ML	43.04489811764235	-79.4842454601555	140376
f0a2f81ee2a002801bddccf4a32df3570dbcd5d6	local error detection in sparse magnetic resonance imaging	compressed sensing;image coding;generalized gaussian distributions sparse representations compressed sensing magnetic resonance imaging steerable pyramids;wavelet transforms;accuracy abstracts indexes visualization;statistical analysis;medical image processing;wavelet transforms biomedical mri compressed sensing error detection gaussian distribution image coding medical image processing statistical analysis;error detection;local error detection sparse mri volumetric scan single nyquist sampled image acquisition boundary signatures joint statistical model multivariate generalized gaussian distributions wavelet coefficients recovered image regions sparse recovered images low acceleration factors sparse mri techniques signal encoding compressed sensing physiological constraints physical constraints sparse magnetic resonance imaging;gaussian distribution;biomedical mri	Due to the physical and the physiological constraints, the sparse magnetic resonance imaging (MRI) techniques operate in a regime where the theoretical guarantees of compressed sensing for recovery with high fidelity are improbable. Thus, the effective signal encoding in sparse MRI techniques is lossy, even at low acceleration factors. For widespread clinical use of sparse MRI, following two problems are proposed: 1) detection of errors in sparse recovered images and, 2) localized and highly fast acquisition of lossless image information in regions with high errors. This paper focuses on the former problem of detecting erroneously recovered image regions and proposes a solution based on joint statistics of wavelet coefficients across multiple subbands. The proposed technique uses multivariate generalized Gaussian distributions to jointly model the wavelet coefficients for all local regions conforming to a unique boundary signature in the image. Detection of local errors is formulated as measuring the degree of variation in the joint statistical model for boundary signatures between the recovered image and a training image. The training image can be a single Nyquist sampled image acquired prior to or during the sparse MRI volumetric scan. The preliminary experimental results show good conformance of the proposed method in detecting local error regions. The high error regions are detected with an accuracy of (91.8±1.6)% at (29.2±4.7)% false detection rate for acceleration factors up to 4.	antivirus software;coefficient;compressed sensing;conformance testing;distance (graph theory);error detection and correction;lossless compression;lossy compression;regular expression;resonance;sensor;sparse matrix;standard test image;statistical model;wavelet	Vimal Singh;Ahmed H. Tewfik	2013	2013 IEEE Global Conference on Signal and Information Processing	10.1109/GlobalSIP.2013.6737051	computer vision;feature detection;computer science;machine learning;pattern recognition;sparse approximation	Vision	50.48705011814704	-76.79998205269239	140389
c8f372455dae017aad42ac37d424c640adf36041	comparison of local and global region merging in the topological map	image segmentation;image processing;3d imaging;combinatorial maps;intervoxel boundaries;region merging	The topological map is a model that represents 2D and 3D images subdivision. It aims to allow the use of topological and geometrical features of the subdivision in image processing operations. When handling regions in an image, one of the main operation is the region merging, for example in segmentation process. This paper presents two algorithms of region merging in 3D topological maps: one local which modifies locally the map around merged regions, and another one global which runs through all the elements of the map. We study their complexities and present experimental results to compare both approaches.	algorithm;bottom-up parsing;combinatorial map;connected component (graph theory);experiment;image processing;interactivity;subdivision surface;top-down and bottom-up design	Alexandre Dupas;Guillaume Damiand	2008		10.1007/978-3-540-78275-9_37	generalized map;stereoscopy;computer vision;topology;image processing;computer science;quasi-open map;image segmentation	Vision	45.31700109203101	-68.9300425341338	140521
05ee9bf11ca1445944a9924c35a74d12f8fb183c	lv contour tracking in mri sequences based on the generalized fuzzy gvf	medical image processing maximum likelihood estimation optical tracking image sequences biomedical mri fuzzy systems image segmentation cardiology optimisation motion estimation gradient methods probability;optimisation;map cardiac left ventricle contour tracking mri sequences generalized fuzzy gradient vector flow image segmentation active contours model cardiac motion estimation motion tracking optical flow field motion equations maximum a posteriori probability;gradient vector flow;active contour;probability;image segmentation;cardiology;motion estimation;maximum likelihood estimation;left ventricle;motion tracking;computer vision;magnetic resonance imaging equations tracking computational intelligence society biomedical imaging robustness active contours motion estimation biomedical optical imaging optimization methods;optical tracking;medical image processing;gradient methods;optical flow;optimal algorithm;deformable model;fuzzy systems;active contour model;biomedical mri;image sequences	"""For the segmentation and robust tracking of the cardiac left ventricle (LV) in MRI sequences, an optimized algorithm is presented; it is based on the active contour framework. To use the active contours model (ACM) (Kass, M. et al., Int. J. Comput. Vision, vol.1, p.321-31, 1998) to estimate cardiac motion, a new concept of generalized fuzzy gradient vector flow (GFGVF) is presented and compared with the classical gradient vector flow (GVF) (Chenyang Xu and Prince, J.L., """"Gradient Vector Flow Deformable Models"""", Academic Press, 2000; Chung-Chu Leung and Wufan Chen, Proc. IEEE ICIP Conf., 2003). Then, a modified ACM is proposed for motion tracking, which is based on two new external forces: one is the GFGVF field; the other is the relativity of the optical flow field (OFF) on the predictive contour. For robust tracking of the outline of interest, a set of motion equations is presented to describe two correlative updating steps. Also, given some prior terms and likelihood one, the motion state of each point can be found by the maximum a posteriori probability (MAP)."""	active contour model;algorithm;entity–relationship model;gradient;logical volume management;numerical relativity;optical flow;prince	Wufan Chen;Shoujun Zhou;Bin Liang	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1418768	computer vision;mathematical optimization;computer science;pattern recognition;active contour model;mathematics;fuzzy control system	Vision	46.03806631604132	-77.12189791624098	140741
79b3fcefad295915ebe5260aa126b589656183e6	a finite element computational framework for active contours on graphs		In this paper we present a new framework for the solution of active contour models on graphs. With the use of the Finite Element Method we generalize active contour models on graphs and reduce the problem from a partial differential equation to the solution of a sparse non-linear system. Additionally, we extend the proposed framework to solve models where the curve evolution is locally constrained around its current location. Based on the previous extension, we propose a fast algorithm for the solution of a wide range active contour models. Last, we present a supervised extension of Geodesic Active Contours for image segmentation and provide experimental evidence for the effectiveness of our framework. Index Terms Active contours, finite element analysis, image segmentation, graph segmentation	active contour model;approximation algorithm;computation;computational complexity theory;delaunay triangulation;finite difference;finite element method;graph (discrete mathematics);image segmentation;linear system;nonlinear system;sparse matrix	Nikolaos Kolotouros;Petros Maragos	2017	CoRR		geodesic;machine learning;artificial intelligence;finite element method;partial differential equation;active contour model;mathematical optimization;graph;image segmentation;mathematics	Vision	53.209548163287124	-71.18980927031053	140866
64c7d4cf64fecec63a217e87ad2e3b5d0b20d443	stochastic vector quantization of images	ar model;cluster algorithm;image coding;image processing;modelo markov;stochastic method;image models;compresion senal;procesamiento imagen;segmentation;traitement image;compression signal;codificacion;markov model;cuantificacion vectorial;vector quantization;image compression;signal compression;coding;methode stochastique;vector quantizer;modele markov;image modeling;article;segmentacion;codage;quantification vectorielle;metodo estocastico	One of the most important steps in the vector quantization of images is the design of the codebook. The codebook is generally designed using the LBG algorithm, that is in essence a clustering algorithm which uses a large training set of empirical data that is statistically representative of the image to be quantized. The LBG algorithm, although quite effective for practical applications, is computationally very expensive and the resulting codebook has to be recalculated each time the type of image to be encoded changes. One alternative to the generation of the codebook, called stochastic vector quantization, is presented in this paper. Stochastic vector quantization (SVQ) is based on the generation of the codebook according to some previous model defined for the image to be encoded. The well-known AR model has been used to model the image in the current implementations of the technique, and has shown good performance in the overall scheme. To show the merit of the technique in different contexts, stochastic vector quantization is discussed and applied to both pixel-based and segmentation-based image coding schemes. @ 1997 Elsevier Science B.V.	ar (unix);autoregressive model;cluster analysis;codebook;encode;linde–buzo–gray algorithm;line code;location-based game;markov chain;markov model;neural coding;noise shaping;pixel;quantization (signal processing);test set;vector quantization;white noise;whole earth 'lectronic link	Luis Torres;Josep R. Casas;E. Arias	1997	Signal Processing	10.1016/S0165-1684(97)00130-8	computer vision;learning vector quantization;u-matrix;image processing;image compression;computer science;theoretical computer science;machine learning;mathematics;autoregressive model;coding;markov model;linde–buzo–gray algorithm;segmentation;vector quantization;statistics	Vision	52.416842813639136	-67.62932456488667	140937
6fad016e45a7622395e8f0d2fe8acbfc509ef3e1	segmenting trus video sequences using local shape statistics	transrectal ultrasound;image segmentation;ultrasound;automatic segmentation;magnetic resonance image;adaptive local shape statistics;biopsy;magnetic resonance imaging;adaptive learning;image guided intervention;ultrasonography;video;signal to noise ratio;deformable model;prostate;prostate cancer	Automatic segmentation of the prostate in transrectal ultrasound (TRUS) may improve the fusion of TRUS with magnetic resonance imaging (MRI) for TRUS/MRI-guided prostate biopsy and local therapy. It is very challenging to segment the prostate in TRUS images, especially for the base and apex of the prostate due to the large shape variation and low signal-to-noise ratio. To successfully segment the whole prostate from 2D TRUS video sequences, this paper presents a new model based algorithm using both global population-based and adaptive local shape statistics to guide segmentation. By adaptively learning shape statistics in a local neighborhood during the segmentation process, the algorithm can effectively capture the patient-specific shape statistics and the large shape variations in the base and apex areas. After incorporating the learned shape statistics into a deformable model, the proposed method can accurately segment the entire gland of the prostate with significantly improved performance in the base and apex. The proposed method segments TRUS video in a fully automatic fashion. In our experiments, 19 video sequences with 3064 frames in total grabbed from 19 different patients for prostate cancer biopsy were used for validation. It took about 200ms for segmenting one frame on a Core2 1.86 GHz PC. The average mean absolute distance (MAD) error was 1.65±0.47mm for the proposed method, compared to 2.50±0.81mm and 2.01±0.63mm for independent frame segmentation and frame segmentation result propagation, respectively. Furthermore, the proposed method reduced the MAD errors by 49.4% and 18.9% in the base and by 55.6% and 17.7% in the apex, respectively.	algorithm;apex (geometry);experiment;image segmentation;mad;medical ultrasound;resonance;signal-to-noise ratio;software propagation;statistical shape analysis	Pingkun Yan;Sheng Xu;Baris Turkbey;Jochen Kruecker	2010		10.1117/12.844324	computer vision;video;magnetic resonance imaging;ultrasound;image segmentation;signal-to-noise ratio;adaptive learning;medical physics	Vision	41.4436154041766	-79.24347637160206	141085
8c3d9c4905ef6251b101f22b40aed7b2e2d729f3	global segmentation and curvature analysis of volumetric data sets using trivariate b-spline functions	image tridimensionnelle;processus gauss;symbolic computation;courbure gauss;image segmentation;analisis datos;gaussian processes;data analysis spline image segmentation image analysis image reconstruction surface reconstruction object recognition image databases image recognition;analisis forma;three dimensional shape;segmentation;indexing terms;index terms gaussian and mean curvature;forma tridimensional;global computing;splines mathematics;analyse globale;calculo simbolico;data analysis;feature extraction image segmentation splines mathematics gaussian processes;forme tridimensionnelle;feature extraction;gaussian and mean curvature;spline function;differential scalar field global segmentation curvature analysis volumetric data sets trivariate b spline functions volumetric images convex iso surface concave iso surfaces cylindrical iso surfaces saddle like iso surfaces gaussian curvature;surface concave;scalar field;concave surface;tridimensional image;analyse donnee;b spline;pattern analysis;gaussian process;mean curvature;donnee volumique;proceso gauss;segmentation index terms gaussian and mean curvature symbolic computation global analysis;algorithms 1 0 0 0 0 0 0 0 artificial intelligence 1 0 0 0 0 0 0 0 databases factual 1 0 0 0 0 0 0 0 image enhancement 1 0 0 0 0 0 0 0 image interpretation computer assisted 1 0 0 0 0 0 0 0 imaging three dimensional 1 0 0 0 0 0 0 0 information storage and retrieval 1 0 0 0 0 0 0 0 numerical analysis computer assisted 1 0 0 0 0 0 0 0 pattern recognition automated 1 0 0 0 0 0 0 0;calcul symbolique;superficie concava;segmentacion;analyse forme;b splin;imagen tridimensional;global analysis	This paper presents a method to globally segment volumetric images into regions that contain convex or concave (elliptic) iso-surfaces, planar or cylindrical (parabolic) iso-surfaces, and volumetric regions with saddle-like (hyperbolic) iso-surfaces, regardless of the value of the iso-surface level. The proposed scheme relies on a novel approach to globally compute, bound, and analyze the Gaussian and mean curvatures of an entire volumetric data set, using a trivariate B-spline volumetric representation. This scheme derives a new differential scalar field for a given volumetric scalar field, which could easily be adapted to other differential properties. Moreover, this scheme can set the basis for more precise and accurate segmentation of data sets targeting the identification of primitive parts. Since the proposed scheme employs piecewise continuous functions, it is precise and insensitive to aliasing.	aliasing;b-spline;concave function;isoproterenol;isosurface;normal statistical distribution;parabolic antenna;biologic segmentation;cellular targeting	Octavian Soldea;Gershon Elber;Ehud Rivlin	2006	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2006.36	computer vision;symbolic computation;topology;computer science;gaussian process;mathematics;geometry;statistics	Visualization	47.56069886020962	-75.05071771656755	141200
cef4432cb86d3ab5c5a17a8378c43a4ca560df5e	optimization of landmark selection for cortical surface registration	gaussian processes;biomedical mri;error statistics;image registration;learning (artificial intelligence);medical image processing;minimisation;gaussian process;biomedical mri;cerebral cortical surface;cortical surface registration;error metric;image registration;landmark selection;manual labeling;optimal curve subset;optimization	Manually labeled landmark sets are often required as inputs for landmark-based image registration. Identifying an optimal subset of landmarks from a training dataset may be useful in reducing the labor intensive task of manual labeling. In this paper, we present a new problem and a method to solve it: given a set of N landmarks, find the k(<; N) best landmarks such that aligning these k landmarks that produce the best overall alignment of all N landmarks. The resulting procedure allows us to select a reduced number of landmarks to be labeled as a part of the registration procedure. We apply this methodology to the problem of registering cerebral cortical surfaces extracted from MRI data. We use manually traced sulcal curves as landmarks in performing inter-subject registration of these surfaces. To minimize the error metric, we analyze the correlation structure of the sulcal errors in the landmark points by modeling them as a multivariate Gaussian process. Selection of the optimal subset of sulcal curves is performed by computing the error variance for the subset of unconstrained landmarks conditioned on the constrained set. We show that the registration error predicted by our method closely matches the actual registration error. The method determines optimal curve subsets of any given size with minimal registration error.	computation (action);extraction;gaussian process;genetic selection;image registration;iterative closest point;landmark point;normal statistical distribution;sample variance;subgroup;tracer;newton;registration - actclass	Anand A. Joshi;David W. Shattuck;Dimitrios Pantazis;Quanzheng Li;Hanna Damasio;Richard M. Leahy	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206560	computer vision;text mining;image processing;computer science;machine learning;signal processing;data mining;gaussian process;statistics;neuroimaging	Vision	44.31975008891312	-76.95889396640221	141561
4e5330204c9eed6cf8679f411ec9bab19164d7b0	criterion for automatic selection of the most suitable maximum-likelihood thresholding algorithm for extracting object from their background in a still image	maximum likelihood	Three Maximum-Likelihood thresholding algorithms based on population mixture models are investigated and a criterion is introduced for automatic selection of the most suitable Maximum-Likelihood thresholding algorithm for extracting object from their background in an arbitrary still image. The most suitable algorithm is that whose population mixture model approximates better the probability density function of the intensity values. The probability density function is estimated from the histogram of the intensity values. The criterion was implemented and applied to real images with different illumination conditions. A subjective analysis of the experimental results showed that for each image,the proposed criterion was always able to select automatically from the three algorithms the one which delivers the best thresholding results.	algorithm;mixture model;thresholding (image processing)	Geovanni Martinez	2005			artificial intelligence;probability density function;mixture model;mathematics;pattern recognition;real image;maximum likelihood;population;algorithm;balanced histogram thresholding;thresholding;histogram	Vision	49.719731429898275	-68.60253679403543	141822
8710e4c72693167dcfa5a0f62495d8340803d420	liver segmentation based on skfcm and improved growcut for ct images	liver;image segmentation;computed tomography;active contours;shape;clustering algorithms;noise	Accurate liver segmentation is an essential and crucial step for computer-aided liver disease diagnosis and surgical planning. In this paper, a new coarse-to-fine method is proposed to segment liver for abdominal computed tomography (CT) images. This hierarchical framework consists of rough segmentation and refined segmentation. The rough segmentation is implemented based on a kernel fuzzy C-means algorithm with spatial information (SKFCM) algorithm and the refined segmentation is performed based on the proposed improved GrowCut (IGC) algorithm. The SKFCM algorithm introduces a kernel function and spatial constraint based on fuzzy c-means clustering (FCM) algorithm, which can reduce the effect of noise and improve the clustering ability. The IGC algorithm makes good use of the continuity of CT series in space which can automatically generate the seed labels and improve the efficiency of segmentation. The proposed method was applied to segment the liver for the whole dataset of abdominal CT images. The performance evaluation of segmentation results shows that the proposed liver segmentation method is accurate and efficient. Experimental results have been shown visually and achieve reasonable consistency.	ct scan;cluster analysis;fuzzy cognitive map;growcut algorithm;performance evaluation;rough set;scale-space segmentation;scott continuity;tomography	Hong Song;Qian Zhang;Shuliang Wang	2014	2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2014.6999179	computer vision;shape;computer science;noise;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;cluster analysis;computed tomography;scale-space segmentation	Robotics	40.71034666063963	-76.61406462315912	141882
3190815fe22bce379bffc1cac7f86e69ed8bc6db	cascaded regression for ct slice localization	pyramid representation;cavity resonators;estimation theory;heart;automated computational tools;computed tomography;ct slice localization;cascaded regression;arteries;image retrieval non contrast ct regression;non contrast ct;regression analysis computerised tomography estimation theory image representation image retrieval medical image processing;biomedical imaging;calcium;regression;intensity features;image representation;feature extraction;medical image processing;position estimation;computerised tomography;image analysis;regression analysis;slice retrieval;computed tomography arteries biomedical imaging feature extraction calcium heart cavity resonators;image retrieval;intensity features cascaded regression ct slice localization automated computational tools position estimation slice retrieval image analysis pyramid representation	Automated computational tools are needed to estimate the position of a slice of interest within a contiguous stack of slices. Such estimation is useful to retrieve relevant slices from a volume of slices in clinical analysis or it can be used as an initialization step to other post-processing and image analysis techniques. In this paper, we present a novel method to determine the location of a slice of interest within a given volume by formulating it as a regression problem. The input variables for the regression are obtained from simple intensity features computed from a pyramid representation of the slice. We assess the performance of the proposed method by comparing the estimated positions of slices of interest in CT data with manual annotations. Our method was validated on a dataset of 45 volumes and promising results were obtained for 5 different target slices, the average error being 2 slices.	ct scan;image analysis;pyramid (image processing);video post-processing	Olga C. Avila-Montes;Uday Kurkure;Ryo Nakazato;Daniel S. Berman;Damini Dey;Ioannis A. Kakadiaris	2011	2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2011.5872775	computer vision;image analysis;regression;radiology;medicine;calcium;feature extraction;image retrieval;computer science;machine learning;pattern recognition;estimation theory;heart;regression analysis;statistics	Vision	44.13621118943743	-77.15097734974228	141885
074e0c228975c0ebf8d13c8e5b3b77958270b333	segmentation of mr images of the brain based on statistical and spatial properties	brain;smoothing	Segmentation solely based on statistical approaches do not take into account spatial properties of the images. However, regions are not only characterised in statistical terms. Structural and/or spatial properties are also important and should be both considered. This paper presents a method which incorporates statistical and spatial image properties under a uni ed scheme for segmentation of MR images of the brain. It combines a pyramidal or quadtree smoothing operation with statistical segmentation performed at variable levels of the quad-tree, followed by a download boundary estimation. After the segmentation step (k{means clustering algorithm), all regions and their belonging pixels are computed and stored on a data structure suitable for the quad-tree smoothing and boundary estimation. This paper describes this technique in detail and shows some results obtained on both test and MR data.	algorithm;cluster analysis;computation;data structure;download;feature data;grayscale;image segmentation;pixel;quadtree;scale space;smoothing;software propagation	Joao E. Batista	2001		10.1117/12.430971	computer vision;computer science;machine learning;segmentation-based object categorization;data mining;scale-space segmentation	Vision	43.289564745072	-71.78390917879437	141960
9f9e2a0a199c184aaf80b71589d6bbf18edf2f2f	automated segmentation and retrieval system for ct head images	fuzzy c mean;unsupervised clustering;computed tomography;automatic segmentation;unsupervised segmentation;medical image;expectation maximization;brain imaging;k means clustering;cerebrospinal fluid;image retrieval	"""In this paper, automatic segmentation and retrieval of medical images are presented. For the segmentation, different unsupervised clustering techniques are employed to partition the Computed Tomography (CT) brain images into three regions, which are the abnormalities, cerebrospinal fluids (CSF) and brain matters. The novel segmentation method proposed is a dual level segmentation approach. The first level segmentation, which purpose is to acquire abnormal regions, uses the combination of fuzzy c-means (FCM) and k-means clustering. The second level segmentation performs either the expectation-maximization (EM) technique or the modified FCM with population-diameter independent (PDI )   to segment the remaining intracranial area into CSF and brain matters. The system automatically determines which algorithm to be utilized in order to produce optimum results. The retrieval of the medical images is based on keywords such as """"no abnormal region"""", """"abnormal region(s) adjacent to the skull"""" and """"abnormal region(s) not adjacent to the skull"""". Medical data from collaborating hospital are experimented and promising results are observed."""	ct scan;computed tomography of the head	Hau-Lee Tong;Mohammad Faizal Ahmad Fauzi;Ryoichi Komiya	2009		10.1007/978-3-642-05036-7_11	computer vision;geography;machine learning;segmentation-based object categorization;pattern recognition;region growing;image segmentation;scale-space segmentation	Vision	41.73231399909321	-73.36962064219456	142057
55d974a0cbd6ed8a09e5b03edb40913c7e282d0e	motion compensated color video classification using markov random fields	analisis imagen;theoretical framework;image segmentation;image processing;modelo markov;procesamiento imagen;simulated annealing;traitement image;markov random field;motion compensated;optimisation combinatoire;markov model;segmentation image;map estimation;bayesian estimator;image analysis;parameter estimation;modele markov;combinatorial optimization;imagen color;analyse image;human perception;image couleur;color image;optimizacion combinatoria	This paper deals with the classification of color video sequences using Markov Random Fields (MRF) taking into account motion information. The theoretical framework relies on Bayesian estimation associated with MRF modelization and combinatorial optimization (Simulated Annealing). In the MRF model, we use the CIE-luv color metric because it is close to human perception when computing color differences. In addition, intensity and chroma information is separated in this space. The sequence is regarded as a stack of frames and both intraand inter-frame cliques are defined in the label field. Without motion compensation, an inter-frame clique would contain the corresponding pixel in the previous and next frame. In the motion compensated model, we add a displacement field and it is taken into account in inter-frame interactions. The displacement field is also a MRF but there are no inter-frame cliques. The Maximum A Posteriori (MAP) estimate of the label and displacement field is obtained through Simulated Annealing. Parameter estimation is also considered in the paper and results are shown on color video sequences using both the simple and motion compensated models.	clique (graph theory);color;combinatorial optimization;computability in europe;displacement mapping;estimation theory;interaction;markov chain;markov random field;mathematical model;mathematical optimization;metropolis;metropolis–hastings algorithm;motion compensation;pixel;simulated annealing;unsupervised learning	Zoltan Kato;Ting-Chuen Pong;John Chung-Mong Lee	1998		10.1007/3-540-63930-6_189	computer vision;image analysis;color image;simulated annealing;image processing;combinatorial optimization;computer science;machine learning;pattern recognition;mathematics;image segmentation;markov model;estimation theory;perception	Vision	51.382338390774514	-69.01316225408615	142376
a56cb581b15c53ed17cebe48a6ce5110c2f481ac	kernel bandwidth estimation in methods based on probability density function modelling	bandwidth estimation;kernel bandwidth probability density function density functional theory quantum mechanics pattern recognition smoothing methods iterative algorithms synthetic aperture radar computer science;kernel;probability;probability density function;kernel function;mean shift;probability density functional theory;functional approximation;density functional theory;scale space;estimation;function approximation;quantum mechanics;kernel bandwidth estimation;pattern recognition kernel bandwidth estimation probability density function functional approximation second order statistics;kernel density estimate;mathematical model;pattern recognition;bayesian estimator;bandwidth;second order statistics	In kernel density estimation methods, an approximation of the data probability density function is achieved by locating a kernel function at each data location. The smoothness of the functional approximation and the modelling ability are controlled by the kernel bandwidth. In this paper we propose a Bayesian estimation method for finding the kernel bandwidth. The distribution corresponding to the bandwidth is estimated from distributions characterizing the second order statistics estimates calculated from local neighbourhoods. The proposed bandwidth estimation method is applied in three different kernel density estimation based approaches: scale space, mean shift and quantum clustering. The third method is a novel pattern recognition approach using the principles of quantum mechanics.	approximation;bandwidth (signal processing);cluster analysis;hybrid functional;k-nearest neighbors algorithm;kernel (operating system);kernel density estimation;mean shift;neighbourhood (graph theory);pattern recognition;radar;sampling (signal processing);scale space;the principles of quantum mechanics;topography	Adrian G. Bors;Nikolaos Nasios	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761215	kernel;kernel regression;kernel density estimation;kernel method;econometrics;mathematical optimization;estimation;probability density function;kernel;scale space;kernel embedding of distributions;density estimation;radial basis function kernel;mean-shift;function approximation;kernel principal component analysis;probability;mathematical model;multivariate kernel density estimation;mathematics;density functional theory;variable kernel density estimation;polynomial kernel;bandwidth;statistics;kernel smoother	Vision	52.14811471267681	-73.96795968273378	142379
42bd3e679b4dc31bc10cd63f5844f14cbd49aa8b	3d image interpolation based on anisotropic diffusion of feature point correspondence	interpolation;gradient vector flow;anisotropic diffusion;registration	An automatic method has been developed to interpolate between neighboring slices in a gray-scale data set by anisotropic diffusion of feature point correspondence. The feature point extracted is registered to form the feature vector. Thus a three dimensional (3D) weight anisotropic vector diffusion is introduced to spread the feature vector to the correspondence vector, which estimates spatial correspondence between adjacent slices. Bilinear interpolation is made along the direction of correspondence vector. Experiments are performed on medical data sets to evaluate the proposed method, showing that the new algorithm achieves good quality and improvement in efficiency relative to the traditional methods. VC 2013 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 23, 338–345, 2013; Published online in Wiley Online Library (wileyonlinelibrary.com). DOI: 10.1002/ima.22069	algorithm;anisotropic diffusion;bilinear transform;experiment;feature vector;grayscale;interpolation;john d. wiley	Qiang Sang;Jianzhou Zhang	2013	Int. J. Imaging Systems and Technology	10.1002/ima.22069	computer vision;mathematical optimization;interpolation;computer science;machine learning;mathematics;geometry;anisotropic diffusion	Vision	44.14254068312075	-74.02093645179981	142638
37ed47264e2a81b6b629c0442c56b183a22a4698	prior knowledge on cortex organization in the reconstruction of source current densities from eeg	source current density;source reconstruction;cortex parcellation;eeg	The reconstruction of the generators of electroencephalographic (EEG) signals is important for understanding brain processes. Since the inverse problem has no unique solution, additional knowledge or assumptions are needed. Often, results from other anatomical or functional measurement modalities are difficult to interpret directly in terms of EEG source strengths, but they provide valuable information about the functional similarity between brain regions, for example, in form of parcellations. We propose a novel approach to the incorporation of such parcellations as priors into the reconstruction of distributed source current densities from EEG. Two algorithms are described, based on a surface-constrained LORETA (Low Resolution Electromagnetic TomogrAphy) approach. The first, patchLORETA1, uses both topological neighborhood and prior information to define smoothness, while the second, patchLORETA2, neglects topological neighborhood. Computer simulations, using a smooth reconstruction surface on the brain envelope, reveal important aspects of the algorithms' performance, in particular the influences of noise and incongruence between measurements and prior information. It turns out that patchLORETA1 makes efficient use of the provided prior information and at the same time is quite robust towards faulty priors as well as noise. The algorithms are also tested on the localization of the sources of event-related potentials. Here, both the smooth brain and folded cortical surfaces serve as reconstruction spaces. We find that patchLORETA1 becomes ineffective on the folded cortex, while patchLORETA2 yields plausible results. We also discuss the extension of the proposed algorithms to other types of priors and propose ways to overcome shortcomings of the current implementation.	algorithm;computer simulation;electroencephalography;expectation–maximization algorithm;iterative method;linear programming;magnetoencephalography;mathematical optimization;nonlinear programming;nonlinear system;postsynaptic potentials;pyramidal cells;schmidt decomposition;simulation;word lists by frequency;density;funding grant	Thomas R. Knösche;Markus Gräser;Alfred Anwander	2013	NeuroImage	10.1016/j.neuroimage.2012.11.013	psychology;computer vision;neuroscience;radiology;electroencephalography;artificial intelligence;machine learning;mathematics	ML	48.781961834677766	-78.19597087552793	142747
53de82e928ed43313e779052ee9b01bf4dfeb3ad	local filtering and global optimization methods for 3-d magnetic-resonance angiography image enhancement	shape constraints;brain;magnetism;volume rendering;blood vessel;angiography;visualization;image enhancement;stochastic optimization;blood;brain structure;magnetic resonance angiography;global optimization;mr angiography;blood vessels	In this presentation, we discuss the visualization of cerebral blood vessels in 3-D MR angiography images. Two techniques for an improved visualization are investigated: 3-D non- linear morphological filters that enhance the contrast of blood-vessel-like structures and a global stochastic optimization framework incorporating shape constraints. The resulting filtered images are combined into a novel hybrid volume rendering visualization method for the integrated viewing of brain structures and cerebral vasculature.© (1992) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	global optimization;image editing;mathematical optimization;resonance	Dirk Vandermeulen;D. Delaere;Paul Suetens;Hilde Bosmans;Guy Marchal	1992		10.1117/12.131084	computer vision;radiology;computer science;medical physics	Vision	43.734673326355775	-74.64490781025997	142773
5ac1bbc582e591e2560d1a3167f30fd5a9073b25	lip image segmentation using fuzzy clustering incorporating an elliptic shape function	fuzzy e means algorithm lip image segmentation fuzzy clustering speech recognition speaker authentication lip image analysis color information spatial distance elliptic shape function color dissimilarity nonlip region membership distribution;analisis imagen;fuzzy c means algorithm;pattern clustering;analyse amas;image segmentation;image processing;dissimilarity measure;color;lip;logique floue;procesamiento imagen;iterative methods image segmentation fuzzy set theory pattern clustering speaker recognition image colour analysis;logica difusa;elliptic function;iterative algorithm;traitement image;fuzzy set theory;speaker recognition;fuzzy logic;iterative methods;fuzzy clustering;forme elliptique;cluster analysis;levre;image colour analysis;clustering method;algorithms cluster analysis fuzzy logic humans image enhancement image interpretation computer assisted information storage and retrieval lip lipreading pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique video recording;lip segmentation;segmentation image;image segmentation shape measurement clustering methods clustering algorithms colored noise speech recognition iterative algorithms image color analysis noise robustness image sequences;speech recognition;labio;elliptic shape;image analysis;analisis cluster;funcion eliptica;analyse image;forma eliptica;fonction elliptique	Recently, lip image analysis has received much attention because its visual information is shown to provide improvement for speech recognition and speaker authentication. Lip image segmentation plays an important role in lip image analysis. In this paper, a new fuzzy clustering method for lip image segmentation is presented. This clustering method takes both the color information and the spatial distance into account while most of the current clustering methods only deal with the former. In this method, a new dissimilarity measure, which integrates the color dissimilarity and the spatial distance in terms of an elliptic shape function, is introduced. Because of the presence of the elliptic shape function, the new measure is able to differentiate the pixels having similar color information but located in different regions. A new iterative algorithm for the determination of the membership and centroid for each class is derived, which is shown to provide good differentiation between the lip region and the nonlip region. Experimental results show that the new algorithm yields better membership distribution and lip shape than the standard fuzzy c-means algorithm and four other methods investigated in the paper.	algorithm;appendix;authentication;cluster analysis;color space;computational complexity theory;fosfomycin;fuzzy clustering;fuzzy cognitive map;image analysis;image segmentation;iterative method;myoclonus, familial cortical;pixel;rendering (computer graphics);speech recognition;time complexity;uruguay faciocardiomusculoskeletal syndrome;biologic segmentation;statistical cluster	Shu Hung Leung;Shi-Lin Wang;Wing Hong Lau	2004	IEEE Transactions on Image Processing	10.1109/TIP.2003.818116	speaker recognition;computer vision;image analysis;speech recognition;image processing;computer science;machine learning;pattern recognition;mathematics;iterative method	Vision	43.73370571909101	-66.86120269439049	142960
9727ac663a621488d4d14ec6250512779eb8d983	visualization enhancement of arthrosis tissues structure in ultrasound image based on improved diffusion	image enhencement image visualization ultrasound image;nonlinear diffusion;reverberation;reverberation noise removal;speckle;image visualization;tensile stress;computed tomography;clinical application;ultrasound;biodiffusion;ultrasonic imaging;structure identification;arthrosis diagnosis;speckle smoothing;arthrosis operation planning visualization enhancement arthrosis tissues structure ultrasound image diffusion structure visualization structure identification reverberation noise removal speckle smoothing arthrosis diagnosis;structure visualization;ultrasound imaging;image enhencement;ultrasound image;visualization;image enhancement;smoothing methods;bones;visualization ultrasonic imaging reverberation smoothing methods tensile stress speckle image edge detection bones image enhancement computed tomography;arthrosis tissues structure;automatic detection;image edge detection;medical image processing;visualization enhancement;bone;robust method;arthrosis operation planning;image denoising;diffusion;smoothing methods biodiffusion biomedical ultrasonics bone image denoising image enhancement medical image processing;biomedical ultrasonics	Structure visualization and identification for ultrasound (US) arthral structure images is essential in US-guided arthrosis operation and diagnosis for arthrosis disorder. US imaging can image arthrosis tissues and cartilaginous, but it often depicts the structure poorly, because of speckle, reverberation, shadowing feature and other artifacts. The enhancement of arthrosis structure visualization remains great challenge in US images. This paper proposes an improved diffusion to remove reverberation noise, smoothing speckles and improve the visualization of arthrosis structure in US images. The structure tensor from automatic detection is applied to keep edges structure. The potential of the method is demonstrated with the results compared to nonlinear diffusion. The preliminary results show that improved diffusion is a robust method to enhance surfaces visualization of US arthrosis structure. The method shows promise in a clinical application of arthrosis operation planning and arthrosis diagnosis.	anisotropic diffusion;convergence insufficiency;experiment;image editing;noise reduction;nonlinear system;sensor;smoothing;structure tensor	Wei Wang;Yang Liu;Yingxia Shen;Baowei Liu;Yanjun Shi;Ping Gao;Yuerong Wang	2008	2008 International Conference on BioMedical Engineering and Informatics	10.1109/BMEI.2008.147	speckle pattern;computer vision;visualization;radiology;reverberation;ultrasound;diffusion;stress;computed tomography	Visualization	39.724889486567896	-77.86115520902914	142983
937b8d5e8a9e4945216786e833eec99d54573678	fast linear elastic matching without landmarks	linear elasticity	Non-rigid matchers generally constrain estimated displacement fields in uniform intensity regions of medical data. Elastic models are popular regularizers since they are easy to understand and simulate, and their smoothness properties may be as likely as other constraints.	elastic matching	Samson J. Timoner;W. Eric L. Grimson;Ron Kikinis;William M. Wells	2001		10.1007/3-540-45468-3_228	computer science;mathematics;linear elasticity	EDA	48.17536705862052	-76.36854796773066	143116
10e54f450b7b6edf3801f8729a595e70f122b35e	efficient topology-controlled sampling of implicit shapes	image sampling;topology;image segmentation;convergence of numerical methods;segmentation markov chain monte carlo mcmc metropolis hastings level sets;iterative methods;mcmc method topology controlled sampling implicit shapes energy functionals image segmentation metropolis hastings method magnitude speed convergence topological constraints markov chain monte carlo method;topology shape proposals image segmentation histograms markov processes level set;topology convergence of numerical methods image sampling image segmentation iterative methods markov processes monte carlo methods;markov processes;monte carlo methods	Sampling from distributions of implicitly defined shapes enables analysis of various energy functionals used for image segmentation. Recent work [1] describes a computationally efficient Metropolis- Hastings method for accomplishing this task. Here, we extend that framework so that samples are accepted at every iteration of the sampler, achieving an order of magnitude speed up in convergence. Additionally, we show how to incorporate topological constraints.	computational complexity theory;image segmentation;iteration;sampling (signal processing);speedup	Jason Chang;John W. Fisher	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6466904	econometrics;mathematical optimization;multiple-try metropolis;hybrid monte carlo;markov chain monte carlo;slice sampling;mathematics;iterative method;rejection sampling;image segmentation;markov process;statistics;monte carlo method	Robotics	51.32619847867426	-70.82435708206656	143138
c8437800f0d82a9f033821707c3a95f76bfa272b	a vector momenta formulation of diffeomorphisms for improved geodesic regression and atlas construction	health research;numerical stability;uk clinical guidelines;estimation vectors shape optimization magnetic resonance imaging equations;biological patents;vector momentum;optimisation;brain;lddmm;europe pubmed central;differential geometry;citation search;variational techniques;geodesic regression;uk phd theses thesis;image representation;medical image processing;vector momentum lddmm geodesic regression atlas;life sciences;atlas;regression analysis;uk research reports;medical journals;europe pmc;variational techniques biomedical mri brain differential geometry image representation medical image processing numerical stability optimisation regression analysis;biomedical research;biomedical mri;bioinformatics;vector momenta formulation 3d mri brain scan optimization problem scalar momenta case theoretical optimal solution atlas estimation problem template estimation closed form update variational problem standard scalar momentum diffeomorphism initial condition vector momenta representation numerical stability convergence diffeomorphic image regression atlas construction geodesic regression	This paper presents a novel approach for diffeomorphic image regression and atlas estimation that results in improved convergence and numerical stability. We use a vector momenta representation of a diffeomorphism's initial conditions instead of the standard scalar momentum that is typically used. The corresponding variational problem results in a closed-form update for template estimation in both the geodesic regression and atlas estimation problems. While we show that the theoretical optimal solution is equivalent to the scalar momenta case, the simplification of the optimization problem leads to more stable and efficient estimation in practice. We demonstrate the effectiveness of our method for atlas estimation and geodesic regression using synthetically generated shapes and 3D MRI brain scans.	atlases;calculus of variations;clinical use template;convergence (action);initial condition;level of detail;mathematical optimization;numerical stability;optimization problem	Nikhil Singh;Jacob Hinkle;Sarang C. Joshi;P. Thomas Fletcher	2013	2013 IEEE 10th International Symposium on Biomedical Imaging	10.1109/ISBI.2013.6556700	differential geometry;mathematical optimization;combinatorics;atlas;mathematics;geometry;numerical stability;regression analysis;statistics	Vision	47.72127715431199	-77.56432651375647	143356
e571a02b674ba36fe08be43e39d0cfecba002f1a	automatic extraction of ellipsoidal features for planetary image registration	mars;remote sensing image;geophysical image processing;image segmentation;generalized hough transform;edge detection;water resources;meteorite craters;crater extraction method planetary image registration automatic ellipsoidal feature extraction method planetary mission image data analysis automatic processing technique earth remote sensing image uneven illumination characteristics compact rock shape image processing technique watershed segmentation generalized hough transform;water resources feature extraction geophysical image processing geophysical techniques hough transforms image registration image segmentation meteorite craters remote sensing rocks;shape;image edge detection;feature extraction;remote sensing;image registration;watershed segmentation;feature extraction rocks image segmentation shape mars image registration image edge detection;hough transforms;image processing techniques;hough transform;watershed segmentation crater detection feature extraction hough transform;rocks;crater detection;geophysical techniques	With the launch of several planetary missions in the last decade, a large amount of planetary images has been already acquired and much more will be available for analysis in the coming years. The image data need to be analyzed, preferably by automatic processing techniques because of the huge amount of data. Although many automatic feature extraction methods have been proposed and utilized for earth remote sensing images, these methods are not always applicable to planetary data that often present low contrast and uneven illumination characteristics. Here, we propose a new unsupervised method for the extraction of different features of elliptical and geometrically compact shapes, such as craters and rocks of compact shape (e.g., boulders), to be used for image registration purposes. This approach is based on the combination of several image processing techniques, including watershed segmentation and the generalized Hough transform. The method potentially has application for extraction of craters, rocks, and other geological features.	feature extraction;generalised hough transform;image processing;image registration;planetary scanner;unsupervised learning;watershed (image processing)	Giulia Troglio;Jacqueline Le Moigne;Jon Atli Benediktsson;Sebastiano B. Serpico	2012	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2011.2161263	hough transform;computer vision;water resources;mars exploration program;edge detection;watershed;feature extraction;shape;computer science;image registration;image segmentation;remote sensing;computer graphics (images)	Vision	41.214768685591785	-69.2574087384051	143371
a0860f91989e785f50826cdaec3e0307c60f5295	automatic grey level thresholding through index of fuzziness and entropy	image processing;fuzzy sets;pattern recognition	Algorithms for automatic thresholding of grey levels (without reference to histogram) are described using the terms 'index of fuzziness' and 'entropy' of a fuzzy sel. Their values are seen to be minimum when the crossover point of an S-function corresponds to boundary levels among different regions in image space. The effectiveness of the algorithms is demonstrated for images having both bimodal and multimodal grey level distributions.	algorithm;grayscale;multimodal interaction;thresholding (image processing)	Sankar K. Pal;Robert A. King;Abdulla Hashim	1983	Pattern Recognition Letters	10.1016/0167-8655(83)90053-3	computer vision;image processing;computer science;machine learning;pattern recognition;balanced histogram thresholding;mathematics;fuzzy set	Vision	42.229596076821316	-67.55410474652629	143544
7341fc31cb7d9c8fb8a4c247d21c3b26005e9109	a new supervised evaluation criterion for region based segmentation methods	image processing;natural images;region segmentation;subjective evaluation	We present in this article a new supervised evaluation criterion that enables the quantification of the quality of region segmentation algorithms. This criterion is compared with seven well-known criteria available in this context. To that end, we test the different methods on natural images by using a subjective evaluation involving different experts from the French community in image processing. Experimental results show the benefit of this new criterion.		Adel Hafiane;Sébastien Chabrier;Christophe Rosenberger;Hélène Laurent	2007		10.1007/978-3-540-74607-2_40	computer vision;image processing;computer science;machine learning;segmentation-based object categorization;pattern recognition;scale-space segmentation	Vision	40.04723606743146	-68.92506242266622	143721
7a40c6e272b8f6fc92f29600d8de7ea67fd1ccb2	cost aggregation strategy for stereo matching based on a generalized bilateral filter model		Stereo matching is a kernel problem in the field of stereo vision. An adaptive cost aggregation strategy based on a generalized bilateral filter model is proposed. The strategy extends range weights of the original bilateral filter by the inner and outer weighted average processes. A pixel is assigned a high range weight to the central pixel not only if the patches of the two pixels are similar but also if the neighboring patches around the two pixels are similar. The final range weights could more accurately reflect the similarities of relevant two pixels. Different cost aggregation methods can be derived from the model by modifying parameters. Experimental results compared with the other state-of-the-art cost aggregation methods demonstrate the effectiveness of our proposed cost aggregation strategy. © Springer-Verlag 2010.	bilateral filter;computer stereo vision	Li Li;Caiming Zhang;Hua Yan	2010		10.1007/978-3-642-16336-4_26	computer vision;mathematical optimization;simulation	Vision	46.879100941593805	-68.76437624501301	143840
a63ffe491838acb9c6cdaaf400d99eaef5ae3dd5	nonrigid image registration using conditional mutual information	intensity dimension;histograms;tensor product b spline nonrigid registration method;spline;optimisation;spatial dimension;biomedical measurements;image registration mutual information biomedical imaging biomedical measurements histograms computed tomography robustness spline probability speech processing;image motion analysis;probability;rigid body;image processing;computed tomography;image matching;speech processing;spline functions;conditional mutual information;voxel based similarity measure;biomedical imaging;algorithms image processing computer assisted information theory magnetic resonance imaging reproducibility of results tomography x ray computed;psi_mic;histogram construction;generalized partial volume kernel;splines mathematics;medical image registration;phantom;tensor product;parzen window;medical image;partial volume;spatial distribution;computational complexity;voxel based similarity measure b splines biomedical image processing conditional mutual information free form deformation image matching image motion analysis image processing image registration mutual information nonrigid registration spline functions;medical image processing;3d joint histogram nonrigid image registration maximization of mutual information similarity measure medical image registration conditional mutual information spatial dimension intensity dimension tensor product b spline nonrigid registration method histogram construction parzen window generalized partial volume kernel phantom;image registration;spline function;nonrigid image registration;biomedical image processing;mutual information;free form deformation;robustness;b splines;nonrigid registration;similarity measure;maximization of mutual information;3d joint histogram;splines mathematics computational complexity image registration medical image processing optimisation	Maximization of mutual information (MMI) is a popular similarity measure for medical image registration. Although its accuracy and robustness has been demonstrated for rigid body image registration, extending MMI to nonrigid image registration is not trivial and an active field of research. We propose conditional mutual information (cMI) as a new similarity measure for nonrigid image registration. cMI starts from a 3-D joint histogram incorporating, besides the intensity dimensions, also a spatial dimension expressing the location of the joint intensity pair. cMI is calculated as the expected value of the cMI between the image intensities given the spatial distribution. The cMI measure was incorporated in a tensor-product B-spline nonrigid registration method, using either a Parzen window or generalized partial volume kernel for histogram construction. cMI was compared to the classical global mutual information (gMI) approach in theoretical, phantom, and clinical settings. We show that cMI significantly outperforms gMI for all applications.	alignment;b-spline;bin;body image;choose (action);conditional mutual information;contain (action);dimensions;expectation–maximization algorithm;graph - visual representation;histogram;image registration;kernel density estimation;line level;local optimum;medical image;muscle rigidity;peripheral;phantom reference;phantoms, imaging;similarity measure;situated;technetium;window function;registration - actclass	Dirk Loeckx;Pieter Slagmolen;Frederik Maes;Dirk Vandermeulen;Paul Suetens	2010	IEEE Transactions on Medical Imaging	10.1109/TMI.2009.2021843	spline;computer vision;mathematical optimization;radiology;image processing;computer science;pattern recognition;speech processing;mathematics;statistics	Vision	44.92066600090463	-78.1995112491186	143919
55fa415bcf8c85f9f64ed346166aabb30301dae8	model driven quantification of left ventricular function from sparse single-beat 3d echocardiography	heart;left ventricular function;image segmentation;ultrasound;point to point;transducers;correspondence problem;prior knowledge;segmentation;left ventricle;3d ultrasound;3d model;left ventricle quantification;echocardiography;cross section;model updating;ultrasonography;estimation error;cardiovascular;active shape model;leave one out cross validation	This paper presents a novel model based segmentation technique for quantification of Left Ventricular (LV) function from sparse single-beat 3D echocardiographic data acquired with a Fast Rotating Ultrasound (FRU) transducer. This transducer captures cardiac anatomy in a sparse set of radially sampled, curved cross sections within a single cardiac cycle. The method employs a 3D Active Shape Model of the Left Ventricle (LV) in combination with local appearance models as prior knowledge to steer the segmentation. A set of local appearance patches generate the model update points for fitting the model to the LV in the curved FRU cross-sections. Updates are then propagated over the dense 3D model mesh to overcome correspondence problems due to the data sparsity, whereas the 3D Active Shape Model serves to retain the plausibility of the generated shape. Leave-one-out cross validation was carried out on single-beat FRU data from 28 patients suffering from various cardiac pathologies. Error measurements against expert-annotated contours yielded an average point-to-point distance of around 3.8 ± 2.4 mm and point-to-surface distance of 2.0 ± 1.8 mm and average volume estimation error of around 9 ± 7%. Robustness tests with respect to different model initializations showed acceptable performance for initial positions within a range of 22 mm for displacement and 12° for orientation. This demonstrates that the method combines robustness with respect to initialization with an acceptable accuracy, while using sparse single-beat FRU data.	active shape model;cross-validation (statistics);displacement mapping;experiment;logical volume management;peer-to-peer;plausibility structure;point-to-point protocol;sparse language;sparse matrix;transducer	Meng Ma;Marijn van Stralen;Johan H. C. Reiber;Johannes G Bosch;Boudewijn P. F. Lelieveldt	2009		10.1117/12.812490	active shape model;computer vision;transducer;point-to-point;ultrasonography;circulatory system;ultrasound;cross section;image segmentation;correspondence problem;segmentation;heart;cross-validation	Vision	42.237671897476545	-79.92891705532337	143935
e0b73636a59fba481b2c108f06becf08beca272d	streaming spatio-temporal video segmentation using gaussian mixture model	video segmentation;gaussian mixture model;clustering	Development of an automatic streaming video segmentation method is crucial for many video analysis applications. However, consistency of temporal segmentation and scalability for real-time applications are difficult to achieve. This work proposes a linear-time video segmentation method which is scalable and temporally consistent for streaming videos. A Gaussian Mixture Model (GMM) is used to segment each frame while a recursive filtering updates the parameters of the GMM. This hybrid methodology can uniquely propagate Gaussian clusters through each new frame, update the variance recursively, and create or remove clusters as necessary. In this way, the model automatically manipulates the number of clusters in run-time and adapts to any video sequence over streaming frames maintaining temporal coherence. The method needs a distance threshold value as the main parameter. The creation and removal of new clusters are governed by a cluster similarity criterion that can be based on user-defined distance measure. The experimental results are presented with two possible distance measures. The performance of the proposed method on several datasets is found to be comparable to state-of-the-art video segmentation algorithms.	coherence (physics);collision detection;computer cluster;google map maker;mixture model;real-time clock;recursion;scalability;streaming algorithm;streaming media;time complexity;video content analysis	Dibyendu Mukherjee;Q. M. Jonathan Wu	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025890	computer vision;computer science;machine learning;video tracking;pattern recognition;mixture model;image segmentation;cluster analysis;scale-space segmentation	Vision	51.45031279452876	-70.03588165192066	143941
16e9ae43acf863b1a0fdb64d7692cd0eda68012b	generalized image models and their application as statistical models of images	statistical shape model;kalman filter;statistical model;statistical shape models;registration;iterative closest point;generalized image models;image modeling;icp	A generalized image model (GIM) is presented. Images are represented as sets of four-dimensional (4D) sites combining position and intensity information, as well as their associated uncertainty and joint variation. This model seamlessly allows for the representation of both images and statistical models (such as those used for classification of normal/abnormal anatomy and for inter-patient registration), as well as other representations such as landmarks or meshes. A GIM-based registration method aimed at the construction and application of statistical models of images is proposed. A procedure based on the iterative closest point (ICP) algorithm is modified to deal with features other than position and to integrate statistical information. Furthermore, we modify the ICP framework by using a Kalman filter to efficiently compute the transformation. The initialization and update of the statistical model are also described. Preliminary results show the feasibility of the approach and its potentialities.	algorithm;anatomic structures;iterative closest point;iterative method;kalman filter;landmark point;statistical model;registration - actclass	Miguel Ángel González Ballester;Xavier Pennec;Marius George Linguraru;Nicholas Ayache	2004	Medical image analysis	10.1016/j.media.2004.06.012	active shape model;kalman filter;statistical model;computer vision;computer science;machine learning;mathematics;iterative closest point;statistics	Vision	46.36384196917011	-77.3233968166652	144210
95e19ec90e85662140c0f1d52d3beb45e9083720	outlining of the prostate using snakes with shape restrictions based on the wavelet transform	transformation ondelette;radioterapia;radiotherapy;prostata;cancerology;medical imagery;image processing;tumor maligno;urinary system disease;methode echelle multiple;image understanding;multiscale analysis;procesamiento imagen;hombre;metodo escala multiple;segmentation;radiotherapie;calculo diadico;traitement image;automatic recognition;wavelet transform;medical image;calcul dyadique;cancer diagnosis;cancerologie;dyadic calculus;human;imagerie medicale;pattern recognition;volume measurement;multiscale method;tumeur maligne;imageneria medical;cancerologia;transformacion ondita;reconnaissance forme;reconocimiento patron;medical image segmentation;template matching;segmentacion;prostate;wavelet transformation;object detection;reconocimiento automatico;malignant tumor;reconnaissance automatique;homme;appareil urinaire pathologie;aparato urinario patologia	This paper considers the problem of deformable contour initialization and modeling for segmentation of the human prostate in medical images. We propose a new technique for elastic deformation restriction to particular object shapes of any closed planar curve using localized multiscale contour parameterization based on the 1D dyadic wavelet transform. For this purpose we define internal curve deformation forces as a result of multiscale parametrical contour analysis. The form restricted contour deformation and its initialization by template matching are performed in a coarse to fine segmentation process based on a multiscale image edge representation containing the important edges of the image at various scales. The method is useful for 3D conformal radiotherapy planning and automatic prostate volume measurements in ultrasonographic diagnosis.	wavelet transform	Christian Juan Knoll;Mariano Alcañiz Raya;Vicente Grau;Carlos Monserrat Aranda;Maria del Carmen Juan Lizandra	1999	Pattern Recognition	10.1016/S0031-3203(98)00177-0	computer vision;radiation therapy;template matching;image processing;computer science;artificial intelligence;mathematics;segmentation;wavelet transform	Vision	45.84975977240608	-79.08740391980841	144249
890648caba787abb2d8e48e73663c6645385e7a2	salient feature region: a new method for retinal image registration	diagnostic techniques ophthalmological humans image processing computer assisted retina retinal vessels;eye;transformation model;diagnostic techniques ophthalmological;cross correlation;salient feature region;local adaptive variance;retinal image registration;indexing terms;image processing computer assisted;retina deterioration;local adaptation;cross correlation salient feature region retinal image registration eye disease retina deterioration local adaptive variance gradient field entropy;feature extraction retina image registration entropy pathology pixel robustness;salient feature region sfr;local features;retina;feature extraction;medical image processing;image registration;pixel;medical image processing diseases eye feature extraction image registration;eye disease;diseases;robustness;gradient field entropy;humans;entropy;retinal vessels;retinal imaging;pathology;normalized cross correlation;salient feature region sfr retinal image registration	Retinal image registration is crucial for the diagnoses and treatments of various eye diseases. A great number of methods have been developed to solve this problem; however, fast and accurate registration of low-quality retinal images is still a challenging problem since the low content contrast, large intensity variance as well as deterioration of unhealthy retina caused by various pathologies. This paper provides a new retinal image registration method based on salient feature region (SFR). We first propose a well-defined region saliency measure that consists of both local adaptive variance and gradient field entropy to extract the SFRs in each image. Next, an innovative local feature descriptor that combines gradient field distribution with corresponding geometric information is then computed to match the SFRs accurately. After that, normalized cross-correlation-based local rigid registration is performed on those matched SFRs to refine the accuracy of local alignment. Finally, the two images are registered by adopting high-order global transformation model with locally well-aligned region centers as control points. Experimental results show that our method is quite effective for retinal image registration.	alignment;cluster analysis;cross-correlation;disorder of eye;experiment;folic acid;gradient;image analysis;image registration;image scaling;kl-one;kullback–leibler divergence;matching;medical image;medical imaging;multimodal interaction;muscle rigidity;overlap–add method;pineal region yolk sac tumor;retina;retinal perforations;sample variance;smith–waterman algorithm;test scaling;visual descriptor;whole earth 'lectronic link;registration - actclass;retinal image;statistical cluster	Jian Zheng;Jie Tian;Kexin Deng;Xiaoqian Dai;Xing Zhang;Min Xu	2011	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2010.2091145	computer vision;computer science;cross-correlation;statistics;computer graphics (images)	Vision	40.454709148985536	-75.92397963413491	144472
3dc4504e6c327fe228db0de32f77e4175a440508	class-specific, top-down segmentation	bottom up;top down;grouping and segmentation;top down processing;figure ground;object classification	In this paper we present a novel class-based segmentation method, which is guided by a stored representation of the shape of objects within a general class (such as horse images). The approach is different from bottom-up segmentation methods that primarily use the continuity of grey-level, texture, and bounding contours. We show that the method leads to markedly improved segmentation results and can deal with significant variation in shape and varying backgrounds. We discuss the relative merits of class-specific and general image-based segmentation methods and suggest how they can be usefully combined.	bottom-up parsing;bounding volume hierarchy;scott continuity;texture mapping	Eran Borenstein;Shimon Ullman	2002		10.1007/3-540-47967-8_8	computer vision;computer science;segmentation-based object categorization;pattern recognition;top-down and bottom-up design;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Vision	47.23954401085996	-68.13218998573092	144609
8e63f8c07536aa77f4ac67b3c1a251e83dd52d23	hybrid segmentation framework for 3d medical image analysis	brain;heart;image segmentation;colon;lung;medical image analysis;medical image;region of interest;magnetic resonance imaging;medical imaging;relative efficiency;ultra sound;ground truth;medical image segmentation;deformable model;x rays	Medical image segmentation is the process that defines the region of interest in the image volume. Classical segmentation methods such as region-based methods and boundary-based methods cannot make full use of the information provided by the image. In this paper we proposed a general hybrid framework for 3D medical image segmentation purposes. In our approach we combine the Gibbs Prior model, and the deformable model. First, Gibbs Prior models are applied onto each slice in a 3D medical image volume and the segmentation results are combined to a 3D binary masks of the object. Then we create a deformable mesh based on this 3D binary mask. The deformable model will be lead to the edge features in the volume with the help of image derived external forces. The deformable model segmentation result can be used to update the parameters for Gibbs Prior models. These methods will then work recursively to reach a global segmentation solution. The hybrid segmentation framework has been applied to images with the objective of lung, heart, colon, jaw, tumor, and brain. The experimental data includes MRI (T1, T2, PD), CT, X-ray, Ultra-Sound images. High quality results are achieved with relatively efficient time cost. We also did validation work using expert manual segmentation as the ground truth. The result shows that the hybrid segmentation may have further clinical use.© (2003) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	image analysis;medical image computing	Ting Chen;Dimitris N. Metaxas	2003		10.1117/12.480116	image texture;computer vision;simulation;geography;segmentation-based object categorization;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;computer graphics (images)	Vision	40.609635877705855	-79.91947677038448	144768
0899ae5fb2763b57742a1d13c495604d8ddffbea	probabilistic framework for the characterization of surfaces and edges in range images, with application to edge detection		We develop a powerful probabilistic framework for the local characterization of surfaces and edges in range images. We use the geometrical nature of the data to derive an analytic expression for the joint probability density function (pdf) for the random variables used to model the ranges of a set of pixels in a local neighborhood of an image. We decompose this joint pdf by considering independently the cases where two real world points corresponding to two neighboring pixels are locally on the same real world surface or not. In particular, we show that this joint pdf is linked to the Voigt pdf and not to the Gaussian pdf as it is assumed in some applications. We apply our framework to edge detection and develop a locally adaptive algorithm that is based on a probabilistic decision rule. We show in an objective evaluation that this new edge detector performs better than prior art edge detectors. This proves the benefits of the probabilistic characterization of the local neighborhood as a tool to improve applications that involve range images.	adaptive algorithm;assumed;detectors;edge detection;normal statistical distribution;pixel;portable document format;benefit	Antoine Lejeune;Jacques G. Verly;Marc Van Droogenbroeck	2018	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2746618	artificial intelligence;canny edge detector;probability density function;decision rule;pattern recognition;computer vision;adaptive algorithm;joint probability distribution;probabilistic logic;random variable;edge detection;mathematics	Vision	50.09995030410351	-70.06141816150384	145018
21a9ddbb719f884c3e423d2115fe9822538abe92	morphological appearance manifolds in computational anatomy: groupwise registration and morphological analysis	brain;deformation based morphometry;shape descriptor;diagnostic accuracy;image processing computer assisted;voxel based morphometry;morphological analysis;morphological variation;global optimization;humans;computational biology;anatomy	We present an extension of the conventional computational anatomy framework to account for confounding variations due to selection of parameters and templates, by learning the equivalence class derived from the multitude of representations of an individual anatomy. A morphological appearance manifold obtained by varying parameters of the template warping procedure is estimated. Group-wise registration and statistical analysis is then based on a constrained optimization framework, which employs a minimum variance criterion to perform manifold walking, i.e. to traverse each individual's morphological appearance manifold until group variance is minimal. Effectively, this process removes the aforementioned confounding effects and potentially leads to morphological representations that reflect purely underlying biological variations, instead of variations introduced by modeling assumptions and parameter settings. The nonlinearity of a morphological appearance manifold is treated via local linear approximations of the manifold via PCA.	anatomic structures;approximation;clinical use template;computational anatomy;constrained optimization;mathematical optimization;nonlinear system;population parameter;sample variance;traverse;turing completeness;manifold;registration - actclass	Sajjad Baloch;Christos Davatzikos	2009	2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1016/j.neuroimage.2008.10.048	computer vision;morphological analysis;artificial intelligence;mathematics;geometry;global optimization;voxel-based morphometry	Vision	44.13231489595789	-78.3263957160403	145089
bae027d3295d1561d1845ff47e296abd91b8c490	improved method for automatic cerebrovascular labelling using stochastic tunnelling	stochastic processes blood vessels brain computerised tomography feature extraction image resolution image segmentation medical image processing simulated annealing;simulated annealing optimization automatic cerebrovascular labelling stochastic tunnelling high morphological variation cerebral vasculature complexity vessel patterning cerebral vessels high resolution 3d images segmented vasculature attributed relational graph relational features stochastic relaxation algorithm optimization schemes microcomputerised tomography images c57bl 6j mice leave one out test mean recognition rate;labeling mice tunneling simulated annealing energy states arteries	The complexity and high morphological variation of cerebral vasculature make comparison and analysis of the vessel patterning difficult and laborious. A framework for automatic labelling of the cerebral vessels in high resolution 3D images has been introduced in the literature that addresses this need. The segmented vasculature is represented as an attributed relational graph. Each vessel segment is an edge in the graph with local attributes such as diameter and length, as well as relational features representing the connectivity of the vessel segments. Each edge in the graph is automatically labelled with an anatomical name through a stochastic relaxation algorithm. In this paper, we compare the performance of four different optimization schemes, including stochastic tunnelling, for automatic labelling. We validated our method on 7 micro-CT images of C57Bl/6J mice with a leave-one-out test. The mean recognition rate of complete cerebrovasculature using stochastic tunnelling is 80% and shows a 2% (>60 vessel segments) improvement compared to simulated annealing optimization.	algorithm;ct scan;diameter (protocol);image resolution;linear programming relaxation;machine learning;mathematical optimization;relaxation (iterative method);run time (program lifecycle phase);simulated annealing;stochastic gradient descent;test set	Sahar Ghanavati;Jason P. Lerch;John G. Sled	2014	2014 International Workshop on Pattern Recognition in Neuroimaging	10.1109/PRNI.2014.6858519	computer vision;computer science;artificial intelligence;machine learning	Vision	40.29552853529944	-78.04733321945754	145353
7ecee144f35abed3369ecbb25d9035cf3043e40f	knowledge guided information fusion for segmentation of multiple sclerosis lesions in mri images	fuzzy set;information sources;tissues;information extraction;spectrum;mr imaging;magnetic resonance imaging;expert knowledge;spectral imaging;information fusion;multiple sclerosis;partial volume effect	ABSTRACT In this work, T1-, T2- and PD-weighted MR images of multiple sclerosis (MS) patients, providing information on the properties of tissues from different aspects, are treated as three independent information sources for the detection and segmentation of MS lesions. Based on information fusion theory, a knowledge guided information fusion framework is proposed to accomplish 3-D segmentation of MS lesions. This framework consists of three parts: (1) information extraction, (2) information fusion, and (3) decision. Information provided by different spectral images is extracted and modeled separately in each spectrum using fuzzy sets, aiming at managing the uncertainty and ambiguity in the images due to noise and partial volume effect. In the second part, the possible fuzzy map of MS lesions in each spectral image is constructed from the extracted information under the guidance of experts knowledge, and then the final fuzzy map of MS lesions is constructed through the fusion of the fuzzy maps obtained from different spectrum. Finally, 3-D segmentation of MS lesions is derived from the final fuzzy map. Experimental results show that this method is fast and accurate. Keywords:		Chaozhe Zhu;Tianzi Jiang	2003		10.1117/12.480312	computer vision;data mining;mathematics;communication	Vision	41.53694594302329	-72.36753007635178	145421
ec3c46314f5084a6d68865282c0b1788bf972159	relationship between the stroma edge and skin-air boundary for generating a dependency approach to skin-line estimation in screening mammograms	artefacto;background noise;metodo adaptativo;criblage;base donnee;adaptive thresholding;piel;threshold detection;distance measure;mastografia;peau;image databank;skin;screening;mammary gland;database;base dato;courbure;methode adaptative;segmentation;data mining;artefact;glandula mamaria;detection seuil;deteccion umbral;mammographie;realite terrain;fouille donnee;banco imagen;adaptive method;banque image;depistage;descubrimiento;pattern recognition;cernido;curvatura;ruido fondo;realidad terreno;curvature;ground truth;glande mammaire;medical screening;reconnaissance forme;mammography;reconocimiento patron;bruit fond;busca dato;segmentacion	Breast area segmentation or skin-line extraction in mammograms is very important in many aspects. Prior segmentation can reduce the effects of background noise and artifacts on the analysis of mammograms. In this paper, we investigate a novel method to estimate the breast skin-line in mammograms. Adaptive thresholding [1] yields a nearly perfect skin-line at the center of the image and around the nipple area with images from the MIAS database [2], but the upper and lower portions of the extracted boundary have been observed to be erroneous due to noise and artifacts. Because the distance from the edge of the stroma to the actual skin-line is usually uniform, we propose a method to estimate the skin-line from the edge of the stroma, with the information provided by the center portion around the nipple from adaptive thresholding. The results are compared with the ground-truth boundaries drawn by a radiologist [3] using polyline distance measure and shape smoothness measure. The results on 83 mammograms from the MIAS database are demonstrated. The proposed methods led to a decrease in a shape smoothness measure based upon curvature, on the average, from 65.6 to 20.0 over the 83 mammograms tested, resulting in an improvement of 69.5%.		Yajie Sun;Jasjit S. Suri;Rangaraj M. Rangayyan;Roman Janer	2005		10.1007/11552499_82	computer vision;ground truth;computer science;artificial intelligence;database;background noise;geometry;curvature;skin;thresholding;segmentation	Robotics	47.309574543962	-74.38830749239635	145488
49ab611c58de36c0ca9208c813d1f193e470058c	geodesic bezier curves: a tool for modeling on triangulations	seed relative segmentation robustness;image foresting transform;image segmentation;fuzzy connectedness approach;robustness image segmentation iterative methods focusing inverse problems cost function computer graphics image processing image analysis filtering;inverse problem seed relative segmentation robustness fuzzy connectedness approach iterative relative fuzzy object extraction watershed transforms image foresting transform minimum spanning forest tie zone transform;minimum spanning forest;watershed transforms;fuzzy set theory;iterative methods;watershed transform;inverse problem;seed set;transforms fuzzy set theory image segmentation inverse problems iterative methods object detection;transforms;tie zone transform;iterative relative fuzzy object extraction;object detection;inverse problems	We define a new class of curves, called geodesic Bezier curves, that are suitable for modeling on manifold triangulations. As a natural generalization of Bezier curves, the new curves are as smooth as possible. We discuss the construction of C0 and C1 piecewise Bezier splines. We also describe how to perform editing operations, such as trimming, using these curves. Special care is taken to achieve interactive rates for modeling tasks.	3d modeling;3d scanner;business continuity;bézier curve;computation;cyberware;graph coloring;image resolution;manifold regularization;point cloud;scott continuity;statistical manifold;subdivision surface;texture mapping	Dimas Martínez Morera;Paulo Cezar Pinto Carvalho;Luiz Velho	2007	XX Brazilian Symposium on Computer Graphics and Image Processing (SIBGRAPI 2007)	10.1109/SIBGRAPI.2007.38	computer vision;mathematical optimization;machine learning;segmentation-based object categorization;mathematics;image segmentation;scale-space segmentation	Graphics	46.02374559625908	-70.35913926085907	145505
a4a47677e5f9afa3fed45328178e82526c717f12	segmentation of color images using genetic algorithm with image histogram	image segmentation;algorithms;genetic algorithms;color image segmentation	ABSTRACT This paper proposes a family of color image segmentation algorithms using genetic approach and color similarity threshold in terns of Just noticeable difference. Instead of segmenting and then optimizing, the proposed technique directly uses GA for optimized segmentation of color images. Application of GA on larger size color images is computationally heavy so they are applied on 4D-color image histogram table. The performance of the proposed algorithms is benchmarked on BSD dataset with color histogram based segmentation and Fuzzy C-means Algorithm using Probabilistic Rand Index (PRI). The proposed algorithms yield better analytical and visual results. Keywords: 4D-histogram, Genetic algorithms, Histogram based segmentation, PRI. 1. INTRODUCTION Unavailability of a universal segmentation algorithm with performance matching to human vision especially for color image segmentation still keeps this complex but wel res earched field well open for further research. A color image has more information but it increases the complexity of the clustering problem due to increased color space dimensionality and also increased expectations due to enhanced human visibility and subsequent interpretation. A way of handling the computational complexity is to use a directed search methods like genetic algorithms. Ge net ic algorithms [1],[2] mimic the process of evolution and have many qualities that make them suitable for image segmentation. Max Mignotte in [3] presented a mu ltidimensional scaling based segmentatio n i.e., a hybrid model based on a preliminary spatially adaptive non-linear data dimensionality reduction step integrating color, contour and texture cues. Most of the segmentation techniques provide rough or approximate solution to the image segmentation problem, especially when applied to color image segmentation. They further need optimization for enhancing the performance. The results of various segmentation techniques [3] are shown in Table I. Fuzzy set theory has been extensively applied in the area of image segmentation. Among the available techniques Fuzzy C means (FCM) technique is widely used [4 ]. This technique fails to segment noisy images properly. To overcome this problem this approach is modified and is presented in [5],[6]. The modified FCM technique has been presented in [7] to compensate for intensity non-homogeneities. Just Notic eable Difference (JND) color image histogram is a good framework for color discrimination. Colo r corresponding to each bin in such histogram is visually dissimilar from that of any other neighboring bin as each bin represents only one color [8]. The JND color similarity measure is derived using human vision physiologica l capabilities and the Euclidean distance in the 3D RGB space [2]. Most of the techniques surveyed in [3] are either computationally very complex or use multiple features for segmentation. This paper presents a simple computationally light but efficient segmentation technique in terms of PRI. Section 1 presents literature survey and introduction to the topic. Section 2 presents required methods and materials including a brief human vision background. Section 3 presents the proposed algorithms and the related discussion. Section 4 presents experimental results, discussions and benchmarking with visual results of the other algorithms. Finally, section 5 presents conclusion.	genetic algorithm;image histogram	P. Sneha Latha;Pawan Kumar;Samruddhi Y. Kahu;K. M. Bhurchandi	2014		10.1117/12.2180559	color histogram;image texture;computer vision;color normalization;computer science;machine learning;segmentation-based object categorization;pattern recognition;region growing;image segmentation;scale-space segmentation;histogram equalization	Robotics	42.12840521197346	-68.27471585528652	145656
5aaa7ed3c9b00b7d900f74de454563f206612c6e	automatic segmentation of human brain sulci	bayesian framework;bayesian classification;automatic segmentation;complex structure;watershed method;structural complexity;human brain mapping;gray matter;cortical surface mesh;sulcus segmentation;human brain	The neocortical surface has a rich and complex structure comprised of folds (gyri) and fissures (sulci). Sulci are important macroscopic landmarks for orientation on the cortex. A precise segmentation and labeling of sulci is helpful in human brain mapping studies relating brain anatomy and function. Due to their structural complexity and inter-subject variability, this is considered as a non-trivial task. An automatic algorithm is proposed to accurately segment neocortical sulci: vertices of a white/gray matter interface mesh are classified under a Bayesian framework as belonging to gyral and sulcal compartments using information about their geodesic depth and local curvature. Then, vertices are collected into sulcal regions by a watershed-like growing method. Experimental results demonstrate that the method is accurate and robust.	anatomic structures;anatomical compartments;classification;fissure;gray matter;heart rate variability;interface device component;outline of brain mapping;structural complexity (applied mathematics);vertex (geometry);watershed (image processing);algorithm;biologic segmentation	Faguo Yang;Frithjof Kruggel	2008	Medical image analysis	10.1016/j.media.2008.01.003	computer vision;structural complexity;naive bayes classifier;computer science;artificial intelligence;machine learning;generalized complex structure;mathematics;anatomy	Vision	42.68231646731214	-78.01085112751036	145665
03aa1367e8504b432eba452939042ff6778757a8	investigation on an em framework for partial volume image segmentation	expectation maximization algorithms;computer simulations;pv effect;image segmentation;tissues;statistical models;statistical model;expectation maximization;mixture model;partial volume;map image segmentation;tissue mixture;parameter estimation;em algorithm;computer simulation	This work investigates a new partial volume (PV) image segmentation framework with comparison to a previous PV approach. The new framework utilizes an expectation-maximization (EM) algorithm to estimate simultaneously (1) tissue fractions in each image voxel and (2) statistical model parameters of the image data under the principle of maximum a posteriori probability (MAP). The previous EM approach models the PV effect by down-sampling a voxel and then labels each subvoxel as a pure tissue type, where the number of subvoxels labeled by a given tissue type over the total number of subvoxels reflects the fraction of that tissue type inside the original voxel. The tissue fractions in each voxel in this discrete PV model are represented by a limited number of percentage values. In the new MAPEM approach, the PV effect is modeled in a continuous space and estimated directly as the fraction of each tissue type in the original voxel. The previous discrete PV model would converge to the proposed continuous PV tissue-mixture model if there is an infinite number of subvoxels within a voxel. However, in practice a voxel is usually downsampled once or twice for computational reasons. A simulation study reveals that the continuous PV model is not only more realistic but also more accurate than the discrete PV model.	converge;decimation (signal processing);expectation–maximization algorithm;image segmentation;mixture model;page view;sampling (signal processing);simulation;statistical model;voxel	Daria Eremina;Xiang Li;Wei Zhu;Jing Wang;Zhengrong Liang	2006		10.1117/12.653843	mathematical optimization;machine learning;mathematics;statistics	ML	52.39218025427498	-75.37307602668044	145783
a2010c40b1af979665902fbbb1777a7be8e6233e	towards a balanced trade-off between speed and accuracy in unsupervised data-driven image segmentation	spatial resolution;image sampling;image segmentation;parallel algorithms	When it comes to image segmentation in the megapixel domain, most state-of-the-art algorithms use sampling to reduce the amount of data to be processed to reach a lower running time. Random patterns and equidistant sampling usually result in a suboptimal result because, in general, the distribution of image content is not homogeneous. The segmentation framework we propose in this paper, employs a content-adaptive technique that samples homogeneous and inhomogeneous regions sparsely and densely, respectively, thus it preserves information content in a computationally efficient way. Both the sampling procedure and the pixel-cluster assignment are guided by the same nonlinear confidence value, calculated for each image pixel with no overhead, which describes the strength of the pixel-cluster bond. Building on this confidence scheme, each pixel is associated with the most similar class with respect to its spatial position and color. We compare the performance of our framework to other segmentation algorithms on publicly available segmentation databases and using a set of 10-megapixel images, we show that it provides similar segmentation quality to a mean shift-based reference in an order of magnitude shorter time, the speedup being proportional to the amount of details in the input image. Based on our findings, we also sketch up novel design aspects to be taken into account when designing a high resolution evaluation framework.	adaptive system;algorithm;algorithmic efficiency;cluster analysis;color space;computation;database;heuristic (computer science);image resolution;image segmentation;manycore processor;mean shift;modular design;nonlinear system;numerical analysis;overhead (computing);pixel;reference implementation;sampling (signal processing);self-information;sketchup;speedup;time complexity;topography;unsupervised learning	Balázs Varga;Kristóf Karacs	2013	Machine Vision and Applications	10.1007/s00138-013-0503-3	computer vision;range segmentation;computer science;machine learning;segmentation-based object categorization;data mining;region growing;image segmentation;scale-space segmentation	Vision	46.21587911124645	-68.79426694606606	145797
9b9346346f7e4fed0948d148506fbd46982ce335	adaptive mesh generation for diffuse optical tomography (invited paper)	diffuse optical tomography;mesh generation adaptive optics tomography inverse problems us department of transportation biomedical optical imaging image reconstruction optical imaging error analysis biomedical engineering;numerical solution;generic algorithm;adaptive mesh;optical image reconstruction;optical tomography;discretization;adaptive refinement;discretization adaptive mesh generation diffuse optical tomography optical image reconstruction;error analysis;optical imaging;biomedical engineering;inverse problem;optical tomography biomedical optical imaging image reconstruction inverse problems medical image processing mesh generation;image reconstruction;medical image processing;us department of transportation;adaptive mesh generation;numerical experiment;biomedical optical imaging;error estimate;mesh generation;tomography;adaptive optics;inverse problems	In diffuse optical tomography (DOT), the discretization error in the numerical solutions of the forward and inverse problems results in error in the reconstructed optical images. In this work, based on the analysis presented by Guven, M et al., we present two theorems that constitute the basis for adaptive mesh generation for the forward and inverse DOT problems. The proposed discretization schemes lead to adaptively refined composite meshes that yield the desired level of imaging accuracy while reducing the size of the discretized forward and inverse problems. Our numerical experiments validate the error estimates developed by Guven, M et al. and show that the new adaptive mesh generation algorithms improve the accuracy of the reconstructed optical images	algorithm;ct scan;characteristic impedance;diffuse optical imaging;discretization error;error analysis (mathematics);estimation theory;experiment;mesh generation;microwave;numerical analysis;tomography	Murat Guven;Birsen Yazici;Eldar Giladi	2007	2007 4th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2007.357118	computer vision;mathematical optimization;radiology;medicine;computer science;inverse problem;tomography;optics	Vision	51.75825174087018	-80.1692610235964	145819
a950b47a700befd7345877a8a53e9e2e7d70a13a	active image: a shape and topology preserving segmentation method using b-spline free form deformations	topology;spline;shape preserving;segmentation criteria shape preserving segmentation topology preserving segmentation b spline free form deformation active contour model object boundary active image segmentation multiple object segmentation occlusion;image segmentation;occlusion;object boundary;multiple object segmentation;level set;active contours;splines mathematics;topology preserving segmentation;multiple objectives;shape;image edge detection;topology image segmentation splines mathematics;b spline free form deformation;free form deformation;b spline;image segmentation shape spline topology active contours level set image edge detection;segmentation criteria;topology preservation;topology preserving image segmentation b spline free form deformations shape preserving;active image segmentation;shape preserving segmentation;topology preserving;active contour model;free form deformations	Unlike most conventional active contour models that drive the initial contours to match the object boundaries, we propose an “active image” segmentation method that deforms the image to match the initial contours, and hence can simultaneously segment multiple objects. The deformation field is modeled by B-spline free form deformations (FFD). Penalizing the bending energy of B-spline FFD enables us to preserve both the local shape and the topology of objects of interest. Preliminary results on both synthetic and real world images show that the proposed method is able to overcome low contrast, occlusion, and other defects by using simple segmentation criteria.	active contour model;b-spline;free-form deformation;hidden surface determination;image segmentation;synthetic intelligence	Chao Li;Ying Sun	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5652762	b-spline;spline;computer vision;topology;shape;computer science;level set;active contour model;mathematics;geometry;image segmentation;scale-space segmentation	Vision	47.573969048373705	-71.52092585409673	145841
5cdc21414c4c4486743e7927364790add30a08a0	efficient image fusion with approximate sparse representation	multi modal sensors;image fusion;multi resolution;sparse representation	In this paper, an efficient approximate sparse representation (SR) algorithm with multiselection strategy is used to solve the image fusion problem. We have shown that the approximate SR is effective for image fusion even if the sparse coefficients are not the sparsest ones possible. A multi-selection strategy is used to accelerate the process of generating the approximate sparse coefficients which are used to guide the fusion of image patches. The relative parameters are also investigated experimentally to further reduce the computational time. The proposed method is compared with some state-of-theart image fusion approaches on several pairs of multi-source images. The experimental results exhibit that the proposed method is able to yield superior fusion results with less consumption time.	approximation algorithm;coefficient;computation;experiment;image fusion;multi-source;nl (complexity);numerical aperture;openmp;sparse approximation;sparse matrix;time complexity	Yang Bin;Yang Chao;Huang Guoyu	2016	IJWMIP	10.1142/S0219691316500247	mathematical optimization;computer science;machine learning;pattern recognition;sparse approximation;image fusion	Vision	50.14019302854838	-67.73662475833586	146792
d36d99fcefeddf0cc637898a7ef33057566dca3a	geodesic distances in probabilistic spaces for patch-based ultrasound image processing		Many recent ultrasound image processing methods are based on patch comparison, such as filtering and segmentation. Identifying similar patches in noise-corrupted images is a key factor for the performance of these methods. While the Euclidean distance is ideal to handle the patch comparison under additive Gaussian noise, finding good measures to compare patches corrupted by multiplicative noise is still an open research. In this paper, we deduce several new geodesic distances, arising from parametric probabilistic spaces, and suggest them as similarity measures to process RF and log-compressed ultrasound images in patch-based methods. We provide practical examples using these measures in the fields of ultrasound image filtering and segmentation, with results that confirm the potential of the technique.	euclidean distance;experiment;filter (signal processing);image processing;multiplicative noise;normal statistical distribution;open research;patch (computing);radio frequency;rayleigh–ritz method;statistical model;synthetic intelligence;utility functions on indivisible goods;version;biologic segmentation;format	Cid A. N. Santos;Nelson D. A. Mascarenhas	2019	IEEE Transactions on Image Processing	10.1109/TIP.2018.2866705	image processing;computer vision;euclidean distance;filter (signal processing);probabilistic logic;geodesic;gaussian noise;parametric statistics;pattern recognition;artificial intelligence;mathematics;multiplicative noise	Vision	50.48347705458789	-75.7149581501784	146853
9165ed2e481709d5cf982aaf5f062f8ea9d8c8d1	multiscale edge detection using wavelet transform			edge detection;wavelet transform	Rasha M. Ayed;Ramadan Moawad;Alaa Abd El-Rehim;Ben Bella S. Tawfik	2011	Egyptian Computer Science Journal		stationary wavelet transform;second-generation wavelet transform;wavelet;wavelet transform;computer vision;engineering;electronic engineering;harmonic wavelet transform;artificial intelligence;wavelet packet decomposition;discrete wavelet transform;lifting scheme	Theory	51.76320041689527	-66.18107048937405	146866
0c4411434c131692b01087b23c77ca3fd59978d5	an expectation-maximization approach to joint curve evolution for medical image segmentation	image features;expectation maximization algorithms;brain;image segmentation;curve evolution;level set;statistical model;maximum a posterior;expectation maximization;brain imaging;shape parameter;algorithms;missing data;shape modeling;shape priors;medical image segmentation;em algorithm	This paper proposes a new Expectation-Maximization curve evolution algorithm for medical image segmentation. Traditional level set algorithms perform poorly when image information is incomplete, missing or some objects are corrupted. In such cases, statistical model-based segmentation methods are widely used since they allow object shape variations subject to shape prior constraints to overcome the incomplete or noisy information. Although matching robustly in dealing with noisy and low contrast images, the shape parameters are estimated intractably through the Maximum A Posterior (MAP) framework by using incomplete image features. In this paper, we present a statistical shape-based joint curve evolution algorithm for image segmentation based on the assumption that using hidden features of the image as missing data can simplify the estimation problem and help improve the matching performance. In our method, these hidden features are designed to be the local voxel labeling data determined based on the intensity distribution of the image and priori anatomical knowledge. Using an Expectation-Maximization formulation, both the hidden features and the object shapes can be extracted. In addition, this EM-based algorithm is applied to the joint parameter and non-parameter shape model for more accurate segmentation. Comparative results on segmenting putamen and caudate shapes in MR brain images confirm both robustness and accuracy of the proposed curve evolution algorithm.	computational complexity theory;encode;estimation theory;evolution;expectation–maximization algorithm;image segmentation;information model;information theory;missing data;statistical model;voxel	Mahshid Farzinfar;Eam Khwang Teoh;Zhong Xue	2010		10.1117/12.844055	active shape model;image texture;computer vision;expectation–maximization algorithm;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation;neuroimaging	Vision	44.08475387035781	-77.58657642910701	146975
49b9eda9c1cb1879515bce8ea2b94782bcda0a30	robust segmentation and accurate target definition for positron emission tomography images using affinity propagation	image segmentation;target definition;positron emission tomography diseases image segmentation medical image processing;positron emission tomography;positron emission tomography image segmentation histograms lesions measurement rabbits;medical image processing;affinity propagation;tuberculosis robust segmentation accurate target definition positron emission tomography images affinity propagation distributed inflammation infectious diseases immunopathology pet uptake region based methods intensity based segmentation optimal thresholding value gray level histogram data points rabbits;pet segmentation;diseases;thresholding;target definition pet segmentation affinity propagation thresholding image segmentation	Distributed inflammation in infectious diseases cause variable uptake regions in positron emission tomography (PET) images. Due to this distributed nature of immuno-pathology and associated PET uptake, intensity based methods are much better suited over region based methods for segmentation. The most commonly used intensity based segmentation is thresholding, but it has a major drawback of a lack of consensus on the selection of the thresholding value. We propose a method to select an optimal thresholding value by utilizing a novel similarity metric between the data points along the gray-level histogram of the image then using Affinity Propagation (AP) to cluster the intensities based on this metric. This method is tested against the PET images of rabbits infected with tuberculosis with distributed uptakes with promising results.	affinity propagation;ct scan;data point;grayscale;image segmentation;polyethylene terephthalate;processor affinity;software propagation;thresholding (image processing);tomography	Brent Foster;Ulas Bagci;Brian M Luna;Bappaditya Dey;William R Bishai;Sanjay Jain;Ziyue Xu;Daniel J. Mollura	2013	2013 IEEE 10th International Symposium on Biomedical Imaging	10.1109/ISBI.2013.6556810	computer vision;computer science;machine learning;thresholding;image segmentation;nuclear medicine;scale-space segmentation;affinity propagation;medical physics	Vision	42.52202579616511	-75.69683440509283	147013
928560e47f9076ac94547ee87ddc0dde8e04abf9	fast binarisation with chebyshev inequality		In order to enhance the binarization result of degraded document images with smudged and bleed-through background, we present a fast binarization technique that applies the Chebyshev theory in the image preprocessing. We introduce the Chebyshev filter which uses the Chebyshev inequality in the segmentation of objects and background. Our result shows that the Chebyshev filter is not only effective, but also simple, robust and easy to implement. Because of its simplicity, our method is sufficiently efficient to process live image sequences in real-time. We have implemented and compared with the Document Image Binarization Contest datasets (H-DIBCO 2014) for testing and evaluation. The experimental outcomes have demonstrated that this method achieved good result in this literature.	binary image;chebyshev filter;chebyshev polynomials;grayscale;live usb;mathematical optimization;pixel;preprocessor;real-time clock;robustness (computer science);social inequality;thresholding (image processing)	Ka-Hou Chan;Sio Kei Im;Wei Ke	2017		10.1145/3103010.3121033	computer science;chebyshev's inequality;chebyshev filter;preprocessor;computer vision;thresholding;artificial intelligence	Vision	41.724867891795476	-68.19325047691754	147039
0c3f44b2c2e74a849c205551edfcf4f5b1b71e6f	multiscale segmentation with vector-valued nonlinear diffusions on arbitrary graphs	equation non lineaire;graph theory;texture nonlinear diffusions scale space segmentation stabilized inverse diffusion equations sides;nonlinear diffusion;nonlinear diffusions;ecuacion no lineal;evaluation performance;texture;shape information multiscale segmentation vector valued nonlinear diffusions arbitrary graphs nonlinear diffusion equations multivalued image segmentation stabilized inverse diffusion equations scalar valued signals vector valued image processing;image segmentation;proceso difusion;performance evaluation;image processing;methode echelle multiple;algorithms artificial intelligence computer simulation diffusion image enhancement image interpretation computer assisted information storage and retrieval models statistical nonlinear dynamics numerical analysis computer assisted pattern recognition automated signal processing computer assisted;evaluacion prestacion;ecuacion vectorial;processus diffusion;procesamiento imagen;metodo escala multiple;segmentation;nonlinear equations image segmentation graph theory;indexing terms;traitement image;stabilized inverse diffusion equations sides;scale space;ecuacion difusion;segmentation image;textura;diffusion equation;nonlinear equations;multiscale method;diffusion process;equation vectorielle;vector equation;non linear equation;equation diffusion;image segmentation nonlinear equations gaussian processes image restoration gaussian noise filtering image edge detection signal restoration shape diffusion processes	We propose a novel family of nonlinear diffusion equations and apply it to the problem of segmentation of multivalued images. We show that this family can be viewed as an extension of stabilized inverse diffusion equations (SIDEs) which were proposed for restoration, enhancement, and segmentation of scalar-valued signals and images in . Our new diffusion equations can process vector-valued images defined on arbitrary graphs which makes them well suited for segmentation. In addition, we introduce novel ways of utilizing the shape information during the diffusion process. We demonstrate the effectiveness of our methods on a large number of segmentation tasks.	circuit restoration;diffusion;graph - visual representation;image segmentation;nonlinear system;biologic segmentation	Xiaogang Dong;Ilya Pollak	2006	IEEE Transactions on Image Processing	10.1109/TIP.2006.873473	system of linear equations;computer vision;diffusion equation;mathematical optimization;discrete mathematics;scale space;index term;image processing;computer science;graph theory;diffusion process;segmentation-based object categorization;mathematics;image segmentation;texture;scale-space segmentation;segmentation	Vision	52.332981710151195	-69.86886513349731	147290
2591ecfd9ad55f92c4a3f351019d52eadbb54340	automatic localization of the lumbar vertebral landmarks in ct images with context features		A recent research direction for the localization of anatomical landmarks with learning-based methods is to explore ways to enrich the trained models with context information. Lately, the addition of context features in regression-based approaches has been tried in the literature. In this work, a method is presented for the addition of context features in a regression setting where the locations of many vertebral landmarks are regressed all at once. As this method relies on the knowledge of the centers of the vertebral bodies (VBs), an automatic, endplate-based approach for the localization of the VB centers is also presented. The proposed methods are evaluated on a dataset of 28 lumbar-focused computed tomography images. The VB localization method detects all of the lumbar VBs of the testing set with a mean localization error of 3.2 mm. The multi-landmark localization method is tested on the task of localizing the tips of all the inferior articular processes of the lumbar vertebrae, in addition to their VB centers. The proposed method detects these landmarks with a mean localization error of 3.0 mm.	ct scan;dendritic spine;experiment;sensor	Dimitrios Damopoulos;Ben Glocker;Guoyan Zheng	2017		10.1007/978-3-319-74113-0_6	computed tomography;lumbar;lumbar vertebrae;computer vision;articular processes;computer science;artificial intelligence	Robotics	41.84504102780277	-78.19399799419647	147540
713adbd636bd4a45c0d6150c5faee793c511e4dd	a cost minimization approach to edge detection using simulated annealing	vision ordenador;cost function;weighted cost factors;edge detection;localization;weighting;picture processing;segmentation;simulated annealing;ponderacion;funcion coste;computer vision;minimizacion costo;minimisation cout;cost minimization;image edge detection simulated annealing cost function detection algorithms image processing surface fitting pixel guidelines temperature image segmentation;simulated annealing pattern recognition signal processing;signal processing;pattern recognition;fonction cout;vision ordinateur;ponderation;thinness;segmentacion;thinness picture processing pattern recognition cost minimization edge detection simulated annealing weighted cost factors localization	The authors cast edge detection as a problem in cost minimization. This is achieved by the formulation of a cost function that evaluates the quality of edge configurations. The function is a linear sum of weighted cost factors. The cost factors capture desirable characteristics of edges such as accuracy in localization, thinness, and continuity. Edges are detected by finding the edge configurations that minimize the cost function. The authors give a mathematical description of edges and analyze the cost function in terms of the characteristics of the edges in minimum cost configurations. Through the analysis, guidelines are provided on the choice of weights to achieve certain characteristics of the detected edges. The cost function is minimized by the simulated annealing method. A set of strategies is presented for generating candidate states and to devise a suitable temperature schedule. >	edge detection;simulated annealing	Hin Leong Tan;Saul B. Gelfand;Edward J. Delp	1992	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.107010	reduced cost;computer vision;mathematical optimization;edge detection;internationalization and localization;simulated annealing;computer science;machine learning;signal processing;weighting;mathematics;segmentation	Vision	48.20035481069331	-69.41959728450749	147549
f244108ad21385d13b9daef3c472e812f2d84e86	robust and efficient linear registration of white-matter fascicles in the space of streamlines	diffusion mri;fiber tracking;streamlines;tracts;registration;fascicles;bundles	The neuroscientific community today is very much interested in analyzing specific white matter bundles like the arcuate fasciculus, the corticospinal tract, or the recently discovered Aslant tract to study sex differences, lateralization and many other connectivity applications. For this reason, experts spend time manually segmenting these fascicles and bundles using streamlines obtained from diffusion MRI tractography. However, to date, there are very few computational tools available to register these fascicles directly so that they can be analyzed and their differences quantified across populations. In this paper, we introduce a novel, robust and efficient framework to align bundles of streamlines directly in the space of streamlines. We call this framework Streamline-based Linear Registration. We first show that this method can be used successfully to align individual bundles as well as whole brain streamlines. Additionally, if used as a piecewise linear registration across many bundles, we show that our novel method systematically provides higher overlap (Jaccard indices) than state-of-the-art nonlinear image-based registration in the white matter. We also show how our novel method can be used to create bundle-specific atlases in a straightforward manner and we give an example of a probabilistic atlas construction of the optic radiation. In summary, Streamline-based Linear Registration provides a solid registration framework for creating new methods to study the white matter and perform group-level tractometry analysis.	adobe streamline;align (company);atlases;eye;fascicle - nerve fibers;jaccard index;nonlinear system;piecewise linear continuation;population;requirements analysis;sex characteristics;tract (literature);white matter;registration - actclass	Eleftherios Garyfallidis;Omar Ocegueda;Demian Wassermann;Maxime Descoteaux	2015	NeuroImage	10.1016/j.neuroimage.2015.05.016	psychology;diffusion mri;computer vision;neuroscience;simulation;radiology;streamlines, streaklines, and pathlines;computer science;artificial intelligence	Vision	42.085152891544574	-80.10645101401123	147582
e72bf4c43ce1a4bab1066fc0c60c3ab81dd9aec7	hybrid self organizing map for improved implementation of brain mri segmentation	self organising feature maps biomedical mri fuzzy set theory image segmentation medical image processing pattern clustering;pattern clustering;fuzzy c mean;image segmentation;principal tissue structures;tumor detection;segmentation;fuzzy set theory;tumor detection image analysis segmentation hsom fuzzy c mean;mr imaging;clustering based approach;organizing magnetic resonance imaging image segmentation neoplasms filters clustering algorithms partitioning algorithms cancer biomedical imaging medical diagnostic imaging;self organising feature maps;hybrid self organizing map algorithm;medical image processing;clustering method;magnetic resonance imaging;pixel;classification algorithms;fuzzy c mean clustering algorithm;tumors;brain imaging;clustering algorithms;unsupervised mr image segmentation method;image analysis;self organized map;neurons;unsupervised mr image segmentation method hybrid self organizing map algorithm brain mri segmentation clustering based approach second phase image segmentation principal tissue structures fuzzy c mean clustering algorithm;fuzzy c means clustering;medical image segmentation;second phase image segmentation;biomedical mri;brain mri segmentation;hsom	Image segmentation denotes a process of partitioning an image into distinct regions. A large variety of different segmentation approaches for images have been developed. Among them, the clustering methods have been extensively investigated and used. In this paper, a clustering based approach using a Self Organizing Map (SOM) algorithm is proposed for medical image segmentation. This paper describe segmentation method consists of two phases. In the first phase, the MRI brain image is acquired from patient database. In that film artifact and noise are removed. In the second phase (MR) image segmentation is to accurately identify the principal tissue structures in these image volumes. A new unsupervised MR image segmentation method based on fuzzy C-Mean clustering algorithm for the Segmentation is presented	algorithm;cluster analysis;image noise;image segmentation;organizing (structure);self-organizing map;unsupervised learning	T. Logeswari;Marcus Karnan	2010	2010 International Conference on Signal Acquisition and Processing	10.1109/ICSAP.2010.56	image texture;computer vision;image analysis;computer science;magnetic resonance imaging;machine learning;segmentation-based object categorization;pattern recognition;region growing;fuzzy set;image segmentation;cluster analysis;minimum spanning tree-based segmentation;scale-space segmentation;segmentation;pixel	Robotics	42.022940504911354	-73.3514931803697	147783
0fdea6fbe40ec9281c2268d613058daabb8add3b	non-euclidean basis function based level set segmentation with statistical shape prior	image segmentation;statistical analysis differential equations image segmentation medical image processing;level set image segmentation shape measurement conferences computational modeling mathematical model;statistical analysis;medical image processing;differential equations;image information image segmentation statistical shape model enhanced level sets noneuclidean radial basis functions statistical shape prior probabilistic map ordinary differential equations	We present a new framework for image segmentation with statistical shape model enhanced level sets represented as a linear combination of non-Euclidean radial basis functions (RBFs). The shape prior for the level set is represented as a probabilistic map created from the training data and registered with the target image. The new framework has the following advantages: 1) the explicit RBF representation of the level set allows the level set evolution to be represented as ordinary differential equations and reinitialization is no longer required. 2) The non-Euclidean distance RBFs makes it possible to incorporate image information into the basis functions, which results in more accurate and topologically more flexible solutions. Experimental results are presented to demonstrate the advantages of the method, as well as critical analysis of level sets versus the combination of both methods.	euclidean distance;experiment;image segmentation;medical image;radial (radio);radial basis function;registration;solutions;statistical shape analysis;biologic segmentation	Esmeralda Ruiz Pujadas;Marco Reisert;Li Bai	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610701	active shape model;image texture;computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation;differential equation;quantum mechanics;statistics	Vision	44.2661083232861	-76.6834430267836	147813
1850a29a00838c9db5cdc252453625af06a494f2	mrf energy minimization for unsupervised image segmentation	graph cut energy optimization method;minimization;estimation theory;optimisation;bayesian estimation;theoretical framework;image segmentation;min cut max flow algorithm mrf energy minimization unsupervised image segmentation markov random field bayesian estimation graph cut energy optimization method gaussian process expectation maximization algorithm perceptually uniform cielab color values;gaussian processes;bayes methods;min cut max flow;perceptually uniform;mrf energy minimization;perceptually uniform cielab color values;optimisation bayes methods estimation theory expectation maximisation algorithm gaussian processes image colour analysis image segmentation markov processes;data mining;markov random field;energy function;image segmentation markov random fields optimization methods parameter estimation probability computer science bayesian methods neural networks stochastic resonance recurrent neural networks;computational modeling;energy optimization;min cut max flow algorithm;expectation maximization;image color analysis;image colour analysis;graph cut;pixel;expectation maximization algorithm;power optimization;bayesian estimator;energy minimization;markov processes;gaussian process;near real time;perceptually uniform energy optimization em min cut max flow graph cuts image segmentation;em algorithm;graph cuts;em;labeling;unsupervised image segmentation;expectation maximisation algorithm	A Markov Random Field (MRF) model is proposed for unsupervised image segmentation in this paper. The theoretical framework is based on Bayesian estimation via the graph-cut energy optimization method. A Gaussian is used to model the density associated with each image segment (or class), and parameters are estimated with an expectation maximization (EM) algorithm. Here we use the perceptually uniform CIELAB color values instead of the RGB color. Graph cuts have emerged as a powerful optimization technique for minimizing MRF energy functions that arise in low-level vision problems. We adopt a new min-cut/max-flow algorithm which works several times faster than any of the other max-flow methods, which makes near real-time performance possible. Experimental results have been provided to illustrate the performance of our method.	cut (graph theory);energy minimization;expectation–maximization algorithm;high- and low-level;image segmentation;markov chain;markov random field;mathematical optimization;max-flow min-cut theorem;maxima and minima;maximum flow problem;minimum cut;real-time clock;real-time computing	Qiuxu Li;Jieyu Zhao	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.354	mathematical optimization;machine learning;pattern recognition;mathematics	Vision	51.025588133745345	-70.57585052350514	147854
2399af65c1fb6e22544e57e6bd090fb896191e17	automatic determination of lv orientation from spect data	clinical data;goniometria;topology;informatica biomedical;biomedical data processing;heart;image segmentation;image processing;genie biomedical;photon;cardiology;data visualization biomedical imaging medical diagnostic imaging geometry heart shape tomography cardiology nuclear medicine myocardium;long axis;topologie;informatique biomedicale;image segmentation medical image processing single photon emission computed tomography cardiology;hombre;tecnica medida;tomocentelleografia;segmentation;left ventricle;three dimensional;goniometrie;computer graphic;foton;topologia;orientation spatiale;algorithme;algorithm;accuracy;circulatory system;biomedical engineering;tl left ventricle orientation determination cardiac spect data clinical data visualization clinical data quantification topological goniometry volume visualization computer graphics ideas 3d object overall shape clinical data phantom data interactive graphical interface technetium 99m thallium 201 medical diagnostic imaging diagnostic nuclear medicine tomographic slices volume segmentation tc;ventricule gauche;exploration radioisotopique;medical image processing;emission tomography;three dimensional calculations;human;computer aid;single photon emission computed tomography;technique mesure;radionuclide study;volume visualization;asistencia ordenador;goniometry;ingenieria biomedica;nuclear medicine;interactive graphics;biology and medicine basic studies;appareil circulatoire;exploracion radioisotopica;orientacion espacial;measurement technique;aparato circulatorio;assistance ordinateur;segmentacion;tomoscintigraphie;ventriculo izquierdo;spatial orientation;homme;algoritmo	Presents a new method to determine the orientation or pose of the left ventricle (LV) of the heart from cardiac SPECT (single photon emission computed tomography) data. This proposed approach offers an accurate, fast, and robust delineation of the LV long-axis. The location and shape of the generated long-axis can then be utilized to define automatically the tomographic slices for enhanced visualization and quantification of the clinical data. The methodology is broadly composed of two main steps: (1) volume segmentation of cardiac SPECT data; and (2) topological goniometry, a novel approach incorporating volume visualization and computer graphics ideas to determine the overall shape of 3-D objects. The outcome of the algorithm is a 3-D curve representing the overall pose of the LV long-axis. Experimental results on both phantom and clinical data (50 technetium-99m and 74 thallium-201) are presented. An interactive graphical interface to visualize the volume (3-D) data, the left ventricle, and its pose is an integral part of the overall methodology. This technique is completely data driven and expeditious, making it viable for routine clinical use.	apache axis;axis vertebra;ct scan;computer graphics;floor and ceiling functions;graphical user interface;graphics visualization;heart ventricle;imagery;left ventricular structure;logical volume management;phantom reference;phantoms, imaging;photons;physical object;quantitation;scientific visualization;technetium 99m;thallium;tomography, emission-computed;tomography, emission-computed, single-photon;web slice;x-ray computed tomography;algorithm;biologic segmentation	Rakesh Mullick;Norberto F. Ezquerra	1995	IEEE transactions on medical imaging	10.1109/42.370405	three-dimensional space;computer vision;radiology;spatial disorientation;image processing;goniometer;computer science;circulatory system;photon;mathematics;accuracy and precision;image segmentation;nuclear medicine;segmentation;heart	Visualization	45.381171738185294	-79.48745143059597	147999
afd797ad3a08e028cf60485803432ca22381aeb6	gyral folding pattern analysis via surface profiling	sensitivity and specificity;cluster algorithm;female;parametric model;models neurological;imaging three dimensional;shape descriptor;clinical application;adolescent;male;profiles methods;power function;schizophrenia;image enhancement;image interpretation computer assisted;magnetic resonance imaging;indexation;affinity propagation;reproducibility of results;smooth transition;cerebral cortex;algorithms;pattern recognition automated;pattern analysis;humans;computer simulation;models anatomic;invariant feature	Folding is an essential shape characteristic of the human cerebral cortex. Descriptors of cortical folding patterns have been studied for decades. However, many previous studies are either based on local shape descriptors such as curvature, or based on global descriptors such as gyrification index or spherical wavelets. This paper proposes a gyrus-scale folding pattern analysis technique via cortical surface profiling. Firstly, we sample the cortical surface into 2D profiles and model them using a power function. This step provides both the flexibility of representing arbitrary shape by profiling and the compactness of representing shape by parametric modeling. Secondly, based on the estimated model parameters, we extract affine-invariant features on the cortical surface, and apply the affinity propagation clustering algorithm to parcellate the cortex into cortical regions with strict hierarchy and smooth transitions among them. Finally, a second-round surface profiling is performed on the parcellated cortical surface, and the number of hinges is detected to describe the gyral folding pattern. We have applied the surface profiling method to two normal brain datasets and a schizophrenia patient dataset. The experimental results demonstrate that the proposed method can accurately classify human gyri into 2-hinge, 3-hinge and 4-hinge patterns. The distribution of these folding patterns on brain lobes and the relationship between fiber density and gyral folding patterns are further investigated. Results from the schizophrenia dataset are consistent with commonly found abnormality in former studies by others, which demonstrates the potential clinical applications of the proposed technique.	affinity propagation;algorithm;cerebral cortex;cluster analysis;legendre wavelet;parametric model;patients;pattern recognition;profiling (computer programming);schizophrenia;silo (dataset);software propagation;tissue fiber;lobe;statistical cluster	Kaiming Li;Lei Guo;Gang Li;Jingxin Nie;Carlos Faraco;Guangbin Cui;Qun Zhao;L. Stephen Miller;Tianming Liu	2010	NeuroImage	10.1016/j.neuroimage.2010.04.263	computer simulation;computer vision;parametric model;power function;radiology;bioinformatics;magnetic resonance imaging;machine learning;schizophrenia;mathematics;affinity propagation;statistics	ML	42.97884308398491	-78.80419063245397	148183
4af291b63023d092ec9813b3632068a451407e55	differential evolution and covariance ellipsoid for non-rigid transformation tracking of internal organs		In nowadays research, tools that help to visualize physical phenomena are helpful to address a great variety of modeling problems. In particular, tools that offer a three-dimensional representation of internal organs represent a huge advantage to understand their functions and anomalies, which could lead to better diagnosis and treatments of diseases. In this paper, we propose a novel internal organs modeling algorithm, based on Differential Evolution algorithm and Covariance Ellipsoid computation. The proposed algorithm uses k-means++ clustering over the registration of a 4D-Magnetic Resonance Imaging of an internal organ and calculates the representative covariance ellipsoid, then we track the non-rigid transformation using the spectral decomposition of the covariance matrix and Differential Evolution to optimize the geometric representation. The proposal offers a compact 4D map with local optimized non-rigid transformations, and the algorithm scheme is simple to implement with low computational cost with good accuracy. It avoids calculating the covariance matrices every time. We show the experimental results of a liver’s right lobe movement during the respiratory cycle.	acoustic lobing;algorithm;cluster analysis;computation;computational complexity theory;differential evolution;k-means++;resonance	Carlos Villaseñor;Nancy Arana-Daniel;Alma Y. Alanis;Carlos López-Franco	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489463	differential evolution;ellipsoid;cluster analysis;covariance matrix;rigid transformation;pattern recognition;matrix (mathematics);matrix decomposition;artificial intelligence;computer science;covariance	Vision	48.639302834854426	-79.51765055159083	148457
0de2da179be24e257c2339abb2a24f7ba248ba00	hypothesis testing for coarse region estimation and stable point determination applied to markovian texture segmentation	parameter estimation markov processes iterative methods image texture image segmentation;stochastic procedure hypothesis testing coarse region estimation stable point determination texture segmentation markovian postprocessing scheme iterative methods image processing fixed points geometry constraints boundary distortion reduction;image segmentation;image processing;testing image segmentation image processing signal processing costs pixel convergence geometry labeling color;texture segmentation;image texture;fixed point;iterative methods;markov processes;parameter estimation;iteration method;hypothesis test	In this paper we show the benefits of applying hypothesis testing to the problem of texture segmentation. In our approach, hypothesis testing is used at two different stages that help to reduce the computational burden associated to iterative methods commonly used in image processing. Specifically, hypothesis testing is used to initially estimate the number of regions the image must be divided into, and to determine a set of points that will remain unchanged after the Markovian postprocessing scheme. These fixed points will contribute to reduce the number of iterations required by the Markovian stage and introduce geometry constraints that will reduce the boundary distortion caused by the stochastic procedure.		Lorenzo J. Tardón;Javier Portillo-García;Carlos Alberola-López;Juan Ignacio Trueba-Santander	1996		10.1109/ICIP.1996.560411	image texture;computer vision;mathematical optimization;feature detection;image processing;computer science;segmentation-based object categorization;mathematics;iterative method;region growing;image segmentation;scale-space segmentation;statistics	Vision	48.91745966920518	-67.43640105906056	148474
5ff8226b8961265fdc98e522d694a416503d6cfc	modeling and segmentation of noisy and textured images using gibbs random fields	dynamic programming;gibbs state;gibbs distribution;texture;vision ordenador;programacion dinamica;processing;general and miscellaneous mathematics computing and information science;mathematics;image segmentation;gibbs distributions;image processing;analisis textura;lattices;application software;texture modeling;approximation algorithms;markov random fields;image segmentation parameter estimation image processing dynamic programming markov random fields computer vision application software stochastic processes image restoration robustness;image restoration;dynamic program;segmentation;etat gibbs;mathematical logic;estimation of parameters in gibbs distributions;image bruitee;noise measurement;markov random field;computer vision;data covariances;algorithme;modelisation;imagen sonora;algorithm;algorritmo;gibbs random fields;texture analysis;computational modeling;campo aleatorio;stochastic processes;estado gibbs;mathematical models;noisy image;pixel;programmation dynamique;statistics;artificial intelligence;algorithms;robustness;vision ordinateur;textured image segmentation;parameter estimation;programming 990210 supercomputers 1987 1989;modeling;analyse texture;segmentacion;images;modelaje;champ aleatoire;texture modeling computer vision estimation of parameters in gibbs distributions gibbs distributions gibbs random fields image processing image segmentation markov random fields textured image segmentation;data models;random field	This paper presents a new approach to the use of Gibbs distributions (GD) for modeling and segmentation of noisy and textured images. Specifically, the paper presents random field models for noisy and textured image data based upon a hierarchy of GD. It then presents dynamic programming based segmentation algorithms for noisy and textured images, considering a statistical maximum a posteriori (MAP) criterion. Due to computational concerns, however, sub-optimal versions of the algorithms are devised through simplifying approximations in the model. Since model parameters are needed for the segmentation algorithms, a new parameter estimation technique is developed for estimating the parameters in a GD. Finally, a number of examples are presented which show the usefulness of the Gibbsian model and the effectiveness of the segmentation algorithms and the parameter estimation procedures.	approximation;computational economics;dynamic programming;estimated;estimation theory;nonclinical distribution;population parameter;version;algorithm;biologic segmentation	Haluk Derin;Howard Elliott	1987	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1987.4767871	image restoration;data modeling;computer vision;mathematical logic;application software;random field;systems modeling;image processing;boltzmann distribution;computer science;noise measurement;processing;machine learning;dynamic programming;pattern recognition;lattice;mathematical model;image segmentation;texture;estimation theory;scale-space segmentation;computational model;segmentation;pixel;statistics;robustness	Vision	51.609571824069896	-69.0654777332587	148503
3405d5240501ca1cb5df285c964dcaa00d4ec5bb	automatic somatic cell operating process for nuclear transplantation	visual servo control;texture;nuclear transplantation;image segmentation;somatic cells;visual servoing cellular biophysics image segmentation medical image processing texture;path planning;visual servo control automatic somatic cell operating process nuclear transplantation cloning texture image segmentation cell morpha injection pipette;automatic operating process;obstacle avoidance;visual servo nuclear transplantation automatic operating process texture image segmentation;medical image processing;visual servo;success rate;servomotors;image segmentation servomotors robot kinematics;texture image segmentation;visual servoing;template matching;cellular biophysics;robot kinematics	Somatic cell operating, a key procedure of nuclear transplantation, is a process of picking up an appropriate donor cell. Improving its degree of automation and reducing operating time is very crucial for enhancing success rate of cloning. This paper presents an automatic somatic cell operating process for nuclear transplantation. Firstly, somatic cells are localized employing texture image segmentation and then the most active cell is selected for operation by cell morpha. Secondly, injection pipette is positioned utilizing template matching. Afterwards path planning for injection pipette to pick the selected cell is conducted considering obstacle avoidance, preventing other cells from sticking the pipette. Finally the pipette moves to the target cell along the path automatically under the guidance of visual servo control. Validity and efficiency of this operating process are proved by a dozen time of experiment.	image segmentation;motion planning;obstacle avoidance;servo;template matching;visual servoing	Yiliao Wang;Xin Zhao;Qili Zhao;Mingzhu Sun;Guizhang Lu	2012	2012 7th IEEE International Conference on Nano/Micro Engineered and Molecular Systems (NEMS)	10.1109/NEMS.2012.6196793	somatic cell;template matching;motion planning;obstacle avoidance;image segmentation;texture;visual servoing;servomotor;robot kinematics	Robotics	39.65365609069838	-71.43975968412538	148647
a0f21763c30d2e5f201361c943be0c438a14a00b	assessment of tear film surface quality using dynamic-area high-speed videokeratoscopy	medical image processing biomedical optical imaging contact lenses endoscopes eye high speed optical techniques image classification light interference;eye;classification algorithm;tear film;image processing;signal separation;optical refraction;contact lenses tear film surface quality assessment dynamic area high speed videokeratoscopy noninvasive assessment corneal area eyelashes region based classification interference classification algorithm anterior eye condition signal separation;high speed optical techniques;image classification;interference;surface topography;corneal area;anterior eye condition;high speed videokeratoscopy hsv;tear film cornea high speed videokeratoscopy hsv image processing;algorithms analysis of variance blinking contact lenses cornea corneal topography eyelashes humans image interpretation computer assisted image processing computer assisted tears;medical image processing;endoscopes;lenses;optical films interference lenses optical refraction surface topography cornea high speed optical techniques eyelashes image processing australia;interference classification algorithm;dynamic area high speed videokeratoscopy;eyelashes;region based classification;biomedical optical imaging;contact lenses;tear film surface quality assessment;high speed;cornea;noninvasive assessment;light interference;optical films;australia	A new method for noninvasive assessment of tear film surface quality (TFSQ) is proposed. The method is based on high-speed videokeratoscopy in which the corneal area for the analysis is dynamically estimated in a manner that removes videokeratoscopy interference from the shadows of eyelashes but not that related to the poor quality of the precorneal tear film that is of interest. The separation between the two types of seemingly similar videokeratoscopy interference is achieved by region-based classification in which the overall noise is first separated from the useful signal (unaltered videokeratoscopy pattern), followed by a dedicated interference classification algorithm that distinguishes between the two considered interferences. The proposed technique provides a much wider corneal area for the analysis of TFSQ than the previously reported techniques. A preliminary study with the proposed technique, carried out for a range of anterior eye conditions, showed an effective behavior in terms of noise to signal separation, interference classification, as well as consistent TFSQ results. Subsequently, the method proved to be able to not only discriminate between the bare eye and the lens on eye conditions but also to have the potential to discriminate between the two types of contact lenses.	eyelash;interference (communication);lens (device);algorithm	David Alonso-Caneiro;D. Robert Iskander;Michael J. Collins	2009	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2008.2011993	computer vision;image processing;interference;optics;engineering drawing;physics;quantum mechanics	Visualization	41.46739169709709	-74.14045620081431	148822
325242b666e408bd5ac9bc4de35f47f3447d42ff	a novel geometrical approach for a rapid estimation of the hardi signal in diffusion mri		In this paper, we address the problem of the diffusion signal reconstruction from a limited number of samples. The HARDI (High Angular Resolution Diffusion Imaging) technique was proposed as an alternative to resolve the problems of crossing fibers in the case of Diffusion Tensor Imaging (DTI). However, it requires a long scanning time for the acquisition of the Diffusion Weighted (DW) images. This fact makes hard the clinical applications. We propose here a novel geometrical approach to accurately estimate the HARDI signal from a few number of DW images. The missing diffusion data are obtained according to their neighborhood from a reduced set of diffusion directions on the sphere of the q-space. The experimentations are performed on both synthetic data and many digital phantoms simulating crossing fibers on the brain tissues. The obtained results show the accuracy of the reconstruction of the Fiber Orientation Distribution (FOD) function from the estimated diffusion signal.	dreamwidth;experiment;gradient;signal reconstruction;simulation;synthetic data	Ines Ben Alaya;Majdi Jribi;Faouzi Ghorbel;Tarek Kraiem	2016		10.1007/978-3-319-33618-3_26	computer vision;artificial intelligence;angular resolution;computer science;diffusion mri;pattern recognition;delaunay triangulation;signal reconstruction	Vision	48.4866947791596	-79.76908313717584	148926
f57445f502ad3c85f78805075960c85f34c8a3f5	a markov random field model-based approach to natural image matting	color space;model based approach;natural images;simulated annealing;markov random field;map estimation;maximum a posteriori;user interaction;blue screen matting;natural image matting	This paper proposes a Markov Random Field (MRF) model-based approach to natural image matting with complex scenes. After the trimap for matting is given manually, the unknown region is roughly segmented into several joint sub-regions. In each sub-region, we partition the colors of neighboring background or foreground pixels into several clusters in RGB color space and assign matting label to each unknown pixel. All the labels are modelled as an MRF and the matting problem is then formulated as a maximum a posteriori (MAP) estimation problem. Simulated annealing is used to find the optimal MAP estimation. The better results can be obtained under the same user-interactions when images are complex. Results of natural image matting experiments performed on complex images using this approach are shown and compared in this paper.	color space;estimation theory;experiment;interaction;markov chain;markov random field;pixel;simulated annealing	Sheng-You Lin;Jiao-Ying Shi	2007	Journal of Computer Science and Technology	10.1007/s11390-007-9022-x	computer vision;simulated annealing;computer science;maximum a posteriori estimation;machine learning;pattern recognition;color space;computer graphics (images)	Vision	49.82686379225651	-69.09585670160364	148935
5b5231fb0bec7bd463c27dcad6f0eab0ff4ee8e3	adaptive optimal shape prior for easy interactive object segmentation	histograms;image segmentation;easy interactive object segmentation segmentation refinement shape consistency evaluation shape prior shape registration shape space;shape recognition image registration image segmentation object tracking optimisation;object segmentation;shape;shape image segmentation object segmentation histograms robustness image edge detection;adaptive optimal shape prior interactive object segmentation nonrigid shape registration shape consistency evaluation system;image edge detection;shape space easy interactive object segmentation segmentation refinement shape consistency evaluation shape prior shape registration;robustness;image registration image segmentation object tracking optimisation shape recognition;shape consistency evaluation system adaptive optimal shape prior interactive object segmentation nonrigid shape registration	For interactive segmentation approaches, object segmentation in complicated background is cumbersome, and usually needs tedious interactions to refine the incomplete segmentations . In this paper, an adaptive optimal shape prior is proposed for easy interactive object segmentation. Different from the traditional shape priors which only provide loose constraint, our adaptive shape prior gives more accurate and individualized constraint by exploiting the shape information of incomplete segmentation. Moreover, by combining the non-rigid shape registration and a local shape consistency evaluation system presented in this paper, such adaptive optimal shape prior could be achieved automatically. Both of these contributions greatly lighten the burden on users and make interactive segmentation much easier. The comparison experiments on the newly-built TypShape dataset with the related algorithms have demonstrated good performance of the proposed algorithm.	algorithm;experiment;interaction;norm (social);refinement (computing);sensor;shape context;time complexity;vii	Kunqian Li;Wenbing Tao	2015	IEEE Transactions on Multimedia	10.1109/TMM.2015.2433795	active shape model;computer vision;shape;computer science;machine learning;segmentation-based object categorization;pattern recognition;histogram;image segmentation;programming language;scale-space segmentation;statistics;robustness	Vision	46.27854092323206	-70.18060284478926	149126
32617326e5ed07dd3931148ad6d2fe71a0b6df58	a new approach of image enhancement based on multi-scale morphological reconstruction	image enhancement image reconstruction degradation image processing tree data structures protection histograms image analysis hybrid intelligent systems educational institutions;mathematical morphology;pediatrics;grey value;method of image;maximum tree;minimum tree structure;morphological reconstruction;trees mathematics;data mining;nonidempotent connected operator;image enhancement;morphology;contrast;image reconstruction;pixel;tree structure;enhancemente;minimum tree structure image enhancement method multiscale morphological reconstruction nonidempotent connected operator grey value;maximum tree enhancemente contrast connected operator morphological reconstruction;trees mathematics image enhancement image reconstruction mathematical morphology;image enhancement method;connected operator;multiscale morphological reconstruction;noise	In this paper, a new method of image enhancement, which only depends on the difference of grey values of adjacency flat zones of an image, is proposed according to the theory of morphological reconstruction. First of all, a non-idempotent connected operator which can be the criterion for multi-scale enhancement is defined and the criterion is a key to enhancement of an image. Subsequently, the grey values in same the simple connected zones of an image are enhanced overall so the way can keep the immovability of the image contour. At last the connected zones of an image and their relations are described and the way of the multi-scale morphological reconstruction enhancement for an image is implemented by using maximum tree or minimum tree structure. Lots of experimental results indicate that this method can not only stretch the contrast according to the character of gray level of connected zones of an image, but also protect the relations of gray level of connected zones. It’s an effective enhancement method of an image.	eisenstein's criterion;grayscale;idempotence;image editing;tree structure	Shu Yang;Cairong Wang;Liguo Deng	2009	2009 Ninth International Conference on Hybrid Intelligent Systems	10.1109/HIS.2009.30	computer vision;feature detection;binary image;machine learning;pattern recognition;mathematics	Vision	43.750449310968854	-67.2332347051505	149442
cfd1d21ee7c92b15243d87897d2bfd78d763240d	direct image reconstruction for electrical capacitance tomography using shortcut d-bar method		In this paper, the shortcut D-bar method is introduced to electrical capacitance tomography to directly reconstruct the permittivity distribution within the region of interest. The method reveals the analytic relationship between the permittivity distribution and the Dirichlet-to-Neumann (DN) map. The DN map can be directly estimated with measured capacitances. However, the direct estimate of the DN map is inaccurate in the cases of small electrode numbers. To better adapt the method to real applications of small electrode numbers, the original shortcut D-bar method is modified by splitting estimation of the DN map into the analytical expression of a homogeneous background and the estimation of the other variation part. Comparisons are made among the original, modified shortcut D-bar methods, and the classical Landweber iteration method through simulation and experiment, respectively. Results confirm the improved tomographic imaging quality by using the modified shortcut D-bar method.		Jiayu Zhao;Lijun Xu;Zhang Cao	2019	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2018.2851839	landweber iteration;iterative reconstruction;electronic engineering;electrical capacitance tomography;region of interest;permittivity;homogeneous;mathematics;tomographic reconstruction;capacitance	Visualization	52.746401396441264	-77.11983428832437	149531
211c08f3b3cc52ba3db2f3cc5ba33a2548bcd99a	human perception based color image quantization	color vector;local k-means algorithm;human perception;spatial color distribution;color image quantization;representative color;human color perception property;distinctiveness map;human visual system;weighted color vector;color image;homogeneity map;feature extraction;image reconstruction;color perception;k means algorithm;vectors;statistical analysis;image segmentation;cost function	We present a new algorithm for color image quantization based on human color perception properties. We construct two kinds of map by analyzing the spatial color distributions to take account of the human visual system: homogeneity map (H-map) and distinctiveness map (D-map). Then, we assign weight value to all color vectors by combining these maps to consider two factors at the same time. To extract representative colors, we define a new cost function and use the LKMA (local k-means algorithm) with weighted color vectors. In this stage, we utilize an incremental splitting scheme with a penalty term to determine optimal number of clusters adaptively. The experimental results show that the proposed algorithm reproduces an image preserving significant local features while removing unimportant details of an original image from the viewpoint of human.	algorithm;cluster analysis;color image;color quantization;color space;color vision;human visual system model;image segmentation;k-means clustering;loss function;map;pattern recognition;pixel;quantization (image processing)	Kuk-Jin Yoon;In-So Kweon	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334255	iterative reconstruction;color histogram;computer vision;color quantization;color depth;feature extraction;computer science;machine learning;pattern recognition;mathematics;color vision;image segmentation;human visual system model;perception;map coloring;k-means clustering	Robotics	44.63166349741928	-66.85438494847712	149664
791d0f011282a2059609c90b91f73af6cb3c7e47	region based segmentation in presence of intensity inhomogeneity using legendre polynomials	active contour;image segmentation;segmentation active contour level set;level set;segmentation;polynomials;nonhomogeneous media;image edge detection;lighting;polynomials image segmentation level set lighting nonhomogeneous media image edge detection	We propose a novel region based segmentation method capable of segmenting objects in presence of significant intensity variation. Current solutions use some form of local processing to tackle intra-region inhomogeneity, which makes such methods susceptible to local minima. In this letter, we present a framework which generalizes the traditional Chan-Vese algorithm. In contrast to existing local techniques, we represent the illumination of the regions of interest in a lower dimensional subspace using a set of pre-specified basis functions. This representation enables us to accommodate heterogeneous objects, even in presence of noise. We compare our results with three state of the art techniques on a dataset focusing on biological/biomedical images with tubular or filamentous structures. Quantitatively, we achieve a 44% increase in performance, which demonstrates efficacy of the method.	algorithm;approximation;basis function;black and burst;legendre polynomials;maxima and minima;polynomial;region of interest;spline (mathematics);wavelet	Suvadip Mukherjee;Scott T. Acton	2015	IEEE Signal Processing Letters	10.1109/LSP.2014.2346538	image texture;computer vision;mathematical optimization;range segmentation;computer science;level set;segmentation-based object categorization;active contour model;lighting;mathematics;geometry;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;segmentation;polynomial	Vision	52.67262285253509	-70.04360318938694	149772
caad843921852c64dbbbf51ccb8909e460810045	segmentation of scoliotic spine silhouettes from enhanced biplanar x-rays using a prior knowledge bayesian framework	databases;bayesian framework;image noise reduction;belief networks;morphological distribution;learning process;bayesian framework scoliosis spine silhouettes biplanar xrays prior knowledge segmentation;image segmentation;x ray imaging;anterior spinal body;bone structures;automatic learning process;bayesian methods;hessian image matrix;spine silhouettes;vertebral contour segmentation;silhouette segmentation;neurophysiology belief networks bone diagnostic radiography image segmentation medical image processing;prior knowledge;x rays bayesian methods image segmentation geometry spine data mining adaptive filters bones noise reduction training data;scoliotic spinex;prior knowledge segmentation;adaptive nonlinear enhancement filter;shape;medical image processing;a prior knowledge bayesian framework;pixel;bone;image response map;vertebral contour segmentation scoliotic spinex silhouette segmentation x ray imaging a prior knowledge bayesian framework anterior spinal body adaptive nonlinear enhancement filter bone structures image noise reduction morphological distribution automatic learning process hessian image matrix image response map;biplanar xrays;neurophysiology;probabilistic logic;scoliosis;biplanar x rays;diagnostic radiography;qualitative evaluation;x rays	In this paper, we propose a novel segmentation method which takes into account the variable appearance and geometry of a scoliotic spine (rotation, wedging) from X-ray images of poor quality in order to automatically isolate and extract the silhouettes of the anterior spinal body. An adaptive non-linear enhancement filter is first presented to enhance bone structures and reduce image noise. By incorporating prior anatomical information through a Bayesian formulation of the morphological distribution, a multiscale spine segmentation framework is then proposed for scoliotic patients. The likelihood of the model is computed based on an automatic learning process derived from labeled training data, while the Hessian image matrix is exploited to create an image-response map by attributing at each pixel the likeliness presence of a structure of interest. A qualitative evaluation of the vertebral contour segmentations obtained from the proposed method gave promising results while the quantitative comparison to manual identification yields an accuracy of 1.5±0.6mmbased on the localization of the spine boundaries by a radiology expert.	dendritic spine;hessian;image noise;image response;nonlinear system;pixel;radiology	Samuel Kadoury;Farida Cheriet;Hubert Labelle	2009	2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2009.5193088	computer vision;bayesian probability;shape;computer science;machine learning;pattern recognition;mathematics;image segmentation;probabilistic logic;neurophysiology;pixel	Vision	43.140192443779725	-77.0480467236141	149776
44311bc35a8879ddfa33c315e14b503b8e292050	voxelwise atlas rating for computer assisted diagnosis: application to congenital heart diseases of the great arteries	biological patents;biomedical journals;computer aided diagnosis;text mining;europe pubmed central;citation search;atlases;voxel rating;citation networks;image synthesis;research articles;abstracts;congenital heart diseases;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Atlas-based analysis methods rely on the morphological similarity between the atlas and target images, and on the availability of labelled images. Problems can arise when the deformations introduced by pathologies affect the similarity between the atlas and a patient's image. The aim of this work is to exploit the morphological dissimilarities between atlas databases and pathological images to diagnose the underlying clinical condition, while avoiding the dependence on labelled images. We propose a voxelwise atlas rating approach (VoxAR) relying on multiple atlas databases, each representing a particular condition. Using a local image similarity measure to assess the morphological similarity between the atlas and target images, a rating map displaying for each voxel the condition of the atlases most similar to the target is defined. The final diagnosis is established by assigning the condition of the database the most represented in the rating map. We applied the method to diagnose three different conditions associated with dextro-transposition of the great arteries, a congenital heart disease. The proposed approach outperforms other state-of-the-art methods using annotated images, with an accuracy of 97.3% when evaluated on a set of 60 whole heart MR images containing healthy and pathological subjects using cross validation.	arterial system;atlases;computer assisted diagnosis;congenital heart disease;cross infection;cross-validation (statistics);d - transposition of the great vessels;database;discordant ventriculoarterial connection;heart diseases;heart valve disease;musculoskeletal diseases;patients;similarity measure;transposition of great vessels;transposition table;voxel	Maria A. Zuluaga;Ninon Burgos;Alex F. Mendelson;Andrew Mayall Taylor;Sébastien Ourselin	2015		10.1016/j.media.2015.09.001	text mining;medical research;computer science;bioinformatics;data science;data mining	Vision	40.1142257244877	-79.49013754425837	149846
54cb348780be478677f900a365054f38528c46dd	a simple push-pull algorithm for blue-noise sampling	journal_article;optimization correlation harmonic analysis radio frequency clustering algorithms density measurement noise measurement	We describe a simple push-pull optimization (PPO) algorithm for blue-noise sampling by enforcing spatial constraints on given point sets. Constraints can be a minimum distance between samples, a maximum distance between an arbitrary point and the nearest sample, and a maximum deviation of a sample's capacity (area of Voronoi cell) from the mean capacity. All of these constraints are based on the topology emerging from Delaunay triangulation, and they can be combined for improved sampling quality and efficiency. In addition, our algorithm offers flexibility for trading-off between different targets, such as noise and aliasing. We present several applications of the proposed algorithm, including anti-aliasing, stippling, and non-obtuse remeshing. Our experimental results illustrate the efficiency and the robustness of the proposed approach. Moreover, we demonstrate that our remeshing quality is superior to the current state-of-the-art approaches.	alias;adaptive sampling;algorithm;aliasing;amino acids;anatomy, regional;cell (microprocessor);colors of noise;computer graphics (computer science);delaunay triangulation;double-balloon enteroscopy;general-purpose modeling;gibbs sampling;image noise;index;iterative method;mathematical optimization;rendering (computer graphics);sampling (signal processing);sampling - surgical action;spatial anti-aliasing;voronoi diagram	Abdalla G. M. Ahmed;Jianwei Guo;Dong-Ming Yan;Jean-Yves Franceschia;Xiaopeng Zhang;Oliver Deussen	2017	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2016.2641963	mathematical optimization;computer science;mathematics;statistics	Visualization	53.08268283617498	-73.84760074534489	149913
d200e18321c62d8d6dc0d194a49837e1c12a9229	automatic construction of 3-d building model from airborne lidar data through 2-d snake algorithm	topology light detection and ranging lidar snake algorithm;buildings topology laser radar solid modeling atmospheric modeling data models heuristic algorithms;airborne lidar data polynomial time 2d topology refinement multiple geometric constraint graph reduction technique dynamic programming method 1d topology global minima computing energy function building topology adjustment building topology extraction tree separation nonterrain object identification 3d building construction model object tracking surface reconstruction object segmentation computer vision problem remote sensing 2d planar snake graph algorithm;polynomials airborne radar buildings structures dynamic programming graph theory optical radar	The snake algorithm has been proposed to solve many remote sensing and computer vision problems such as object segmentation, surface reconstruction, and object tracking. This paper introduces a framework for 3-D building model construction from LIDAR data based on the snake algorithm. It consists of nonterrain object identification, building and tree separation, building topology extraction, and adjustment by the snake algorithm. The challenging task in applying the snake algorithm to building topology adjustment is to find the global minima of energy functions derived for 2-D building topology. The traditional snake algorithm uses dynamic programming for computing the global minima of energy functions which is limited to snake problems with 1-D topology (i.e., a contour) and cannot handle problems with 2-D topology. In this paper, we have extended the dynamic programming method to address the snake problems with a 2-D planar topology using a novel graph reduction technique. Given a planar snake, a set of reduction operations is defined and used to simplify the graph of the planar snake into a set of isolated vertices while retaining the minimal energy of the graph. Another challenging task for 3-D building model reconstruction is how to enforce different kinds of geometric constraints during building topology refinement. This framework proposed two energy functions, deviation and direction energy functions, to enforce multiple geometric constraints on 2-D topology refinement naturally and efficiently. To examine the effectiveness of the framework, the framework has been applied on different data sets to construct 3-D building models from airborne LIDAR data. The results demonstrate that the proposed snake algorithm successfully found the global optima in polynomial time for all of the building topologies and generated satisfactory 3-D models for most of the buildings in the study areas.	airborne ranger;algorithm;computer vision;dynamic programming;graph reduction;maxima and minima;refinement (computing);time complexity	Jianhua Yan;Keqi Zhang;Chengcui Zhang;Shu-Ching Chen;Giri Narasimhan	2015	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2014.2312393	computer vision;machine learning;mathematics;remote sensing	Vision	47.98922808223487	-67.84089017883218	149995
977539bb59cd1abc48f8fbda7ad9316de0715e67	unsupervised texture segmentation using multichannel decomposition and hidden markov models	texture;canal multiple;image segmentation;image processing;modelo markov;hidden markov models image segmentation image edge detection feature extraction filtering artificial intelligence merging distributed computing computer vision;hidden markov model;texture segmentation;segmentation;traitement image;experimental result;image texture;multiple channel;feature vector;markov model;hidden markov models;image segmentation image texture hidden markov models;resultado experimental;directional macromasks unsupervised texture segmentation multichannel decomposition hidden markov models feature map 4 d feature vectors model parameters laws micromasks coarse segmentation fine segmentation discrimination information postprocessing stage multiscale majority filtering pipeline parallel implementation;parallel implementation;modele markov;resultat experimental;segmentacion;textures	In this paper, we describe an automatic unsupervised texture segmentation scheme using hidden Markov models (HMMs). First, the feature map of the image is formed using Laws' micromasks and directional macromasks. Each pixel in the feature map is represented by a sequence of 4-D feature vectors. The feature sequences belonging to the same texture are modeled as an HMM. Thus, if there are M different textures present in an image, there are M distinct HMMs to be found and trained. Consequently, the unsupervised texture segmentation problem becomes an HMM-based problem, where the appropriate number of HMMs, the associated model parameters, and the discrimination among the HMMs become the foci of our scheme. A two-stage segmentation procedure is used. First, coarse segmentation is used to obtain the approximate number of HMMs and their associated model parameters. Then, fine segmentation is used to accurately estimate the number of HMMs and the model parameters. In these two stages, the critical task of merging the similar HMMs is accomplished by comparing the discrimination information (DI) between the two HMMs against a threshold computed from the distribution of all DI's. A postprocessing stage of multiscale majority filtering is used to further enhance the segmented result. The proposed scheme is highly suitable for pipeline/parallel implementation. Detailed experimental results are reported. These results indicate that the present scheme compares favorably with respect to other successful schemes reported in the literature.		Jia-Lin Chen;Amlan Kundu	1995	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.382495	image texture;computer vision;feature vector;computer science;machine learning;pattern recognition;image segmentation;markov model;texture;segmentation;hidden markov model	Vision	47.757490569358076	-67.7657184630564	150055
222ef99108c9ffb8f14f3a40abc770aee0535731	analytic heuristics for a fast dsc-mri	brain;compressed sensing;reconstruction algorithms;chemical compounds;hemodynamics;magnetic resonance imaging;image quality;wavelets	In this paper we propose a deterministic approach for the reconstruction of Dynamic Susceptibility Contrast magnetic resonance imaging data and compare it with the compressed sensing solution existing in the literature for the same problem. Our study is based on the mathematical analysis of the problem, which is computationally intractable because of its non polynomial complexity, but suggests simple heuristics that perform quite well. We give results on real images and on artificial phantoms with added noise.		Marco Virgulin;Marco Castellaro;Fabio Marcuzzi	2014		10.1117/12.2042835	image quality;wavelet;computer vision;simulation;telecommunications;computer science;theoretical computer science;magnetic resonance imaging;hemodynamics;optics;compressed sensing	AI	53.482639158230505	-77.81170619453695	150269
91659a94ea92826bf5c42183e7a9019ca987dd4f	iteratively parsing contour fragments for object detection	contour fragment;edge linking;shape feature;object detection	In this paper, we explicitly consider the effect of contour fragments on the object detection performance and propose a new approach for linking edges into contour fragments. Our main observation is that the covering condition describing how contour fragments cover objects of interest is the critical factor affecting the detection accuracy. We utilize the general min-cover framework to explain an edge map. During the optimization procedure, we sequentially select best contour fragments in each connected component of the edge map for obtaining locally optimal contour fragments. Furthermore, the sequential selection of best contour fragments is reduced to an iterative parsing procedure. We conduct experiments on the ETHZ and INRIA horse datasets and compare the proposed method with other typical methods of generating contour fragments. Experimental results illustrate that our method achieves a proper covering condition and produces contour fragments that lead to better object detection performance. Besides, the proposed method is easy to compute, leading to a variety of potential real-time applications.	connected component (graph theory);contour line;experiment;iteration;iterative method;local optimum;mathematical optimization;object detection;parsing;preprocessor;real-time clock;real-time transcription;vertex cover	Xiao Huang;Yuanqi Su;Yuehu Liu	2016	Neurocomputing	10.1016/j.neucom.2015.10.099	computer vision;computer science;pattern recognition	Vision	45.97020909562059	-68.73459216737365	150289
b141a05869126b4a46d7c07df6018ec3a7c1541b	segmentation of cellular structures in actin tagged fluorescence confocal microscopy images	image segmentation level set microscopy active contours fluorescence mathematical model equations;biology computing;spatial context;cellular structure;cell segmentation;cell culture;active contour;fluorescence;image segmentation;cell morphology;image segmentation biology computing cellular biophysics image reconstruction;level set;microscopy;active contours;software engineering;geodesic distance;civil engineering;image reconstruction;topological constraints cell segmentation active contour geodesic distance confocal microscopy;mathematical model;quantitative analysis;confocal microscopy;cellular biophysics;topological constraints;pnt2 culture cellular structure segmentation actin tagged fluorescence confocal microscopy images cellular feature reconstruction cell nuclei actin organisation cell morphology stressed cell cultures controlled cell cultures derived level set evolution pde geodesic distance maps inclusion exclusion topological constraints human prostate cell culture	The paper reports on a novel method for reconstruction of cellular features including cell nuclei and cellular boundaries from actin tagged fluorescence confocal microscopy images. Such reconstruction can provide spatial context for subsequent quantitative analysis of changes to actin organisation and cell morphology in both controlled and stressed cell cultures. The proposed method is fully automatic and is formulated within active contour multiphase level set framework. The derived level set evolution PDEs combine previously proposed curvature and advection flows with propagation flow defined by specially designed set of geodesic distance maps. Additionally the proposed PDEs include additional components to impose known inclusion/exclusion topological constraints between cellular structures. The paper gives an overview of the proposed methodology as well as reports on initial results obtained for monolayer of human prostate cells (PNT2) culture visualised using acting tagged fluorescence confocal microscopy.	active contour model;cell signaling;distance (graph theory);map;mathematical morphology;software propagation;volume rendering	Bogdan J. Matuszewski;Mark F. Murphy;David R. Burton;Tom Marchant;Christopher J. Moore;Aymeric Histace;Frédéric Precioso	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116316	iterative reconstruction;computer vision;geodesic;fluorescence;computer science;quantitative analysis;microscopy;level set;spatial contextual awareness;confocal laser scanning microscopy;mathematical model;active contour model;mathematics;image segmentation;bacterial cellular morphologies;cell culture	Vision	40.68883670687735	-73.94396148150366	150441
2b904e53c8e7100d59fb20e1adcfa34044776871	image segmentation by geodesic voting. application to the extraction of tree structures from confocal microscope images	image segmentation;image resolution;fast marching method;least squares approximation;trees mathematics;data mining;microscope images;blood vessel;communication conference;image segmentation voting tree data structures microscopy geophysics computing pixel equations blood vessels biomedical imaging data mining;geodesic voting;three dimensional displays;medical image processing;trees mathematics blood vessels image resolution image segmentation medical image processing;pixel;tree structure;mathematical model;a priori information;transport equation image segmentation geodesic voting tree structures confocal microscope images cerebral blood vessels geodesic density;segmenting tree structures;transport equation;blood vessels	This paper presents a new method to segment thin tree structures, such as extensions of microglia and cardiac or cerebral blood vessels. The Fast Marching method allows the segmentation of tree structures from a single point chosen by the user when a priori information is available about the length of the tree. In our case, no a priori information about the length of the tree structure to extract is available. We propose here to compute geodesics from a set of points scattered in the image. The targeted structure corresponds to image points with a high geodesic density. To compute the geodesic density we propose two methods. The first method defines the geodesic density of pixels in the image as the number of geodesics that cross this pixel. The second method consists in solving the transport equation with a velocity computed from the gradient of the distance map. In this method, the geodesic density is computed by integrating in short time the solution of the transport equation. To our knowledge this is the first time that geodesic voting is introduced. Numerical results from confocal microscope images are presented and show the interest of our approach.	distance transform;fast marching method;gradient;image segmentation;pixel;tree structure;velocity (software development)	Youssef Rouchdy;Laurent D. Cohen	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761763	computer vision;mathematical optimization;image resolution;computer science;convection–diffusion equation;fast marching method;mathematical model;mathematics;geometry;image segmentation;tree structure;least squares;pixel	Vision	44.625977493025886	-75.55431778231714	150521
7a7cb6b94cbb1b859e4a8e62988d48e7167d2c7f	glcm and fuzzy clustering for ocean features classification	pattern clustering;oceans;image segmentation;synthetic aperture radar feature extraction image segmentation matrix algebra oceanography pattern clustering;matrix algebra;computer vision;fuzzy clustering;ocean feature glcm fuzzy clustering;sea surface;marine vehicles;fuzzy c means clustering ocean features classification synthetic aperture radar technology oceanography sar images segmentation gray level co occurrence matrix;internal waves;oceans statistics image segmentation pixel sea surface marine vehicles surface acoustic waves computer vision machine vision man machine systems;machine vision;feature extraction;pixel;sar image;statistics;gray level co occurrence matrix;fuzzy c means clustering;glcm;surface acoustic waves;man machine systems;ocean feature;oceanography;synthetic aperture radar	since Seasat lunched in 1978, much understanding has been gained on the potential of synthetic aperture radar (SAR) technology in oceanography. In this paper, the ocean features, i.e., internal waves, ocean fronts, present in SAR images are discussed. A new method for segmentation of SAR images is presented based on statistics of gray level co-occurrence matrix (GLCM) and the fuzzy C-Means clustering. The experimental results demonstrate its utility in the classification of various ocean features.	aperture (software);cluster analysis;co-occurrence matrix;document-term matrix;fuzzy clustering;grayscale;synthetic intelligence	Ronghua Tao;Jie Chen;Biao Chen;Cuihua Liu	2010	2010 International Conference on Machine Vision and Human-machine Interface	10.1109/MVHI.2010.29	computer vision;synthetic aperture radar;machine vision;fuzzy clustering;feature extraction;internal wave;computer science;machine learning;image segmentation;pixel	Robotics	42.478660540089514	-69.90082870967042	150735
8544ac653a17ab0b649d90108ec117468da17f98	automatic directional analysis of cell fluorescence images and morphological modeling of microfilaments	microfilaments;nuclei;automatic analysis;morphological modeling	Cytoskeleton and nucleus are two important anatomic components in eukaryotic cells. Cell fluorescence images are employed to study their realignment and deformation during cell extrusion. Quantitative analysis and modeling of cell orientation are investigated in this paper. For orientation measurement, alignment orientation of microfilaments is calculated using structure tensor method. Nuclei is segmented and fitted to ellipses in nuclei images. Based on the fitted ellipse, orientation and aspect ratio of each nucleus are computed. A morphological model is proposed to describe the movement of microfilaments quantitatively. The parameters of the model are determined by in-plane stresses obtained by numerical simulation. The proposed automatic orientation measurement algorithms can help to analyze the relationship between cell orientation and stress qualitatively. The proposed morphological model is the first model to quantitatively describe the relationship of microfilament movement with stress. Experimental results show that cell and nucleus tend to align along in-plane maximum shear stress and the proposed morphological model is a reasonable model for cell movement. The modeling of cell behavior under different stress can facilitate biomedical research such as tissue engineering and cancer analysis. Graphical abstractᅟ		Yue Zhou;Huiqi Li;Wanjun Zhang;Jiayi Xu;Xiaojun Li;Baohua Ji	2018	Medical & Biological Engineering & Computing	10.1007/s11517-018-1871-7	microfilament;computer vision;nucleus;cytoskeleton;deformation (mechanics);cell;ellipse;structure tensor;mathematics;artificial intelligence;shear stress	Robotics	44.88482442196907	-80.16813831172128	150750
e99c29409e5cc17b0e4ee5c8ae74e8985780573a	automatic 3d segmentation of the liver from computed tomography images, a discrete deformable model approach	treatment planning;automatic 3d liver segmentation;3d segmentation;g740 computer vision;snakes;liver;image segmentation;computed tomography;automatic segmentation;3d deformable models;gold standard;adaptive remeshing;discrete deformable model;three dimensional;disease diagnosis;medical image processing;stereo image processing;computerised tomography;diseases;surface;organ transplantation;image segmentation computed tomography deformable models liver diseases biomedical engineering medical treatment cancer biomedical imaging medical diagnostic imaging physics;computed tomography images;ct images;image slice;deformable model;adaptive remeshing automatic 3d liver segmentation computed tomography images discrete deformable model disease diagnosis organ transplantation treatment planning image slice;adaptive remeshing 3d segmentation 3d deformable models liver;stereo image processing computerised tomography diseases image segmentation liver medical image processing	Automatic segmentation of the liver has the potential to assist in the diagnosis of disease, preparation for organ transplantation, and possibly assist in treatment planning. This paper presents initial results from work that extends on previous two-dimensional (2D) segmentation methods by implementing full three-dimensional (3D) liver segmentation, using a self-reparameterising discrete deformable model. This method overcomes many of the weaknesses inherent in 2D segmentation techniques, such as the inability to automatically segment separate lobes of the liver in each image slice, and sensitivity to individual-slice noise. Results are presented showing volumetric and overlap comparison of twelve automatically segmented livers with their corresponding manually segmented livers, which were treated as the gold standard for this study	ct scan;consortium;level of detail;tomography	Alun Evans;Tryphon Lambrou;Alf D. Linney;Andrew Todd-Pokropek	2006	2006 9th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2006.345474	three-dimensional space;computer vision;gold standard;segmentation-based object categorization;image segmentation;scale-space segmentation;surface;organ transplantation	Robotics	39.990467401203844	-79.95083739560499	151140
c03c7d186ed5253821098cfb471ac22c44146d52	image segmentation using fuzzy spatial-taxon cut: comparison of two different stage one perception based input models of color (bayesian classifier and fuzzy constraint).				Lauren Barghout	2016		10.2352/ISSN.2470-1173.2016.16.HVEI-121	computer vision;fuzzy classification;machine learning;pattern recognition	Vision	41.47735880764129	-67.55544901442062	151354
629db0cf595805e92ca1088b90730366926efd29	template-free wavelet-based detection of local symmetries	circular harmonic wavelets;wavelet transforms image processing;harmonic analysis junctions wavelet transforms algorithm design and analysis stochastic processes correlation;f test;synthetic imaging template free wavelet based detection rotation invariant way scale invariant way local symmetry measurement circular harmonic wavelet steerable wavelet channel generation electron microscopy imaging biological micrograph;junction detection local symmetries f test circular harmonic wavelets;junction detection;local symmetries	Our goal is to detect and group different kinds of local symmetries in images in a scale- and rotation-invariant way. We propose an efficient wavelet-based method to determine the order of local symmetry at each location. Our algorithm relies on circular harmonic wavelets which are used to generate steerable wavelet channels corresponding to different symmetry orders. To give a measure of local symmetry, we use the F-test to examine the distribution of the energy across different channels. We provide experimental results on synthetic images, biological micrographs, and electron-microscopy images to demonstrate the performance of the algorithm.	algorithm;clinical use template;detection;electron;synthetic intelligence;wavelet;orders - hl7publishingdomain	Zsuzsanna Puspoki;Michael Unser	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2436343	wavelet;f-test;computer vision;mathematical optimization;harmonic wavelet transform;cascade algorithm;mathematics;geometry;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;gabor wavelet;statistics	Vision	40.31560718568605	-74.89682567608709	151371
7585b8719d943126054dbcfc7b1685dbaab5f757	hierarchical mrf of globally consistent localized classifiers for 3d medical image segmentation	discrete optimization;local and global prior;hierarchical markov random field;segmentation;medical image analysis	A suitable object model is crucial in guiding an object segmentation method of three-dimensional medical images to avoid difficulties such as complex object structures, inter-subject variability and ambiguous boundaries between organs. The main challenge is to make the model sufficiently complex to represent a wide range of variations effectively, while maintaining compatibility with the segmentation methodology. To address this problem, we propose a new segmentation method based on a hierarchical Markov random field (H-MRF). The H-MRF is composed of local-level MRFs based on adaptive local priors which model local variations of shape and appearance and a global-level MRF enforcing consistency of the local-level MRFs. The proposed method can successfully model large object variations and weak boundaries and is readily combined with well-established MRF optimization techniques. Furthermore, it works well with limited training data and does not require a complex training model or non-rigid registration. The performance of the proposed method is evaluated for bone and cartilage from knee magnetic resonance (MR) images, the liver from body computed tomography images, and the hippocampus from brain MR images. Both qualitative and quantitative evaluations demonstrate that the proposed method provides robust and accurate segmentation results.	image segmentation;markov random field	Sang Hyun Park;Soochahn Lee;Il Dong Yun;Sang Uk Lee	2013	Pattern Recognition	10.1016/j.patcog.2013.02.014	discrete optimization;computer vision;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation;segmentation	Vision	43.20837821336289	-77.80766231831751	151481
c7a82eed830f5991638352c5d9bacfc923de9107	correlation of the talairach atlas based on the proportional grid system	commissura posterior;medical image correlation;brain;area localization;mri data series;biomedical nmr;objects of interest;correlation methods;biomedical nmr brain medical image processing correlation methods;reference systems;medical image;biomedical imaging medical diagnostic imaging humans magnetic resonance imaging radiography mathematics physics biomedical informatics correlation medical treatment;talairach medical atlas;medical image processing;commissura anterior;proportional grid system;reference system;human brain;mri data series talairach medical atlas human brain proportional grid system medical image correlation commissura anterior commissura posterior reference system area localization objects of interest;grid system	This paper presents a general description of the Talairach (1988) medical atlas of the human brain. We describe the method of correlating medical images based on the commissura anterior and commissura posterior reference system. Then we demonstrate applications of this correlation method. We describe the methods for localizing some areas and objects of interest in MRI data series by using the Talairach medical atlas.		Silvester Czanner;H. Schopp;Robert Boesecke;M. Roth;J. Pross;Reinhold Haux	1997		10.1109/CBMS.1997.596418	computer vision;computer science;nuclear medicine;talairach coordinates;medical physics	HPC	44.18796656194093	-79.73590974154891	151555
0ede8994917c58bbc467e63a33a2b18a297a7a7f	statistical modeling and map estimation for body fat quantification with mri ratio imaging	nuclear magnetic resonance imaging;modelizacion;medical imagery;estimacion;coefficient of variation;imagineria rmn;analisis cuantitativo;small animal imaging;statistical methods;segmentation;cartographie;statistical models;classification;statistical model;fat content;genetics;maximum a posteriori estimate;modelisation;fat quantification;cartografia;estimation;analyse quantitative;partial volume;magnetic resonance imaging;imagineria medica;imagerie medicale;map estimation;quantitative analysis;cartography;ground truth;imagerie rmn;statistical modeling;monte carlo;body fat;correlation coefficient;monte carlo hypothesis testing;kinetics;modeling;metabolic syndrome;hypothesis test	We are developing small animal imaging techniques to characterize the kinetics of lipid accumulation/reduction of fat depots in response to genetic/dietary factors associated with obesity and metabolic syndromes. Recently, we developed an MR ratio imaging technique that approximately yields lipid/ {lipid + water}. In this work, we develop a statistical model for the ratio distribution that explicitly includes a partial volume (PV) fraction of fat and a mixture of a Rician and multiple Gaussians. Monte Carlo hypothesis testing showed that our model was valid over a wide range of coefficient of variation of the denominator distribution (c.v.: 0 - 0.20) and correlation coefficient among the numerator and denominator (p: 0 - 0.95), which cover the typical values that we found in MRI data sets (c.v.: 0.027 - 0.063, p: 0.50 - 0.75). Then a maximum a posteriori (MAP) estimate for the fat percentage per voxel is proposed. Using a digital phantom with many PV voxels, we found that ratio values were not linearly related to PV fat content and that our method accurately described the histogram. In addition, the new method estimated the ground truth within +1.6% vs. +43% for an approach using an uncorrected ratio image, when we simply threshold the ratio image. On the six genetically obese rat data sets, the MAP estimate gave total fat volumes of 279 ± 45mL, values ≃21% smaller than those from the uncorrected ratio images, principally due to the non-linear PV effect. We conclude that our algorithm can increase the accuracy of fat volume quantification even in regions having many PV voxels, e.g. ectopic fat depots.	statistical model	Wilbur C. K. Wong;David H. Johnson;David L. Wilson	2008		10.1117/12.772856	statistical model;magnetic resonance imaging	Vision	52.10882815173235	-75.53672412062045	151577
5dd1d6f5a14a1f9f89398d9cb8965db9dc1ab253	efficient implementation of thelocally constrained watershed transform and seeded region growing	watershed transform;constrained regions.;region growing	The watershed transform and seeded region growing are well known tools for image segmentation. They are members of a class of greedy region growing algorithms that are simple, fast and largely parameter free. The main control over these algorithms come from the selection of the marker image, which defines the number of regions and a starting position for each region.	region growing;watershed (image processing)	Richard Beare	2005		10.1007/1-4020-3443-1_20	parallel computing;computer vision;image segmentation;watershed;region growing;artificial intelligence;computer science	Vision	45.056397298033964	-69.60786379786332	151776
6ee9ad4b4ce1a6f15a8ac5ef78ffa8d77e970ecf	gradient intensity: a new mutual information-based registration method	optimisation;powell algorithm mutual information based registration method pixel intensities image spatial strength gradient intensity histograms scale estimates translation parameters gradient images centroids optimization algorithm;optimization algorithm;image resolution;translation parameters;gradient intensity;scale estimates;optimisation gradient methods image registration image resolution;gradient images centroids;conference paper;pixel intensities;parameter estimation cost function pixel mutual information random variables robustness computational complexity remote monitoring spatial resolution australia;mutual information based registration method;conventional mutual information;powell algorithm;computational complexity;gradient intensity histograms;image registration;pixels;gradient methods;mutual information;optimization;parameter estimation;image spatial strength;keywords algorithms;optimal algorithm;information analysis;spatial information	Conventional mutual information (Ml)-based registration using pixel intensities is time-consuming and ignores spatial information, which can lead to misalignment. We propose a method to overcome these limitation by acquiring initial estimates of transformation parameters. We introduce the concept of 'gradient intensity' as a measure of spatial strength of an image in a given direction. We determine the rotation parameter by maximizing the MI between gradient intensity histograms. Calculation of the gradient intensity MI function is extremely efficient. Our method is designed to be invariant to scale and translation between the images. We then obtain estimates of scale and translation parameters using methods based on the centroids of gradient images. The estimated parameters are used to initialize an optimization algorithm which is designed to converge more quickly than the standard Powell algorithm in close proximity of the minimum. Experiments show that our method significantly improves the performance of the registration task and reduces the overall computational complexity by an order of magnitude.	algorithm;computational complexity theory;converge;experiment;gradient;mathematical optimization;mutual information;pixel;powell's method	Ramtin Shams;Parastoo Sadeghi;Rodney A. Kennedy	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383425	computer vision;mathematical optimization;image resolution;computer science;image registration;pattern recognition;mathematics;spatial analysis;mutual information;estimation theory;data analysis;computational complexity theory;pixel;statistics	Vision	48.69020569828679	-74.87143118843196	151797
abfa90ff56cc93c427d28436d254bc295500b608	minimal paths for tubular structure segmentation with coherence penalty and adaptive anisotropy		The minimal path method has proven to be particularly useful and efficient in tubular structure segmentation applications. In this paper, we propose a new minimal path model associated with a dynamic Riemannian metric embedded with an appearance feature coherence penalty and an adaptive anisotropy enhancement term. The features that characterize the appearance and anisotropy properties of a tubular structure are extracted through the associated orientation score. The proposed the dynamic Riemannian metric is updated in the course of the geodesic distance computation carried out by the efficient single-pass fast marching method. Compared to the state-of-the-art minimal path models, the proposed minimal path model is able to extract the desired tubular structures from a complicated vessel tree structure. In addition, we propose an efficient prior path-based method to search for vessel radius value at each centerline position of the target. Finally, we perform the numerical experiments on both synthetic and real images. The quantitive validation is carried out on retinal vessel images. The results indicate that the proposed model indeed achieves a promising performance.	anisotropy;blood vessel tissue;bone structure of radius;computation;distance (graph theory);embedded system;embedding;experiment;extraction;fast marching method;keyboard shortcut;lifting;mathematical model;numerical analysis;segmentation action;smoothing;software propagation;structure of blood vessel of retina;synthetic intelligence;tree structure;benefit;biologic segmentation	Da Rui Chen;Jiong Zhang;Laurent D. Cohen	2019	IEEE Transactions on Image Processing	10.1109/TIP.2018.2874282	computer vision;feature extraction;tree structure;anisotropy;geodesic;mathematics;fast marching method;real image;path analysis (statistics);artificial intelligence;coherence (physics)	Vision	47.19520938670779	-77.81505005619378	152048
43aca054c49c94718fef45029d832eecdfaf6c0d	adaptive probabilistic thresholding method for accurate breast region segmentation in mammograms	mini mias database adaptive probabilistic thresholding method breast region segmentation accuracy digitized film based mammograms optimal global threshold value gradient information objective function texture information contour growing methods;visual databases image segmentation image texture mammography medical image processing probability;adaptive thresholding;mammogram segmentation;breast image segmentation accuracy x rays databases joints deformable models;adaptive thresholding mammogram segmentation probabilistic thresholding;probabilistic thresholding	Segmentation of the breast region is usually the first step in the analysis of mammograms. Due to the non-uniformity of the background, breast segmentation presents several difficulties especially for film based mammograms. Our experimental results show that 50% of digitized film based mammograms in the mini-MIAS database do not have uniform intensity in the background. For this reason, applying a global thresholding method produces inaccurate results. In addition, finding the optimal global threshold value by only using histogram information requires a reliable objective function that characterizes the statistics of the background and the mammogram regions in the digitized mammograms. A second way to find the boundary of the breast consists in fitting a deformable model, such as snakes, on the mammogram. However, this method has three main shortcomings. First, the model must be initialized near the boundary. Second, using gradient information in the objective function can push the boundary toward the tissues inside the breast rather than the actual boundary. Third, in some mammograms the breast region is occluded by artifacts, such as labels, that have high gradient values on their boundary and cause the deformable model to be fitted on the artifact. To address these problems we propose a probabilistic adaptive thresholding method that uses texture information and its probability to find the most probable threshold values for specific parts of the mammogram. The experimental results on mini-MIAS database show that our proposed method outperforms the state-of-art methods and improves the accuracy at least 37% in comparison with the best results obtained by contour growing methods.	algorithm;binary pattern (image generation);binary splitting;circuit complexity;contour line;equal-loudness contour;gradient descent;loss function;optimization problem;probabilistic automaton;support vector machine;thresholding (image processing)	Hamed Habibi Aghdam;Domenec Puig;Agusti Solanas	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.578	computer vision;machine learning;pattern recognition;balanced histogram thresholding;mathematics;region growing;thresholding	Vision	45.09297854690709	-70.84722306731292	152315
dbb65ec258973334f5be723a82e1166d1f020877	motor neuron morphology estimation for its classification in the drosophila brain	image reconstruction motor neuron morphology estimation classification drosophila brain type specific dendritic arborization patterns synaptic connectivity neuronal function nervous system diverse dendritic morphologies motor circuit genetic tractability green fluorescent protein 3d with confocal microscopy segmentation soma axon dendrites cosegmentation neighboring depths haar features;brain;shape image segmentation three dimensional displays manganese nerve fibers morphology;image segmentation;nervous system;animals biological markers brain cell shape cluster analysis drosophila melanogaster green fluorescent proteins imaging three dimensional larva models biological motor neurons solutions;green fluorescent protein;image classification;nerve fibers;motor neuron;three dimensional;genetics;manganese;morphology;proteins;shape;three dimensional displays;image reconstruction;medical image processing;proteins biomedical optical imaging brain cellular biophysics haar transforms image classification image reconstruction image segmentation medical image processing molecular biophysics neurophysiology;molecular biophysics;neurophysiology;biomedical optical imaging;haar transforms;confocal microscopy;cellular biophysics	Type-specific dendritic arborization patterns dictate synaptic connectivity and are fundamental determinants of neuronal function. We exploit the morphological stereotypy and relative simplicity of the Drosophila nervous system to model the diverse dendritic morphologies of individual motor neurons (MNs) to understand underlying principles of synaptic connectivity in a motor circuit. The genetic tractability of Drosophila allows us to label single MNs with green fluorescent protein (GFP) and serially reconstruct identifiable MNs in 3D with confocal microscopy. Our computational approach aims at the robust segmentation of the MN volumes and the simultaneous partitioning into their compartments, namely the soma, axon and dendrites. We use the idea of co-segmentation, where every image along the z-axis (depth) is clustered using information from ‘neighboring’ depths. As appearance we use a 3D extension of Haar features and for the shape we define an implicit representation of the segmentation domain.	3d modeling;anatomical compartments;apache axis;axis vertebra;axon;cns disorder;cell body of neuron;dendrites;dendritic spine;galaxy morphological classification;green fluorescent proteins;haar wavelet;mathematical morphology;microscopy, confocal;motor neuron disease;motor neurons;nervous system structure;stereotypic movement disorder;subtype (attribute);synaptic package manager;synaptic connectivity;biologic segmentation	Gavriil Tsechpenakis;Ruwan Egoda Gamage;Michael D. Kim;Akira Chiba	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091911	biology;computer vision;neuroscience;green fluorescent protein;morphology;shape;computer science;manganese;mathematics;image segmentation;nervous system;neurophysiology;anatomy;molecular biophysics	Vision	45.06203454563364	-77.10250772401484	152329
6b6e8a1aa95c309afcdfcc4d41e8f80c4b1172d8	hybrid spline-based multimodal registration using a local measure for mutual information	similarity metric;hybrid approach;medical image;mutual information	We introduce a new hybrid approach for spline-based elastic registration of multimodal medical images. The approach uses point landmarks as well as intensity information based on local analytic measures for mutual information. The intensity similarity metrics are computationally efficient and can be optimized independently for each voxel. We have successfully applied our approach to synthetic images, brain phantom images, as well as real multimodal medical images.	algorithmic efficiency;computational human phantom;elastic matching;multimodal interaction;mutual information;spline (mathematics);synthetic intelligence;voxel	Andreas Biesdorf;Stefan Wörz;Hans-Jürgen Kaiser;Karl Rohr	2009		10.1007/978-3-540-93860-6_26	computer vision;machine learning;pattern recognition	Vision	47.42792587370266	-76.46767737770587	152414
676b77130218f2b959a9d65483abe3f6a6a793a0	precise segmentation of the optic disc in retinal fundus images	segmentation;retinal image;optic disc	This paper presents a methodology for the precise extraction of the optic disc in retinal images. We propose a methodology that combines an enhancement algorithm, a fast location methodology and, finally, a segmentation technique to fit the optic disc boundaries. At each stage, we analyze several techniques and we present the results obtained in a public image data set.		A. Fraga;Noelia Barreira;Marcos Ortega;Manuel G. Penedo;Maria José Carreira	2011		10.1007/978-3-642-27549-4_75	computer vision;segmentation	Vision	39.55386578853991	-76.38051683800519	152463
da3eb6ad5db48aae2373459512c976e404bf1eb4	salient object segmentation based on superpixel and background connectivity prior		Salient object segmentation is well known for detecting and segmenting objects using saliency map as input. In this paper, we propose a salient object segmentation method which integrates saliency, superpixel, and background connectivity prior. First, our method extracts the superpixels of an image by the simple linear iterative clustering algorithm. Second, based on superpixel representation, we use background connectivity prior to characterize the spatial layout of each superpixel in a color space with respect to the image boundary. Third, considering both saliency and background connectivity values, we label four kinds of superpixel-level seeds and feed them to superpixel-level GrabCut method. Because superpixel representation generates only a few seeds, the optimization of GrabCut method converges fast. Finally, for further improvement, we crop a rectangular region which contains the segmented object obtained in the third step and apply GrabCut at the pixel level to produce the final segmentation result. Experimental results on eight typical datasets demonstrate that in terms of both performance and computational efficiency, the proposed segmentation method outperforms existing state-of-the-art methods.	algorithm;cluster analysis;color space;computation;grabcut;iterative method;mathematical optimization;pixel;sensor	Yuzhen Niu;Maria Giovanna Gagliardi;Wenzhong Guo	2018	IEEE Access	10.1109/ACCESS.2018.2873022	pixel;market segmentation;grabcut;computer vision;distributed computing;object detection;salience (neuroscience);image segmentation;cluster analysis;computer science;artificial intelligence;color space	Vision	45.90989625432591	-68.75069489823781	152825
684a68d965f1881f5af38883988ed04c3290ee62	segmentation of ultrasound image data by two dimensional autoregressive modelling	imagerie ultrasonore;traitement signal;ar model;image processing;maximum likelihood;maximum vraisemblance;procesamiento imagen;segmentation;traitement image;ultrasound imaging;signal processing;procesamiento senal;likelihood function;imageria ultrasonico;segmentacion;maxima verosimilitud	Abst rac t . In this paper we treat ultrasound image data as a two dimensional autoregressive (AR) signal. The image is modelled as consisting of distinct regions each described by one of a small number of AR models. Segmentation is performed by maximising the image likelihood function, which takes on a convenient form due to the AR model. Image data is presented to the algorithm in complex amplitude form. Results from application of this method to a cardiac phantom data set are presented.	algorithm;autoregressive model;phantom reference;phasor	Phillip Abbott;Michael Braun	1997		10.1007/3-540-63508-4_182	computer vision;speech recognition;image processing;computer science;segmentation-based object categorization;signal processing;pattern recognition;mathematics;maximum likelihood;likelihood function;image segmentation;autoregressive model;scale-space segmentation;segmentation;statistics	Vision	53.1238898873069	-74.42388116515684	153189
2d445b20c169b85ba3d9dc74717682c6fa258cc3	method for segmentation of the layers in the outer retina	retinitis pigmentosa outer retina visible layer number layer segmentation gaussian function multiple models maximum likelihood estimation model complexity in vivo macular images human retina optical coherence tomography attenuation coefficients;optical tomography biomedical optical imaging eye gaussian distribution image segmentation maximum likelihood estimation medical image processing;retina data models complexity theory image segmentation maximum likelihood estimation mathematical model biomedical optical imaging	This paper presents a method to determine the number of visible layers in the outer retina and perform segmentation. Each layer in the outer retina is represented by a Gaussian function, and multiple models with a different number of layers are used to form the outer retina. Parameters of competing models are calculated by using maximum likelihood estimation after which the model that best describes the data is selected. Model selection is based on the goodness of fit and model complexity thereby ensuring that the model that best represents the data is chosen. The method was applied to in-vivo macular images of human retinas acquired by optical coherence tomography after conversion to attenuation coefficients. Examples of detected number of visible layers and corresponding segmentation results are shown in both normal and retinitis pigmentosa affected retinas.	coefficient;maximum likelihood estimation;model selection;normal statistical distribution;retina;retinitis pigmentosa;segmentation action;tomography, optical coherence;video-in video-out;anatomical layer;biologic segmentation;tomography	Jelena Novosel;Koen A. Vermeer;Laurence Pierrache;Caroline C. W. Klaver;L. I. van den Born;Lucas J. van Vliet	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319673	computer vision;mathematical optimization;mathematics;optics	Robotics	47.43244828894558	-79.69033254371045	153286
64740f99bbd140e4ecbb3f49146dac56ef65fbce	a fuzzy linguistic-based software tool for seismic image interpretation	fuzzy theory;software tool;image segmentation;performance evaluation;image processing;seismology;software tools fuzzy systems graphics user interfaces automatic control geology image analysis humans image segmentation petroleum;indexing terms;three dimensional;graphical user inter face;cooperative behavior;image interpretation;graphical user interfaces;parameter tuning;human system interaction fuzzy linguistics fuzzy system parameter tuning software tool attribute fusion three dimensional seismic image interpretation graphic user interface;geophysical signal processing;graphic user interface;computational linguistics fuzzy systems seismology geophysical signal processing image processing graphical user interfaces sensor fusion;image analysis;interactive image segmentation;attribute fusion;computational linguistics;human system interaction;sensor fusion;fuzzy systems;fuzzy system	The contribution of this paper concerns the well-known problem of fuzzy system parameter tuning. At this aim, a software tool based on a fuzzy linguistic approach and applied to an attribute fusion system devoted to three-dimensional (3-D) seismic image analysis is proposed. The fusion is based on interpreters’ knowledge and a graphic user interface has been developed in order to have a cooperative behavior between the experts and the system. It provides an original way to adjust, on a two-dimensional part of the block, some of the fusion parameters which are understandable and close to the interpreters’ language. Then, in order to control the detection propagation to the whole 3-D seismic block, an automatic parameter adjustment is realized based on a quantitative performance evaluation of the detection. The results obtained for the detection as well as for the handling of the system by interpreters’ show the interest of the proposed method.	fuzzy control system;graphical user interface;image analysis;performance evaluation;programming tool;software propagation;whole earth 'lectronic link	Lionel Valet;Gilles Mauris;Philippe Bolon;Naamen Keskes	2003	IEEE Trans. Instrumentation and Measurement	10.1109/TIM.2003.814701	computer vision;computer science;theoretical computer science;data mining;graphical user interface;fuzzy control system	Robotics	42.28432379143641	-69.95258451568901	153405
edf68aacd9707e6c4aeec1ae6f70f53d779a4fa2	image feature extraction with the perceptual graph based on the ant colony system	graph theory;image features;image segmentation;image processing;edge detection;computer vision;ant colony system;machine vision;feature extraction;feature extraction digital images machine vision image edge detection image segmentation image processing particle swarm optimization ant colony optimization paints artificial intelligence;image segmentation ant colony system perceptual graph image feature extraction digital images machine vision system;digital image;image segmentation feature extraction graph theory computer vision edge detection	In this paper, perceptual graph is proposed to represent the relationship between neighboring points in digital images. The ant colony system is applied to build the perceptual graph of digital images, which makes the basis of the layered model of a machine vision system, in the experiments, edge feature extraction and image segmentation are implemented with the proposed machine vision model. The experimental results show that the ant colony system can effectively extract image features.	ant colony;digital image;experiment;feature extraction;image segmentation;machine vision	Xinhua Zhuang	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1401398	image texture;computer vision;feature detection;image analysis;edge detection;machine vision;binary image;image processing;feature extraction;computer science;graph theory;machine learning;kanade–lucas–tomasi feature tracker;segmentation-based object categorization;digital image processing;pattern recognition;image segmentation;scale-space segmentation;automatic image annotation;feature;digital image;connected-component labeling	Robotics	42.28668167148678	-66.26855386071765	153541
5653849219ea5b8e5f5c2edee87696a92e189036	orientation robust text line detection in natural images	higher order correlation clustering;texton based texture classifier orientation robust text line detection natural images higher order correlation clustering hocc graph partitioning problem maximally stable extremal region mser appearance consistency spatial alignment long range interactions regularization method semidefinite programming problem;higher order correlation clustering text detection;text detection edge detection filtering theory mathematical programming pattern clustering;text detection;correlation programming training vectors image edge detection image color analysis computer vision	In this paper, higher-order correlation clustering (HOCC) is used for text line detection in natural images. We treat text line detection as a graph partitioning problem, where each vertex is represented by a Maximally Stable Extremal Region (MSER). First, weak hypothesises are proposed by coarsely grouping MSERs based on their spatial alignment and appearance consistency. Then, higher-order correlation clustering (HOCC) is used to partition the MSERs into text line candidates, using the hypotheses as soft constraints to enforce long range interactions. We further propose a regularization method to solve the Semidefinite Programming problem in the inference. Finally we use a simple texton-based texture classifier to filter out the non-text areas. This framework allows us to naturally handle multiple orientations, languages and fonts. Experiments show that our approach achieves competitive performance compared to the state of the art.	cluster analysis;correlation clustering;edge detection;emoticon;entity–relationship model;experiment;f1 score;graph (discrete mathematics);graph partition;interaction;matrix regularization;maximally stable extremal regions;orientation (graph theory);partition problem;precision and recall;semidefinite programming;sensor;texton;yao graph	Le Kang;Yi Li;David S. Doermann	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.514	correlation clustering;computer vision;machine learning;pattern recognition;mathematics	Vision	45.95744115440082	-68.89544539766426	153548
0bb9f52ad9f0c3edb9757a8b283295808adc7341	an energy of asymmetry for accurate detection of global reflection axes	generalised complex moments;energy minimisation;energy function;symmetric axis;local minima;human brain;energy for symmetry and asymmetry;steepest descent;reflectional symmetry	We de®ne in this paper two energy terms, symmetric energy term and asymmetric energy term, which respectively correspond to the symmetric and asymmetric components of an object. The asymmetric energy term must be zero, if the studied object is invariant under a re ̄ection about the x-axis. Accordingly, we formulate the problem of detecting re ̄ectional symmetries as a problem of minimising the asymmetric energy term. From the local minima of the asymmetric energy term, we can detect all the symmetric axes of any object. Since the asymmetric energy term is expressed as a summation of a set of generalised complex (GC) moments computed for an object, the proposed symmetry detection method is robust against both noise and slight deformation. We use the steepest descent technique to calculate the local minima of the asymmetric energy term, whose initialisation is calculated from the most dominant GC moment. Experiments on typical logo images and human brain image have shown the effectiveness and the robustness of the proposed method. To our knowledge, this is the ®rst theory on energy functions that describe the symmetric and asymmetric components of a 2D pattern. q 2001 Elsevier Science B.V. All rights reserved.	apache axis;gradient descent;logo;maxima and minima;sensor	Dinggang Shen;Horace Ho-Shing Ip;Eam Khwang Teoh	2001	Image Vision Comput.	10.1016/S0262-8856(00)00077-9	gradient descent;reflection symmetry;mathematical optimization;combinatorics;maxima and minima;mathematics;geometry	Robotics	48.57288818494148	-69.95040987853181	153756
c745285642305904ff1d393624d40b8fcb248564	segmentation and enhancement of brain mr images using fuzzy clustering based on information theory	brain mr image;segmentation;bias;mutual information;fcm	In recent decades, fuzzy segmentation methods, FCM algorithm in particular, have been widely employed for medical image segmentation, because they can save more information of the original image. But some artifacts like spatial noise and bias in medical images disturb segmentation results. This research presents an improved and robust FCM method based on information theoretic clustering, which estimated and corrected the heterogeneity of the magnetic field image (bias) and minimized the noise effects. To increase accuracy against any noise, the mutual information between data distribution of each cluster and those out of that cluster were maximized. The simulation results of the proposed algorithm are compared with previous fuzzy segmentation methods and its superiority in terms of segmentation of MR images in the database of brain images and synthetic images is illustrated.	cluster analysis;fuzzy clustering;information theory	Mohamad Amin Bakhshali	2017	Soft Comput.	10.1007/s00500-016-2210-2	fuzzy logic;computer science;fuzzy clustering;image segmentation;scale-space segmentation;cluster analysis;segmentation-based object categorization;computer vision;mutual information;artificial intelligence;segmentation;pattern recognition	Robotics	43.73305258302825	-72.51377774895269	153771
a27240fb4ffbdc0261931fd2838c745017a01031	robust curve detection by temporal geodesics	detectors;robust curve detection;splitting;image segmentation;robustness detectors merging clustering algorithms cities and towns hardware testing noise level image segmentation adaptive arrays;dotted partially coherent noise;parallel adaptive classification;testing;computerised pattern recognition;velocity field;synthetic dotted images;temporal geodesics;parallel machines computerised pattern recognition curve fitting;noise level;parallel merging;levels of abstraction;adaptive arrays;merging;dotted curve detector;clustering algorithms;cities and towns;splitting robust curve detection temporal geodesics velocity field parallel hardware dotted curve detector synthetic dotted images dotted partially coherent noise parallel adaptive classification parallel merging;robustness;parallel machines;parallel hardware;curve fitting;hardware	The formation of temporal geodesics (i.e., fast routes) in a velocity field seems to offer a means of detecting curves by parallel hardware in a wide range of images with little a pr ior i design information. We tested this hypothesis by devising and constructing a dotted-curve detector based on it, and applying the detector to several synthetic dotted images covering a wide range of dotted curves and dotted partially coherent noise. The results were encouraging. The dotted-curve detector searches for fast routes in a velocity field determined locally by the approximate collinearity or cocircularity of clusters of dots, and globally by the tortuosity of the route and the uniformity of the spacing of dots along the route. The routes are found by a parallel adaptive classification of the dots into curve dots and noise dots, followed by a parallel merging and splitting of graphs spanning temporally close dots or subgraphs at successively higher levels of abstraction. The classification is carried out by “mixed adaptation” a mixture of supervised and unsupervised training.	approximation algorithm;circuit complexity;coherence (physics);edge detection;file spanning;principle of abstraction;sensor;supervised learning;synthetic intelligence;unsupervised learning;velocity (software development)	Dan Gutfinger;R. Nishimura;H. Doi;Jack Sklansky	1990		10.1109/ICCV.1990.139634	computer vision;detector;vector field;computer science;theoretical computer science;machine learning;mathematics;software testing;image segmentation;cluster analysis;splitting;statistics;robustness;curve fitting	ML	48.385706607395804	-67.80201580603959	153823
e597a808f25a129791ab24ecbcd70f63c8c7da3a	fast intelligent watermarking of heterogeneous image streams through mixture modeling of pso populations	digital watermarking;intelligent watermarking;memory based optimization;dynamic particle swarm optimization dpso;gaussian mixture modeling gmm	In intelligent watermarking (IW), evolutionary computing (EC) is employed in order to automatically set the embedding parameters of digital watermarking systems for each image. However, the computational complexity of EC techniques makes IW unfeasible for large scale applications involving heterogeneous images. In this paper, we propose a Dynamic Particle Swarm Optimization (DPSO) technique which relies on a memory of Gaussian mixture models (GMMs) of solutions in the optimization space. This technique is employed in the optimization of embedding parameters of a multi-level (robust/fragile) bi-tonal watermarking system in high data rate applications. A compact density representation of previously-found DPSO solutions is created through GMM in the optimization space, and stored in memory. Solutions are re-sampled from this memory, re-evaluated for new images and have their distribution of fitness values compared with that stored in the memory. When the distributions are similar, memory solutions are employed in a straightforward manner, avoiding costly re-optimization operations. A specialized memory management mechanism allows to maintain and adapt GMM distributions over time, as the image stream changes. This memory of GMMs allows an accurate representation of the topology of a stream of optimization problems. Consequently, new cases of optimization can be matched against previous cases more precisely (when compared with a memory of static solutions), leading to considerable decrease in computational burden. Simulation results on heterogeneous streams of images indicate that compared to full re-optimization for each document image, the proposed approach allows to decrease the computational requirement linked to EC by up to 97.7% with little impact on the accuracy for detecting watermarks. Comparable results were obtained for homogeneous streams of document images. Email addresses: evellasques@livia.etsmtl.ca (Eduardo Vellasques), robert.sabourin@etsmtl.ca (Robert Sabourin), eric.granger@etsmtl.ca (Eric Granger) Preprint submitted to Elsevier May 2, 2012 Page 4 of 45 Ac ce pt ed M an us cr ip t	computational complexity theory;digital watermarking;email;evolutionary computation;google map maker;iw engine;mathematical optimization;memory management;mixture model;particle swarm optimization;population;sensor;simulation;uncompressed video;eric	Eduardo Vellasques;Robert Sabourin;Eric Granger	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.08.040	mathematical optimization;digital watermarking;computer science;artificial intelligence;theoretical computer science;machine learning	EDA	50.43211809191182	-68.0034205491604	153834
8401a1fb61d315de5107414edf3febaf4ea6b263	localised manifold learning for cardiac image analysis	image analysis;cardiovascular magnetic resonance imaging	Manifold learning is increasingly being used to discover the underlying structure of medical image data. Traditional approaches operate on whole images with a single measure of similarity used to compare entire images. In this way, information on the locality of differences is lost and smaller trends may be masked by dominant global differences. In this paper, we propose the use of multiple local manifolds to analyse regions of images without any prior knowledge of which regions are important. Localised manifolds are created by partitioning images into regular subsections with a manifold constructed for each patch. We propose a framework for incorporating information from the neighbours of each patch to calculate a coherent embedding. This generates a simultaneous dimensionality reduction of all patches and results in the creation of embeddings which are spatially-varying. Additionally, a hierarchical method is presented to enable a multi-scale embedding solution. We use this to extract spatially-varying respiratory and cardiac motions from cardiac MRI. Although there is a complex interplay between these motions, we show how they can be separated on a regional basis. We demonstrate the utility of the localised joint embedding over a global embedding of whole images and over embedding individual patches independently.	coherence (physics);image analysis;locality of reference;nonlinear dimensionality reduction	Kanwal K. Bhatia;Anthony N. Price;Joseph V. Hajnal;Daniel Rueckert	2012		10.1117/12.911455	computer vision;simulation;machine learning;physics	ML	47.72296017088428	-77.60542978922251	153853
817448b9a2e384e797fee3a119f5f63665aa97ac	corner detection using slit rotational edge-feature detector	stable corner detection;detectors;corner detection;interpolation;weighted sized;image segmentation;information science;application software;edge detection;region separability;detection accuracy;computer vision;image edge detection detectors image segmentation information science computer vision application software image registration pixel image analysis;image edge detection;feature extraction;image registration;pixel;interpolation edge detection feature extraction;weighting pixels slit rotational edge feature detector image corners computer vision edge certainties low contrast region detection accuracy interpolated sized weighted sized region separability stable corner detection;weighting pixels;image analysis;image corners;edge certainties;interpolated sized;slit rotational edge feature detector;low contrast region	Corners in images include useful information and they play an important role in computer vision. However, stable and accurate comer detection is very difficult, because the edges composing a comer break easily around the corner. To overcome this difficulty, a corner detection method using slit rotational edge-feature detector (SRED) has been proposed. By evaluating the edge certainties at each pixel in all directions, this method can detect corners stably. Nevertheless, this method cannot detect the corners in the region of low contrast, and its detection accuracy depends on the edge directions. To solve these problems, we propose a new comer detection method using weighted and interpolated SIZED (WI-SRED). By using the edge certainties based on the region separability, the proposed method achieves stable corner detection in the region of low contrast, and moreover, by interpolating and weighting pixels, this method prevents the detection accuracy from depending on the edge directions.	corner detection	Yasutaka Etou;Takahiro Sugiyama;Keiichi Abe;Toru Abe	2002		10.1109/ICIP.2002.1040071	corner detection;computer vision;detector;application software;image analysis;edge detection;feature extraction;information science;interpolation;computer science;image registration;mathematics;image segmentation;interest point detection;pixel;computer graphics (images)	Vision	45.22156390120877	-66.42348150334605	153873
24079f4ad359a0b011378cff91a050c83c158a23	electron microscope tomography of cells and tissues: studying the 3d structure of molecular machines at molecular resolution	bilateral filtering;biology computing;electron microscopes;image recognition;biological tissues;image segmentation;protein complex;image processing;3d visualization;molecular machine;virtual reality;video game;protein structure;proteins;electron microscope;feature extraction;molecular biophysics;pattern recognition;virtual reality like 3d visualization electron microscope tomography tissue molecular machine molecular resolution protein structure multiprotein complex cellular environment hearing machinery macromolecular crowding image processing analysis tool bilateral filtering gradient based feature extraction watershed immersion segmentation skeletonization scheme automated image segmentation template matching pattern recognition;electron microscopy tomography image segmentation proteins assembly auditory system machinery working environment noise image processing image analysis;cellular biophysics biology computing molecular biophysics image recognition electron microscopes proteins image segmentation biological tissues;3d structure;template matching;cellular biophysics	Despite the large number of protein structures, we know little about their assembly into multi-protein complexes in cells. Such molecular machines are often transient, depend on their cellular environment, and therefore too complex, rare, and fragile to be purified, and therefore unsuitable for most structural techniques. I will illustrate the potential of EM tomography of cells and tissues with an emphasis on hearing machinery. I will describe the difficulties encountered, such as noise, macromolecular crowding and the complexity of the 3D data. I show how we currently overcome these challenges in order to achieve biological interpretations of the 3D cellular sceneries. We are using a variety of image processing analysis tools, mostly provided by others, such as bilateral filtering, gradient-based feature extraction, watershed immersion segmentation. We have started to employ skeletonization schemes to assess differences and similarities between these molecular machines in their respective cellular environment. In order to achieve the ultimate goal of understanding cells and tissues at a truly molecular level, we will need further improvements in automated image segmentation, template matching, as well as pattern recognition. Moreover, we hope to interest the audience in the development of an interactive, virtual reality-like 3D visualization and segmentation program, that may-make scientific analysis of cellular 3D volumes as easy and enjoyable as a video game. (Support: HFSPO (LT-0532); Agouron Institute/Jane Coffin Childs Fund; US-DOE (DE-A C03-76SF00098)).	bilateral filter;ct scan;crowding;electron;feature extraction;gradient;image noise;image processing;image segmentation;immersion (virtual reality);jane (software);pattern recognition;template matching;tomography;topological skeleton;virtual reality;visualization (graphics);watershed (image processing)	Manfred Auer	2005	2005 IEEE Computational Systems Bioinformatics Conference - Workshops (CSBW'05)	10.1109/CSBW.2005.61	computer vision;image processing;computer science;bioinformatics;virtual reality;electron microscope;molecular biophysics;computer graphics (images)	Visualization	40.014343618166315	-73.86751337071388	154085
30f725d8f6fec46d7030a540e56fbd0d1a0f2ba0	a hybrid approach using gaussian smoothing and genetic algorithm for multilevel thresholding	gaussian kernel smoothing;hybrid approach;best fit;thresholding performance;multilevel thresholding;real image;genetic algorithm;image histogram;automatic multilevel image thresholding;gaussian smoothing;gaussian function;parameter estimation;image segmentation	In this paper, a hybrid approach, which is based on Gaussian smoothing and a genetic algorithm (GA), is proposed for automatic multilevel image thresholding. Using a mixture probability density function of several Gaussian functions to fit an image histogram and then find the optimal threshold(s) is a well-known optimal thresholding method. In the proposed approach, the Gaussian kernel smoothing is used to estimate the number of classes in an image. Since the parameter estimation in the method is typically a nonlinear optimization problem, the parameters used in the mixture of Gaussian functions that give the best fit to the processed histogram are determined using GA. In experiments, synthetic data and real images were processed to evaluate the thresholding performance. The experimental results to confirm the proposed approach are also included.	gaussian blur;genetic algorithm;smoothing;thresholding (image processing)	Chih-Chin Lai;Din-Chang Tseng	2004	Int. J. Hybrid Intell. Syst.		mathematical optimization;genetic algorithm;computer science;machine learning;pattern recognition;balanced histogram thresholding;thresholding;image segmentation;estimation theory	AI	49.90728363837375	-68.90468654795639	154163
387acc33a51ba1b6624788340ee1b91fb1c29433	video segmentation through multiscale texture analysis	multiscale analysis;video segmentation;texture analysis	Segmenting a video sequence into different coherent scenes requires analyzing those aspects which allow finding the changes where a transition is to be found. Textures are an important feature when we try to identify or classify elements in a scene and, therefore, can be very helpful to find those frames where there is a transition. Furthermore, analyzing the textures in a given environment at different scales provides more information than considering the features which can be extracted from a single one. A standard multiscale texture analysis would require an adjustment of the scales in the comparison of the textures. However, when analyzing video sequences, this process can be simplified by assuming that the frames have been acquired at the same resolution. In this paper, we present a multiscale approach for segmenting video scenes by comparing the textures which are present in their frames.	coherence (physics);gradient;sensitivity and specificity;structure tensor;texture mapping	Miguel Alemán-Flores;Luis Álvarez-León	2004		10.1007/978-3-540-30126-4_42	image texture;image segmentation;scale-space segmentation	Vision	47.20395641178404	-67.7472066537632	154172
99647c2822f82834434172295de768c4acee25b5	level set gait analysis for synthesis and reconstruction	shape estimation;level set;feature space;gait analysis;cubic spline	We describe a new technique to extract the boundary of a walking subject, with ability to predict movement in missing frames. This paper uses a level sets representation of the training shapes and uses an interpolating cubic spline to model the eigenmodes of implicit shapes. Our contribution is to use a continuous representation of the feature space variation with time. The experimental results demonstrate that this level set-based technique can be used reliably in reconstructing the training shapes, estimating in-between frames to help in synchronizing multiple cameras, compensating for missing training sample frames, and the recognition of subjects based on their gait.	autoregressive model;basis function;coefficient;cubic hermite spline;cubic function;feature vector;framing (world wide web);gait analysis;inbetweening;interpolation;normal mode;numerical analysis;robustness (computer science);spatial variability;spline (mathematics);test set	Muayed S. Al-Huseiny;Sasan Mahmoodi;Mark S. Nixon	2009		10.1007/978-3-642-10520-3_35	spline;computer vision;gait analysis;feature vector;computer science;level set;pattern recognition;mathematics;geometry	Vision	48.17070735889008	-76.37199797668353	154809
a9b3e1ee085523a8df121f4b9ec2a81033de8cb1	an orientation inference framework for surface reconstruction from unorganized point clouds	minimisation;implicit surface;graph node;surface reconstruction graph theory minimisation radial basis function networks solid modelling;graph theory;iterative method;evaluation performance;optimisation;global solution;graph optimization problem;mise a jour;optimal labeling;enfoque credal;performance evaluation;image processing;orientation inference framework;nudo grafo;fitting accuracy;fonction energie;unoriented point clouds;optimizacion;hierarchized structure;evaluacion prestacion;fonction base radiale;front propagation;procesamiento imagen;reconstruction algorithms;structure hierarchisee;credal approach;local inference;indexing terms;surface reconstruction;etiquetage;traitement image;three dimensional;energy function;etiquetaje;metodo iterativo;surface reconstruction surface treatment image reconstruction reconstruction algorithms labeling clouds three dimensional displays;algorithme;actualizacion;optimization problem;radial basis function networks;algorithm;graph nodes;front propagation fashion;reconstruction image;nube;accuracy;reconstruction surface;surface treatment;3 d data sets;precision;radial basis function;reconstruccion imagen;belief propagation;three dimensional displays;methode iterative;image reconstruction;clouds;3 d data sets orientation inference framework unoriented point clouds surface approximation hierarchy radial basis functions graph optimization problem energy function optimal labeling graph nodes local inference front propagation fashion progressive reconstruction algorithm fitting accuracy surface reconstruction;labelling;surface approximation hierarchy;funcion energia;optimization;point cloud;reconstruccion superficie;nuage;reconstruction algorithm;funcion radial base;graph optimization;approche credibiliste;estructura jerarquizada;updating;labeling;noeud graphe;surface reconstruction belief propagation graph optimization implicit surface orientation inference;solid modelling;radial basis functions;progressive reconstruction algorithm	In this paper, we present an orientation inference framework for reconstructing implicit surfaces from unoriented point clouds. The proposed method starts from building a surface approximation hierarchy comprising of a set of unoriented local surfaces, which are represented as a weighted combination of radial basis functions. We formulate the determination of the globally consistent orientation as a graph optimization problem by treating the local implicit patches as nodes. An energy function is defined to penalize inconsistent orientation changes by checking the sign consistency between neighboring local surfaces. An optimal labeling of the graph nodes indicating the orientation of each local surface can, thus, be obtained by minimizing the total energy defined on the graph. The local inference results are propagated over the model in a front-propagation fashion to obtain the global solution. The reconstructed surfaces are consolidated by a simple and effective inspection procedure to locate the erroneously fitted local surfaces. A progressive reconstruction algorithm that iteratively includes more oriented points to improve the fitting accuracy and efficiently updates the RBF coefficients is proposed. We demonstrate the performance of the proposed method by showing the surface reconstruction results on some real-world 3-D data sets with comparison to those by using the previous methods.	algorithm;anatomic node;approximation;checking (action);coefficient;computation (action);graph - visual representation;graphics hardware;graphics processing unit;implicit surface;inference;mathematical optimization;multiresolution analysis;numerical linear algebra;optimization problem;patch (computing);point cloud;radial (radio);radial basis function;scanning;seizures;software propagation;the matrix	Yi-Ling Chen;Shang-Hong Lai	2011	IEEE Transactions on Image Processing	10.1109/TIP.2010.2076297	mathematical optimization;radial basis function;combinatorics;image processing;computer science;graph theory;machine learning;mathematics;geometry;accuracy and precision;algorithm;statistics	Vision	48.021751863505614	-73.7989097532087	154890
92e2c04c7041f6f6e68985dff44deaf80d9bc269	fogbank: a single cell segmentation across multiple cell lines and image modalities	animals;female;mice;saccharomyces cerevisiae;nih 3t3 cells;breast;computational biology bioinformatics;cells;image interpretation computer assisted;microscopy fluorescence;microscopy phase contrast;algorithms;humans;combinatorial libraries;computational biology;computer appl in life sciences;microarrays;bioinformatics	Many cell lines currently used in medical research, such as cancer cells or stem cells, grow in confluent sheets or colonies. The biology of individual cells provide valuable information, thus the separation of touching cells in these microscopy images is critical for counting, identification and measurement of individual cells. Over-segmentation of single cells continues to be a major problem for methods based on morphological watershed due to the high level of noise in microscopy cell images. There is a need for a new segmentation method that is robust over a wide variety of biological images and can accurately separate individual cells even in challenging datasets such as confluent sheets or colonies. We present a new automated segmentation method called FogBank that accurately separates cells when confluent and touching each other. This technique is successfully applied to phase contrast, bright field, fluorescence microscopy and binary images. The method is based on morphological watershed principles with two new features to improve accuracy and minimize over-segmentation. First, FogBank uses histogram binning to quantize pixel intensities which minimizes the image noise that causes over-segmentation. Second, FogBank uses a geodesic distance mask derived from raw images to detect the shapes of individual cells, in contrast to the more linear cell edges that other watershed-like algorithms produce. We evaluated the segmentation accuracy against manually segmented datasets using two metrics. FogBank achieved segmentation accuracy on the order of 0.75 (1 being a perfect match). We compared our method with other available segmentation techniques in term of achieved performance over the reference data sets. FogBank outperformed all related algorithms. The accuracy has also been visually verified on data sets with 14 cell lines across 3 imaging modalities leading to 876 segmentation evaluation images. FogBank produces single cell segmentation from confluent cell sheets with high accuracy. It can be applied to microscopy images of multiple cell lines and a variety of imaging modalities. The code for the segmentation method is available as open-source and includes a Graphical User Interface for user friendly execution.	algorithm;binary image;confluence (abstract rewriting);cultured cell line;distance (graph theory);graphical user interface;high-level programming language;histogram;image noise;interface device component;microscopy, fluorescence;microscopy, phase-contrast;open-source software;pixel;product binning;stem cells;usability;watershed (image processing);biologic segmentation	Joe Chalfoun;Michael Majurski;Alden Dima;Christina H. Stuelten;Adele P. Peskin;Mary Brady	2014		10.1186/s12859-014-0431-x	biology;dna microarray;cell biology;computer science;bioinformatics;genetics	Visualization	39.574122173978	-73.29819919373854	155166
48b5ae2a56041c81bba8a8a2994758398ae08376	segmenting by seeking the symmetry axis	cost function symmetry axis image segmentation fit function minimization dijkstra algorithm global minimum;goodness of fit;image segmentation;cost function;symmetry image segmentation;symmetry;dijkstra s algorithm;shape image segmentation electrical capacitance tomography read only memory cost function humans image edge detection skeleton laser sintering smoothing methods	We introduce a method for segmenting a shape from an image and simultaneously determining its symmetry axis. The symmetry is used to help the segmentation and in turn the segmentation determines the symmetry. The problem is formulated as one of minimizing a goodness of fitness function and Dijkstra’s algorithm is used to find the global minimum of the cost function. The results are illustrated on real images.	apache axis;dijkstra's algorithm;fitness function;loss function;maxima and minima;optic axis of a crystal	Tyng-Luh Liu;Davi Geiger;Alan L. Yuille	1998		10.1109/ICPR.1998.711856	computer vision;mathematical optimization;dijkstra's algorithm;computer science;machine learning;segmentation-based object categorization;mathematics;geometry;image segmentation;symmetry;goodness of fit;scale-space segmentation	Vision	46.23471101991299	-71.45430622308909	155313
3d257135dd408b7612b0b466d604954eb2830233	detection of lung injury using 4d-ct chest images	radiation therapy biomechanics computerised tomography feature extraction image classification image registration injuries lung medical image processing;radiation induced lung injury 4d ct lung deformation;random forest classifier 4d ct chest image radiation induced lung injury detection radiation therapy computed tomography elastic image registration lung segmentation appearance extraction functional feature extraction lung tissue classification respiratory cycle adaptive shape prior model first order intensity model second order homogeneity descriptor regional feature lung functionality ventilation elasticity;lungs feature extraction ventilation computed tomography image segmentation adaptation models injuries;4d ct;lung deformation;radiation induced lung injury	This paper proposes a novel framework for the identification of the radiation-induced lung injury (RILI) after radiation therapy (RT) using 4D computed tomography (CT). The proposed methodology involves elastic image registration; segmentation of the lung fields; extraction of appearance and functional features; and classification of the lung tissues. The first step locally aligns the consecutive phases of the respiratory cycle using an elastic image registration approach based on descent minimization of the sum of squared difference similarity metric. Secondly, lung fields are segmented using a hybrid framework that integrates an adaptive shape prior model, a first-order intensity model, and a second order homogeneity descriptor of the lung tissues. Next, regional features that describe different types of lung functionality (e.g., ventilation and elasticity) are estimated from the segmented lungs. Finally, a random forest classifier has been applied to distinguish between injured and normal lung tissues. The proposed framework has been tested on data sets collected from 13 patients who had undergone RT treatment. Experimental results demonstrate the promise of the proposed framework for the identification of the injured lung region, and thus hold the promise as a valuable tool for early detection of RILI.	ct scan;elasticity (data store);first-order predicate;html5 in mobile devices;image registration;radiation hardening;random forest;tomography	Ahmed Soliman;Fahmi Khalifa;Ahmed Shaffie;Neal Dunlap;Brian Wang;Adel Said Elmaghraby;Ayman El-Baz	2016	2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2016.7493499	radiology;medicine;pathology;medical physics	Vision	40.04875281009341	-79.14327303188102	155328
607f84617ebea9859662b87fd35365a97c75682d	structure-aware rank-1 tensor approximation for curvilinear structure tracking using learned hierarchical features		Tracking of curvilinear structures (CS), such as vessels and catheters, in X-ray images has become increasingly important in recent interventional applications. However, CS is often barely visible in low-dose X-ray due to overlay of multiple 3D objects in a 2D projection, making robust and accurate tracking of CS very difficult. To address this challenge, we propose a new tracking method that encodes the structure prior of CS in the rank-1 tensor approximation tracking framework, and it also uses the the learned hierarchical features via a convolutional neural network (CNN). The three components, i.e., curvilinear prior modeling, high-order information encoding and automatic feature learning, together enable our algorithm to reduce the ambiguity rising from the complex background, and consequently improve the tracking robustness. Our proposed approach is tested on two sets of X-ray fluoroscopic sequences including vascular structures and catheters, respectively. In the tests our approach achieves a mean tracking error of 1.1 pixels for vascular structure and 0.8 pixels for catheter tracking, significantly outperforming state-of-the-art solutions on both datasets.	3d projection;algorithm;approximation;artificial neural network;code;convolutional neural network;encode;feature learning;ibm notes;pixel;x-ray (amazon kindle)	Peng Chu;Yu Pang;Erkang Cheng;Ying J. Zhu;Yefeng Zheng;Haibin Ling	2016		10.1007/978-3-319-46720-7_48	computer vision;machine learning;geometry	Vision	49.68109923170177	-74.81528326900113	155369
d78893d098c6f5fb44f99f9e15c8f99bb3a69b47	estimation of shape model parameters for 3d surfaces	databases;recursive estimation;optimisation;shape surface fitting parameter estimation newton method least squares methods recursive estimation costs databases pelvic bones data mining;shape model parameter estimation;porcine pelvic bone;gauss newton optimization scheme;statistical shape model;3d surfaces;standard deviation;optimal method;optimisation bone computerised tomography image registration medical image processing;gauss newton;surface fitting;x ray tomography shape model parameter estimation 3d surfaces gauss newton optimization scheme distance maps statistical shape model porcine pelvic bone ct scans image shape analysis image registration biomedical image processing optimization methods;pelvic bones;data mining;ct scan;distance maps;shape;medical image processing;image registration;bone;computerised tomography;biomedical image processing;newton method;shape modeling;x ray tomography image shape analysis image registration biomedical image processing optimization methods;parameter estimation;ct scans;image shape analysis;x ray tomography;least squares methods;leave one out;optimization methods	Statistical shape models are widely used as a compact way of representing shape variation. Fitting a shape model to unseen data enables characterizing the data in terms of the model parameters. In this paper a Gauss-Newton optimization scheme is proposed to estimate shape model parameters of 3D surfaces using distance maps, which enables the estimation of model parameters without the requirement of point correspondence. For applications with acquisition limitations such as speed and cost, this formulation enables the fitting of a statistical shape model to arbitrarily sampled data. The method is applied to a database of 3D surfaces from a section of the porcine pelvic bone extracted from 33 CT scans. A leave-one-out validation shows that the parameters of the first 3 modes of the shape model can be predicted with a mean difference within [-0.01,0.02] from the true mean, with a standard deviation less than 0.34.	ct scan;curve fitting;gauss–newton algorithm;map;mathematical optimization;newton;statistical model;statistical shape analysis	Søren G. H. Erbou;Sune Darkner;Jurgen Fripp;Sébastien Ourselin;Bjarne K. Ersbøll	2008	2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2008.4541073	active shape model;point distribution model;computer vision;mathematical optimization;radiology;shape;image registration;shape analysis;mathematics;geometry;newton's method;computed tomography;estimation theory;standard deviation;statistics	Vision	44.49950526291017	-77.2185912913456	155398
82c5a2ffab26c44cea32acba4dc0183755047ee7	knowledge-based segmentation: using simultaneous shape and histogram information to segment brain structures	knowledge base			Nematollah Batmanghelich;Hamid Soltanian-Zadeh;Babak Nadjar Araabi	2005			scale-space segmentation;image segmentation;artificial intelligence;computer vision;knowledge base;histogram;segmentation;pattern recognition;computer science	Vision	41.83070827857695	-71.58580747035515	155403
81805b0df1bcc64e4f49fda8ca6cb89c22003971	novel classification and segmentation techniques with application to remotely sensed images	remote sensing image;granular computing;rough entropy;fuzzy set;image segmentation;soft computing;active learning;supervised classification;image classification;fuzzy entropy;support vector classifier;fuzzy sets;fuzzy correlation;comparative study;unsupervised classification;rough sets;hough transform;support vector machine;rough set;remotely sensed images	The article deals with some new results of investigation, both theoretical and experimental, in the area of image classification and segmentation of remotely sensed images. The article has mainly four parts. Supervised classification is considered in the first part. The remaining three parts address the problem of unsupervised classification (segmentation). The effectiveness of an active support vector classifier that requires reduced number of additional labeled data for improved learning is demonstrated in the first part. Usefulness of various fuzzy thresholding techniques for segmentation of remote sensing images is demonstrated in the second part. A quantitative index of measuring the quality of classification/segmentation in terms of homogeneity of regions is introduced in this regard. Rough entropy (in granular computing framework) of images is defined and used for segmentation in the third part. In the fourth part a homogeneous region in an image is defined as a union of homogeneous line segments for image segmentation. Here Hough transform is used to generate these line segments. Comparative study is also made with related techniques.		B. Uma Shankar	2007	Trans. Rough Sets	10.1007/978-3-540-71663-1_19	computer vision;range segmentation;rough set;granular computing;computer science;machine learning;segmentation-based object categorization;pattern recognition;soft computing;region growing;fuzzy set;image segmentation;scale-space segmentation	HCI	41.26471892980127	-67.3940248940705	155627
514bee4f5138888826fa9f0a508f7bb1e1a8019f	local estimation of the noise level in mri using structural adaptation	noise estimation;maximum likelihood estimation;noncentral chi distribution;magnetic resonance imaging	We present a method for local estimation of the signal-dependent noise level in magnetic resonance images. The procedure uses a multi-scale approach to adaptively infer on local neighborhoods with similar data distribution. It exploits a maximum-likelihood estimator for the local noise level. The validity of the method was evaluated on repeated diffusion data of a phantom and simulated data using T1-data corrupted with artificial noise. Simulation results were compared with a recently proposed estimate. The method was also applied to a high-resolution diffusion dataset to obtain improved diffusion model estimation results and to demonstrate its usefulness in methods for enhancing diffusion data.	acclimatization;image resolution;inference;magnetic resonance imaging;noise (electronics);numerous;phantom reference;phantoms, imaging;silo (dataset);simulation	Karsten Tabelow;Henning U. Voss;Jörg Polzehl	2015	Medical image analysis	10.1016/j.media.2014.10.008	econometrics;magnetic resonance imaging;noncentral chi distribution;pattern recognition;mathematics;maximum likelihood;statistics	ML	52.479920654618425	-76.23242341597675	155800
490d378bc2ee1d13ff7906064d95caee92c86689	sasaki metrics for analysis of longitudinal data on manifolds	health research;uk clinical guidelines;biological patents;tensile stress;measurement;manifolds;europe pubmed central;statistical testing differential geometry diseases medical image processing shape recognition;differential geometry;citation search;shape recognition;shape;vectors;uk phd theses thesis;medical image processing;life sciences;diseases;statistical testing;manifolds vectors measurement data models shape tensile stress equations;uk research reports;aging control sasaki metrics longitudinal data riemannian manifold anatomical shape change anatomy disease generative hierarchical model mean geodesic trend tangent bundle natural distance metric statistical hypothesis test hotelling t 2 statistic longitudinal corpus callosum data dementia;medical journals;europe pmc;biomedical research;data models;bioinformatics	Longitudinal data arises in many applications in which the goal is to understand changes in individual entities over time. In this paper, we present a method for analyzing longitudinal data that take values in a Riemannian manifold. A driving application is to characterize anatomical shape changes and to distinguish between trends in anatomy that are healthy versus those that are due to disease. We present a generative hierarchical model in which each individual is modeled by a geodesic trend, which in turn is considered as a perturbation of the mean geodesic trend for the population. Each geodesic in the model can be uniquely parameterized by a starting point and velocity, i.e., a point in the tangent bundle. Comparison between these parameters is achieved through the Sasaki metric, which provides a natural distance metric on the tangent bundle. We develop a statistical hypothesis test for differences between two groups of longitudinal data by generalizing the Hotelling T2 statistic to manifolds. We demonstrate the ability of these methods to distinguish differences in shape changes in a comparison of longitudinal corpus callosum data in subjects with dementia versus healthily aging controls.	anatomic structures;body of uterus;corpus callosum;dementia;entity;estimated;gene distance metric;hierarchical database model;ibm notes;statistical test;velocity (software development);manifold	Prasanna Muralidharan;P. Thomas Fletcher	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247780	data modeling;differential geometry;computer vision;econometrics;statistical hypothesis testing;manifold;shape;machine learning;mathematics;geometry;stress;measurement;statistics	Vision	43.94988085938554	-78.95980826771473	155863
1814c9b558c452710e67fce7a70b5f228da0a43f	learning-based meta-algorithm for mri brain extraction	brain extraction;existing brain extraction algorithm;novel learning-based meta-algorithm;robust brain extraction result;fusion method;multiple extraction;multiple-segmentation-and-fusion method;test subject;mri brain extraction;learning-based meta-algorithm	Multiple-segmentation-and-fusion method has been widely used for brain extraction, tissue segmentation, and region of interest (ROI) localization. However, such studies are hindered in practice by their computational complexity, mainly coming from the steps of template selection and template-to-subject nonlinear registration. In this study, we address these two issues and propose a novel learning-based meta-algorithm for MRI brain extraction. Specifically, we first use exemplars to represent the entire template library, and assign the most similar exemplar to the test subject. Second, a meta-algorithm combining two existing brain extraction algorithms (BET and BSE) is proposed to conduct multiple extractions directly on test subject. Effective parameter settings for the meta-algorithm are learned from the training data and propagated to subject through exemplars. We further develop a level-set based fusion method to combine multiple candidate extractions together with a closed smooth surface, for obtaining the final result. Experimental results show that, with only a small portion of subjects for training, the proposed method is able to produce more accurate and robust brain extraction results, at Jaccard Index of 0.956 +/- 0.010 on total 340 subjects under 6-fold cross validation, compared to those by the BET and BSE even using their best parameter combinations.	algorithm;biological systems engineering;clinical use template;cross reactions;encephalopathy, bovine spongiform;extraction;gambling;jaccard index;metaheuristic;nonlinear system;population parameter;region of interest;standard template library;biologic segmentation	Feng Shi;Li Wang;John H. Gilmore;Weili Lin;Dinggang Shen	2011	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-23626-6_39	speech recognition;computer science;artificial intelligence;machine learning;data mining	EDA	41.49639956228767	-77.75188935040029	155983
1695e74ca233b774136609b24f7342116e2f4fc6	data driven mean-shift belief propagation for non-gaussian mrfs	random processes belief maintenance computer vision markov processes;time complexity;belief propagation graphical models inference algorithms computer vision gaussian processes data engineering application software probability distribution convergence bandwidth;mean shift;joints;markov random field data driven mean shift belief propagation non gaussian mrf ddmsbp method computer vision scale space theory multimodal mrf model neuroimage registration;markov random field;non gaussian mrf;computer vision;neuroimage registration;data driven mean shift belief propagation;graphical models;scale space;belief propagation;heuristic algorithms;random processes;multimodal mrf model;scale space theory;inference algorithms;optimization;approximation methods;belief maintenance;markov processes;ddmsbp method	We introduce a novel data-driven mean-shift belief propagation (DDMSBP) method for non-Gaussian MRFs, which often arise in computer vision applications. With the aid of scale space theory, optimization of non-Gaussian, multimodal MRF models using DDMSBP becomes less sensitive to local maxima. This is a significant improvement over standard BP inference, and extends the range of methods that are computationally tractable. In particular, when pair-wise potentials are Gaussians, the time complexity of DDMSBP becomes bilinear in the numbers of states and nodes in the MRF. Experimental results from simulation and non-rigid deformable neuroimage registration demonstrate that our method is faster and more accurate than state-of-the-art inference algorithms.	algorithm;backpropagation;belief propagation;bilinear filtering;cobham's thesis;computer simulation;computer vision;diagonally dominant matrix;experiment;image registration;inference engine;markov random field;mathematical optimization;maxima and minima;mean shift;multimodal interaction;reference frame (video);scale space;simulation;software propagation;time complexity	Minwoo Park;Somesh Kashyap;Robert T. Collins;Yanxi Liu	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5539946	time complexity;stochastic process;computer vision;scale space;mean-shift;computer science;machine learning;pattern recognition;graphical model;markov process;belief propagation	Vision	51.195317249940935	-72.19100567258316	156192
d6f0b05950bfe1d49246deb593c2c96d10e119ef	research on segmentation algorithm of 3d medical data	modelizacion;tissu;image tridimensionnelle;occupation time;interfase usuario;theorie locale;eigenvalue problem;local theory;realite virtuelle;aplicacion medical;realidad virtual;user interface;virtual reality;probleme valeur propre;local character structure;segmentation;modelo input output;voxel;modelisation;tejido;realite augmentee;realidad aumentada;input output model;local structure;temps occupation;modele entree sortie;tiempo ocupacion;tissue;tridimensional image;interface utilisateur;medical application;estructura local;structure locale;teoria local;augmented reality;boundary model;modeling;matrice hessienne;segmentacion;imagen tridimensional;problema valor propio;hessian matrices;application medicale	A segmentation algorithm in 3d medical data is proposed based on boundary model and local character structure in this paper. We found out inner voexls and outer voexls by pre-appointed voxel based on boundary model. And then, boundary voexls are correctly classified into different tissues by their eigenvalues of Hessian matrix based on the local character structure. Only eigenvalues of the boundary voxels are computed, so little time is used compared with other algorithms based on local character structure. It can quickly and effectively realize the segmentation of single tissue.	algorithm	Yanjun Peng;Dandan Zhang;Weidong Zhao;Jiaoying Shi;Yongguo Zheng	2006		10.1007/11941354_111	augmented reality;simulation;systems modeling;input–output model;computer science;artificial intelligence;virtual reality;scale-space segmentation;user interface;voxel;segmentation	Vision	46.636229094993624	-79.28528352522528	156222
ef7705c9882bf8bcf9273b7c116755fa0d022ecd	a novel cooperative approach for cardiac pet image segmentation	heart photonics myocardium positron emission tomography image segmentation detectors topology;image segmentation;bayes methods;cardiology;positron emission tomography;statistical analysis bayes methods cardiology image segmentation medical image processing positron emission tomography;statistical analysis;medical image processing;myocardium wall thickness cardiac pet image segmentation cooperative segmentation method mouse myocardium pet image deformable model topological constraint statistical analysis deformation contour left ventricle heart boundaries energy functional region of interest roi bayesian classification mean shift clustering algorithm myocardial boundary ejection fraction	The main objective of this work is to develop a cooperative segmentation method for the mouse myocardium PET images based on deformable models with topological constraints and statistical analysis of the regions where the deformation contours are initialized. Two moving curves, one from inside of the left ventricle and one from the outside of the heart will be deformed to track heart boundaries. More precisely, topology constraints are incorporated to the energy functional governing the evolution of the contours to avoid any collision while allowing them to compete against each other until stabilization. First, we locate the heart, which is the region of interest (ROI) for our study, using level sets with high internal energy initialized from the extremities of the image. It is followed by a Bayesian classification and the application of the mean shift clustering algorithm to locate the center of the left ventricle region. This is where a second contour (interior contour) is initialized. The coupled contours allow to detect the correct myocardial boundaries and compute a number of useful quantities such as the ejection-fraction of the left ventricle and the myocardium wall thickness. The model was applied successfully to the automatic segmentation of the PET images of a mouse myocardium as measured by the Sherbrooke LabPET scanner.	algorithm;cluster analysis;contour line;image segmentation;mean shift;polyethylene terephthalate;region of interest;thickness (graph theory)	Renato Dedic;Madjid Allili	2012	2012 11th International Conference on Information Science, Signal Processing and their Applications (ISSPA)	10.1109/ISSPA.2012.6310474	computer vision;computer science;segmentation-based object categorization;image segmentation;scale-space segmentation;statistics	Vision	43.72285665632055	-75.24478959959224	156265
fe877ae214036c1a5847f506e2b29e2686b20124	enhancement of visual contrast in fluorescence endoscopy	eigenvalues and eigenfunctions;nonfluorescing pixels;sensitivity and specificity;pdd;realtimeframe visual contrast fluorescence endoscopy bladder tumors white light source photo dynamic diagnosis tumor detection cystoscopy cancerous tissue color space transformation nonlinear transformation nonfluorescing pixels fluorescing pixels rgb color space;rgb color space;fluorescence endoscopy;fluorescence;cancer;pdd bladder endoscopy cystoscopy fluorescence photo dynamic diagnosis;color space;real time;endoscopy;photo dynamic diagnosis;realtimeframe;cystoscopy;tumours;tumours cancer endoscopes image colour analysis medical image processing object detection;white light source;clinical evaluation;tumor detection;indexing terms;cancerous tissue;white light;visual contrast;nonlinear transformation;image color analysis;image colour analysis;bladder;medical image processing;endoscopes;tumors;software framework;linear transformation;bladder tumors;fluorescing pixels;color space transformation;object detection;real time systems;image color analysis fluorescence bladder tumors endoscopes real time systems eigenvalues and eigenfunctions	Bladder tumors are difficult to recognize during an endoscopy of the bladder (cystoscopy) using a conventional white light source. Therefore, a technique called photo dynamic diagnosis (PDD) or fluorescence endoscopy is used to enhance both sensitivity and specificity of tumor detection. As preparation for a PDD cystoscopy, a marker substance is brought into the patient's bladder which leads to a fluorescence of cancerous tissue if a suitable light source is used. Even though tumors are better distinguishable from healthy bladder tissue, the contrast between both types of tissue can still be low and it further decreases with the decay of the marker substance. To overcome these difficulties, we enhance the contrast between fluorescence and background. To this end, we perform a color space transformation which separates fluorescence information from the remaining information. In this new color space, we apply a non-linear transformation to the fluorescence component which leads to a better separation of fluorescing and non-fluorescing pixels. Finally, the image is transferred back to the RGB color space. For clinical evaluation, we implemented the proposed algorithm as plug-in for the real-time capable software framework RealTimeFrame.	algorithm;color space;nonlinear system;pixel;plug-in (computing);real-time clock;sensitivity and specificity;software framework	Thomas Stehle;Alexander Behrens;Til Aach	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607490	computer vision;index term;fluorescence;computer science;rgb color space;software framework;linear map;color space;cancer	Robotics	39.600750673071516	-75.84202574759566	156329
59a9c0ff30bd0091b8c235cff7b9560d969c127f	coupled multi-shape model and mutual information for medical image segmentation	parametric model;distance function;image segmentation;cost function;principal component analysis;mutual information;shape modeling;medical image segmentation;active contour model	This paper presents extensions which improve the performance of the shape-based deformable active contour model presented earlier in [9]. In contrast to that work, the segmentation framework that we present in this paper allows multiple shapes to be segmented simultaneously in a seamless fashion. To achieve this, multiple signed distance functions are employed as the implicit representations of the multiple shape classes within the image. A parametric model for this new representation is derived by applying principal component analysis to the collection of these multiple signed distance functions. By deriving a parametric model in this manner, we obtain a coupling between the multiple shapes within the image and hence effectively capture the co-variations among the different shapes. The parameters of the multi-shape model are then calculated to minimize a single mutual information-based cost functional for image segmentation. The use of a single cost criterion further enhances the coupling between the multiple shapes as the deformation of any given shape depends, at all times, upon every other shape, regardless of their proximity. We demonstrate the utility of this algorithm to the segmentation of the prostate gland, the rectum, and the internal obturator muscles for MR-guided prostate brachytherapy.	active contour model;brachytherapy;class;image segmentation;internal obturator muscle structure;mathematical optimization;medical image;mutual information;numerous;obturators;parametric model;principal component analysis;rectum;seamless3d;signature;algorithm;biologic segmentation	Andy Tsai;William M. Wells;Clare M. Tempany;W. Eric L. Grimson;Alan S. Willsky	2003	Information processing in medical imaging : proceedings of the ... conference	10.1007/978-3-540-45087-0_16	image texture;computer vision;parametric model;metric;computer science;machine learning;segmentation-based object categorization;pattern recognition;active contour model;mathematics;region growing;image segmentation;mutual information;minimum spanning tree-based segmentation;scale-space segmentation;statistics;principal component analysis	Vision	44.26265295117688	-76.4188916756912	156429
dc69aaeaf633bebda77c3ee56b9ef925ac9108be	segmentation of embryonic and fetal 3d ultrasound images based on pixel intensity distributions and shape priors	multi phase level set segmentation;3d ultrasound;anatomical models;obstetric imaging;statistical shape priors	This paper presents a novel variational segmentation framework combining shape priors and parametric intensity distribution modeling for extracting the fetal envelope on 3D obstetric ultrasound images. To overcome issues related to poor image quality and missing boundaries, we inject three types of information in the segmentation process: tissue-specific parametric modeling of pixel intensities, a shape prior for the fetal envelope and a shape model of the fetus' back. The shape prior is encoded with Legendre moments and used to constraint the evolution of a level-set function. The back model is used to post-process the segmented fetal envelope. Results are presented on 3D ultrasound data and compared to a set of manual segmentations. The robustness of the algorithm is studied, and both visual and quantitative comparisons show satisfactory results obtained by the proposed method on the tested dataset.	3d computer graphics;algorithm;fetus;image quality;image segmentation;pixel;silo (dataset);variational principle;biologic segmentation	Sonia Dahdouh;Elsa D. Angelini;Gilles Grange;Isabelle Bloch	2015	Medical image analysis	10.1016/j.media.2014.12.005	computer vision;pattern recognition	Vision	43.78000660744255	-78.46261392276168	156469
3bad98f9c6bfbf2e73992c09ed6e6be009c716e7	registration of 3d multi-modality medical images using surfaces and point landmarks	multi modality;traitement;tratamiento;medical imagery;treatment;image processing;geometry;volume;procesamiento imagen;geometrie;outlier;psi_mic;intelligence artificielle;punto;traitement image;algorithme;observacion aberrante;algorithm;point;dot;volumen;multimodality;geometric registration;medical image;outlier treatment;imagerie medicale;pattern recognition;superficie;observation aberrante;artificial intelligence;surface;geometria;imageneria medical;inteligencia artificial;reconnaissance forme;reconocimiento patron;multi modalite;algoritmo	An algorithm is presented for the geometric registration of 3D multi-modality medical images using a weighted combination of point landmarks and anatomical surfaces. The algorithm includes a new fully automatic outlier treatment method.	algorithm;medical imaging;modality (human–computer interaction)	André Collignon;Dirk Vandermeulen;Paul Suetens;Guy Marchal	1994	Pattern Recognition Letters	10.1016/0167-8655(94)90137-6	computer vision;point;outlier;image processing;computer science;artificial intelligence;mathematics;geometry;surface;volume	Vision	46.039896533804736	-79.07352928575138	156489
c75e2150251d6e9f95249962b749287bb68c3e6d	a supervoxel based random forest synthesis framework for bidirectional mr/ct synthesis	ct;jlf;mr;mrf;random forest;segmentation;synthesis	Synthesizing magnetic resonance (MR) and computed tomography (CT) images (from each other) has important implications for clinical neuroimaging. The MR to CT direction is critical for MRI-based radiotherapy planning and dose computation, whereas the CT to MR direction can provide an economic alternative to real MRI for image processing tasks. Additionally, synthesis in both directions can enhance MR/CT multi-modal image registration. Existing approaches have focused on synthesizing CT from MR. In this paper, we propose a multi-atlas based hybrid method to synthesize T1-weighted MR images from CT and CT images from T1-weighted MR images using a common framework. The task is carried out by: (a) computing a label field based on supervoxels for the subject image using joint label fusion; (b) correcting this result using a random forest classifier (RF-C); (c) spatial smoothing using a Markov random field; (d) synthesizing intensities using a set of RF regressors, one trained for each label. The algorithm is evaluated using a set of six registered CT and MR image pairs of the whole head.	algorithm;ct scan;cervical atlas;computation (action);emoticon;image processing;image registration;markov chain;markov random field;modal logic;neuroimaging;numerous;radio frequency;random forest;resonance;smoothing (statistical technique);x-ray computed tomography	Can Zhao;Aaron Carass;Junghoon Lee;Amod Jog;Jerry L. Prince	2017	Simulation and synthesis in medical imaging : second International Workshop, SASHIMI 2017, Held in Conjunction with MICCAI 2017, Quebec City, QC, Canada, September 10, 2017, Proceedings. SASHIMI (Workshop) (2nd : 2017 : Quebec City, Can...	10.1007/978-3-319-68127-6_4	markov random field;computer science;pattern recognition;computer vision;image processing;artificial intelligence;neuroimaging;random forest;image registration;magnetic resonance imaging;segmentation;smoothing	Vision	40.801717651808595	-79.15907107379773	156492
6179689e093d269c48574213a41acb7a4d3a8c6d	advanced level-set-based cell tracking in time-lapse fluorescence microscopy	level sets;fluorescence microscopy cells biology image segmentation level set biological system modeling evolution biology robustness labeling biology computing;cell segmentation;fluorescence;image segmentation;level set;algorithms bayes theorem biological markers cell division cell movement cell separation databases factual fluorescent dyes hela cells humans luminescent proteins microscopy fluorescence reproducibility of results sensitivity and specificity;cell tracking;fluorescence microscopy;medical image processing;cell migration;cell division advanced level set cell tracking time lapse fluorescence microscopy cell segmentation cell migration cell proliferation mitosis multiple level set method cell biological imaging coupled active surfaces algorithm;cell division;multiple object tracking cell segmentation cell tracking fluorescence microscopy level sets;cellular transport;biomedical optical imaging;multiple object tracking;level set method;optical microscopy biomedical optical imaging cellular transport fluorescence image segmentation medical image processing;optical microscopy	Cell segmentation and tracking in time-lapse fluorescence microscopy images is a task of fundamental importance in many biological studies on cell migration and proliferation. In recent years, level sets have been shown to provide a very appropriate framework for this purpose, as they are well suited to capture topological changes occurring during mitosis, and they easily extend to higher dimensional image data. This model evolution approach has also been extended to deal with many cells concurrently. Notwithstanding its high potential, the multiple-level-set method suffers from a number of shortcomings, which limit its applicability to a larger variety of cell biological imaging studies. In this paper, we propose several modifications and extensions to the coupled-active-surfaces algorithm, which considerably improve its robustness and applicability. Our algorithm was validated by comparing it to the original algorithm and two other cell segmentation algorithms. For the evaluation, four real fluorescence microscopy image datasets were used, involving different cell types and labelings that are representative of a large range of biological experiments. Improved tracking performance in terms of precision (up to 11%), recall (up to 8%), ability to correctly capture all cell division events, and computation time (up to nine times reduction) is achieved.	algorithmic efficiency;cell division;computation;computation (action);definition;distance transform;energy minimization;experiment;large;leukemia, b-cell;medical imaging;mental suffering;microscopy, fluorescence;migration, cell;mitosis;numerous;propagation of uncertainty;radon;robustness (computer science);sensor;software propagation;solutions;time complexity;watershed (image processing);weight;algorithm;biologic segmentation;percent positive cells	Oleh Dzyubachyk;Wiggert A. van Cappellen;Jeroen Essers;Wiro J. Niessen;Erik H. W. Meijering	2010	IEEE Transactions on Medical Imaging	10.1109/TMI.2009.2038693	computer vision;computer science;level set;mathematics	Visualization	39.91973975802127	-73.36644635381113	156631
326329b46f31f15e6583b7ea26313d0c6edbb87c	on second order operators and quadratic operators	second order;eigenvalues and eigenfunctions;detectors;network theory graphs mathematical operators;tensile stress;image processing;approximation algorithms;computer vision and image processing;motion estimation;image processing pattern recognition computer vision tensile stress motion estimation motion detection detectors machine learning algorithms image converters machine learning;mathematical operators;engineering and technology;teknik och teknologier;linear operator;machine learning;principal component analysis;sum of squares;pattern recognition;network theory graphs;3d structure;linear operators second order operators quadratic operators pattern recognition computer vision image processing second order networks 3d structure tensor motion estimation harris corner detector weighted quadratic operator machine learning	In pattern recognition, computer vision, and image processing, many approaches are based on second order operators. Well-known examples are second order networks, the 3D structure tensor for motion estimation, and the Harris corner detector. A subset of second order operators are quadratic operators. It is lesser known that every second order operator can be written as a weighted quadratic operator. The contribution of this paper is to propose an algorithm for converting an arbitrary second order operator into a quadratic operator. We apply the method to several examples from image processing and machine learning. The advantages of the alternative implementation by quadratic operators is two-fold: The underlying linear operators allow new insights into the theory of the respective second order operators and replacing second order networks with sums of squares of linear networks reduces significantly the computational burden when the trained network is in operation phase.	algorithm;computer vision;corner detection;harris affine region detector;image processing;machine learning;mike lesser;motion estimation;pattern recognition;quadratic function;structure tensor	Michael Felsberg	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761685	microlocal analysis;constant coefficients;computer vision;mathematical optimization;spectral theorem;detector;quasinormal operator;operator theory;compact operator on hilbert space;image processing;operator norm;computer science;operator;machine learning;motion estimation;mathematics;linear map;explained sum of squares;stress;nuclear operator;fourier integral operator;second-order logic;principal component analysis	Vision	52.51103784547167	-70.76584620592945	156698
1574091e5569b91db7415375dc527e0c9012ae94	a method for mapping tissue volume model onto target volume using volumetric self-organizing deformable model	tissues;self organizing deformable model;statistical volume model;volume model correspondence;tissue volume model;geometrical feature preserving mapping	ABSTRACT This paper proposes a new method for mapping volume models of human tissues onto a target volume withsimple shapes. The proposed method is based on our modi ed self-organizing deformable model (mSDM) 1,2 which nds the one-to-one mapping with no foldovers between arbitrary surface model and a target surface. Byextending mSDM to apply to volume models, the proposed method, called volumetric SDM (vSDM), establishesthe one-to-one correspondence between the tissue volume model and its target volume. At the same time, vSDMcan preserve geometrical properties of the original model before and after mapping. This characteristic of vSDMmakes it easy to nd the correspondence between tissue models.Keywords: tissue volume model, statistical volume model, self-organizing deformable model, volume modelcorrespondence, geometrical feature preserving mapping, 1. PURPOSE Statistical shape model (SSM) identi es the considerable natural variability and commonality of the shapes ofhuman tissues by statistical analysis. Since SSM provides the prior information about the tissue shape, theSSM-based techniques have obtained considerable success in the segmentation of tissue shapes.	organizing (structure);self-organization	Shoko Miyauchi;Ken'ichi Morooka;Tokuo Tsuji;Yasushi Miyagi;Takaichi Fukuda;Ryo Kurazume	2016		10.1117/12.2217367	computer vision;artificial intelligence	HCI	43.852245500296334	-77.91316840253785	156707
01600f16ec012dbe54e1f177e09095efa0158ea2	segmenting subcellular structures in histology tissue images	minimization;polynomial time subcellular structure segmentation histology tissue images nucleus cytoplasm immune cells cell type classification intraclass variations interclass similarity complex image background crowded image background noisy data energy minimization framework photometric priors geometric priors layered graph model energy minimization problem;image segmentation;bismuth;plasmas;minimisation biomedical optical imaging cellular biophysics image segmentation medical image processing;image edge detection;feature extraction;image segmentation feature extraction cells biology plasmas image edge detection minimization bismuth;cells biology	Pathologists often rely on features of the subcellular structures, i.e., nucleus and cytoplasm, to differentiate different types of immune cells. Accurate segmentation of the sub-cellular structures can help classify cell types. However, segmentation in histology tissue images is very challenging, due to, intra-class variations and inter-class similarity, complex and crowded background, and noisy data. In this paper, we propose a novel energy minimization framework with constraints, that can easily incorporate various photometric and geometric priors to alleviate such challenges. We also propose a novel layered graph model to solve our energy minimization problem optimally in polynomial time. Experiments on clinical data demonstrate our method obtain better results on both segmentation, and classification of cell types using features extracted from the segmented subcellular structures, than other popular state-of-the-art general segmentation methods.	energy minimization;modality (human–computer interaction);polynomial;signal-to-noise ratio;time complexity	Jiazhuo Wang;John D. MacKenzie;Rageshree Ramachandran;Yongzhi Zhang;Huaqing Wang;Danny Ziyi Chen	2015	2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2015.7163934	plasma;computer vision;feature extraction;computer science;machine learning;segmentation-based object categorization;bismuth;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	42.16151702795166	-75.1914708392549	156983
aaa2bd8c2b1a9f76b2e286d48e917a9dbc013738	pixel clustering for color image segmentation		Image segmentation using a hierarchical sequence of piecewise constant approximations that minimally differ from the original image in terms of the total squared error is discussed. It is proposed to obtain these approximations by two combined clustering and segmentation methods based on clustering image pixels using Ward’s method. In the first method, the number of segments in clusters is reduced in the course of hierarchical clustering by reclassifying pixels from one cluster to another. In the second method, a limited number of superpixels representing connected segments of the image are formed by enlarging source pixels, and then the superpixels are clusterized by Ward’s method. To decompose the image into superpixels, the segmentation quality is improved while preserving the number of segments. As a result, a noticeable improvement in the quality of image approximations is achieved, and their invariant encoding gives a marking of the image for subsequent object detection.	algorithm;approximation;cluster analysis;color image;computation;computer;hierarchical clustering;image segmentation;item unique identification;object detection;pixel;time complexity;ward's method;whole earth 'lectronic link	Mikhail V. Kharinov	2015	Programming and Computer Software	10.1134/S0361768815050047	image texture;computer vision;range segmentation;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	45.48199601590782	-67.64010151058568	157088
59a1625fab0151e9f92eaef1fe8e30fe3c4bbae3	shape matching based on diffusion embedding and on mutual isometric consistency	eigenvalues and eigenfunctions;shape laplace equations kernel eigenvalues and eigenfunctions robustness sampling methods topology euclidean distance histograms visual databases;3d shape matching;time scale;kernel;diffusion embedding;manifolds;shape descriptor;point to point;image matching;scale space local shape descriptor;shape recognition;heating;eigenvalues and eigenvectors;laplace equations;scale space;shape;three dimensional displays;shape matching;feature extraction;point to point match;unit hypersphere normalization;discrete diffusion operator;shape recognition eigenvalues and eigenfunctions image matching;mutual isometric consistency;point to point match diffusion embedding mutual isometric consistency 3d shape matching discrete diffusion operator scale space local shape descriptor unit hypersphere normalization	In this paper we address the problem of matching two 3D shapes by representing them using the eigenvalues and eigenvectors of the discrete diffusion operator. This provides representation framework useful for scale-space local shape descriptors and shape comparisons. We formally introduce diffusion embedding and we propose unit hypersphere normalizations of this embedding. We also propose a method to find compatible time scale while matching two different shapes with varying size and sampling. We propose a practical algorithm that seeks the largest set of mutually consistent point-to-point matches between two shapes based on isometric consistency between the two embedded shapes. We illustrate our method with several examples of matching shapes at various scales.	algorithm;embedded system;isometric projection;point-to-point protocol;sampling (signal processing);scale space;sparse matrix	Avinash Sharma;Radu Horaud	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops	10.1109/CVPRW.2010.5543278	computer vision;mathematical optimization;kernel;scale space;topology;manifold;feature extraction;eigenvalues and eigenvectors;shape;point-to-point;computer science;shape analysis;mathematics;geometry	Vision	49.72784603218527	-70.40053592539327	157169
2a78cc5db6993752408cf3d3b0edd8be3c6c9df7	kernel density estimation and intrinsic alignment for shape priors in level set segmentation	nonparametric density estimation;closed form solution;image segmentation;transformation group;dissimilarity measure;kernel density estimation;bayesian inference;level set;left ventricle;ultrasound imaging;kernel density estimate;level set methods;shape priors;level set method;alignment;gaussian distribution	In this paper, we make two contributions to the field of level set based image segmentation. Firstly, we propose shape dissimilarity measures on the space of level set functions which are analytically invariant under the action of certain transformation groups. The invariance is obtained by an intrinsic registration of the evolving level set function. In contrast to existing approaches to invariance in the level set framework, this closed-form solution removes the need to iteratively optimize explicit pose parameters. The resulting shape gradient is more accurate in that it takes into account the effect of boundary variation on the object’s pose. Secondly, based on these invariant shape dissimilarity measures, we propose a statistical shape prior which allows to accurately encode multiple fairly distinct training shapes. This prior constitutes an extension of kernel density estimators to the level set domain. In contrast to the commonly employed Gaussian distribution, such nonparametric density estimators are suited to model aribtrary distributions. We demonstrate the advantages of this multi-modal shape prior applied to the segmentation and tracking of a partially occluded walking person in a video sequence, and on the segmentation of the left ventricle in cardiac ultrasound images. We give quantitative results on segmentation accuracy and on the dependency of segmentation results on the number of training shapes.	data domain;encode;euler;euler–lagrange equation;experiment;gradient;image scaling;image segmentation;iterative method;kernel density estimation;local coordinates;mathematical optimization;modal logic;normal mode;numerical analysis;statistical shape analysis	Daniel Cremers;Stanley Osher;Stefano Soatto	2006	International Journal of Computer Vision	10.1007/s11263-006-7533-5	active shape model;kernel density estimation;computer vision;mathematical optimization;pattern recognition;mathematics;image segmentation;scale-space segmentation;statistics	Vision	49.40908618076448	-71.45310674569556	157174
a387905d4b9efd13e118ba43cd01d23dace0c0f9	fully automatic deformable model integrating edge, texture and shape - application to cardiac images segmentation				Clément Beitone;Christophe Tilmant;Frédéric Chausse	2015		10.5220/0005304005170522	computer vision;pattern recognition	Vision	44.99499157106661	-74.89053000762277	157431
02dbf6950ce2a7c4a2a2b32e0753d653d9fc4210	medical image computing and computer-assisted intervention – miccai 2012		Model-based segmentation approaches have been proven to produce very accurate segmentation results while simultaneously providing an anatomic labeling for the segmented structures. However, variations of the anatomy, as they are often encountered e.g. on the drainage pattern of the pulmonary veins to the left atrium, cannot be represented by a single model. Automatic model selection extends the model-based segmentation approach to handling significant variational anatomies without user interaction. Using models for the three most common anatomical variations of the left atrium, we propose a method that uses an estimation of the local fit of different models to select the best fitting model automatically. Our approach employs the support vector machine for the automatic model selection. The method was evaluated on 42 very accurate segmentations of MRI scans using three different models. The correct model was chosen in 88.1 % of the cases. In a second experiment, reflecting average segmentation results, the model corresponding to the clinical classification was automatically found in 78.0 % of the cases.	ct scan;medical image computing;model selection;support vector machine;variational principle	Gerhard Goos;Juris Hartmanis;Jan van Leeuwen;Nicholas Ayache;Hervé Delingette;Polina Golland;Kensaku Mori	2012		10.1007/978-3-642-33418-4	simulation;multimedia;medical physics	Vision	41.73357655084716	-79.1615172731924	157583
e286f52702ddf07b1d3a53ba3c2b9e22690fca9b	multiscale signal enhancement: beyond the normality and independence assumption	transformation ondelette;traitement signal;nonlinear filters;probability;non linear filtering;normal distribution;methode echelle multiple;piecewise smooth;filtrado no lineal;multiscale analysis;prior information;nonlinear filter;metodo escala multiple;indexing terms;reduccion ruido;wavelet transforms;image enhancement;smoothing methods;scale space;noisy image multiscale signal enhancement normality assumption independence assumption normally distributed perturbations probability distribution noise process nonlinear filtering wavelet domain wavelet coefficients suppression curvature extrema scale space representation piecewise smooth signal curvature mask filtering signal pointwise holder exponents singular points multiscale analysis human visual system wavelet transform;signal processing;noise reduction;probability distribution;wavelet coefficients filtering wavelet analysis signal analysis signal processing minimax techniques noise reduction probability distribution image analysis information analysis;reduction bruit;multiscale method;transformacion ondita;smoothing methods image enhancement probability nonlinear filters noise wavelet transforms;procesamiento senal;wavelet transformation;filtrage non lineaire;noise;singular point	Current approaches to denoising or signal enhancement in a wavelet-based framework have generally relied on the assumption of normally distributed perturbations. In practice, this assumption is often violated and sometimes prior information of the probability distribution of a noise process is not even available. To relax this assumption, we propose a novel nonlinear filtering technique in this paper. The key idea is to project a noisy signal onto a wavelet domain and to suppress wavelet coefficients by a mask derived from curvature extrema in its scale space representation. For a piecewise smooth signal, it can be shown that filtering by this curvature mask is equivalent to preserving the signal pointwise Hölder exponents at the singular points and lifting its smoothness elsewhere.		Yun He;Hamid Krim	2002	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2002.999676	normal distribution;probability distribution;nonlinear filter;computer vision;mathematical optimization;singular point of a curve;mathematical analysis;scale space;index term;computer science;noise;signal processing;noise reduction;probability;mathematics;statistics;wavelet transform	Visualization	53.448998097583065	-67.31045968844265	157639
59f64f849231caeb2fe97e005ea58adc8d889200	multiple lrek active contours for knee meniscus ultrasound image segmentation	kernel;active contour;multiregion segmentation;image segmentation;ultrasound;ultrasonic imaging;kernel image segmentation active contours level set ultrasonic imaging force joints;level set;multiregion segmentation knee joint meniscus ultrasound image segmentation active contour level set;active contours;joints;force;meniscus;knee joint	Quantification of knee meniscus degeneration and displacement in an ultrasound image requires simultaneous segmentation of femoral condyle, meniscus, and tibial plateau in order to determine the area and the position of the meniscus. In this paper, we present an active contour for image segmentation that uses scalable local regional information on expandable kernel (LREK). It includes using a strategy to adapt the size of a local window in order to avoid being confined locally in a homogeneous region during the segmentation process. We also provide a multiple active contours framework called multiple LREK (MLREK) to deal with multiple object segmentation without merging and overlapping between the neighboring contours in the shared boundaries of separate regions. We compare its performance to other existing active contour models and show an improvement offered by our model. We then investigate the choice of various parameters in the proposed framework in response to the segmentation outcome. Dice coefficient and Hausdorff distance measures over a set of real knee meniscus ultrasound images indicate a potential application of MLREK for assessment of knee meniscus degeneration and displacement.	active contour model;arabic numeral 0;autostereogram;bone structure of tibia;calculus of variations;concave function;condyle of femur;convergence (action);displacement mapping;first variation;gradient;hausdorff dimension;image segmentation;kernel;mathematical optimization;meniscus structure of joint;multiple personality disorder;physical object;plug (physical object);population parameter;psychologic displacement;quantitation;scalability;segmentation action;structure of condyle;sørensen–dice coefficient;window function;biologic segmentation	Amir Faisal;Siew-Cheok Ng;Siew-Li Goh;John George;Eko Supriyanto;Khin Wee Lai	2015	IEEE Transactions on Medical Imaging	10.1109/TMI.2015.2425144	computer vision;meniscus;kernel;radiology;computer science;level set;ultrasound;active contour model;mathematics;image segmentation;scale-space segmentation;force;anatomy	Vision	44.71108496094187	-76.65112394351297	157885
327f37f8f6a6398d5697c9d8cc8e8448f3e343eb	level set methods for dynamic tomography	coordinate descent algorithm;dynamic tomography;dynamic object reconstruction;image reconstruction;object-based scene model;one-to-one mapping;energy function minimization;single photon emission computed tomography;differentiable mapping;level set methods;minimisation;medical image processing;inverse problems;layout;tomography;level set method;nuclear medicine;one to one mapping;level set;information systems;pixel	In this paper, we propose a novel variational framework for the reconstruction of dynamic objects from sparse and noisy tomographic data. Using an object-based scene model, we developed a general object dynamic model based on a one to one and differentiable mapping. We then propose a novel distance between curves to incorporate the object dynamics into the variational framework. For the minimization of the energy function, we developed a coordinate descent algorithm based on the level set methods. Experimental results for reconstructing a sequence of multiple dynamic objects are presented.	algorithm;calculus of variations;coordinate descent;mathematical model;mathematical optimization;object-based language;sparse matrix;tomography;variational principle	Yonggang Shi;W. Clem Karl	2004	2004 2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro (IEEE Cat No. 04EX821)		iterative reconstruction;layout;computer vision;minimisation;mathematical optimization;radiology;medicine;computer science;inverse problem;level set;theoretical computer science;injective function;mathematics;tomography;information system;level set method;pixel	Vision	50.438721544730825	-74.7263981471653	158199
862ce164d283a45ec2f3712118f9395617206509	image understanding algorithms for segmentation evaluation and region-of-interest identification using bayesian networks	image features;bayesian network;networks;image segmentation;image understanding;cell phones;segmentation evaluation;mobile phone;image compression;region of interest;algorithms;color image	A two-fold image understanding algorithm based on Bayesian networks is introduced. The methodology has modules for image segmentation evaluation and region of interest (ROI) identification. The former uses a set of segmentation maps (SMs) of a target image to identify the optimal one. These SMs could be generated from the same segmentation algorithm at different thresholds or from different segmentation techniques. Global and regional low-level image features are extracted from the optimal SM and used along with the original image to identify the ROI. The proposed algorithm was tested on a set of 4000 color images that are publicly available and compared favorably to the state-of-the-art techniques. Applications of the proposed framework include image compression, image summarization, mobile phone imagery, digital photo cropping, and image thumb-nailing.© (2011) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	algorithm;bayesian network;image segmentation	Mustafa I. Jaber;Eli Saber	2011		10.1117/12.887046	image quality;image texture;computer vision;feature detection;color image;binary image;image processing;image compression;computer science;segmentation-based object categorization;digital image processing;bayesian network;data mining;multimedia;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;automatic image annotation;feature;region of interest	AI	40.829632323764166	-66.26521885247315	158308
2ca26898bfa1c28c1a36bcf694e4e86a2c8dd7c3	image reconstruction algorithm for modified sart based on pm model	pm model;standards;image reconstruction mathematical model noise reduction image edge detection data models signal to noise ratio standards;image edge detection;image reconstruction;noise reduction;peak signal to noise ratio image reconstruction algorithm modified sart algorithm pm model ct image reconstruction simultaneous algebraic reconstruction technique noise interference image quality diffusion coefficient denoising principle psnr;image reconstruction algebra image denoising;mathematical model;simultaneous algebraic reconstruction technique;image reconstruction pm model simultaneous algebraic reconstruction technique;signal to noise ratio;data models	Simultaneous Algebraic reconstruction technique (SART) has been widely applied to CT image reconstruction, but it is influenced by the noise interference, and as a result, the image quality is affected. In this article, we propose a modified SART algorithm to improve the quality of the image reconstruction, which constructs a new diffusion coefficient by the PM model, and makes the SART algorithm and PM model alternately according to the denoising principle of PM model. Experimental results demonstrate that the scheme can reduce the effect of noise on image quality substantially, increase the value of SNR and PSNR, and improve the performance of image reconstruction significantly.	algorithm;ct scan;coefficient;image quality;interference (communication);iterative reconstruction;noise reduction;peak signal-to-noise ratio;simultaneous algebraic reconstruction technique	Jie Liang;Wenzhang He;Dongjiang Ji;Lina Wang	2015	2015 11th International Conference on Computational Intelligence and Security (CIS)	10.1109/CIS.2015.50	iterative reconstruction;image noise;data modeling;computer vision;mathematical optimization;speech recognition;computer science;noise reduction;mathematical model;mathematics;signal-to-noise ratio	Vision	53.652281514392655	-75.35388799920504	158353
fa6d599744f7df520243dc53fc62ed996508e2b1	analysis of stroke structures of handwritten chinese characters	input device;character recognition writing algorithm design and analysis image recognition geometrical optics optical character recognition software data mining approximation algorithms euclidean distance approximation methods;approximation method;rule based;euclidean distance;indexing terms;geometric feature;stroke segmentation handwritten chinese characters stroke structures character recognition on line recognition system off line recognition handwritten chinese character thinning algorithm euclidean distance transformation gradient oriented tracing line approximation curvature segmentation artifact removal;handwritten chinese character recognition;character sets handwritten character recognition;character sets;character recognition;handwritten character recognition	Most handwritten Chinese character recognition systems suffer from the variations in geometrical features for different writing styles. The stroke structures of different styles have proved to be more consistent than geometrical features. In an on-line recognition system, the stroke structure can be obtained according to the sequences of writing via a pen-based input device such as a tablet. But in an off-line recognition system, the input characters are scanned optically and saved as raster images, so the stroke structure information is not available. In this paper, we propose a method to extract strokes from an off-line handwritten Chinese character. We have developed four new techniques: 1) a new thinning algorithm based on Euclidean distance transformation and gradient oriented tracing, 2) a new line approximation method based on curvature segmentation, 3) artifact removal strategies based on geometrical analysis, and 4) stroke segmentation rules based on splitting, merging and directional analysis. Using these techniques, we can extract and trace the strokes in an off-line handwritten Chinese character accurately and efficiently.	algorithm;approximation;cerebrovascular accident;euclidean distance;gradient;input device;online and offline;optical character recognition;personality character;raster graphics;rule (guideline);scanning;tablet dosage form;tablet computer;thinning;biologic segmentation	Hung-Hsin Chang;Hong Yan	1999	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.740165	rule-based system;computer vision;speech recognition;index term;character encoding;intelligent character recognition;computer science;intelligent word recognition;pattern recognition;euclidean distance;input device	Robotics	44.62134736778827	-71.71504575474594	158391
4566a20186399615af64437b3b0bfb3f964fb964	reconstructing the 3-d medial axes of coronary arteries in single-view cineangiograms	simulation ordinateur;image tridimensionnelle;processing;medical diagnostic imaging 3d medial axes reconstruction coronary arteries single view cineangiograms skeletal structure local features arterial segments correspondence similarity measure 2d image space angular coordinates change depth coordinates 3d angular coordinates reconstruction error 3d scaling transformation software models successive frames;exploration radiologique;modele mathematique;image processing;radiology and nuclear medicine;genie biomedical;organs;body;arteries;cardiology;artere coronaire;exploracion radiologica;modelo matematico;coronarographie;blood vessel;three dimensional;radiologic investigation;cardiovascular diseases;algorithme;ionizing radiations;angiography;algorithm;reconstruction image;biomedical engineering;reconstruccion imagen;gradient descent;local features;image reconstruction;medical image processing;three dimensional calculations;coronary arteriography;tecnica;mathematical model;x radiation;diseases;tridimensional image;coronarografia;ingenieria biomedica;cardiovascular system;simulacion computadora;ionizing radiation;cineangiography;radiations 550602 medicine external radiation in diagnostics 1980;coronary artery;arteries image reconstruction ultrasonic imaging noninvasive treatment coronary arteriosclerosis echocardiography spatial resolution visualization angiography lifting equipment;angiographie;diagnosis;angiografia;cineangiographie;similarity measure;coronaries;computer simulation;technique;diagnostic radiography;blood vessels;electromagnetic radiation;imagen tridimensional;arteria coronaria;diagnostic radiography image reconstruction medical image processing cardiology;algoritmo	Describes a technique for reconstructing the skeletal structure of coronary arteries from a succession of frames of a single-view cineangiogram. The authors use local features in each frame to determine correspondences of arterial segments in successive frames. They define a similarity measure in 2D image space as the change in angular coordinates of corresponding pairs. They use a form of gradient descent to find those depth coordinates that minimize the average deviation of the 3D angular coordinates of all points on the skeleton from the coordinates produced by a 3D scaling transformation. In experiments with software models the reconstruction error was approximately two pixels when the initial guessed reconstruction was as large as 30 pixels.		Thinh V. Nguyen;Jack Sklansky	1994	IEEE transactions on medical imaging	10.1109/42.276145	iterative reconstruction;computer simulation;gradient descent;three-dimensional space;computer vision;electromagnetic radiation;radiology;medicine;image processing;computer science;processing;circulatory system;mathematical model;mathematics;ionizing radiation;nuclear medicine;algorithm	Vision	45.620323560470915	-79.74876187835933	158407
c75a6cfd6bb35f73833543ea6079de6dac2b30fd	3d brain tumor segmentation in mri using fuzzy classification, symmetry analysis and spatially constrained deformable models	engineering;fuzzy classification;resonance;fuzzy set;procesamiento informacion;spatial relations;resonancia;conjunto difuso;ensemble flou;segmentation;classification;magnetic resonance image;brain tumor;ingenierie;symmetry plane;spatial relation;general methods;information processing;ingenieria;sistema difuso;systeme flou;35b34;traitement information;deformable model;clasificacion;segmentacion;fuzzy system;fuzzy model	We propose a new general method for segmenting brain tumors in 3D magnetic resonance images. Our method is applicable to different types of tumors. First, the brain is segmented using a new approach, robust to the presence of tumors. Then a first tumor detection is performed, based on selecting asymmetric areas with respect to the approximate brain symmetry plane and fuzzy classification. Its result constitutes the initialization of a segmentation method based on a combination of a deformable model and spatial relations, leading to a precise segmentation of the tumors. Imprecision and variability are taken into account at all levels, using appropriate fuzzy models. The results obtained on different types of tumors have been evaluated by comparison with manual segmentations. © 2008 Elsevier B.V. All rights reserved.	approximation algorithm;fuzzy classification;fuzzy concept;heart rate variability;refinement (computing);resonance	Hassan Khotanlou;Olivier Colliot;Jamal Atif;Isabelle Bloch	2009	Fuzzy Sets and Systems	10.1016/j.fss.2008.11.016	spatial relation;computer vision;information processing;computer science;artificial intelligence;machine learning;mathematics;fuzzy control system	Vision	42.74398934359073	-71.74341434243664	158474
8d3299b6db4660feee7a5939d539d4adde55fd46	bidirectional composition on lie groups for gradient-based image alignment	union bidireccional;bcl method;evaluation performance;lie groups gradient methods image representation;convergence;alignement sequence;liaison bidirectionnelle;performance evaluation;image processing;bidirectional composition;bidirectional formulation;complexite calcul;image alignment;gradient method;image matching;groupe lie;estudio comparativo;relacion convergencia;evaluacion prestacion;video compression;procesamiento imagen;taux convergence;convergence rate;minimization methods;alineacion secuencia;lie groups bidirectional image alignment gradient methods image registration;estimacion a priori;traitement image;registro imagen;etude comparative;a priori estimation;methode gradient;etat actuel;complejidad computacion;gradient based image alignment;performance improvement;noise level;night vision;recalage image;image registration bidirectional composition lie groups gradient based image alignment bcl method bidirectional formulation state of the art gradient based approaches computational complexity;metodo gradiente;computational complexity;grupo lie;image representation;image registration;state of the art;digital filters;comparative study;estimation a priori;gradient methods;lie group;estado actual;noise level convergence image registration video compression night vision digital filters spatial resolution minimization methods computational complexity frequency;sequence alignment;bidirectional image alignment;lie groups;numerical experiment;point of view;frequency;bidirectional link;appariement image;state of the art gradient based approaches;spatial resolution	In this paper, a new formulation based on bidirectional composition on Lie groups (BCL) for parametric gradient-based image alignment is presented. Contrary to the conventional approaches, the BCL method takes advantage of the gradients of both template and current image without combining them a priori. Based on this bidirectional formulation, two methods are proposed and their relationship with state-of-the-art gradient based approaches is fully discussed. The first one, i.e., the BCL method, relies on the compositional framework to provide the minimization of the compensated error with respect to an augmented parameter vector. The second one, the projected BCL (PBCL), corresponds to a close approximation of the BCL approach. A comparative study is carried out dealing with computational complexity, convergence rate and frequence of convergence. Numerical experiments using a conventional benchmark show the performance improvement especially for asymmetric levels of noise, which is also discussed from a theoretical point of view.	alignment;approximation;bcl-xl protein;benchmark (computing);binary combinatory logic;clinical use template;computational complexity theory;convergence (action);experiment;gradient;numerical method;population parameter;projections and predictions;rate of convergence	Rémi Mégret;Jean-Baptiste Authesserre;Yannick Berthoumieu	2010	IEEE Transactions on Image Processing	10.1109/TIP.2010.2048406	computer vision;image processing;computer science;mathematics;geometry;lie group;algorithm	Vision	53.63362140254341	-71.84546048465512	158521
14d91752ddcafde8711628ee7675b7b629509570	total-variation minimization on unstructured volumetric mesh: biophysical applications on reconstruction of 3d ischemic myocardium	tv myocardium image reconstruction approximation methods minimization heart accuracy;tv minimization total variation minimization biophysical applications 3d ischemic myocardium reconstruction geometrically complex mesh 3d ischemic region reconstruction noninvasive body surface potential data piecewise smooth region reconstruction healthy electrical properties ischemic electrical properties localized gradient unstructured volumetric mesh heart mesh resolutions gradient calculation variational tv prior iteratively re weighted least square concept;minimisation bioelectric phenomena cardiology image reconstruction iterative methods least squares approximations medical computing mesh generation	This paper describes the development and application of a new approach to total-variation (TV) minimization for reconstruction problems on geometrically-complex and unstructured volumetric mesh. The driving application of this study is the reconstruction of 3D ischemic regions in the heart from noninvasive body-surface potential data, where the use of a TV-prior can be expected to promote the reconstruction of two piecewise smooth regions of healthy and ischemic electrical properties with localized gradient in between. Compared to TV minimization on regular grids of pixels/voxels, the complex unstructured volumetric mesh of the heart poses unique challenges including the impact of mesh resolutions on the TV-prior and the difficulty of gradient calculation. In this paper, we introduce a variational TV-prior and, when combined with the iteratively re-weighted least-square concept, a new algorithm to TV minimization that is computationally efficient and robust to the discretization resolution. In a large set of simulation studies as well as two initial real-data studies, we show that the use of the proposed TV prior outperforms L2-based penalties in reconstruct ischemic regions, and it shows higher robustness and efficiency compared to the commonly used discrete TV prior. We also investigate the performance of the proposed TV-prior in combination with a L2- versus L1-based data fidelity term. The proposed method can extend TV-minimization to a border range of applications that involves physical domains of complex shape and unstructured volumetric mesh.	algorithm;algorithmic efficiency;calculus of variations;discretization;gradient;pixel;polygon mesh;simulation;variational principle;voxel	Jingjia Xu;Azar Rahimi;Fei Gao;Linwei Wang	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.389	mathematical optimization;mathematics;geometry	Vision	47.86871971663822	-78.05065088779354	158576
2cda80069da068eccd51be4655faf12b13ecca2d	a 3d thin nets extraction method for medical imaging	image hypersurface;image processing;2d thin nets;differential geometry;geometry;filters;biomedical imaging;data mining;blood vessel;crest lines;differential properties;medical image;image edge detection;roads;3d thin nets extraction method;feature extraction;medical image processing;medical imaging;image surface;characteristic lines;3d grey level function;robustness;biomedical imaging blood vessels data mining geometry image processing roads filters robustness image edge detection equations;3d volumetric images;extraction method;blood vessels;blood vessels 3d thin nets extraction method medical imaging characteristic lines 3d volumetric images 3d grey level function 2d thin nets crest lines image surface differential properties image hypersurface	This paper presents a powerful tool designed to extract characteristic lines, called 3D thin nets, from 3D volumetric images. 3D thin nets are the lines where the 3D grey level function is locally extremum in a given plane. Recently, we have shown that it is possible to characterize 2D thin nets as the crest lines of the image surface. This paper generalizes this approach to 3D data having three principal curvatures of the hypersurface deened by the 3D volumetric image. Using diierential properties of image hypersurface, we explain that 3D thin nets can be extracted by intersecting of two surfaces, one corresponding to the maximization of maximum curvature in its associated direction, and the other one to the maximization of medium curvature in its associated direction. We apply this approach to the extraction of blood vessels in 3D medical images.	expectation–maximization algorithm;grayscale;maxima and minima;medical imaging;volumetric display	Nasser Armande;Philippe Montesinos;Olivier Monga	1996		10.1109/ICPR.1996.546103	medical imaging;computer vision;feature extraction;computer science;mathematics;geometry;robustness	Vision	49.18983274844444	-66.84612072846605	158592
917b9c645b92426825671b45dd97ddfc712f3bd8	shape-constrained multi-atlas based segmentation with multichannel registration	image segmentation;image registration;image quality	Multi-atlas based segmentation methods have recently attracted much attention in medical image segmentation. The multi-atlas based segmentation methods typically consist of three steps, including image registration, label propagation, and label fusion. Most of the recent studies devote to improving the label fusion step and adopt a typical image registration method for registering atlases to the target image. However, the existing registration methods may become unstable when poor image quality or high anatomical variance between registered image pairs involved. In this paper, we propose an iterative image segmentation and registration procedure to simultaneously improve the registration and segmentation performance in the multi-atlas based segmentation framework. Particularly, a two-channel registration method is adopted with one channel driven by appearance similarity between the atlas image and the target image and the other channel optimized by similarity between atlas label and the segmentation of the target image. The image segmentation is performed by fusing labels of multiple atlases. The validation of our method on hippocampus segmentation of 30 subjects containing MR images with both 1.5T and 3.0T field strength has demonstrated that our method can significantly improve the segmentation performance with different fusion strategies and obtain segmentation results with Dice overlap of 0.892±0.024 for 1.5T images and 0.902±0.022 for 3.0T images to manual segmentations.	control theory;image quality;image registration;image segmentation;iterative method;software propagation;texture atlas	Yongfu Hao;Tianzi Jiang;Yong Fan	2012		10.1117/12.911370	image quality;image texture;computer vision;range segmentation;computer science;image registration;segmentation-based object categorization;pattern recognition;data mining;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Vision	42.64662987982186	-77.0337981363126	158699
dc0f47148caeaf3bd8da20bcaf9fd3921ed4ebb9	coupling anatomical and functional information for the computer-aided delineation of phase-contrast mri images using active contours	image segmentation;cardiology;medical image processing biomedical mri cardiology image segmentation;magnetic resonance imaging noise measurement force active contours biomedical imaging blood couplings;phase quality map anatomical information functional information computer aided delineation phase contrast mri image magnetic resonance imaging phase contrast velocimetry pc velocimetry cardiovascular pathology vessel segmentation vessel boundary;medical image processing;external force magnetic resonance imaging image segmentation phase contrast active models;biomedical mri	Phase-Contrast (PC) velocimetry MRI is a useful modality to explore cardiovascular pathologies, but requires the automatic segmentation of vessels. Most existing segmentation approaches focus on the exploitation of anatomical information to guide algorithms, such as widely used active contours we consider in this paper, although acquisitions simultaneously provide functional information. Due to noisy regions surrounding vessels, it is difficult to integrate this information as a complementary guiding force for attracting contours towards vessel boundaries. As we illustrate, the recently proposed phase quality map is interesting but appears not appropriately formulated to build useful attraction forces. In this paper, a refinement is presented to overcome this issue, together with preliminary experimental results. We detail and illustrate how this refinement operates on phase information, and how the coupling of both anatomical and functional information can improve the efficiency of active contours, compared to the only use of anatomical information.	algorithm;norm (social);refinement (computing)	Guillaume Trebuchet;Jean-Baptiste Fasquel;Christine Cavaro-Ménard;Serge Willoteaux	2012	2012 3rd International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2012.6469539	computer vision;computer science;image segmentation;scale-space segmentation	Robotics	43.45262354535015	-74.90847072202244	158712
c6f383ca27c18f4177072e8004e3899f9584dee8	snakes with tangent-based control and energies for bioimage analysis	ridge detection snake model tangent based control bioimage analysis filament like structure analysis feature extraction parametric representation hermite spline basis functions parameterization edge detection;feature detection active contours 2d parameterization microscopy hermite splines steerable filters orientation segmentation;active contours image edge detection vectors optimization biomedical imaging splines mathematics image segmentation;splines mathematics edge detection feature extraction image representation medical image processing	We propose a novel active contour for the analysis of filament-like structures and boundaries - features that traditional snakes based on closed curves have difficulties to delineate. Our method relies on a parametric representation of an open curve involving Hermite-spline basis functions. This allows us to impose constraints both on the contour and on its derivatives. The proposed parameterization enables tangential controls and facilitates the design of an energy term that considers oriented features. In this way, our technique can be used to detect edges as well as ridges. The use of the Hermite-spline basis is well suited to a semi-interactive implementation. We developed an ImageJ plugin, and present experimental results on real biological data.	active contour model;b-spline;basis function;cubic hermite spline;imagej;semiconductor industry	Virginie Uhlmann;Ricard Delgado-Gonzalo;Michael Unser	2014	2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2014.6867993	computer vision;mathematical optimization;feature detection;pattern recognition;mathematics	Vision	49.00626271347639	-71.1860284660077	158724
a604125db416e6028dc4ba47239d0f502ed1564e	body-wide anatomy recognition in pet/ct images	databases;object recognition;computed tomography;magnetism;positron emission tomography;abdomen;diagnostics;chest;diseases and disorders	With the rapid growth of positron emission tomography/computed tomography (PET/CT)-based medical applications, body-wide anatomy recognition on whole-body PET/CT images becomes crucial for quantifying body-wide disease burden. This, however, is a challenging problem and seldom studied due to unclear anatomy reference frame and low spatial resolution of PET images as well as low contrast and spatial resolution of the associated low-dose CT images. We previously developed an automatic anatomy recognition (AAR) system [15] whose applicability was demonstrated on diagnostic computed tomography (CT) and magnetic resonance (MR) images in different body regions on 35 objects. The aim of the present work is to investigate strategies for adapting the previous AAR system to low-dose CT and PET images toward automated body-wide disease quantification. Our adaptation of the previous AAR methodology to PET/CT images in this paper focuses on 16 objects in three body regions – thorax, abdomen, and pelvis – and consists of the following steps: collecting whole-body PET/CT images from existing patient image databases, delineating all objects in these images, modifying the previous hierarchical models built from diagnostic CT images to account for differences in appearance in low-dose CT and PET images, automatically locating objects in these images following object hierarchy, and evaluating performance. Our preliminary evaluations indicate that the performance of the AAR approach on low-dose CT images achieves object localization accuracy within about 2 voxels, which is comparable to the accuracies achieved on diagnostic contrast-enhanced CT images. Object recognition on low-dose CT images from PET/CT examinations without requiring diagnostic contrast-enhanced CT seems feasible.	association for automated reasoning;bayesian network;boolean satisfiability problem;ct scan;contrast resolution;database;encode;experiment;linear canonical transformation;outline of object recognition;polyethylene terephthalate;reference frame (video);resonance;tomography;voxel	Huiqian Wang;Jayaram K. Udupa;Dewey Odhner;Yubing Tong;Liming Zhao;Drew A. Torigian	2015		10.1117/12.2082718	magnetism;image registration;cognitive neuroscience of visual object recognition;computed tomography;physics;quantum mechanics;medical physics	Vision	40.753175215019155	-78.40012062942105	158922
31cc93a6b745ddce610277823f7f66dbe78bef8c	low rank magnetic resonance fingerprinting	standards;radio frequency;gold;image reconstruction;magnetic resonance imaging;dictionaries	Magnetic Resonance Fingerprinting (MRF) is a relatively new approach that provides quantitative MRI using randomized acquisition. Extraction of physical quantitative tissue values is preformed off-line, based on acquisition with varying parameters and a dictionary generated according to the Bloch equations. MRF uses hundreds of radio frequency (RF) excitation pulses for acquisition, and therefore high under-sampling ratio in the sampling domain (k-space) is required. This under-sampling causes spatial artifacts that hamper the ability to accurately estimate the quantitative tissue values. In this work, we introduce a new approach for quantitative MRI using MRF, called Low Rank MRF. We exploit the low rank property of the temporal domain, on top of the well-known sparsity of the MRF signal in the generated dictionary domain. We present an iterative scheme that consists of a gradient step followed by a low rank projection using the singular value decomposition. Experiments on real MRI data demonstrate superior results compared to conventional implementation of compressed sensing for MRF at 15% sampling ratio.	bloch sphere;compressed sensing;dictionary [publication type];excitation;experiment;fingerprint (computing);gradient;iterative method;magnetic resonance spectroscopy;markov random field;morphologic artifacts;online and offline;radio frequency;randomized algorithm;sampling (signal processing);sampling - surgical action;singular value decomposition;sparse matrix;whole earth 'lectronic link	Gal Mazor;Lior Weizman;Assaf Tal;Yonina C. Eldar	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7590734	gold;iterative reconstruction;computer vision;radiology;medicine;magnetic resonance imaging;mathematics;nuclear magnetic resonance;radio frequency	Vision	53.36283781061875	-77.39323887409181	158937
c70de48de91c25b4022504093b8487bca1ee1ba9	extraction of flower regions in color images using ant colony optimization	image segmentation;ant colony optimization;color space;natural images;clustering method;color image;color image segmentation;image retrieval	Extraction of flower regions from complex background is a difficult task and it is an important part of a flower image retrieval and recognition. In this article, we propose an Ant Colony Optimization (ACO) algorithm as a general color clustering method, and test it on flower images as a case study of object boundary extraction. The segmentation methodology on flower images consists of six steps: color space conversion, generation of candidate color cluster centers, ant colony optimization method to select optimum color cluster centers, merging of cluster centers which are close to each other, image segmentation by clustering, and extraction of flower region from the image. To evince that ACO algorithm can be a general segmentation method, some results of natural images in Berkeley segmentation benchmark have been presented. The method as a case study on flower region extraction has also been tested on the images of Oxford-17 Flowers dataset, and the results have confronted with other well established flower region extraction approaches.	ant colony optimization algorithms;mathematical optimization	Dogan Aydin;Aybars Ugur	2011		10.1016/j.procs.2010.12.088	computer vision;ant colony optimization algorithms;color image;image retrieval;computer science;artificial intelligence;segmentation-based object categorization;region growing;image segmentation;color space;scale-space segmentation	Vision	42.50470987530659	-68.36903464834988	159050
a41e766671e7de1f0307fed3d45415cc8ff0a1fe	a 3-subiteration 3d thinning algorithm for extracting medial surfaces	layer by layer;digital topology;subiteration based thinning;3d parallel thinning algorithm;topology preservation	Thinning is an iterative layer by layer erosion for extracting skeletons. This paper presents an efficient 3D parallel thinning algorithm which produces medial surfaces. A 3-subiteration strategy is proposed: the thinning operation is changed from iteration to iteration with a period of 3 according to the three deletion directions. 2002 Elsevier Science B.V. All rights reserved.	algorithm;iteration;medial graph;thinning	Kálmán Palágyi	2002	Pattern Recognition Letters	10.1016/S0167-8655(01)00142-8	layer by layer;mathematical optimization;computer science;mathematics;algorithm;digital topology	Vision	46.638316246910485	-69.86739490267085	159080
28a1043f503636e964f842dc7675542328572394	fast image segmentation algorithm using wavelet transform	cluster algorithm;image segmentation;edge detection;wavelet transform;fast imaging;traffic surveillance;k means clustering;point of interest	Fast image segmentation algorithm is discussed, where first significant points for segmentation are determined. Reduced set of image points is then used in K-means clustering algorithm for image segmentation. Our method reduces segmentation of the whole image to segmentation of significant points. Reduction of points of interest is made by introducing some kind of intelligence in decision step before clustering algorithm. It is numerically less complex and suitable for implementation in the low speed computing devices, such as smart cameras for the traffic surveillance systems. Multiscale edge detection and segmentation are discussed in detail in the paper.	bandelet (computer science);cluster analysis;edge detection;experiment;fast fourier transform;flooding algorithm;image segmentation;k-means clustering;numerical analysis;point of interest;second generation multiplex plus;wavelet transform	Tomaz Romih;Peter Planinsic	2008		10.1007/978-3-540-68127-4_12	computer vision;segmentation-based object categorization;cascade algorithm;wavelet packet decomposition;stationary wavelet transform;image segmentation;scale-space segmentation;discrete wavelet transform;fast wavelet transform;lifting scheme	Vision	42.605436279053315	-70.20755092356103	159091
99840ed7cc49c379273099b57bf58f55f5274e64	active shape model segmentation with optimal features	mri brain images model based segmentation active shape model segmentation optimal features nonlinear knn classifier automatic features selection medical diagnostic imaging synthetic data medical segmentation tasks lung fields chest radiographs overlap error measure cerebellum corpus callosum paired t test;mahalanobis distance;optimisation;image segmentation;image classification;lung;adolescent adult aged algorithms cerebellum computer simulation corpus callosum humans image enhancement image interpretation computer assisted lung middle aged models biological pattern recognition automated quality control reproducibility of results sensitivity and specificity;local features;medical image processing;active shape model image segmentation image databases spatial databases medical tests biomedical imaging lungs radiography magnetic resonance imaging brain;image classification lung diagnostic radiography biomedical mri image segmentation medical image processing optimisation;active shape model;diagnostic radiography;biomedical mri	An active shape model segmentation scheme is presented that is steered by optimal local features, contrary to normalized first order derivative profiles, as in the original formulation [Cootes and Taylor, 1995, 1999, and 2001]. A nonlinear kNN-classifier is used, instead of the linear Mahalanobis distance, to find optimal displacements for landmarks. For each of the landmarks that describe the shape, at each resolution level taken into account during the segmentation optimization procedure, a distinct set of optimal features is determined. The selection of features is automatic, using the training images and sequential feature forward and backward selection. The new approach is tested on synthetic data and in four medical segmentation tasks: segmenting the right and left lung fields in a database of 230 chest radiographs, and segmenting the cerebellum and corpus callosum in a database of 90 slices from MRI brain images. In all cases, the new method produces significantly better results in terms of an overlap error measure (p<0.001 using a paired T-test) than the original active shape model scheme.	active shape model;body of uterus;corpus callosum;cortical cell layer of the cerebellum;feature selection;mathematical optimization;nonlinear system;radiography;segmentation action;structure of parenchyma of lung;synthetic data;biologic segmentation	Bram van Ginneken;Alejandro F. Frangi;Joes Staal;Bart M. ter Haar Romeny;Max A. Viergever	2002	IEEE Transactions on Medical Imaging	10.1109/TMI.2002.803121	active shape model;computer vision;contextual image classification;computer science;mahalanobis distance;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	41.597715787343745	-78.48979930076142	159232
0b1eb15096552fe3465c2bc2c8db9ac4970756f0	acceleration of landweber-type algorithms by suppression of projection on the maximum singular vector	biophysics;vitesse;proyeccion;positron emission tomography algorithm acceleration nuclear medicine projection suppression medical diagnostic imaging landweber type algorithms maximum singular vector iterative image reconstruction;radioisotope scanning and imaging;velocity;biophysique;algebraic reconstruction technique;etude experimentale;objet test;singular value;tomocentelleografia;velocidad;positron emission tomography;test objet;algorithme;algorithm;acceleration iterative algorithms image reconstruction positron emission tomography convergence subspace constraints reconstruction algorithms character generation roundoff errors nuclear medicine;reconstruction image;conjugate gradient;iteraccion;singular vector;reconstruccion imagen;exploration radioisotopique;image reconstruction;projection;radioisotope scanning and imaging computerised tomography image reconstruction;positron;computerised tomography;radionuclide study;iteration;procedure suppression dc;iteration type landweber;maximum likelihood expectation maximization;positon;exploracion radioisotopica;objeto prueba;estudio experimental;tomoscintigraphie;steepest descent;biofisica;algoritmo	A procedure is developed that speeds up the convergence during the initial stage (the first 100 forward and backward projections) of Landweber-type algorithms for iterative image reconstruction, which include the Landweber, generalized Landweber, and steepest descent algorithms. In this procedure, the singular vector associated with the maximum singular value of the PET (positron emission tomography) system matrix is identified, and then projection of the data on this singular vector is suppressed after a single Landweber iteration. Typical PET system matrices have a significant gap between their two largest singular values; hence this suppression allows larger gains in subsequent iterations, speeding up convergence by roughly a factor of three. The present work includes: (1) a study of the singular value spectra of typical PET system matrices; (2) a study of the effect on convergence of projection on the maximum singular vector; and (3) a study of the convergence behavior of the procedure applied to the Landweber, generalized Landweber, steepest descent, conjugate gradient, and algebraic reconstruction technique algorithms. A comparison is made with the maximum-likelihood expectation-maximization algorithm.<<ETX>>	algebraic reconstruction technique;conjugate gradient method;convergence (action);expectation–maximization algorithm;gradient descent;immunostimulating conjugate (antigen);iterative method;iterative reconstruction;landweber iteration;large;largest;linear algebra;polyethylene terephthalate;positron-emission tomography;positrons;projections and predictions;singular;speed (motion);zero suppression	Tin-Su Pan;Andrew E. Yagle	1991	Conference Record of the 1991 IEEE Nuclear Science Symposium and Medical Imaging Conference	10.1109/42.192683	iterative reconstruction;gradient descent;mathematical optimization;iteration;projection;positron;landweber iteration;calculus;algebraic reconstruction technique;mathematics;geometry;conjugate gradient method;velocity;singular value	ML	52.92571642530407	-78.89141954367318	159479
11821cad515b976f3a466f7f541a50e923a86e75	robust higher order potentials for enforcing label consistency	minimisation;unsupervised learning;minimization;object recognition;vision ordenador;fonction potentiel;image segmentation;statistical mechanics;image processing;image resolution;fonction energie;discrete energy minimization;robustness image segmentation object segmentation labeling pixel inference algorithms testing image resolution image generation stereo image processing;proceso markov;probabilidad condicional;red petri;multiclass object segmentation;integration information;coupe graphe;probabilite conditionnelle;procesamiento imagen;mecanique statistique;minimizacion;modele potts;reconnaissance objet;testing;apprentissage non supervise;segmentation;conditional random fields;probabilistic approach;object recognition image segmentation;etiquetage;object boundaries label consistency multiple segmentations conditional random fields unsupervised segmentation algorithms higher order potential functions powerful graph cut move making algorithms multiclass object segmentation;traitement image;energy function;higher order;computer vision;etiquetaje;potts model;object segmentation;corte grafo;information integration;label consistency;image generation;multiple segmentations;enfoque probabilista;approche probabiliste;graph cut;processus markov;object boundaries;powerful graph cut;pixel;stereo image processing;funcion potencial;integracion informacion;inferencia;markov process;conditional random field;pattern recognition;labelling;funcion energia;potencial campo;move making algorithms;object recognition and segmentation;robustness;vision ordinateur;inference algorithms;energy minimization;reconnaissance forme;reconocimiento patron;mecanica estadistica;potential function;modelo potts;markov and conditional random fields;petri net;conditional probability;segmentacion;inference;reseau petri;labeling;potentiel champ;higher order potential functions;field potential;unsupervised segmentation algorithms	This paper proposes a novel framework for labelling problems which is able to combine multiple segmentations in a principled manner. Our method is based on higher order conditional random fields and uses potentials defined on sets of pixels (image segments) generated using unsupervised segmentation algorithms. These potentials enforce label consistency in image regions and can be seen as a generalization of the commonly used pairwise contrast sensitive smoothness potentials. The higher order potential functions used in our framework take the form of the Robust P n model and are more general than the P n Potts model recently proposed by Kohli et al. We prove that the optimal swap and expansion moves for energy functions composed of these potentials can be computed by solving a st-mincut problem. This enables the use of powerful graph cut based move making algorithms for performing inference in the framework. We test our method on the problem of multi-class object segmentation by augmenting the conventional crf used for object segmentation with higher order potentials defined on image regions. Experiments on challenging data sets show that integration of higher order potentials quantitatively and qualitatively improves results leading to much better definition of object boundaries. We believe that this method can be used to yield similar improvements for many other labelling problems.	algorithm;conditional random field;cut (graph theory);experiment;graph cuts in computer vision;instance (computer science);max-flow min-cut theorem;minimum cut;pixel;potts model;quantum gate;unsupervised learning	Pushmeet Kohli;Lubor Ladicky;Philip H. S. Torr	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1007/s11263-008-0202-0	unsupervised learning;computer vision;image processing;statistical mechanics;computer science;machine learning;pattern recognition;mathematics;conditional random field;statistics	Vision	51.13735554953612	-69.23410470988135	159528
51e24ad2a87597600ca3042d33d1c699f4150aa7	shape and appearance priors for level set-based left ventricle segmentation	single photon emission computed tomography cardiology image segmentation medical image processing;level set based left ventricle segmentation;4d gated cardiac spect;spatiotemporal regularisation;level set deformable model;appearance priors;shape priors maximum a posteriori framework spatiotemporal regularisation soft to hard probabilistic constraint perfusion defects 4d gated cardiac spect level set deformable model spatiotemporal constraint level set based left ventricle segmentation appearance priors;shape priors;perfusion defects;spatiotemporal constraint;soft to hard probabilistic constraint;maximum a posteriori framework	The authors propose a novel spatiotemporal constraint based on shape and appearance and combine it with a level-set deformable model for left ventricle (LV) segmentation in four-dimensional gated cardiac SPECT, particularly in the presence of perfusion defects. The model incorporates appearance and shape information into a ‘soft-to-hard’ probabilistic constraint, and utilises spatiotemporal regularisation via a maximum a posteriori framework. This constraint force allows more flexibility than the rigid forces of shape constraint-only schemes, as well as other state of the art joint shape and appearance constraints. The combined model can hypothesise defective LV borders based on prior knowledge. The authors present comparative results to illustrate the improvement gain. A brief defect detection example is finally presented as an application of the proposed method.	logical volume management;software bug;spatiotemporal database	Ronghua Yang;Majid Mirmehdi;Xianghua Xie;David H Hall	2013	IET Computer Vision	10.1049/iet-cvi.2012.0081	computer vision;mathematical optimization;mathematics;geometry	Vision	43.829374261101755	-77.42303890946367	159618
b07744c5dfab359f1a718778b1b7664cbf2ebc61	pixel based meshfree modeling of skeletal muscles		This paper introduces the meshfree Reproducing Kernel Particle Method (RKPM) in conjunction with a stabilized conforming nodal integration for 3D image-based modeling of skeletal muscles. This approach allows for construction of simulation model based on pixel data obtained from medical images. The model consists of different materials and muscle fiber direction obtained from Diffusion Tensor Imaging (DTI) is input at each pixel point. The reproducing kernel (RK) approximation also allows a representation of material heterogeneity with smooth transition. A multiphase multichannel level set based segmentation using Magnetic Resonance Images (MRI) and DTI formulated under a modified functional has been integrated into RKPM framework. The use of proposed methods for modeling the human lower leg is demonstrated.	pixel	Ramya Rao Basava;Jiun-Shyan Chen;Yantao Zhang;Shantanu Sinha;Usha S. Sinha;John Hodgson;Robert Csapo;Vadim Malis	2014		10.1007/978-3-319-09994-1_32	level set;pixel;kernel (linear algebra);computer vision;diffusion mri;artificial intelligence;mathematics	Robotics	45.83696536448145	-76.37705264464044	159759
101a736c0ca1041bfdf39c2e5750d8f132669ffa	fuzzy cognitive maps for stereovision matching	comparative analysis;ordering;stereovision;fuzzy clustering;fuzzy cognitive maps;fuzzy;matching;smoothness;similarity;relaxation;fuzzy cognitive map;high performance;epipolar;uniqueness;matching method	This paper outlines a method for solving the stereovision matching problem using edge segments as the primitives. In stereovision matching the following constraints are commonly used: epipolar, similarity, smoothness, ordering and uniqueness. We propose a new matching strategy under a fuzzy context in which such constraints are mapped. The fuzzy context integrates both Fuzzy Clustering and Fuzzy Cognitive Maps. With such purpose a network of concepts (nodes) is designed, each concept represents a pair of primitives to be matched. Each concept has associated a fuzzy value which determines the degree of the correspondence. The goal is to achieve high performance in terms of correct matches. The main findings of this paper are reflected in the use of the fuzzy context that allows building the network of concepts where the matching constraints are mapped. Initially, each concept value is loaded via the Fuzzy Clustering and then updated by the Fuzzy Cognitive Maps framework. This updating is achieved through the influence of the remainder neighboring concepts until a good global matching solution is achieved. Under this fuzzy approach we gain quantitative and qualitative matching correspondences. This method works as a relaxation matching approach and its performance is illustrated by comparative analysis against some existing global matching methods. 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	epipolar geometry;fuzzy clustering;fuzzy cognitive map;fuzzy logic;linear programming relaxation;pattern recognition;qualitative comparative analysis;stereopsis	Gonzalo Pajares;Jesús Manuel de la Cruz	2006	Pattern Recognition	10.1016/j.patcog.2006.04.003	computer vision;mathematical optimization;discrete mathematics;fuzzy cognitive map;defuzzification;fuzzy clustering;adaptive neuro fuzzy inference system;fuzzy classification;computer science;3-dimensional matching;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy associative matrix;fuzzy set operations	Vision	43.541039042960456	-66.37326869407697	159869
fa3eca1bae0a05292d4b6a4df054209194bfe438	semi-automatic segmentation of vessels by mathematical morphology: application in mri	nonlinear filters;mathematical morphology;nonlinear filters mathematical morphology image segmentation blood vessels biomedical mri;arterial images semi automatic segmentation mathematical morphology mri blood vessel isolation magnetic resonance imaging nonlinear filter invariance properties extrema contours virtual contour;image segmentation;automatic segmentation;nonlinear filter;morphological operation;magnetic resonance image;morphology magnetic resonance imaging nonlinear filters pixel shape magnetic analysis magnetic resonance magnetic separation testing robustness;blood vessels;biomedical mri	We propose a semi-automated method to isolate vessels or other structures from magnetic resonance imaging acquisitions. This method is divided into two parts. The first part is a nonlinear filter, using morphological operators, based on the invariance properties of the image. The second consists of finding the extrema contours. The contour which is proposed here is called the virtual contour, because it is located between the extrema contours. We have tested our method on 20 patients. The results showed that the method is robust and efficient when it comes to segmenting vessels from MRI.	mathematical morphology;semiconductor industry	Abdelmalik Taleb-Ahmed;Xavier Leclerc;T. Saint Michel	2001		10.1109/ICIP.2001.958310	nonlinear filter;computer vision;mathematical morphology;computer science;magnetic resonance imaging;mathematics;image segmentation	Vision	40.99036537753081	-75.56284219001337	159880
1352eb73dbfbf9eba54e02d7dbd08e26198698d0	groupwise spectral log-demons framework for atlas construction	groupwise registration;symmetric log-demons algorithm;groupwise log-demons;local nonrigid deformation;new framework;high shape variability;atlas construction;groupwise spectral log-demons framework;groupwise spectral log-demons;complex deformation	We introduce a new framework to construct atlases from images with very large and complex deformations. The atlas is build in parallel with groupwise registrations by extending the symmetric Log-Demons algorithm. We describe and evaluate two forms of our framework: the Groupwise LogDemons (GL-Demons) is faster but is limited to local nonrigid deformations, and the Groupwise Spectral Log-Demons (GSL-Demons) is slower but, due to isometry-invariant representations of images, can construct atlases of organs with high shape variability. We demonstrate our framework by constructing atlases from hearts with high shape variability.	algorithm;approximation;computation;gnu scientific library;gradient;graphics processing unit;image resolution;matlab;parallel computing;spatial variability;time complexity;video-in video-out	Herve Lombaert;Leo Grady;Xavier Pennec;Jean-Marc Peyrat;Nicholas Ayache;Farida Cheriet	2012		10.1007/978-3-642-36620-8_2	computer vision;computer science;geometry;engineering drawing	Vision	47.16983490272656	-77.03681267957998	159896
2e75703a4ae49dd6c3343a6554e868605693569d	a spot segmentation approach for 2d gel electrophoresis images based on 2d histograms	software;medical image processing electrophoresis image segmentation;histograms;image segmentation;2d page;electrophoresis;image segmentation histograms proteins pixel algorithm design and analysis software shape;software programs;image segmentation proteomics 2d page;proteins;shape;medical image processing;pixel;software programs spot segmentation approach 2d gel electrophoresis images 2d histograms;2d gel electrophoresis images;proteomics;2d histograms;algorithm design and analysis;spot segmentation approach;2d gel electrophoresis	Spot-Segmentation, an essential stage of processing 2D gel electrophoresis images, remains a challenging process. The available software programs and techniques fail to separate overlapping protein spots correctly and cannot detect low intensity spots without human intervention. This paper presents an original approach to spot segmentation in 2D gel electrophoresis images. The proposed approach is based on 2D-histograms of the aforementioned images. The conducted experiments in a set of 16-bit 2D gel electrophoresis images demonstrate that the proposed method is very effective and it outperforms existing techniques even when it is applied to images containing several overlapping spots as well as to images containing spots of various intensities, sizes and shapes.	16-bit;2d to 3d conversion;experiment;graphic art software	Eleni Zacharia;Eirini Kostopoulou;Dimitrios E. Maroulis;Sophia Kossida	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.622	algorithm design;computer vision;two-dimensional gel electrophoresis;electrophoresis;shape;computer science;bioinformatics;histogram;image segmentation;proteomics;pixel	Robotics	39.2476105750428	-74.21361914102795	159971
36b835415f441429d1d8ccc0f060db54f94090ca	local vs global descriptors of hippocampus shape evolution for alzheimer's longitudinal population analysis	karcher mean;communication conference;population analysis;time series image data;geodesic shooting;alzheimer s disease;brain imaging	In the context of Alzheimer’s disease (AD), state-of-the-art methods separating normal control (NC) from AD patients or CN from progressive MCI (mild cognitive impairment patients converting to AD) achieve decent classification rates. However, they all perform poorly at separating stable MCI (MCI patients not converting to AD) and progressive MCI. Instead of using features extracted from a single temporal point, we address this problem using descriptors of the hippocampus evolutions between two time points. To encode the transformation, we use the framework of large deformations by diffeomorphisms that provides geodesic evolutions. To perform statistics on those local features in a common coordinate system, we introduce an extension of the Kärcher mean algorithm that defines the template modulo rigid registrations, and an initialization criterion that provides a final template leading to better matching with the patients. Finally, as local descriptors transported to this template do not directly perform as well as global descriptors (e.g. volume difference), we propose a novel strategy combining the use of initial momentum from geodesic shooting, extended Kärcher algorithm, density transport and integration on a hippocampus subregion, which is able to outperform global descriptors.	algorithm;encode;geodesic grid;global illumination;global serializability;modulo operation	Jean-Baptiste Fiot;Laurent Risser;Laurent D. Cohen;Jurgen Fripp;François-Xavier Vialard	2012		10.1007/978-3-642-33555-6_2	artificial intelligence;machine learning;mathematics;cartography	Vision	45.04618628428287	-76.00963865574597	160121
4b8483498461860a0a23b88a2bfc6a04f92d6b72	comparison of bicubic and bézier polynomials for surface parameterization in volumetric image	ct;modelling polynomials image registration medical image processing biomedical mri computerised tomography;gaussian curvature;active contour;surface parameterization;computed tomography;mong basis;mri surface parameterization active contours mong basis feature computation surface extraction medical diagnostic imaging bezier polynomials casteljau algorithm bicubic polynomials magnetic resonance angiography gaussian curvature active contour models mean curvature ct;surface extraction;singular value decomposition;surface fitting;biomedical imaging;active contours;testing;polynomials surface fitting surface reconstruction biomedical imaging image registration data mining active contours least squares methods singular value decomposition testing;surface reconstruction;data mining;polynomials;medical image;medical image processing;image registration;surface model;medical imaging;bicubic polynomials;least square;active contour models;mri;computerised tomography;magnetic resonance angiography;casteljau algorithm;mean curvature;parametric surface;bezier polynomials;feature computation;least squares methods;active contour model;medical diagnostic imaging;biomedical mri	Curvature-based surface features are well suited for use in multimodal medical image registration. The accuracy of such feature-based registration techniques is dependent upon the reliability of the feature computation. The computation of curvature features requires second derivative information that is best obtained from a parametric surface representation. We present a method of explicitly parameterizing surfaces from volumetric data. Surfaces are extracted, without a global thresholding, using active contour models. A Mong basis for each surface patch is estimated and used to transform the patch into local, or parametric, coordinates. Surface patches are fit to first a bicubic polynomial and second to a Bezier polynomial. The bicubic polynomial is fit in local coordinates using least squares solved by singular value decomposition. Bezier polynomial is fit using de Casteljau algorithm. We tested our method by reconstructing surfaces from the surface model and analytically computing Gaussian and mean curvatures. The model was tested on analytical and medical data and the results of both methods are compared.	bicubic interpolation;bézier curve;volumetric display	Francis K. H. Quek;Vishwas Kulkarni;Cemil Kirbas	2003		10.1109/BIBE.2003.1188935	medical imaging;computer vision;mathematical optimization;computer science;active contour model;mathematics;geometry	Vision	45.82596741729054	-77.0290241525116	160189
671174157fb94215d7571ed877264fe08dff049e	spect image reconstruction using compound models	bayesian framework;electromagnetic scattering;degradation;single photon emission computerised tomography;gaussian processes;image reconstruction single photon emission computed tomography nuclear medicine isotopes degradation attenuation electromagnetic scattering particle scattering bayesian methods gaussian processes;bayes methods;bayesian methods;scattering;attenuation;spect;simulated annealing single photon emission computed tomography medical image processing maximum likelihood estimation image reconstruction random noise bayes methods iterative methods deterministic algorithms stochastic processes;simulated annealing;maximum likelihood estimation;deterministic iterative method;maximum a posteriori estimate;iterative methods;random noise;deterministic algorithms;stochastic processes;compound gauss markov random field;image reconstruction;medical image processing;radioactive isotope;single photon emission computed tomography;nuclear medicine;simulated annealing spect single photon emission computerised tomography image reconstruction nuclear medicine radioactive isotope attenuation scattering bayesian framework compound gauss markov random field maximum a posteriori estimate map estimate stochastic iterative method deterministic iterative method;map estimate;stochastic iterative method;particle scattering;isotopes	SPECT (Single Photon Emission Computed Tomography) is used in nuclear medicine to determine the distribution of a radioactive isotope within a patient from tomographic views or projection data. These images are severely degraded due to the presence of noise and several physical factors like attenuation and scattering. In this paper we use, within the Bayesian framework, a Compound Gauss Markov Random Field (CGMRF) as prior model to reconstruct such images. In order to find the Maximuma Posteriori(MAP) estimate we propose a new iterative method, which is stochastic for the line process and deterministic for the reconstruction. The proposed method is tested and compared with other reconstruction methods on both synthetic and real SPECT images.	ct scan;image noise;iterative method;iterative reconstruction;markov chain;markov random field;synthetic intelligence;tomography	Antonio López;Rafael Molina;Aggelos K. Katsaggelos;Javier Mateos	2001		10.1109/ICASSP.2001.941318	iterative reconstruction;attenuation;stochastic process;econometrics;mathematical optimization;degradation;simulated annealing;bayesian probability;maximum a posteriori estimation;gaussian process;mathematics;iterative method;maximum likelihood;scattering;isotope;statistics	Vision	53.42742771785522	-75.84771390948006	160199
37f3ffab6c713dd0e7576e79b59dbf3efd1ed5a7	a two-stage image segmentation method for blurry images with poisson or multiplicative gamma noise	52a41;image segmentation;90c47;90c30;65k15;65k10;multiplicative noise;gamma noise;total variation;primal dual algorithm;convexity	In this paper, a two-stage method for segmenting blurry images in the presence of Poisson or multiplicative Gamma noise is proposed. The method is inspired by a previous work on two-stage segmentation and the usage of an I-divergence term to handle the noise. The first stage of our method is to find a smooth solution u to a convex variant of the Mumford-Shah model where the L2 data-fidelity term is replaced by an I-divergence term. A primal-dual algorithm is adopted to efficiently solve the minimization problem. We prove the convergence of the algorithm and the uniqueness of the solution u. Once u is obtained, then in the second stage, the segmentation is done by thresholding u into different phases. The thresholds can be given by the users or can be obtained automatically by using any clustering method. In our method, we can obtain any K-phase segmentation (K ≥ 2) by choosing (K−1) thresholds after u is found. Changing K or the thresholds does not require u to be re-computed. Experimental results show that our two-stage method performs better than many standard two-phase or multi-phase segmentation methods for very general images, including anti-mass, tubular, MRI and low-light images.	approximation algorithm;cluster analysis;code;convolution;diffusing update algorithm;experiment;image segmentation;ising model;k-means clustering;maxima and minima;multiphase particle-in-cell method;numerical analysis;smoothing;surround sound;thresholding (image processing);two-phase commit protocol;two-phase locking	Raymond H. Chan;Hongfei Yang;Tieyong Zeng	2014	SIAM J. Imaging Sciences	10.1137/130920241	computer vision;mathematical optimization;convexity;computer science;machine learning;multiplicative noise;mathematics;image segmentation;total variation	Vision	51.428866294584	-71.50539248222793	160398
1a754bcf4fbba78afa657b43e04c8d07f71342ae	on the characterization of single-event related brain activity from functional magnetic resonance imaging (fmri) measurements	vectors covariance matrices noise measurement kalman filters noise brain modeling convergence;real fmri measurements single event related brain activity characterization functional magnetic resonance imaging measurements numerical technique mathematical model single event related brain response regularized newton technique kalman filtering procedure biophysiological parameters balloon model hemodynamic brain responses synthetic fmri measurements;physiological models biomedical mri brain haemodynamics kalman filters newton method	We propose an efficient numerical technique for calibrating the mathematical model that describes the single-event related brain response when fMRI measurements are given. This method employs a regularized Newton technique in conjunction with a Kalman filtering procedure. We have applied this method to estimate the biophysiological parameters of the Balloon model that describes the hemodynamic brain responses. Illustrative results obtained with both synthetic and real fMRI measurements are presented.	electroencephalography;hemodynamics;kalman filter;mathematical model;mathematics;newton;numerical analysis;resonance;synthetic intelligence;fmri	Nafiseh Khoram;Chadia Zayane-Aissa;Taous-Meriem Laleg-Kirati;Rabia Djellouli	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944104	computer vision;speech recognition;computer science;nuclear magnetic resonance	Robotics	50.75084782316518	-80.20380333942376	160633
7bfe8fb59640f7ee92780dbd1a450db98a9a70fa	mr image segmentation by nonparametric model	cluster algorithm;image segmentation;image segmentation data models bayesian methods clustering algorithms magnetic resonance imaging accuracy magnetic resonance;natural images;magnetic resonance image;bayesian method;data model;mr imaging;magnetic resonance;dirichlet process mixture;clustering image segmentation nonparametric dirichlet process mixtures;image segmentation biomedical mri;biomedical mri;nonparametric segmentation mr image segmentation nonparametric dirichlet process mixtures model magnetic resonance images dice similarity coefficients	Nonparametric Dirichlet Process Mixtures (MDP) model algorithm is applied to segment images, which can obtain the segmentation class numbers automatically without initialization. The algorithm is used to segment noisy natural images and magnetic resonance images with biasing field. Compared with classical Markov Field (MRF) segmentation, the nonparametric segmentation results show the greater performance. This method is also analyzed quantitatively on the belly magnetic resonance images. The Dice Similarity Coefficients (DSC) of all slices exceed 93%, which show that the proposed method is robust and accurate.	algorithm;biasing;coefficient;image segmentation;markov chain;markov random field;resonance	Yisu Lu;Wufan Chen	2011	2011 4th International Conference on Biomedical Engineering and Informatics (BMEI)	10.1109/BMEI.2011.6098246	computer vision;radiology;data model;bayesian probability;computer science;magnetic resonance imaging;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	43.918461132062546	-77.03366804494146	160698
92ced391a297f2984e9fb1b2fd78dece51c1794b	a fuzzy-rule-based image enhancement method for medical applications	fuzzy rule based system;image enhancement biomedical imaging fuzzy set theory fuzzy systems knowledge based systems smoothing methods information filtering information filters retina biological cells;edge enhancement;nonimpulsive noise;fuzzy rules;impulse noise;satisfiability;fuzzy set theory;medical expert systems;retinal images;image enhancement;fuzzy rule base;fuzzy rule based image enhancement;medical expert systems image enhancement fuzzy set theory medical image processing;medical image processing;impulsive noise;medical application;chromosome images;selection criteria;medical applications;retinal images fuzzy rule based image enhancement medical applications fuzzy set theory fuzzy rule based system image enhancement impulsive noise nonimpulsive noise edge enhancement fuzzy rules chromosome images	Using fuzzy set theory, we develop a fuzzy rule-based system to perform some of the most common tasks of image enhancement: removing impulsive noise; smoothing nonimpulsive noise; and enhancing edges. Three different filters for each task and the selection criteria based on local information are derived. The selection criteria constitute the antecedent clauses of the fuzzy rules, and the corresponding filters constitute the consequent clauses of the fuzzy rules. The overall result of the fuzzy rule-based system is computed as the combination of the results of the individual filters, where each result contributes to the degree that the corresponding antecedent clause is satisfied. This approach gives us a powerful and flexible image enhancement paradigm. We present results on several types of images such as retinal and chromosome images. >	fuzzy rule;image editing	YoungSik Choi;Raghu Krishnapuram	1995		10.1109/CBMS.1995.465444	computer vision;defuzzification;impulse noise;fuzzy classification;computer science;fuzzy number;machine learning;pattern recognition;edge enhancement;fuzzy set;fuzzy set operations;satisfiability	Vision	42.3400042371504	-72.39055608570276	160771
3df7b4a690e43ae6c142f5292cd672cd388b678f	contour segmentation of the intima, media, and adventitia layers in intracoronary oct images: application to fully automatic detection of healthy wall regions	optical coherence tomography;coronary artery;contour segmentation;machine learning	Quantitative and automatic analysis of intracoronary optical coherence tomography images is useful and time-saving to assess cardiovascular risk in the clinical arena. First, the interfaces of the intima, media, and adventitia layers are segmented, by means of an original front propagation scheme, running in a 4D multi-parametric space, to simultaneously extract three non-crossing contours in the initial cross-sectional image. Second, information resulting from the tentative contours is exploited by a machine learning approach to identify healthy and diseased regions of the arterial wall. The framework is fully automatic. The method was applied to 40 patients from two different medical centers. The framework was trained on 140 images and validated on 260 other images. For the contour segmentation method, the average segmentation errors were $$29 \pm 46~\upmu \text {m}$$ 29 ± 46 μ m for the intima–media interface, $$30 \pm 50~\upmu \text {m}$$ 30 ± 50 μ m for the media–adventitia interface, and $$50 \pm 64~\upmu \text {m}$$ 50 ± 64 μ m for the adventitia–periadventitia interface. The classification method demonstrated a good accuracy, with a median Dice coefficient equal to 0.93 and an interquartile range of (0.78–0.98). The proposed framework demonstrated promising offline performances and could potentially be translated into a reliable tool for various clinical applications, such as quantification of tissue layer thickness and global summarization of healthy regions in entire pullbacks.	adventitia;amendment;automatic identification and data capture;consent forms;contour line;cross-sectional data;declaration (computer programming);dental plaque;dynamic programming;extraction;helsinki declaration;image segmentation;interquartile range;intima;intracoronary optical coherence tomography;license;machine learning;maxima and minima;numerous;online and offline;patients;performance;quantlib;quantitation;software propagation;sørensen–dice coefficient;thickness (graph theory);tomography, optical coherence;algorithm;anatomical layer;biologic segmentation;quantitative;standards characteristics	Guillaume Zahnd;Ayla Hoogendoorn;Nicolas Combaret;Antonios Karanasos;Emilie Péry;Laurent Sarry;Pascal Motreff;Wiro Niessen;Evelyn Regar;Gijs van Soest;Frank J. H. Gijsen;Theo van Walsum	2017		10.1007/s11548-017-1657-7		Vision	40.001984705723665	-79.16601249802278	160839
02b730a7f62ed4760199db12337dfdc55a612a05	area-preserving surface flattening using lie advection	area preserving flattening;lie advection;brain mapping	In this paper, we propose a novel area-preserving surface flattening method, which is rigorous in theory, efficient in computation, yet general in application domains. Leveraged on the state-of-the-art flattening techniques, an infinitesimal area restoring diffeomorphic flow is constructed as a Lie advection of differential 2-forms on the manifold, which yields strict equality of area elements between the flattened and the original surfaces at its final state. With a surface represented by a triangular mesh, we present how an deterministic algorithm can be faithfully implemented to its continuous counterpart. To demonstrate the utility of this method, we have applied our method to both the cortical hemisphere and the entire cortex. Highly complied results are obtained in a matter of seconds.	application domain;computation (action);deterministic algorithm;discretization;map;mathematical optimization;modality (human–computer interaction);polygon mesh;sampling (signal processing);shape analysis (digital geometry);thickness (graph theory);utility;manifold;triangulation	Guangyu Zou;Jiaxi Hu;Xianfeng Gu;Jing Hua	2011	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-23629-7_41	topology;radiology;calculus;mathematics;geometry;brain mapping	Visualization	49.71690761323212	-74.28328471017929	160899
609186ec17426c3c0af55dab17b7ed997a7c69f4	feature tree clustering for image segmentation	learning process;image segmentation;tree data structures;tree structure;tree data structures image segmentation;global property feature tree clustering image segmentation learning process clustering process integrated tree local property;feature tree;image segmentation image recognition image coding computer science tree data structures computer networks remote sensing computer vision merging image analysis	A new image segmentation method using a feature tree is proposed in this paper. The feature tree reflects the feature of an image. The proposed method is composed of two processes: (I) learning process and (II) clustering process. In the learning process, many efficient feature trees are made that construct an integrated tree. The integrated tree is used to segment images in the clustering process. Dividing an image is kept on from global point to local point. So, the proposed method can divide images considering not only the local property but also the global property. We applied the proposed method to some images, and obtained good results.	cluster analysis;image segmentation	Suguru Inoue;Masafumi Hagiwara	2001		10.1109/ICSMC.2001.973727	computer vision;feature detection;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;tree structure;tree;minimum spanning tree-based segmentation;scale-space segmentation;feature	Vision	44.42157499572669	-67.30333729040701	160939
8f7277c7efe432daad46d2537ce6843db7eb9f92	nonlinear poisson image completion using color manifold	interpolation image processing image analysis image restoration;interpolation large scale systems image color analysis image texture analysis image restoration manifolds geophysics computing eigenvalues and eigenfunctions automation filling;interpolation;image processing;image restoration;indexing terms;stochastic processes image colour analysis image restoration interpolation;large scale;stochastic processes;linear interpolation;image colour analysis;missing regions recovery nonlinear poisson image completion color manifold embedding nonlinear subspace linear interpolation image restoration nonlinearly progressive color changes;image analysis;iterative solution;image completion	While most image completion methods focus on filling regions with structures or stationary textures, few are suitable for completing large-scale missing parts on complex background with nonlinearly progressive color changes. In this paper, we propose a novel approach, termed as nonlinear Poisson completion, to solve this problem. The visible parts of the background serve as a training set, from which we learn the embedding nonlinear subspace of progressive colors, namely color manifold. A Poisson image completion procedure, which works efficiently for smoothly linear interpolation, is extended to nonlinearly recover the missing regions with iteration solution confined to the manifold. In some especially challenging cases, a simple post-processing serves to generate more natural-looking results. Experiments on both synthetic and real images verify the effectiveness of the proposed algorithm.	algorithm;color;experiment;iteration;linear interpolation;nonlinear system;smoothing;stationary process;synthetic intelligence;test set;video post-processing	Su Xue;Qionghai Dai	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379998	demosaicing;image texture;image restoration;computer vision;mathematical optimization;feature detection;image analysis;index term;image gradient;binary image;image processing;interpolation;computer science;stairstep interpolation;mathematics;geometry;linear interpolation;nearest-neighbor interpolation;multivariate interpolation;image scaling	Vision	49.046480754649906	-66.9948082171233	160943
ebd8d912a23d2bebfce5e9743fee9b0a6fad7998	image segmentation using active contours and evidential distance	bhattacharyya distance;belief function;dempster shafer rule;active contours;characteristic function	We proposed a new segmentation based on Active Contours AC for vector-valued image that incorporates evidential distance. The proposed method combine both Belief Functions BFs and probability functions in the Bhattacharyya distance framework. This formulation allows all features issued from vector-valued image and guide the evolution of AC using an inside/outside descriptor. The imprecision caused by the variation of the contrast issued from the multiple channels is incorporated in the BFs as weighted parameters. We demonstrated the performance of the proposed algorithm using some challenging color biomedical images.	image segmentation	Foued Derraz;Antonio Pinti;Miloud Boussahla;Laurent Peyrodie;Hechmi Toumi	2013		10.1007/978-3-642-41822-8_59	computer vision;bhattacharyya distance;characteristic function;machine learning;pattern recognition;mathematics;statistics	Vision	42.68243845237853	-71.63245473544272	161192
0d05b1dd33d8a0bf2f7fb6f0608ab0a8a6effff5	new tools for gray level histogram analysis, applications in segmentation		This paper summarizes three algorithms used for the analysis of gray level histograms. Two of them are developed generalizing standard techniques and the other is a completely new method. We will study the advantages of each and give examples of real-world use. Gray level histogram analysis (mainly threshold computation) is a known technique that allows easy and fast segmentation of the regions of interest in an image (1). Many methods have been proposed for this problem (2), but almost all of them focus only on the problem of bimodal histograms. We will deal with multimodal histograms and, besides, we improve the time efficiency of the most widely used method (Otsu's (3)).		Fernando Martín Rodríguez	2013		10.1007/978-3-642-39094-4_37	computer vision;computer science;artificial intelligence;computer graphics (images)	Vision	41.83202354562381	-68.21847650517053	161313
d8905b7ce4211fbd925af0e0fa077dfcf02fd5d0	tree-pruning: a new algorithm and its comparative analysis with the watershed transform for automatic image segmentation	image analysis algorithm design and analysis image segmentation pixel image processing licenses image databases leak detection tree graphs robustness;comparative analysis;image segmentation;image processing;image forest transform;automatic segmentation;trees mathematics;watershed transform;tree pruning;optimum path forest problem;graph;graph tree pruning watershed transform image segmentation image forest transform image processing optimum path forest problem;transforms;trees mathematics image segmentation transforms;point of view	Image segmentation using tree pruning (TP) and watershed (WS) has been presented in the framework of the image forest transform (IFT) - a method to reduce image processing problems related to connectivity into an optimum-path forest problem in a graph. Given that both algorithms use the IFT with similar parameters, they usually produce similar segmentation results. However, they rely on different properties of the IFT which make TP more robust than WS for automatic segmentation tasks. We propose and demonstrate an important improvement in the TP algorithm, clarify the differences between TP and WS, and provide their comparative analysis from the theoretical and practical points of view. The experiments involve automatic segmentation of license plates in a database with 990 images	experiment;fast fourier transform;genetic algorithm;hoc (programming language);image processing;image segmentation;outline of object recognition;pixel;qualitative comparative analysis;transcutaneous electrical nerve stimulation;watershed (image processing)	Paulo André Vechiatto Miranda;Felipe P. G. Bergo;Leonardo M. Rocha;Alexandre X. Falcão	2006	2006 19th Brazilian Symposium on Computer Graphics and Image Processing	10.1109/SIBGRAPI.2006.44	image texture;computer vision;feature detection;binary image;computer science;machine learning;segmentation-based object categorization;pattern recognition;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;top-hat transform;connected-component labeling	Vision	44.062408590783065	-69.98421495563684	161498
3156e1ef15370a56801b5f83aac346fab0002573	characterizing the shape of anatomical structures with poisson's equation	poisson equation biomedical mri brain diseases medical image processing paediatrics;poisson equation;sensitivity and specificity;female;partial differential equation;brain;data interpretation statistical;white matter;imaging three dimensional;caudate nucleus;shape analysis brain morphometry caudate nucleus poisson s equation;shape analysis;male;adult algorithms anatomy cross sectional artificial intelligence brain data interpretation statistical demyelinating diseases female humans image enhancement image interpretation computer assisted imaging three dimensional infant newborn information storage and retrieval magnetic resonance imaging male pattern recognition automated poisson distribution reproducibility of results schizophrenia sensitivity and specificity;premature infant;three dimensional;poisson s equation;schizophrenia;image enhancement;mr imaging;brain morphometry;image interpretation computer assisted;paediatrics;infant newborn;data extraction;magnetic resonance;medical image processing;adult;magnetic resonance imaging;reproducibility of results;structural shape differences anatomical structures poisson equation partial differential equation shape analysis equipotential set smoothness solution graph displacement map function complexity robust algorithms three dimensional neuroanatomical data magnetic resonance images brain caudate nucleus schizotypal personality disorder premature infants asymmetric white matter injury;schizotypal personality disorder;diseases;artificial intelligence;algorithms;anatomy cross sectional;pattern recognition automated;humans;information storage and retrieval;shape anatomical structure poisson equations partial differential equations physics length measurement magnetic field measurement displacement measurement robustness image analysis;demyelinating diseases;poisson distribution;biomedical mri	"""Poisson's equation, a fundamental partial differential equation in classical physics, has a number of properties that are interesting for shape analysis. In particular, the equipotential sets of the solution graph become smoother as the potential increases. We use the displacement map, the length of the streamlines formed by the gradient field of the solution, to measure the """"complexity"""" (or smoothness) of the equipotential sets, and study its behavior as the potential increases. We believe that this function complexity=f(potential), which we call the shape characteristic, is a very natural way to express shape. Robust algorithms are presented to compute the solution to Poisson's equation, the displacement map, and the shape characteristic. We first illustrate our technique on two-dimensional synthetic examples and natural silhouettes. We then perform two shape analysis studies on three-dimensional neuroanatomical data extracted from magnetic resonance (MR) images of the brain. In the first study, we investigate changes in the caudate nucleus in Schizotypal Personality Disorder (SPD) and confirm previously published results on this structure . In the second study, we present a data set of caudate nuclei of premature infants with asymmetric white matter injury. Our method shows structural shape differences that volumetric measurements were unable to detect"""		Haissam Haidar;Sylvain Bouix;James J. Levitt;Robert W. McCarley;Martha Elizabeth Shenton;Janet S. Soul	2006	IEEE Trans. Med. Imaging	10.1109/TMI.2006.881378	computer vision;radiology;medicine;artificial intelligence;magnetic resonance imaging;poisson's equation;mathematics	Vision	43.820282895695634	-79.26898285767443	161512
5b5c44806eafe1c2e56287f6c3c70fc35ab3da66	fuzzy segmentation for finger vessel pattern extraction of infrared images	finger vessel;infrared images;probabilistic fuzzy c means;pattern extraction;fuzzy segmentation	In this paper, an algorithm for robust finger vessel pattern extraction from infrared images is presented based on image processing, edge suppression, fuzzy enhancement, and fuzzy clustering. Initially, the brightness variations of the images are eliminated using histogram normalization and the vessel patterns are enhanced, to facilitate the separation process from other tissue parts through fuzzy clustering. Several vessel measurements and processes are applied, including the second-order local statistical information contained in the Hessian matrix and the matched filter applied in the direction of the largest curvature. Edge suppression reduces sharp brightness changes at finger borders and image contrast is non-linearly increased through fuzzy enhancement. A novel probabilistic fuzzy C-means clustering algorithm is used to derive vessels from the surrounding tissue regions using spatial information in the membership function. Therefore, as shown experimentally, better segmentation and classification rates than the standard C-means algorithm is achieved using the primitive set of features. Moreover, the segmentation results are validated using two cluster-based functions: partition coefficient and partition entropy. Over-segmentation conditions are handled using a two-stage morphological post-processing. Morphological majority filter smoothes the vessel contours and removes small isolated regions which have been misclassified as vessels. Morphological reconstruction is used to obtain outlier-free vessel pattern. The proposed algorithm is evaluated both in real and artificially created images and under different noise types and signal-to-noise ratios, giving excellent segmentation accuracy in the main vessels, even in case where strong artificial noise is used to distort the images. Furthermore, the algorithm can readily be applied in many image enhancement and segmentation applications.	ansi c;algorithm;cluster analysis;coefficient;database normalization;distortion;experiment;fuzzy clustering;fuzzy cognitive map;fuzzy concept;hessian;image editing;image processing;matched filter;pattern recognition;perimeter;pixel;sensitivity and specificity;shading;signal-to-noise ratio;smoothing;utility functions on indivisible goods;video post-processing;zero suppression	Marios Vlachos;Evangelos Dermatas	2014	Pattern Analysis and Applications	10.1007/s10044-014-0413-7	computer vision;machine learning;pattern recognition;mathematics	Vision	43.19546116885424	-71.4669655839516	161830
96cf6a29e0b86534c633770f4af9e4ea9fd96797	disjunctive normal parametric level set with application to image segmentation	level set image segmentation shape computational efficiency mathematical model computational modeling bayes methods;image segmentation;bayes methods;level set disjunctive normal forms segmentation parametric level set multiphase level set variational bayesian methods;level set;computational modeling;shape;disjunctive normal parametric level set method image segmentation convenient shape representation numerical computation topological changes irregularity formation signed distance function dnls two phase image segmentation multiphase image segmentation polytope union bayesian framework variational approach energy minimization appearance model local region based image segmentation;image representation image segmentation set theory shape recognition;mathematical model;computational efficiency	Level set methods are widely used for image segmentation because of their convenient shape representation for numerical computations and capability to handle topological changes. However, in spite of the numerous works in the literature, the use of level set methods in image segmentation still has several drawbacks. These shortcomings include formation of irregularities of the signed distance function, sensitivity to initialization, lack of locality, and expensive computational cost, which increases dramatically as the number of objects to be simultaneously segmented grows. In this paper, we propose a novel parametric level set method called disjunctive normal level set (DNLS), and apply it to both two-phase (single object) and multiphase (multiobject) image segmentations. DNLS is a differentiable model formed by the union of polytopes, which themselves are created by intersections of half-spaces. We formulate the segmentation algorithm in a Bayesian framework and use a variational approach to minimize the energy with respect to the parameters of the model. The proposed DNLS can be considered as an open framework that allows the use of different appearance models and shape priors. Compared with the conventional level sets available in the literature, the proposed DNLS has the following major advantages: it requires significantly less computational time and memory, it naturally keeps the level set function regular during the evolution, it is more suitable for multiphase and local region-based image segmentations, and it is less sensitive to noise and initialization. The experimental results show the potential of the proposed method.	active contour model;algorithm;algorithmic efficiency;computation;courant–friedrichs–lewy condition;cure for lymphoma foundation;dimensionality reduction;discriminant;disjunctive normal form;image segmentation;locality of reference;numerical analysis;physical object;signature;time complexity;two-phase locking;variational principle;biologic segmentation	Fitsum Mesadi;Mujdat Cetin;Tolga Tasdizen	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2682980	computer vision;mathematical optimization;shape;computer science;level set;machine learning;segmentation-based object categorization;mathematical model;mathematics;image segmentation;scale-space segmentation;computational model;level set method	Vision	49.07903825968921	-71.52406433236277	162286
0a7797ae4390bba971184e97ccae7f429bc32e52	model-resolution-based basis pursuit deconvolution improves diffuse optical tomographic imaging	experimental gelatin phantom model resolution based basis pursuit deconvolution method diffuse optical tomographic imaging image reconstruction numerical gelatin phantom;near infrared imaging basis pursuit deconvolution diffuse optical tomography image reconstruction;image resolution;phantoms;gelatin;optical tomography;supercomputer education research centre;image reconstruction optical imaging deconvolution optical scattering jacobian matrices biomedical optical imaging tomography;image reconstruction;medical image processing;deconvolution;biomedical optical imaging;phantoms biomedical optical imaging deconvolution gelatin image reconstruction image resolution medical image processing optical tomography	The image reconstruction problem encountered in diffuse optical tomographic imaging is ill-posed in nature, necessitating the usage of regularization to result in stable solutions. This regularization also results in loss of resolution in the reconstructed images. A frame work, that is attributed by model-resolution, to improve the reconstructed image characteristics using the basis pursuit deconvolution method is proposed here. The proposed method performs this deconvolution as an additional step in the image reconstruction scheme. It is shown, both in numerical and experimental gelatin phantom cases, that the proposed method yields better recovery of the target shapes compared to traditional method, without the loss of quantitativeness of the results.	algorithm;algorithmic efficiency;appendix;basis pursuit;deconvolution;entity class - imaging modality;gelatin;image resolution;imaging phantom;iterative reconstruction;jacobian matrix and determinant;matrix regularization;modality (human–computer interaction);nn304;numerical analysis;phantoms, imaging;reconstruction conjecture;singular value decomposition;snowflake vitreoretinal degeneration;solutions;the matrix;well-posed problem	Jaya Prakash;Hamid Dehghani;Brian W. Pogue;Phaneendra K. Yalavarthy	2014	IEEE Transactions on Medical Imaging	10.1109/TMI.2013.2297691	iterative reconstruction;computer vision;radiology;image resolution;computer science;digital image correlation;deconvolution;mathematics;optics;medical physics	Vision	52.0145071830987	-80.13404538943793	162573
4fd620738a73c017184746d8fd906ace26c9bee0	volume preserving image registration via a post-processing stage	liver;volume;velocity field;volumen;medical image;image registration;registration;fourier analysis;algorithms;fluid dynamics;synthetic data;volume change;vector field;cyclic reduction	In this paper a method to remove the divergence from a vector field is presented. When applied to a displacement field, this will remove all local compression and expansion. The method can be used as a post-processing step for (unconstrained) registered images, when volume changes in the deformation field are undesired. The method involves solving Poisson’s equation for a large system. Algorithms to solve such systems include Fourier analysis and Cyclic Reduction. These solvers are vastly applied in the field of fluid dynamics, to compensate for numerical errors in calculated velocity fields. The application to medical image registration as described in this paper, has to our knowledge not been done before. To show the effect of the method, it is applied to the registration of both synthetic data and dynamic MR series of the liver. The results show that the divergence in the displacement field can be reduced by a factor of 10 − 1000 and that the accuracy of the registration increases.	algorithm;computation;cyclic reduction;decade (log scale);digital library;digraphs and trigraphs;displacement mapping;experiment;fourier analysis;gradient descent;image registration;image resolution;numerical analysis;synthetic data;velocity (software development);video post-processing;voxel	Reinhard Hameeteman;Jifke F. Veenland;Wiro J. Niessen	2008		10.1117/12.770646	computer vision;mathematical optimization;vector field;fluid dynamics	Vision	49.492017681061036	-75.82749087039859	162600
890eed4018dc70646c73e0dc681472831f151b1f	unsupervised recognition and characterization of the reflected laser lines for robotic gas metal arc welding	welding;measurement by laser beam;lasers;laser noise;image segmentation;cameras;robustness	Unsupervised recognition of the reflected laser lines from the arc-light-modified background is prerequisite for the subsequent measurement and characterization of the weld pool shape, which is of great importance for the modeling and control of robotic arc welding. To facilitate the unsupervised recognition, the reflected laser lines need to be segmented as accurate as possible, which requires the segmented laser lines to be as continuous as possible to decrease the adverse effect of the noise blobs. In this paper, the intensity distribution caused by the arc light in the captured image is modeled. Based on the model, an efficient and robust approach is proposed, and it comprises six parts: reduction of the uneven image background by a difference operation, spline enhancement to remove the fuzziness, a gradient detection filter to eliminate the uneven background further, segmentation by an effective threshold selection method, removal of the noise blobs adaptively, and clustering based on the online computed slope of the laser line. After the laser line is clustered, a second-order polynomial is fitted to it. Finally, the weld pool is characterized by the parameters of the clustered laser line and its fitted polynomial. Experimental results verified that the proposed approach for unsupervised reflected laser line recognition is significantly superior to the state-of-the-art approach in terms of recognition accuracy.	cluster analysis;edge enhancement;gradient;polynomial;robot;spline (mathematics)	Zhen Zhou Wang	2017	IEEE Transactions on Industrial Informatics	10.1109/TII.2017.2657786	computer science;weld pool;robustness (computer science);cluster analysis;welding;arc welding;computer vision;image segmentation;laser;gas metal arc welding;artificial intelligence	Vision	46.751283693821414	-67.15051252364296	162638
335f345ead8c00ff2558da7cc0ce2924226e3b9b	fast automatic medical image segmentation based on spatial kernel fuzzy c-means on level set method	spatial kernel fuzzy c means;fast two cycle;chan vese;medical image segmentation;level set method	Fast two-cycle (FTC) model is an efficient and the fastest Level set image segmentation. But, its performance is highly dependent on appropriate manual initialization. This paper proposes a new algorithm by combining a spatially constrained kernel-based fuzzy c-means (SKFCM) algorithm and an FTC model to overcome the mentioned problem. The approach consists of two successive stages. First, the SKFCM makes a rough segmentation to select the initial contour automatically. Then, a fuzzy membership matrix of the region of interest, which is generated by the SKFCM, is used in the next stage to produce an initial contour. Eventually, the FTC scheme segments the image by a curve evolution based on the level set. Moreover, the fuzzy membership degree from the SKFCM is incorporated into the fidelity term of the Chan–Vese model to improve the robustness and accuracy, and it is utilized for the data-dependent speed term of the FTC. A performance evaluation of the proposed algorithm is carried out on the synthetic and real images. The experimental results show that the proposed algorithm has advantages in accuracy, computational time and robustness against noise in comparison with the KFCM, the SKFCM, the hybrid model of the KFCM and the FTC, and five different level set methods on medical image segmentation.	approximation algorithm;computation;data dependency;database;dhrystone;fastest;fermat's principle;fuzzy clustering;image segmentation;loss function;matthews correlation coefficient;optimization problem;performance evaluation;pixel;region of interest;robustness (computer science);rough set;run time (program lifecycle phase);sørensen–dice coefficient;time complexity	Siavash Alipour;Jamshid Shanbehzadeh	2014	Machine Vision and Applications	10.1007/s00138-014-0606-5	computer vision;mathematical optimization;computer science;machine learning;mathematics;image segmentation;scale-space segmentation;level set method	Vision	44.09645638331941	-72.62740054721935	162645
0e6848aa6a09313fe1ea12798252b704fb570f34	image segmentation by histogram thresholding using hierarchical cluster analysis	image segmentation;method of image;inter class variance;clustering;intra class variance;image thresholding;hierarchical cluster analysis	This paper proposes a new method of image thresholding by using cluster organization from the histogram of an image. A new similarity measure proposed is based on inter-class variance of the clusters to be merged and the intra-class variance of the new merged cluster. Experiments on practical images illustrate the effectiveness of the new method. 2006 Elsevier B.V. All rights reserved.	algorithm;binary prefix;cluster analysis;grayscale;ground truth;hierarchical clustering;image segmentation;iterative method;newman's lemma;otsu's method;similarity measure;thresholding (image processing)	Agus Zainal Arifin;Akira Asano	2006	Pattern Recognition Letters	10.1016/j.patrec.2006.02.022	computer vision;computer science;machine learning;pattern recognition;balanced histogram thresholding;hierarchical clustering;region growing;thresholding;image segmentation;cluster analysis;image histogram	Vision	45.00134206331198	-68.3807108786071	162729
39eb9a38e3ed60a2da4fe754227846d991560ba4	active shape model with inter-profile modeling paradigm for cardiac right ventricle segmentation		In this work, a novel active shape model (ASM) paradigm is proposed to segment the right ventricle (RV) in cardiac magnetic resonance image sequences. The proposed paradigm includes modifications to two fundamental steps in the ASM algorithm. The first modification includes employing the 2D-principal component analysis (PCA) to capture the inter-profile relations among shape's neighboring landmarks and then model the inter-profile variations between the training set. The second modification is based on using a multi-stage searching algorithm to find the best profile match based on the best maintained profile's relations and thus the best shape fitting in an iterative manner. The developed methods are validated using a database of short axis cine bright blood MRI images for 30 subjects with total of 90 images. Our results show that the segmentation error can be reduced by about 0.4 mm and contour overlap increased by about 4% compared to the classical ASM technique with paired Student's t-test indicates statistical significance to a high degree for our results. Furthermore, comparison with literature shows that the proposed method decreases the RV segmentation error significantly.	active shape model;apache axis;cine procedure;heart ventricle;iterative method;numerous;p-value;principal component analysis;programming paradigm;resonance;right ventricular structure;search algorithm;sinoatrial node;test set;biologic segmentation;t test	Mohammed S. ElBaz;Ahmed S. Fahmy	2012	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-33415-3_85	artificial intelligence;computer vision;pattern recognition;active shape model;ventricle;active appearance model;training set;mathematics;segmentation;statistical significance;search algorithm	Vision	41.66827220199105	-78.15274296641648	162906
1a05e8e798c8b0294f616ad0f3444bf69e93a5e2	image segmentation with a bounding box prior	graph theory;global energy minimization framework;image segmentation;standalone heuristic image segmentation bounding box prior object bounding box interaction paradigm topological prior global energy minimization framework np hard integer program optimization strategy linear relaxation graph cut algorithm pinpointing rounding method fractional lp solution;object bounding box;standalone heuristic;optimization strategy;relaxation theory computational complexity graph theory image segmentation integer programming linear programming;pinpointing;linear relaxation;shape;machine learning;topological prior;integer programming;image edge detection;fractional lp solution;computational complexity;heuristic algorithms;graph cut;rounding method;np hard integer program;pixel;linear programming;ip networks;interactive image segmentation;optimization;interaction paradigm;energy minimization;bounding box prior;image segmentation iterative algorithms power generation economics mice active contours computer vision linear programming image reconstruction;integer program;relaxation theory;graph cut algorithm	User-provided object bounding box is a simple and popular interaction paradigm considered by many existing interactive image segmentation frameworks. However, these frameworks tend to exploit the provided bounding box merely to exclude its exterior from consideration and sometimes to initialize the energy minimization. In this paper, we discuss how the bounding box can be further used to impose a powerful topological prior, which prevents the solution from excessive shrinking and ensures that the user-provided box bounds the segmentation in a sufficiently tight way. The prior is expressed using hard constraints incorporated into the global energy minimization framework leading to an NP-hard integer program. We then investigate the possible optimization strategies including linear relaxation as well as a new graph cut algorithm called pinpointing. The latter can be used either as a rounding method for the fractional LP solution, which is provably better than thresholding-based rounding, or as a fast standalone heuristic. We evaluate the proposed algorithms on a publicly available dataset, and demonstrate the practical benefits of the new prior both qualitatively and quantitatively.	algorithm;cut (graph theory);energy minimization;graph cuts in computer vision;heuristic;image segmentation;integer programming;linear programming relaxation;mathematical optimization;minimum bounding box;np-hardness;programming paradigm;rounding;thresholding (image processing)	Victor S. Lempitsky;Pushmeet Kohli;Carsten Rother;Toby Sharp	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459262	mathematical optimization;combinatorics;discrete mathematics;bounding interval hierarchy;integer programming;cut;shape;computer science;linear programming;graph theory;machine learning;mathematics;geometry;image segmentation;computational complexity theory;energy minimization;pixel	Vision	53.72675386913709	-72.81172932988977	162941
184de13694c6050c3544d60990008800bc091b24	contour tracking on sequences of ventriculographic images. a comparison between gradient of gaussian and first order absolute moment	first order	 In this paper the properties of both the GoG and FOAM operators, aswell as the results obtained with their respective localization procedures, arecompared. FOAM (first order absolute moment) and GoG (gradient ofGaussian) filters are used to enhance the discontinuities between the gray levelsof the different structures of the images. The contour we are tracking is thenlocalized through the filtered images by starting from an approximate contour. 	contour line;gradient	Vincenzo Gemignani;Marcello Demi;Antonio Benassi	1998				Vision	49.01391995784669	-68.61159639493543	162956
c620678dbe3d837a0b1f700841d439b5263bb0c9	a framework for the detection of acute renal rejection with dynamic contrast enhanced magnetic resonance imaging	anatomical structure;respiratory system;kidney function;image segmentation;perfusion curves;pneumodynamics biomedical mri image classification image registration kidney medical image processing;patient breathing;acute rejection;image classification;kidney dysfunction acute renal rejection dynamic contrast enhanced magnetic resonance imaging graft failure kidney transplantation rejection transplant classification nonrigid registration algorithms kidney motion patient breathing perfusion curves contrast agent cortex;deformable models;cortex;magnetic resonance image;nonrigid registration algorithms;rejection transplant classification;density functional theory;kidney motion;dynamic contrast enhanced;acute renal rejection;kidney transplantation;shape;graft failure;image edge detection;biopsy;magnetic resonance imaging biopsy image segmentation image edge detection immune system anatomical structure deformable models density functional theory shape transportation;kidney dysfunction;medical image processing;magnetic resonance imaging;image registration;transportation;immune system;dynamic contrast enhanced magnetic resonance imaging;biomedical image processing;kidney transplant;kidneys;ultrasonography;contrast agent;nonrigid registration;automatic classification;pneumodynamics;deformable model;early detection;kidney;biomedical mri	Acute rejection is the most common reason of graft failure after kidney transplantation, and early detection is crucial to survive the transplanted kidney function. In this paper we introduce a new approach for the automatic classification of normal and acute rejection transplants from dynamic contrast enhanced magnetic resonance imaging (DCE-MRI). The proposed algorithm consists of three main steps; the first step isolates the kidney from the surrounding anatomical structures by evolving a deformable model based on two density functions; the first function describes the distribution of the gray level inside and outside the kidney region and the second function describes the prior shape of the kidney. In the second step, nonrigid-registration algorithms are employed to account for the motion of the kidney due to patient breathing, and finally, the perfusion curves that show the transportation of the contrast agent into the tissue are obtained from the cortex and used in the classification of normal and acute rejection transplants. Applications of the proposed approach yield promising results that would, in the near future, replace the use of current technologies such as nuclear imaging and ultrasonography, which are not specific enough to determine the type of kidney dysfunction	algorithm;contrast ratio;grafting (decision trees);grayscale;medical ultrasound;rejection sampling;resonance	Aly A. Farag;Ayman El-Baz;Seniha Esen Yuksel;Mohamed Abou El-Ghar;Tarek Eldiasty	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1624942	transport;contextual image classification;immune system;radiology;medicine;pathology;shape;image registration;magnetic resonance imaging;respiratory system;renal function;cortex;image segmentation;density functional theory;anatomy	Vision	40.59350754601902	-76.89906163024587	163056
9a8b94e7e8867dc9fc488ed4e0327ece1b68ac38	a surface-volume matching process using a markov random field model for cardiac motion extraction in msct imaging	3d segmentation;computed tomography;simulated annealing;surface reconstruction;markov random field;velocity field;temporal resolution;image sequence;global optimization;kinetics	Multislice Computed Tomography (MSCT) scanners offers new perspectives for cardiac kinetics evaluation with 3D time image sequences of high contrast and spatio-temporal resolutions. A new method is proposed for cardiac motion extraction in Multislice CT. Based on a 3D surface-volume matching process, it provides the detection of the heart left cavities along the acquired sequence and the estimation of their 3D surface velocity fields. A 3D segmentation step and surface reconstruction process are first applied on only one image of the sequence to obtain a 3D mesh representation for one t time. A Markov Random Field model is defined to find best correspondences between 3D mesh nodes at t time and voxels in the next volume at t + 1 time. A simulated annealing is used to perform a global optimization of the correspondences. First results obtained on simulated and real data show the good behaviour of this method.	ct scan;dynamic data;global optimization;kinetics internet protocol;markov chain;markov random field;mathematical optimization;tomography;velocity (software development);voxel	Antoine Simon;Mireille Garreau;Dominique Boulmier;Jean-Louis Coatrieux;Hervé Le Breton	2005		10.1007/11494621_45	computer vision;mathematical optimization;simulation;geography	Vision	44.695041562969735	-75.62004469400952	163066
82f1035edba3f5b81768d833ac00edc4f690cafd	a registration method of fundus images based on edge detection and phase-correlation	eye;time complexity;edge detection;edge extraction;correlation methods;gray sensitive deficiency;gray sensitive deficiency image registration method edge detection phase correlation retinal fundus image optic disc edge extraction retinal vein retinal artery;feature extraction;medical image processing;image registration;retinal artery;retinal vein;retinal fundus image;image edge detection retina veins arteries diseases image processing image resolution image registration fourier transforms data mining;image registration method;blood vessels;optic disc;phase correlation;medical image processing blood vessels correlation methods edge detection eye feature extraction image registration	According to the characteristics of retinal fundus images, one novel method of registration based on phase-correlation was proposed in this paper. This method takes advantage of optic disc to match two images coarsely. Meanwhile edge detection was applied to extract the edge of abundant retinal veins and arteries in order to raise the precision of registration. This method also overcomes the gray-sensitive deficiency in phase-correlation method. And the experiment results show that the method not only enhances the precision of registration but also reduces time complexity	angular defect;edge detection;in-phase and quadrature components;phase correlation;time complexity	Wei Wang;Houjin Chen;Jupeng Li;Jiangbo Yu	2006	First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)	10.1109/ICICIC.2006.397	time complexity;computer vision;edge detection;feature extraction;computer science;image registration;phase correlation;computer graphics (images)	Robotics	39.92514748612622	-75.73761384131666	163076
a0070dacc57634e9a8f953694bab5daeab8f7eab	quantitative comparison of spot detection methods in fluorescence microscopy	unsupervised learning;learning spot detection methods fluorescence microscopy biological image analysis live cell imaging signal to noise ratio unsupervised methods supervised methods h dome transform mathematical morphology multiscale variance stabilizing transform;detectors;multiscale variance stabilizing transform;image filtering;mathematical morphology;fluorescence;learning;supervised methods;biological image analysis;biomedical imaging;h dome transform;supervised machine learning;object detection fluorescence microscopy image filtering machine learning noise reduction;optical imaging;proteins;machine learning;fluorescence microscopy;medical image processing;noise reduction;unsupervised learning biomedical optical imaging fluorescence mathematical morphology medical image processing object detection optical microscopy;live cell imaging;quantitative analysis;fluorescence image analysis object detection optical microscopy biomedical imaging optical imaging signal to noise ratio machine learning detectors proteins;algorithms artificial intelligence computer simulation discriminant analysis eukaryotic cells green fluorescent proteins hela cells humans image processing computer assisted microscopy fluorescence microtubules models theoretical normal distribution poisson distribution roc curve sensitivity and specificity;image analysis;ground truth;biomedical optical imaging;signal to noise ratio;unsupervised methods;optical microscopy;quantitative evaluation;object detection;spot detection methods	Quantitative analysis of biological image data generally involves the detection of many subresolution spots. Especially in live cell imaging, for which fluorescence microscopy is often used, the signal-to-noise ratio (SNR) can be extremely low, making automated spot detection a very challenging task. In the past, many methods have been proposed to perform this task, but a thorough quantitative evaluation and comparison of these methods is lacking in the literature. In this paper, we evaluate the performance of the most frequently used detection methods for this purpose. These include seven unsupervised and two supervised methods. We perform experiments on synthetic images of three different types, for which the ground truth was available, as well as on real image data sets acquired for two different biological studies, for which we obtained expert manual annotations to compare with. The results from both types of experiments suggest that for very low SNRs ( ¿ 2), the supervised (machine learning) methods perform best overall. Of the unsupervised methods, the detectors based on the so-called h -dome transform from mathematical morphology or the multiscale variance-stabilizing transform perform comparably, and have the advantage that they do not require a cumbersome learning stage. At high SNRs ( > 5), the difference in performance of all considered detectors becomes negligible.	detectors;exanthema;experiment;ground truth;machine learning;mathematical morphology;mathematics;microscopy, fluorescence;sample variance;signal-to-noise ratio;supervised learning;synthetic intelligence;unsupervised learning;quantitative	Ihor Smal;Marco Loog;Wiro J. Niessen;Erik H. W. Meijering	2010	IEEE Transactions on Medical Imaging	10.1109/TMI.2009.2025127	unsupervised learning;fluorescence microscope;computer vision;detector;image analysis;mathematical morphology;ground truth;fluorescence;computer science;quantitative analysis;machine learning;noise reduction;optical imaging;optical microscope;signal-to-noise ratio;live cell imaging	Vision	40.19088051203937	-73.08879295610257	163212
44c4e2a112c2b0603bcbff336cdb17ec204a6713	abnormality detection via iterative deformable registration and basis-pursuit decomposition	training;abnormality detection;biomedical imaging;convex optimization;sparse representation abnormality detection basis pursuit brain pathology convex optimization medical image registration;medical image registration;alzheimer s disease patients mri iterative deformable registration basis pursuit decomposition automatic abnormal region detection medical images image decomposition image grid statistical model regional sparse learning markov random field regularization learned normative model pathological patterns statistical significance test multiple image features spatial normalization brain lesions;lesions;feature extraction;pathology training robustness lesions biomedical imaging feature extraction;statistical testing biomedical mri brain diseases image registration iterative methods learning artificial intelligence markov processes medical image processing neurophysiology random processes;robustness;basis pursuit;sparse representation;brain pathology;pathology	We present a generic method for automatic detection of abnormal regions in medical images as deviations from a normative data base. The algorithm decomposes an image, or more broadly a function defined on the image grid, into the superposition of a normal part and a residual term. A statistical model is constructed with regional sparse learning to represent normative anatomical variations among a reference population (e.g., healthy controls), in conjunction with a Markov random field regularization that ensures mutual consistency of the regional learning among partially overlapping image blocks. The decomposition is performed in a principled way so that the normal part fits well with the learned normative model, while the residual term absorbs pathological patterns, which may then be detected through a statistical significance test. The decomposition is applied to multiple image features from an individual scan, detecting abnormalities using both intensity and shape information. We form an iterative scheme that interleaves abnormality detection with deformable registration, gradually improving robustness of the spatial normalization and precision of the detection. The algorithm is evaluated with simulated images and clinical data of brain lesions, and is shown to achieve robust deformable registration and localize pathological regions simultaneously. The algorithm is also applied on images from Alzheimer's disease patients to demonstrate the generality of the method.	alzheimer's disease;atrophic;basis pursuit;cerebral cortex;congenital abnormality;database;databases;fits;generic drugs;infarction;iteration;iterative method;markov chain;markov random field;numerous;p-value;patients;sensor;software framework;sparse matrix;statistical model;algorithm;registration - actclass	Ke Zeng;Güray Erus;Aristeidis Sotiras;Russell Takeshi Shinohara;Christos Davatzikos	2016	IEEE Transactions on Medical Imaging	10.1109/TMI.2016.2538998	computer vision;convex optimization;basis pursuit;feature extraction;computer science;machine learning;pattern recognition;sparse approximation;mathematics;robustness	Vision	42.65110738711773	-77.12329067521719	163238
86a85909f77f599a72278c02aa2547ef3284f800	chromatic / achromatic separation in noisy document images	image segmentation;colored noise;image resolution;chromatic achromatic;gray scale;noise measurement;estimation;image color analysis;image colour analysis;color noise;image segmentation document image processing image colour analysis;document image processing;color noise achromatic separation noisy document image chromatic zones;split document image color noise chromatic achromatic;split document image;colored noise image color analysis gray scale estimation noise measurement image resolution	This paper presents a new method to split an image into chromatic and achromatic zones. The proposed algorithm is dedicated to document images. It is robust to the color noise introduced by scanners and image compression. It is also parameter-free since it automatically adapts to the image content.	algorithm;arbitrary-precision arithmetic;baseline (configuration management);closing (morphology);image compression;image scaling;mathematical morphology;pixel;thickness (graph theory)	Asma Ouji;Yann Leydier;Frank Lebourgeois	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.42	computer vision;colors of noise;computer science;statistics;computer graphics (images)	Vision	50.69348540776709	-66.87567329033013	163265
7ab1ca0f38b935d468e8420e07841724f2ead609	cell segmentation using hessian-based detection and contour evolution with directional derivatives	health research;level sets;uk clinical guidelines;biological patents;cell segmentation;geodesic active contour;intensity profile perpendicular;active contour;intensity profile perpendicular cell segmentation hessian based detection contour evolution directional derivatives biological live cell imaging geodesic active contour halo effect phase contrast microscopy level set contour evolution;image segmentation;halo effect;curve evolution;europe pubmed central;hessian based detection;differential geometry;contour evolution;citation search;level set;active contours;indexing terms;directional derivative;force;ridge detection;evolution biology;medical image processing differential geometry hessian matrices image segmentation;uk phd theses thesis;phase contrast microscopy;image edge detection;directional derivatives;medical image processing;imaging;evolution biology cells biology image segmentation shape measurement image analysis algorithm design and analysis level measurement phase measurement phase detection active contours;life sciences;live cell imaging;biomedical image processing;biological live cell imaging;level set contour evolution;uk research reports;medical journals;europe pmc;biomedical research;hessian matrices;bioinformatics;ridge detection cell segmentation biomedical image processing level sets active contour	The large amount of data produced by biological live cell imaging studies of cell behavior requires accurate automated cell segmentation algorithms for rapid, unbiased and reproducible scientific analysis. This paper presents a new approach to obtain precise boundaries of cells with complex shapes using ridge measures for initial detection and a modified geodesic active contour for curve evolution that exploits the halo effect present in phase-contrast microscopy. The level set contour evolution is controlled by a novel spatially adaptive stopping function based on the intensity profile perpendicular to the evolving front. The proposed approach is tested on human cancer cell images from LSDCAS and achieves high accuracy even in complex environments.	active contour model;algorithm;cell (microprocessor);evolution;hessian;in-phase and quadrature components;medical imaging;microscopy, phase-contrast;neoplasms;biologic segmentation;cancer cell	Ilker Ersoy;Filiz Bunyak;Michael A. Mackey;Kannappan Palaniappan	2005	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712127	medical imaging;differential geometry;computer vision;computer science;level set;directional derivative;mathematics;computer graphics (images)	Robotics	40.729715376156186	-74.64703210325855	163317
3f0585b8a544e0f2d66cef02d8050fa0bb07ada5	an algorithm of extracting intracranial structures from cerebral computed tomography	mathematic morphology;mathematical morphology;image segmentation;cerebral computed tomography;computed tomography;circuit faults;current transformers;skull image;automatic segmentation;fault currents;biomedical imaging;skull;linear filtering;cerebral computed;feature extraction;medical image processing;computed tomography filtering algorithms skull image segmentation software algorithms lesions morphological operations biomedical imaging maximum likelihood detection morphology;bone;computerised tomography;cerebral computed image segmentation;extraction algorithm;automatic segmentation intracranial structure cerebral computed tomography extraction algorithm skull image linear filtering mathematic morphology horizontal scanning algorithm;intracranial structure;horizontal scanning algorithm;medical image processing bone computerised tomography feature extraction filtering theory image segmentation;filtering theory	A novel algorithm is proposed in this paper in terms of the existing extraction algorithms for intracranial structures on cerebral computed tomography (CT) lacking automation. The skull image from CT is characterized by great width and high gray-level, considering that, the proposed algorithm uses linear filtering to extract the outline of skull. and then mathematic morphology and horizontal scanning algorithm is employed for automatic segmentation of intracranial structures on cerebral CT. Experiment on 100 cases of cerebral CT demonstrates that the ratio of automatic segmentation reaches up to 99%, and the results are accurate and encouraging.	algorithm;ct scan;mathematical morphology;tomography	Haibo Wang;Xueyao Li	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1636	medical imaging;current transformer;computer vision;mathematical morphology;feature extraction;computer science;machine learning;linear filter;image segmentation;computed tomography	Vision	40.27665200235288	-75.51048941082507	163508
42e76b54764151e7158560f17e7ba4084d33cfc0	number-driven perceptual segmentation of natural color images for easy decision of optimal result	image segmentation;natural color images;fuzzy inference perceptual image segmentation natural color images hierarchical algorithm;indexing terms;texture features;boundary refinement number driven perceptual segmentation natural color image segmentation fuzzy based hierarchical algorithm optimal segmentation fuzzy based homogeneity measure texture feature hierarchical segmentation;image texture;image colour analysis;fuzzy inference;perceptual image segmentation;image texture image colour analysis image segmentation;computer simulation;hierarchical algorithm;image segmentation color merging gabor filters clustering algorithms computer science systems engineering and theory computer simulation partitioning algorithms inference algorithms;color image	This paper proposes number-driven perceptual segmentation of natural color images using a fuzzy-based hierarchical algorithm for an easy decision of the optimal segmentation result. A fuzzy-based homogeneity measure makes a fusion of the L*a*b* color features and the SGF texture features. Proposed hierarchical segmentation method is performed in four stages: simple splitting, local merging, global merging and boundary refinement. The effectiveness of the proposed method is confirmed through computer simulations that demonstrate an easy determination of the optimal segmentation result.	algorithm;computer simulation;refinement (computing);smart game format	Junji Maeda;Akimitsu Kawano;Sato Saga;Yukinori Suzuki	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379143	computer simulation;image texture;computer vision;index term;color image;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;connected-component labeling	Robotics	43.534591878457846	-66.77993426895632	163656
a844014feeaf399a0c23cdb0c8524253e05db5ef	preprocessing for digital video using mathematical morphology			digital video;mathematical morphology;preprocessor	Nicky Young	2003				Vision	41.75640508950524	-70.89966677404279	163819
b04a57052f5c3d9f961eca3c7564f699c3b9bdbf	edge detection based on ordered directionally monotone functions		We present an image edge detection algorithm that is based on the concept of ordered directionally monotone functions, which permit our proposal to consider the direction of the edges at each pixel and perform accordingly. The results of this method are presented to the EUSFLAT 2017 Competition on Edge Detection.	edge detection;monotone	Mikel Sesma-Sara;Humberto Bustince;Edurne Barrenechea Tartas;Julio Lafuente;Anna Kolesárová;Radko Mesiar	2017		10.1007/978-3-319-66827-7_27	image processing;monotone polygon;pixel;mathematical optimization;edge detection;mathematics	Logic	45.87423301814368	-67.756754931956	164271
9b143547d8d1f10b96ee7f004263b9dd8d725b10	shape extraction via heat flow analogy	heat conduction;heat flow;multiple objectives;medical image;region of interest;active contour model	In this paper, we introduce a novel evolution-based segmentation algorithm by using the heat flow analogy, to gain practical advantage. The proposed algorithm consists of two parts. In the first part, we represent a particular heat conduction problem in the image domain to roughly segment the region of interest. Then we use geometric heat flow to complete the segmentation, by smoothing extracted boundaries and removing possible noise inside the prior segmented region. The proposed algorithm is compared with active contour models and is tested on synthetic and medical images. Experimental results indicate that our approach works well in noisy conditions without pre-processing. It can detect multiple objects simultaneously. It is also computationally more efficient and easier to control and implement in comparison to active contour models.	active contour model;algorithm;gradient;preprocessor;region of interest;simulation;smoothing;synthetic intelligence	Cem Direkoglu;Mark S. Nixon	2007		10.1007/978-3-540-74607-2_50	computer vision;mathematical optimization;computer science;active contour model;mathematics;thermal conduction;heat transfer;region of interest	Vision	45.50453356059845	-71.743912591662	164475
edc650410b40dd5f8d6dbdcad6321066bd0f9083	a cascaded refinement gan for phase contrast microscopy image super resolution		Phase contrast microscopy is a widely-used non-invasive technique for monitoring live cells over time. High-throughput biological experiments expect a wide-view (i.e., a low microscope magnification) to monitor the entire cell population and a high magnification on individual cell’s details, which is hard to achieve simultaneously. In this paper, we propose a cascaded refinement Generative Adversarial Network (GAN) for phase contrast microscopy image super-resolution. Our algorithm uses an optic-related data enhancement and super-resolves a phase contrast microscopy image in a coarse-to-fine fashion, with a new loss function consisting of a content loss and an adversarial loss. The proposed algorithm is both qualitatively and quantitatively evaluated on a dataset of 500 phase contrast microscopy images, showing its superior performance for super-resolving phase contrast microscopy images. The proposed algorithm provides a computational solution on achieving a high magnification on individual cell’s details and a wide-view on cell populations at the same time, which will benefit the microscopy community.	super-resolution imaging	Liang Han;Zhaozheng Yin	2018		10.1007/978-3-030-00934-2_39	artificial intelligence;computer vision;computer science;entire cell;magnification;phase contrast microscopy;microscope;microscopy;superresolution;population	Vision	39.884524258933965	-72.89738115624948	164629
d9847893ceddc9a6dfeba40118d07be118d4811f	similarity measure for fiber clustering: a constant time complexity algorithm	time measurement;time complexity;fiber segmentation white matter fiber tracts diffusion tensor imaging density based clustering fiber clustering;shape measurement;hdd distance constant time complexity algorithm fiber clustering algorithm neuroscience white matter brain fiber data set flexible fibers similarity measure center of mass distance similarity density based clustering algorithm distance based technique dynamic time warping dtw mean of closest point distance mcp distance hausdorf distance;gold;shape;clustering algorithms;set theory brain computational complexity medical image processing neurophysiology pattern clustering;diffusion tensor imaging;shape clustering algorithms shape measurement time measurement time complexity diffusion tensor imaging gold	Recently, fiber clustering algorithms have become an important tool in neuroscience for grouping the white matter tracts into anatomically meaningful bundles. The results of clustering can be used for quantification and comparison between different brains to find out abnormalities or unusual features. One essential problem in fiber clustering is to provide a similarity measure for a pair of fibers. Although there are many methods proposed in literature, most of them suffer from high time complexity, which causes difficulty in dealing with very large fiber data sets. In this paper, we propose a new flexible fibers similarity measure, which is only based on center of mass, start and end point of each fiber to capture the shape and distance similarity between them with only O(1) time complexity. Our new method is used together with a density-based clustering algorithm to segment fibers into groups. Experiments on real data sets prove the efficiency and effectiveness of our approach in comparison with other distance-based techniques like namely Dynamic Time Warping (DTW), Mean of Closest Point (MCP) distance and Hausdorf (HDD) distance.	algorithm;cluster analysis;dynamic time warping;hard disk drive;optical fiber;similarity measure;time complexity;time series	Tran Anh Quan;Bay Vo	2015	2015 Seventh International Conference on Knowledge and Systems Engineering (KSE)	10.1109/KSE.2015.48	gold;time complexity;diffusion mri;correlation clustering;k-medians clustering;fuzzy clustering;shape;computer science;theoretical computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;time;clustering high-dimensional data	DB	42.41371173947793	-75.36692688151939	164644
c6754d93efdc1ede9d763e03b2f154d2b97a8d42	a bayesian reconstruction method with marginalized uncertainty model for camera motion in microrotation imaging	feature based method;modelizacion;relative position;image tridimensionnelle;statistical cost function;photomicrography;projection direction;cell manipulation;fluorescence;uncertainty modeling;bayes theorem cells computer simulation humans image processing computer assisted microscopy fluorescence phantoms imaging photomicrography rotation;image processing;motion compensation;cost function;bayesian approach;statistical inverse problem;genie biomedical;uncertainty;fluorescence imaging;anatomia patologica;bayes methods;methode bayes;microscopie optique;marginalized uncertainty model;simulacion numerica;procesamiento imagen;bayes theorem;biologie cellulaire;reconstruction algorithms;bayesian methods;motion estimation;prior knowledge;uncertainty handling;bayesian methods reconstruction algorithms uncertainty cameras image reconstruction optical imaging parameter estimation motion estimation optical microscopy cost function;anatomic pathology;light microscopy;problema inverso;image processing computer assisted;gaussian uncertainty model;biologia celular;optical microscope;traitement image;statistical model;fluorescencia;modelisation;cells;camera motion;reconstruction image;image series;optical imaging;posterior distribution;nuisance parameter;biomedical engineering;inverse problem;reconstruccion imagen;image reconstruction;medical image processing;simulation numerique;imaging;microscopy fluorescence;statistical inverse problems;phantoms imaging;modele statistique;optical microscopy bayesian reconstruction marginalized uncertainty model camera motion microrotation imaging 3d structure reconstruction projection direction feature based method nuisance parameter posterior distribution gaussian uncertainty model statistical cost function image series;tridimensional image;formation image;anatomopathologie;modelo estadistico;ingenieria biomedica;3d structure reconstruction;microrotation imaging;humans;microscopia optica;formacion imagen;parameter estimation	Reconstruction of a 3-D structure from multiple projection images requires prior knowledge of projection directions or camera motion parameters that describe the relative positions and orientations of 3-D structure with respect to the camera. These parameters can be estimated using, for instance, the conventional correlation alignment and feature-based methods. However, the alignment methods are not perfect, where the inaccuracy of the estimated motion parameters causes artifacts in the reconstruction. To overcome this problem, we propose a Bayesian approach to reconstruct the object that takes the motion uncertainty distribution into account. Moreover, we consider the motion parameters as nuisance parameters and integrate them out from the posterior distribution, assuming a Gaussian uncertainty model, which yields a statistical cost function to be minimized. The proposed method is applied in microrotation fluorescence imaging, where we aim at 3-D reconstruction of a rotating object from an image series, acquired by an optical microscope. The experiments with simulated and real microrotation datasets demonstrate that the proposed method provides visually and numerically better results than the traditional reconstruction methods, which ignore the uncertainty of the motion estimates.	alignment;approximation;estimated;experiment;fluorescence imaging;handling (psychology);loss function;matrix regularization;mental orientation;microscope device component;morphologic artifacts;nonlinear system;normal statistical distribution;numerical analysis;population parameter;raw image format;series - set of composite instances;standard map;technical standard;density	Danai Laksameethanasan;Sami S. Brandt	2010	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2010.2043674	computer simulation;computer vision;bayesian probability;computer science;optical microscope;mathematics;optics;motion field;physics;statistics	Vision	47.439306329127255	-80.15355528404967	164831
8412fde054871160e0ec06007bbef587ad726ba0	unsupervised image-set clustering using an information theoretic framework	image content based retrieval unsupervised image set clustering information theoretic framework discrete image model continuous image model gaussian densities image database management image representation;unsupervised learning;modelizacion;hierarchical database analysis;pattern clustering;analyse amas;retrieval;gaussian processes;recherche image;image databank;kullback leibler divergence;image database;informacion mutual;apprentissage non supervise;indexing terms;image clustering;information bottleneck ib;modelisation;image databases spatial databases biomedical engineering mutual information image retrieval biomedical measurements navigation transaction databases jacobian matrices data analysis;information mutuelle;cluster analysis;image database management;content based retrieval pattern clustering visual databases image representation gaussian processes image retrieval;image representation;banco imagen;banque image;retrieval hierarchical database analysis image clustering image database management image modeling information bottleneck ib kullback leibler divergence mixture of gaussians mutual information;image search;gestion base donnee;mutual information;analisis cluster;mixture of gaussians;theorie information;information bottleneck;modeling;information theoretic;image modeling;content based retrieval;data base management;algorithms artificial intelligence cluster analysis databases factual image enhancement image interpretation computer assisted information storage and retrieval information theory pattern recognition automated;information theory;visual databases;image retrieval;teoria informacion	In this paper, we combine discrete and continuous image models with information-theoretic-based criteria for unsupervised hierarchical image-set clustering. The continuous image modeling is based on mixture of Gaussian densities. The unsupervised image-set clustering is based on a generalized version of a recently introduced information-theoretic principle, the information bottleneck principle. Images are clustered such that the mutual information between the clusters and the image content is maximally preserved. Experimental results demonstrate the performance of the proposed framework for image clustering on a large image set. Information theoretic tools are used to evaluate cluster quality. Particular emphasis is placed on the application of the clustering for efficient image search and retrieval.	biologic preservation;cluster analysis;image retrieval;information theory;mutual information;normal statistical distribution;unsupervised learning;density;statistical cluster	Jacob Goldberger;Shiri Gordon;Hayit Greenspan	2006	IEEE Transactions on Image Processing	10.1109/TIP.2005.860593	unsupervised learning;correlation clustering;constrained clustering;computer vision;determining the number of clusters in a data set;data stream clustering;systems modeling;information bottleneck method;index term;k-medians clustering;fuzzy clustering;information theory;image retrieval;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;mixture model;data mining;gaussian process;mathematics;cluster analysis;kullback–leibler divergence;mutual information;brown clustering;biclustering;statistics;clustering high-dimensional data;conceptual clustering	Vision	50.29153931000569	-73.73237565003777	164852
027e7b6fa1bc6944f413ea3faf5877b615bc117f	applying training hidden features to joint curve evolution for brain mri segmentation	active contour;spatial dependence;image segmentation;intensity based classification methods;curve evolution;expectation maximization algorithm statistical shape model active contours level set methods;statistical shape model;joint active contour model;training;level set;shape image segmentation principal component analysis three dimensional displays level set computational modeling training;shape recognition;region labeling;statistical method;active contours;spatial dependencies;shape variation;statistical model;shape based statistical model training hidden features joint curve evolution brain mri segmentation region labeling intensity based classification methods tissue intensity local statistical methods spatial dependencies shape variation joint active contour model medical image segmentation expectation maximization framework;local statistical methods;incomplete data;computational modeling;shape;statistical analysis;expectation maximization framework;expectation maximization;training hidden features;three dimensional displays;hidden variables;medical image processing;principal component analysis;expectation maximization algorithm;brain imaging;level set methods;shape based statistical model;tissue intensity;statistical analysis biomedical mri expectation maximisation algorithm image segmentation medical image processing shape recognition;medical image segmentation;level set method;likelihood function;joint curve evolution;active contour model;biomedical mri;brain mri segmentation;expectation maximisation algorithm	According to the level of information provided in images, segmentation techniques can be categorized into two groups. One is region-labeling, which obeys the intensity-based classification methods. Although modeling the tissue intensity is straightforward by applying local statistical methods and spatial dependencies, the results might suffer from noise and incomplete data. The second group of techniques applies active contour models, in which the objective is to find the optimal partition of the image domain using a closed or open curve by using prior constraints on the shape variation. However, estimating optimal curve is intractable due to the incomplete observation data. This paper extends a previously reported joint active contour model for medical image segmentation in a new Expectation-Maximization (EM) framework, wherein the evolution curve is constrained not only by a shape-based statistical model but also by applying a hidden variable model from the image observation. In this approach, the hidden variable model is defined by the local voxel labeling computed from its likelihood function, depended on the image functions and the prior anatomical knowledge. Comparative results on segmenting putamen and caudate shapes in MR brain images confirmed both robustness and accuracy of the proposed curve evolution algorithm.	active contour model;categorization;connected-component labeling;encode;evolution;expectation–maximization algorithm;hidden markov model;hidden variable theory;image segmentation;spatial reference system;statistical model;voxel	Mahshid Farzinfar;Eam Khwang Teoh;Zhong Xue	2010	2010 11th International Conference on Control Automation Robotics & Vision	10.1109/ICARCV.2010.5707770	computer vision;expectation–maximization algorithm;computer science;machine learning;pattern recognition;active contour model;mathematics;statistics;neuroimaging	Vision	43.89763909626997	-77.58985819283056	164959
8618aecb2c3437efc1331ef274ad3ecc3ef13dd8	automatic attribute threshold selection for blood vessel enhancement	image features;histograms;manuals;mathematical morphology;pattern clustering;image segmentation;automatic attribute threshold selection;rats;blood vessel enhancement;shape analysis;biomedical imaging;attribute filters;pattern clustering blood vessels feature extraction filtering theory image enhancement image segmentation medical image processing;blood vessel;data clustering;image enhancement;data clustering techniques;shape;clustering;feature extraction;medical image processing;automatic thresholding;connected filters;entropy;rats histograms entropy blood vessels biomedical imaging manuals shape;blood vessel filtering automatic attribute threshold selection blood vessel enhancement attribute filters feature extraction biomedical imaging image segmentation data clustering techniques;attribute filters connected filters mathematical morphology blood vessel enhancement clustering automatic thresholding;filtering theory;blood vessels;blood vessel filtering	Attribute filters allow enhancement and extraction of features without distorting their borders, and never introduce new image features. These are highly desirable properties in biomedical imaging, where accurate shape analysis is paramount. However, setting the attribute-threshold parameters has to date only been done manually. This paper explores simple, fast and automated methods of computing attribute threshold parameters based on image segmentation, thresholding and data clustering techniques. Though several techniques perform well on blood-vessel filtering, the choice of technique appears to depend on the imaging mode.	attribute grammar;cluster analysis;distortion;image segmentation;medical imaging;shape analysis (digital geometry);thresholding (image processing)	Fred N. Kiwanuka;Michael H. F. Wilkinson	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.566	medical imaging;computer vision;computer science;machine learning;pattern recognition;data mining;cluster analysis	Vision	41.696578613431676	-73.16002274262551	165238
f1d3c227df7a105c7a19aa7c0f6984c2c6cdabc0	infrared image segmentation using enhanced fuzzy c-means clustering for automatic detection systems	silicon;si efcm k means em gs;cluster algorithm;pattern clustering;fuzzy c mean;pattern clustering fuzzy set theory image segmentation infrared imaging object detection;image segmentation;infrared image segmentation;gs;k means;separation index;clustering algorithms clustering methods image segmentation silicon image color analysis boilers indexes;fuzzy set theory;global silhouette index;indexes;efcm;automatic detection;boilers;infrared imaging;image color analysis;clustering method;indexation;number of clusters;automatic detection system;clustering algorithms;enhanced fuzzy c means clustering;si;fuzzy c means clustering;clustering methods;separation index infrared image segmentation enhanced fuzzy c means clustering automatic detection system efcm global silhouette index;em;object detection	This paper proposes Enhanced Fuzzy C-means technique (EFCM) based infrared image segmentation and its broad application in Automatic detection systems. The EFCM based image segmentation is able to approximate the exact number of clusters present in the image. EFCM based segmentation is applied on various infrared images that can be used for automatic detection systems and compared with widely used clustering techniques such as K-means and EM. Clustering performance has been compared in terms of well-proven and widely accepted validation indices, Global Silhouette Index and Separation Index. The segments or clusters obtained from above mentioned clustering methods have been assessed visually. Automatic Detection Systems based on EFCM can help in reducing complexities present in conventional systems.	approximation algorithm;cluster analysis;computer cluster;em (typography);image segmentation;k-means clustering	Sitanshu Gupta;Asim Mukherjee	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007478	database index;computer vision;computer science;machine learning;pattern recognition;mathematics;em;fuzzy set;image segmentation;cluster analysis;silicon;k-means clustering	Robotics	42.280887870912274	-68.3751943119063	165404
6cbfbecb04429a5dc3305882313a4afc8632cb81	an improved 3d shape context registration method for non-rigid surface registration	smoothing method;geodesic distance;context model;3d shape context;small animal skeleton registration;non rigid registration;microcomputed tomography;smoothing;thin plate spline;radiometric corrections	3D shape context is a method to define matching points between similar shapes as a pre-processing step to non-rigid registration. The main limitation of the approach is point mismatching, which includes long geodesic distance mismatch and neighbors crossing mismatch. In this paper, we propose a topological structure verification method to correct the long geodesic distance mismatch and a correspondence field smoothing method to correct the neighbors crossing mismatch. A robust 3D shape context model is proposed and further combined with thin-plate spline model for non-rigid surface registration. The method was tested on phantoms and rat hind limb skeletons from micro CT images. The results from experiments on mouse hind limb skeletons indicate that the approach is robust.	ct scan;distance (graph theory);experiment;image registration;iterative closest point;preprocessor;robustness (computer science);shape context;smoothing;thin plate spline	Di Xiao;David Zahra;Pierrick Bourgeat;Paula Berghofer;Oscar Acosta;Catriona Wimberley;Marie Claude Grégoire;Olivier Salvado	2010		10.1117/12.844058	computer vision;geodesic;context model;thin plate spline;smoothing	Vision	42.25234525143049	-78.3358919326347	165446
f336dd24585331fc2e48e08e6aedc3eb11e5256a	penalized weighted least-squares image reconstruction for dual energy x-ray transmission tomography	metodo cuadrado menor;computerized axial tomography;poisson noise;tomodensitometria;metodo estadistico;methode moindre carre;radiodiagnostic;medical imagery;desviacion tipica;image reconstruction x ray imaging computed tomography positron emission tomography reconstruction algorithms least squares methods amorphous silicon detectors gaussian processes attenuation;penalized weighted least squares;least squares method;least mean squares methods;medical image processing image reconstruction computerised tomography least mean squares methods positron emission tomography densitometry;amorphous silicon;objet test;standard deviation;weighting;pet imaging;error sistematico;statistical method;attenuation correction;indexing terms;ponderacion;qualite image;positron emission tomography;si h penalized weighted least squares image reconstruction dual energy x ray transmission tomography nonnegativity constraints density domain amorphous silicon detectors nonpoisson noise gauss seidel algorithm objective function minimisation high noise cases low flux cases opaque regions quantitative computed tomography methods pet images bone mineral densitometry metal streak artifacts removal;objective function;reconstruction image;radiodiagnostico;tomodensitometrie;mode transmission;reconstruccion imagen;bias;double energie;methode statistique;quantitative computed tomography;image reconstruction;medical image processing;image quality;filtered back projection;algorithms artifacts computer simulation data interpretation statistical information storage and retrieval least squares analysis models biological models statistical numerical analysis computer assisted phantoms imaging radiation dosage radiographic image enhancement radiographic image interpretation computer assisted reproducibility of results sensitivity and specificity signal processing computer assisted tomography x ray computed;ecart type;computerised tomography;transmission mode;imagerie medicale;calidad imagen;imageneria medical	Presents a dual-energy (DE) transmission computed tomography (CT) reconstruction method. It is statistically motivated and features nonnegativity constraints in the density domain. A penalized weighted least squares (PWLS) objective function has been chosen to handle the non-Poisson noise added by amorphous silicon (aSi:H) detectors. A Gauss-Seidel algorithm has been used to minimize the objective function. The behavior of the method in terms of bias/standard deviation tradeoff has been compared to that of a DE method that is based on filtered back projection (FBP). The advantages of the DE PWLS method are largest for high noise and/or low flux cases. Qualitative results suggest this as well. Also, the reconstructed images of an object with opaque regions are presented. Possible applications of the method are: attenuation correction for positron emission tomography (PET) images, various quantitative computed tomography (QCT) methods such as bone mineral densitometry (BMD), and the removal of metal streak artifacts.	bone tissue;bone scintigraphy;ct scan;densitometry;detectors;diffusion weighted imaging;dual;dual-energy x-ray absorptiometry;flow-based programming;gauss;gauss–seidel method;iterative reconstruction;least squares;loss function;low back pain;morphologic artifacts;optimization problem;positron-emission tomography;positrons;quantitative computed tomography;silicon;standard deviation;tomography, x-ray;algorithm	Predrag Sukovic;Neal H. Clinthorne	2000	IEEE Transactions on Medical Imaging	10.1109/42.896783	iterative reconstruction;image quality;computer vision;radon transform;index term;correction for attenuation;shot noise;bias;weighting;mathematics;optics;nuclear medicine;densitometry;standard deviation;least squares;statistics;gauss–seidel method	Vision	53.475890664168844	-76.07272984260231	165468
ce32b09f545de8e0c400b4e62d1108380701e31b	image segmentation algorithms comparison	image segmentation image resolution;image segmentation;phantoms;supervised segmentation quality measure image segmentation algorithm comparison multiresolution segmentation algorithm free tool interimage commercial tool ecognition optical image landsat5 tm phantom image simulated image parameter configurations;segmenters comparison;indexes;remote sensing;interimage;position measurement;interimage segmenters comparison ecognition;ecognition;image segmentation spatial resolution phantoms indexes remote sensing position measurement;spatial resolution	This work aims to compare two segmentation tools that are based on the same multiresolution segmentation algorithm. The main significance of this investigation is to assess the feasibility of the use of a free tool (InterIMAGE) instead of a commercial one (eCognition) but still achieving equivalent results. Samples extracted from an optical image (LANDSAT5/TM) were used to fill the segments of a phantom image, creating 100 repetition of 3 simulated image. The obtained simulated images were then segmented using both tools with several parameter configurations and the results were evaluated through a supervised segmentation quality measure. Results obtained by eCognition are better in all tested cases.	algorithm;image segmentation;imaging phantom;memory segmentation;supervised learning	Mariane Souza Reis;Maria Antonia Falcão de Oliveira;Thales Sehn Korting;Eliana Pantaleao;Sidnei J. S. Sant'Anna;Luciano Vieira Dutra;Dengsheng Lu	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7326787	image texture;database index;computer vision;image resolution;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation;remote sensing	Robotics	39.582090509922246	-76.95720278903974	165535
a2067fbd103bc0e6435c0519461aef99209572f1	a new interactive algorithm for image segmentation	image segmentation;会议论文	In this paper, a new interactive algorithm for image segmentation is proposed. First, threshold segmentation is applied to the original image, and the corresponding foreground image and background image are obtained. In the foreground image, some manually selected contour pixel points of the pattern to be segmented will become the initial seed points. The set composed of seed points is defined as set E. Then, the distances between each pixel point in foreground image and each seed point in set E are computed. If the minimum distance is less than the threshold, the pixel point with minimum distance is labeled as seed point, and added to the set E. Until all the pixel points in the foreground image have been labeled, the seed points in the final set E compose the segmentation image. At last, the effectiveness of the proposed algorithm is proved by a simulation. © (2015) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	algorithm;image segmentation	Haiying Zhao;Changping Sun;Hong Chen	2015		10.1117/12.2229465	image texture;computer vision;feature detection;range segmentation;binary image;morphological gradient;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;random walker algorithm;scale-space segmentation;computer graphics (images)	Vision	45.215760575455725	-67.61422862899539	165586
4684ea19db559f699e4bc2116a090d355e3c88bb	curious snakes: a minimum latency solution to the cluttered background problem in active contours	cluttered background problem;change detection;homogeneous photometric characteristics;bayesian classification;delay active contours statistics detection algorithms photometry bayesian methods robustness solid modeling layout object detection;active contour;image segmentation;minimum latency solution;edge detection;bayes methods;robust statistics;computational geometry;proceedings;active contours;computational modeling;statistical analysis;automatic detection;minimum latency set point change detection;photometry;image color analysis;statistical analysis bayes methods computational geometry edge detection object detection photometry;geometric modeling;robustness;optimization;post print;geometric model;bayesian classification terms;region based active contour detection algorithm;object detection;curious snakes;data models;object detection curious snakes minimum latency solution cluttered background problem active contours region based active contour detection algorithm homogeneous photometric characteristics bayesian classification terms robust statistics minimum latency set point change detection geometric modeling	We present a region-based active contour detection algorithm for objects that exhibit relatively homogeneous photometric characteristics (e.g. smooth color or gray levels), embedded in complex background clutter. Current methods either frame this problem in Bayesian classification terms, where precious modeling resources are expended representing the complex background away from decision boundaries, or use heuristics to limit the search to local regions around the object of interest. We propose an adaptive lookout region, whose size depends on the statistics of the data, that are estimated along with the boundary during the detection process. The result is a “curious snake” that explores the outside of the decision boundary only locally to the extent necessary to achieve a good tradeoff between missed detections and narrowest “lookout” region, drawing inspiration from the literature of minimum-latency set-point change detection and robust statistics. This development makes fully automatic detection in complex backgrounds a realistic possibility for active contours, allowing us to exploit their powerful geometric modeling capabilities compared with other approaches used for segmentation of cluttered scenes. To this end, we introduce an automatic initialization method tailored to our model that overcomes one of the primary obstacles in using active contours for fully automatic object detection.	active contour model;algorithm;clutter;decision boundary;embedded system;geometric modeling;grayscale;heuristic (computer science);naive bayes classifier;object detection;sensor	Ganesh Sundaramoorthi;Stefano Soatto;Anthony J. Yezzi	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540020	computer vision;computational geometry;computer science;geometric modeling;machine learning;pattern recognition;statistics	Vision	47.266699178131034	-68.24466418524912	165638
d918290986cd9a1b17b632ae581e0ad370cfba0f	automatic quantification of ct images for traumatic brain injury	computed tomography image segmentation blood magnetic resonance imaging brain injuries pathology;multi template1 traumatic brain injury ct registration segmentation classification prognosis;dichotomized favorable outcome automatic ct image quantification acute computed tomography images traumatic brain injury tbi permanent disability multitemplate approach image registration spatial normalization tissue segmentation;medical image processing biological tissues brain computerised tomography image registration image segmentation injuries	Traumatic brain injury (TBI) is a major health problem and the most common cause of permanent disability in people under the age of 40 years. In this paper, we present a fully automatic framework for the analysis of acute computed tomography (CT) images in TBI. Different pathologies common in TBI are quantified and all the information is combined for clinical outcome prediction in individual patients. We propose a multi-template approach for the registration of CT data, which improves the robustness and accuracy of spatial normalization. This is especially important for noisy CT data and TBI images with large areas of pathology. The tissue segmentation methods we use have been optimized to deal with these challenges. The methods we describe have been evaluated on acute CTs from 104 TBI patients. We demonstrate on this dataset that the prediction of dichotomized favorable or unfavorable outcome can be made with an accuracy of 79%.	ct scan;quantifier (logic);tomography	Juha Koikkalainen;Jyrki Lötjönen;Christian Ledig;Daniel Rueckert;Olli Tenovuo;David K. Menon	2014	2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2014.6867825	radiology;pathology;medical physics	Vision	40.3199925114668	-80.0175013989539	165663
0d9667402e28e475f61f45f329208d0886415c39	clinical validation and performance evaluation of enhancement methods acquired from interventional c-arm x-ray	databases;evaluation performance;performance evaluation;evaluacion prestacion;methode;anisotropic diffusion;blood vessel;angiography;visualization;image quality;arm;bras;brazo;c;metodo;diffusion;method;digital subtraction angiography;blood vessels;x rays	Digital Subtraction Angiography (DSA) is a well-established powerful modality for the visualization of stenosis and blood vessels in general. This paper presents two novel approaches which address image quality. In the first approach we combine anisotropic diffusion with nonlinear normalization. The second approach consists of an introduction of a regularization strategy followed by a classification procedure to improve the enhancement. The performances of two strategies are evaluated based on a database of 73 subjects using SNR, CNR and Tenengrad’s metric. Compared with conventional DSA, Eigen’s diffusion embedded nonlinear enhancement strategies can improve image quality 95.25% in terms of SNR. The regularization embedded linear enhancement strategy can also improve SNR 51.46% compared with conventional DSA. Similar results are obtained by CNR and Tenengrad’s metric measurements. Our system runs on a PC-based workstation using C++ in Windows environment.	anisotropic diffusion;c++;eigen (c++ library);embedded system;image quality;microsoft windows;modality (human–computer interaction);nonlinear system;performance evaluation;personal computer;signal-to-noise ratio;workstation	Liyang Wei;Dinesh Kant Kumar;Animesh Khemka;Ram Turlapati;Jasjit S. Suri	2008		10.1117/12.770734	image quality;computer vision;method;visualization;diffusion;arm architecture;anisotropic diffusion	Visualization	47.32164818299839	-78.76980877833817	165721
02583d880e6b53aa1ac08892109ea4f333cb5e21	comparison of fuzzy connectedness and graph cut segmentation algorithms	image foresting transform;image segmentation;graph cut;algorithms	The goal of this paper is a theoretical and experimental comparison of two popular image segmentation algorithms: fuzzy connectedness (FC) and graph cut (GC). On the theoretical side, our emphasis will be on describing a common framework in which both of these methods can be expressed. We will give a full analysis of the framework and describe precisely a place which each of the two methods occupies in it. Within the same framework, other region based segmentation methods, like watershed, can also be expressed. We will also discuss in detail the relationship between FC segmentations obtained via image forest transform (IFT) algorithms, as opposed to FC segmentations obtained by other standard versions of FC algorithms. We also present an experimental comparison of the performance of FC and GC algorithms. This concentrates on comparing the actual (as opposed to provable worst scenario) algorithms’ running time, as well as influence of the choice of the seeds on the output.	algorithm;cut (graph theory);graph cuts in computer vision;image segmentation;provable security;time complexity;transcutaneous electrical nerve stimulation;watershed (image processing)	Krzysztof Ciesielski;Jayaram K. Udupa;Alexandre X. Falcão;Paulo André Vechiatto Miranda	2011		10.1117/12.872522	computer vision;cut;machine learning;image segmentation	Vision	46.77736094427137	-72.77289697901244	165740
cb254cc4ecbdb275b5945d2bcb0297efb307b6c6	impact of the number of atlases in a level set formulation of multi-atlas segmentation		In this paper, we present a multi-atlas segmentation method based on the level set formulation for performing label fusion that takes into account the image information and regularity of the region of interest (ROI). In the presented method, multiple atlases are first registered to a target image by deformable registration via attribute matching and mutual saliency weighting (DRAMMS) and advanced neuroimaging tools (ANTs) to get the warped labels. Then, an optimal labeling is sought by label fusion for segmentation of target image. Label fusion is achieved by seeking an optimal level set function which minimizes an energy functional in regards to three terms: label fusion term, image based term, and regularization term. In this work, we discussed the impact of subset on the accuracy of segment results. Results show that segmentation results will be much more accurate if an appropriate subset of atlases are selected for each target image than those given by non-selective combination of random atlas subsets.		Yihua Song;Zhaoxuan Gong;Dazhe Zhao;Chaolu Feng;Chunming Li	2015		10.1007/978-3-319-27857-5_48	computer vision;bioinformatics;pattern recognition;scale-space segmentation	Vision	42.813729627714814	-76.1857042745602	166212
c0f57a2c5ffc368ffc58d32dcf6f517745879f43	unmatched projector/backprojector pairs in an iterative reconstruction algorithm	iterative method;image tridimensionnelle;radiodiagnostic;algorithms humans image processing computer assisted likelihood functions models theoretical research scattering radiation;algorithm performance;reconstruction algorithms iterative algorithms image reconstruction attenuation detectors scattering solid modeling geometry physics eigenvalues and eigenfunctions;maximum likelihood;objet test;medical diagnostic imaging unmatched projector backprojector pairs iterative reconstruction algorithm computational burden attenuation detector response scatter correction imaging geometry negative eigenvalues;maximum vraisemblance;projection method;pairing;radiografia;indexing terms;satisfiability;eigenvalues;iterative algorithm;three dimensional;scatter correction;radiography;metodo iterativo;iterative reconstruction;iterative methods;reconstruction image;radiodiagnostico;methode projection;reconstruccion imagen;resultado algoritmo;methode iterative;image reconstruction;medical image processing;metodo proyeccion;performance algorithme;tridimensional image;emparejamiento;radiodiagnosis;appariement;objeto prueba;test object;maxima verosimilitud;radiographie;imagen tridimensional;medical image processing iterative methods image reconstruction	Computational burden is a major concern when an iterative algorithm is used to reconstruct a three-dimensional (3-D) image with attenuation, detector response, and scatter corrections. Most of the computation time is spent executing the projector and backprojector of an iterative algorithm. Usually, the projector and the backprojector are transposed operators of each other. The projector should model the imaging geometry and physics as accurately as possible. Some researchers have used backprojectors that are computationally less expensive than the projectors to reduce computation time. This paper points out that valid backprojectors should satisfy a condition that the projector/backprojector matrix must not contain negative eigenvalues. This paper also investigates the effects when unmatched projector/backprojector pairs are used.	algorithm;computation (action);detectors;iterative method;iterative reconstruction;movie projector;time complexity;video projector;executing - querystatuscode	Gengsheng Lawrence Zeng;Grant T. Gullberg	2000	IEEE Transactions on Medical Imaging	10.1109/42.870265	computer vision;mathematical optimization;radiology;mathematics;geometry;iterative method	Visualization	52.76017762349147	-79.14910749410627	166601
9678b197f37da9b21c685738bc2698e337b3426a	automatic extraction of brain from cerebral mr image based on improved bet method	brain contour brain automatic extraction cerebral mr image improved bet method smoothing force expansionary force;brain;pet imaging;data mining;force;mr imaging;smoothing methods;brain contour;medical image processing biomedical mri brain feature extraction;cerebral mr image;image edge detection;brain smoothing methods data mining robustness surface morphology automation magnetic resonance positron emission tomography biomedical engineering biomedical imaging;feature extraction;medical image processing;expansionary force;mathematical model;improved bet method;brain automatic extraction;smoothing force;extraction method;biomedical mri	BET is a well known brain extraction method from cerebral MR, FMR and PET images, but there are cases that it can not get ideal result whatever you set the parameters in BET method when processing real cerebral MR images. To overcome this problem, this paper modifies the definitions of smoothing force and expansionary force used in the BET algorithm to evolve the contour of brain according to the intensity distribution of images and adds a new path to search the local maximum and minimum intensity of image. Experiments show that the improved method is more robust than BET method when processing real MR images. KeywordsBET (brain extraction tool), cerebral MR image	algorithm;experiment;maxima and minima;smoothing	Shaofeng Jiang;Suhua Yang;Zhen Chen;Wufan Chen	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5304856	computer vision;feature extraction;computer science;artificial intelligence;machine learning;mathematical model;force;statistics	Robotics	41.259129306671596	-75.92081441526186	166627
6e8a138c021b2bbe1c953fcb72b9dc821d7b9b0c	segmentation of cells from spinning disk confocal images using a multi-stage approach		Live cell imaging in 3D platforms is a highly informative approach to visualize cell function and it is becoming more commonly used for understanding cell behavior. Since these experiments typically generate large data sets their analysis manually would be very laborious and error prone. This has led to the necessity of automatic image analysis tools. Cell segmentation is an essential initial step for any detailed automatic quantitative analysis. When the images are captured from the 3D culture containing proliferating and moving cells, cell-cell interactions and collisions cannot be avoided. In these conditions the segmentation of individual cells becomes very challenging. Here we present a method which utilizes the edge probability map and graph cuts to detect and segment individual cells from cell clusters. The main advantage of our method is that it is capable of handling complex cell shapes because it does not make any assumptions about the cell shape.	cell signaling;cognitive dimensions of notations;cut (graph theory);experiment;image analysis;information;interaction	Saad Ullah Akram;Juho Kannala;Mika Kaakinen;Lauri Eklund;Janne Heikkilä	2014		10.1007/978-3-319-16811-1_20	artificial intelligence;live cell imaging;pattern recognition;computer vision;confocal;computer science;cut;segmentation	Visualization	40.07117167985149	-73.78038912038771	166884
023da40dae52e1f417ca776c8f71f33bae46f0d9	fast algorithm for neural network reconstruction	standards;machine learning connectivity analysis nerves;nerves;calcium;neurons biological neural networks calcium entropy imaging standards;machine learning;imaging;connectivity analysis;time series biomedical optical imaging brain fluorescence image reconstruction matrix algebra medical image processing neural nets neurophysiology;entropy;neurons;simulated calcium fluorescence dataset fast algorithm neural network reconstruction brain connectivity matrix time series signals;biological neural networks	We propose an efficient and accurate way of predicting the connectivity of neural networks in the brain represented by simulated calcium fluorescence data. Classical methods to neural network reconstruction compute a connectivity matrix whose entries are pairwise likelihoods of directed excitatory connections based on time-series signals of each pair of neurons. Our method uses only a fraction of this computation to achieve equal or better performance. The proposed method is based on matrix completion and a local thresholding technique. By computing a subset of the total entries in the connectivity matrix, we use matrix completion to determine the rest of the connection likelihoods, and apply a local threshold to identify which directed connections exist in the underlying network. We validate the proposed method on a simulated calcium fluorescence dataset. The proposed method outperforms the classical one with 20% of the computation.	adjacency matrix;algorithm;artificial neural network;computation;thresholding (image processing);time complexity;time series	Sean R Bittner;Siheng Chen;Jelena Kovacevic	2015	2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2015.7164008	medical imaging;entropy;radiology;medicine;calcium;computer science;artificial intelligence;theoretical computer science;machine learning	Vision	47.319042316629556	-79.85838782739322	166961
e24bb29de2af129420ab7892989ecdb3d92c6993	a marker-controlled watershed segmentation: edge, mark and fill	geophysical image processing;image segmentation;local accuracy marker controlled watershed segmentation vhr image segmentation very high resolution images complex urban scenarios fine man made details edge based segmentation methods region based methods global accuracy;image edge detection image segmentation skeleton merging transforms image resolution accuracy;geophysical techniques;image segmentation geophysical image processing geophysical techniques	The segmentation of very high resolution (VHR) images portraying complex urban scenarios is a rather challenging problem. In particular, great attention must be devoted to preserve fine man-made details, of major interest for most user applications. For this reason, edge-based segmentation methods are likely preferable to region-based methods. The latter, in fact, e.g. [1], [2], succeed in taking into account long range interactions and hence perform typically well in terms of “global” accuracy, but exhibit a lower “local” accuracy with respect to former, [3].	image resolution;interaction;watershed (image processing)	Raffaele Gaetano;Giuseppe Masi;Giuseppe Scarpa;Giovanni Poggi	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6351713	image texture;computer vision;computer science;morphological gradient;segmentation-based object categorization;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;remote sensing	Vision	47.29101760363498	-67.57140945210196	167195
9cb49e8327892b8f5b43f842ea8473154d3430d0	a novel, fast, and complete 3d segmentation of vertebral bones	method validation;3d segmentation;spine;image segmentation;computed tomography;level set;trabecular bone;bones image segmentation computed tomography three dimensional displays accuracy level set spine;graph cuts segmentation spine bone vertebral body vb trabecular bone;accuracy;spine bone;bones;graph cuts segmentation;three dimensional displays;bone mineral density;graph cut;ground truth;vertebral body vb;matched filter;random field	Bone mineral density (BMD) measurements and fracture analysis of the spine bones are restricted to the Vertebral bodies (VBs), especially the trabecular bones (TBs). In this paper, we propose a novel, fast, and robust 3D framework to segment VBs and trabecular bones in clinical computed tomography (CT) images without any user intervention. The Matched filter is employed to detect the VB region automatically. To segment the whole VB, the graph cuts method which integrates a linear combination of Gaussians (LCG) and Markov Gibbs Random Field (MGRF) is used. Then, the cortical and trabecular bones are segmented using local volume growing methods. Validity was analyzed using ground truths of data sets (expert segmentation) and the European Spine Phantom (ESP) as a known reference. Experiments on the data sets show that the proposed segmentation approach is more accurate than other known alternatives.	ct scan;cut (graph theory);experiment;linear congruential generator;markov chain;markov random field;matched filter;napier's bones;phantom reference;tomography	Melih S. Aslan;Asem M. Ali;Ham M. Rara;Ben Arnold;Rachid Fahmi;Aly A. Farag;Ping Xiang	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495133	random field;cut;spine;ground truth;computer science;level set;mathematics;accuracy and precision;image segmentation;computed tomography;matched filter;statistics	Vision	42.54141610846641	-79.46046268006153	167290
c31cd22842786b1291bf0e9d204865121fd1e4bc	an iterative framework for registration with reconstruction	iterative closest point	The core of most registration algorithms aligns scan data by pairs, minimizing their relative distance. This local optimization must generally pass through a validation procedure to ensure the global coherence of the resulting alignments. This work introduces an iterative framework to guarantee the global coherence of the registration process. The iteration alternates registration and reconstruction steps, including alignments with the proper reconstructed surface, until the alignment of all the scans converges. The framework adapts to different contexts by choosing which scans are aligned and which are used for the reconstruction. This choice is based on the alignment and reconstruction errors. Derivations of this framework are presented with a rough automatic registration, increasing its robustness.	algorithm;iteration;mathematical optimization;reconstruction filter;scheduling (computing)	Thales Vieira;Adelailson Peixoto;Luiz Velho;Thomas Lewiner	2007			computer vision;mathematical optimization;computer science;data mining;mathematics;iterative closest point	Vision	47.82036254914325	-79.02913248198348	167395
024953f47b3d2d5dee138ccac69232b445402c95	dynamic image reconstruction using temporally adaptive regularization for emission tomography	phantoms;cardiology;temporal basis functions;smoothing methods;adaptive signal processing;image reconstruction;medical image processing;emission tomography;image reconstruction smoothing methods spline distribution functions statistics data acquisition reconstruction algorithms positron emission tomography imaging phantoms myocardium;single photon emission computed tomography;dynamic reconstruction;statistics;time varying data;adaptive regularization;myocardium dynamic image reconstruction temporally adaptive regularization dynamic emission tomography temporal basis functions time varying data statistics tracer distribution function spatial smoothing tc99m teboroxime spect imaging gated mathematical cardiac torso phantom perfusion defect;statistics adaptive signal processing cardiology image reconstruction medical image processing phantoms single photon emission computed tomography smoothing methods;adaptive regularization dynamic reconstruction emission tomography temporal basis functions	Temporal basis functions have been found to be effective for regularizing the time-varying image activities in dynamic emission tomography. By modelling the tracer distribution function at individual pixels as a linear combination of a set of basis functions, the reconstruction problem becomes that of estimating the weights of the basis functions. In this work, we explore the use of temporally adaptive regularization in the basis function domain, where spatial smoothing is enforced in an adaptive fashion according to the time-varying data statistics. In our experiments the proposed method was demonstrated using simulated Tc99m-Teboroxime SPECT imaging with the gated mathematical cardiac-torso (gMCAT) phantom. Our results show that the proposed approach can lead to more accurate reconstruction of the time activities, which is important for differentiation between a perfusion defect and the normal myocardium.	adaptive filter;basis function;experiment;iterative reconstruction;matrix regularization;phantom reference;pixel;reconstruction conjecture;smoothing;software bug;tomography	Mingwu Jin;Yongyi Yang;Miles N. Wernick	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379974	iterative reconstruction;adaptive filter;computer vision;computer science;mathematics;statistics	Vision	47.87463530017552	-79.99607899412025	167406
eed6220240c9d8cad3cb2404b7fee5e3f07858d2	observation of dynamics inside an unlabeled live cell using bright-field photon microscopy: evaluation of organelles' trajectories		This article presents an algorithm for the evaluation of organelles’ movements inside of an unmodified live cell. We used a timelapse image series obtained using wide-field bright-field photon transmission microscopy as an algorithm input. The benefit of the algorithm is the application of the Rényi information entropy, namely a variable called a point information gain, which enables to highlight the borders of the intracellular organelles and to localize the organelles’ centers of mass with the precision of one pixel.	algorithm;entropy (information theory);information gain in decision trees;kullback–leibler divergence;pixel;rényi entropy	Renata Rychtáriková;Dalibor Stys	2017		10.1007/978-3-319-56154-7_62	nanotechnology;mathematics	Robotics	40.729074334215234	-73.75366363306094	167651
69312e5cee92942ecdb22555709c7d2cc7405918	two-step evidential fusion approach for accurate breast region segmentation in mammograms		In mammograms, the breast skin line often appears ambiguous and poorly defined. This is mainly due to the breast organ compression during the image acquisition process along with the inherent low density of the tissue in that area. The accurate delimitation of the breast region becomes a challenging task to conventional segmentation techniques. In this study, the authors propose a new segmentation approach allowing to overcome this challenge. This approach is based on the application of two complementary segmentation techniques exploring each, respectively, the grey-scale intensities and the local-homogeneity domains. The knowledge resulting from each segmentation technique is considered as a knowledge source and is modelled using the belief functions formalism. The two considered knowledge sources are then fused using an iterative process. The obtained results show the efficiency of the proposed evidential approach especially in terms of ambiguity removal and decision quality improvement for accurate breast border delimitation (which is often under-segmented and assimilated to the background by most of the existing segmentation techniques).		Maria A Larrazabal;Karim Kalti;Asma Touil;Bassel Solaiman;Najoua Essoukri Ben Amara	2018	IET Image Processing	10.1049/iet-ipr.2018.5325	computer vision;artificial intelligence;pattern recognition;iterative and incremental development;formalism (philosophy);mathematics;ambiguity;fusion;decision quality;segmentation	Robotics	41.54545213209588	-72.42032674726552	167768
2db97371159d22e463d2f752bb4168897cf953a5	image segmentation by em-based adaptive pulse coupled neural networks in brain magnetic resonance imaging	fuzzy c mean;white matter;image segmentation;clinical application;magnetic resonance image;expectation maximization;bias correction;magnetic resonance imaging;gray matter;brain imaging;pulse coupled neural network;cerebrospinal fluid	We propose an automatic hybrid image segmentation model that integrates the statistical expectation maximization (EM) model and the spatial pulse coupled neural network (PCNN) for brain magnetic resonance imaging (MRI) segmentation. In addition, an adaptive mechanism is developed to fine tune the PCNN parameters. The EM model serves two functions: evaluation of the PCNN image segmentation and adaptive adjustment of the PCNN parameters for optimal segmentation. To evaluate the performance of the adaptive EM-PCNN, we use it to segment MR brain image into gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF). The performance of the adaptive EM-PCNN is compared with that of the non-adaptive EM-PCNN, EM, and Bias Corrected Fuzzy C-Means (BCFCM) algorithms. The result is four sets of boundaries for the GM and the brain parenchyma (GM+WM), the two regions of most interest in medical research and clinical applications. Each set of boundaries is compared with the golden standard to evaluate the segmentation performance. The adaptive EM-PCNN significantly outperforms the non-adaptive EM-PCNN, EM, and BCFCM algorithms in gray mater segmentation. In brain parenchyma segmentation, the adaptive EM-PCNN significantly outperforms the BCFCM only. However, the adaptive EM-PCNN is better than the non-adaptive EM-PCNN and EM on average. We conclude that of the three approaches, the adaptive EM-PCNN yields the best results for gray matter and brain parenchyma segmentation.		J. C. Fu;C. C. Chen;J. W. Chai;Stephen T. C. Wong;I. C. Li	2010	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1016/j.compmedimag.2009.12.002	computer vision;radiology;medicine;expectation–maximization algorithm;computer science;artificial intelligence;magnetic resonance imaging;image segmentation;nuclear medicine;scale-space segmentation	Vision	41.484984943483916	-78.19390966979036	167819
dc0dae0e93538b722271feb3a70b2f78cdffdaea	higher order energies for image segmentation		A novel energy minimization method for general higher order binary energy functions is proposed in this paper. We first relax a discrete higher order function to a continuous one, and use the Taylor expansion to obtain an approximate lower order function, which is optimized by the quadratic pseudo-Boolean optimization or other discrete optimizers. The minimum solution of this lower order function is then used as a new local point, where we expand the original higher order energy function again. Our algorithm does not restrict to any specific form of the higher order binary function or bring in extra auxiliary variables. For concreteness, we show an application of segmentation with the appearance entropy, which is efficiently solved by our method. Experimental results demonstrate that our method outperforms the state-of-the-art methods.	approximation algorithm;boolean;energy minimization;energy, physics;entropy (information theory);higher-order function;image segmentation;mathematical optimization;pseudo brand of pseudoephedrine;biologic segmentation	Jianbing Shen;Jianteng Peng;Xingping Dong;Ling Shao;Fatih Murat Porikli	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2722691	iterative method;mathematics;approximation algorithm;taylor series;energy minimization;mathematical optimization;image segmentation;restrict;upper and lower bounds;binary function	Vision	52.90126723143587	-71.41418314943718	167820
0c400bb4d76027350acdf523f91755846fe004f6	functional principal component model for high-dimensional brain imaging	models neurological;high resolution;high dimensionality;brain imaging data;bepress selected works;singular value decomposition;functional principal components;brain mapping;image interpretation computer assisted;voxel based morphometry;svd;singular vector;principal component analysis;magnetic resonance imaging;mri;brain imaging;humans;fpca;voxel based morphometry vbm;principal component	We explore a connection between the singular value decomposition (SVD) and functional principal component analysis (FPCA) models in high-dimensional brain imaging applications. We formally link right singular vectors to principal scores of FPCA. This, combined with the fact that left singular vectors estimate principal components, allows us to deploy the numerical efficiency of SVD to fully estimate the components of FPCA, even for extremely high-dimensional functional objects, such as brain images. As an example, a FPCA model is fit to high-resolution morphometric (RAVENS) images. The main directions of morphometric variation in brain volumes are identified and discussed.	brain implant;component-based software engineering;functional principal component analysis;image resolution;international conference on functional programming;morphometrics;numerical analysis;physical object;singular value decomposition	Vadim Zipunnikov;Brian Caffo;David M. Yousem;Christos Davatzikos;Brian S. Schwartz;Ciprian M. Crainiceanu	2011	NeuroImage	10.1016/j.neuroimage.2011.05.085	psychology;functional principal component analysis;computer vision;radiology;medicine;artificial intelligence;magnetic resonance imaging;machine learning;mathematics;singular value decomposition;principal component analysis	ML	48.76761277847601	-78.03648754363182	168161
c0e48f5aefcc8d7d0d0f70e7e9ac0f447a3aa170	automatic construction of parts+geometry models for initializing groupwise registration	detectors;optimisation;correspondences;optimisation image registration medical image processing;computer model;complex structure;geometry;parts geometry model automatic construction groupwise nonrigid image registration statistical models optimization problem affine transformation sparse matches;joints;statistical model;optimization problem;computational modeling;shape computational modeling joints detectors optimization geometry humans;shape;groupwise nonrigid registration;medical image processing;affine transformation;image registration;nonrigid image registration;optimization;humans;nonrigid registration;parts geometry models;algorithms humans imaging three dimensional pattern recognition automated radiographic image enhancement radiographic image interpretation computer assisted reproducibility of results sensitivity and specificity subtraction technique tomography x ray computed;parts geometry models correspondences groupwise nonrigid registration initialization;initialization;quantitative evaluation	Groupwise nonrigid image registration is a powerful tool to automatically establish correspondences across sets of images. Such correspondences are widely used for constructing statistical models of shape and appearance. As existing techniques usually treat registration as an optimization problem, a good initialization is required. Although the standard initialization-affine transformation-generally works well, it is often inadequate when registering images of complex structures. In this paper we present a more sophisticated method that uses the sparse matches of a parts+geometry model as the initialization. We show that both the model and its matches can be automatically obtained, and that the matches are able to effectively initialize a groupwise nonrigid registration algorithm, leading to accurate dense correspondences. We also show that the dense mesh models constructed during the groupwise registration process can be used to accurately annotate new images. We demonstrate the efficacy of the approach on three datasets of increasing difficulty, and report on a detailed quantitative evaluation of its performance.	active appearance model;algorithm;algorithmic efficiency;anatomy, regional;detectors;experiment;failure cause;global optimization;graph - visual representation;graphical model;image registration;loose coupling;mathematical optimization;optimization problem;scale-invariant feature transform;sparse matrix;statistical model;test set;registration - actclass	Pei Zhang;Timothy F. Cootes	2012	IEEE Transactions on Medical Imaging	10.1109/TMI.2011.2169077	computer simulation;optimization problem;statistical model;computer vision;initialization;detector;shape;computer science;image registration;pattern recognition;affine transformation;generalized complex structure;mathematics;geometry;computational model	Vision	42.76934222585685	-76.91783145200927	168252
850ff024f977a43af6fc6ed40488bdfdbd04d0e9	a fast scheme for multilevel thresholding based on a modified bees algorithm	levy flight;maximum entropy thresholding;multilevel thresholding;otsu thresholding;bees algorithm;patch environment	Image segmentation is one of the most important tasks in image processing and pattern recognition. One of the most efficient and popular techniques for image segmentation is image thresholding. Among several thresholding methods, Kapur’s (maximum entropy (ME)) and Otsu’s methods have been widely adopted for their simplicity and effectiveness. Although efficient in the case of bi-level thresholding, they are very computationally expensive when extended to multilevel thresholding because they employ an exhaustive search for the optimal thresholds. In this paper, a fast scheme based on a modified Bees Algorithm (BA) called the Patch-Levy-based Bees Algorithm (PLBA) is adopted to render Kapur’s (ME) and Otsu’s methods more practical; this is achieved by accelerating the search for the optimal thresholds in multilevel thresholding. The experimental results demonstrate that the proposed PLBA-based thresholding algorithms are able to converge to the optimal multiple thresholds much faster than their corresponding methods based on Basic BA. The experiments also show that the thresholding algorithms based on BA algorithms outperform corresponding state-of-the-art metaheuristic-based methods that employ Bacterial Foraging Optimization (BFO) and quantum mechanism (quantum-inspired algorithms) and perform better than the nonmetaheuristic-based Two-Stage Multi-threshold Otsu method (TSMO) in terms of the segmented image quality. In addition, the results show the high degree of stability of the proposed PLBAbased algorithms. © 2016 Elsevier B.V. All rights reserved.	analysis of algorithms;basic formal ontology;bees algorithm;black and burst;brute-force search;business architecture;converge;experiment;image processing;image quality;image segmentation;metaheuristic;otsu's method;pattern recognition;thresholding (image processing)	Wasim Abdulqawi Hussein;Shahnorbanun Sahran;Siti Norul Huda Sheikh Abdullah	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.03.010	mathematical optimization;lévy flight;computer science;artificial intelligence;machine learning;otsu's method;bees algorithm;pattern recognition;balanced histogram thresholding;thresholding	AI	42.82871785893747	-68.7643836475067	168410
acdba6bd93cbfff78c5cb03cff85b70390f77f85	shape-based normalized cuts using spectral relaxation for biomedical segmentation	eigenvalues and eigenfunctions;image segmentation;medical segmentation image segmentation normalized cuts normalized cuts with shape prior shape model spectral relaxation;medical image processing eigenvalues and eigenfunctions image segmentation;eigenvalue problem shape based normalized cuts spectral relaxation biomedical segmentation cost function dissimilarity shape model optimization;medical image processing;image segmentation shape biomedical imaging myocardium principal component analysis vectors eigenvalues and eigenfunctions	We present a novel method to incorporate prior knowledge into normalized cuts. The prior is incorporated into the cost function by maximizing the similarity of the prior to one partition and the dissimilarity to the other. This simple formulation can also be extended to multiple priors to allow the modeling of the shape variations. A shape model obtained by PCA on a training set can be easily integrated into the new framework. This is in contrast to other methods that usually incorporate prior knowledge by hard constraints during optimization. The eigenvalue problem inferred by spectral relaxation is not sparse, but can still be solved efficiently. We apply this method to biomedical data sets as well as natural images of people from a public database and compare it with other normalized cut based segmentation algorithms. We demonstrate that our method gives promising results and can still give a good segmentation even when the prior is not accurate.	3d computer graphics;algorithm;constraint (mathematics);database;eigenvalue;experiment;incised wound;inference;linear programming relaxation;loss function;mathematical optimization;medical imaging;relaxation (approximation);segmentation action;segmentation-based object categorization;shape context;sparse matrix;test set	Esmeralda Ruiz Pujadas;Marco Reisert	2014	IEEE Transactions on Image Processing	10.1109/TIP.2013.2287604	computer vision;mathematical optimization;computer science;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	49.88776088037063	-71.99808094400343	168411
5774c8fdb120ae9a6916a1c31f6e365a103af6fe	geometric features based framework for colonic polyp detection using a new color coding scheme	topology;polyp visualization colonic polyp detection color coding scheme curvature based geometric features polygonal dataset topology property geometry property triangulated mesh surface colon dataset computer simulation;geometry;indexing terms;geometric feature;data visualisation;automatic detection;virtual colonoscopy;colorectal cancer;colonic polyps colon shape cancer virtual colonoscopy surface fitting computational modeling computer simulation computational complexity image coding;color coding;topology data visualisation digital simulation geometry medical diagnostic computing mesh generation;false positive;mesh generation;medical diagnostic computing;color coding colorectal cancer curvature based geometric features virtual colonoscopy;curvature based geometric features;digital simulation	Curvature-based geometric features have been proven to be important for colonic polyp detection. In this paper, we present an automatic detection framework and color coding scheme to highlight the detected polyps. The key idea is to place the detected polyps at the same locations in a newly created polygonal dataset with the same topology and geometry properties as the triangulated mesh surface of real colon dataset, and assign different colors to the two separated datasets to highlight the polyps. Finally, we validate the proposed framework by computer simulated and real colon datasets. For fifteen synthetic polyps with different shapes and different sizes, the sensitivity is 100%, and false positive is 0. For four real colon datasets, the proposed algorithm has achieved the sensitivity of 75%.	algorithm;colon classification;color;computer simulation;synthetic data	Dongqing Chen;Aly A. Farag;M. Sabry Hassouna;Robert Falk;Gerald W. Dryden	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379753	mesh generation;computer vision;index term;type i and type ii errors;colorectal cancer;theoretical computer science;color-coding;mathematics;data visualization;statistics	Vision	42.00934491205128	-74.43835506462769	168660
8ab8aff7bf3d1227e0ca0581906e81169ae63647	efficient procedure and methods to determine critical electroporation parameters	image processing;normal dermal human fibroblasts cell culture electroporation control high throughput signal processing method microscopic images electric field strength nonexpert visualization platform;electric fields fluorescence cathodes microscopy biomembranes estimation;biological system modeling;light microscopy;biological system modeling electroporation clustering image processing light microscopy;clustering;electroporation;optical microscopy bioelectric phenomena biological techniques biomembrane transport image processing	Controlling electroporation and determination of its design parameters govern the performance in many electroporation applications in the fields of medicine, biology, and food industry. This work provides an enhanced high throughput signal processing method to enable non-expert, objective, automatic and fast analysis of large datasets of microscopic images of high throughput electroporation experiments. This can enable optimal design of vital electroporation parameters including the optimal thresholds for the electric field strength, and the optimal number of pulses. In this work the non-expert visualization platform and the methods are described, and implemented on normal dermal human fibroblasts cell culture.	apriori algorithm;computational complexity theory;experiment;maximal set;optimal design;signal processing;throughput	Gaddi Blumrosen;Alireza Abazari;Alexander Golberg;Mehmet Tonner;Martin L. Yarmush	2014	2014 IEEE 27th International Symposium on Computer-Based Medical Systems	10.1109/CBMS.2014.18	computer vision;image processing;computer science;machine learning;electroporation;cluster analysis	Visualization	40.07390205992669	-74.19541319758325	168725
8d3df18789a39b1693226d9126863c216b664f56	a variational shape optimization framework for image segmentation	mathematics;gunay;dissertation;college park ricardo h nochetto dogan;computer science a variational shape optimization framework for image segmentation university of maryland	Title of dissertation: A VARIATIONAL SHAPE OPTIMIZATION FRAMEWORK FOR IMAGE SEGMENTATION Günay Doǧan Doctor of Philosophy, 2006 Dissertation directed by: Professor Ricardo H. Nochetto Department of Mathematics Image segmentation is one of the fundamental problems in image processing. The goal is to partition a given image into regions that are uniform with respect to some image features and possibly to extract the region boundaries. Recently methods based on PDEs have been found to be an effective way to address this problem. These methods in general fall under the category of shape optimization, as the typical approach is to assign an energy to a shape, say a curve in 2d, and to deform the curve in a way that decreases its energy. In the end when the optimization terminates, the curve is not only at a minimum of the energy, but also at a boundary in the image. In this thesis, we emphasize the shape optimization view of image segmentation and develop appropriate tools to pursue the optimization in 2d and 3d. We first review the classes of shape energies that are used within the context of image segmentation. Then we introduce the analytical results that will help us design energy-decreasing deformations or flows for given shapes. We describe the gradient flows minimizing the energies, by taking into account the shape derivative information. In particular we emphasize the flexibility to accommodate different velocity spaces, which we later demonstrate to be quite beneficial. We turn the problem into the solution of a system of linear PDEs on the shape. We describe the corresponding space discretization based on the finite element method and an appropriate time discretization scheme as well. To handle mesh deterioration and possibly singularities due to motion of nodes, we describe time step control, mesh smoothing and angle width control procedures, also a topology surgery procedure that allows curves to undergo topological changes such as merging and splitting. In addition we introduce space adaptivity algorithms that help maintain accuracy of the method and reduce computational cost as well. Finally we apply our method to two major shape energies used for image segmentation: the minimal surface model (a.k.a geodesic active contours) and the Mumford-Shah model. For both we demonstrate the effectiveness of our method with several examples. A VARIATIONAL SHAPE OPTIMIZATION FRAMEWORK FOR IMAGE SEGMENTATION	algorithm;algorithmic efficiency;computation;discretization;finite element method;gradient;image processing;image segmentation;mathematical optimization;shape optimization;smoothing;variational principle;velocity (software development)	Gunay Dogan	2006			active shape model;mathematical optimization;simulation;shape optimization;segmentation-based object categorization;mathematics;geometry;image segmentation	Vision	49.13551785219587	-71.42758288257963	168904
15ee2ee5044042a9c4394c0ff07be792c5fc5a8b	automatic roi size selection and parameter initialization for model-based localization of 3d anatomical point landmarks		We introduce a new approach for the automatic selection of an optimal 3D ROI size for effective fitting of a deformable model for the purpose of landmark localization. In addition, we propose an algorithm to initialize the parameters of our previously introduced 3D ellipsoidal intensity model. The newly developed approaches have been successfully applied to 3D synthetic data as well as 3D MR images.	region of interest	Stefan Wörz;Karl Rohr	2004			computer vision;machine learning;pattern recognition	Robotics	43.555014312795805	-77.00803278025606	168944
79d77757d48e5a1192bdc2ce0b6878552c397d36	automated segmentation of the cerebellar lobules using boundary specific classification and evolution	sensitivity and specificity;female;cerebellum;middle aged;male;image enhancement;image interpretation computer assisted;adult;magnetic resonance imaging;reproducibility of results;algorithms;pattern recognition automated;humans;subtraction technique;aged	The cerebellum is instrumental in coordinating many vital functions ranging from speech and balance to eye movement. The effect of cerebellar pathology on these functions is frequently examined using volumetric studies that depend on consistent and accurate delineation, however, no existing automated methods adequately delineate the cerebellar lobules. In this work, we describe a method we call the Automatic Classification of Cerebellar Lobules Algorithm using Implicit Multi-boundary evolution (ACCLAIM). A multiple object geometric deformable model (MGDM) enables each boundary surface of each individual lobule to be evolved under different level set speeds. An important innovation described in this work is that the speed for each lobule boundary is derived from a classifier trained specifically to identify that boundary. We compared our method to segmentations obtained using the atlas-based and multi-atlas fusion techniques, and demonstrate ACCLAIM's superior performance.	algorithm;atlases;biological evolution;cerebellar model articulation controller;cortical cell layer of the cerebellum;eye movements;lobule;numerous;statistical classification	John A. Bogovic;Pierre-Louis Bazin;Sarah H. Ying;Jerry L. Prince	2013	Information processing in medical imaging : proceedings of the ... conference	10.1007/978-3-642-38868-2_6	computer vision;radiology;computer science;artificial intelligence;magnetic resonance imaging	Visualization	41.877681120961796	-78.4169222592705	169583
d1d4e8869d1fb1da72b58972dd6eae521b5f798e	learning shape statistics for hierarchical 3d medical image segmentation	3d image segmentation;image segmentation;computer science and information systems;manifolds;3d imaging;training;shape recognition;manifold learning;deformable models;shape image segmentation three dimensional displays training manifolds deformable models solid modeling;three dimensional;statistical analysis biomedical mri image segmentation medical image processing shape recognition solid modelling;mr imaging;shape;statistical analysis;medical image;three dimensional displays;medical image processing;solid modeling;mr images medical imaging hierarchical 3d image segmentation method patient specific shape prior surface patch shape statistics surpass model coarse to fine two stage strategy global segmentation local segmentation manifold learning techniques;surface patch shape statistics 3d image segmentation shape modeling manifold learning;shape modeling;shape priors;medical image segmentation;deformable model;surface patch shape statistics;solid modelling;biomedical mri	Accurate image segmentation is important for many medical imaging applications, whereas it remains challenging due to the complexity in medical images, such as the complex shapes and varied neighbor structures. This paper proposes a new hierarchical 3D image segmentation method based on patient-specific shape prior and surface patch shape statistics (SURPASS) model. In the segmentation process, a coarse-to-fine, two-stage strategy is designed, which contains global segmentation and local segmentation. In the global segmentation stage, patient-specific shape prior is estimated by using manifold learning techniques to achieve the overall segmentation. In the second stage, SURPASS is computed to solve the problem of poor segmentation at certain surface patches. The effectiveness of the proposed 3D image segmentation method has been demonstrated by the experiments on segmenting the prostate from a series of MR images.	experiment;global optimization;image segmentation;medical imaging;nonlinear dimensionality reduction;shape context;statistical shape analysis	Wuxia Zhang;Yuan Yuan;Xuelong Li;Pingkun Yan	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116068	image texture;stereoscopy;three-dimensional space;computer vision;range segmentation;manifold;shape;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;nonlinear dimensionality reduction;image segmentation;solid modeling;minimum spanning tree-based segmentation;scale-space segmentation	Vision	42.70541312139979	-77.32565352642568	169637
435ace68aa855103d76f869a88d34fee0771383b	computer vision — eccv 2002	general direction;general illumination;hamilton-jacobi equation;shading partial differential equation;general setting;discrete grid;viscosity solutions;shading method;previous work;shading equation;shading problem;partial differential equation;shape from shading;viscosity solution;finite difference	We present a novel algorithm for recovering a smooth manifold of unknown dimension and topology from a set of points known to belong to it. Numerous applications in computer vision can be naturally interpreted as instanciations of this fundamental problem. Recently, a non-iterative discrete approach, tensor voting, has been introduced to solve this problem and has been applied successfully to various applications. As an alternative, we propose a variational formulation of this problem in the continuous setting and derive an iterative algorithm which approximates its solutions. This method and tensor voting are somewhat the differential and integral form of one another. Although iterative methods are slower in general, the strength of the suggested method is that it can easily be applied when the ambient space is not Euclidean, which is important in many applications. The algorithm consists in solving a partial differential equation that performs a special anisotropic diffusion on an implicit representation of the known set of points. This results in connecting isolated neighbouring points. This approach is very simple, mathematically sound, robust and powerful since it handles in a homogeneous way manifolds of arbitrary dimension and topology, embedded in Euclidean or non-Euclidean spaces, with or without border. We shall present this approach and demonstrate both its benefits and shortcomings in two different contexts: (i) data visual analysis, (ii) skin detection in color images.	alpha shape;ambient occlusion;anisotropic diffusion;approximation algorithm;blind deconvolution;calculus of variations;computational complexity theory;convolution;deblurring;embedded system;euler;euler–lagrange equation;european conference on computer vision;experiment;focal (programming language);gaussian blur;image plane;iterative method;lecture notes in computer science;markov chain;markov random field;maxima and minima;numerical analysis;optimality criterion;pixel;spectrogram;springer (tank);variational principle;voxel;well-posed problem	Jan van Leeuwen;Anders Heyden;Gunnar Sparr;M. Nielsen;Peter Johansen	2002		10.1007/3-540-47967-8	computer vision;artificial intelligence;computer science	Vision	53.52295865448932	-70.66231941536292	169710
76ffd22acd0244339ea3429a093eacd60df76670	multiscale geometric active contour model and boundary extraction in kidney mr images		Active contour methods (ACM) are model-based approaches for image segmentation and were developed in the late 1980s. ACM can be divided into two classes: parametric active contour model and geometric active contour model. Geometric method is intrinsic model. Because of its completeness in mathematics, geometric active contour model overcomes many difficulties of the parametric active contour model. However, in medical images with heavy structural noise, the evolution of the geometric active contour will be seriously affected. To handle this problem, this paper proposed a multiscale geometric active contour model, based on the multiscale analysis method—bidimensional empirical mode decomposition. In the human kidney MR images, the proposed multiscale geometric active contour model successfully extracts the complex kidney contour.	active contour model;contour line	Ling X. Li;Jia Gu;Tiexiang Wen;Wenjian Qin;Hua Xiao;Jiaping Yu	2014		10.1007/978-3-319-06269-3_23	computer vision;mathematical optimization;pattern recognition	Vision	43.04252396019442	-74.13857646294127	169749
6a3ac548470a0fa4c96ff13ecd08807894dc0c33	a delaunay triangulation approach for segmenting clumps of nuclei	biology computing;97;delaunay triangulation;fluorescence;image segmentation;image processing;nuclear segmentation;nuclei;fluorescence imaging;geometric grouping;geometric grouping nuclear segmentation delaunay triangulation;computational geometry;nuclei clump segmentation;cell clump;data mining;mesh generation biological techniques biology computing cellular biophysics computational geometry fluorescence image processing;constraint satisfaction;low level features;image edge detection;high level geometric constraints;60;transforms;watersheds nuclear segmentation delaunay triangulation geometric grouping;delaunay triangulation vertices;cells biology image segmentation laboratories computer science nuclear electronics voting fluorescence image analysis testing biosensors;quantitative analysis;delaunay triangulation vertices nuclei clump segmentation cell based fluorescence imaging assays cell clump cellular phenotypic read out high level geometric constraints low level features maximum curvature points;cell based fluorescence imaging assays;biological techniques;hypothesis;geometric constraints;mesh generation;titanium;cellular biophysics;cellular phenotypic read out;cells biology;nuclear segmentation delaunay triangulation geometric grouping;maximum curvature points	Cell-based fluorescence imaging assays have the potential to generate massive amount of data, which requires detailed quantitative analysis. Often, as a result of fixation, labeled nuclei overlap and create a clump of cells. However, it is important to quantify phenotypic read out on a cell-by-cell basis. In this paper, we propose a novel method for decomposing clumps of nuclei using high-level geometric constraints that are derived from low-level features of maximum curvature computed along the contour of each clump. Points of maximum curvature are used as vertices for Delaunay triangulation (DT), which provides a set of edge hypotheses for decomposing a clump of nuclei. Each hypothesis is subsequently tested against a constraint satisfaction network for a near optimum decomposition. The proposed method is compared with other traditional techniques such as the watershed method with/without markers. The experimental results show that our approach can overcome the deficiencies of the traditional methods and is very effective in separating severely touching nuclei.	constraint satisfaction;delaunay triangulation;high- and low-level;watershed (image processing)	Quan Wen;Hang Chang;Bahram Parvin	2009	2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2009.5192970	titanium;mesh generation;computer vision;combinatorics;hypothesis;delaunay triangulation;constraint satisfaction;image processing;fluorescence;computational geometry;computer science;quantitative analysis;fluorescence-lifetime imaging microscopy;mathematics;geometry;image segmentation	Vision	39.63998158638075	-73.85200927960645	169981
58654b66686eee499a866b5b6fc043a884ccb5c9	flows of diffeomorphisms for multimodal image registration	minimization;filtering;convergence;anatomical mri volumes multimodal image registration diffeomorphism flows computational framework nonrigid multimodal registration minimization global statistical similarity criteria local statistical similarity criteria variational framework large deformations template propagation method small displacements regularization fast filtering techniques robust matching algorithms computational efficiency epi images fmri;large deformations;epi images;fmri;image matching;local statistical similarity criteria;template propagation method;fluid flow measurement;fast filtering techniques;small displacements;regularization;image registration low pass filters robustness magnetic resonance imaging filtering chromium computational efficiency mutual information convergence fluid flow measurement;diffeomorphism flows;statistical analysis;nonrigid multimodal registration;chromium;medical image processing;anatomical mri volumes;magnetic resonance imaging;image registration;statistical analysis image registration image sequences medical image processing biomedical mri filtering theory image matching;global statistical similarity criteria;multimodal image registration;mutual information;variational framework;robustness;low pass filters;large deformation;computational efficiency;robust matching algorithms;computational framework;filtering theory;biomedical mri;image sequences	We present a theoretical and computational framework for no nrigid multimodal registration. We proceed by minimization f statistical similarity criteria (global and local) in a variat onal framework, and use the corresponding gradients to drive a flow of di ffeomorphisms allowing large deformations. This flow is introdu ced through a new template propagation method, by composition o f small displacements. Regularization is performed using fa st filtering techniques. This approach yields robust matching algor ithms offering a good computational efficiency. We apply this meth od to compensate distortions between EPI images (fMRI) and anato mical MRI volumes.	distortion;gradient;image registration;matrix regularization;multimodal interaction;software propagation	Christophe Chefd'Hotel;Gerardo Hermosillo;Olivier D. Faugeras	2002		10.1109/ISBI.2002.1029367	regularization;computer vision;mathematical optimization;chromium;radiology;convergence;computer science;image registration;magnetic resonance imaging;pattern recognition;mathematics;mutual information;statistics;robustness	Vision	50.30974706710364	-72.30527982402043	170343
19082840000c1d0b17e0c47fdc3fa342dd11b6c9	tree-structured wavelet decomposition based on the maximization of fisher's distance	optimisation;gaussian processes;image classification;random processes trees mathematics wavelet transforms information theory matrix algebra optimisation image texture image classification radar imaging synthetic aperture radar gaussian processes markov processes;gaussian markov random field;matrix algebra;trees mathematics;wavelet decomposition;image texture;wavelet transforms;image classification tree structured wavelet decomposition fisher s distance maximization decomposition law tree structured wavelet transform texture discrimination optimization criterion simulation gaussian markov random fields real synthetic aperture radar images;optimization methods wavelet transforms image analysis analytical models markov random fields;wavelet transform;radar imaging;tree structure;random processes;markov processes;information theory;synthetic aperture radar	The aim of this work is to propose a method for optimizing the decomposition law of a tree-structured wavelet transform in order to maximize the capability of discriminating different textures. The optimization criterion is the maximization of the Fisher's distance. The analysis is carried out theoretically and by simulation on gaussian Markov random fields and is then applied to the classification of real Synthetic Aperture Radar images.	entropy maximization;expectation–maximization algorithm;fisher–yates shuffle;markov chain;markov random field;mathematical optimization;simulation;synthetic data;wavelet transform	Sergio Barbarossa;Laura Parodi	1995		10.1109/ICASSP.1995.480062	computer vision;information theory;machine learning;pattern recognition;mathematics;wavelet packet decomposition;statistics;wavelet transform	Vision	50.12502610463011	-69.62648422661742	170444
671e7cedc9d58e62417bbe8e5122594b9ecdb92e	uniqueness of the perfect fusion grid on ℤ d	chessboard;perfect fusion graph;image segmentation;image processing;perfect fusion grid;digital topology;discrete geometry;image analysis;line graph;region merging;adjacency relation	Region merging methods consist of improving an initial segmentation by merging some pairs of neighboring regions. In a graph, merging two regions, separated by a set of vertices, is not straightforward. The perfect fusion graphs defined in J. Cousty et al. (J. Math. Imaging Vis. 30:(1):87–104, 2008) verify all the basic properties required by region merging algorithms as used in image segmentation. Unfortunately, the graphs which are the most frequently used in image analysis (namely, those induced by the direct and the indirect adjacency relations) are not perfect fusion graphs. The perfect fusion grid, introduced in the above mentioned reference, is an adjacency relation on ℤ d which can be used in image analysis, which indeed induces perfect fusion graphs and which is “between” the graphs induced by the direct and the indirect adjacencies. One of the main results of this paper is that the perfect fusion grid is the only such graph whatever the dimension d.	algorithm;graph (discrete mathematics);image analysis;image segmentation;visual instruction set	Jean Cousty;Gilles Bertrand	2009	Journal of Mathematical Imaging and Vision	10.1007/s10851-009-0147-0	claw-free graph;discrete geometry;pathwidth;perfect graph theorem;adjacency list;computer vision;split graph;combinatorics;discrete mathematics;cograph;image analysis;topology;image processing;computer science;trivially perfect graph;mathematics;distance-hereditary graph;image segmentation;chordal graph;line graph;digital topology	Vision	44.28353749144521	-69.3206886150132	170625
9dcd428c872024a0dbd5a43af09da571dd377127	kruskal's minimum spanning tree approach to brain fiber tractography computation	tensile stress;conferences signal processing optical fiber communication tensile stress signal processing algorithms algorithm design and analysis radio frequency;tractography kruskal s minimum spanning tree diffusion tensor;radio frequency;signal processing;signal processing algorithms;optical fiber communication;algorithm design and analysis;conferences	Kruskal's minimum spanning tree algorithm is a graph algorithm which constructs the tree with the minimum total edge length. The starting node of the tree generated by Kruskal's algorithm is not important. This is an advantage in diffusion tensor fiber analysis. In this study, a tractography approach, which does not depend on user selection, independent from a starting node, covers the whole region of interest, and calculates all the connectivity, is proposed. In this diffusion tensor tractography study, the advantages and limitations of the Kruskal's minimum spanning tree method are discussed, and a tractography algorithm based on this approach is developed.	computation;file spanning;kruskal's algorithm;list of algorithms;minimum spanning tree;region of interest	Dilek Göksel Duru;Adil Deniz Duru	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830556	algorithm design;computer vision;combinatorics;computer science;theoretical computer science;machine learning;signal processing;stress;radio frequency	Theory	41.64515964742799	-74.7490408579803	170636
a3c9950a630349a866a57fbd8fb9372cdfba4a64	filtered stochastic optimization for binary tomography	radon transforms;stochastic processes bone computerised tomography diagnostic radiography image reconstruction medical image processing radon transforms;image reconstruction tomography bones noise tv radio frequency transforms;radon transform;x ray imaging;binary tomography;noisy bone cross section;filtered stochastic optimization;bones;radio frequency;stochastic diffusion equation;stochastic processes;image reconstruction;medical image processing;transforms;bone;total variation regularization method;computerised tomography;radon transform filtered stochastic optimization binary tomography stochastic diffusion equation image reconstruction total variation regularization method noisy bone cross section;total variation tv;tv;tomography;stochastic methods;x ray imaging inverse problems binary tomography total variation tv stochastic methods;diagnostic radiography;noise;inverse problems	In this work, we use stochastic diffusion equations to improve the reconstruction of binary tomography cross-sections obtained from a small number of projections. A first reconstruction image is obtained with the Total Variation regularization method. The reconstruction is then refined with stochastic approaches. This method is applied to a noisy bone cross-section with 10 projection angles. The main purpose of this work is to show the improvements obtained with a filter taking into account wavefront set properties of the Radon transform.	mathematical optimization;projection-slice theorem;stochastic optimization;tomography;total variation denoising	Lin Wang;Bruno Sixou;Françoise Peyrin	2015	2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2015.7164187	iterative reconstruction;computer vision;mathematical optimization;radon transform;radiology;medicine;inverse problem;noise;mathematics;tomography;optics;radio frequency;tomographic reconstruction	Vision	52.05296232249884	-78.56309504146225	170705
309aa0bdf91f857b102d28e375c8ae92708cee79	the role of key-points in finding contours	edge detection;key point detection;edge linking;contour completion	This paper describes a method for aggregating local edge evidences into coherent pieces of contour. An independent representation of corner and junction features provides suitable stop-conditions for the aggregation process and allows to divide contours into meaningful sub-strings, right from the beginning. The active role of corner and junction points makes the contours converge onto them and greatly reduces the problems associated with purely edge-based methods. A second stage is concerned with completing established contours across regions that are less well-deened by contrast. The algorithm suggested uses the attributes of established structures (e.g. direction of termination) as well as local orientation and edge evidences to constrain possible completions in a rigorous way.	algorithm;coherence (physics);converge	Olof Henricsson;Friedrich Heitger	1994		10.1007/BFb0028369	computer vision;edge detection;computer science;pattern recognition	Vision	47.97521966907994	-70.09410464082524	170810
b236c71f1f719c4603e9b3ef2acb0203374ab6b5	first order augmentation to tensor voting for boundary inference and multiscale analysis in 3d	tensile stress voting inference algorithms surface treatment data mining computer vision application software delay data analysis psychology;second order;sensitivity and specificity;brain;tensile stress;imaging three dimensional;automatic scale selection first order augmentation tensor voting boundary inference multiscale analysis computer vision boundary detection first order voting fields 3d surface boundaries three dimensional surface boundaries 3d volume boundaries three dimensional volume boundaries;application software;3d perceptual organization;edge detection;tensor voting;three dimensional volume boundaries;multiscale analysis;psychology;3d volume boundaries;data mining;computer vision;3d surface boundaries;signal processing computer assisted;data analysis;first order voting;edge detection computer vision tensors;image enhancement;numerical analysis computer assisted;surface treatment;cluster analysis;discontinuities;first order;first order augmentation;image interpretation computer assisted;voting;first order voting fields;reproducibility of results;artificial intelligence;algorithms;pattern recognition automated;inference algorithms;humans;missing data;subtraction technique;automatic scale selection;three dimensional surface boundaries;boundary inference;boundary detection;algorithms artificial intelligence brain cluster analysis computer simulation humans image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique;computer simulation;information storage and retrieval;perceptual organization;tensors	Most computer vision applications require the reliable detection of boundaries. In the presence of outliers, missing data, orientation discontinuities, and occlusion, this problem is particularly challenging. We propose to address it by complementing the tensor voting framework, which was limited to second order properties, with first order representation and voting. First order voting fields and a mechanism to vote for 3D surface and volume boundaries and curve endpoints in 3D are defined. Boundary inference is also useful for a second difficult problem in grouping, namely, automatic scale selection. We propose an algorithm that automatically infers the smallest scale that can preserve the finest details. Our algorithm then proceeds with progressively larger scale to ensure continuity where it has not been achieved. Therefore, the proposed approach does not oversmooth features or delay the handling of boundaries and discontinuities until model misfit occurs. The interaction of smooth features, boundaries, and outliers is accommodated by the unified representation, making possible the perceptual organization of data in curves, surfaces, volumes, and their boundaries simultaneously. We present results on a variety of data sets to show the efficacy of the improved formalism.	algorithm;computer vision;handling (psychology);hidden surface determination;inference;large;missing data;scott continuity;semantics (computer science)	Wai-Shun Tong;Chi-Keung Tang;Philippos Mordohai;Gérard G. Medioni	2004	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2004.1273934	computer simulation;computer vision;application software;edge detection;tensor;voting;missing data;computer science;theoretical computer science;machine learning;first-order logic;cluster analysis;stress;data analysis;second-order logic;statistics;classification of discontinuities	Vision	45.586084806145614	-77.80734627396568	170855
1aa16cb01a471f34b4f54585a4f7ddef2456f1ba	learning-based topological correction for infant cortical surfaces		Reconstruction of topologically correct and accurate cortical surfaces from infant MR images is of great importance in neuroimaging mapping of early brain development. However, due to rapid growth and ongoing myelination, infant MR images exhibit extremely low tissue contrast and dynamic appearance patterns, thus leading to much more topological errors (holes and handles) in the cortical surfaces derived from tissue segmentation results, in comparison to adult MR images which typically have good tissue contrast. Existing methods for topological correction either rely on the minimal correction criteria, or ad hoc rules based on image intensity priori, thus often resulting in erroneous correction and large anatomical errors in reconstructed infant cortical surfaces. To address these issues, we propose to correct topological errors by learning information from the anatomical references, i.e., manually corrected images. Specifically, in our method, we first locate candidate voxels of topologically defected regions by using a topology-preserving level set method. Then, by leveraging rich information of the corresponding patches from reference images, we build region-specific dictionaries from the anatomical references and infer the correct labels of candidate voxels using sparse representation. Notably, we further integrate these two steps into an iterative framework to enable gradual correction of large topological errors, which are frequently occurred in infant images and cannot be completely corrected using one-shot sparse representation. Extensive experiments on infant cortical surfaces demonstrate that our method not only effectively corrects the topological defects, but also leads to better anatomical consistency, compared to the state-of-the-art methods.	anatomy, regional;bibliographic reference;dictionary [publication type];experiment;hoc (programming language);inference;iterative method;memory segmentation;neuroimaging;rule (guideline);sparse approximation;sparse matrix;voxel;biologic segmentation;brain development;myelination	Shijie Hao;Gang Li;Li Wang;Yu Meng;Dinggang Shen	2016	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-319-46720-7_26	voxel;computer vision;pattern recognition;topological defect;computer science;artificial intelligence;software;neuroimaging;level set method;sparse approximation;segmentation;topology	Vision	42.790045700461015	-78.03878741240072	170870
07074006f9dab53765b6bb63b547c3d279e50bed	encoding probabilistic brain atlases using bayesian inference	databases;deformable registration;brain;image coding;image segmentation;mesh based representation;pulse sequence adaptive segmentation;data compression;spatial blurring;encoding bayesian methods training data biomedical imaging magnetic resonance imaging databases frequency deformable models brain modeling anatomy;brain atlases;bayes methods;bayesian inference;bayes theorem;bayesian methods;biomedical imaging;deformable models;image processing computer assisted;model comparison;training data;deformation model;brain modeling;frequency of occurrence;brain mapping;brain mri scan probabilistic brain atlas bayesian inference generative image model mesh based representation deformation model group wise registration atlas estimation scheme data compression spatial blurring 2d training dataset atlas construction technique pulse sequence adaptive segmentation neuroanatomical structure;neurophysiology bayes methods biomedical mri brain data compression image coding image registration image segmentation image sequences medical image processing mesh generation;medical image processing;magnetic resonance imaging;image registration;brain mri scan;probabilistic brain atlas;generative image model;reproducibility of results;atlas estimation scheme;2d training dataset;atlas formation;neuroanatomical structure;atlas construction technique;models statistical;group wise registration;artificial intelligence;algorithms;humans;neurophysiology;model comparison atlas formation bayesian inference brain modeling computational anatomy image registration mesh generation;frequency;mesh generation;encoding;image modeling;deformable model;anatomy;computational anatomy;optimization model;markov chains;biomedical mri;image sequences	This paper addresses the problem of creating probabilistic brain atlases from manually labeled training data. Probabilistic atlases are typically constructed by counting the relative frequency of occurrence of labels in corresponding locations across the training images. However, such an ldquoaveragingrdquo approach generalizes poorly to unseen cases when the number of training images is limited, and provides no principled way of aligning the training datasets using deformable registration. In this paper, we generalize the generative image model implicitly underlying standard ldquoaveragerdquo atlases, using mesh-based representations endowed with an explicit deformation model. Bayesian inference is used to infer the optimal model parameters from the training data, leading to a simultaneous group-wise registration and atlas estimation scheme that encompasses standard averaging as a special case. We also use Bayesian inference to compare alternative atlas models in light of the training data, and show how this leads to a data compression problem that is intuitive to interpret and computationally feasible. Using this technique, we automatically determine the optimal amount of spatial blurring, the best deformation field flexibility, and the most compact mesh representation. We demonstrate, using 2-D training datasets, that the resulting models are better at capturing the structure in the training data than conventional probabilistic atlases. We also present experiments of the proposed atlas construction technique in 3-D, and show the resulting atlases' potential in fully-automated, pulse sequence-adaptive segmentation of 36 neuroanatomical structures in brain MRI scans.	addresses (publication format);atlases;bayesian approaches to brain function;blurred vision;cervical atlas;data compression;experiment;inference;probabilistic automaton;probabilistic database;scheme;texture atlas;tracer;registration - actclass	Koenraad Van Leemput	2009	IEEE Transactions on Medical Imaging	10.1109/TMI.2008.2010434	data compression;mesh generation;computer vision;training set;markov chain;radiology;bayesian probability;computer science;image registration;magnetic resonance imaging;machine learning;frequency;pattern recognition;mathematics;image segmentation;bayes' theorem;brain mapping;bayesian inference;neurophysiology;encoding;statistics	Vision	44.519987008185275	-77.22343807982813	171371
7faf90eaeb6d6fcb6b13220ae706b59fe152d41a	bayesian scale space analysis of images	belief networks;image resolution;inference mechanisms;statistical analysis;bayesian inference;bayesian paradigm;bayesian scale space image analysis;artefact features;flexible image models;multiresolution analysis;random noise;scale-dependent components;statistical scale space methodologies	Two new statistical scale space methodologies are discussed. The first method aims to detect differences between two images obtained from the same object at two different instants of time. Both small scale sharp changes and large scale average changes are detected. The second method detects features that differ in intensity from their surroundings and it produces a multiresolution analysis of an image as a sum of scale-dependent components. As images are usually noisy, Bayesian inference is used to separate real differences and features from artefacts caused by random noise. The use of the Bayesian paradigm facilitates application of flexible image models and it also allows one to take advantage of an expert's prior knowledge about the images considered.	bayesian approaches to brain function;bayesian network;multiresolution analysis;noise (electronics);programming paradigm;scale space	Leena Pasanen;Lasse Holmström	2013	2013 8th International Symposium on Image and Signal Processing and Analysis (ISPA)		frequentist inference;computer science;machine learning;pattern recognition;bayesian statistics;statistics;scale-space axioms	Vision	51.221574744216355	-74.39109006397307	171473
6511cebd3db25fa87bd4f623477674a7c10d2e11	unsupervised superpixel-based segmentation of histopathological images with consensus clustering		We present a framework for adapting consensus clustering methods with superpixels to segment oropharyngeal cancer images into tissue types (epithelium, stroma and background). The simple linear iterative clustering algorithm is initially used to split-up the image into binary superpixels which are then used as clustering elements. Colour features of the superpixels are extracted and fed into several base clustering approaches with various parameter initializations. Two consensus clustering formulations are then used, the Evidence Accumulation Clustering (EAC) and the voting-based function. They both combine the base clustering outcomes to obtain a single more robust consensus result. Unlike most unsupervised tissue image segmentation approaches that depend on individual clustering methods, the proposed approach allows for a robust detection of tissue compartments. For the voting-based consensus function, we introduce a technique based on image processing to generate a consistent labelling scheme among the base clustering outcomes. Experiments conducted on forty five hand-annotated images of oropharyngeal cancer tissue microarray cores show that the ensemble algorithm generates more accurate and stable results than individual clustering algorithms. The clustering performance of the voting-based consensus function using our re-labelling technique also outperforms the existing		Shereen Fouad;David A. Randell;Antony Galton;Hisham Mehanna;Gabriel Landini	2017		10.1007/978-3-319-60964-5_67	consensus clustering;image processing;cluster analysis;tissue microarray;artificial intelligence;image segmentation;segmentation;computer science;pattern recognition	ML	40.68307225285803	-72.06442982211996	171678
698e7a043b2223b398978e12d5aadb8e6fd5be9c	disjunctive normal level set: an efficient parametric implicit method	memory management;image segmentation;standards;segmentation level set method parametric level set method multiphase level set;level set;segmentation;parametric level set method;shape;mathematical model;multiphase level set;computational efficiency;level set method;level set image segmentation shape computational efficiency memory management mathematical model standards;tk electrical engineering electronics nuclear engineering	Level set methods are widely used for image segmentation because of their capability to handle topological changes. In this paper, we propose a novel parametric level set method called Disjunctive Normal Level Set (DNLS), and apply it to both two phase (single object) and multiphase (multi-object) image segmentations. The DNLS is formed by union of polytopes which themselves are formed by intersections of half-spaces. The proposed level set framework has the following major advantages compared to other level set methods available in the literature. First, segmentation using DNLS converges much faster. Second, the DNLS level set function remains regular throughout its evolution. Third, the proposed multiphase version of the DNLS is less sensitive to initialization, and its computational cost and memory requirement remains almost constant as the number of objects to be simultaneously segmented grows. The experimental results show the potential of the proposed method.	algorithmic efficiency;computation;disjunctive normal form;explicit and implicit methods;image segmentation;numerous;physical object;biologic segmentation	Fitsum Mesadi;Müjdat Çetin;Tolga Tasdizen	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7533171	computer vision;mathematical optimization;discrete mathematics;shape;computer science;level set;theoretical computer science;mathematical model;mathematics;image segmentation;segmentation;set function;level set method;level set;memory management	Vision	49.037062941318396	-71.50610099823209	171763
5981eba46405254a7fadb5f0bb112cb0108d5af7	shape-aware surface reconstruction from sparse data		The reconstruction of an object’s shape or surface from a set of 3D points is a common topic in materials and life sciences, computationally handled in computer graphics. Such points usually stem from optical or tactile 3D coordinate measuring equipment. Surface reconstruction also appears in medical image analysis, e.g. in anatomy reconstruction from tomographic measurements or the alignment of intra-operative navigation and preoperative planning data. In contrast to mere 3D point clouds, medical imaging yields contextual information on the 3D point data that can be used to adopt prior information on the shape that is to be reconstructed from the measurements. In this work we propose to use a statistical shape model (SSM) as a prior for surface reconstruction. The prior knowledge is represented by a point distribution model (PDM) that is associated with a surface mesh. Using the shape distribution that is modelled by the PDM, we reformulate the problem of surface reconstruction from a probabilistic perspective based on a Gaussian Mixture Model (GMM). In order to do so, the given measurements are interpreted as samples of the GMM. By using mixture components with anisotropic covariances that are oriented according to the surface normals at the PDM points, a surface-based fitting is accomplished. By estimating the parameters of the GMM in a maximum a posteriori manner, the reconstruction of the surface from the given measurements is achieved. Extensive experiments suggest that our proposed approach leads to superior surface reconstructions compared to Iterative Closest Point (ICP) methods.	computer graphics;experiment;google map maker;image analysis;iterative closest point;iterative method;medical image computing;medical imaging;mixture model;normal (geometry);point cloud;point distribution model;polygon mesh;sparse;statistical shape analysis;tomography	Florian Bernard;Luis Salamanca;Johan Thunberg;Alexander Tack;Dennis Jentsch;Hans Lamecker;Stefan Zachow;Frank Hertel;Jorge M. Gonçalves;Peter Gemmar	2016	CoRR		computer vision;computer science;machine learning;pattern recognition;statistics	Vision	46.89404670070481	-78.07114765099301	171796
23b389eb1f80a0f974b3b8e607405c7342ad378a	stereo disparity computation in the dct domain using genetic algorithms	biologically inspired optimisation stereo disparity computation dct domain genetic algorithms stereo matching camera discrete cosine transform idct correspondence problem image block dct coefficients estimated disparity map statistical properties ac coefficients dc component image domain intensity similarity measure inverse transform;discrete cosine transforms genetic algorithms evolution biology biological cells ac generators systems engineering and theory layout cameras image generation robots;idct;dct coefficients;image matching;biologically inspired optimisation;correspondence problem;image block;discrete cosine transform;dct domain;ac coefficients;statistical properties;estimated disparity map;image domain;stereo matching;statistical analysis;dc component;discrete cosine transforms;stereo image processing;genetic algorithms stereo image processing discrete cosine transforms inverse problems image matching statistical analysis;block matching;genetic algorithm;intensity similarity measure;genetic algorithms;inverse transform;stereo disparity computation;similarity measure;camera;inverse problems	The objective of stereo matching is, given a point in one view of a scene, to find the homologous point in another view from an adjacent camera. We use the discrete cosine transform (DCT) with evolutionary methods as an approach to solve the correspondence problem. The matching process is implemented by generating for each image block sets of DCT coefficients, which when reverse transformed become the estimated disparity map for that block. Assumed statistical properties of the AC coefficients are taken into account, while the DC component is estimated from a preliminary block matching stage. Matching is performed in the image domain using an intensity similarity measure. The job of optimising the DCT coefficients, whose inverse transform gives the disparity, is carried out by a biologically inspired optimisation technique: the genetic algorithm.	binocular disparity;computation;discrete cosine transform;genetic algorithm	Carla L. Pagliari;Tim J. Dennis	1997		10.1109/ICIP.1997.632080	computer vision;mathematical optimization;genetic algorithm;computer science;theoretical computer science;mathematics;algorithm;statistics	Robotics	43.93268533850991	-66.47695212845429	171858
ab5fce8f49b40a5ddbe84409dce2515ac621f3dd	fuzzified gabor filter for license plate detection	parameter fuzzification;fuzzy logic;gabor filter;license plate detection;membership function	This paper presents a new algorithm for detecting and extracting license plates from a complex image by using a fuzzy two-dimensional Gabor filter. The filter parameters are fuzzified to optimize the Gabor filter. In particular, the orientation and wavelengths of the Gabor filter are fuzzified. Fuzzification of the wavelength results in greater selectivity. These parameters dominate the filtering results. Bell’s function and triangular membership function were proven to be the most efficient method for selecting the filter parameters in our experiments and were used for fuzzification. The use of these filters provided satisfactory results; the components of interest were efficiently extracted, and the procedure was found to be very noise-resistant. & 2015 Elsevier Ltd. All rights reserved.	algorithm;automatic number plate recognition;bell's theorem;digital image processing;experiment;fuzzy logic;fuzzy set;gabor filter;image quality;image scaling;outline of object recognition;reflection (computer graphics);requirement;selectivity (electronic);sensor	Vladimir Tadic;Miodrag Popovic;Péter Odry	2016	Eng. Appl. of AI	10.1016/j.engappai.2015.09.009	fuzzy logic;membership function;computer science;artificial intelligence;machine learning;pattern recognition;data mining	Vision	41.22880816973309	-67.17373500039277	171990
919071ff8ccfb8d125adea726c54636086632ce9	detecting genetic association of common human facial morphological variation using high density 3d image registration	mouth;imaging three dimensional;journal;permutation;lips;face;humans;han chinese;morphogenesis;fibroblast growth factor;polymorphism single nucleotide;models anatomic;phenotypes	Human facial morphology is a combination of many complex traits. Little is known about the genetic basis of common facial morphological variation. Existing association studies have largely used simple landmark-distances as surrogates for the complex morphological phenotypes of the face. However, this can result in decreased statistical power and unclear inference of shape changes. In this study, we applied a new image registration approach that automatically identified the salient landmarks and aligned the sample faces using high density pixel points. Based on this high density registration, three different phenotype data schemes were used to test the association between the common facial morphological variation and 10 candidate SNPs, and their performances were compared. The first scheme used traditional landmark-distances; the second relied on the geometric analysis of 15 landmarks and the third used geometric analysis of a dense registration of ∼30,000 3D points. We found that the two geometric approaches were highly consistent in their detection of morphological changes. The geometric method using dense registration further demonstrated superiority in the fine inference of shape changes and 3D face modeling. Several candidate SNPs showed potential associations with different facial features. In particular, one SNP, a known risk factor of non-syndromic cleft lips/palates, rs642961 in the IRF6 gene, was validated to strongly predict normal lip shape variation in female Han Chinese. This study further demonstrated that dense face registration may substantially improve the detection and characterization of genetic association in common facial variation.	alignment;chinese room;cleft lip;distance;face;galaxy morphological classification;geometric analysis;han unification;image registration;inference;lip structure;mental association;palate;performance;phenotype;pixel;sensor;single nucleotide polymorphism;surrogates;trait;registration - actclass	Shouneng Peng;Jingze Tan;Sile Hu;Hang Zhou;Jing Guo;Li Jin;Kun Tang	2013		10.1371/journal.pcbi.1003375	face;biology;computer vision;morphogenesis;bioinformatics;phenotype;permutation;genetics;fibroblast growth factor	Vision	42.269768339792826	-77.38957036794224	172114
d9c8734196c4504e3e99f4a68d32d3b9da2521e5	an application of lbf energy in image/video frame text detection	wavelet transforms feature extraction image texture text analysis text detection;level set image edge detection feature extraction discrete wavelet transforms laplace equations;level set function wavelet transform local binary fitting energy;image intensity information lbf energy local binary fitting energy image frame video frame text detection texture feature extraction wavelet transform	In the present paper, we propose a new method for detecting texts in image and video frames. Initially, texture features are extracted using wavelet transform. Further, local binary fitting energy is introduced to obtain local image intensity information. In addition to that variational level set function is integrated to detect an accurate boundary of the texts present in images/video frames. The proposed method is able to detect a region of text with high complexity as well low intensity texts and also detects a text region in varying background. Experiment is conducted on ICDAR datasets and 101 video images dataset. Performance comparison shows that the proposed method exhibits better results in terms of computational efficiency and accuracy.	computation;international conference on document analysis and recognition;k-means clustering;sensor;variational principle;wavelet transform	V. N. Manjunath Aradhya;M. S. Pavithra	2014	2014 14th International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2014.133	image texture;wavelet;computer vision;feature detection;speech recognition;harmonic wavelet transform;pattern recognition;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;feature	Vision	44.484403591301444	-67.35507276590991	172252
7cbcab809753e53c1cae2a19bd9fbd005a0e1189	a linear wavelet filter for parametric imaging with dynamic pet	wavelet analysis;sensitivity and specificity;nonlinear filters;linear estimation;brain;dynamic pet;theoretical framework;dynamic positron emission tomography;space time problem;pet imaging;hospitals;male;parametric imaging;models biological;tomography emission computed;space time;indexing terms;aged algorithms alzheimer disease brain computer simulation fluorodeoxyglucose f18 humans image enhancement linear models male models biological phantoms imaging raclopride radioisotopes reproducibility of results sensitivity and specificity signal processing computer assisted stochastic processes tomography emission computed;positron emission tomography;signal processing computer assisted;wavelet transforms;normal multivariate vector;james stein estimator;james stein linear estimator;image enhancement;medical image processing positron emission tomography filtering theory wavelet transforms image sequences;kinetic theory;wavelet transform;vectors;stochastic processes;kinetic analysis;image reconstruction;medical image processing;least square;reproducibility of results;phantoms imaging;pet images;radioisotopes;simulation study;algorithms;nonstationary filter;nonlinear filters positron emission tomography kinetic theory image reconstruction hospitals wavelet transforms wavelet coefficients vectors wavelet analysis in vivo;humans;fluorodeoxyglucose f18;least squares risk;point of view;linear wavelet filter;alzheimer disease;linear models;nonstationary filter dynamic pet parametric imaging linear wavelet filter dynamic positron emission tomography wavelet transform space time problem pet sequence modeling normal multivariate vector independent wavelet coefficients least squares risk pet images james stein linear estimator nonlinear wavelet filter;computer simulation;in vivo;wavelet coefficients;filtering theory;nonlinear wavelet filter;aged;pet sequence modeling;image sequences;raclopride;independent wavelet coefficients	Describes a new filter for parametric images obtained from dynamic positron emission tomography (PET) studies. The filter is based on the wavelet transform following the heuristics of a previously published method that are here developed into a rigorous theoretical framework. It is shown that the space-time problem of modeling a dynamic PET sequence reduces to the classical one of estimation of a normal multivariate vector of independent wavelet coefficients that, under least-squares risk, can be solved by straightforward application of well established theory. From the study of the distribution of wavelet coefficients of PET images, it is inferred that a James-Stein linear estimator is more suitable for the problem than traditional nonlinear procedures that are incorporated in standard wavelet filters. This is confirmed by the superior performance of the James-Stein filter in simulation studies compared to a state-of-the-art nonlinear wavelet filter and a nonstationary filter selected from literature. Finally, the formal framework is interpreted for the practitioner's point of view and advantages and limitations of the method are discussed.	ct scan;coefficient;heuristics;inference;least squares;nonlinear system;point of view (computer hardware company);polyethylene terephthalate;positron-emission tomography;positrons;scientific publication;simulation;tomography, emission-computed;wavelet transform	Federico E. Turkheimer;John A. D. Aston;Richard B. Banati;Cyril Riddell;Vincent J. Cunningham	2003	IEEE Transactions on Medical Imaging	10.1109/TMI.2003.809597	computer simulation;wavelet;computer vision;econometrics;kernel adaptive filter;second-generation wavelet transform;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;filter design;discrete wavelet transform;statistics;wavelet transform	Visualization	51.67214143485453	-78.60994152054704	172304
85f9f7bdbfdcf1fbf5857b81f34029d28b55e0f4	analytic regularization of uniform cubic b-spline deformation fields	deformable registration;bending energy;b spline;thin plate;analytic regularization	Image registration is inherently ill-posed, and lacks a unique solution. In the context of medical applications, it is desirable to avoid solutions that describe physically unsound deformations within the patient anatomy. Among the accepted methods of regularizing non-rigid image registration to provide solutions applicable to medical practice is the penalty of thin-plate bending energy. In this paper, we develop an exact, analytic method for computing the bending energy of a three-dimensional B-spline deformation field as a quadratic matrix operation on the spline coefficient values. Results presented on ten thoracic case studies indicate the analytic solution is between 61-1371x faster than a numerical central differencing solution.	algorithm;anatomic structures;autoregressive integrated moving average;b-spline;bsd;bending - changing basic body position;chest;coefficient;computation (action);cubic function;decompression sickness;ibm notes;image registration;matrix regularization;medical image computing;monumenta germaniae historica;muscle rigidity;musculoskeletal diseases;nih roadmap initiative tag;national centers for biomedical computing;nephrogenic systemic fibrosis;numerical analysis;patients;solutions;spline (mathematics);united states national institutes of health;well-posed problem	James A. Shackleford;Qi Yang;Ana M. Lourenço;Nadya Shusharina;Nagarajan Kandasamy;Gregory C. Sharp	2012	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-33418-4_16	b-spline;mathematical optimization;topology;computer science;mathematics;geometry	Visualization	48.27580504641903	-78.29587747522325	172388
9273dbc8ad4c710434865dba5b6173012e23f6dd	a csc based classification method for ct bone images	ct bone images;three dimensional imaging;biological tissues;image segmentation;computed tomography;application software;medical image processing bone computerised tomography image classification image segmentation;computer tomography data;image classification;biomedical imaging;csc based classification method;prior knowledge;2d segmentation method;visualization;computed tomography bones image segmentation biological tissues visualization robustness application software military computing biomedical imaging medical diagnostic imaging;bones;computer tomography data csc based classification method ct bone images color structure code 2d segmentation method;medical image processing;bone;computerised tomography;robustness;color structure code;military computing;medical diagnostic imaging	The CSC (color structure code) is a robust and fast two dimensional segmentation method which has been already generalized to three dimensional images. As the CSC does not need any prior knowledge it can be used for different applications. In this paper we focus on the segmentation of bones from computer tomography data (CT) with the CSC. In the postprocessing step CSC segments will be classified according to their average hounsfield value. The classification is steered by some application specific topological rules.	ct scan;color structure code;tomography	Patrick Sturm;Lutz Priese;Haojun Wang	2006	Third International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT'06)	10.1109/3DPVT.2006.11	computer vision;radiology;engineering;medical physics	Arch	41.860868418569225	-74.93545960883905	172651
ab06a095d739a7102a0eb38d9a04651fc33cd5c6	direct estimation of cardiac biventricular volumes with an adapted bayesian formulation	probability;image segmentation;probability cardiovascular system diseases image segmentation medical disorders medical image processing physiological models;medical disorders;global cardiac functions cardiac biventricular volumes adapted bayesian formulation global cardiac functions left ventricle right ventricle segmentation free method likelihood function probability model rv cavities lv cavities biventricular volumes independent ground truth cardiac functions;medical image processing;cavity resonators correlation bayes methods computational modeling estimation adaptation models image segmentation;diseases;cardiovascular system;physiological models;ventricular volume bayesian estimation cardiac function assessment cardiac mri	Accurate estimation of the ventricular volumes is essential to the assessment of global cardiac functions. The existing estimation methods are mostly restricted to the left ventricle (LV), and often require segmentation which is challenging and computationally expensive. This paper proposes to estimate the volumes of both LV and right ventricle (RV) jointly with an efficient segmentation-free method. The proposed method employs an adapted Bayesian formulation. It introduces a novel likelihood function to exploit multiple appearance features, and a novel prior probability model to incorporate the area correlation between LV and RV cavities. The method is validated on a comprehensive dataset containing 56 clinical subjects (3360 images in total). The experimental results demonstrate that the estimated biventricular volumes are highly correlated to their independent ground truth. As a result, the proposed method enables a direct, efficient, and accurate assessment of global cardiac functions.	analysis of algorithms;artificial cardiac pacemaker;estimated;ground truth;heart ventricle;left ventricular structure;logical volume management;real-time clock;real-time computing;right ventricular structure;segmentation action;silo (dataset)	Zhijie Wang;Mohamed Ben Salah;Bin Gu;Ali Islam;Aashish Goela;Shuo Li	2014	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2014.2299433	computer vision;mathematical optimization;computer science;engineering;circulatory system;probability;mathematics;biological engineering;image segmentation;statistics	Vision	42.782615535243	-79.22914640899224	172735
cabaa544f55b8ac627e18f4244a610d5de1bf775	parameter selection for a markovian signal reconstruction with edge detection	energy resolution;gaussian noise;estimation theory;lattices;gaussian processes;markovian signal reconstruction;truncated quadratic potential functions;2d signals;edge detection;bayes methods;markov random fields;bayesian methods;maximum likelihood estimation;weak string;markov model;locally homogeneous regions;parameter selection;image edge detection;posterior energy;fourier transforms;map estimator;map estimation;piecewise gaussian markov model;signal resolution;signal reconstruction;markov processes;potential function;bayesian reconstruction;1d signals;weak membrane;signal reconstruction image edge detection fourier transforms bayesian methods energy resolution signal resolution lattices gaussian noise markov random fields estimation theory;map estimator parameter selection markovian signal reconstruction edge detection bayesian reconstruction 2d signals 1d signals locally homogeneous regions piecewise gaussian markov model truncated quadratic potential functions weak string weak membrane posterior energy;maximum likelihood estimation signal reconstruction edge detection bayes methods gaussian processes markov processes	We propose a method for the parameter selection for a Bayesian reconstruction of 1D or 2D signals, constituted by locally homogeneous regions, from incomplete and noisy projection data. A piecewise Gaussian Markov model (PG MM), defined as a sum of truncated quadratic potential functions, is used to regularise the reconstruction, which is otherwise ill-posed. This model is called the weak string in 1D and the weak membrane in 2D. The posterior energy is highly non-convex and the MAP estimator is piecewise continuous; the model parameters play a particularly decisive role. The resolution of the reconstruction-the finest recoverable features-is determined jointly by the parameters and the observation model. On the other hand, we propose a method for the determination of the parameters in order to reach, or at least to approach as closely as possible, a desired resolution. This model needs the evaluation of several posterior edge detection thresholds.	edge detection;signal reconstruction	Mila Nikolova	1995		10.1109/ICASSP.1995.480087	gaussian noise;signal reconstruction;fourier transform;mathematical optimization;edge detection;bayesian probability;pattern recognition;lattice;gaussian process;mathematics;maximum likelihood;markov process;markov model;estimation theory;statistics	Vision	52.46040576285663	-73.87060873978179	172804
23dd618f9b797023064e6eaa798889a5b28cac5a	a stochastic approach to content adaptive digital image watermarking	noise visibility function;serveur institutionnel;archive institutionnelle;open access;content adaptation;digital image watermarking;archive ouverte unige;cybertheses;institutional repository	"""This paper presents a new stochastic approach which can be applied with di erent watermark techniques. The approach is based on the computation of a Noise Visibility Function (NVF) that characterizes the local image properties, identifying textured and edge regions where the mark should be more strongly embedded. We present precise formulas for the NVF which enable a fast computation during the watermark encoding and decoding process. In order to determine the optimal NVF, we rst consider the watermark as noise. Using a classical MAP image denoising approach, we show how to estimate the """"noise"""". This leads to a general formulation for a texture masking function, that allows us to determine the optimal watermark locations and strength for the watermark embedding stage. We examine two such NVFs, based on either a non-stationary Gaussian model of the image, or a stationary Generalized Gaussian model. We show that the problem of the watermark estimation is equivalent to image denoising and derive content adaptive criteria. Results show that watermark visibility is noticeably decreased, while at the same time enhancing the energy of the watermark."""	computation;digital image;digital watermarking;embedded system;image noise;noise reduction;stationary process	Sviatoslav Voloshynovskiy;Alexander Herrigel;Nazanin Baumgaertner;Thierry Pun	1999		10.1007/10719724_16	computer vision;computer science;theoretical computer science;multimedia;watermark	AI	51.66019211793767	-69.76876104711432	172805
a87b32b915b9959f2cc69fb0c341038337ff0dbd	surface curvature line clustering for polyp detection in ct colonography	ct colonography;categories and subject descriptors according to acm ccs i 3 3 computer graphics picture image generation line and curve generation	Automatic polyp detection is a helpful addition to laborious visual inspection in CT colonography. Traditional detection methods are based on calculating image features at discrete positions on the colon wall. However large-scale surface shapes are not captured. This paper presents a novel approach to aggregate surface shape information for automatic polyp detection. The iso-surface of the colon wall can be partitioned into geometrically homogeneous regions based on clustering of curvature lines, using a spectral clustering algorithm and a symmetric line similarity measure. Each partition corresponds with the surface area that is covered by a single cluster. For each of the clusters, a number of features are calculated, based on the volumetric shape index and the surface curvedness, to select the surface partition corresponding to the cap of a polyp. We have applied our clustering approach to nine annotated patient datasets. Results show that the surface partition-based features are highly correlated with true polyp detections and can thus be used to reduce the number of false-positive detections.	aggregate data;algorithm;ct scan;cluster analysis;colon classification;eurographics;isosurface;optimizing compiler;sensor;similarity measure;spectral clustering;virtual colonoscopy;visual inspection	Lingxiao Zhao;Vincent Frans van Ravesteijn;Charl P. Botha;Roel Truyen;Frans Vos;Frits H. Post	2008		10.2312/VCBM/VCBM08/053-060	computer vision;mathematics;engineering drawing;computer graphics (images)	Vision	39.237289774127504	-74.3443149713442	172965
0a69b163c97f866a5dda307656eeebc562249a7c	an improved segmentation of high spatial resolution remote sensing image using marker-based watershed algorithm	geophysical image processing;image segmentation;image resolution;texture gradient;small region removal high spatial resolution remote sensing image segmentation marker based watershed algorithm image oversegmentation reduction image preprocessing image postprocessing redundant minimal region merging watershed transform original image texture gradient extraction gray level cooccurrence matrix gradient image fusion;image fusion;matrix algebra;image texture;feature extraction;remote sensing;watershed segmentation;transforms;merging;transforms feature extraction geophysical image processing image fusion image resolution image segmentation image texture matrix algebra remote sensing;texture gradient image analysis watershed segmentation;image analysis	This study presents a novel approach to reduce over-segmentation using both pre- and post-processing for watershed segmentation. We make use of more prior knowledge in pre-processing and merge the redundant minimal regions in post-processing. In the initial stage of the watershed transform, this not only produces a gradient image from the original image, but also introduces the texture gradient. The texture gradient can be extracted using a gray-level co-occurrence matrix. Then, both gradient images are fused to give the final gradient image. After the initial results of segmentation, we use the merging region technique to remove small regions. Experiments show the effectiveness of segmentation.	algorithm;co-occurrence matrix;document-term matrix;experiment;gradient;preprocessor;video post-processing;watershed (image processing)	Boren Li;Mao Pan;Zixing Wu	2012	2012 20th International Conference on Geoinformatics	10.1109/Geoinformatics.2012.6270304	image texture;computer vision;range segmentation;watershed;image gradient;geography;morphological gradient;segmentation-based object categorization;pattern recognition;region growing;image segmentation;scale-space segmentation;remote sensing	Vision	45.85582169169637	-67.90009006890627	173168
fcdb508a033ac94533543cfcf678052025201704	hippin: a semiautomatic computer program for selecting hip prosthesis femoral components	analisis imagen;orthopedic surgery;computer program;radiodiagnostic;analisis numerico;informatica biomedical;base donnee;biomedical data processing;image processing;x ray imaging;genie biomedical;prothese;systeme aide decision;membre inferieur;edge detection;protesis;hip;cirugia ortopedica;informatique biomedicale;database;base dato;hombre;chirurgie orthopedique;radiografia;sistema ayuda decision;analyse numerique;radiography;deteccion contorno;detection contour;radiodiagnostico;decision support system;numerical analysis;biomedical engineering;hanche;cadera;human;computer aid;hip prosthesis;lower limb;asistencia ordenador;image analysis;evaluation;ingenieria biomedica;evaluacion;radiodiagnosis;computer aided preoperation planning;prosthesis;analyse image;assistance ordinateur;radiographie;preoperation planning;miembro inferior;homme	This work illustrates a computer program designed to aid surgeons in selecting the hip prosthesis femoral component during the preoperation planning stage of hip replacement surgery. Starting from the processing of the patient's coxo-femoral region X-ray image, the program, called Hippin, interacts with the user to outline the femoral region, including the head and the inner contour of the proximal femur. It automatically examines all possible couplings with the patient's femur outlines from a database containing the outlines of the available prostheses created by digitizing the templates normally used in preoperation planning. The resulting images enable the surgeon to visually compare all the alternatives. In addition, the program provides numerical values for the distances between the physiological rotation and prosthesis centers, helping the surgeon in selecting from among the possibilities. The program has been validated by comparing the computer results with actual surgeon selections.		Domenico Bongini;Monica Carfagni;Lapo Governi	2000	Computer methods and programs in biomedicine	10.1016/S0169-2607(00)00078-X	computer vision;image analysis;radiography;edge detection;decision support system;image processing;orthopedic surgery;numerical analysis;computer science;evaluation;surgery	ML	46.09455670988933	-79.49797758632091	173175
d22e3f48f43ea3486447ac9b2210023f030d88e0	the effectiveness of geometry features on multi-resolution diffeomorphic demons registration in the implementation of human cortex surface parcellation	fast automated cerebral cortex labeling;subject surface;anatomical features;brain;mean dice metric;measurement;neurophysiology biomedical mri brain computational geometry image registration medical image processing;mean dice metric geometry feature effectiveness multiresolution diffeomorphic demons registration human cortex surface parcellation fast automated cerebral cortex labeling human cerebral cortex automated pipeline brain morphology analysis spherical diffeomorphic demons registration atlas surface subject surface atlas subject correspondence multiresolution scheme surface registration procedure anatomical features;atlas subject correspondence;geometry;computational geometry;surface roughness;surface registration procedure;multiresolution scheme;surface morphology;multiresolution diffeomorphic demons registration;atlas surface;surface treatment;human cerebral cortex;surface parcellation;medical image processing;brain morphology analysis;image registration;automated labeling;similarity index human cerebral cortex surface parcellation spherical demons registration automated labeling;cerebral cortex;human cortex surface parcellation;humans;neurophysiology;multi resolution;surface treatment geometry humans surface morphology measurement labeling surface roughness;automated pipeline;spherical demons registration;geometry feature effectiveness;labeling;similarity index;biomedical mri;spherical diffeomorphic demons registration	Fast automated labeling of the human cerebral cortex remains a challenging problem. We have implemented a completely automated pipeline to analyze brain morphology. For labeling of the cerebral cortex, a spherical diffeomorphic demons registration is used to drive an atlas surface into correspondence with the subject surface. This study focuses on identifying features or combination of features that will provide optimal correspondence between the atlas and subject. A multi-resolution scheme is used for the surface registration procedure. The roles of anatomical features in multi-resolution surface registration have been evaluated by using them across all resolutions of the registration procedure as well as combining them from low to high anatomical detail corresponding to the registration refinement. We found that combining features from low to high anatomical features provided the best automated labeling of the surface. The mean Dice metric for the four lobar regions studied was 0.85 using the combined features.	iterative closest point;mathematical morphology;pipeline (computing);refinement (computing)	Wen Li;Luis Ibáñez;Nancy Andreasen;Vincent Magnotta	2011	2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2011.5872475	computer vision;labeling theory;surface roughness;computational geometry;computer science;image registration;artificial intelligence;mathematics;geometry;neurophysiology;measurement	Vision	41.71533699688506	-79.11399562428093	173486
3a25a08781b90903fb7affa815d037f8e5db2d8c	rsem: an accelerated algorithm on repeated em	random swap em;image segmentation;image segmentation rsem random swap em accelerated algorithm repeated em expectation maximization gradient ascent algorithm gmm estimation theory color texture images;color texture image segmentation;image texture expectation maximisation algorithm government policies gradient methods image colour analysis image segmentation;algorithm design and analysis indexes optimization maximum likelihood estimation graphics search problems;maximum likelihood estimation;image texture;gaussian mixture model expectation maximization random swap em color texture image segmentation;indexes;gaussian mixture model;maximum likelihood estimate;expectation maximization;image colour analysis;indexation;government policies;gradient methods;optimization;search problems;algorithm design;algorithm design and analysis;graphics;expectation maximisation algorithm	Expectation maximization (EM) algorithm, being a gradient ascent algorithm depends highly on the initialization. Repeating EM multiple times with different initial solutions and taking the best result is used to attack this problem. However, the solution space is searched inefficiently in Repeated EM, because after each restart it can take a long time to converge without any guarantee that it leads to an improved solution. A random swap EM algorithm utilizes random swap strategy to improve the problem in a more efficient way. In this paper, a theoretical and experimental comparison between RSEM and REM is conducted. Based on GMM estimation theory, it is proved that RSEM reaches the optimal result faster than REM with high probability. It is also shown experimentally that RSEM speeds up REM from 9\% to 63\%. A study in color-texture images demonstrates an application of EM algorithms in a segmentation task.	converge;estimation theory;expectation–maximization algorithm;experiment;feasible region;google map maker;gradient descent;paging;times ascent;with high probability	Qinpei Zhao;Ville Hautamäki;Pasi Fränti	2011	2011 Sixth International Conference on Image and Graphics	10.1109/ICIG.2011.110	public policy;algorithm design;computer vision;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;statistics	DB	49.16173962421862	-68.38305523723436	173675
043b14845c57abb02263701aa93f4c2070030e10	anisotropic filtering of non-linear surface features	computacion informatica;mean curvature flow;free boundary;grupo de excelencia;geometric feature;ciencias basicas y experimentales;mean curvature;noise removal	A new method for noise removal of arbitrary surfaces meshes is presented which focuses on the preservation and sharpening of non-linear geometric features such as curved surface regions and feature lines. Our method uses a prescribed mean curvature flow (PMC) for simplicial surfaces which is based on three new contributions: 1. the definition and efficient calculation of a discrete shape operator and principal curvature properties on simplicial surfaces that is fully consistent with the well-known discrete mean curvature formula, 2. an anisotropic discrete mean curvature vector that combines the advantages of the mean curvature normal with the special anisotropic behaviour along feature lines of a surface, and 3. an anisotropic prescribed mean curvature flow which converges to surfaces with an estimated mean curvature distribution and with preserved nonlinear features. Additionally, the PMC flow prevents boundary shrinkage at constrained and free boundary segments.	algorithm;anisotropic filtering;computation;nonlinear system;subdivision surface;whole earth 'lectronic link	Klaus Hildebrandt;Konrad Polthier	2004	Comput. Graph. Forum	10.1111/j.1467-8659.2004.00770.x	willmore energy;center of curvature;total curvature;mathematical optimization;scalar curvature;topology;principal curvature;mean curvature flow;mean curvature;mathematics;geometry;torsion of a curve;curvature;riemann curvature tensor;constant-mean-curvature surface;sectional curvature;curvature form;radius of curvature	Graphics	51.86647917229368	-71.28450190900587	173746
2a06abb00da44e4748b17e18308e39083b79c918	unsupervised fuzzy clustering for the segmentation and annotation of upwelling regions in sea surface temperature images	fuzzy k means;domain knowledge;fuzzy clustering;sea surface temperature;visual inspection;roc analysis;automatic annotation;number of clusters;coastal upwelling;spatial visualization;sequential extraction	The Anomalous Pattern algorithm is explored as an initialization strategy to the Fuzzy  K  -Means (FCM), with the sequential extraction of clusters, that simultaneously allows the determination of the number of clusters. The composed algorithm, Anomalous Pattern Fuzzy Clustering (AP-FCM), is applied in the segmentation of Sea Surface Temperature (SST) images for the identification of Coastal Upwelling.#R##N##R##N#A set of features are constructed from the AP-FCM clustering segmentation taking into account domain knowledge and a threshold procedure is defined in order to identify the transition cluster whose frontline is automatically annotated on SST images to separate the upwelling regions from the background.#R##N##R##N#Two independent data samples in a total of 61 SST images covering large diversity of upwelling situations are analysed. Results show that by tuning the AP-FCM stop conditions it fits a good number of clusters providing an effective segmentation of the SST images whose spatial visualization of fuzzy membership closely reproduces the original images. Comparing the AP-FCM with the FCM using several validation indices shows the advantage of the AP-FCM avoiding under or over-segmented images. Quantitative assessment of the segmentations is accomplished through ROC analysis. Compared to FCM, the number of iterations of the AP-FCM is significantly decreased.#R##N##R##N#The automatic annotation of upwelling frontlines from the AP-FCM segmentation overcomes the subjective visual inspection made by the Oceanographers.	fuzzy clustering	Susana Nascimento;Pedro Franco	2009		10.1007/978-3-642-04747-3_18	upwelling;computer vision;sea surface temperature;fuzzy clustering;computer science;artificial intelligence;machine learning;data mining;receiver operating characteristic;domain knowledge;visual inspection	Vision	40.3819778125508	-70.15563602598465	173854
6b740b32683043cf0cb17ad6733f9eea4c140cec	the metric spaces, euler equations, and normal geodesic image motions of computational anatomy	minimisation;equations geophysics computing anatomy shape biology computing joining processes usa councils extraterrestrial measurements focusing level set;image motion analysis;metric space;differential geometry;variational formulation;minimisation differential geometry image motion analysis vectors;vectors;point of view;vector field;vector field flow minimization euler equation normal geodesic image motion computational anatomy biological shape coordinatized anatomical manifold construction morphometric change inference anatomical shape comparison anatomical structure imagery metric mapping geodesics generation geodesic diffeomorphic flow variational geodesic formulation;euler equation	1. INTRODUCTION Over the past several years our group has been studying Biological shape in the emerging new discipline of Computational Anatomy (CA) [ 11. CA consists of several components: (i) the construction of coordinatized anatomical manifolds, (ii) comparison of anatomical manifolds, and (iii) inference of morphometric change on anatomical manifolds. In this paper we focus on (ii) the comparison of anatomical shapes and structures in imagery via metric mapping. Our group has been studying natural shapes in imagery using the diffeomorphisms [2, 3, 4, 1, 5, 61 the natural extension of the finite dimensional matrix groups. Shapes and imagery are studied as an orbit via the action of diffeomorphic transformation .	computation;computational anatomy;euler;morphometrics	Michael I. Miller;Alain Trouvé;Laurent Younes	2003		10.1109/ICIP.2003.1246760	differential geometry;minimisation;mathematical analysis;vector field;geodesic;topology;geodesic map;metric space;mathematics;geometry;euler equations	Vision	47.10813502925235	-76.09221803791772	173898
75f45c93fdc45c92ab8c1da1fb54e9d6b529a910	joint graph cut and relative fuzzy connectedness image segmentation algorithm	sensitivity and specificity;image segmentation;fuzzy logic;image enhancement;image interpretation computer assisted;graph cut;reproducibility of results;artificial intelligence;algorithms;robustness;pattern recognition automated;subtraction technique;fuzzy connectedness	"""We introduce an image segmentation algorithm, called GC(sum)(max), which combines, in novel manner, the strengths of two popular algorithms: Relative Fuzzy Connectedness (RFC) and (standard) Graph Cut (GC). We show, both theoretically and experimentally, that GC(sum)(max) preserves robustness of RFC with respect to the seed choice (thus, avoiding """"shrinking problem"""" of GC), while keeping GC's stronger control over the problem of """"leaking though poorly defined boundary segments."""" The analysis of GC(sum)(max) is greatly facilitated by our recent theoretical results that RFC can be described within the framework of Generalized GC (GGC) segmentation algorithms. In our implementation of GC(sum)(max) we use, as a subroutine, a version of RFC algorithm (based on Image Forest Transform) that runs (provably) in linear time with respect to the image size. This results in GC(sum)(max) running in a time close to linear. Experimental comparison of GC(sum)(max) to GC, an iterative version of RFC (IRFC), and power watershed (PW), based on a variety medical and non-medical images, indicates superior accuracy performance of GC(sum)(max) over these other methods, resulting in a rank ordering of GC(sum)(max)>PW∼IRFC>GC."""	algorithm;choose (action);cobham's thesis;combinatorial optimization;epilepsy, generalized;experiment;graph - visual representation;graph cuts in computer vision;image resolution;image segmentation;iterative method;kinetic data structure;laser therapy, low-level;mathematical optimization;mathematics;maximum flow problem;numerous;peripheral;physical object;plant seeds;relative value scales;requirement;robustness (computer science);subroutine;time complexity;watershed (image processing);biologic segmentation	Krzysztof Ciesielski;Paulo André Vechiatto Miranda;Alexandre X. Falcão;Jayaram K. Udupa	2013	Medical image analysis	10.1016/j.media.2013.06.006	fuzzy logic;computer vision;discrete mathematics;cut;computer science;theoretical computer science;machine learning;mathematics;image segmentation;algorithm;statistics;robustness	Vision	46.814720163344525	-72.8554166255824	173978
0f9b070fcd7a3dd336130640eb44d1a053429800	computer-aided detection and segmentation of objects on medical images.	medical image;computer aided detection			Tatjana P. Belikova;Roman M. Palenichka;Iryna B. Ivasenko	2002			computer vision;object-class detection;computer science	Vision	41.41255148251001	-71.66409897784725	173995
9173cd064c3488eef7e54d9b1207470460cd8e87	image registration based on boundary mapping	nuclear magnetic resonance imaging;biomedical nmr image registration medical image processing edge detection brain;medical imagery;brain;active contour;contour;edge detection;biomedical nmr;hombre;encefalo;magnetic resonance image;image registration polynomials biomedical imaging brain interpolation radiology active contours testing magnetic resonance linear regression;medical diagnostic imaging boundary mapping nonlinear brain image registration active contour algorithm homothetic one to one map two dimensional transformation elastic body deformation magnetic resonance images atlas images;algorithme;imageria rmn;algorithm;encephale;medical image processing;image registration;human;imagerie medicale;brain imaging;imagerie rmn;contorno;imageneria medical;homme;algoritmo;brain vertebrata	A new two-stage approach for nonlinear brain image registration is proposed. In the first stage, an active contour algorithm is used to establish a homothetic one-to-one map between a set of region boundaries in two images to be registered. This mapping is used in the second step: a two-dimensional transformation which is based on an elastic body deformation. This method is tested by registering magnetic resonance images to atlas images.	active contour model;algorithm;cervical atlas;image registration;nonlinear system;one-to-one (data model);resonance;stage level 1;registration - actclass	Christos Davatzikos;Jerry L. Prince;R. Nick Bryan	1996	IEEE transactions on medical imaging	10.1109/42.481446	computer vision;edge detection;radiology;computer science;image registration;magnetic resonance imaging;active contour model;nuclear medicine;neuroimaging;medical physics	Vision	45.41212054946468	-78.84876119345611	174268
8897030034cf5079c1d6f8f82ba14d1c724c6748	myocardium tracking via matching distributions	level sets;matching distributions;cardiac imaging;curve evolution;level set;prior information;satisfiability;magnetic resonance mr;bhattacharyya measure;evolution equation;euler lagrange;magnetic resonance;shape priors;myocardium tracking;similarity measure;cardiac magnetic resonance;quantitative evaluation	The goal of this study is to investigate automatic myocardium tracking in cardiac Magnetic Resonance (MR) sequences using global distribution matching via level-set curve evolution. Rather than relying on the pixelwise information as in existing approaches, distribution matching compares intensity distributions, and consequently, is well-suited to the myocardium tracking problem. Starting from a manual segmentation of the first frame, two curves are evolved in order to recover the endocardium (inner myocardium boundary) and the epicardium (outer myocardium boundary) in all the frames. For each curve, the evolution equation is sought following the maximization of a functional containing two terms: (1) a distribution matching term measuring the similarity between the non-parametric intensity distributions sampled from inside and outside the curve to the model distributions of the corresponding regions estimated from the previous frame; (2) a gradient term for smoothing the curve and biasing it toward high gradient of intensity. The Bhattacharyya coefficient is used as a similarity measure between distributions. The functional maximization is obtained by the Euler–Lagrange ascent equation of curve evolution, and efficiently implemented via level-set. The performance of the proposed distribution matching was quantitatively evaluated by comparisons with independent manual segmentations approved by an experienced cardiologist. The method was applied to ten 2D mid-cavity MR sequences corresponding to ten different subjects. Although neither shape prior knowledge nor curve coupling were used, quantitative evaluation demonstrated that the results were consistent with manual segmentations. The proposed method compares well with existing methods. The algorithm also yields a satisfying reproducibility. Distribution matching leads to a myocardium tracking which is more flexible and applicable than existing methods because the algorithm uses only the current data, i.e., does not require a training, and consequently, the solution is not bounded to some shape/intensity prior information learned from of a finite training set.	2d computer graphics;biasing;coefficient;computation;dental caries;endocardium;epicardium;euler;euler–lagrange equation;expectation–maximization algorithm;frame (physical object);gradient;jaccard index;matching;myocardium;resonance;sampling - surgical action;similarity measure;smoothing (statistical technique);test set;times ascent;quantitative	Ismail Ben Ayed;Shuo Li;Ian G. Ross;Ali Islam	2008	International Journal of Computer Assisted Radiology and Surgery	10.1007/s11548-008-0265-y	mathematical optimization;topology;level set;magnetic resonance imaging;mathematics;geometry	Vision	42.78049938478146	-79.80268700501142	174321
c9aec4c1604f24cef9cd4ee8e4dc8bf801d20675	automatic magnetic resonance spinal cord segmentation with topology constraints for variable fields of view	atlas construction;spinal cord;spinal cord segmentation;image processing computer assisted;digital homeomorphism;topology preserving segmentation;magnetic resonance imaging;algorithms;humans;atlases as topic	Spinal cord segmentation is an important step in the analysis of neurological diseases such as multiple sclerosis. Several studies have shown correlations between disease progression and metrics relating to spinal cord atrophy and shape changes. Current practices primarily involve segmenting the spinal cord manually or semi-automatically, which can be inconsistent and time-consuming for large datasets. An automatic method that segments the spinal cord and cerebrospinal fluid from magnetic resonance images is presented. The method uses a deformable atlas and topology constraints to produce results that are robust to noise and artifacts. The method is designed to be easily extended to new data with different modalities, resolutions, and fields of view. Validation was performed on two distinct datasets. The first consists of magnetization transfer-prepared T2*-weighted gradient-echo MRI centered only on the cervical vertebrae (C1-C5). The second consists of T1-weighted MRI that covers both the cervical and portions of the thoracic vertebrae (C1-T4). Results were found to be highly accurate in comparison to manual segmentations. A pilot study was carried out to demonstrate the potential utility of this new method for research and clinical studies of multiple sclerosis.	anatomic structures;anatomy, regional;artifact (software development);atlases;atrophic;body dysmorphic disorders;bone structure of spine;bone structure of thoracic vertebra;cerebrospinal fluid;cervical vertebrae;chest;color gradient;image noise;java programming language;magnetic resonance imaging;morphologic artifacts;multiple sclerosis;neck;network topology;progressive disease;robustness (computer science);segmentation action;semiconductor industry;spinal cord;algorithm;biologic segmentation;nervous system disorder;registration - actclass	Min Chen;Aaron Carass;Jiwon Oh;Govind Nair;Dzung L. Pham;Daniel S. Reich;Jerry L. Prince	2013	NeuroImage	10.1016/j.neuroimage.2013.07.060	radiology;medicine;pathology;magnetic resonance imaging;anatomy;surgery	Vision	41.67875040624753	-79.96853209225851	174419
969493ce2665ae05750008e2c73aaae1b7de9133	simultaneous geometric - iconic registration	automatic generation;markov random field;objective function	In this paper, we introduce a novel approach to bridge the gap between the landmark-based and the iconic-based voxel-wise registration methods. The registration problem is formulated with the use of Markov Random Field theory resulting in a discrete objective function consisting of thee parts. The first part of the energy accounts for the iconic-based volumetric registration problem while the second one for establishing geometrically meaningful correspondences by optimizing over a set of automatically generated mutually salient candidate pairs of points. The last part of the energy penalizes locally the difference between the dense deformation field due to the iconic-based registration and the implied displacements due to the obtained correspondences. Promising results in real MR brain data demonstrate the potentials of our approach.	loss function;markov chain;markov random field;optimization problem;voxel;registration - actclass	Aristeidis Sotiras;Yangming Ou;Ben Glocker;Christos Davatzikos;Nikos Paragios	2010	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-15745-5_83	computer vision;computer science;artificial intelligence;mathematics	Vision	45.107970304895865	-75.67187359518232	174766
0956fca5266b2afb9603a330539fefe88b7d4a84	detecting fish in underwater video using the em algorithm	akaike information criterion fish detection underwater video em algorithm multivariate gaussian gaussian mixture modeling parameter estimation;akaike information criterion;video signal processing;gaussian processes;parameter estimation zoology video signal processing gaussian processes;gaussian mixture model;zoology;underwater tracking marine animals sea measurements monitoring shape parameter estimation stress distortion measurement optical imaging cameras;parameter estimation;em algorithm	This study is an attempt to simulate aspects of human visual perception by automating the detection of specific types of objects in digital images. The success of the methods attempted here was measured by how well results of experiments corresponded to what a typical human’s assessment of the data might be. The subject of the study was images of live fish taken underwater by digital video or digital still cameras. It is desirable to be able to automate the processing of such data for efficient stock assessment for fisheries management. In this study some well known statistical pattern classification techniques were tested and new syntactical/structural pattern recognition techniques were developed. For testing of statistical pattern classification, the pixels belonging to fish were separated from the background pixels and the EM algorithm for Gaussian mixture models was used to locate clusters of pixels. The means and the covariance matrices for the components of the model were used to indicate the location, size and shape of the clusters. Because the number of components in the mixture is unknown, the EM algorithm has to be run a number of times with different numbers of components and then the best model chosen using a model selection criterion. The AIC (Akaike Information Criterion) and the MDL (Minimum Description Length) were tested. The MDL was found to estimate the numbers of clusters of pixels more accurately than the AIC, which tended to overestimate cluster numbers. In order to reduce problems caused by initialisation of the EM algorithm (i.e. starting positions of mixtures and number of mixtures), the Dynamic Cluster Finding algorithm (DCF) was developed (based on the Dog-Rabbit strategy). This algorithm can produce an estimate of the locations and numbers of clusters of pixels. The Dog-Rabbit strategy is based on early studies of learning behaviour in neurons. The main difference between Dog-Rabbit and DCF is that DCF is based on a toroidal topology which removes the tendency of cluster locators to migrate to the centre of mass of the data set and miss clusters near the edges of the image. In the second approach to the problem, data was extracted from the image using an edge detector. The edges from a reference object were compared with the edges from a new image to determine if the object occurred in the new image. In order to compare edges, the edge pixels were first assembled into curves using an UpWrite procedure; then the curves were smoothed by fitting parametric cubic polynomials. Finally the curves were converted to arrays of numbers which represented the signed curvature of the curves at regular intervals. Sets of curves from different images can be compared by comparing the arrays of signed curvature values, as well as the relative orientations and locations of the curves. Discrepancy values were calculated to indicate how well curves and sets of curves matched the reference object. The total length of all matched curves was used to indicate what fraction of the reference object was found in the new image. The curve matching procedure gave results which corresponded well with what a human being being might observe.	akaike information criterion;computational anatomy;cubic function;design rule for camera file system;digital image;digital video;discrepancy function;edge detection;expectation–maximization algorithm;experiment;matched filter;minimum description length;mixture model;model selection;pixel;polynomial;sensor;simulation;smoothing;statistical classification;structural pattern;syntactic pattern recognition;toroidal graph	Fiona F. Evans	2003		10.1109/ICIP.2003.1247423	econometrics;akaike information criterion;expectation–maximization algorithm;pattern recognition;mixture model;gaussian process;mathematics;estimation theory;statistics	ML	49.57687961621932	-68.87419505038136	174878
38b2a8c8f802b1e8553bfee4e10a53618fc57a6b	hierarchical statistical shape analysis and prediction of sub-cortical brain structures	partial least square regression;statistical shape analysis;shape modelling;morphometry;mr imaging;canonical correlation analysis;registration;brain structure;prediction accuracy;multivariate statistics;model fitting;correlation coefficient;prediction	In this paper, we present the application of two multivariate statistical techniques to investigate how different structures within the brain vary statistically relative to each other. The first of these techniques is canonical correlation analysis which extracts and quantifies correlated behaviour between two sets of vector variables. The second technique is partial least squares regression which determines the best factors within a first set of vector variables for predicting a vector variable from a second set. We describe how these techniques can be used to quantify and predict correlated behaviour in sub-cortical structures within the brain using 3D MR images.	partial least squares regression;statistical shape analysis	Anil Rao;Paul Aljabar;Daniel Rueckert	2006	2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)	10.1016/j.media.2007.06.006	generalized least squares;simple linear regression;econometrics;multivariate statistics;canonical correlation;canonical analysis;prediction;morphometrics;pattern recognition;mathematics;partial least squares regression;coefficient of determination;statistics	Vision	43.79785579299215	-79.4615113111573	174901
7c848cfa05081e18c8328c74e1ed5b663a98d366	blending textured images using a non-parametric multiscale mrf method	high resolution;weighted averaging;texture synthesis;markov random field;conditional probability	"""In this paper we describe a new method for improving the representation of textures in blends of multiple images based on a Markov Random Field (MRF) algorithm. We show that direct application of an MRF texture synthesis algorithm across a set of images is unable to capture both the """"averageness"""" of the global image appearance as well as specific textural components. To overcome this problem we vary the width of the Parzen window (used to smooth the conditional probability distribution of the pixel's intensity) as a function of scale, thus making lower pyramid resolutions closer to the Gaussian mean, while maintaining the high resolution textures. We also show that approximating the maxima of the conditional probability distributions with a weighted-average produces very similar results with a significant increase in speed."""	algorithm;alpha compositing;image resolution;kernel density estimation;markov chain;markov random field;maxima;pixel;texture synthesis;window function	Bernard Tiddeman	2004			computer vision;image resolution;conditional probability;computer science;machine learning;pattern recognition;mathematics;texture synthesis;statistics	Vision	51.09993513159643	-69.29941242374164	174966
78eda6c41ed4ee4917900f28b37bf2d0f3e7da47	binary image filtering under noise distribution constraints	image filtering;binary image;weighted order statistic;conditional expectation;data storage;modeling	The most general filtering methods for binary images are binary increasing or non-increasing filters. The amount of memory used by both methods during training rises exponentially with the window size. Also, both methods need training sets whose size rises exponentially with the number of the window elements. This limits the size of the window to maximum 5 by 5 pixels. In this paper we search for a suboptimal filter which can be implemented for larger window sizes. Applying certain constraints to the noise distribution, the conditional expectation is decomposed into noise dependent and original image dependent components. The resulting filter can be trained with the original image to learn the ideal patterns, while the noise properties can be extracted by both modeling or training on a reasonable training set. The amount of memory used by the new filter is proportional to a finite power of window size. It is shown that the new filter is a generalization of weighted order statistic filter.© (1996) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	binary image	Octavian Valeriu Sarca;Jaakko Astola	1996		10.1117/12.235838	machine learning;pattern recognition;mathematics;statistics	Vision	50.50802529904771	-67.1123438484869	175404
02b9b06a54a32c1d0b734e54c183cd1cfb04a424	a joint acquisition-reconstruction paradigm for correcting inhomogeneity artifacts in mr echo planar imaging	image reconstruction silicon carbide nonhomogeneous media joints estimation signal to noise ratio;silicon carbide;magnetic field;models theoretical;blind source separation;joints;nonhomogeneous media;echo planar imaging humans models theoretical;mmse estimation joint acquisition reconstruction paradigm inhomogeneity artifacts correction mr echo planar imaging signal degradation rapid mr acquisitions epi magnetic field variations field inhomogeneities susceptibility gradients spatially varying field blind image deconvolution problem multiimage acquisition strategy minimum mean squared error;medical image processing biomedical mri blind source separation deconvolution image reconstruction;estimation;image acquisition;spatial distribution;image reconstruction;medical image processing;blind image deconvolution;deconvolution;echo planar imaging;humans;reconstruction algorithm;signal to noise ratio;minimum mean square error;biomedical mri	One of the main sources of signal degradation in rapid MR acquisitions, such as Echo Planar Imaging (EPI), is magnetic field variations caused by field inhomogeneities and susceptibility gradients. If unaccounted for during the reconstruction process, this spatially-varying field can cause severe image artifacts. In this paper, we show that correcting for the resulting degradations can be formulated as a blind image deconvolution problem. We propose a novel joint acquisition-processing paradigm to solve this problem. We describe a practical implementation of this paradigm using a multi-image acquisition strategy and a corresponding joint estimation-reconstruction algorithm. The estimation step computes the spatial distribution of the field maps, while the reconstruction step yields a Minimum Mean Squared Error (MMSE) estimate of the imaged slice. Our simulations show that this proposed joint acquisition-reconstruction method is robust and efficient, offering factors of improvement in the quality of the reconstructed image as compared to other traditional methods.	algorithm;artifact (software development);deconvolution;echo-planar imaging;elegant degradation;estimation theory;feasible region;gradient;magnetic fields;magnetic resonance imaging;map;mean squared error;morphologic artifacts;motion estimation;newton's method;numerous;programming paradigm;sensorineural hearing loss (disorder);signal-to-noise ratio;simplified instructional computer;simulation;standard industrial classification;visual artifact;fmri	Joseph C. Dagher;François G. Meyer	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090638	iterative reconstruction;minimum mean square error;computer vision;estimation;electronic engineering;magnetic field;computer science;deconvolution;mathematics;blind signal separation;nuclear magnetic resonance;signal-to-noise ratio;statistics	Vision	52.14075604957972	-77.73921533164015	175415
082f089fa6847fd816a42fbc9f4c3a8d8dc2f7ea	image segmentation using an efficient rotationally invariant 3d region-based hidden markov model	optimisation;brain mri data;image segmentation;lattices;tree structured parameter estimation algorithm;clinical magnetic resonance imaging;hidden markov model;optimization process;parameter estimation biomedical mri expectation maximisation algorithm hidden markov models image segmentation medical image processing optimisation;grid based hmm framework;clinical magnetic resonance imaging data;3d region based hidden markov model;image segmentation hidden markov models magnetic resonance imaging shape lattices parameter estimation labeling brain modeling solid modeling optimization methods;rectangular lattice;hidden markov random field;magnetic resonance image;three dimensional;locally optimal data labeling;brain modeling;computational modeling;hidden markov models;shape;expectation maximization;object rotation;three dimensional displays;image representation;medical image processing;gray matter segmentation;magnetic resonance imaging;3d hmm estimation unsupervised image segmentation image representation rectangular lattice optimization process tree structured parameter estimation algorithm locally optimal data labeling clinical magnetic resonance imaging gray matter segmentation expectation maximization hidden markov random field 3d region based hidden markov model optimal data labeling object rotation synthetic image clinical magnetic resonance imaging data geometric shape data grid based hmm framework brain mri data white matter segmentation;solid modeling;tree structure;gray matter;white matter segmentation;parameter estimation;geometric shape data;expectation maximization hidden markov random field;algorithm design and analysis;labeling;synthetic image;optimal data labeling;unsupervised image segmentation;biomedical mri;3d hmm estimation;optimization methods;expectation maximisation algorithm	We present a novel three dimensional (3D) region-based hidden Markov model (rbHMM) for unsupervised image segmentation. Our contributions are twofold. First, our rbHMM employs a more efficient representation of the image than approaches based on a rectangular lattice or grid; thus, resulting in a faster optimization process. Second, our proposed novel tree-structured parameter estimation algorithm for the rbHMM provides a locally optimal data labeling that is invariant to object rotation. We demonstrate the advantages of our segmentation technique by validating on synthetic images of geometric shapes as well as both simulated and clinical magnetic resonance imaging (MRI) data of the brain. For the geometric shape data, we show that our method produces more accurate results in less time than a grid-based HMM framework using a similar optimization strategy. For the brain MRI data, our white and gray matter segmentation results in substantially greater accuracy than both block-based 3D HMM estimation and expectation-maximization hidden Markov random field (HMRF-EM) approaches.	algorithmic efficiency;computation;estimation theory;expectation–maximization algorithm;geometric modeling;hidden markov model;hidden markov random field;image segmentation;local optimum;markov chain;mathematical optimization;resonance;synthetic intelligence;viterbi algorithm	Albert Huang;Rafeef Abugharbieh;Roger C. Tam	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4563014	three-dimensional space;algorithm design;computer vision;labeling theory;expectation–maximization algorithm;shape;computer science;magnetic resonance imaging;machine learning;pattern recognition;lattice;mathematics;image segmentation;tree structure;solid modeling;estimation theory;scale-space segmentation;computational model;hidden markov model	Vision	44.77137558813068	-76.83557557821008	175487
04b7b8d975f19a45698312c45b94a4505107d80c	multi-scale mesh saliency with local adaptive patches for viewpoint selection	saliency;graphs;patches;3d meshes	Our visual attention is attracted by specific areas into 3D objects (represented by meshes). This visual attention depends on the degree of saliency exposed by these areas. In this paper, we propose a novel multi-scale approach for detecting salient regions. To do so, we define a local surface descriptor based on patches of adaptive size and filled in with a local height field. The single-scale saliency of a vertex is defined as its degree measure in the mesh with edges weights computed from adaptive patch similarities weighted by the local curvature. Finally, the multi-scale saliency is defined as the average of single-scale saliencies weighted by their respective entropies. The contribution of the multi-scale aspect is analyzed and showed through the different results. The strength and the stability of our approach with respect to noise and simplification are also studied. Our approach is compared to the state-of-the-art and presents competitive results.	entropy (information theory);heightmap;level of detail;noise (electronics);sensor	Anass Nouri;Christophe Charrier;Olivier Lézoray	2015	Sig. Proc.: Image Comm.	10.1016/j.image.2015.08.002	computer vision;topology;salience;mathematics;geometry;graph	Vision	46.08362940071796	-68.13105566158282	175627
59777e28caeb71851400d158bd3158ee5b2800db	image segmentation based on constrained spectral variance difference and edge penalty	constrained spectral variance difference;edge penalty;multi scale;remote sensing image segmentation;remote sensing;region merging	Segmentation, which is usually the first step in object-based image analysis (OBIA), greatly influences the quality of final OBIA results. In many existing multi-scale segmentation algorithms, a common problem is that under-segmentation and over-segmentation always coexist at any scale. To address this issue, we propose a new method that integrates the newly developed constrained spectral variance difference (CSVD) and the edge penalty (EP). First, initial segments are produced by a fast scan. Second, the generated segments are merged via a global mutual best-fitting strategy using the CSVD and EP as merging criteria. Finally, very small objects are merged with their nearest neighbors to eliminate the remaining noise. A series of experiments based on three sets of remote sensing images, each with different spatial resolutions, were conducted to evaluate the effectiveness of the proposed method. Both visual and quantitative assessments were performed, and the results show that large objects were better preserved as integral entities while small objects were also still effectively delineated. The results were also found to be superior to those from eCongnition’s multi-scale segmentation.	algorithm;coexist (image);edge enhancement;entity;expectation propagation;experiment;image analysis;image segmentation;object-based language	Bo Chen;Fang Qiu;Bingfang Wu;Hongyue Du	2015	Remote Sensing	10.3390/rs70505980	computer vision;mathematical optimization;segmentation-based object categorization;scale-space segmentation;physics;remote sensing	Vision	47.12870950891621	-69.06235100338483	175942
697289b74242bed86a5f3154ca7cebaa9a416351	new iterative algorithms for thinning binary images	object recognition;parallel skeletonization pattern recognition;binary image;edge detection;iterative algorithm;skeleton;iterative methods;shape;image edge detection;object recognition edge detection image thinning iterative methods;pixel;classification algorithms;transforms;parallel;pattern recognition;erosion condition iterative algorithms binary image thinning digital image machine recognition object recognition image analysis edge detection edge subtraction;pixel image edge detection classification algorithms skeleton algorithm design and analysis transforms shape;image analysis;skeletonization;digital image;image thinning;algorithm design and analysis	Thinning is a process of reducing an object in a digital image to the minimum size necessary for machine recognition of that object. Efficient and reliable thinning of image patterns is essential to a variety of applications in the field of image analysis and recognition system. In this paper we propose two new iterative algorithms for thinning binary images. In the first algorithm, thinning is accomplished by using two operations: edge detection and subtraction. Another algorithm is based on repeatedly conditionally eroding the pixels until a one pixel thick pattern is obtained. Erosion conditions are devised to assure preserving connectivity. Results of applying the algorithms on the variety of images will be shown. To judge performance of our algorithms, we made a comparison with some other major algorithm.	algorithm;binary image;digital image;edge detection;graphics;image analysis;iteration;iterative method;medical imaging;pixel;straight skeleton;thinning	G. V. Padole;S. B. Pokle	2010	2010 3rd International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2010.173	computer vision;computer science;machine learning;pattern recognition	Vision	45.161034353358836	-66.4536533572749	175966
02d9064704f32870bfe46d682f29cddfdac94792	minimizing sparse higher order energy functions of discrete variables	higher order functions;random variables;pixel;minimisation;graph theory;message passing;machine learning;graph cut;labeling;cost function;higher order;computer vision;image texture;image restoration	Higher order energy functions have the ability to encode high level structural dependencies between pixels, which have been shown to be extremely powerful for image labeling problems. Their use, however, is severely hampered in practice by the intractable complexity of representing and minimizing such functions. We observed that higher order functions encountered in computer vision are very often “sparse”, i.e. many labelings of a higher order clique are equally unlikely and hence have the same high cost. In this paper, we address the problem of minimizing such sparse higher order energy functions. Our method works by transforming the problem into an equivalent quadratic function minimization problem. The resulting quadratic function can be minimized using popular message passing or graph cut based algorithms for MAP inference. Although this is primarily a theoretical paper, it also shows how higher order functions can be used to obtain impressive results for the binary texture restoration problem.	algorithm;circuit restoration;cut (graph theory);encode;graph cuts in computer vision;high-level programming language;higher-order function;loss function;mathematical optimization;message passing;np-hardness;pixel;quadratic function;sparse matrix	Carsten Rother;Pushmeet Kohli;Wei Feng;Jiaya Jia	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206739	image texture;random variable;image restoration;computer vision;minimisation;mathematical optimization;labeling theory;message passing;higher-order logic;cut;computer science;graph theory;theoretical computer science;machine learning;mathematics;geometry;higher-order function;pixel	Vision	53.100666179925575	-70.59139844009277	176167
a37f9ebb88f480200e48b96139b3205aae568ba9	semi-automated region of interest generation for the analysis of optically recorded neuronal activity	animals;time varying;rats;binary image;low pass filter;signal extraction;neonatal rat;brain mapping;single cell;image interpretation computer assisted;brain stem;region of interest;rats sprague dawley;prebotc;optical recording;microscopy fluorescence;pattern recognition automated;ca 2 indicator;neurons;signal to noise ratio;action potentials;neuronal activity	Bath-applied membrane-permeant Ca(2+) indicators offer access to network function with single-cell resolution. A barrier to wider and more efficient use of this technique is the difficulty of extracting fluorescence signals from the active constituents of the network under study. Here we present a method for semi-automatic region of interest (ROI) detection that exploits the spatially compact, slowly time-varying character of the somatic signals that these indicators typically produce. First, the image series is differenced to eliminate static and very slowly varying fluorescence values, and then the differenced image series undergoes low-pass filtering in the spatial domain, to eliminate temporally isolated fluctuations in brightness. This processed image series is then thresholded so that pixel regions of fluctuating brightness are set to white, while all other regions are set to black. Binary images are averaged, and then subjected to iterative thresholding to extract ROIs associated with both dim and bright cells. The original image series is then analyzed using the generated ROIs, after which the end-user rejects spurious signals. These methods are applied to respiratory networks in the neonate rat tilted sagittal slab preparation, and to simulations with signal-to-noise ratios ranging between 1.0-0.2. Simulations established that algorithm performance degraded gracefully with increasing noise. Because signal extraction is the necessary first step in the analysis of time-varying Ca(2+) signals, semi-automated ROI detection frees the researcher to focus on the next step: selecting traces of interest from the relatively complete set generated using these methods.	algorithm;calcium;computer simulation;degraded mode;diploid cell;fluorescence;infant, newborn;iterative method;pixel;region of interest;sagittal plane;semiconductor industry;series - set of composite instances;signal-to-noise ratio;slab allocation;tissue membrane;tracing (software);transfer function;brightness	Nicholas M. Mellen;Chi-Minh Tuong	2009	NeuroImage	10.1016/j.neuroimage.2009.04.016	psychology;computer vision;neuroscience;binary image;low-pass filter;computer science;brain mapping;communication;signal-to-noise ratio;action potential;premovement neuronal activity;region of interest	Vision	39.530583005938055	-73.27983934509453	176248
d0c64ad1551d8aeec67b405a95834303c9a9ab95	detection of abnormalities on carotid angiograms using syntactic techniques	vaisseau sanguin encephale pathologie;systeme nerveux pathologie;ateroesclerosis;genie biomedical;hombre;atherosclerosis;analyse syntaxique;nervous system diseases;cerebrovascular disease;biomedical engineering;analisis sintaxico;syntactic analysis;sistema nervioso patologia;stenose;human;pattern recognition;estenosis;ingenieria biomedica;reconnaissance forme;carotid bifurcation;carotidographie;reconocimiento patron;angiografia carotidea;vaso sanguineo encefalo patologia;carotid angiography;bifurcation carotidienne;bifurcacion de la carotida;atherosclerose;homme;stenosis	Abstract   Syntactic pattern recognition techniques are used to classify a small number of normal and abnormal bifurcations in carotid angiograms. The approach seems successful but necessitates the development of a sophisticated preprocessor if it is to be of practical value.		Tim Ritchings;Alan C. F. Colchester	1986	Pattern Recognition Letters	10.1016/0167-8655(86)90058-9	stroke;computer science;parsing;pattern recognition	Vision	45.995975218250386	-79.03763888090351	176275
e8cd07f4215195ae753a46650eb613d5a7003c42	dense 2d displacement reconstruction from spamm-mri with constrained elastic splines: implementation and validation	displacement vector field;vector field;thin plate spline	Abstrac t . Efficient constrained thin-plate spline warps are proposed in this paper which can warp an area in the plane such that two embedded snake grids obtained from two SPAMM frames are brought into registration, interpolating a dense displacement vector field. The reconstructed vector field adheres to the known displacement information at the intersections, forces corresponding snakes to be warped into one another, and for all other points in the myocardium, where no information is available, a C 1 continuous vector field is interpolated. The formalism proposed in this paper improves on our previous variational-based implementation and generalizes warp methods to include biologically relevant contiguous open curves, in addition to standard landmark points. The method has been extensively validated with a cardiac motion simulator, in addition to in-vivo tagging data sets.	displacement mapping;embedded system;interpolation;landmark point;motion simulator;semantics (computer science);simulation;spline (mathematics);thin plate spline;variational principle;video-in video-out	Amir A. Amini;Yasheng Chen;Jean Sun;Vaidy Mani	1998		10.1007/BFb0056199	mathematical optimization;vector field;topology;computer science;mathematics;geometry;thin plate spline	Vision	47.40307109429178	-75.71380226195691	176289
fd0d8eaa261015ba42ced098598033bc13549a56	multivariate mixture model for cardiac segmentation from multi-sequence mri		Cardiac segmentation is commonly a prerequisite for functional analysis of the heart, such as to identify and quantify the infarcts and edema from the normal myocardium using the late-enhanced (LE) and T2-weighted MRI. The automatic delineation of myocardium is however challenging due to the heterogeneous intensity distributions and indistinct boundaries in the images. In this work, we present a multivariate mixture model (MvMM) for text classification, which combines the complementary information from multi-sequence (MS) cardiac MRI and perform the segmentation of them simultaneously. The expectation maximization (EM) method is adopted to estimate the segmentation and model parameters from the log-likelihood (LL) of the mixture model, where a probabilistic atlas is used for initialization. Furthermore, to correct the intra- and inter-image misalignments, we formulate the MvMM with transformations, which are embedded into the LL framework and thus can be optimized by the iterative conditional mode approach. We applied MvMM for segmentation of eighteen subjects with three sequences and obtained promising results. We compared with two conventional methods, and the improvements of segmentation performance on LE and T2 MRI were evident and statistically significant by MvMM.	mixture model	Xiahai Zhuang	2016		10.1007/978-3-319-46723-8_67	computer vision;pattern recognition;mixture model;computer science;artificial intelligence;initialization;expectation–maximization algorithm;probabilistic logic;functional analysis;multivariate statistics;segmentation	Vision	43.18657128100015	-78.10703927931552	176495
3060e48068fcf7950e05e1a8845b3b31cb8060a2	topology adaptive deformable surfaces for medical image volume segmentation	nuclear magnetic resonance imaging;image tridimensionnelle;topology;medical imagery;modele geometrique;algorithms brain humans magnetic resonance angiography models neurological surface properties;image segmentation;image processing;systeme nerveux central;analyse surface;medical diagnostic imaging topology adaptive deformable surfaces medical image volume segmentation model based medical image analysis technique affine cell image decomposition efficient reparameterization mechanism t surfaces complex anatomic structures;implementation;deformability;procesamiento imagen;hombre;segmentation;encefalo;indexing terms;systeme adaptatif;traitement image;topology biomedical imaging image segmentation deformable models shape image analysis image decomposition geometry surface reconstruction image reconstruction;imageria rmn;medical image analysis;ejecucion;reconstruction image;sistema nervioso central;circulatory system;medical image;encephale;reconstruccion imagen;image reconstruction;medical image processing;analisis superficie;deformabilite;deformabilidad;human;adaptive system;imagerie medicale;sistema adaptativo;tridimensional image;imagerie rmn;imageneria medical;image decomposition;appareil circulatoire;deformable model;aparato circulatorio;segmentacion;central nervous system;imagen tridimensional;topology modelling image segmentation medical image processing;geometrical model;homme;surface analysis;brain vertebrata;modelo geometrico	Deformable models, which include deformable contours (the popular snakes) and deformable surfaces, are a powerful model-based medical image analysis technique. The authors develop a new class of deformable models by formulating deformable surfaces in terms of an affine cell image decomposition (ACID). The authors' approach significantly extends standard deformable surfaces, while retaining their interactivity and other desirable properties. In particular, the ACID induces an efficient reparameterization mechanism that enables parametric deformable surfaces to evolve into complex geometries, even modifying their topology as necessary. The authors demonstrate that their new ACID-based deformable surfaces, dubbed T-surfaces, can effectively segment complex anatomic structures from medical volume images.	acid;anatomic structures;anatomy, regional;image analysis;interactivity;medical image computing;biologic segmentation	Tim McInerney;Demetri Terzopoulos	1999	IEEE Transactions on Medical Imaging	10.1109/42.811261	iterative reconstruction;computer vision;index term;image processing;computer science;artificial intelligence;adaptive system;central nervous system;surface weather analysis;circulatory system;image segmentation;implementation;segmentation;computer graphics (images)	Vision	45.82862465851778	-78.24565563869537	176737
d01bbd847581ee090a6915ef3f77e78584794f8e	integrated surface model optimization for freehand three-dimensional echocardiography	bayesian framework;pixel appearance probability model integrated surface model optimization freehand three dimensional echocardiography ultrasound image quality surface finding algorithms decent quality boundaries satisfactory surface models low level image evidence high level prior shape knowledge pixel class prediction mechanism unreliable edge detection image segmentation problem pixel correspondence problem pixel feature vector smoothed grayscale value ground truth surfaces projection distance error endocardial surface projection distance errors medical diagnostic imaging;optimisation;probability;image segmentation;edge detection;bayes methods;correspondence problem;posterior probability;indexing terms;surface reconstruction;directional derivative;left ventricle;three dimensional;edge detection optimisation echocardiography bayes methods vectors probability medical image processing;optimization problem;feature vector;ultrasound imaging;vectors;medical image processing;surface model;echocardiography;image analysis;ground truth;algorithms bayes theorem echocardiography three dimensional humans image processing computer assisted probability ventricular function left;three dimensional echocardiography;probability model;echocardiography surface reconstruction pixel predictive models shape probability ultrasonic imaging image quality computer vision inference algorithms;image shape analysis	The major obstacle of three-dimensional (3-D) echocardiography is that the ultrasound image quality is too low to reliably detect features locally. Almost all available surface-finding algorithms depend on decent quality boundaries to get satisfactory surface models. We formulate the surface model optimization problem in a Bayesian framework, such that the inference made about a surface model is based on the integration of both the low-level image evidence and the high-level prior shape knowledge through a pixel class prediction mechanism. We model the probability of pixel classes instead of making explicit decisions about them. Therefore, we avoid the unreliable edge detection or image segmentation problem and the pixel correspondence problem. An optimal surface model best explains the observed images such that the posterior probability of the surface model for the observed images is maximized. The pixel feature vector as the image evidence includes several parameters such as the smoothed grayscale value and the minimal second directional derivative. Statistically, we describe the feature vector by the pixel appearance probability model obtained by a nonparametric optimal quantization technique. Qualitatively, we display the imaging plane intersections of the optimized surface models together with those of the ground-truth surfaces reconstructed from manual delineations. Quantitatively, we measure the projection distance error between the optimized and the ground-truth surfaces. In our experiment, we use 20 studies to obtain the probability models offline. The prior shape knowledge is represented by a catalog of 86 left ventricle surface models. In another set of 25 test studies, the average epicardial and endocardial surface projection distance errors are 3.2 /spl plusmn/ 0.85 mm and 2.6 /spl plusmn/ 0.78 mm, respectively.	adobe freehand;algorithm;class;correspondence problem;directional derivative;echocardiography;echocardiography, three-dimensional;edge detection;endocardium;feature vector;grayscale color map;ground sample distance;heart ventricle;high- and low-level;image quality;image segmentation;inference;left ventricular structure;mathematical optimization;online and offline;optimization problem;pixel;smoothing (statistical technique)	Mingzhou Song;Robert M. Haralick;Florence H. Sheehan;Richard K. Johnson	2002	IEEE Transactions on Medical Imaging	10.1109/TMI.2002.804433	optimization problem;three-dimensional space;computer vision;image analysis;edge detection;index term;feature vector;surface reconstruction;ground truth;computer science;directional derivative;machine learning;pattern recognition;probability;mathematics;image segmentation;posterior probability;correspondence problem	Vision	43.86751717632205	-77.22186713375407	177003
a52de9b04e548412da15d8349520399207450181	bessel fourier orientation reconstruction: an analytical eap reconstruction using multiple shell acquisitions in diffusion mri		The estimation of the ensemble average propagator (EAP) directly from q-space DWI signals is an open problem in diffusion MRI. Diffusion spectrum imaging (DSI) is one common technique to compute the EAP directly from the diffusion signal, but it is burdened by the large sampling required. Recently, several analytical EAP reconstruction schemes for multiple q-shell acquisitions have been proposed. One, in particular, is Diffusion Propagator Imaging (DPI) which is based on the Laplace's equation estimation of diffusion signal for each shell acquisition. Viewed intuitively in terms of the heat equation, the DPI solution is obtained when the heat distribution between temperatuere measurements at each shell is at steady state. We propose a generalized extension of DPI, Bessel Fourier Orientation Reconstruction (BFOR), whose solution is based on heat equation estimation of the diffusion signal for each shell acquisition. That is, the heat distribution between shell measurements is no longer at steady state. In addition to being analytical, the BFOR solution also includes an intrinsic exponential smootheing term. We illustrate the effectiveness of the proposed method by showing results on both synthetic and real MR datasets.	bessel filter;diffusion spectrum imaging;diffusion weighted imaging;propagator;sampling (signal processing);spectroscopy, fourier transform infrared;steady state;synthetic intelligence;exponential	Ameer Pasha Hosseinbor;Moo K. Chung;Yu-Chien Wu;Andrew L. Alexander	2011	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-23629-7_27	mathematical optimization;mathematics	Vision	49.29320461305459	-79.74549724384624	177037
e7a42a1be5850d5e8e799961bb59fa6a6596a073	contour tracking based on marginalized likelihood ratios	image features;active contour;generic model;marginal likelihood;kalman filter;shape deformation;expectation maximization;particle filter;generative model;em algorithm	When fitting contour models to image data, it is necessary to take into account unmodelled shape variability. Traditionally, this has been done either by blurring the input image or by looking for image features in the neighborhood of the contour. A more statistically rigorous approach is to marginalize over all possible shape deformations. When this is done, the resulting likelihood model has similarities to both the blurring approach and the feature-based approach. A tracking application is used to demonstrate the marginalized likelihood model and compare it to the blurring approach. The best tracking results were obtained with the new model when combined with the Expectation-Maximization (EM) algorithm.	contour line	Arthur E. C. Pece	2006	Image Vision Comput.	10.1016/j.imavis.2005.07.024	computer vision;expectation–maximization algorithm;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	49.716663121158476	-69.51709681532544	177240
99476774a39edf628b6c86e47eabdd55d03ba727	fast k-space sample selection in mrsi with a limited region of support	traitement signal;tiempo espera;evaluation performance;optimisation;sample selection;spectrometrie rmn;rapid technique;tecnica rapida;spectroscopy magnetic resonance imaging image sampling spatial resolution image resolution image reconstruction hardware heart myocardium lipidomics;espace k;performance evaluation;optimizacion;real time;evaluacion prestacion;echantillonnage;selective sampling;indexing terms;nmr spectroscopy;temps attente;sampling;algorithme;algorithm;reconstruction image;image acquisition;reconstruccion imagen;waiting time;image reconstruction;algorithms humans image processing computer assisted magnetic resonance imaging magnetic resonance spectroscopy;signal processing;nmr spectrometry;temps reel;acquisition;technique rapide;0 5 min fast k space sample selection limited region of support mrsi magnetic resonance spectroscopic imaging phase encoding steps number sampling density selection criterion evaluation shift pattern optimization imaging hardware real time image acquisition setting selective sampling medical diagnostic imaging;tiempo real;magnetic resonance spectroscopic imaging;optimization;espectrometria rmn;muestreo;procesamiento senal;biomedical mri nmr spectroscopy;adquisicion;biomedical mri;algoritmo	One of the primary drawbacks in the application of magnetic resonance spectroscopic imaging is the long acquisition times required to obtained the desired resolution. When region of support information is available, the number of phase-encoding steps and thus time can be reduced without loss of information if the k-space locations are chosen well. The authors propose to select locations using a rectangular sampling array that is shifted to various positions in k-space to obtain the necessary sampling density. This method allows multiple samples to be selected simultaneously and reduces the computation required to evaluate the selection criterion. The authors present an efficient forward selection algorithm for optimizing the shift pattern so that the image can be reconstructed as reliably as possible from a periodic nonuniform set of samples. The proposed algorithm has important practical potential in that it can finish the selection in less than half a minute for typical image sizes and can reconstruct the image with fewer samples than regular sampling. With appropriate imaging hardware, this new algorithm makes selective sampling possible in a real-time image acquisition setting.	computation;real-time clock;sampling (signal processing);sampling - surgical action;selection algorithm;stepwise regression;magnetic resonance spectroscopic imaging	Yun Gao;Stanley J. Reeves	2001	IEEE Transactions on Medical Imaging	10.1109/42.952725	iterative reconstruction;sampling;computer vision;index term;radiology;computer science;signal processing;mathematics;magnetic resonance spectroscopic imaging;nuclear magnetic resonance	Vision	52.82945957329945	-79.03241709527974	177412
cc35439b772c425a42fbd4e7a3e8ac2d98f19db4	an efficient and robust hybrid method for segmentation of zebrafish objects from bright-field microscope images		Accurate segmentation of zebrafish from bright-field microscope images is crucial to many applications in the life sciences. Early zebrafish stages are used, and in these stages the zebrafish is partially transparent. This transparency leads to edge ambiguity as is typically seen in the larval stages. Therefore, segmentation of zebrafish objects from images is a challenging task in computational bio-imaging. Popular computational methods fail to segment the relevant edges, which subsequently results in inaccurate measurements and evaluations. Here we present a hybrid method to accomplish accurate and efficient segmentation of zebrafish specimens from bright-field microscope images. We employ the mean shift algorithm to augment the colour representation in the images. This improves the discrimination of the specimen to the background and provides a segmentation candidate retaining the overall shape of the zebrafish. A distance-regularised level set function is initialised from this segmentation candidate and fed to an improved level set method, such that we can obtain another segmentation candidate which preserves the explicit contour of the object. The two candidates are fused using heuristics, and the hybrid result is refined to represent the contour of the zebrafish specimen. We have applied the proposed method on two typical datasets. From experiments, we conclude that the proposed hybrid method improves both efficiency and accuracy of the segmentation of the zebrafish specimen. The results are going to be used for high-throughput applications with zebrafish.	algorithm;approximation;artificial neural network;biological specimen;british informatics olympiad;computation;convolutional neural network;experiment;heuristic (computer science);high-throughput computing;image processing;mean shift;refinement (computing);region of interest;single-instance storage;throughput;tomography	Yuanhao Guo;Zhan Xiong;Fons J. Verbeek	2018	Machine Vision and Applications	10.1007/s00138-018-0934-y	pattern recognition;computer science;computer vision;zebrafish;artificial intelligence;transparency (graphic);segmentation;bright-field microscopy;heuristics;level set method;mean-shift	Vision	39.971994830851635	-73.11279816910775	177417
3f5be81ddd2b20ca39387beb88e129f876f5b19a	shape-based regularization of electron tomographic reconstruction	image segmentation;image resolution;phantoms;dna directed rna polymerases;image segmentation electron tomographic reconstruction method shape based regularization technique shape information spatial model digital phantoms simulated data experimental electron tomography data et data virus complexes reconstructed volume spike boundaries viral membranes et reconstructions feature visualization image deblurring;reconstruction;bayes theorem;bayesian methods;models biological;image restoration;image processing computer assisted;hiv 1;biomembranes;electron microscope tomography;engineering and technology;electron microscopy;teknik och teknologier;tomography bayesian methods electron microscopy reconstruction shape based regularization;shape;image reconstruction;medical image processing;probability distribution;phantoms imaging;optimization;humans;head;tomography biomembranes cellular biophysics image reconstruction image resolution image restoration image segmentation medical image processing microorganisms phantoms;computer simulation;tomography;microorganisms;cellular biophysics;noise;shape based regularization;data models;image reconstruction shape tomography probability distribution noise data models optimization	We introduce a tomographic reconstruction method implemented using a shape-based regularization technique. Spatial models of known features in the structure being reconstructed are integrated into the reconstruction process as regularizers. Our regularization scheme is driven locally through shape information obtained from segmentation and compared with a known spatial model. We demonstrated our method on tomography data from digital phantoms, simulated data, and experimental electron tomography (ET) data of virus complexes. Our reconstruction showed reduced blurring and an improvement in the resolution of the reconstructed volume was also measured. This method also produced improved demarcation of spike boundaries in viral membranes when compared with popular techniques like weighted back projection and the algebraic reconstruction technique. Improved ET reconstructions will provide better structure elucidation and improved feature visualization, which can aid in solving key biological issues. Our method can also be generalized to other tomographic modalities.	acquired immunodeficiency syndrome;algebraic reconstruction technique;algorithm;bead dosage form;blurred vision;convergence (action);demarcation point;distortion (economics);drug discovery;electron tomography;experiment;feature detection (web development);hiv;image scaling;imagery;line search;linear algebra;mandibular right second molar tooth;matrix regularization;phantom reference;phantoms, imaging;rover (the prisoner);spectral density;test scaling;the spike (1997);tissue membrane;tomographic reconstruction;biologic segmentation	Ajay Gopinath;Guoliang Xu;David Ress;Ozan Öktem;Sriram Subramaniam;Chandrajit L. Bajaj	2012	IEEE Transactions on Medical Imaging	10.1109/TMI.2012.2214229	iterative reconstruction;computer simulation;probability distribution;image restoration;data modeling;computer vision;radiology;image resolution;bayesian probability;shape;computer science;noise;mathematics;tomography;image segmentation;microorganism;optics;bayes' theorem;head;tomographic reconstruction;electron microscope;statistics;medical physics	Vision	52.64824502384441	-77.27966725549909	177419
