id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
0ed114563ebef0f800e2db1d7f8acebc0640a53a	automatic surveillance and analysis of snow and ice coverage on electrical insulators of power transmission lines	insulator ice detection;power line;cross correlation;real time;electric insulator;histogram;power transmission line;detection rate;image analysis;economic loss;voltage collapse;insulator image analysis;electric power;insulator swing angle;boundary detection;insulator snow detection	One of the large problems for electrical power delivery through power lines in the Northern countries is when snow or ice accumulates on electrical insulators. This could lead to snow or ice-induced outages and voltage collapse, causing huge economic loss. This paper proposes a novel real-time automatic surveillance and image analysis system for detecting and estimating the snow and ice coverage on electric insulators using images captured from an outdoor 420 kV power transmission line. In addition, the swing angle of insulators is estimated, as large swing angles due to wind cause short circuits. Hybrid techniques by combining histogram, edges, boundaries and cross-correlations are employed for handling a broad range of scenarios caused by changing weather and lighting conditions. Experiments have been conducted on the captured images over several month periods. Results have shown that the proposed system has provided valuable estimation results. For image pixels related to snows on the insulator, the current system has yielded an average detection rate of 93% for good quality images, and 67.6% for images containing large amount of poor quality ones, and the corresponding average false alarm ranges from 9% to 18.1%. Further improvement may be achieved by using video-based analysis and improved camera settings.	transmission line	Irene Y. H. Gu;Unai Sistiaga;Sonja M. Berlijn;Anders Fahlström	2008		10.1007/978-3-642-02345-3_36	insulator;image analysis;electric power;telecommunications;computer science;cross-correlation;histogram;mathematics;statistics	Vision	42.04519519173572	-42.43755984561674	48289
0e75b0be734caf4291d6c4bb2b70a1412df4a564	3-d facial pose and gaze point estimation using a robust real-time tracking paradigm	face recognition;real-time systems;tracking;3d facial pose;feature positions;feature tracking;gaze point estimation;head rotation;monocular camera;multiple triplet triangulation;real-time video stream;robust real-time tracking paradigm;visually directed human-machine interface	Facial pose and gaze point are fundamental to any visually directed human-machine interface. In this paper, we propose a system capable of tracking a face and estimating the 3-D pose and the gaze point all in a real-time video stream of the head. This is done by using a 3-D model together with multiple triplet triangulation of feature po sitions assuming an affine projection. Using feature-based tracking the calculation of a 3-D eye gaze direction vector is possible even with head rotation and using a monocular camera. The system is also able to automatically initialise the feature tracking and to recover from total tracking fail ures which can occur when a person becomes occluded or temporarily leaves the image.	3d modeling;angularjs;emoticon;eye tracking;motion estimation;operating system;paradigm;real-time clock;real-time computing;real-time locating system;streaming media;the australian;triplet state;user interface;vxworks	Jochen Heinzmann;Alexander Zelinsky	1998			facial recognition system;human–machine interface;computer vision;simulation;tracking system;eye tracking;computer science;point estimation;video tracking;tracking;computer graphics (images)	Vision	48.753291149724525	-43.82225969896875	48569
efed30bfff2ebad11841ef2b15492d6069363bd5	recovery of circular motion geometry in spite of varying intrinsic parameters	tensile stress;conference_paper;videoconference;computational geometry;computer vision;cameras image sequences computational geometry lenses tracking rendering computer graphics computer vision calibration tensile stress videoconference;image sequence;lenses;rendering computer graphics;calibration;cameras;tracking;image sequences	Previous algorithms for recovering 3D geometry from uncalibrated circular motion image sequences of unknown rotation angles are mostly for constant intrinsic parameters. In this paper, a new and simple method for recovering circular motion geometry in spite of varying intrinsic parameters is proposed. It is shown that the movement of the camera forms two concentric circles on the motion plane. By identifying the concentric conic loci in 3D projective frame, the geometry of circular motion can be recovered. Compared with existing rotation angle recovery methods, the new method is (i) more flexible in that it allows the intrinsic parameters to vary from image to image; (ii) simpler by avoiding the calculation of the intersection points of two conics which is notoriously complicated. Experimental results for real images are provided to show the performance of the proposed method.	algorithm;camera resectioning;entity	Yong Li;Yeung Sam Hung	2006	2006 IEEE International Conference on Video and Signal Based Surveillance	10.1109/AVSS.2006.97	computer vision;calibration;simulation;computational geometry;computer science;lens;mathematics;tracking;stress;videoconferencing;motion field;computer graphics (images)	Robotics	53.47831725033592	-50.2589492755237	48660
4458bb1e5da65cd2b993de71f67d17c7c0e01546	learning to detect pointing gestures from wearable imus		We propose a learning-based system for detecting when a user performs a pointing gesture, using data acquired from IMU sensors, by means of a 1D convolutional neural network. We quantitatively evaluate the resulting detection accuracy, and discuss an application to a human-robot interaction task where pointing gestures are used to guide a quadrotor landing.	artificial neural network;convolutional neural network;human–robot interaction;sensor	Denis Broggini;Boris Gromov;Alessandro Giusti;Luca Maria Gambardella	2018			artificial intelligence;computer vision;computer science;machine learning;wearable computer;gesture	HCI	46.8660951929595	-40.6548031712777	48754
0775798bdfb44db28790a2ef9e6022c04862690d	human fall detection by mean shift combined with depth connected components	mean shift;high effectiveness;consistent color;unobtrusive fall detection;fall detection;depth image sequence;reliable fall detection;indoor environment;depth image;component algorithm;human fall detection	Depth is very useful cue to achieve reliable fall detection since humans may not have consistent color and texture but must occupy an integrated region in space. In this work we demonstrate how depth images are extracted by low-cost Kinect device. The person undergoing monitoring is extracted through mean-shift clustering. The fall alarm is triggered on the basis of the distance of the person's gravity center to the altitude at which the Kinect is placed. The experimental results indicate high effectiveness of fall detection in indoor environments and low computational overhead of the algorithm. Human Fall Detection by Mean Shift Combined with Depth Connected Components	algorithm;cluster analysis;computation;connected component (graph theory);kinect;mean shift;overhead (computing)	Michal Kepski;Bogdan Kwolek	2012		10.1007/978-3-642-33564-8_55	computer vision;simulation	Robotics	40.71536714351641	-43.646493623276555	48764
78f930c28d93051df0b14db4e205cabdca0a5634	detecting repeated motion patterns via dynamic programming using motion density	dynamic programming;polynomial time repeated motion patterns detecting dynamic programming motion density motion sequence intelligent systems human activity combinatorial optimization;density measurement;time measurement;repeated motion patterns detecting;path planning;repeated measures;motion detection dynamic programming humans object detection intelligent systems density measurement motion measurement time measurement optimization methods polynomials;combinatorial optimization problem;contextual information;dynamic program;polynomials;motion sequence;motion segmentation;trajectory;shape;time series analysis;computational complexity;discrete cosine transforms;principal component analysis;robots;intelligent systems;intelligent system;polynomial time;pattern recognition;humans;combinatorial optimization;motion measurement;motion density;motion detection;human activity;object detection;robots computational complexity dynamic programming path planning pattern recognition;optimization methods	In this paper, we propose a method that detects repeated motion patterns in a long motion sequence efficiently. Repeated motion patterns are the structured information that can be obtained without knowledge of the context of motions. They can be used as a seed to find causal relationships between motions or to obtain contextual information of human activity, which is useful for intelligent systems that support human activity in everyday environment. The major contribution of the proposed method is two-fold: (1) motion density is proposed as a repeatability measure and (2) the problem of finding consecutive time frames with large motion density is formulated as a combinatorial optimization problem which is solved via Dynamic Programming (DP) in polynomial time O(N logN) where N is the total amount of data. The proposed method was evaluated by detecting repeated interactions between objects in everyday manipulation tasks and outperformed the previous method in terms of both detectability and computational time.	algorithm;causality;combinatorial optimization;computation;dynamic programming;energy minimization;interaction;mathematical optimization;optimization problem;polynomial;repeatability;sensor;time complexity;time series	Koichi Ogawara;Yasufumi Tanabe;Ryo Kurazume;Tsutomu Hasegawa	2009	2009 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2009.5152643	robot;time complexity;computer vision;mathematical optimization;repeated measures design;simulation;combinatorial optimization;shape;computer science;artificial intelligence;trajectory;dynamic programming;time series;motion estimation;mathematics;motion planning;computational complexity theory;polynomial;time;principal component analysis	Robotics	48.55692428649488	-47.6766604456676	48776
ada56c9ceef50aa5159f1f8aa45ca2040d1ed15c	soft biometrics: globally coherent solutions for hair segmentation and style recognition based on hierarchical mrfs	face recognition image classification image segmentation inference mechanisms markov processes object recognition;image segmentation;human recognition in the wild facial hair labeling facial hair segmentation biological data complementary layer working adjacent image patches hierarchical architecture multilayered architecture inference phase computer vision problems hierarchical markov random fields hierarchical mrf style recognition hair segmentation soft biometric labels;biometrics access control;shape;soft biometrics visual surveillance homeland security;three dimensional displays;image color analysis;head;hair image color analysis head shape image segmentation biometrics access control three dimensional displays;hair	Markov Random Fields (MRFs) are a popular tool in many computer vision problems and faithfully model a broad range of local dependencies. However, rooted in the Hammersley–Clifford theorem, they face serious difficulties in enforcing the global coherence of the solutions without using too high order cliques that reduce the computational effectiveness of the inference phase. Having this problem in mind, we describe a multi-layered (hierarchical) architecture for MRFs that is based exclusively in pairwise connections and typically produces globally coherent solutions, with 1) one layer working at the local (pixel) level, modeling the interactions between adjacent image patches; and 2) a complementary layer working at the object (hypothesis) level pushing toward globally consistent solutions. During optimization, both layers interact into an equilibrium state that not only segments the data, but also classifies it. The proposed MRF architecture is particularly suitable for problems that deal with biological data (e.g., biometrics), where the reasonability of the solutions can be objectively measured. As test case, we considered the problem of hair / facial hair segmentation and labeling, which are soft biometric labels useful for human recognition in-the-wild. We observed performance levels close to the state-of-the-art at a much lower computational cost, both in the segmentation and classification (labeling) tasks.	algorithmic efficiency;biometric device;clique (graph theory);coherence (physics);coherent;computation;computational complexity theory;computer vision;converge;experiment;hammersley–clifford theorem;interaction;markov chain;markov random field;mathematical optimization;mind;pixel;reference frame (video);soft biometrics;test case	Hugo Proen&#x00E7;a;João C. Neves	2017	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2017.2680246	computer science;pattern recognition;artificial intelligence;architecture;computer vision;random field;biological data;scale-space segmentation;image segmentation;pairwise comparison;machine learning;soft biometrics;markov chain	Vision	45.287305022960815	-51.44542688302165	48824
7541302679a893b653707e53a6541b5ca8e948b4	camera motion estimation using particle filters	particle filter	In this paper a novel algorithm for estimating the parametric form of the camera motion is proposed. A novel stochastic vector field model is presented which can handle smooth motion patterns derived from long periods of stable camera movement and also can cope with rapid motion changes and periods where camera remains still. A set of rules for robust and online updating of the model parameters is also proposed, based on the Expectation Maximization algorithm. Finally, we fit this model in a particle filters framework, in order to predict the future camera motion based on current and prior knowledge.	expectation–maximization algorithm;motion estimation;online algorithm;particle filter	Symeon Nikitidis;Stefanos P. Zafeiriou;Ioannis Pitas	2008			probability vector;artificial intelligence;vector field;computer vision;computer science;particle filter;expectation–maximization algorithm;camera auto-calibration;motion estimation;camera matrix;camera resectioning	Vision	48.28884659193301	-47.16343354501205	48937
a4b910a292141f6ac53452c1e5df7eb802fb4837	real-time range imaging for dynamic scenes using colour-edge based structured light	real time;structured light;data acquisition system;dynamic range layout cameras colored noise optical reflection acoustic reflection optical imaging lungs computed tomography man machine systems;image colour analysis;range image;pattern recognition;pattern recognition real time systems data acquisition gesture recognition image colour analysis;off the shelf;data acquisition;gesture recognition;3d gesture recognition real time range imaging dynamic scenes colour edge based structured light real time 3d data acquisition system single projection pattern pattern recognition low cost system off the shelf components experimental results;dynamic scenes;real time systems	A novel real time 3D-data acquisition system for highly dynamic scenes is presented. It is based on colour coded structured light using a single projection pattern. Existing systems utilising this approach are usually limited to scenes with neutral surfaces or not very robust. The presented system overcomes these shortcomings by using an innovative approach for recognising the projected pattern based on colour edges. The low-cost system is integrated using off-the-shelf components only. Experimental results are presented including a report on the successful use of the system with a 30 gesture recognition application. -	color vision;data acquisition;gesture recognition;neutral spine;range imaging;real-time clock;requirement;structured light;video projector	Frank Forster;Manfred K. Lang;Bernd Radig	2002		10.1109/ICPR.2002.1048021	computer vision;computer science;gesture recognition;data acquisition;computer graphics (images)	Vision	45.31509792556673	-43.41589774990091	49172
46633fb632b9cf5a4e5f7d059425148a12f58f36	an online hierarchical supervoxel segmentation algorithm based on uniform entropy slice	image segmentation;approximation algorithms;object segmentation;streaming media;spatiotemporal phenomena;clustering algorithms;entropy	Supervoxel plays a significant role in video segmentation task in which moving objects in the video are detected and their external boundaries are obtained. This is due to the observation that hierarchies of supervoxel can process the video into a multiscale decomposition with more information for later analysis than other approaches using the concept of supervoxels to video segmentation. Most available supervoxel video segmentation methods require the whole voxels in the video to be loaded into memory first before processing can occur which is impossible even for medium sized videos containing 100 frames, of reasonable complex background and foreground of moving objects. The results of deploying these more traditional approaches show that they tend to under-segment at lower levels and over-segment at high levels, so it is a challenge to use the results to analyze the video, e.g., in detection of objects. In this paper, we present an algorithm to overcome these limitations. Our method, called StreamUES, is based on the Uniform Entropy Slice (UES). It not only can seek a selection of supervoxels by using the post hoc feature criterion such as optical flow, but also can work on streaming modem use a clip of frames at one time until the end of the video clip is reached. Our results indicate the StreamUES can work very well with a challenging dataset viz., VSB 100, which we used to evaluate the proposed StreamUES technique.	algorithm;earthbound;hoc (programming language);mathematical optimization;modem;optical flow;ues (cipher);video clip;viz: the computer game;voxel	Cheng Peng;Sio-Long Lo	2016	2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2016.7852674	computer vision;entropy;simulation;computer science;machine learning;video tracking;image segmentation;cluster analysis;scale-space segmentation;approximation algorithm	Vision	40.24907850428569	-51.52804583686749	49195
dfc4a3a2b148860f079601d9cc1b79bd163372cc	automated fall detection using computer vision		The population of elderly people is increasing day-by-day in the world. One of the major health issues of an old person is injury during a fall and this issue becomes compounded for elderly people living alone. In this paper, we propose a novel framework for automated fall detection of a person from videos. Background subtraction is used to detect the moving person in the video. Different features are extracted by applying rectangle and ellipse on human shape to detect the fall of a person. Experiments have been carried out on the UR Fall Dataset which is publicly available. The proposed method is compared with existing methods and significantly better results are achieved.	computer vision	Pramod Kumar Soni;Ayesha Choudhary	2018		10.1007/978-3-030-04021-5_20	computer vision;artificial intelligence;computer science;rectangle;ellipse;background subtraction;population	Vision	39.894952963884556	-44.93603058113123	49296
0eb52f7685d5d4243e52fb72db55e9d813abd7cd	suppression of detection ghosts in homography based pedestrian detection	automatic video surveillance ghost detection suppression homography based pedestrian detection multicamera pedestrian detection multicamera object detection background subtraction surveillance scenes people detection people tracking;ghosts surveillance detection homography false detection;reliability;video surveillance;false detection;surveillance;detection;image sensors;accuracy;shape;homography;object tracking;video surveillance image sensors object detection object tracking;cameras tracking surveillance reliability object detection shape accuracy;ghosts;cameras;tracking;object detection	"""One popular approach for multi-camera detection of pedestrians or other objects of interest in surveillance scenes is to perform background subtraction and project the resulting foreground mask images to a common scene plane using homographies. As the complexity of the scene increases, it is unavoidable that so called """"ghost"""" detections should occur. These are false positives, indicating the presence of an object of interest where no such object actually exists. This paper proposes an approach to predicting where these ghost detections will occur, and provides a mechanism for suppressing their appearance."""	background subtraction;ghost;homography (computer vision);object detection;pedestrian detection;sensor;zero suppression	Murray Evans;Longzhen Li;James M. Ferryman	2012	2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance	10.1109/AVSS.2012.73	computer vision;homography;shape;video tracking;image sensor;reliability;accuracy and precision;tracking;computer graphics (images)	Vision	44.56800755593054	-45.822268032465296	49431
73d09b5c3f452d1f6dc84fa0555dc966c3086a6b	human action recognition in video data using invariant characteristic vectors	image recognition;image motion analysis;video signal processing;video signal processing cameras image motion analysis image recognition;projective depth action recognition;vectors cameras elbow humans databases character recognition watches;motion capture data human action recognition invariant characteristic vectors video data arbitrary homography epipolar geometry camera orientation;cameras	We introduce the concept of the “characteristic vector” as an invariant vector associated with a set of freely moving points relative to a plane. We show that if the motion of two sets of points differ only up to a similarity transformation, then the elements of the characteristic vector differ up to scale regardless of viewing directions and cameras. Furthermore, this invariant vector is given by any arbitrary homography that is consistent with epipolar geometry. The characteristic vector of moving points can thus be used to recognize the transitions of a set of points in an articulated body during the course of an action regardless of the camera orientation and parameters. Our extensive experimental results on both motion capture data and real data indicates very good performance.	epipolar geometry;homography (computer vision);motion capture	Nazim Ashraf;Hassan Foroosh	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467127	computer vision;computer science;motion estimation;mathematics;geometry;epipolar geometry;computer graphics (images)	Vision	48.60137337672912	-45.94501697851128	49778
f39f65bf064084e4626d6860f0e5713ba83dde60	autonomous visual exploration of complex objects	complex objects;optimisation;optimisation robot vision object representation set membership models knowledge representation stochastic models ellipsoidal calculus motion estimation approximation;optimisation robot vision image representation knowledge representation motion estimation stochastic processes approximation theory;layout stochastic processes cameras uncertainty robot vision systems knowledge representation calculus motion estimation context modeling state estimation;motion estimation;process integration;approximation theory;robot vision;stochastic processes;image representation;knowledge representation;efficient estimation;large classes	Whatever the application, a main issue for robots is to model the environment. In this article, we present a suitable object knowledge representation, based on a mixture of stochastic and set membership models. We consider that, for a large class of applications, an approximated representation of objects is suficient to build a preliminary map of the scene. Our approximation mainly results in ellipsoidal calculus by means of a normal assumption for stochastic laws and ellipsoidal Over or inner bounding for uniform laws. These approximations allow us to build an efficient estimation process integrating visual data online. Based on this estimation scheme, we peijonn online and optimal exploratory motions for the camera.	approximation algorithm;autonomous robot;knowledge representation and reasoning;stochastic process	Grégory Flandin;François Chaumette	2001		10.1109/IROS.2001.977197	knowledge representation and reasoning;stochastic process;computer vision;mathematical optimization;computer science;artificial intelligence;machine learning;motion estimation;mathematics;process integration;approximation theory	AI	50.24559082362894	-49.971781623842915	49846
95e35ac9f097290e8637a8439ebb40bb9ba4eeed	stereo odometry based on careful feature selection and tracking	visual odometry;kalman filters;accuracy;visualization;stereo image processing distance measurement embedded systems feature selection image filtering kalman filters;estimation;three dimensional displays;feature extraction;stereo vision;odroid u3 arm based embedded computer feature selection feature tracking stereo visual odometry soft drift reduction rotation estimation translation estimation kitti leaderboard imu aided version embedded systems outlier rejection kalman filter rotation refinement;cameras;estimation cameras visualization three dimensional displays kalman filters feature extraction accuracy	In this paper we present a novel algorithm for fast and robust stereo visual odometry based on feature selection and tracking (SOFT). The reduction of drift is based on careful selection of a subset of stable features and their tracking through the frames. Rotation and translation between two consecutive poses are estimated separately. The five point method is used for rotation estimation, whereas the three point method is used for estimating translation. Experimental results show that the proposed algorithm has an average pose error of 1.03% with processing speed above 10 Hz. According to publicly available KITTI leaderboard, SOFT outperforms all other validated methods. We also present a modified IMU-aided version of the algorithm, fast and suitable for embedded systems. This algorithm employs an IMU for outlier rejection and Kalman filter for rotation refinement. Experiments show that the IMU based system runs at 20 Hz on an ODROID U3 ARM-based embedded computer without any hardware acceleration. Integration of all components is described and experimental results are presented.	arm architecture;algorithm;cross-validation (statistics);embedded system;feature selection;hardware acceleration;kalman filter;odroid-c2;refinement (computing);rejection sampling;visual odometry	Igor Cvisic;Ivan Petrovic	2015	2015 European Conference on Mobile Robots (ECMR)	10.1109/ECMR.2015.7324219	kalman filter;computer vision;estimation;simulation;visualization;feature extraction;computer science;stereopsis;visual odometry;pattern recognition;accuracy and precision;statistics	Robotics	53.30807026579752	-41.46776017665097	49862
50d79afba0a195dbc39238c6b4fc6e4765a61f39	a comparison of modified evolutionary computation algorithms with applications to three-dimensional endoscopic camera motion tracking		Endoscope 3D motion tracking plays an irreplaceable role for computer-assisted endoscopy systems development. Without such tracking, it is impossible to synchronize pre- and intraoperative images in a reference coordinate frame. Currently available methods are comprised of video-based and electromagnetic tracking. These methods limit to either video image artifacts or inaccurate sensor measurements and dynamic errors. This paper proposes two modified evolutionary computation algorithms: (a) adaptive particle swarm optimization (APSO) and (b) observation-boosted differential evolution (OBDE), to augment current endoscopic camera motion tracking. The experimental results demonstrate that our modified algorithms, which combine endoscopic video images with sensor measurements to estimate endoscope movements, can improve tracking accuracy from 4.8 mm to 2.9 mm. OBDE outperforms APSO for endoscope tracking.	algorithm;differential evolution;emoticon;evolutionary computation;mathematical optimization;particle swarm optimization;software development process;video tracking;visual artifact	Xióngbiao Luó;Ying Wan;Xiangjian He	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8297001	evolutionary computation;computer vision;differential evolution;artificial intelligence;algorithm;synchronization;visualization;computer science;endoscopic camera;endoscope;match moving;particle swarm optimization	Robotics	46.879809257403046	-46.039442140799245	49907
19f33b34a690b826ce283850a21b2c5e1c7fe1f0	computationally inexpensive parallel parking supervisor based on video processing	axles;turning wheels space vehicles geometry cameras axles;automated vehicle control;video signal processing;turning;road traffic;parking;geometry;vehicle electronics;estacionamientos;procesamiento de imagenes digitales;parking space computationally inexpensive parallel parking supervisor video processing inexperienced drivers stressful situation traffic tickets parking violation crash related violations collision free parallel parking computationally effective approach;video signal processing road traffic traffic engineering computing;video display terminals;state of the art;info eu repo semantics masterthesis;traffic engineering computing;vehiculos de motor;cameras;driver support systems;space vehicles;wheels	Parallel parking, in general, is a moderate difficulty maneuver. Moreover, for inexperienced drivers, it can be a stressful situation that can lead to mistakes such as being far from the sidewalk or damage another vehicle resulting in traffic tickets that range from simple parking violation to crash-related violations. In this paper we propose a computationally effective approach to perform a collision-free parallel parking. The method will calculate the minimum parking space needed and then the minimum possible path for the parallel parking. This method is computationally inexpensive in comparison with the state of art. Moreover, it could be used by any car because the parameters needed to perform all computations are taken from published datasheets.	camera resectioning;computation;datasheet;experience;simulation;video processing	Caterina M. Espejo;Paul Rodríguez	2012	2012 15th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2012.6338623	embedded system;simulation;engineering;transport engineering	Robotics	44.42536134859189	-41.34970171660562	49961
a369e27a8679d5780cf0eb391fb96c0d0f9f6377	robust object tracking of irregular terrain vehicle	stability mobile robots robot vision target tracking image matching;robustness analysis;mobile robot;image matching;real time;mobile robots;irregular terrain vehicle;irregular terrain vehicle robot vision target image matching robustness analysis for tracking target tracking detectability mobile robot;stability;robot vision;detectability;natural environment;robustness vehicles target tracking robot vision systems cameras image analysis mobile robots computational efficiency object detection performance analysis;affine transformation;object tracking;performance analysis;robustness;image analysis;vehicles;target tracking;target image matching;computational efficiency;robustness analysis for tracking;robot vision systems;cameras;object detection	Tracking a target robustly by utilizing vision is very difficult for a mobile robot running over the irregular terrain of a natural environment, due to the image deformation caused by the rolling and pitching of the camera, as well as due to the relative movement between the target and the camera. One approach for coping with such problems is matching the target image with many affine-transformed candidate images while tracking. However, when the number of candidate images increases significantly, such an approach is not practical to real-time tasks due to the high computational cost involved. In the present paper, we propose a new system called robustness analysis for tracking (RAT) that improves tracking ability. RAT is an analysis based on the features of the object image, in which three parameters - detectability, robustness for depth and robustness for rotation - are defined. Many more robust templates can be found by analyzing the object image using RAT before the tracking task is performed. The experimental results are shown to verify the effectiveness of this method.		Jinsong Ding;Hiroshi Kondou;Hiroshi Kimura;Yoshiro Hada;Kunikatsu Takase	2002		10.1109/IRDS.2002.1041380	control engineering;mobile robot;computer vision;image analysis;simulation;computer science;artificial intelligence;video tracking	Robotics	51.92925519219445	-39.258660568858936	49965
a1a76ae4caea4bac571dd6a1152c0aeed8012632	particle filter-based visual detection of approaching aircraft in complex background images		This paper describes a particle-filter based visual processing algorithm that detects approaching aircraft in video feed with complex background images. The algorithm isolates moving aerial objects from homographic image differences of successive frames taken from an onboard camera, and characterizes them as approaching aerial vehicles based on the output of a particle filter with multi-threshold binarization. This algorithm is suitable for low-altitude flight tests as it effectively suppresses noise in complex, dynamically shifting background images, and it is less sensitive than other algorithms to color variations and outdoor sunlight conditions. The performance of the algorithm is validated by applying it to the onboard video feed recorded during a circle-turn flight test using two UAVs.	aerial photography;algorithm;particle filter;unmanned aerial vehicle;video	Sungsik Huh;Sungwook Cho;Hyoung Sik Choi;David Hyunchul Shim	2012		10.2514/6.2012-2486	computer vision;simulation;geography;computer graphics (images)	Vision	43.64441320455884	-45.84548269007237	50030
68fc9c92c188bae59b29d44030a979c09fb4074d	text recognition on traffic panels from street-level imagery	driver assistance text recognition traffic panel street level imagery text detection computer vision text extraction road panel street view image intelligent transportation system its panel detection word recognition hidden markov model web map service distance estimation text height direction vector real image google street view traffic sign inventory;image recognition;image segmentation;detection algorithms;text recognition vehicles image recognition roads image color analysis image segmentation detection algorithms;automated highways;computer vision;hidden markov models;roads;image color analysis;traffic engineering computing;text detection;vehicles;text recognition;traffic engineering computing automated highways computer vision hidden markov models image recognition text detection	Text detection and recognition in images taken in uncontrolled environments still remains a challenge in computer vision. This paper presents a method to extract the text depicted in road panels in street view images as an application to Intelligent Transportation Systems (ITS). It applies a text detection algorithm to the whole image together with a panel detection method to strengthen the detection of text in road panels. Word recognition is based on Hidden Markov Models, and a Web Map Service is used to increase the effectiveness of the recognition. In order to compute the distance from the vehicle to the panels, a function that estimates the distance in meters from the text height in pixels has been obtained. After computing the direction vector of the vehicle, world coordinates are computed for each panel. Experimental results on real images from Google Street View prove the efficiency of our proposal and give way to using street-level images for different applications on ITS such as traffic signs inventory or driver assistance.	algorithm;computer vision;glossary of computer graphics;google street view;hidden markov model;markov chain;optical character recognition;pixel;uncontrolled format string;web map service	Álvaro Gonzalez;Luis Miguel Bergasa;José Javier Yebes Torres;Javier Almazán	2012	2012 IEEE Intelligent Vehicles Symposium	10.1109/IVS.2012.6232157	computer vision;feature detection;simulation;speech recognition;computer science	Vision	42.393208606740785	-43.66308036709987	50138
27a561985f5e1231dacd372c933bbc864c80870e	model-based 3d scene reconstruction using a moving rgb-d camera		This paper presents a scalable model-based approach for 3D scene reconstruction using a moving RGB-D camera. The proposed approach enhances the accuracy of pose estimation due to exploiting the rich information in the multi-channel RGB-D image data. Our approach has lots of advantages on the reconstruction quality of the 3D scene as compared with the conventional approaches using sparse features for pose estimation. The pre-learned image-based 3D model provides multiple templates for sampled views of the model, which are used to estimate the poses of the frames in the input RGB-D video without the need of a priori internal and external camera parameters. Through template-to-frame registration, the reconstructed 3D scene can be loaded in an augmented reality (AR) environment to facilitate displaying, interaction, and rendering of an image-based AR application. Finally, we verify the ability of the established reconstruction system on publicly available benchmark datasets, and compare it with the sate-of-the-art pose estimation algorithms. The results indicate that our approach outperforms the compared methods on the accuracy of pose estimation.		Shyi-Chyi Cheng;Jui-Yuan Su;Jing-Min Chen;Jun-Wei Hsieh	2017		10.1007/978-3-319-51811-4_18	camera auto-calibration;camera resectioning;pinhole camera model	Vision	53.256323690477366	-46.35347520306464	50257
838aaccb03de052e15533b3ce4b06a6c4e4f129e	online global non-rigid registration for 3d object reconstruction using consumer-level depth cameras				Jiamin Xu;Weiwei Xu;Yin Yang;Zhigang Deng;Hujun Bao	2018	Comput. Graph. Forum	10.1111/cgf.13542	computer vision;artificial intelligence;computer science;computing methodologies	Vision	53.05395432112519	-47.06664509584363	50273
b0122893ba62a3cac738d2d20e209d35446e45e4	unrestricted recognition of 3d objects for robotics using multilevel triplet invariants	engineering and technology;teknik och teknologier	mensional objects was developed. By unrestricted, we imply that the recognition will be done independently of object position, scale, orientation, and pose against a structured background. It does not assume any preceding segmentation or allow a reasonable degree of occlusion. The method uses a hierarchy of triplet feature invariants, which are at each level defined by a learning procedure. In the feedback learning procedure, percepts are mapped on system states corresponding to manipulation parameters of the object. The method uses a learning architecture with channel information representation. This article discusses how objects can be represented. We propose a structure to deal with object and contextual properties in a transparent manner.	hidden surface determination;robotics;triplet state	Gösta H. Granlund;Anders Moe	2004	AI Magazine		computer vision;computer science;artificial intelligence;mathematics	Robotics	42.8251898151615	-39.97598124341769	50369
34fac9f7faa53a35a2117ce57941a8841b5ae725	explicit contour model for vehicle tracking with automatic hypothesis validation	bayesian framework;image motion analysis;vehicle detection shape road vehicles target tracking monitoring layout vehicle dynamics land vehicles cameras surveillance;conference_paper;bayes methods;bayesian framework explicit contour model vehicle tracking automatic hypothesis validation uncalibrated camera vehicles motion direction contour dynamics;image motion analysis tracking cameras bayes methods image sequences;contour dynamics;approximation theory;vehicle tracking;parameterized templates;cameras;tracking;image sequences	This paper addresses the problem of vehicle tracking under a single static, uncalibrated camera without any constraints on the scene or on the motion direction of vehicles. We introduce an explicit contour model, which not only provides a good approximation to the contours of all classes of vehicles but also embeds the contour dynamics in its parameterized template. We integrate the model into a Bayesian framework with multiple cues for vehicle tracking, and evaluate the correctness of a target hypothesis, with the information implied by the shape, by monitoring any conflicts within the hypothesis of every single target as well as between the hypotheses of all targets. We evaluated the proposed method using some real sequences, and demonstrated its effectiveness in tracking vehicles, which have their shape changed significantly while moving on curly, uphills roads.	approximation;correctness (computer science);vehicle tracking system	Boris Wai-Sing Yiu;Kwan-Yee Kenneth Wong;Francis Y. L. Chin;Ronald H. Y. Chung	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530122	computer vision;simulation;tracking system;mathematics;tracking;approximation theory;computer graphics (images)	Robotics	49.80494401783929	-48.759500113435294	50423
b79ef14da796fa1d5b7511eac533fa5f3649d42f	structure from motion estimation with positional cues	datorseende och robotik autonoma system;matematik	We present a system for structure from motion estimation using additional positioning data such as GPS data. The system incorporates the additional data in the outlier detection, the initial estimates and the final bundle adjustment. The initial solution is based on a novel objective function which is solved using convex optimization. This objective function is also used for outlier detection and removal. The initial solution is then refined based on a novel near L2 minimization of the reprojection error using convex optimization methods. We present results on synthetic and real data, that shows the robustness, accuracy and speed of the proposed method.	anomaly detection;approximation algorithm;bundle adjustment;convex optimization;experiment;global positioning system;global optimization;loss function;map projection;mathematical optimization;motion estimation;optimization problem;reprojection error;structure from motion;synthetic intelligence;virtual reality headset	Linus Svärm;Magnus Oskarsson	2013		10.1007/978-3-642-38886-6_49	computer vision;mathematical optimization;simulation;computer science	Vision	52.367092238353536	-48.242587437549794	50648
43fa2a9f0ae3654382dc86b74d0de40f84ff4c87	hand gesture spotting based on 3d dynamic features using hidden markov models	adaptive thresholding;confidence limit;hidden markov model;confidence measure;mean shift;kalman filter;time delay;computer vision;pattern recognition;depth map;gesture recognition;k means clustering;color image	  In this paper, we propose an automatic system that handles hand gesture spotting and recognition simultaneously in stereo  color image sequences without any time delay based on Hidden Markov Models (HMMs). Color and 3D depth map are used to segment  hand regions. The hand trajectory will determine in further step using Mean-shift algorithm and Kalman filter to generate  3D dynamic features. Furthermore, k-means clustering algorithm is employed for the HMMs codewords. To spot meaningful gestures accurately, a non-gesture model  is proposed, which provides confidence limit for the calculated likelihood by other gesture models. The confidence measures  are used as an adaptive threshold for spotting meaningful gestures. Experimental results show that the proposed system can  successfully recognize isolated gestures with 98.33% and meaningful gestures with 94.35% reliability for numbers (0-9).    		Mahmoud Elmezain;Ayoub Al-Hamadi;Bernd Michaelis	2009		10.1007/978-3-642-10546-3_2	computer vision;speech recognition;computer science;pattern recognition	Vision	39.83338498847255	-48.80270502956332	50654
7182b006bb5343a32f3adaef4d0e9996f18c57b1	markerless tracking and gesture recognition using polar correlation of camera optical flow	optical flow;polar correlation;multi camera;markerless	We present a novel, real-time, markerless vision-based tracking system, employing a rigid orthogonal configuration of two pairs of opposing cameras. Our system uses optical flow over sparse features to overcome the limitation of vision-based systems that require markers or a pre-loaded model of the physical environment. We show how opposing cameras enable cancellation of common components of optical flow leading to an efficient tracking algorithm that captures five degrees of freedom including direction of translation and angular velocity. Experiments comparing our device with an electromagnetic tracker show that its average tracking accuracy is 80 % over 185 frames, and it is able to track large range motions even in outdoor settings. We also present how our tracking system can be used for gesture recognition by combining it with a simple linear classifier over a set of 15 gestures. Experimental results show that we are able to achieve 86.7 % gesture recognition accuracy.	algorithm;angularjs;closing (morphology);experiment;gesture recognition;image plane;laser tracker;linear classifier;optical flow;prototype;real-time clock;real-time computing;sparse matrix;tracking system;velocity (software development);visual odometry	Prince Gupta;Niels da Vitoria Lobo;Joseph J. LaViola	2012	Machine Vision and Applications	10.1007/s00138-012-0451-3	computer vision;simulation;computer graphics (images)	Vision	53.38726042515238	-41.97088281947652	50818
948fb0a91cc691f57c443291fadf9cbe07c537bb	robust semi-dense matching across uncalibrated and widely separated views	fundamental matrix;affine transformation;matching method	This paper proposes an iterative framework to obtain a semi-dense matching from two wide-baseline images, when camera parameters and positions are unknown. The matching process is computed in three steps. First, a small number of robust correspondences is selected with SIFT descriptor. Then, this initial set is multiplied by iteratively reinforcing the epipolar constraint on rectified images during the estimation of the fundamental matrix. At a final refinement, point correspondences are boosted by applying a local affine transform in the image. The method has been tested on several pairs of images. Between three to four thousands points are paired on medium resolution images (1024x768 pixels) by this matching method: a sufficient number for accurate volumetric measurement.	3d modeling;baseline (configuration management);computer display standard;distortion;epipolar geometry;fundamental matrix (computer vision);iterative method;pixel;refinement (computing);rejection sampling;scale-invariant feature transform;semiconductor industry	Benjamin Albouy;Sylvie Treuillet;Yves Lucas	2007			computer vision;mathematical optimization;computer science;affine transformation;mathematics;geometry;fundamental matrix	Vision	51.778639673438455	-50.03386795183703	50844
ccd126c1337d29e3bd366579549fa4dc04443fed	efficient 3d tracking in urban environments with semantic segmentation		In this paper, we present a new 3D tracking approach for self-localization in urban environments. In particular, we build on existing tracking approaches (i.e., visual odometry tracking and SLAM), additionally using the information provided by 2.5D maps of the environment. Since this combination is not straightforward, we adopt ideas from semantic segmentation to find a better alignment between the pose estimated by the tracker and the 2.5D model. Specifically, we show that introducing edges as semantic classes is highly beneficial for our task. In this way, we can reduce tracker inaccuracies and prevent drifting, thus increasing the tracker’s stability. We evaluate our approach for two different challenging scenarios, also showing that it is generally applicable in different application domains and that we are not limited to a specific tracking method.	2.5d;application domain;map;polygonal modeling;simultaneous localization and mapping;visual odometry	Martin Hirzer;Peter M. Roth;Vincent Lepetit	2017			computer vision;artificial intelligence;computer science;segmentation	Vision	52.05167559623872	-45.20373288148411	50855
deaa0220e3ccdb97d2838cb17819cf0679c89d38	robust left object detection and verification in video surveillance		Left objects pose a real threat to security in public areas such as railway stations and airports. Detection of these objects therefore forms an important part in any intelligent video surveillance system that is deployed at such locations. Successful left object detection algorithms must operate in real time and produce sufficient detection accuracy with low false positive rates. However in reality, the requirement of both speed and performance is not often achieved due to the huge variation in image appearance caused by illumination, scene, and foreground objects (both dynamic and static). This paper tackles the challenge using a background subtraction scheme coupled with three other techniques. Short-term frame averaging is used to reduce the effect of moving objects such as pedestrians and vehicles. Statistical image background modelling is applied to enhance the visual contrast between the object and the background. Pixel colour modelling is employed to verify the results of left object segmentation. All three techniques are computationally lightweight and thus enable the left object detection to operate in real time.	algorithm;background subtraction;closed-circuit television;object detection;pixel;scalability;video content analysis	Yijun Xiao;Abdul R. Farooq;Melvyn L. Smith;Daniel Wright;Glynn Wright	2013			pattern recognition;pixel;computer vision;artificial intelligence;object-class detection;object detection;viola–jones object detection framework;visual contrast;computer science;segmentation;background subtraction	Vision	42.031745256030845	-45.518114173419676	50883
987ab3a25cd8e382f80153bf5233ceee114ccec7	optical flow-based probabilistic tracking	optical flow based probabilistic tracking;image motion analysis;probability;flow calculation techniques optical flow based probabilistic tracking particle filter algorithms matching techniques optical flow information;image matching;optical filters;particle filter algorithms;optical computing;layout;incomplete information;current measurement;optical tracking;particle filter;probability distribution;optical filters image motion analysis optical computing predictive models particle tracking particle filters layout probability distribution current measurement signal to noise ratio;optical flow information;flow calculation techniques;predictive models;optical flow;matching techniques;particle tracking;particle filters;signal to noise ratio;optical tracking image sequences image matching probability;object model;image sequences	In this paper, we present an observation model to track objects using particle filter algorithms based on matching techniques for computing optical flow. Although optical flow information enables us to know the displacement of objects present in a scene, it cannot be used directly to displace an object model since flow calculation techniques lack the necessary precision. In view of the fact that probabilistic tracking algorithms enable imprecise or incomplete information to be handled naturally, this model has been used as a natural means of incorporating flow information into the tracking.	algorithm;displacement mapping;optical flow;particle filter	Manuel J. Lucena;José Manuel Fuertes;José I. Gómez;Nicolas Pérez de la Blanca;Antonio Garrido Carrillo	2003		10.1109/ISSPA.2003.1224853	computer vision;mathematical optimization;particle filter;computer science;theoretical computer science;mathematics;statistics	Vision	52.60877287017342	-50.77973763627911	50916
bde02f615e6a0346e52fa14e58ebe67ab6c3fada	an empirical evaluation of interest point detectors	point matching;interest points;computer vision;feature descriptors	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	additive white gaussian noise;algorithm;computation;computer vision;francis;freak;fastest;feature (computer vision);high- and low-level;image scaling;memory management;mobile device;primary source;real-time clock;requirement;sensor;simultaneous localization and mapping;time complexity;utility functions on indivisible goods;visual descriptor;ipad	Iñigo Barandiaran;Manuel Graña;Marcos Nieto	2013	Cybernetics and Systems	10.1080/01969722.2013.762232	point set registration;computer vision;mathematical optimization;feature detection;simulation;computer science;mathematics;interest point detection	Robotics	51.70008275796087	-43.50670632045878	51202
4df1948cb36d1762a7265b35440339fbb8a8e01d	object localization by bayesian correlation	bayesian methods image analysis electrical capacitance tomography image sampling probability filter bank read only memory image coding motion pictures sampling methods;object recognition;observation likelihood;optimisation;image coding;probability;filter bank;cross correlation;correlation theory;bayes methods;computer and information science;statistical modelling;matching function;object localization;probability distribution;asymptotic properties object localization bayesian correlation cross correlation maximisation intensity based object localization sequential inference ambiguity probability distribution object location consistent probabilistic treatment correlation matching functions probabilistic terms observation likelihoods probability distributions filter bank responses training examples response learning statistical modelling background intensities image coding independent component analysis multi scale processing bayesian context layered sampling;component analysis;asymptotic properties;correlation theory object recognition bayes methods probability optimisation;data och informationsvetenskap	Maximisation of cross-correlation is a commonly used principle for intensity-based object localization that gives a single estimate of location. However, to facilitate sequential inference (eg over time or scale) and to allow the representation of ambiguity, it is desirable to represent an entire probability distribution for object location. Although the crosscorrelation itself (or some function of it) has sometimes been treated as a probability distribution, this is not generally jus tifiable. Bayesian correlation achieves a consistent probabilistic treatment by combining several developments. The first is the interpretation of correlation matching functions in probabilistic terms, as observation likelihoods. Second, probability distributions of filter-bank responses are learned from training examples. Inescapably, response-learning also demands statistical modelling of background intensities, and there are links here with image coding and Independent Component Analysis. Lastly, multi-scale processing is achieved, in a Bayesian context, by means of a new algorithm,layered sampling, for which asymptotic properties are derived.	algorithm;bayesian network;cross-correlation;filter bank;independent component analysis;javascript;sampling (signal processing);statistical model	Josephine Sullivan;Andrew Blake;Michael Isard;John MacCormick	1999		10.1109/ICCV.1999.790391	probability distribution;statistical model;cross-correlation;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;probability;filter bank;mathematics;bayesian statistics;empirical probability;statistics	Vision	45.06382332570811	-51.6426611623941	51270
8e87ccb863c33336dda70a545500154d05651510	inverted tracking algorithm for the field survey through artificial vision and robotics		The area of artificial vision and robotics has very important advances in the recognition and tracking of objects, not only in indoor scenes but also in outdoor ones. These methods and algorithms have given rise to very important technological advances in different areas of knowledge. In the area of Precision Agriculture, the main problem of its use lies in its application in field surveys, whereas in the case of cultivation, we will have fixed objects (seedlings) in established spaces (furrows and plots), but in uncontrolled environments. The determination of the density of these crops and their distance between furrows among other data is in many cases, relevant to their performance. It is the purpose of this paper to solve the automated sensing of this data through the use of cameras and artificial vision techniques. In this work, an inverted tracking algorithm is defined in order to automatically determine the necessary shot-points by means of which the cameras involved as sensors on a robotic platform capture scene images. This will help to survey the density and distance of the crop to be analyzed.	algorithm;computer vision;robot;robotics;sensor;uncontrolled format string	Eduardo Alvarez;Sandra Serafino;Benjamin Cicerchia;Agustin Balmer;Claudia Russo;Hugo Ramon	2017	2017 XLIII Latin American Computer Conference (CLEI)	10.1109/CLEI.2017.8226374	algorithm;precision agriculture;image segmentation;artificial intelligence;geography;robotics	Robotics	52.04261029075165	-38.46518529248648	51304
e1426ca65fa30963a450e02de5ad7eb940afb5ef	orbfusion: real-time and accurate dense slam at large scale		We present a new SLAM system capable of producing high quality globally consistent surface reconstructions with accurate real-time tracking and localization abilities. The system works on an off the shelf laptop with a typical GPU. This paper proposes an approach to unify feature-based keyframe techniques with fused volumetric surface reconstruction methods to overcome both of their limitations. On one hand, feature-based keyframe SLAM techniques have reached a level of maturity and can guarantee accurate and real-time tracking and localization ability, but their raw RGB-D point clouds are too noisy. On the other hand, volumetric surface reconstruction methods can produce a dense surface reconstruction of the environment, which will be helpful for Augmented Reality (AR) applications and scene understanding. However, current dense SLAM systems have limited tracking ability, which is vital for the quality of surface reconstruction. Moreover most of the current dense SLAM systems have to run on a powerful desktop PC to guarantee realtime performance. By unifying the feature-based keyframe tracking ability and adopting a multi-threaded design, our system improves both the tracking ability and the real-time performance. We present results of a wide variety of aspects of our system and evaluate it using the widely used TUM RGB-D and ICL-NUIM Datasets. Our system achieves unprecedented performance in terms of trajectory estimation, surface reconstruction, real-time and computational performance in comparison to other start-ofthe-art dense RGB-D SLAM systems.	real-time transcription	Juting Dai;Xinyi Tang;Leif Oppermann	2017		10.1109/ISMAR-Adjunct.2017.46	computer vision;artificial intelligence;computer science;point cloud;laptop;rgb color model;surface reconstruction;augmented reality	Robotics	53.537333780308565	-45.79986214645266	51410
04cf5b67fa4623512843acfb9f0066e5c11b8887	patch warping and local constraints for improved block matching stereo correspondence	memory management;hamming distance;shape;estimation;three dimensional displays;transforms;cameras	Depth estimation of the surrounding environment using a stereoscopic camera setup is an important and fundamental research topic in computer vision. Due to its running time and quality performance in real situations the semi global matching algorithm is often used. The biggest disadvantage of the semi global approach is its large memory footprint. On the other hand, block matching stereo is leaner when it comes to memory consumption and therefore it is commonly used in applications where we do not have many resources, in order to obtain coarse depth information of the environment. The poor quality performance of such algorithms make them impractical for many real life applications. In this paper we focus on improving the quality of the classical block matching (BM) stereo method by proposing a novel approach which tackles the problem of stereo matching for slanted and fronto-parallel surfaces by using different types of binary masks on the matching window. Another improvement consists in the usage of different types of local constraints in the generation of the winning disparity for a specific position, such that possible outliers are eliminated from the start. The validation of our results has been done on the KITTI stereo benchmark dataset.	algorithm;benchmark (computing);binocular disparity;computer stereo vision;computer vision;memory footprint;real life;semiconductor industry;stereoscopy;time complexity	Mircea Paul Muresan;Sergiu Nedevschi;Radu Danescu	2016	2016 IEEE 12th International Conference on Intelligent Computer Communication and Processing (ICCP)	10.1109/ICCP.2016.7737167	computer vision;simulation;mathematics;computer graphics (images)	Vision	51.55254625470856	-47.7549461280861	51537
67748e2f4357b56bd270692dcd1a0a68e31028df	integration of motion fields through shape	minimisation;shape estimation;decoupling translation from rotation;image segmentation;image resolution;integration of motion fields;computational geometry;motion estimation;computational geometry motion estimation minimisation image colour analysis image segmentation image sequences image resolution;constrained minimization;shape motion estimation layout cameras parameter estimation automation educational institutions motion segmentation computer vision image resolution;multiple views;motion segmentation;flow field;image colour analysis;image sequence;3d motion estimation;structural estimation;structure from motion;shape and rotation;shape and rotation 3d motion estimation integration of motion fields decoupling translation from rotation;image sequences;image sequence 3d motion estimation structure estimation constrained minimization problem rank 3 constraint motion segmentation planar patches scene patches image gradients motion field integration shape estimation	Structure from motion from single flow fields has been studied intensively, but the integration of information from multiple flow fields has not received much attention. Here we address this problem by enforcing constraints on the shape (surface normals) of the scene in view, as opposed to constraints on the structure (depth). The advantage of integrating shape is two-fold. First, we do not need to estimate feature correspondences over multiple frames, but we only need to match patches. Second, the shape vectors in the different views are related only by rotation. This constraint on shape can be combined easily with motion estimation, thus formulating motion and structure estimation from multiple views as a practical constrained minimization problem using a rank-3 constraint. Based on this constraint, we develop a 3D motion technique, which locates through color and motion segmentation, planar patches in the scene, matches patches over multiple frames, and estimates the motion between multiple frames and the shape of the selected scene patches using the image gradients. Experiments evaluate the accuracy of the 3D motion estimation and demonstrate the motion and shape estimation of the technique by super-resolving an image sequence.	gradient;motion estimation;normal (geometry);structure from motion	Hui Ji;Cornelia Fermüller	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.190	computer vision;minimisation;mathematical optimization;structure from motion;image resolution;computational geometry;computer science;motion estimation;mathematics;geometry;image segmentation;motion field	Vision	53.29627771592011	-51.23642908307434	51649
33b013d22e9bfe3e8af14a5b4b35f018d8a7b833	detecting wires in cluttered urban scenes using a gaussian model	histograms;gaussian model;linear patterns wire detection algorithm unmanned aerial vehicles low altitude urban reconnaissance gaussian model urban search and rescue operation military reconnaissance operation image identification;detection algorithms;gaussian processes;linear patterns;edge detection;unmanned aerial vehicle;image identification;wires;mobile robots;remotely operated vehicles;fitting;robot vision;robot vision aerospace robotics edge detection gaussian processes mobile robots object detection remotely operated vehicles;image edge detection;pixel;aerospace robotics;detection algorithm;urban search and rescue;military reconnaissance operation;unmanned aerial vehicles;object detection;wires pixel videos fitting histograms image edge detection detection algorithms;videos;low altitude urban reconnaissance;wire detection algorithm;urban search and rescue operation	A novel wire detection algorithm for use by unmanned aerial vehicles (UAV) in low altitude urban reconnaissance is presented. This is of interest to urban search and rescue and military reconnaissance operations. Detection of wires plays an important role, because thin wires are hard to discern by tele-operators and automated systems. Our algorithm is based on identification of linear patterns in images. Most existing methods that search for linear patterns use a simple model of a line, which does not take into account the line surroundings. We propose the use of a robust Gaussian model to approximate the intensity profile of a line and its surroundings which allows effective discrimination of wires from other visually similar linear patterns. The algorithm is able to cope with highly cluttered urban backgrounds, moderate rain, and mist. Experimental results show a 17.7% detection improvement over the baseline.	aerial photography;approximation algorithm;baseline (configuration management);sensor;television;unmanned aerial vehicle	Joshua Candamo;Dmitry B. Goldgof;Rangachar Kasturi;Sridhar Godavarthy	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.114	remotely operated underwater vehicle;mobile robot;computer vision;simulation;edge detection;gaussian network model;computer science;gaussian process;histogram;pixel;statistics	Robotics	42.96851509064371	-42.84373486016889	51657
231b12095ddd88b9ff6ffbbaf5a812ccf1090b33	using 3d gng-based reconstruction for 6dof egomotion	lasers;sensors lasers mobile robots;time of flight;sensors;mobile robot;datorseende och robotik autonoma system;growing neural gas;mobile robots;3d model;3d mapping gng based reconstruction 6dof egomotion six degrees of freedom 3d data mobile robotic problems sensor time of flight cameras 3d lasers growing neural gas planar patches 3d models registration algorithm;three dimensional displays neurons solid modeling robot sensing systems cameras computational modeling	Several recent works deal with 3D data in mobile robotic problems, e.g. mapping. Data come from any kind of sensor (time of flight cameras and 3D lasers) providing a huge amount of unorganized 3D data. In this paper we detail an efficient method to build complete 3D models from a Growing Neural Gas (GNG). We show that the use of GNG provides better results than other approaches. The GNG obtained is then applied to a sequence. From GNG structure, we propose to calculate planar patches and thus obtaining a fast method to compute the movement performed by a mobile robot by means of a 3D models registration algorithm. Final results of 3D mapping are also shown.	3d modeling;algorithm;computation;fits;level of detail;mobile robot;need to know;neural gas;simultaneous localization and mapping;visual odometry	Diego Viejo;José García Rodríguez;Miguel Cazorla;David Gil Méndez;Magnus Johnsson	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033337	mobile robot;computer vision;simulation;computer science;artificial intelligence	Robotics	52.832698233719675	-41.38923405752617	51757
b3e624c9dfcfa48e86f29f7a605e8d62fa6d7134	video object segmentation based on object enhancement and region merging	video object;histograms;image motion analysis;image segmentation;video signal processing;histogram based merging;bayesian methods;standard video test sequence video object segmentation object enhancement motion variance object color histogram based merging occlusion handling island detection;size measurement;motion estimation;testing;object segmentation;image enhancement;video object segmentation;trajectory;object enhancement;image colour analysis;island detection;occlusion handling;object segmentation merging histograms tracking motion estimation bayesian methods motion measurement testing trajectory size measurement;video signal processing image colour analysis image enhancement image motion analysis image segmentation image sequences object detection;merging;standard video test sequence;object color;cumulant;motion measurement;region merging;tracking;object detection;image sequences;motion variance	This paper proposes a number of improvements to existing work in off line video object segmentation. Object color and motion variance, and histogram-based merging are used to improve the initial segmentation. Segmentation quality measures taken from throughout the clip are used to enhance video objects. Cumulative histogram-based merging, occlusion handling, and island detection are used to help group regions into meaningful objects. Objective and subjective tests were performed on a set of standard video test sequences which demonstrate improved accuracy and greater success in identifying the real objects in a video clip compared to the reference method	video clip	Ken Ryan;Aishy Amer;Langis Gagnon	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262451	computer vision;bayesian probability;computer science;trajectory;segmentation-based object categorization;video tracking;pattern recognition;motion estimation;histogram;tracking;software testing;image segmentation;scale-space segmentation;statistics;cumulant;computer graphics (images)	Vision	44.639111143638736	-49.68041596841369	51813
2b2433f0e8ce23f8f01b6fc2fd956cefcddf08f1	epipolar based structured light pattern design for 3-d reconstruction of moving surfaces	coded structured lighting;image coding;hamming codes;grey levels coding;search space;image matching;path planning;feature codeword;real time;spectral harmful effects epipolar based structured light pattern design real time 3 d moving surface reconstruction robust matching technique coded structured lighting grey levels coding pattern coding hamming distance feature codeword pattern design sl neighbourhood scheme autonomous navigation epipolar geometry reoriented features;geometry;structured light;encoding shape geometry hamming distance three dimensional displays robustness cameras;real time 3 d moving surface reconstruction;epipolar based structured light pattern design;geometric feature;three dimensional;image texture;epipolar geometry;robot vision gray codes grey systems hamming codes image coding image matching image texture lightning path planning;hamming distance;robot vision;shape;three dimensional displays;spectral harmful effects;reoriented features;lightning;grey systems;robustness;robust matching technique;spectral properties;autonomous navigation;encoding;pattern coding;sl neighbourhood scheme;pattern design;cameras;gray codes	In this paper, we address a robust matching technique based on coded structured lighting (SL) to achieve real-time 3-D reconstructions. To that purpose, most existing approaches involve color or grey levels coding but they are well-known to be sensitive to spectral properties and texture of the viewed surfaces. Therefore, the overall robustness of the proposed technique comes first from the geometrical features used in conjunction with the SL neighbourhood scheme to carry out pattern coding. Second, a desired minimum Hamming distance between features' codewords drives the pattern design. This is suited for autonomous navigation in unknown environments as this parameter enables codewords correction capabilities during the decoding stage in real-time. Furthermore, we take advantage of the known epipolar geometry by including projective invariants between corresponding epipolar lines directly in the pattern components. Thus, the final pattern displays relocated and reoriented (cuneiform) features along the epipolar lines. This grouping also contributes to reduce the search space more and results in a significantly less constrained coding. Finally, we show how this coding and this non-grid based pattern offer efficient and fast correction of mislabeled features due to blurring, spectral harmful effects and surfaces discontinuities prior to the 3-D reconstruction of real scenes.	3d reconstruction;autonomous robot;code word;color;correspondence problem;cuneiform;depth map;epipolar geometry;grayscale;hamming distance;real-time clock;sl (complexity);structured light;visual servoing	Xavier Maurice;Pierre Graebling;Christophe Doignon	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5979582	image texture;gray code;lightning;three-dimensional space;computer vision;hamming distance;structured light;shape;computer science;hamming code;mathematics;geometry;motion planning;optics;encoding;robustness;epipolar geometry	Robotics	48.38512433567303	-49.849493412831556	51936
3d4217dda9d9327b5ef87e1d8b3644de79d452b1	direct monocular odometry using points and lines		Most visual odometry algorithm for a monocular camera focuses on points, either by feature matching, or direct alignment of pixel intensity, while ignoring a common but important geometry entity: edges. In this paper, we propose an odometry algorithm that combines points and edges to benefit from the advantages of both direct and feature based methods. It works better in texture-less environments and is also more robust to lighting changes and fast motion by increasing the convergence basin. We maintain a depth map for the keyframe then in the tracking part, the camera pose is recovered by minimizing both the photometric error and geometric error to the matched edge in a probabilistic framework. In the mapping part, edge is used to speed up and increase stereo matching accuracy. On various public datasets, our algorithm achieves better or comparable performance than state-of-the-art monocular odometry methods. In some challenging texture-less environments, our algorithm reduces the state estimation error over 50%.	algorithm;brute-force search;bundle adjustment;computation;computer stereo vision;depth map;edge detection;general inter-orb protocol;graph (discrete mathematics);key frame;matching (graph theory);pattern matching;pixel;semiconductor industry;serial digital video out;visual odometry	Shichao Yang;Sebastian Scherer	2017	2017 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2017.7989446	computer science;pattern recognition;pixel;artificial intelligence;robustness (computer science);odometry;speedup;simultaneous localization and mapping;computer vision;monocular;visual odometry;depth map	Robotics	52.04335710231462	-46.508182627895266	52001
076e66980605bc744bab1b16d739379b257fab58	object tracking based on particle filtering with multiple appearance models		In this paper, we propose a novel method to track an object whose appearance is evolving in time. The tracking procedure is performed by a particle filter algorithm in which all possible appearance models are explicitly considered using a mixture decomposition of the likelihood. Then, the component weights of this mixture are conditioned by both the state and the current observation. Moreover, the use of the current observation makes the estimation process more robust and allows handling complementary features, such as color and shape information. In the proposed approach, these estimated component weights are computed using a Support Vector Machine. Tests on a mouth tracking problem show that the multiple appearance model outperforms classical single appearance likelihood.	c date and time functions;color;computer vision;emoticon;experiment;mixture model;online and offline;particle filter;peterson's algorithm;robustness (computer science);support vector machine	Nicolas Widynski;Emanuel Aldea;Séverine Dubuisson;Isabelle Bloch	2011			computer vision	Vision	44.43645115096128	-48.48455845505218	52036
474369cc5d62e5a1fd11abb64fdcc68725f99d13	multi-body factorization with uncertainty: revisiting motion consistency	multi body factorization;model selection;directional uncertainty;feature tracking;motion segmentation;dense segmentation;dynamic analysis	Dynamic analysis of video sequences often relies on the segmentation of the sequence into regions of consistent motions. Approaching this problem requires a definition of which motions are regarded as consistent. Common approaches to motion segmentation usually group together points or image regions that have the same motion between successive frames (where the same motion can be 2D, 3D, or non-rigid). In this paper we define a new type of motion consistency, which is based on temporal consistency of behaviors across multiple frames in the video sequence. Our definition of consistent “temporal behavior” is expressed in terms of multi-frame linear subspace constraints. This definition applies to 2D, 3D, and some non-rigid motions without requiring prior model selection. We further show that our definition of motion consistency extends to data with directional uncertainty, thus leading to a dense segmentation of the entire image. Such segmentation is obtained by applying the new motion consistency constraints directly to covariance-weighted image brightness measurements. This is done without requiring prior correspondence estimation nor feature tracking.	algorithm;consistency model;model selection;motion estimation	Lihi Zelnik-Manor;Moshe Machline;Michal Irani	2005	International Journal of Computer Vision	10.1007/s11263-005-4840-1	computer vision;mathematical optimization;quarter-pixel motion;segmentation-based object categorization;pattern recognition;motion estimation;mathematics;dynamic program analysis;motion field;scale-space segmentation;model selection;statistics	Vision	52.1616422341442	-50.36623977440241	52168
18cf5ac7e602cea7d743cbde9b66a6da6e1120dc	learning skeletons for shape and pose	skeleton estimation;generic model;game engine;linear blend skinning;body shape	In this paper a method for estimating a rigid skeleton, including skinning weights, skeleton connectivity, and joint positions, given a sparse set of example poses is presented. In contrast to other methods, we are able to simultaneously take examples of different subjects into account, which improves the robustness of the estimation. It is additionally possible to generate a skeleton that primarily describes variations in body shape instead of pose. The shape skeleton can then be combined with a regular pose varying skeleton. That way pose and body shape can be controlled simultaneously but separately. As this skeleton is technically still just a skinned rigid skeleton, compatibility with major modelling packages and game engines is retained. We further present an approach for synthesizing a suitable bind shape that additionally improves the accuracy of the generated model.	game engine;pose (computer vision);robustness (computer science);shape context;sparse language;sparse matrix	Nils Hasler;Thorsten Thormählen;Bodo Rosenhahn;Hans-Peter Seidel	2010		10.1145/1730804.1730809	computer vision;simulation;body shape	Vision	47.1734227415191	-51.9950315853449	52202
77274d4f302d5d49b239f8fa65144544cbb627e6	focusing on target's features while tracking	target tracking feature extraction image motion analysis image segmentation object detection;image motion analysis;image segmentation;target tracking cameras focusing robustness object detection face detection delta modulation registers system performance upper bound;moving object tracking;motion segmentation;object detection target tracking moving object tracking motion segmentation;feature extraction;target tracking;object detection	Usually tracking objects by means of a moving camera means solving the problem of segmenting its motion from the background and maintain the gaze on it. Usually key points, i.e. features like corners, are matched throughout consecutive frame to register frames and detecting objects or to compute a fixation point on the object of interest. None of the methods proposed so far investigates how, more then just matching features between frames, the tuning of their quality would increase the robustness of the methods. Hence, the novelty proposed in this paper concerns the automatic tuning of the focus parameter to increase the tracking capabilities and strength the robustness of the entire system.	sensor	Christian Micheloni;Gian Luca Foresti	2006		10.1109/ICPR.2006.582	computer vision;simulation;tracking system;feature extraction;computer science;viola–jones object detection framework;machine learning;video tracking;pattern recognition;image segmentation	Vision	44.869527719621566	-46.237394468427	52277
a3f0d7ad19dd62143f47a706bc9360fe353a4396	from depth and optical flow to rigid body motion	image motion analysis;optical noise;picture processing;mobile robots;depth;computer vision;navigation;rigid body motion;depth picture recognition picture processing rigid body motion optical flow;image motion analysis optical sensors optical noise equations computer vision mobile robots navigation monitoring optical losses;picture recognition;monitoring;pattern recognition;optical flow;optical sensors;picture processing pattern recognition;optical losses	The authors develop an algorithm to determine uniquely the rigid body motion from optical flow and depth, where the depth, however, does not involve any derivative information. Thus, the original assumptions made by D.H. Ballard and O.A. Kimball (1983) and by R.M. Haralick and X. Zhuang (1986) are relaxed. The proposed algorithm is appealing; in contrast to the existing linear optical flow-motion algorithms, it requires only three instead of eight optical flow image points. >	optical flow	Xinhua Zhuang;Robert M. Haralick;Yunxin Zhao	1988		10.1109/CVPR.1988.196265	mobile robot;computer vision;navigation;rigid body;simulation;computer science;optical flow	Robotics	53.36068968561301	-50.774375206673255	52300
4ad94d423a48ea4f5e5af83d729eb188c6dda7e0	robust multiple targets tracking using object segmentation and trajectory estimation in video	video object;mathematical morphology;mobile camera;image motion analysis;video streaming;robust unsupervised video object tracking algorithm;image segmentation;mathematical morphological edge detector;edge detection;multiple video object tracking;robustness target tracking object segmentation trajectory partitioning algorithms morphology merging object detection motion detection detectors;object motion analysis;multiple target tracking;video streaming edge detection feature extraction image motion analysis image segmentation mathematical morphology object detection target tracking;object segmentation;feature extraction;robust multiple target tracking;multiple video object tracking robust multiple target tracking edge based object segmentation algorithm trajectory estimation robust unsupervised video object tracking algorithm mathematical morphological edge detector region growing region merging video frame contour object partitioning object motion analysis mobile camera;edge based object segmentation algorithm;video frame contour;target tracking;object partitioning;region growing;region merging;object detection;trajectory estimation	In this paper, a novel robust unsupervised video object tracking algorithm is proposed. The proposed algorithm combines several techniques: mathematical morphology, region growing, region merging, and trajectory estimation, for tracking several predetermined video objects, simultaneously. A modified mathematical morphological edge detector was employed to sketch the contour of the video frame; and an edge-based object segmentation algorithm was applied to the contour for partitioning the predetermined objects; moreover, according to the motion of the objects, the proposed algorithm can estimate and partition the objects in following video frames, automatically. The proposed algorithm is also robustness against mobile cameras. The experimental results show that the proposed algorithm can precisely partition and track multiple video objects	algorithm;contour line;edge detection;mathematical morphology;region growing;scheme	Ying-Tung Hsiao;Cheng-Long Chuang;Joe-Air Jiang;Cheng-Chih Chien	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571289	computer vision;mathematical morphology;edge detection;feature extraction;computer science;machine learning;video tracking;pattern recognition;block-matching algorithm;region growing;image segmentation;computer graphics (images)	Vision	44.4863820863001	-50.28565007145516	52310
463a060cb152eac33789ed63672dab7f816eb523	a non-intrusive method for user focus of attention estimation in front of a computer monitor	eye;pose estimation cameras eye learning artificial intelligence;user focus attention estimation;computer displays cameras magnetic heads tracking machine learning real time systems feedback hardware biomedical monitoring calibration;real time;computer monitor;distance measurement;user head position estimation;estimation;machine learning;monitoring;focus of attention;eye movements;eye movement;web camera;robustness;face;head;learning artificial intelligence;infrared;non intrusive method;eye gaze;cameras;machine learning user focus attention estimation non intrusive method computer monitor web camera user head position estimation eye movements;real time systems;pose estimation	In this work, we present a system that estimates a user's focus of attention in front of a computer screen, using a Web camera, based on detection and tracking of the user's head position and eye movements. Utilizing machine learning concepts, the system gives real time feedback on the user's attention, by combining information coming from eye gaze, head pose, and distance from the screen. The system is completely un-intrusive and no special hardware (such as infrared cameras or wearable devices) is needed. Furthermore, it adjusts to every user, not necessitating initial calibration, and can work under real and unconstrained conditions in terms of lighting.	computer monitor;focus (computing);machine learning;wearable technology;webcam	Stylianos Asteriadis;Paraskevi K. Tzouveli;Kostas Karpouzis;Stefanos D. Kollias	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813330	computer vision;simulation;computer science;computer graphics (images)	Robotics	47.517142169320216	-43.65362600497819	52316
38f8ef3ab088bacf63760ce316ed078a9ca6d330	trinocular stereovision: recent results	number agreement;important role;co-occurrence constraint;natural language processing;rule-based system;agreement restriction;recent result;selectional restriction;trinocular stereovision	We present an original approach to build rapidly and reliably a 3D description of the environment of a mobile robot by means of passive stereovision using three cameras. This technique has been successfully applied to many indoor scenes and has proved faster than a previously developed binocular stereo technique, while providing more reliable and more accurate results. Moreover, the algorithm is highly parallelisable and has indeed been parallelised, thus highly increasing its speed. Results showing the construction of the 3D visual map of a complex indoor scene are included.	algorithm;binocular vision;mobile robot;stereopsis;stereoscopy	Nicholas Ayache;Francis Lustman	1987			artificial intelligence;machine learning;computer science	Robotics	52.926728298541896	-38.62596410815583	52464
a0169b8793d22fe45796994bb4d5fdd193d08746	attributed image matching using a minimum representation size criterion	image features;image matching;image matching polynomials data engineering sensor systems and applications robustness pattern matching systems engineering and theory sensor phenomena and characterization robot sensing systems application software;multisensor data matching robots computer vision attributed image mapping minimum representation size criterion noisy gray level images attributed models three dimensional models;three dimensional;computer vision;robots;robots computer vision;graph model	The authors describe a novel approach to image matching which utilizes the minimal representation criterion as a means to obtain robust matching performance, even when image data are extremely noisy. They describe the application of this approach to the problem of matching noisy gray-level images to attributed models. Using the minimum representation criterion, the match between gray-level image features and an attributed graph model incorporates a representation size measure for the modeled points, the data residuals, and the unmodeled points. This structural representation identifies correspondence between a subset of data points and a subset of model points in a manner which minimizes the complexity of the resulting model. The proposed minimum representation matching algorithm is polynomial in complexity, and exhibits robust matching performance on examples where less than 30% of the features are reliable. The minimum representation principle is extensible to related problems using three-dimensional models and multisensor data matching. >	image registration	Arthur C. Sanderson;Nigel J. Foster	1989		10.1109/ROBOT.1989.100014	robot;three-dimensional space;computer vision;template matching;computer science;3-dimensional matching;machine learning;pattern recognition;mathematics;feature	Vision	50.58538810627975	-38.627657682199505	52594
4b0652eeb1599357f1104478b17f8112844dfd2a	people tracking across two distant self-calibrated cameras	image features;video surveillance;surveillance system;image sensors;pets 2006 benchmark people geometric tracking self calibrated cameras multicamera surveillance systems multicamera tracking;people tracking;cameras calibration geometry surveillance layout lighting head positron emission tomography image segmentation robustness;video surveillance cameras image sensors tracking;cameras;tracking	People tracking is of fundamental importance in multi-camera surveillance systems. In recent years, many approaches for multi-camera tracking have been discussed. Most methods use either various image features or the geometric relation between the cameras or both as a cue. It is a desire to know the geometry for distant cameras, because geometry is not influenced by, for example, drastic changes in object appearance or in scene illumination. However, the determination of the camera geometry is cumbersome. The paper tries to solve this problem and contributes in two different ways. On the one hand, an approach is presented that calibrates two distant cameras automatically. We continue previous work and focus especially on the calibration of the extrinsic parameters. Point correspondences are used for this task which are acquired by detecting points on top of people's heads. On the other hand, qualitative experimental results with the PETS 2006 benchmark data show that the self-calibration is accurate enough for a solely geometric tracking of people across distant cameras. Reliable features for a matching are hardly available in such cases.	benchmark (computing);camera resectioning;global illumination;match moving;scene graph;sensor	Roman P. Pflugfelder;Horst Bischof	2007	2007 IEEE Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2007.4425343	computer vision;simulation;computer science;video tracking;image sensor;tracking;feature;computer graphics (images)	Vision	49.79442733552226	-46.772234822450294	52674
f18d16ecb80d8cd768bddf519f93ba74e18fb733	multi-object particle filter tracking with automatic event analysis	goodness of fit;video surveillance;event analysis;indexing and retrieval;video coding;particle filter;object tracking;consistency checking;video content analysis;multiple non rigid object tracking;multi resolution	The automatic video content analysis is an important step to provide the content-based video coding, indexing and retrieval. It is also a key issue to the event analysis in video surveillance. In this paper, an automatic event analysis approach is presented. It is based on our previous method of Multi-object Particle Filter Tracking with Dual Consistency Check. The multiple non-rigid objects are first tracked individually in parallel by multi-resolution technique and particle filter method. The events including object presence and occlusion identification are then detected and analyzed by measuring the Goodness-of-Fit Coefficient based on Schwartz's inequality and the Backward Projection. The method is then tested in different indoor and outdoor environments with cluttered background. The experimental results show the robustness and the effectiveness of the method.	closed-circuit television;coefficient;data compression;digital video;particle filter;social inequality;video content analysis	Yifan Zhou;Jenny Benois-Pineau;Henri Nicolas	2010		10.1145/1877868.1877876	computer vision;simulation;computer science;video tracking;multimedia	Vision	42.72284434439704	-47.99277778457788	52697
ed42b4b11a6c334ce27d838af9fd50f2929eb4c6	camera placement optimization in object localization systems		This paper focuses on the placement of cameras in order to achieve the highest possible localization accuracy with a multi-camera system. The cameras have redundant fields of view. They have to be placed according to some natural constraints but user defined constraints are allowed as well. A camera model is described and the components causing the localization errors are identified. Some localization accuracy measures are defined for any number of cameras. The multi-camera placement is analytically formulated using the expanded measures for multiple cameras. An example of placing two cameras is shown and the generalizations into higher dimensional parameter spaces are examined. There are publications where camera placement algorithms are formulated or compared. We make an attempt to examine the analytical solution of this problem in case of different objective functions.		Dávid Szalóki;Sándor Kolumbán;Kristóf Csorba;Gábor Tevesz	2015	Acta Cybern.	10.14232/actacyb.22.1.2015.13	computer vision;camera auto-calibration;camera resectioning	Vision	49.89253075891989	-47.40922405009065	52754
b62076944a3b899b6eafe1a3aca5761374556c3b	dominant plane detection using optical flow and independent component analysis	optical flow field;independent component analysis;dominant plane corresponds;dominant plane;mobile robot;mobile robot motion;autonomous navigation;dominant plane detection;robot move;optical flow;real image sequence show;path planning;vision system	  Dominant plane is an area which occupies the largest domain in an image. Estimation of the dominant plane is an essential  task for the autonomous navigation and the path planning of the mobile robot equipped with a vision system, since the robot  moves on the dominant plane. In this paper, we develop an algorithm for dominant plane detection using optical flow and Independent  Component Analysis(ICA). Since the optical flow field is a mixture of flows of the dominant plane and the other area, we separate  the dominant plane using ICA. Using the initial data as a supervisor signal, the robot detects the dominant plane. For each  image in a sequence, the dominant plane corresponds to an independent component. This relation provides us a statistical definition  of the dominant plane. Experimental results using real image sequence show that our method is robust against a perturbation  of the motion speed of robots.    	independent component analysis;optical flow	Naoya Ohnishi;Atsushi Imiya	2005		10.1007/11565123_47	mobile robot;independent component analysis;computer vision;navigation;image processing;computer science;autonomous system;artificial intelligence;optical flow;motion planning;image plane;robotics	Vision	52.01433459728254	-38.23352954147878	52923
fa989b1c688217752bd6409dc5cf2202ea467eaa	on the use of snakes for 3-d robotic visual tracking	robot sensing systems;image gradient;eye in hand robot arm configuration;servoing;snakes;generic model;deformable models robot sensing systems shape robot kinematics orbital robotics robot vision systems cameras target tracking robot control inspection;real time;active vision robot vision optical tracking;elastic structures;deformable models;orbital robotics;inspection;three dimensional;3 d robotic visual tracking;robot arm;robot vision;robot control;shape;optical tracking;end effector;target tracking;visual tracking;end effector deformable active models snakes 3 d robotic visual tracking servoing moving rigid object dynamic behavior elastic structures image gradient eye in hand robot arm configuration;deformable model;robot vision systems;deformable active models;cameras;moving rigid object;robot kinematics;active vision;dynamic behavior	A new approach for robotic visual tracking and servoing is discussed. Deformable active models are introduced as a powerful means for tracking a moving rigid object (an eye-in-hand robot arm is used). Deformable models imitate, in real-time, the dynamic behavior of elastic structures. These computer-generated models are designed to capture the silhouette of rigid objects with well defined boundaries in terms of image gradient. By means of an eye-in-hand robot arm configuration, the desired motion of the end-effector is computed with the objective of keeping the target's position and shape invariant with respect to the camera frame. Experimental results are presented for the tracking of a rigid object moving in the three-dimensional space. >	robot;video tracking	Philippe A. Couvignou;Nikolaos Papanikolopoulos;Pradeep K. Khosla	1993		10.1109/CVPR.1993.341156	three-dimensional space;computer vision;robot end effector;simulation;robotic arm;active vision;inspection;image gradient;eye tracking;shape;computer science;robot control;robot kinematics	Robotics	50.27376469630393	-40.27371708738841	53046
d139f4f00c497902196406ab58b07ca6e700a8a0	traffic monitoring based on real-time image tracking	tracking system;real time tracking;edge detection;road traffic;real time;kalman filter;computer vision;traffic information real time system image tracking system automatic traffic monitoring edge detection module ccd image processing feature extraction vehicle tracking system kalman filtering multilane moving vehicles car accident detection;vehicles image edge detection traffic control computerized monitoring printed circuits charge coupled devices image processing feature extraction image resolution pixel;feature extraction;proceedings paper;computer vision road traffic real time systems edge detection feature extraction road vehicles target tracking;vehicle tracking;traffic monitoring;target tracking;active contour model;road vehicles;real time systems	This paper presents a study on a stand-alone image tracking system for automatic traffic monitoring. The proposed image tracker consists of three parts: an edge detection module, an image tracking module and a traffic monitoring module. The edge-detection module is a special designed circuit board, which features fast CCD image processing and feature extraction. Frame rate (60 Hz) edge detection of an image with resolution of 320/spl times/240 pixels is obtained on-board. The image tracking module performs the vehicle tracking in real time. Adopting active contour models and Kalman filtering techniques, we successfully achieved real-time image tracking of multi-lane moving vehicles. The traffic-monitoring module determines the traffic information from the tracked locations of vehicles. The current system provides three types of traffic information: the velocity of multi-lane vehicles, the number of vehicles and car accident detection. Experimental results are presented to demonstrate the real-time tracking of vehicles on an urban artery.	real-time clock	Ching-Po Lin;Jen-Chao Tai;Kai-Tai Song	2003		10.1109/ROBOT.2003.1241902	kalman filter;embedded system;computer vision;feature detection;simulation;edge detection;tracking system;feature extraction;computer science;engineering;active contour model	Robotics	43.717547513651276	-42.51027149795982	53077
0c2a011ffe4aae6cf7dfffd5719b3e735721b0b9	multiple target tracking with motion priors	frequent occlusion;corresponding discrete distribution field;particle filter tracker;particle filter-based approach;estimation scheme;target identity;camera field;hybrid motion model;motion prior;multiple target tracking	This paper presents a particle filter-based approach for multiple target tracking in video streams in single static cameras settings. We aim in particular to manage mid-dense crowds situations, where, although tracking is possible, it is made complicated by the presence of frequent occlusions among targets and with scene clutter. Moreover, the appearance of targets is sometimes very similar, which makes standard trackers often switch their target identity. Our contribution is two-fold: (1) we first propose an estimation scheme for motion priors in the camera field of view, that integrates sparse optical flow data and regularizes the corresponding discrete distribution fields on velocity directions and magnitudes; (2) we use these motion priors in a hybrid motion model for a particle filter tracker. Through several results on video-surveillance datasets, we show the pertinence of this approach.	clutter;high- and low-level;interaction;matrix regularization;netapp filer;optical flow;particle filter;relevance;sparse matrix;streaming media;velocity (software development)	Francisco Madrigal;Mariano Rivera;Jean-Bernard Hayet	2011		10.1007/978-3-642-25330-0_36	computer vision;simulation;control theory	Vision	46.20884701326243	-47.74171556383037	53264
222615623d2bde0fb56bbdcf4c0c2deec51d18cc	lane positioning in highways based on road-sign tracking using kalman filter	roads vehicles cameras feature extraction kalman filters tracking;trajectory analysis autonomous vehicle localization road sign recognition kalman filter;video signal processing image classification image motion analysis kalman filters object recognition object tracking traffic engineering computing;gps positioning lane positioning road sign tracking kalman filter vehicle localization driving lane recognition motion analysis pinhole camera model road sign classification orb features oriented fast and rotated brief features videos highway driving	This paper proposes a localization of a vehicle on highway in the cross-sectional direction for the purpose of recognizing the driving lane. By tracking road signs over the highway, the relative position between the vehicle and the sign is calculated and the absolute position is obtained based on the a priori known information of the road sign as traffic regulations for installation. The proposed method uses Kalman filter for road sign tracking, analyzes the motion using the pinhole camera model, and classifies the type of the road sign using ORB (Oriented fast and Rotated BRIEF) features. Then, the driving lane is recognized from the relative position of the vehicle with the road sign. The experiments performed on videos acquired from real-world highway driving demonstrate that the proposed method is capable of compensating the limit of GPS positioning.	cross-sectional data;database;experience;experiment;global positioning system;internationalization and localization;kalman filter;openmp;parallel computing;pinhole camera model;poor posture;real-time clock;smartphone;thresholding (image processing);uncrewed vehicle	Hyoungrae Kim;Jae-hong Lee;Hakil Kim;Daehyuk Park	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974282	computer vision;simulation	Robotics	44.43314755932196	-44.84173867711641	53299
095f62e68ee27094b88c04a32d0b7a5ca3a8b50b	egomotion estimation using assorted features	visual odometry;slam;structure from motion;tracking	We propose a novel minimal solver for recovering camera motion across two views of a calibrated stereo rig. The algorithm can handle any assorted combination of point and line features across the four images and facilitates a visual odometry pipeline that is enhanced by well-localized and reliably-tracked line features while retaining the well-known advantages of point features. The mathematical framework of our method is based on trifocal tensor geometry and a quaternion representation of rotation matrices. A simple polynomial system is developed from which camera motion parameters may be extracted more robustly in the presence of severe noise, as compared to the conventionally employed direct linear/subspace solutions. This is demonstrated with extensive experiments and comparisons against the 3-point and line-sfm algorithms.	algorithm;experiment;isometric projection;simultaneous localization and mapping;solver;system of polynomial equations;trifocal tensor;visual odometry;whole earth 'lectronic link	Vivek Pradeep;Jongwoo Lim	2011	International Journal of Computer Vision	10.1007/s11263-011-0504-5	computer vision;structure from motion;simulation;computer science;visual odometry;mathematics;tracking	Vision	52.914629673163674	-48.616202455007624	53391
6a44aff0331266504cacd6f76db9ae54ab56b280	eigenface-based super-resolution for face recognition	eigenvalues and eigenfunctions;image resolution;face recognition signal resolution image resolution image recognition surveillance computational complexity image reconstruction karhunen loeve transforms signal processing image processing;low resolution;face recognition;computational complexity;super resolution;high resolution imager;computational complexity eigenface based super resolution face recognition surveillance cameras resolution initial dimensionality reduction method reduced dimensional domain;dimensional reduction;eigenvalues and eigenfunctions face recognition image resolution	Face images that are captured by surveillance cameras usually have a very low resolution, which significantly limits the performance of face recognition systems. In the past, super-resolution techniques have been proposed that attempt to increase the resolution by combining information from multiple images. These techniques use super-resolution as a preprocessing system to obtain a high resolution image that can later be passed to a face recognition system. Considering that most state-of-the-art face recognition systems use an initial dimensionality reduction method, we propose embedding the super-resolution algorithm into the face recognition system so that super-resolution is not performed in the pixel domain, but is instead performed in a reduced dimensional domain. The advantage of such an approach is a significant decrease in the computational complexity of the super-resolution algorithm because the algorithm no longer tries to construct a visually improved high quality image, but instead constructs the information required by the recognition algorithm directly in the lower dimensional domain without any unnecessary overhead.	eigenface;facial recognition system;super-resolution imaging	Bahadir K. Gunturk;Aziz Umit Batur;Yücel Altunbasak;Monson H. Hayes;Russell M. Mersereau	2002		10.1109/ICIP.2002.1040083	facial recognition system;computer vision;speech recognition;image resolution;computer science;pattern recognition;three-dimensional face recognition;sub-pixel resolution	Vision	40.83998092067529	-50.649638555618566	53419
15c584982ab3563a8e140c5b5de42a8ab8f441b5	hierarchical part-based visual object categorization	detectors;bottom up;probability;generic model;solid modeling biological system modeling detectors robustness humans shape computer vision geometry joining processes design methodology;object categorization;biological system modeling;geometry;computational geometry;image classification;computer vision;hierarchical part based visual object categorization;scale invariant keypoint based local features;shape;expectation maximization;probability image classification computational geometry;probabilistic spatial relations;local features;solid modeling;location scale pyramids;joining processes;generative model;robustness;humans;robust bottom up voting;scale invariance;design methodology;expectation maximization hierarchical part based visual object categorization generative model probabilistic spatial relations scale invariant keypoint based local features robust bottom up voting location scale pyramids	We propose a generative model that codes the geometry and appearance of generic visual object categories as a loose hierarchy of parts, with probabilistic spatial relations linking parts to subparts, soft assignment of subparts to parts, and scale invariant keypoint based local features at the lowest level of the hierarchy. The method is designed to efficiently handle categories containing hundreds of redundant local features, such as those returned by current key-point detectors. This robustness allows it to outperform constellation style models, despite their stronger spatial models. The model is initialized by robust bottom-up voting over location-scale pyramids, and optimized by expectation-maximization. Training is rapid, and objects do not need to be marked in the training images. Experiments on several popular datasets show the method's ability to capture complex natural object classes.	3d modeling;benchmark (computing);categorization;cluster analysis;code;database;discriminant;elementary;expectation–maximization algorithm;experiment;generative model;machine learning;multitier architecture;outline of object recognition;pyramid (geometry);sensor;supervised learning	Guillaume Bouchard;Bill Triggs	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.174	computer vision;detector;contextual image classification;design methods;expectation–maximization algorithm;computational geometry;shape;computer science;machine learning;scale invariance;pattern recognition;top-down and bottom-up design;probability;mathematics;geometry;solid modeling;generative model;robustness	Vision	45.57198477844331	-51.87666940022212	53477
184f8b223387082c7fd002932f2e880ae3d9dfa9	std: a stereo tracking dataset for evaluating binocular tracking algorithms	mobile robots;correlation;target tracking;robot vision systems;calibration;cameras;videos	In this paper, a Stereo Tracking Dataset is proposed for evaluating binocular tracking algorithms. The dataset contains stereoscopic videos which are collected by our mobile platform in different scenarios and videos that are available publicly. All sequences are carefully synchronized and rectified, and the ground truth of object is annotated by authors. Both raw and processed sequences are provided in the dataset. We also develop a Scalable and Occlusion-aware Multi-cues Correlation Filter Tracker (SOMCFT) and evaluate it on the STD. The SOMCFT framework fuses different clues in confidence map level and uses depth information to handle scale changes and occlusion. Quantitative evaluation on STD demonstrates effectiveness of the proposed dataset. All data, including stereo image pairs, calibrations, annotations and attributes, are available for research purposes and comparative evaluation on https://github.com/zhengzhugithub/StereoTracking.	algorithm;benchmark (computing);binocular vision;bittorrent tracker;confidentiality;ground truth;mobile operating system;rectifier (neural networks);std bus;stereoscopy	Zheng Zhu;Wei Zou;Qingbin Wang;Feng Zhang	2016	2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2016.7866659	mobile robot;computer vision;calibration;simulation;computer science;correlation;computer graphics (images)	Vision	52.45264311461477	-42.387133957191814	53601
30568c7206d484b81241bb373e6b0e98b07e7522	cascaded confidence filtering for improved tracking-by-detection	psi_visics;detection algorithm;object detection;object model	We propose a novel approach to increase the robustness of object detection algorithms in surveillance scenarios. The cascaded confidence filter successively incorporates constraints on the size of the objects, on the preponderance of the background and on the smoothness of trajectories. In fact, the continuous detection confidence scores are analyzed locally to adapt the generic detector to the specific scene. The approach does not learn specific object models, reason about complete trajectories or scene structure, nor use multiple cameras. Therefore, it can serve as preprocessing step to robustify many tracking-by-detection algorithms. Our real-world experiments show significant improvements, especially in the case of partial occlusions, changing backgrounds, and similar distractors.	algorithm;experiment;microsoft customer care framework;microsoft windows;object detection;preprocessor;robustification	Severin Stalder;Helmut Grabner;Luc Van Gool	2010		10.1007/978-3-642-15549-9_27	computer vision;object model;computer science;viola–jones object detection framework;machine learning;pattern recognition;programming language	Vision	41.83011149253707	-48.06109357882579	53669
5d7327dc2df1ac1997615bad9fbb64a9970493bd	argos-venice boat classification	irrigation;measurement;boats irrigation measurement accuracy benchmark testing feature extraction cameras;accuracy;feature extraction;vehicle tracking intelligent surveillance systems publicly available data set maritime domain boat classification data sets argos system vehicle classification vehicle detection;object tracking boats image classification object detection;benchmark testing;cameras;boats	Detection, classification, and tracking of people and vehicles are fundamental processes in intelligent surveillance systems. The use of publicly available data set is the appropriate way to compare the relative merits of existing methods and to develop and assess new robust solutions. In this paper, we focus on the maritime domain and we describe the generation of boat classification data sets, containing images of boats automatically extracted by the ARGOS system, operating 24/7 in Venice, Italy. The data sets are unique in their nature, since they come from an incomparable environment like Venice, but they present very interesting challenges to vehicle classification, due to changes in the environmental conditions, boat wakes, waves, reflections, etc. We thus believe that robust techniques, validated through the ARGOS Boat Classification data sets, will improve the development and deployment of solutions in similar applications related to vehicle detection and classification.	baseline (configuration management);benchmark (computing);computer vision;experiment;geographical operations system;machine learning;pattern recognition;performance evaluation;reflection (computer graphics);software deployment;statistical classification;streaming media;test set;wake	Domenico Daniele Bloisi;Luca Iocchi;Andrea Pennisi;Luigi Tombolini	2015	2015 12th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2015.7301727	benchmark;computer vision;simulation;feature extraction;computer science;data mining;irrigation;accuracy and precision;measurement;statistics	Robotics	42.00884714198408	-41.868500232772014	54062
c0ecd342fd9d5161a83a3c66c80c6e0b8718fa18	prin: pointwise rotation-invariant network		In recent years, point clouds have earned quite some research interests by the development of depth sensors. Due to different layouts of objects, orientation of point clouds is often unknown in real applications. In this paper, we propose a new point sets learning framework named Pointwise Rotation-Invariant Network (PRIN), focusing on the rotation problem in point clouds. We construct spherical signals by adaptive sampling from sparse points and employ spherical convolutions, together with tri-linear interpolation to extract rotation-invariant features for each point. Our network can be applied in applications ranging from object classification, part segmentation, to 3D feature matching and label alignment. PRIN shows similar performance on par or better than state-of-the-art methods on part segmentation without data augmentation. We provide theoretical analysis for what our network has learned and why it is robust to input rotation. Our code is available online1.		Yang You;Yujing Lou;Qi Liu;Lizhuang Ma;Weiming Wang;Yu-Wing Tai;Cewu Lu	2018	CoRR			Vision	53.229106031834064	-43.79338147647677	54082
beaa6106071358f6b911a0da72d6a29f568dbd01	back-projective priming: toward efficient 3d model-based object recognition via preemptive top-down constraints		This paper describes a novel framework for context-based object-recognition/pose-estimation. High-level geometric constraints are used to optimize fitting of a 3d model to a 2d image through a process termed “back-projective priming”. A practical problem in robotics, electrical outlet discovery, is used for testing. The robot, experimental setup, and ongoing/future work are described.	3d modeling;outline of object recognition;robot;robotics	Ryan Dellana	2015			geometric hashing;perspective distortion;time complexity;priming (psychology);pose;mobile robot;cognitive neuroscience of visual object recognition;computer vision;heuristic;artificial intelligence;computer science	Robotics	49.509034729157726	-39.777149395777094	54087
3702df875af850c6c89f60ed9ec1ee0d1dc75261	a new way to use hidden markov models for object tracking in video sequences	image colour analysis hidden markov models image sequences object detection object recognition genetic algorithms;object recognition;hidden markov model;forward algorithm hidden markov models color object tracking video sequence spatial domain multidimensional data object learning ghosp algorithm object position prediction object position localisation object recognition;hidden markov models video sequences multidimensional systems pattern recognition character generation mathematical model motion estimation object recognition random variables stochastic processes;hidden markov models;image colour analysis;object tracking;genetic algorithms;object detection;image sequences	In this paper, we are dealing with color object tracking. We propose to use Hidden Markov Models in a different way as classical approaches. Indeed, we use these mathematical tools to model the object in the spatial domain rather than in the temporal domain. Besides in order to manage multidimensional (color) data, Multidimensional Hidden Markov Models are involved. Object learning step is performed using the GHOSP algorithm whereas object tracking step is done by approximate object position prediction and then precise object position localisation. This last step can be seen as an object recognition problem and will be solved using a method based on the Forward algorithm.	approximation algorithm;forward algorithm;hidden markov model;markov chain;outline of object recognition	Sébastien Lefèvre;Emmanuel Bouton;Thierry Brouard;Nicole Vincent	2003		10.1109/ICIP.2003.1247195	forward algorithm;computer vision;genetic algorithm;object model;computer science;viola–jones object detection framework;cognitive neuroscience of visual object recognition;machine learning;video tracking;pattern recognition;3d single-object recognition;markov model;hidden markov model	Vision	45.877178302376336	-49.635088887844695	54108
88555911eddf80f5c7f63f09cc79eeba646fc830	otwc: an efficient object-tracking method		Detection and tracking of moving objects is an important topic in computer vision and has turned into an active field of research with remarkable recent progress. This paper proposes OTWC; an efficient object-tracking method with Code Book (CB) model; a high-speed method with proper accuracy for detection and tracking moving objects. Our proposed method combines CB (Sigari and Fathy in Proceedings of the international multiconference of engineers and computer scientists, pp 19–21, 2008) algorithm, density-based spatial clustering of applications with noise (DBSCAN) (Ester et al. in Proceedings of 24th VLDB conference, 1998) clustering algorithm, and incremental bulk DBSCAN algorithm in order to effectively track moving objects. We have claimed that CB algorithm models the background and foreground simultaneously so that it is possible to use the wasted information in the background estimation step in order to incrementally detect and track the foreground. We have also compared our method with other available alternatives including a general algorithm for tracking two datasets. The results demonstrate that our proposed algorithm has an acceptable performance in terms of both accuracy and speed.		Maryam Koohzadi;Mohammad Reza Keyvanpour	2015	Signal, Image and Video Processing	10.1007/s11760-013-0557-8	computer science;artificial intelligence;machine learning;data mining;algorithm;statistics	EDA	41.12458540251251	-43.55466631192553	54126
2e7adda34e92e09197e25fcee6a319fe2b09368e	plane object-based high-level map representation for slam		High-level map representation providing object-based understanding of the environment is an important component for SLAM. We present a novel algorithm to build plane object-based map representation upon point cloud that is obtained in real–time from RGB-D sensors such as Kinect. On the basis of segmented planes in point cloud we construct a graph, where a node and edge represent a plane and its real intersection with other plane, respectively. After that, we extract all trihedral angles (corners) represented by 3rd order cycles in the graph. Afterwards, we execute systematic aggregation of trihedral angles into object such as trihedral angles of the same plane-based object have common edges. Finally, we classify objects using simple subgraph patterns and determine their physical sizes. Our experiments figured out that the proposed algorithm reliably extracts objects, determines their physical sizes and classifies them with a promising performance.	object-based language;simultaneous localization and mapping	Pavel Gritsenko;Igor Gritsenko;Askar Seidakhmet;Bogdan Kwolek	2018		10.1007/978-3-030-00692-1_9	point cloud;computer vision;artificial intelligence;rgb color model;pattern recognition;computer science;graph	Robotics	50.21936255759011	-43.82657407092089	54314
91383bda13436988d35407ecf03eece73c992d09	a novel linear technique to estimate the epipolar geometry	parameter estimation computational geometry computer vision image reconstruction matrix algebra feature extraction;application software;epipolar geometry estimation;performance;computational geometry;fundamental matrix;layout;matrix algebra;data mining;information geometry;computer vision;linear algorithm;epipolar geometry;real images epipolar geometry estimation 3d scene reconstruction projections camera scene geometry estimation computer vision fundamental matrix linear algorithm performance;feature extraction;image reconstruction;3d scene reconstruction;parameter estimation;real images;real time application;robot vision systems;calibration;cameras;camera scene geometry estimation;layout cameras computer vision computational geometry application software robot vision systems data mining calibration image reconstruction information geometry;projections	The accurate reconstruction of the 3D scene structure from two different projections and the estimation of the camera scene geometry is of paramount importance in many computer vision tasks. Most of the information about the camera-scene geometry is encapsulated in the Fundamental Matrix. Estimating the Fundamental Matrix has been an object of research for many years and continues to be a challenging task in current computer vision systems. While nonlinear iterative approaches have been successful in dealing with the high instability of the underlying problem, their inherent large workload makes these approaches inappropriate for real-time applications. In this paper practical aspects of highly efficient linear methods are studied and a novel low-cost and accurate linear algorithm is introduced. The performance of the proposed approach is assessed by several experiments on real images.	algorithm;computer vision;epipolar geometry;experiment;fundamental matrix (computer vision);instability;iterative method;nonlinear system;real-time clock;real-time computing	Ebroul Izquierdo;Valia Guerra-Ones	2001		10.1109/ICASSP.2001.941255	iterative reconstruction;layout;computer vision;application software;calibration;performance;feature extraction;computational geometry;computer science;machine learning;mathematics;fundamental matrix;estimation theory;real image;information geometry;epipolar geometry;computer graphics (images)	Vision	52.14593305815148	-49.12549931433983	54319
c993fcd7dc835d906157aae1dfbcaca1582b2dfc	locnet: global localization in 3d point clouds for mobile robots		Global localization in 3D point clouds is a challenging problem of estimating the pose of robots without priori knowledge. In this paper, a solution to this problem is presented by achieving place recognition and metric pose estimation in the global priori map. Specifically, we present a semi-handcrafted representation learning method for LIDAR point cloud using siamese LocNets, which states the place recognition problem to a similarity modeling problem. With the final learned representations by LocNet, a global localization framework with range-only observations is proposed. To demonstrate the performance and effectiveness of our global localization system, KITTI dataset is employed for comparison with other algorithms, and also on our own multi-session datasets collected by long-time running robot to see the performance under semistatic dynamics. The result shows that our system can achieve both high accuracy and efficiency.	3d pose estimation;algorithm;feature learning;machine learning;mobile robot;point cloud;semiconductor industry;similarity measure	Huan Yin;Yue Wang;Li Tang;Xiaqing Ding;Rong Xiong	2017	CoRR		point cloud;computer vision;mobile robot;robot;pose;lidar;computer science;artificial intelligence;feature learning	Robotics	52.994822275129785	-42.54342552062652	54451
36f129118c8b2b82a21d26edc8b13aa8f8962baf	a multiple-feature and multiple-kernel scene segmentation algorithm for humanoid robot	image segmentation feature extraction humanoid robots image edge detection support vector machines kernel robot sensing systems;support vector machine humanoid robot interval type 2 fuzzy c means multiple kernel;clustering validity multiple feature scene segmentation algorithm multiple kernel scene segmentation algorithm humanoid robot multiple feature and multiple kernel support vector machine methodology mfmk svm methodology pixel wise intensity c1 smf feature extraction local homogeneity model gabor filter feature validity interval type 2 fuzzy c means clustering algorithm fv it2fcm type 2 fuzzy criterion clustering optimization process iterative optimization;support vector machines feature extraction filtering theory fuzzy set theory gabor filters humanoid robots image segmentation iterative methods optimisation pattern clustering robot vision	This paper presents a multiple-feature and multiple-kernel support vector machine (MFMK-SVM) methodology to achieve a more reliable and robust segmentation performance for humanoid robot. The pixel wise intensity, gradient, and C1 SMF features are extracted via the local homogeneity model and Gabor filter, which would be used as inputs of MFMK-SVM model. It may provide multiple features of the samples for easier implementation and efficient computation of MFMK-SVM model. A new clustering method, which is called feature validity-interval type-2 fuzzy C-means (FV-IT2FCM) clustering algorithm, is proposed by integrating a type-2 fuzzy criterion in the clustering optimization process to improve the robustness and reliability of clustering results by the iterative optimization. Furthermore, the clustering validity is employed to select the training samples for the learning of the MFMKSVM model. The MFMK-SVM scene segmentation method is able to fully take advantage of the multiple features of scene image and the ability of multiple kernels. Experiments on the BSDS dataset and real natural socene images demonstrate the superior performance of our proposed method.	algorithm;cluster analysis;computation;experiment;extraction;farmville;gabor filter;genetic heterogeneity;gradient;humanoid robot;image segmentation;information;iterative method;kernel (operating system);mathematical optimization;pixel;silo (dataset);statistical classification;support vector machine;user space;biologic segmentation;statistical cluster	Zhi Liu;Shuqiong Xu;Yun Zhang;C. L. Philip Chen	2014	IEEE Transactions on Cybernetics	10.1109/TSMC.2013.2297398	computer vision;kernel method;machine learning;pattern recognition;mathematics;scale-space segmentation	Vision	44.35636729598658	-50.91451068633293	54557
c09a0875a19aa89f7049c58b931f2c2b33480f7f	model-based feature refinement by ellipsoidal face tracking	pose estimation face recognition feature extraction image matching image motion analysis image reconstruction object tracking;image motion analysis;image matching;face recognition;feature extraction face vectors solid modeling tracking databases;feature extraction;image reconstruction;object tracking;boston face database model based feature refinement ellipsoidal face tracking head tracking local feature matching feature extraction motion constraints region based feature refinement method distance based feature refinement method ellipsoidal object tracking direct mapping method 3d feature position reconstruction face pose estimation;pose estimation	We describe a new method to relieve common assumptions/ restrictions in head tracking by using a model-based approach. This improves local feature matching which only considers the pattern around the extracted feature excluding the object shape, so that misalignment can occur. In this paper, to overcome constraints on motion we consider region- and distance-based feature refinement methods to validate the local features used when tracking the ellipsoidal object. We also present a direct mapping method to reconstruct 3D feature positions for tracking. The utility of the new method has been demonstrated for face pose estimation using the Boston face database.	feature model;motion capture;refinement (computing)	Sung-Uk Jung;Mark S. Nixon	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		iterative reconstruction;facial recognition system;computer vision;feature detection;pose;feature extraction;computer science;machine learning;video tracking;pattern recognition;feature	Vision	48.60903333778534	-48.691550117577414	54622
a9ca9a4ba0810aceb1aad49a5de6eebac345b871	building local floor map by use of ultrasonic and omni-directional vision sensor	omnidirectional vision;filtering theory mobile robots robot vision ultrasonic transducers sensor fusion;sensor fusion local floor map ultrasonic sensor omnidirectional vision sensor us sensor grid based free space robot conservative range information specular reflection color information edge information inverse perspective transformation metric grid based representation radial fusion processing indoor cluttered environment;floors sonar robot sensing systems orbital robotics optical reflection filtering sensor systems robot vision systems sensor fusion color;mobile robots;ultrasonic transducers;specular reflection;robot vision;sensor fusion;filtering theory;ultrasonic sensor	In this paper, we propose a new fusion approach which uses the ultrasonic sensor aided by an omnidirectional vision sensor to give a grid based free space around the robot. By use of the ultrasonic sensor, the robot can obtain a conservative range information based on our nearby range filtering method. This filtering can give a more reliable result considering the sensor's problem of specular reflection. Also, by use of the special omni-directional vision sensor we developed, the color and edge information can be obtained in a single picture and mapped to the ground plane by the inverse perspective transformation. Thus the range, color and edge information can all be expressed on a metric grid-based representation which forms the basis of our radial fusion processing. Results in an indoor cluttered environment are given which show the usefulness of our proposed sensor fusion approach.		Shih-Chieh Wei;Yasushi Yagi;Masahiko Yachida	1998		10.1109/ROBOT.1998.680725	computer vision;specular reflection;soft sensor;computer science;engineering;sensor fusion;ultrasonic sensor;optics;remote sensing	Robotics	51.468826559816584	-38.73739687011221	54981
8f9943dac5894754b7be2a1488e3d9dbeb720861	study on navigating path recognition for the greenhouse mobile robot based on k-means algorithm	image segmentation;image segmentation greenhouse robots path recognition hsi color space k means algorithm;robot vision greenhouses hough transforms image colour analysis image segmentation mobile robots navigation object recognition pattern clustering position control;green products image segmentation clustering algorithms navigation character recognition cameras robustness;greenhouse path recognition greenhouse mobile robot k means algorithm nonuniform illumination mobile robot navigation path recognition system hsi color space image processing image clustering segmentation morphological corrosion hough transform;path recognition;k means algorithm;hsi color space;greenhouse robots	In order to improve the robustness to the nonuniform illumination and the real-timness of the mobile robot navigation path recognition system in a greenhouse, firstly, the three components H, S and I are respectively separated from HSI color space, and the H component which has nothing to do with light intensity and can restrain effectively the effect of noise is extracted for the subsequent image processing. For the color characteristic of greenhouse environment, the clustering segmentation of the image is performed based on K-means algorithm to achieve the respective cluster of the path and green crop information. Then, the redundant and interference information existing in the clustered image is eliminated by a morphological corrosion so as to obtain the complete and clear path information. Compared with the conventional threshold segmentation methods, the proposed method can solve the problem of too large memory occupation and too long calculation time caused by the unclear segmentation information for the subsequent Hough transform, thus can enhance the rapidity of the greenhouse path recognition and meet the real-time requirements of automatic navigation and operation of the greenhouse robot. The experiment results show that for the greenhouse robot working in the environment with a complex background and variable light, the proposed method can significantly reduce the effect of the nonuniform illumination on the navigation, that is, has a good robustness to the nonuniform illumination. Furthermore, the processing time of a single image is reduced by 53.26%, so the rapidity of the path recognition can be significantly improved.	algorithm;autostereogram;cluster analysis;color space;horizontal situation indicator;hough transform;image processing;interference (communication);k-means clustering;mobile robot;real-time clock;requirement;robotic mapping	Guoqin Gao;Ming Li	2014	Proceedings of the 11th IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2014.6819668	computer vision;simulation;computer science;machine learning;segmentation-based object categorization;image segmentation;scale-space segmentation;k-means clustering	Robotics	47.991415619491356	-41.66132067813233	55036
3896526d063aa05013f3716831605a9f8ca6df5d	robust real-time visual odometry for stereo endoscopy using dense quadrifocal tracking	ucl;discovery;theses;conference proceedings;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;ucl research	Visual tracking in endoscopic scenes is known to be a difficult task due to the lack of textures, tissue deformation and specular reflection. In this paper, we devise a real-time visual odometry framework to robustly track the 6-DoF stereo laparoscope pose using the quadrifocal relationship. The instant motion of a stereo camera creates four views which can be constrained by the quadrifocal geometry. Using the previous stereo pair as a reference frame, the current pair can be warped back by minimising a photometric error function with respect to a camera pose constrained by the quadrifocal geometry. Using a robust estimator can further remove the outliers caused by occlusion, deformation and specular highlights during the optimisation. Since the optimisation uses all pixel data in the images, it results in a very robust pose estimation even for a textureless scene. The quadrifocal geometry is initialised by using real-time stereo reconstruction algorithm which can be efficiently parallelised and run on the GPU together with the proposed tracking framework. Our system is evaluated using a ground truth synthetic sequence with a known model and we also demonstrate the accuracy and robustness of the approach using phantom and real examples of endoscopic augmented reality.	algorithm;augmented reality;graphics processing unit;ground truth;imaging phantom;key frame;mathematical optimization;pixel;real-time clock;reference frame (video);simultaneous localization and mapping;specular highlight;stereo camera;synthetic intelligence;visual odometry	Ping-Lin Chang;Ankur Handa;Andrew J. Davison;Danail Stoyanov;Philip J. Edwards	2014		10.1007/978-3-319-07521-1_2	computer vision;simulation;geography;computer graphics (images)	Vision	52.39173751714209	-47.95892542191742	55181
9326fbb671e10a54a25c7507a378c045636b035a	lanes detection based on unsupervised and adaptive classifier	lanes classifier;roads vehicles cameras brightness classification algorithms videos transforms;road detection;video signal processing;kalman filters;video signal processing hough transforms image classification kalman filters object detection object tracking parameter estimation tensors traffic engineering computing;lanes detection;image classification;kalman filter;kalman filter lanes detection lanes classifier road detection;object tracking;hough transforms;traffic engineering computing;lane tracking road lane detection unsupervised classifier adaptive classifier videos lane brightness hsv image hough transform left road line right road line line middle point line slope tensor kalman filter class parameter estimation;parameter estimation;object detection;tensors	This paper describes an algorithm to detect the road lanes based on an unsupervised and adaptive classifier. We have selected this classifier because in the road we do not know the parameters of lanes, although we know that lanes are there, only they need to be classified. First of all, we tested and measured the brightness of the lanes of the road in many videos. Generally, the lines on the road are white. We used the HSV image and we improved the region of study. Then, we used a Hough transform which yields a set of possible lines. These lines have to be classified. The classifier starts with initial parameters because we suppose that the vehicle is on road and in the center of the lane. There are two classes, the first one is the left road line and the second one is the right road line. Each line has two parameters that are: middle point of line and the line slope. These parameters will be changing in order to adjust to the real lanes. A tensor holds the two lines, so these lines will not separate more than the tensor allows. A Kalman filter estimates the new class's parameters and improves the tracking of the lanes. Finally, we use a mask in order to highlight the lane and show to the user a better image.	algorithm;hough transform;kalman filter;line printer daemon protocol;unsupervised learning	Andres F. Cela;Luis Miguel Bergasa;Franklin L. Sanchez;Marco A. Herrera	2013	2013 Fifth International Conference on Computational Intelligence, Communication Systems and Networks	10.1109/CICSYN.2013.40	kalman filter;computer vision;simulation;computer science;machine learning	Vision	41.88660333258132	-45.272566197057834	55276
3de88529f3e27d888d99307f2fb49a0edde168a2	modelling of anal sphincter tone based on pneumatic and cable-driven mechanisms		Motivated by the need for improving a haptics-based simulation tool for learning and training digital rectal examinations, a sphincter tone model and its actuation is conceived and developed. Two approaches are presented: one based on pneumatics actuation and the other using cable-driven mechanical actuation using servo motors. Clinical scenarios are modelled as profiles based on studies of anorectal manometry and adapted with clinical input. Both designed mechanisms and scenarios were experimentally evaluated by six experts, Nurse Practitioners in Continence and Colorectal Surgeons. Results show that both mechanisms produce enough pressure on examining finger and profiles are able to generate a wide range of healthy and abnormal cases. Either approach could be used to provide a more realistic experience during training of sphincter tone assessment.	comstock–needham system;experiment;haptic technology;medical ultrasound;servo;simulation	Luc Marechal;Alejandro Granados;Lilian Ethapemi;Shengyang Qiu;Christos Kontovounisios;Christine Norton;Fernando Bello	2017	2017 IEEE World Haptics Conference (WHC)	10.1109/WHC.2017.7989931	simulation;sphincter tone;control engineering;computer science;anorectal manometry;actuator;haptic technology;anal sphincter tone;servomotor;pneumatics;colorectal surgeons	Robotics	40.15689962439741	-38.27211095724277	55376
6e31b38bc36ede2129a8120abc86206893b8950c	estimating the fundamental matrix using second-order cone programming	second order cone programming;multiview geometry;optimal algorithms;fundamental matrix;l infinity norm minimization	Computing the fundamental matrix is the first step of many computer vision applications including camera calibration, image rectification and structure from motion. A new method for the estimation of the fundamental matrix from point correspondences is presented. The minimization of the geometric error is performed based Linfinity norm minimization framework. A single global minimum exists and it may be found by SOCP (Second-Order Cone Programming), which is a standard technique in convex optimization. In a SOCP a linear function is minimized over the intersection of an affine set and the product of second-order (quadratic) cones. Several efficient primal-dual interior-point methods for SOCP have been developed. Experiments on real images show that this method provides a more accurate estimate of the fundamental matrix and superior to previous approaches, and the method is no need for normalization of the image coordinates.	camera resectioning;computer vision;convex optimization;experiment;fundamental matrix (computer vision);image rectification;interior point method;linear function;mathematical optimization;maxima and minima;second-order cone programming;structure from motion;t-norm	Min Yang	2011		10.1007/978-3-642-23896-3_72	mathematical optimization;mathematical analysis;second-order cone programming;computer science;mathematics;geometry;fundamental matrix;state-transition matrix	Vision	51.07715760798556	-50.97445752300078	55695
83b60f23aa2c84499cfee71bbf4f0d71f5142d20	computing object-based saliency in urban scenes using laser sensing	graph theory;traffic signs;laser sensing;measurement by laser beam;object detection object based saliency computation urban scenes laser sensing 3d laser points laser scanners object location traffic signs road lamps geometric feature extraction graph matching based method;sensors;optical scanners;urban scenes;object location;geometric feature extraction;computational geometry;laser scanner;mobile robots;geometric feature;graph matching;road lamps;robot vision;vectors;feature extraction vectors sensors laser modes merging measurement by laser beam;3d laser points;range image;feature extraction;laser scanners;merging;object based saliency computation;graph matching based method;laser modes;object detection;robot vision computational geometry feature extraction graph theory mobile robots object detection optical scanners	It becomes a well-known technology that a low-level map of complex environment containing 3D laser points can be generated using a robot with laser scanners. Given a cloud of 3D laser points of an urban scene, this paper proposes a method for locating the objects of interest, e.g. traffic signs or road lamps, by computing object-based saliency. Our major contributions are: 1) a method for extracting simple geometric features from laser data is developed, where both range images and 3D laser points are analyzed; 2) an object is modeled as a graph used to describe the composition of geometric features; 3) a graph matching based method is developed to locate the objects of interest on laser data. Experimental results on real laser data depicting urban scenes are presented; efficiency as well as limitations of the method are discussed.	3d scanner;algorithm;algorithmic efficiency;ct scan;computation;high- and low-level;map;matching (graph theory);missing data;object detection;object-based language;online and offline;point cloud;range imaging	Yipu Zhao;Mengwen He;Huijing Zhao;Franck Davoine;Hongbin Zha	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224940	laser scanning;mobile robot;computer vision;simulation;feature extraction;computational geometry;computer science;sensor;graph theory;matching;remote sensing	Robotics	51.16661449086985	-42.11105113136265	55941
1a734c0304f6d06d9c46e863d67d254e1ee8ba7b	rapid face detection in static video using background subtraction	detectors;video sequence;texture;image resolution;video sequences;computational modeling;background subtraction;face;face detection;adaptation models	This paper presents a method that combines the background subtraction with the Viola-Jones face detector to detect human faces from video sequence captured by a fixed camera. We use a texture based method for background subtraction to extract foreground sub-images for the face detector. It allows the detector to focus on face detection on smaller image regions and thereby reduces its computational cost and lower its false positive rate. Compared to other state-of-the-art techniques, the performance of our method is promising.	algorithmic efficiency;background subtraction;computation;face detection;image resolution;jones calculus;sensor	Wencai Zou;Yao Lu;Mukai Chen;Feng Lv	2014	2014 Tenth International Conference on Computational Intelligence and Security	10.1109/CIS.2014.146	face;computer vision;detector;face detection;object-class detection;image resolution;background subtraction;computer science;multimedia;texture;computational model;computer graphics (images)	Vision	40.857550865526264	-50.43865659303467	55988
e62b28e8f3d197ee83e495e45d890d78e2a78f42	non-rigid point set registration by preserving global and local structures	image registration approximation theory gaussian processes hilbert spaces;probabilistic approach nonrigid point set registration local structures global structures reproducing kernel hilbert space sparse approximation gaussian mixture models;kernel;shape manganese data models context robustness mixture models kernel;manganese;shape;robustness;mixture models;global local registration shape matching non rigid gaussian mixture model;context;data models	In previous work on point registration, the input point sets are often represented using Gaussian mixture models and the registration is then addressed through a probabilistic approach, which aims to exploit global relationships on the point sets. For non-rigid shapes, however, the local structures among neighboring points are also strong and stable and thus helpful in recovering the point correspondence. In this paper, we formulate point registration as the estimation of a mixture of densities, where local features, such as shape context, are used to assign the membership probabilities of the mixture model. This enables us to preserve both global and local structures during matching. The transformation between the two point sets is specified in a reproducing kernel Hilbert space and a sparse approximation is adopted to achieve a fast implementation. Extensive experiments on both synthesized and real data show the robustness of our approach under various types of distortions, such as deformation, noise, outliers, rotation, and occlusion. It greatly outperforms the state-of-the-art methods, especially when the data is badly degraded.	distortion;entity name part qualifier - adopted;experiment;hidden surface determination;hilbert space;kernel;mixture model;muscle rigidity;normal statistical distribution;point set registration;probability;shape context;sparse approximation;sparse matrix;density;registration - actclass	Jiayi Ma;Ji Zhao;Alan L. Yuille	2016	IEEE Transactions on Image Processing	10.1109/TIP.2015.2467217	point set registration;mathematical optimization;kernel;shape;computer science;manganese;machine learning;pattern recognition;mathematics;robustness	Vision	46.950434388768095	-51.55109075417962	56030
af684f8d740a1172a946f41ec3ce884fc1f10773	measurement of 3d motion parameters from range images	robot sensing systems;motion range equation 3d motion detection motion parameter measurement statistical analysis pattern recognition range images;picture processing;computational method;image sensors;three dimensional;motion range equation;robot vision;statistical analysis;statistical analysis pattern recognition picture processing;range image;performance analysis;range images;motion parameter measurement;pattern recognition;motion measurement equations motion detection machinery robot vision systems object detection statistical analysis performance analysis robot sensing systems image sensors;3d motion detection;motion measurement;machinery;motion detection;robot vision systems;object detection	Detecting three dimensional motion is one of the most important issues on robot vision. In this paper, an essential equation, the motion-range equation, is introduced for detecting 3D motion from 'range images'. The equation is represented in linear form against 3D motion parameters. Using the equation, the motion parameters are directly obtained without detecting the positions of an object. Statistical analysis is made and computational method is established by means of the equation. Analysis of the equation is performed. Experimental results on actual range images prove that the motion detection method described is simple and efficient. >		Tamio Arai;Kazunori Umeda	1991		10.1109/IROS.1991.174480	control engineering;three-dimensional space;computer vision;machine;structure from motion;simulation;computer science;motion estimation;image sensor;motion field;linear motion	Vision	51.900584393185	-40.74523099577065	56163
4959843fe588d1c4f23a4ae0fac07ff6a639d037	fusion of stereo and structure from motion for enhancing patchmatch stereo	cost function;kitti datasets patchmatch stereo enhancement method disparity estimation methods cost fitting subpixel disparity value estimation fronto parallel assumption window matching stereo correspondence algorithms 3d planes structure from motion information hci;cost function estimation three dimensional displays robustness cameras transforms;estimation;three dimensional displays;transforms;robustness;visual databases image enhancement image matching motion estimation stereo image processing;cameras	The main issues with classic disparity estimation methods are the limitations of cost fitting for estimating subpixel disparity values and the frequent violations of the fronto-parallel assumption during support window matching. Modern stereo correspondence algorithms model the scene as a collection of 3D planes and estimate the real-valued parameters of each plane in order to obtain a more accurate disparity map. Such an algorithm is PatchMatch stereo that overcomes the problem of searching in an infinite, high dimensional solution space by efficiently traversing the space based on the assumption that planes are similar in a neighborhood region. This work presents a method that integrates structure from motion information with stereo for increasing the robustness of the original PatchMatch stereo method on difficult scenarios. Evaluations of our method on the HCI and Kitti datasets show that our method returns an accurate, denser disparity map.	algorithm;binocular disparity;computation;feasible region;human–computer interaction;iteration;loss function;patchmatch;pixel;second generation multiplex;structure from motion	Claudiu Decean;Sergiu Nedevschi	2015	2015 IEEE International Conference on Intelligent Computer Communication and Processing (ICCP)	10.1109/ICCP.2015.7312627	computer stereo vision;computer vision;simulation;mathematics;computer graphics (images)	Robotics	51.57707978578511	-49.337386105489195	56210
400dca736b1571bb9399e0b0ccdce8e165ae37ff	recording the region of interest from flycam panoramic video	flycam system;videoconferencing;teleconferencing;high resolution;seminars;video signal processing;videoconference;real time;computer aided instruction;real time recording region of interest tracking video recording flycam system high resolution sequences wide angle video sequences multiple stationary cameras video frame stitching classroom lectures videoconferencing kalman filter parameter estimation virtual camera control;kalman filters;kalman filter;video sequences;camera control;tracking filters;layout;method integration;computer vision;multiple stationary cameras;video indexing;camera motion;parameter estimation tracking filters kalman filters video recording computer vision teleconferencing computer aided instruction image sequences video signal processing;real time recording;video conferencing;wide angle video sequences;region of interest;video recording;video frame stitching;robustness;humans;microphone arrays;region of interest tracking;parameter estimation;target tracking;high resolution sequences;video recording cameras videoconference target tracking video sequences seminars humans robustness microphone arrays layout;virtual camera control;cameras;image sequences;classroom lectures	A novel method for region of interest tracking and recording video is presented. The proposed method is based on the FlyCam system [4], which produces high resolution and wide-angle video sequences by stitching the video frames from multiple stationary cameras. The method integrates tracking and recording processes, and targets applications such as classroom lectures and video conferencing. First, the region of interest (which typically covers the speaker) is tracked using a Kalman filter. Then, the Kalman filter estimation results are used for virtual camera control and to record the video. The system has no physical camera motion and the virtual camera parameters are readily available for video indexing. The proposed system has been implemented for real time recording of lectures and presentations.	image resolution;kalman filter;region of interest;stationary process;virtual camera system	Xinding Sun;Jonathan Foote;Don Kimber;B. S. Manjunath	2001		10.1109/ICIP.2001.959040	video compression picture types;computer vision;video;computer science;video capture;video tracking;multimedia;video processing;videoconferencing;motion compensation;three-ccd camera;multiview video coding;computer graphics (images)	Vision	47.575343564369604	-45.19278753049165	56267
20e9a988678a3de84637da91372ca63080560b9d	robust 3d reconstruction with an rgb-d camera	three dimensional displays feature extraction image reconstruction geometry robustness image registration cameras	We present a novel 3D reconstruction approach using a low-cost RGB-D camera such as Microsoft Kinect. Compared with previous methods, our scanning system can work well in challenging cases where there are large repeated textures and significant depth missing problems. For robust registration, we propose to utilize both visual and geometry features and combine SFM technique to enhance the robustness of feature matching and camera pose estimation. In addition, a novel prior-based multicandidates RANSAC is introduced to efficiently estimate the model parameters and significantly speed up the camera pose estimation under multiple correspondence candidates. Even when serious depth missing occurs, our method still can successfully register all frames together. Loop closure also can be robustly detected and handled to eliminate the drift problem. The missing geometry can be completed by combining multiview stereo and mesh deformation techniques. A variety of challenging examples demonstrate the effectiveness of the proposed approach.	3d pose estimation;3d reconstruction;algorithm;bundle adjustment;closure;computation;feature extraction;feature model;frame (physical object);graphics processing unit;kinect;loopback;matching;numerous;parallel computing;random sample consensus;real-time clock;real-time computing;real-time transcription;refinement (computing);scanning probe microscopes (device);registration - actclass	Kangkan Wang;Guofeng Zhang;Hujun Bao	2014	IEEE Transactions on Image Processing	10.1109/TIP.2014.2352851	computer vision;feature detection;computer science	Vision	52.53411874561963	-46.600954175305695	56368
7774aead6329b41f510dc853216970496f39a259	dominant object detection for autonomous vision-based surveillance	unsupervised learning;surveillance;autonomous;visual surveillance;monitoring system;motion segmentation;pattern recognition;deformable model;object detection	"""The deployment of visual surveillance and monitoring systems has reached massive proportions. Consequently, a need to automate the processes involved in retrieving useful information from surveillance videos, such as detecting and counting objects, and interpreting their individual and joint behavior, has emerged. The existing methods targeting such automation and working towards """"smart"""" surveillance solutions largely rely on pattern recognition solutions trained offline in a supervised fashion. In this thesis we address the challenge of minimizing the level of supervision in automating the smart surveillance mechanisms. In particular, we focus on the development of fully autonomous detectors of objects that are typical for the observed scene. Using such solutions, objects can be observed and counted (e.g. pedestrians in a shopping street, cars and trucks on a highway), but also atypical objects can be detected leading to automatic security alerts. The rationale behind our proposed solution is that the characteristics of a scene or objects involved can be learned if sufficient time for observation is given, and if the measured features characterizing the properties of the scene and objects are generic enough. In the first step, we demonstrate how candidates for the typical objects can be separated from the scene background using an existing robust motion segmentation mechanism and our novel shadow removal method. In the second step, the set of candidate objects is filtered using an automatically learned perspective deformation model to remove irrelevant objects and compensate for the imperfections of the motion segmentation and shadow removal processes. Finally, in the third step, a module is developed, in which the candidate typical objects are used as positive training samples to train an advanced typical object detector. The effectiveness of this three-step approach is demonstrated first on the case of a single typical object class present in the observed scene. Then, we also show how the proposed approach can be expanded to simultaneously detect two such object classes, if present."""	autonomous robot;object detection	Hasan Celik	2010			computer vision;simulation;engineering;object-oriented design;multimedia	Robotics	41.78959867849904	-46.00146539513568	56391
cad6eaed8d6020d8ca332a790f2a844595673c6d	fast detection of vehicles based-on the moving region	vehicle detection image edge detection intelligent transportation systems object detection lighting image motion analysis marine vehicles intelligent vehicles optical noise motion detection;image motion analysis;intelligent transport system;edge detection;shadow detection;automated highways;vehicle detection;hsv color space;road vehicles automated highways edge detection image colour analysis image motion analysis optical tracking;hsv color space vehicle detection moving vehicle location moving vehicle tracking intelligent transportation system moving region detection shadow detection edge detection self adaptive background updating method;optical tracking;image colour analysis;road vehicles	The precise location and tracking of the moving vehicles is an important part in intelligent transportation system. This paper presents a new method for detecting vehicles which can remove the shadows fast. The main process is done in three steps: moving region detection, shadow detection, and edge detection. Firstly, a moving region of the image is achieved using the self-adaptive background updating method quickly. Then a coarse shadow area can be taken with the model-based method, and the shadow detection method based on the HSV color space is only applied on the coarse area. Finally, impose edge detection on both the moving regions and shadow areas. Subtraction of the two areas leads to the result of the detection of real vehicle. Experiment results show that this method can improve the efficiency of the moving vehicles detection greatly.	color space;edge detection;sensor	Bo Qin;Zongshun Ma;Zhenghua Fang;Shengke Wang	2007	2007 10th IEEE International Conference on Computer-Aided Design and Computer Graphics	10.1109/CADCG.2007.4407881	computer vision;simulation;object-class detection;hsl and hsv;edge detection;computer science	Robotics	43.278333267017	-45.03675400536266	56447
098f4181c934d2f6b0b919913ce3290a19fd751f	probabilistic tracking and behavior identification of fluorescent particles		Explicit and tractable characterizations of the dynamical behavior of virus particles are pivotal for a thorough understanding of the infection mechanisms of viruses. This thesis deals with the problem of extracting symbolic representations of the dynamical behavior of fluorescent particles from fluorescence microscopy image sequences. The focus is on the behavior of virus particles such as fusion with the cell membrane. A numerical representation is obtained by tracking the particles in the image sequences. We have investigated probabilistic tracking approaches, including approaches based on the Kalman filter as well as based on particle filters. For reasons of efficiency and robustness, we developed a tracking approach based onrnthe probabilistic data association (PDA) algorithm in combination with an ellipsoidal sampling scheme that exploits effectively the image data via parametric appearance models. To track objects in close proximity, we compute the support that each image position provides torneach tracked object relative to the support provided to the objectu0027s neighbors. After tracking, the problem of mapping the trajectory information computed by the tracking approaches to symbolic representations of the behavior arises. To compute symbolic representations of behaviors related to the fusion of single virus particles with the cell membrane based on their intensity over time,rnwe developed a layered probabilistic approach based on stochastic hybrid systems as well as hidden Markov models (HMMs). We use a maxbelief strategy to efficiently combine both representations. The layered approach describes the intensity, intensity models, and behaviors of single virus particles. We introduce models for the evolution of the intensity and the behavior. To compute estimates forrnthe intensity, intensity models, and behaviors we use a hybrid particle filter and the Viterbi algorithm. The developed approaches have been applied to synthetic images as well as to real microscopy image sequences displaying human immunodeficiency virus (HIV-1) particles. We have performed an extensive quantitative evaluation of the performance and a comparison with several existing approaches. It turned out that our approaches outperform previous ones, thus yielding more accurate and more reliable information about the behavior of virus particles. Moreover, we have successfully applied our tracking approaches to 3D image sequences displaying herpes simplex virus (HSV)rnreplication compartments. We also applied the tracking approaches to image data displaying microtubule tips and analyzed their motion. In addition, our tracking approaches were successfully applied to the 2D and 3D image data of a Particle Tracking Challenge.		William J. Godinez	2013			computer vision;simulation;machine learning;mathematics	Robotics	42.043029720843585	-39.74912290274331	56570
e3e0a68ce0dc3d6f09a9c0c356e4fa546696cdfe	ball detection for robotic soccer: a real-time rgb-d approach		The robotic football competition has encouraged the participants to develop new ways of solving different problems in order to succeed in the competition. This article shows a different approach to the ball detection and recognition by the robot using a Kinect System. It has enhanced the capabilities of the depth camera in detecting and recognizing the ball during the football match. This is important because it is possible to avoid the noise that the RGB cameras are subject to for example lighting issues.	real-time transcription	André Morais;Pedro Costa;José Luis Lima	2015		10.1007/978-3-319-27149-1_44	computer vision;image processing;simulation;artificial intelligence;robot;rgb color model;computer science;kalman filter;football	Robotics	46.75548212099069	-42.338915185905	56604
0d318bd299940ec325bcef21d73ad8702f121310	object-space road extraction in rural areas using stereoscopic aerial images	erbium;geophysical image processing;dynamic programming;image segmentation;image processing;road extraction;dynamic programming dp;dynamic programming algorithm;roads stereo image processing optimization remote sensing image segmentation accuracy;3 d road extraction dynamic programming dp road model stereoscopic aerial images;interdisciplinar;dynamic program;stereo image processing dynamic programming feature extraction geophysical image processing object detection radiometry roads;energy function;aerial image;radiometry;accuracy;reference systems;3 d road extraction;roads;feature extraction;remote sensing;stereo image processing;rural area;object space reference system rural area stereoscopic aerial image semiautomatic method 3d road extraction dynamic programming algorithm road centerline tracing stereoscopic image space radiometric information integration energy function;artigo;stereoscopic aerial images;optimization;road model;object detection	This letter proposes a semiautomatic method for 3-D road extraction in rural areas using stereoscopic aerial images. A strategy based on the dynamic programming algorithm provides a solution to the road extraction problem in the object space. The direct tracing of road centerlines in the object space necessitates mathematical relationships connecting road points in the stereoscopic image spaces and in the object space, enabling integration of radiometric information from the stereoscopic images into the associated energy function. The extraction process begins by first measuring a few seed points in one image of the stereoscopic pair and then transforming these into the object-space reference system. Experimental results show that the proposed method was efficient and usually provided accurate road centerlines.	aerial photography;algorithm;dynamic programming;mathematical optimization;stereoscopy;tuple space	Aluir Porfírio Dal Poz;Rodrigo A. B. Gallis;João F. C. da Silva;Érico F. O. Martins	2012	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2011.2177438	computer vision;image processing;computer science;dynamic programming;physics;remote sensing;computer graphics (images)	Robotics	50.188003751947534	-48.885534258571596	56635
0cc748147a8edebbd51b048244a3368baf1894da	incident detection algorithm based on radon transform using high-resolution remote sensing imagery	vehicles roads feature extraction transforms image color analysis remote sensing image edge detection;real time extraction;remote sensing image;geophysical image processing;traffic controlling;angle detection;radon transforms;high resolution;roadway;intensity imagery;image resolution;radon transform;road extraction;traffic flow measurement;edge detection;high resolution remote sensing imagery;road traffic;real time;vehicle detection;false alarm rate;traffic flow;automatic driver warning system;traffic engineering computing feature extraction geophysical image processing image resolution object detection radon transforms remote sensing road traffic;aerial image;learning system;remote sensing imagery;traffic congestion;image edge detection;roads;image color analysis;feature extraction;remote sensing;detection algorithm;transforms;detection rate;vision based traffic controlling;satellite image;traffic engineering computing;incident localization;incident detection;vehicles;road traffic monitoring;traffic controlling incident detection vehicle detection radon transform aerial image analysis;false alarm rate radon transform high resolution remote sensing imagery traffic congestion roadway road traffic monitoring incident detection automatic driver warning system road extraction vehicle detection real time extraction incident localization vision based traffic controlling intensity imagery satellite image neural network angle detection traffic flow measurement detection rate;object detection;aerial image analysis;neural network	One of the most important methods to solve the traffic congestion is to detect the incident state in a roadway. This paper describes the development of segmentation methods for road traffic monitoring aims at the acquisition and analysis remote sensing imagery of traffic figures, such as presence and number of vehicles, incident detection and automatic driver warning systems. We propose a strategy for road extraction, vehicle detection and incident detection from remote sensing imagery based on radon transform method. Real time extraction and localization of incident in aerial images is an emerging research area that can be applied to vision-based traffic controlling. The intensity imagery is used to extract the incident from satellite images. Techniques based on neural network, radon transform for angle detection and traffic flow measurements are used for road extraction, vehicle detection and incident detection. The results show that the proposed approach has a good detection performance. The maximum angle of vehicles applied for incident detection is 45° and the best performance of the learning system achieved by 87% for detection rate (DR) and a false alarm rate (FAR) under 18% on 45 aerial images of roadways.	aerial photography;algorithm;artificial neural network;image resolution;network congestion	Seyed Mostafa Mousavi Kahaki;Mohd Jan Nordin;Amir Hossein Ashtari	2011	Proceedings of the 2011 International Conference on Electrical Engineering and Informatics	10.1109/ICEEI.2011.6021622	computer vision;image resolution;computer science;machine learning;computer security;artificial neural network	Robotics	42.31982507547773	-43.08762426849715	56764
573f55e55183820a434a10cbd28c89cea36422fd	a fast affine-invariant features for image stitching under large viewpoint changes	image alignment;freak;image stitching;asift;affine invariant features	Image alignment and stitching is a popular application on many smart phones, but it is time consuming and creates a critical bottle neck in the course of implementation. In this paper, a fast and high-quality image stitching method is proposed. First, a series of simulated images is obtained by simulating the latitude and longitude angles of a raw image; second, FAST detector is used to detect the features of all the simulated images and described by FREAK (Fast Retina Key-point) before all the feature information is projected to the raw image; third, Hamming distance is used as a feature similarity metric and all the features are matched directly instead of using the repetitive projection in ASIFT (Affine-SIFT). RANSAC is then used to achieve the optimal affine-transformations, and lastly, a weighted average bending algorithm is used to smooth the intensities of the overlapping regions. The experimental results demonstrate that the proposed image stitching method greatly increases the speed of the image alignment process and produces a satisfactory result.		Xiaomin Ma;Ding Liu;Jian Zhang;Jing Xin	2015	Neurocomputing	10.1016/j.neucom.2014.10.045	computer vision;feature detection;simulation;image stitching;computer science;mathematics;computer graphics (images)	Vision	48.75333989104518	-46.033199502498604	56771
1c75875f830ea3e71bc30c93cab188707cbc9193	semi-supervised ensemble tracking	histograms;supervised learning;probability density function;object searching;real visual tracking semisupervised ensemble tracking particle filter object searching unlabelled sample generation environment changing supervised learning;semisupervised ensemble tracking;real visual tracking;environment changing;semisupervised learning particle filters particle tracking boosting robustness sun computer science intelligent systems supervised learning detectors;data mining;semi supervised learning;visual tracking semi supervised learning ensemble tracking;boosting;particle filter;feature extraction;ensemble tracking;robustness;search problems;target tracking learning artificial intelligence object detection particle filtering numerical methods;learning artificial intelligence;target tracking;visual tracking;unlabelled sample generation;object detection;particle filtering numerical methods	In this paper, we propose a semi-supervised ensemble tracking approach under the framework of particle filter. The particle filter is used not only for object searching, but also for unlabelled sample generation. By adopting the semi-supervised learning technology, these unlabelled samples which are generated online are utilized to progressively modify the classifier and make the ensemble tracker to be more robust to environment changing. On the other hand, utilizing semi-supervised learning technology can avoid the drifting phenomenons which are often encountered when using supervised learning. Finally, the performance of the proposed approach is evaluated using real visual tracking examples.	bittorrent tracker;particle filter;semi-supervised learning;semiconductor industry;supervised learning;video tracking	Huaping Liu;Fuchun Sun	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959916	computer vision;probability density function;particle filter;eye tracking;feature extraction;computer science;machine learning;pattern recognition;histogram;ensemble learning;supervised learning;boosting;robustness	Robotics	42.83471665113124	-48.83636028362017	56835
6ed1d98e035f01972509e42c6107a0368cd0b86b	the method for calculating exterior orientation parameters of the image based on straight line features and precision analysis	straight line features;image processing geometry;coplanarity condition straight line features exterior orientation parameters aerial image;image segmentation;image processing;coplanarity condition;eop;exterior orientation parameters;geometry;line segments;line segments exterior orientation parameters straight line features precision analysis eop image space point based method;aerial image;accuracy;vectors;image space;precision analysis;image segmentation parameter estimation equations approximation methods mathematical model vectors accuracy;point based method;mathematical model;approximation methods;parameter estimation	This paper proposes a method for calculating exterior orientation parameters (EOPs) of images based on the condition that the straight line in the image space and corresponding line in the object space are coplanar. Experimental results have proved the feasibility and validity of this method, and the accuracy of EOPs is within one pixel and similar to that of point-based method. The position and direction of line segments have influence on the precision of EOPs, and the influences are analyzed academically. The reliability of precision analytical results has proved by utilizing simulated image.	camera resectioning;control flow;line level;pixel;tuple space	Zhiqing Liu;Zhaohui Xu;Baoming Zhang;Haitao Guo;Pengcheng Li	2013	2013 21st International Conference on Geoinformatics	10.1109/Geoinformatics.2013.6626028	computer vision;mathematical optimization;mathematics;geometry	Robotics	51.56765162661646	-51.755574269610875	56891
0845726e9095453f5ed04f64004063f3be99164b	detecting and tracking moving objects from a mobile platform using a laser range scanner	moving object;legged locomotion;mobile robot;real time;computational geometry;motion detection moving object tracking computer vision laser range scanner real time systems mobile robot;motion estimation;mobile robots;testing;detection and tracking of moving objects;laser ranging;computer vision;moving object tracking;eyes;robot vision;optical tracking;real time systems mobile robots robot vision optical tracking laser ranging motion estimation;robustness;target tracking;object detection cameras computer vision eyes target tracking robot vision systems computational geometry robustness testing legged locomotion;motion detection;robot vision systems;cameras;laser range scanner;object detection;real time systems	In the field of computer vision, the detection and tracking of moving objects from a moving observer is a complex and computationally demanding task. Using a laser range scanner instead of a camera, the problem can be simplified dramatically. An algorithm that identifies range readings in areas that was detected earlier as free is described. This is done without incorporating any gridmaps that are inherently memory and computationally consuming. The algorithm is robust from the real-time test in a furnished living room. It is able to track a moving person walking around, while consuming only about 2% of the available processing power.	mobile device;sensor	Matthias Lindström;Jan-Olof Eklundh	2001		10.1109/IROS.2001.977171	mobile robot;computer vision;simulation;computational geometry;computer science;computer graphics (images)	Robotics	48.827664095353505	-43.6564815863705	56899
4428fe5c6b6191191a588335a6e4efa84149366e	towards generic detection of unusual events in video surveillance	image classification video signal processing video cameras image retrieval content based retrieval image segmentation image motion analysis;image motion analysis;image segmentation;sport videos supervised classification meaningful events in videos camera motion informations probabilistic models video samples training set temporal segmentation dynamic event;video signal processing;supervised classification;image classification;sports video;camera motion;video cameras;event detection videos cameras image motion analysis motion measurement motion detection optical computing motion analysis classification algorithms computer vision;dynamic content;content based retrieval;image retrieval	In this paper, we consider the challenging problem of unusual event detection in video surveillance systems. The proposed approach makes a step toward generic and automatic detection of unusual events in terms of velocity and acceleration. At first, the moving objects in the scene are detected and tracked. A better representation of moving objects trajectories is then achieved by means of appropriate pre-processing techniques. A supervised Support Vector Machine method is then used to train the system with one or more typical sequences, and the resulting model is then used for testing the proposed method with other typical sequences (different scenes and scenarios). Experimental results are shown to be promising. The presented approach is capable of determining similar unusual events as in the training sequences.	algorithm;closed-circuit television;image segmentation;preprocessor;radial (radio);radial basis function;real life;supervised learning;support vector machine;velocity (software development)	Nathalie Peyrard;Patrick Bouthemy	2003	2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/ICIP.2003.1247321	image texture;computer vision;contextual image classification;image processing;image retrieval;computer science;dynamic web page;pattern recognition;multimedia;image segmentation;motion compensation	Vision	39.64882582393326	-47.343613891347914	57006
1d19297df8b00ce2d574e7de893fda45dad95727	statistical background modelling for tracking with a virtual camera	statistical background;virtual camera	"""A method of robust feature-detection is proposed for visual tracking with a pan-tilt head. Even with good foreground models, the tracking process is liable to be disrupted by strong features in the background. Previous researchers have shown that the disruption can be somewhat suppressed by the use of image-subtraction. Building on this idea, a more powerful statistical model of background intensity is proposed in which a Gaussian mixture distribution is fitted to each of the pixels on a """"virtual"""" image plane. A fitting algorithm of the """"Expectation-Maximisation"""" type proves to be particularly effective here. Practical tests with contour tracking show marked improvement over image subtraction methods. Since the burden of computation is off-line, the online tracking process can run in real-time, at video field-rate."""	computation;denial-of-service attack;expectation–maximization algorithm;feature detection (computer vision);field (video);image plane;image subtraction;online and offline;pixel;real-time clock;statistical model;video tracking;virtual camera system	Simon Rowe;Andrew Blake	1995		10.5244/C.9.42	computer vision;camera auto-calibration;tracking system	Vision	44.12209535415988	-48.56990387067743	57064
b00112114927df5baf32710ddae2338b0b08396f	demo: enabling image analysis tasks in visual sensor networks	object recognition;communication systems;binary local visual features;visual sensor networks;object tracking;kommunikationssystem;arm	This demo showcases some of the results obtained by the GreenEyes project, whose main objective is to enable visual analysis on resource-constrained multimedia sensor networks. The demo features a multi-hop visual sensor network operated by BeagleBones Linux computers with IEEE 802.15.4 communication capabilities, and capable of recognizing and tracking objects according to two different visual paradigms. In the traditional compress-then-analyze (CTA) paradigm, JPEG compressed images are transmitted through the network from a camera node to a central controller, where the analysis takes place. In the alternative analyze-then-compress (ATC) paradigm, the camera node extracts and compresses local binary visual features from the acquired images (either locally or in a distributed fashion) and transmits them to the central controller, where they are used to perform object recognition/tracking. We show that, in a bandwidth constrained scenario, the latter paradigm allows to reach better results in terms of application frame rates, still ensuring excellent analysis performance.	computer;image analysis;jpeg;linux;outline of object recognition;programming paradigm;visual sensor network	Luca Baroffio;Antonio Canclini;Matteo Cesana;Alessandro Enrico Cesare Redondi;Marco Tagliasacchi;György Dán;Emil Eriksson;Viktoria Fodor;João Ascenso;Pedro Monteiro	2014		10.1145/2659021.2669477	embedded system;computer vision;simulation;computer science;cognitive neuroscience of visual object recognition;video tracking;arm architecture;communications system;visual sensor network	Vision	46.77173431476471	-38.165029240850686	57082
1bf4b514321a7a32f0a7cff6538ddc130d05e141	towards on-line intensity-based surface recovery from monocular images		We present a novel method for vision-based recovery of three-dimensional structures through simultaneous model reconstruction and camera position tracking from monocular images. Our approach does not rely on robust feature detecting schemes (such as SIFT, Good Features to Track etc.), but works directly on intensity values in the captured images. Thus, it is well-suited for reconstruction of surfaces that exhibit only little texture due to partial homogeneity of the surfaces.	algorithm;cross-correlation;mathematical optimization;medical imaging;scale-invariant feature transform;sensor;simulation;vergence	Oliver Ruepp;Darius Burschka;Robert Bauernschmitt	2010		10.5244/C.24.77	computer vision;computer graphics (images)	Vision	52.278768041111356	-47.01168999873522	57138
d52ae8dfe979f201870f9850e04ec42c1149c9bc	automatic hierarchical classification using time-based co-occurrences	tracking system;image classification;parameter estimation image classification image representation;binary motion silhouettes tracking system hierarchical classification binary tree classifier joint cooccurrences;hierarchical classification;tracking artificial intelligence laboratories pediatrics visual system layout vectors statistics data security object detection;image representation;parameter estimation;binary tree	While a tracking system is unaware of the identity of any object it tracks, the identity remains the same for the entire tracking sequence. Our system leverages this information by using accumulated joint cooccurrences of the representations within the sequence to create a hierarchical binary-tree classifier of the representations. This classifier is useful to classify sequences as well as individual instances. We illustrate the use of this method on two separate representationsthe tracked object’s position, movement, and size; and the tracked object’s binary motion silhouettes.	activity recognition;binary image;binary tree;motion compensation;tracking system	Chris Stauffer	1999		10.1109/CVPR.1999.784654	computer vision;contextual image classification;tracking system;binary tree;computer science;machine learning;pattern recognition;estimation theory	Vision	40.41822772828678	-47.99867938966606	57163
097fca5ad48967618340d5fb9007b5cda895d993	view-based location and tracking of body parts for visual interaction	real time;visual interaction;particle filter;human body;body shape	The purpose of this research is to provide a coarse estimate of body pose. Our main interest is not in 3D biometric accuracy, but rather a sufficient discriminatory representation for visual interaction. The algorithm employs a general approximation to body shape, applied within a condensation [1] framework, while making use of an integral image to maintain near real-time performance. Furthermore, we seek to locate the head and hands in a hierarchical manner. Hand position is disambiguated using a prior on body configurations. This prior is further used to estimate the location of other key body components using statistical methods. We demonstrate the system tracking within a complex, cluttered environment.	algorithm;approximation;biometrics;location-based service;pose (computer vision);real-time clock;real-time computing	Antonio S. Micilotta;Richard Bowden	2004		10.5244/C.18.87	computer vision;human body;simulation;particle filter;body shape;computer science	HCI	47.19748226004379	-43.12090612925742	57446
876e624c4a9e34e4372be31fba471a50dbbc87e5	occlusion-aided support weights for local stereo matching	local stereo matching;disparity calibration;occlusion detection;stereo vision;adaptive support weight	There has been a significant improvement in stereo matching with the introduction of adaptive support weights. Existing local methods mainly focus on the computation of support weight which is critical in cost aggregation and usually get excellent results. However, the negative effects of occluded regions are often ignored, which results in the problem of foreground fattening and blurred depth borders. This paper proposes a novel support aggregation strategy by utilizing the occlusion information obtained from left-right consistency check. The weights of invalid points are noticeably reduced at each disparity estimation stage. Experimental results on the Middlebury images show that our method is highly effective in improving the disparities of points around occluded areas and depth discontinuities. According to the Middlebury benchmark, the proposed method achieves the best performance among all the local methods. Moreover, our approach can be easily integrated into nearly all the existing support weights strategies.	computer stereo vision	Wei Wang;Caiming Zhang;Shuozhen Wang;Xuemei Li	2012	IJPRAI	10.1142/S0218001412550075	computer vision;mathematical optimization;computer science;stereopsis;pattern recognition;mathematics	Vision	42.56959371277365	-49.7153274083332	57450
7891a97bee43d443eb3bf74e92efa5e7e008e4bf	precise segmentation and estimation of pedestrian trajectory using on-board monocular cameras				HyungKwan Kim;Yuuki Shibayama;Shunsuke Kamijo	2012	IEICE Transactions		computer vision	Vision	51.913635978327875	-42.23035479161703	57676
abd91a234b9ce066efed24d1b1ec5c4b79b15152	real-time video stabilization without phantom movements for micro aerial vehicles	signal image and speech processing;detectors;consensus;real time;biometrics;motion estimation;filter;pattern recognition;video stabilization;image processing and computer vision;micro aerial vehicles;motion intention;article;camera	In recent times, micro aerial vehicles (MAVs) are becoming popular for several applications as rescue, surveillance, mapping, etc. Undesired motion between consecutive frames is a problem in a video recorded by MAVs. There are different approaches, applied in video post-processing, to solve this issue. However, there are only few algorithms able to be applied in real time. An additional and critical problem is the presence of false movements in the stabilized video. In this paper, we present a new approach of video stabilization which can be used in real time without generating false movements. Our proposal uses a combination of a low-pass filter and control action information to estimate the motion intention.	aerial photography;algorithm;imaging phantom;low-pass filter;real-time transcription;unmanned aerial vehicle;video post-processing	Wilbert G. Aguilar;Cecilio Angulo	2014	EURASIP J. Image and Video Processing	10.1186/1687-5281-2014-46	computer vision;detector;simulation;consensus;filter;computer science;archaeology;video tracking;pattern recognition;motion estimation;motion compensation;biometrics;image stabilization;computer graphics (images)	Vision	45.87811740777305	-44.00806266584655	57692
02ce77f66d0b633fbb439eb304408be84b899229	finding trails	image sampling;dynamic programming;statistical analysis;roads;cartography;learning artificial intelligence	We present a statistical learning approach for finding recreational trails in aerial images. While the problem of recognizing relatively straight and well defined roadways in digital images has been well studied in the literature, the more difficult problem of extracting trails has received no attention. However, trails and rough roads are less likely to be adequately mapped, and change more rapidly over time. Automated tools for finding trails will be useful to cartographers, recreational users and governments. In addition, the methods developed here are applicable to the more general problem of finding linear structure. Our approach combines local estimates for image pixel trail probabilities with the global constraint that such pixels must link together to form a path. For the local part, we present results using three classification techniques. To construct a global solution (a trail) from these probabilities, we propose a global cost function that includes both global probability and path length. We show that the addition of a length term significantly improves trail finding ability. However, computing the optimal trail becomes intractable as known dynamic programming methods do not apply. Thus we describe a new splitting heuristic based on Dijkstra's algorithm. We then further improve upon the results with a trail sampling scheme. We test our approach on 500 challenging images along the 2500 mile continental divide mountain bike trail, where assumptions prevalent in the road literature are violated.	aerial photography;cartography;digital image;dijkstra's algorithm;dynamic programming;heuristic;köppen climate classification;loss function;machine learning;pixel;sampling (signal processing)	Scott Morris;Kobus Barnard	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587819	computer vision;simulation;computer science;artificial intelligence;machine learning;dynamic programming;mathematics;statistics	Vision	42.47847155676767	-41.6886230996185	57708
ed799bcb818f5c47f574ad88b48cca7a7637c894	vehicle roi extraction based on area estimation gaussian mixture model		The extraction of vehicle region of interest (ROI) is a key step of vehicle detection system. When Gaussian Mixture Model (GMM) is applied to vehicle ROI extraction, the extraction speed is unsatisfied and the results remain some noise blocks. In order to solve these problems, a method of vehicle ROI extraction based on area estimation Gaussian Mixture Model is proposed. First, Gaussian mixture background model based on scale mapping is used for background detection and foreground clump of samples extraction. Then, the vehicle area estimation model is trained using the automatic samples collection and selection mechanism. After that, the model is used for foreground clumps selection, which can finally get the vehicle ROI. Experiments show that this method can provide more accurate vehicle ROI for the follow-up of vehicle detection, thus improves the real-time and reliability performance of vehicle detection.	experiment;feature (computer vision);google map maker;mixture model;real-time clock;region of interest	ZhaoNan Huang;HuaBiao Qin;Qing Liu	2017	2017 3rd IEEE International Conference on Cybernetics (CYBCON)	10.1109/CYBConf.2017.7985821	mixture model;region of interest;gaussian;computer vision;pattern recognition;engineering;artificial intelligence	Robotics	42.66577586371634	-47.798799496794935	57832
1cbaba9bd7a656ed2b609fdd8a9f32d6691f13b2	large scale graph-based slam using aerial images as prior information	mobile robot;localization;prior information;robotics;three dimensional;aerial image;laser range scanning;large scale;simultaneous localization and mapping;robotteknik och automation;aerial images;aerial photograph;mapping	To effectively navigate in their environments and accurately reach their target locations, mobile robots require a globally consistent map of the environment. The problem of learning a map with a mobile robot has been intensively studied in the past and is usually referred to as the simultaneous localization and mapping (SLAM) problem. However, existing solutions to the SLAM problem typically rely on loop-closures to obtain global consistency and do not exploit prior information even if it is available. In this paper, we present a novel SLAM approach that achieves global consistency by utilizing publicly accessible aerial photographs as prior information. Our approach inserts correspondences found between three-dimensional laser range scans and the aerial image as constraints into a graph-based formulation of the SLAM problem. We evaluate our algorithm based on large real-world datasets acquired in a mixed inand outdoor environment by comparing the global accuracy with state-of-the-art SLAM approaches and GPS. The experimental results demonstrate that the maps acquired with our method show increased global consistency.	3d scanner;aerial photography;algorithm;experiment;global positioning system;mobile robot;monte carlo localization;simultaneous localization and mapping	Rainer Kümmerle;Bastian Steder;Christian Dornhege;Alexander Kleiner;Giorgio Grisetti;Wolfram Burgard	2011	Auton. Robots	10.1007/s10514-010-9204-1	mobile robot;three-dimensional space;computer vision;simulation;internationalization and localization;computer science;artificial intelligence;robotics;simultaneous localization and mapping	Robotics	53.721552442115225	-42.51538657022903	57898
cc35644a840a47f400d72b63027896e10f892899	remarks on real-time human posture estimation from silhouette image using neural network	image processing;neural nets;personal computer;humans neural networks artificial neural networks image analysis head elbow knee microcomputers image reconstruction application software;estimation method;real time;computer vision;feature vector;image reconstruction;human body;image reconstruction neural network silhouette image real time human posture estimation input feature vector;real time systems neural nets image reconstruction computer vision;real time application;neural network;real time systems	This work proposes a human body posture estimation method from image, using ANN. The input feature vector of the ANN is composed with the result of analyzing a human silhouette image and the output vector of the ANN indicates the 2D coordinates of the human body's significant points, such as head, shoulders, hands, elbows, knees, and feet. The proposed estimation method is implemented on a personal computer and runs in real-time. By using the estimated results, the estimated human posture is reconstructed with a stick figure in the real time application. Experimental results show both the feasibility and the effectiveness of the proposed method for estimating human body postures.	artificial neural network;feature vector;personal computer;poor posture;real-time clock	Kazuhiko Takahashi;Tomohiko Tanigawa	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1398325	iterative reconstruction;computer vision;feature detection;human body;simulation;feature vector;computer science;artificial intelligence;machine learning;artificial neural network	Robotics	47.98041743345729	-44.338442901328456	58206
e3ac71b21239f882fd13b0bb4a312b0cba235feb	a compact probability model for natural clutter	clutter;probability;image processing;image processing clutter probability statistical analysis;natural images;statistical analysis;statistical image analysis compact probability model natural clutter background clutter modeling 2d projected views 3d real objects analytical density histograms observed densities;probability pixel shape statistics image edge detection image texture analysis image recognition context modeling face detection histograms;probability model	We present a framework for modeling background clutter in natural images. Assuming that: (i) images are made up of 2D (projected) views of 3D (real) objects, and (ii) certain simplifying conditions hold, we present an analytical density for the derivatives of the natural images. This expression is s.hown to match well with the observed densities (histograms).	clutter	Ulf Grenander;Anuj Srivastava	2001		10.1109/ICIP.2001.958553	computer vision;image processing;computer science;pattern recognition;probability;mathematics;clutter;statistics	Vision	45.16929564374014	-51.71362924570981	58329
269bc32c0ab7457d3c78955e6ec988fc45150312	topological map construction and scene recognition for vehicle localization	scene recognition;topological map;scene change detection	This paper presents a vehicle localization method to assist vehicle navigation based on topological map construction and scene recognition. A topological map is constructed using omni-directional image sequences, and the node information of the topological map is used for place recognition and derivation of vehicle location. In topological map construction and scene change detection, we utilize the Extended-HCT method for semantic description and feature extraction. Content-based and feature-based image retrieval approaches are adopted for place recognition and vehicle localization on the real scene image dataset. The proposed technique is able to construct a real-time image retrieval system for navigation assistance and validate the correctness of the route. Experiments are carried out in both the indoor and outdoor environments using real world images.	approximation algorithm;content-based image retrieval;correctness (computer science);feature extraction;image retrieval;internationalization and localization;real-time clock;reduction (complexity);visual descriptor;windows hardware certification kit	Huei-Yung Lin;Chia-Wei Yao;Kai-Sheng Cheng;Tran Van Luan	2018	Auton. Robots	10.1007/s10514-017-9638-9	artificial intelligence;computer vision;computer science;feature extraction;change detection;image retrieval;correctness;derivation;topological map	Robotics	49.43735577114253	-43.82270567170391	58332
bc12de40472cfd161f35e8c235c26626cf957818	a hidden markov model-based continuous gesture recognition system for hand motion trajectory	topology;hidden markov model;real time;skin;training;zero codeword detection;hmm;video sequences;hand motion trajectory;orientation dynamic features;left right;hidden markov models;continuous gesture recognition system;image color analysis;feature extraction;hidden markov models feature extraction gesture recognition;arabic numbers;spatio temporal trajectories;gesture recognition;hidden markov models real time systems topology motion detection image recognition handicapped aids human computer interaction man machine systems system testing color;zero codeword detection hidden markov model continuous gesture recognition system hand motion trajectory arabic numbers hmm orientation dynamic features spatio temporal trajectories;real time systems	In this paper, we propose an automatic system that recognizes both isolated and continuous gestures for Arabic numbers (0-9) in real-time based on hidden Markov model (HMM). To handle isolated gestures, HMM using ergodic, left-right (LR) and left-right banded (LRB) topologies with different number of states ranging from 3 to 10 is applied. Orientation dynamic features are obtained from spatio-temporal trajectories and then quantized to generate its codewords. The continuous gestures are recognized by our novel idea of zero-codeword detection with static velocity motion. Therefore, the LRB topology in conjunction with forward algorithm presents the best performance and achieves average rate recognition 98.94% and 95.7% for isolated and continuous gestures, respectively.	code word;color image;ergodicity;forward algorithm;gesture recognition;hidden markov model;lr parser;markov chain;quantization (signal processing);real-time clock;real-time computing;velocity (software development)	Mahmoud Elmezain;Ayoub Al-Hamadi;Jörg Appenrodt;Bernd Michaelis	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761080	computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;skin;arabic numerals;hidden markov model	Robotics	39.61630510224861	-48.74104877271243	58520
cd03091c300be5a74042bcfe612ff00c6e05c493	a framework for human recognition and counting in restricted area for video surveillance			closed-circuit television	Alessandro Moro;Jun Wakabayashi;Tetsuro Toda;Kazunori Umeda	2018		10.3233/978-1-61499-874-7-139		Vision	40.89555055419607	-46.895130531851734	59431
fa64755784e602fcac51bef5c32d517a81ed2c61	3d object recognition using multiple features for robotic manipulation	filtering;object recognition;image matching;path planning;particle filtering method 3d object recognition robotic manipulation pose estimation photometric feature geometric feature;photometric feature;testing;layout;particle filtering method;indexing terms;geometric feature;diversity reception;3d object recognition;robot manipulator;robotic manipulation;robot vision;shape;photometry;particle filter;feature extraction;robots;stereo vision;next best view;robustness;object recognition robots layout robustness computational efficiency shape diversity reception photometry filtering testing;computational efficiency;robot vision feature extraction image matching object recognition particle filtering numerical methods path planning photometry;particle filtering numerical methods;pose estimation	For robust 3D object recognition in the environment having diverse variances, it is necessary to increase the certainty by using consecutive scenes rather than using a single scene and combining different features. This paper proposes a novel 3D object recognition and pose estimation approach based on combining photometric feature (SIFT) and geometric feature (3D lines) in a sequence of scenes. In order to utilize the consecutive scenes, we use the particle filtering method and all particles which represent the possible pose of object are generated by each feature. These particles are to be spread out where the object is considered to exist, and the probability of each particle is obtained through matching test with each feature in the scene. Then the particle sets derived from SIFT and 3D lines are fused and it gives a pose of the object estimated. For the sake of computational efficiency, this recognition system is performed in a hierarchical process. In this paper, we also introduce a simple method to decide the next best view position based on results of recognition. Lastly we have proved through experiments that the proposed methods are feasible	3d single-object recognition;computation;experiment;line level;outline of object recognition;particle filter;robot;scale-invariant feature transform	Sukhan Lee;Eunyoung Kim;Yeonchool Park	2006	Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.	10.1109/ROBOT.2006.1642278	filter;robot;layout;computer vision;pose;index term;3d pose estimation;particle filter;photometry;feature extraction;shape;computer science;stereopsis;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;motion planning;software testing;3d single-object recognition;robustness	Robotics	50.25555000930907	-39.477852183981334	59444
a00e7e3713de04092f73a48671c25056383a2065	human tracking by particle filtering using full 3d model of both target and environment	humans target tracking particle tracking filtering cameras image reconstruction image sequences surveillance video sequences security;background modeling;video signal processing;3d environmental model;video sequences;3d space;human tracking;occlusions human tracking particle filtering 3d space 3d environmental model video sequences;3d model;particle filter;particle filtering;video signal processing image sequences particle filtering numerical methods target tracking;target tracking;occlusions;environmental modeling;particle filtering numerical methods;image sequences	This work presents a new approach based on particle filtering to directly estimate the 3D positions of humans. Our system can predict occlusions due to other movements because we track humans in a 3D space, not on a 2D image plane. In addition, we introduce a 3D environmental model as the background model for tracking. This makes it easier to handle occlusions due to fixed objects in the environment. The 3D environmental model is automatically constructed by our original method from video sequences. Experiments show that our system is stable under occlusions due to the movements of both other subjects and fixed objects	3d modeling;image plane;motion compensation;particle filter	Tatsuya Osawa;Xiaojun Wu;Kaoru Wakabayashi;Takayuki Yasuno	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.641	computer vision;simulation;particle filter;computer science;statistics;computer graphics (images)	Vision	45.67533095575352	-47.62721045581712	59466
29fa62fad1522514b93f4a49fbd1a8458e857335	real-time analysis of baseball pitching using image processing on smartphone		Pitching is one of the most important elements of baseball. Therefore, both professional and amateur players are interested in measuring their pitching performance, particularly their pitch speed. Conventional equipment such as radar speed guns can be used to measure pitch speed; however this approach is unpopular among amateur players because of its cost. Therefore, we proposed a vision-based speed-measuring method for baseball pitches and developed a smartphone application called iPhoneSG, which can measure the speed of pitched baseballs on a real baseball diamond in near-real-time using a standalone image-processing technique on the smartphone. Relative to conventional radar speed guns, which must be placed on the extended line of the ball trajectory, iPhoneSG widens the possible area of data acquisition. Considering the popularity of smartphones, iPhoneSG provides players with a convenient means of measuring pitch speeds. We confirmed the basic viability of the proposed method as implemented in our iPhoneSG application. c © 2016 The Authors. Published by Elsevier B.V. Peer-review under responsibility of KES International.	cal ripken's real baseball;data acquisition;image processing;mobile app;pitch (music);radar;real-time clock;real-time computing;real-time transcription;smartphone	Yosuke Yamaguchi;Motoki Miura	2016		10.1016/j.procs.2016.08.128	simulation;computer graphics (images)	Robotics	44.666199772877135	-40.8776459579706	59489
1acbab634514f0bd801070212dcd7e3981c890d5	cloud-based knowledge sharing in cooperative robot tracking of multiple targets with deep neural network		Cooperative robot tracking of multiple targets plays an important role in many realistic robot applications. In order to minimize the time during which any target is not tracked, target trading among robots at runtime is a common phenomenon. After a period of successful tracking, the robot can gain a lot of knowledge about the target details, for example, the appearance changes caused by motion and illumination. However, the accumulated knowledge is dropped simply in existing research while robots trading targets, which makes each robot has to learn the knowledge of target details from scratch. The absence of knowledge sharing heavily influences the tracking accuracy in practice. In this paper, we propose a novel approach named Cloudroid Tracking which enables knowledge sharing through the support of the back-end cloud infrastructure. Our approach adopts the deep neural network (DNN) and its online tuning mechanisms to enable the knowledge accumulation. The dynamic connection of multiple DNNs on the cloud infrastructure and multiple robots is enabled. No matter how the target changes, the robot can connect to the corresponding neural network which is responsible for a specific target. The experimental results on both open dataset and real robots show that our approach can promote the accuracy for robot tracking significantly.	deep learning;robot	Hui Bao;Huaimin Wang;Bo Ding;SuNing Shang	2017		10.1007/978-3-319-70136-3_8	robot;machine learning;artificial intelligence;scratch;computer science;artificial neural network;cloud computing;knowledge sharing;phenomenon	Robotics	46.66665532107834	-40.53789790192081	59531
29dbd56bf3ec04055fafe9932bb64bc2130a205b	recent advances in image deblurring		Motion blur is a common artifact that produces disappointing blurry images with inevitable information loss. Due to the nature of imaging sensors that accumulates incoming lights, a motion blurred image will be obtained if the camera sensor moves during exposure. Image (motion) de-blurring is a computational process to remove motion blurs from a blurred image to obtain a sharp latent image. Recently image de-blurring has become a popular topic in computer graphics and vision research, and excellent methods have been developed to improve the quality of de-blurred images and accelerate the computation speed. Image de-blurring has also a variety of applications in image enhancement software and camera industry, and a practical image de-blurring method with quality and speed would be a critical factor to improve the performance of image enhancement and camera systems.  This course will first introduce the concepts, theoretical model, problem definition, and basic approach of image de-blurring. Blind deconvolution and non-blind deconvolution are two main topics of image de-blurring, which are classified by the existence of given kernel (or PSF; point spread function) information that describes the camera motion. For both blind deconvolution and non-blind deconvolution, challenges, classical methods, and recent research trends and successful methods will be presented. A PhotoShop demo will be given to show the performance of a recently developed fast motion de-blurring method.  This course will also cover several advanced issues of image de-blurring, such as hardware based approaches, spatially-varying camera shakes, object motions, and video de-blurring. It will conclude with remaining challenges, such as outliers and noise, computation time, and quality assessment. There will be Q&A at the end of each presentation with a short discussion at the end of the course.	adobe photoshop;blind deconvolution;computation;computer graphics;deblurring;gaussian blur;image editing;image noise;image sensor;latent image;theory;time complexity	Seungyong Lee;Sunghyun Cho	2013		10.1145/2542266.2542272	image quality;image restoration;computer vision;computational photography;feature detection;simulation;image processing;computer science;motion field;computer graphics (images)	Vision	39.606672549527346	-42.64797297261464	59576
35a65313529bfe5be826862c5ec877dfd10ac4d4	inverse composition discriminative optimization for point cloud registration		Rigid Point Cloud Registration (PCReg) refers to the problem of finding the rigid transformation between two sets of point clouds. This problem is particularly important due to the advances in new 3D sensing hardware, and it is challenging because neither the correspondence nor the transformation parameters are known. Traditional local PCReg methods (e.g., ICP) rely on local optimization algorithms, which can get trapped in bad local minima in the presence of noise, outliers, bad initializations, etc. To alleviate these issues, this paper proposes Inverse Composition Discriminative Optimization (ICDO), an extension of Discriminative Optimization (DO), which learns a sequence of update steps from synthetic training data that search the parameter space for an improved solution. Unlike DO, ICDO is object-independent and generalizes even to unseen shapes. We evaluated ICDO on both synthetic and real data, and show that ICDO can match the speed and outperform the accuracy of state-of-the-art PCReg algorithms.		Jayakorn Vongkulbhisal;Beñat Irastorza Ugalde;Fernando De la Torre;João Paulo Costeira	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00316	discriminative model;point cloud;parameter space;computer science;local search (optimization);rigid transformation;artificial intelligence;outlier;pattern recognition;maxima and minima;composition (visual arts)	Vision	49.487492207479	-50.445979412778826	59610
1d802aac1cb1e007f6fe12654d4f3cb0b2e7f52a	the geometry of colorful, lenticular fiducial markers	standards;arrays cameras color standards image color analysis lenses;color;arrays;image color analysis;lenses;cameras	Understanding the pose of an object is fundamental to a variety of visual tasks, from trajectory estimation of UAVs to object tracking for augmented reality. Fiducial markers are visual targets designed to simplify this process by being easy to detect, recognize, and track. They are often based on features that are partially invariant to lighting, pose and scale. Here we explore the opposite approach and design passive calibration patterns that explicitly change appearance as a function of pose. We propose a new, simple fiducial marker made with a small lenticular array, which changes its apparent color based on the angle at which it is viewed. This allows full six degree-of-freedom pose estimation with just two markers and an optimization that fully decouples the estimate of rotation from translation. We derive the geometric constraints that these fiducial markers provide, and show improved pose estimation performance over standard markers through experiments with a physical prototype for form factors that are not well supported by standard markers (such as long skinny objects). In addition, we experimentally evaluate heuristics and optimizations that give robustness to real-world lighting variations.	3d pose estimation;augmented reality;computer form factor;experiment;fiducial marker;heuristic (computer science);lenticular printing;mathematical optimization;pose (computer vision);prototype;unmanned aerial vehicle	Ian Schillebeeckx;Joshua Little;Brendan Kelly;Robert Pless	2015	2015 International Conference on 3D Vision	10.1109/3DV.2015.61	computer vision;3d pose estimation;mathematics;optics;computer graphics (images)	Robotics	53.05004452271512	-44.95638021392556	59629
803e1a9a241880d02197b1905f932890a3e3149f	a volleyball movement trajectory tracking method adapting to occlusion scenes		This paper proposes a volleyball trajectory tracking method adapting to occlusion scenes. Firstly, the target volleyball is obtained in the video manually and the Kalman filter algorithm combined with Continuously Adaptive Mean Shift (CAMSHIFT) algorithm is used to track and determine the size and position of the volleyball in each frame of video, and then it is determined whether there exists an occlusion. If there is no occlusion, positions of the volleyball in each frame of video are connected to obtain the trajectory of the volleyball. If there is occlusion, the Kalman filter algorithm is used to predict the positions of the volleyball in the occlusion section, and the size remains unchanged. Finally, the positions of the volleyball in each frame of video is connected to a line to obtain the trajectory of the volleyball motion. The proposed approach solves the problem of more complicated video background in volleyball movement. When the volleyball is blocked, it can accurately predict the volleyball movement trajectory so as to accurately track the volleyball movement trajectory under dynamic and occlusion scenes.		Ting Yu;Zeyu Hu;Xinyu Liu;Yunli Gong;Jun Xie;Tianlei Zang	2018		10.1007/978-981-13-2203-7_62	kalman filter;computer vision;trajectory;artificial intelligence;mean-shift;computer science;occlusion	Vision	48.435880916191735	-46.152041551992504	59809
c08c377a1a6ddf8c45240e34ea6358955311c533	multi-bernoulli filter for group object tracking and its gaussian-wishart implementation		The problem of multiple group object tracking is a challenging one and has been extensively researched during the last two decades. The problem solution, proposed in this article, is an extension of the well known multi-Bernoulli filter. The model is first formulated generally, then the linear Gaussian-Wishart implementation is proposed. Contrary to many known methods, which track either individual targets or groups, the proposed filter tracks individual objects and combines them into group tracks. The experiments on simulated data have shown a sustainable ability of the method to track group objects.	algorithm;approximation;bayesian network;bernoulli polynomials;calculus of variations;cluster analysis;experiment;heuristic;mathematical model;robustness (computer science);simulation;video content analysis;video tracking;whole earth 'lectronic link	Dmitry Kangin;Garik Markarian	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966250	machine learning;group object;artificial intelligence;tracking system;video tracking;radar tracker;computer science;computer vision;gaussian;bernoulli's principle;wishart distribution	Vision	45.6893149657167	-48.121517261698756	59844
205e2442e52f10818f02eaaf6318c3e3177bc19e	sparse update for loopy belief propagation: fast dense registration for large state spaces	shape belief propagation three dimensional displays clamps convergence topology hippocampus;3d shape matching;topology;object recognition;convergence;image match;image matching;hippocampi registeration;hippocampus;dense point based registration;markov random fields;prior information;hippocampus registration loopy belief propagation dense correspondence 3d shape matching;markov random field;point correspondence;conference paper;iterative methods;mesh models;mesh model;shape;keywords anatomical objects;belief propagation;three dimensional displays;shape matching;nonrigid matching;image registration;neuroanatomical objects;state space;registration;3d anatomical objects;spherical topology;global registration;belief maintenance;solid modelling belief maintenance image matching image registration iterative methods markov processes neurophysiology object recognition;markov processes;neurophysiology;point based;rigid registration;arbitrary shape;clamps;loopy belief propagation;hippocampi registeration loopy belief propagation dense point based registration neuroanatomical objects mesh models 3d anatomical objects markov random field nonrigid matching problem;dense correspondence;solid modelling;nonrigid matching problem	A dense point-based registration is an ideal starting point for detailed comparison between two neuroanatomical objects. This paper presents a new algorithm for global dense point-based registration between anatomical objects without assumptions about their shape. We represent mesh models of the surfaces of two similar 3D anatomical objects using a Markov Random Field and seek correspondence pairs between points in each shape. However, for densely sampled objects the set of possible point by point correspondences is very large. We solve the global non-rigid matching problem between the two objects in an efficient manner by applying loopy belief propagation. Typically loopy belief propagation is of order m^3 for each iteration, where m is the number of nodes in a mesh. By avoiding computation of probabilities of configurations that cannot occur in practice, we reduce this to order m^2. We demonstrate the method and its performance by registering hippocampi from a population of individuals aged 60-69. We find a corresponding rigid registration, and compare the results to a state-of-the-art technique and show comparable accuracy. Our method provides a global registration without prior information about alignment, and handles arbitrary shapes of spherical topology.	algorithm;belief propagation;casio loopy;computation;computational complexity theory;iteration;markov chain;markov random field;random sample consensus;rendering (computer graphics);shape analysis (digital geometry);software propagation;spaces;sparse matrix;state space	Pengdong Xiao;Nick Barnes;Paulette Lieby;Tibério S. Caetano	2010	2010 International Conference on Digital Image Computing: Techniques and Applications	10.1109/DICTA.2010.97	computer vision;computer science;machine learning;pattern recognition;mathematics;neurophysiology;belief propagation	Vision	49.359272476395596	-51.53333225842999	59898
d1d920dd86c5a585a19b3fca40dd0d0937e947aa	registration of multiple laser scans based on 3d contour features	3d laser scanner;mahalanobis distance;range data;optical scanners;image matching;surface fitting;contour feature;laser scanner;layout;independent component analysis;matrix algebra;self adaptive curve fitting;octree searching structure;mechanical engineering;multiple range data registration;multiple range data registration laser scan registration 3d contour features feature extraction self adaptive curve fitting octree searching structure mahalanobis distance leaf node matching transform matrix independent component analysis;image edge detection;global positioning system;target recognition;feature extraction;registration laser scanner contour feature mahalanobis distance matching;image registration;matching;stereo image processing;transforms;registration;leaf node matching;laser scanning;layout feature extraction image edge detection laboratories curve fitting target recognition global positioning system surface fitting mechanical engineering automation;laser scan registration;curve fitting;transform matrix;transforms curve fitting feature extraction image matching image registration independent component analysis matrix algebra octrees optical scanners stereo image processing;mahalanobis;distance;octrees;3d contour features;coordinate system;automation	When 3D laser scanner captures range data of real scenes, one of most important problems is how to align all range data into a common coordinate system. In this paper, we propose an algorithm of registration of multiple range data from real scenes using 3D contour features. Firstly, 3D contour features are extracted using self-adaptive curve fitting, and a searching structure of octree is built from the 3D contour features. Secondly, using Mahalanobis distance, the leaf nodes are matched between two scans to compute original transform matrix, and then transform matrix is refined step by step through ICP until a best transform matrix is obtained. Lastly, a new global registration strategy is given based on the nearby principle. The experiments of multiple range data registration from indoor scenes, outdoor scenes and ancient buildings are done, and the results show the proposed algorithm is robust	algorithm;align (company);contour line;curve fitting;experiment;information visualization;maxima and minima;octree;transformation matrix;tree (data structure)	Shaoxing Hu;Hongbin Zha;Aiwu Zhang	2006	Tenth International Conference on Information Visualisation (IV'06)	10.1109/IV.2006.91	computer vision;computer science;pattern recognition;computer graphics (images)	Robotics	49.5532208257793	-50.66418744659236	59973
8ce4085dadb91482ca0ad95771327462ac4fa27b	dense 3d slam in dynamic scenes using kinect		In this paper, we present a dense 3D SLAM method for dynamic scenes consisting on building, in real-time, a 3D map of the scene using Kinect. The method starts by segmenting and removing moving objects from the scene in order to avoid mismatches in the alignment step, then, calculates the scene current camera pose for each new acquisition. This method has the advantage of producing a dense map through the use of all pixels of the RGBD camera in order to achieve higher pose accuracy. The method also includes a loop closure detection thread to detect and merge duplicate regions. Quantitative evaluations using a various sets of scenes and benchmark datasets show that the proposed method produces a real-time 3D reconstruction with higher accuracy and lower trajectory error compared to the state-of-the-art methods.	kinect	Mohammed Chafik Bakkay;Majdi Arafa;Ezzeddine Zagrouba	2015		10.1007/978-3-319-19390-8_14	computer vision;computer graphics (images)	Robotics	52.078937549226055	-46.50061234508132	60182
92eab0d3008a92ead8556d7604f842b5c3154942	the ko-per intersection laserscanner and video dataset	lasers;intersections;scanners;cameras roads measurement by laser beam vehicles lasers frequency measurement;video cameras;traffic data;video cameras intelligent transportation systems object detection object tracking optical scanners road vehicles sensors;intersection perception systems ko per intersection laserscanner video dataset ko fas video cameras comprehensive dynamic model wireless communication multiobject detection multiobject tracking	Public intersections are due to their complexity challenging locations for drivers. Therefore the german joint project Ko-PER - which is part of the project initiative Ko-FAS has equipped a public intersection with several laserscanners and video cameras to generate a comprehensive dynamic model of the ongoing traffic. Results of the intersection perception can be communicated to equipped vehicles by wireless communication. This contribution wants to share a dataset of the Ko-PER intersection to the research community for further research in the field of multi-object detection and tracking. Therefor the dataset consists of sensor data from the laserscanners network and cameras as well as reference data and object labels. With that dataset, we aim to stimulate further research in this area.	mathematical model;object detection	Elias Strigel;Daniel Alexander Meissner;Florian Seeliger;Benjamin Wilking;Klaus C. J. Dietmayer	2014	17th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2014.6957976	computer vision;simulation;geography;video tracking;computer graphics (images)	Robotics	42.265067276933266	-41.53938906555945	60189
3261ad7b2710b4d8217f701ba28906146e950271	an adaptable, probabilistic, next-best view algorithm for reconstruction of unknown 3-d objects	three-dimensional displays;robot sensing systems;probabilistic logic;shape;mobile robots;gain measurement	Autonomous mobile robots perform many tasks, such as grasping and inspection, that may require complete models of three-dimensional (3-D) objects in the environment. If little or no knowledge about an object is known a priori, the robot must take sensor measurements from strategically determined viewpoints in order to reconstruct a 3-D model of the object. We propose an autonomous object reconstruction approach for mobile robots that is very general, with no assumptions about object shape or size, such as a bounding box or predetermined set of candidate viewpoints. A probabilistic, volumetric method for determining the optimal next-best view is developed based on a partial model of a 3-D object of unknown shape and size. The proposed method integrates an object probability characteristic to determine sensor views that incrementally reconstruct a 3-D model of the object. Experiments in simulation and on a real-world robot validate the work and compare it to the state of the art.	3d modeling;algorithm;autonomous robot;experiment;minimum bounding box;mobile robot;simulation	Jonathan Daudelin;Mark E. Campbell	2017	IEEE Robotics and Automation Letters	10.1109/LRA.2017.2660769	method;object-oriented design;robot;mobile robot;probabilistic logic;object model;viewpoints;minimum bounding box;computer vision;machine learning;computer science;artificial intelligence	Robotics	50.488439928618455	-39.79874840900773	60201
2b817e2863a2d98ab3aec88f3a280f417504c325	multi-modal tracking of people using laser scanners and video camera	surveillance;real time;laser scanner;mean shift;multi modal;computer vision;multiple people tracking;detection algorithm;people tracking;trajectory tracking;visual tracking;video camera	Inspite extensive research on visual tracking of multiple people in computer vision area, the robustness and usability of visual trackers are still discouraging. Recently, a few laser-based detection and tracking methods have been developed in robotics area. However, poor features provided by laser data make the tracker fail in many situations. In this paper, we present a novel method that aims at reliably detecting and tracking multiple people in an open area. Multiple laser scanners and one camera are used as input sensors. In detection stage, laser-based detection algorithm captures newly appeared people and initializes the mean-shift-based visual tracker. In tracking stage, laser-based feet trajectory tracking result and visual body region tracking result are combined with a decision-level Bayesian fusion method. The experimental results demonstrate reliable and real-time performance of the method. 2007 Elsevier B.V. All rights reserved.	3d scanner;algorithm;computer vision;mean shift;modal logic;real-time clock;robotics;sensor;usability;video tracking	Jinshi Cui;Hongbin Zha;Huijing Zhao;Ryosuke Shibasaki	2008	Image Vision Comput.	10.1016/j.imavis.2007.05.005	laser scanning;computer vision;simulation;mean-shift;tracking system;eye tracking;computer science;multimodal interaction;video tracking;computer graphics (images)	Vision	45.93428439213625	-43.915881204406254	60283
3303890312b31a3627e7abe75594aa90783f67ad	vision-based fuzzy 8051 surveillance systems design	surveillance system	This paper proposes a vision-based fuzzy 8051 surveillance system which emphasizes on designing the image processing function to detect the invaded states and the 8051 fuzzy servomotors controller to track the invader region. The techniques include discrete wavelet transformation (DWT) image data reducing, edge detection, motion detection, and fuzzy servomotor control. In this paper, image processing techniques are used for invader detection. Using a two-axis camera mount controlled by fuzzy controller, this system could make the camera exactly keeping the invader object on the center of scene. Also, the proposed tracking system has high potential in the guard of security management, the transaction authentication, the residential monitoring.	apache axis;discrete wavelet transform;edge detection;image processing;intel mcs-51;security management;systems design;tracking system;transaction authentication	Hsuan-Ming Feng;Chih-Yung Chen;Chia-Yun Wu	2007		10.1109/ICPADS.2007.4447784	embedded system;computer vision;simulation;computer science;computer security	Robotics	43.728593642546706	-42.22974778076245	60321
d8d59c328b885af5d7b57570f417816e56587cb3	a convolutional neural network based on double-tower structure for underwater terrain classification		Terrain classification plays a critical role in all robot systems especially in unknown environments. In recent years, researchers have proposed various algorithms to improve the efficiency and accuracy of terrain classification. Nevertheless, these methods still have some deficiencies in classification efficiency. In this paper, a double-tower convolutional neural network has been designed to implement end-to-end underwater terrain classification. The matched sonar image and visual image constitute an image pair, which is obtained at the same time by the sonar sensor and the visual sensor of the robot or underwater vehicle. The corresponding image pairs are set to be the input of the convolutional neural network, and the output of the network is the classification of the terrain. Then, terrain features from sonar and visual images are simultaneously applied to achieve terrain classification. Therefore, an end-to-end convolutional neural network with a classification function has been established in this paper.		Wei Liu;Rongxin Cui;Yang Li;Shuqiang Liu	2018	2018 3rd International Conference on Advanced Robotics and Mechatronics (ICARM)	10.1109/ICARM.2018.8610675	convolutional neural network;sonar;computer vision;terrain;robot;underwater;tower;computer science;artificial intelligence	Robotics	46.59034987594792	-38.99359371657787	60471
70a3e7e137864881548a096f1b073084c9bc1101	linearization of rotations for globally consistent n-scan matching	closed form solution;rigid body;rigid body transformation;image matching;n scan registration;geometry;three dimensional model;error function minimization;three dimensional;robot vision;matrix decomposition;three dimensional displays;linearisation techniques;scan matching;image registration;mathematical model;globally consistent n scan matching;iterative closest point;robot vision geometry image matching image registration linearisation techniques pose estimation;linearization;approximation methods;iterative closest point algorithm;iterative closest point algorithm robotics and automation clouds iterative algorithms closed form solution springs quaternions layout stress usa councils;global scan registration;n scan registration linearization globally consistent n scan matching iterative closest point icp algorithm geometric alignment three dimensional model pose estimate error function minimization closed form solution global scan registration rigid body transformation;pose estimate;quaternions;icp algorithm;geometric alignment;pose estimation	The ICP (Iterative Closest Point) algorithm is the de facto standard for geometric alignment of three-dimensional models when an initial relative pose estimate is available. The basis of the algorithm is the minimization of an error function that takes point correspondences into account. While four closed-form solution methods are known for minimizing this function, linearization seems necessary for solving the global scan registration problem. This paper presents such linear solutions for registering n-scans in a global and simultaneous fashion. It studies parameterizations for the rigid body transformations of the n-scan registration problem.	airborne ranger;algorithm;approximation algorithm;displacement mapping;euler;experiment;front and back ends;gradient descent;iterative closest point;iterative method;linear equation;matching (graph theory);mathematical optimization;system of linear equations	Andreas Nüchter;Jan Elseberg;Peter Schneider;Dietrich Paulus	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509306	three-dimensional space;computer vision;closed-form expression;mathematical optimization;rigid body;pose;rigid transformation;image registration;mathematical model;mathematics;geometry;matrix decomposition;linearization;iterative closest point;quaternion	Robotics	52.104813558486704	-51.098068486269426	60650
090da95a31b7608abcea27183ed931722a5c6e04	designing and experimenting with a distributed tracking system	object recognition;vision based trackers;resource discovery;tracking system;sensors;radar tracking;software services;real time;distributed processing;autonomous vision based trackers;information filtering;distributed tracking;tangible interface;image sensors;dynamic discovery distributed tracking vision based trackers software services;time factors;distributed environment;dynamic discovery;object tracking;empirical validation;distributed tracking system;dynamic resource discovery;augmented reality;visual servoing;information filters;tangible interfaces;cameras;tracking;tracking distributed processing image sensors object recognition;vision based sensors;cameras radar tracking filters application software augmented reality visual servoing computer interfaces concurrent computing distributed computing information science;object tracking distributed tracking system augmented reality visual servoing tangible interfaces autonomous vision based trackers vision based sensors dynamic resource discovery	Tracking objects is an important activity in many applications such as Augmented Reality, Visual Servoing, and Tangible Interfaces. Most of these applications are inherently dynamic and demand realtime response while tracking. Also, due to the issues of economics, such applications can benefit from tracking systems that do not depend on well-engineered setups and instead use inexpensive sensors for tracking. These requirements make distributed tracking a challenging problem that needs a thorough investigation. In this paper an approach to tracking, which uses the concepts of dynamic discovery, in a distributed environment containing autonomous vision-based trackers that use inexpensive sensors (such as Web cameras) is presented along with its empirical validation. The empirical results obtained from experiments are promising and validate the premise that Distributed Tracking Systems can be built using inexpensive vision-based sensors and concepts of dynamic resource discovery.	augmented reality;autonomous robot;experiment;requirement;sensor;tracking system;visual servoing	Girish G. Joshi;Rajeev R. Raje;Mihran Tuceryan	2008	2008 14th IEEE International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2008.103	computer vision;augmented reality;radar tracker;simulation;tracking system;computer science;sensor;cognitive neuroscience of visual object recognition;video tracking;image sensor;distributed computing;tracking;multimedia;visual servoing;distributed computing environment	Robotics	47.930574194954964	-41.98174629047181	60880
616db755e7b9359cd1c2d3398881285e81afd37a	a comparison between heuristic and machine learning techniques in fall detection using kinect v2	aal;software;heuristic;machine learning algorithms;magnetic heads;sensors;magnetic heads sensors floors training three dimensional displays software machine learning algorithms;training;fall detection;kinect;conference paper;machine learning;three dimensional displays;aal kinect fall detection machine learning heuristic;video signal processing biomechanics biomedical optical imaging filtering theory infrared imaging learning artificial intelligence medical signal processing;floors;infrared imaging heuristic learning techniques machine learning techniques video tagging machine learning methods automatic fall detection microsoft kinect v2 skeletal data signal filtering methods dataset adaptive boosting trigger algorithm adaboosttrigger algorithm false positive fall incidents sensor view partial obstructed field of view training dataset gesture detection complexity	In this paper, two algorithms were tested on 11 healthy adults: one based on heuristic and another one on video tagging machine learning methods for automatic fall detection; both utilizing Microsoft Kinect v2. For our heuristic approach, we used skeletal data to detect falls based on a set of instructions and signal filtering methods. For the machine learning approach, we implemented a dataset utilizing the Adaptive Boosting Trigger (AdaBoostTrigger) algorithm via video tagging to enable fall detection. For each approach, each subject on average has performed six true positive and six false positive fall incidents in two different conditions: one with objects partially blocking the sensor's view and one with partial obstructed field of view. The accuracy of each approach has been compared against one another in different conditions. The result showed an average of 95.42 % accuracy in the heuristic approach and 88.33 % in machine learning technique. We conclude that heuristic approach performs more accurately for fall detection when there is a limited number of training dataset available. Nevertheless, as the gesture detection's complexity increases, the need for a machine learning technique is inevitable.	algorithm;blocking (computing);heuristic;kinect;machine learning	Amin Amini;Konstantinos Banitsas;John Cosmas	2016	2016 IEEE International Symposium on Medical Measurements and Applications (MeMeA)	10.1109/MeMeA.2016.7533763	computer vision;speech recognition;computer science;machine learning	SE	39.215237468698035	-44.92611867581932	61008
03b1631f6a32b7b39b8bde6380f9b787a272372f	rolling shutter distortion removal based on curve interpolation	video signal processing curve fitting distortion image denoising image motion analysis interpolation smoothing methods video cameras;camera motion video shot rolling shutter camera rapid camera image distortion rectification method rolling shutter distortion removal curve interpolation object motion analysis image smoothing;interpolation;image motion analysis;cameras interpolation videos solid modeling accuracy delay algorithm design and analysis;video signal processing;post processing technique cmos image sensor rolling shutterdistortion curve interpolation;distortion;accuracy;cmos image sensor;smoothing methods;video cameras;solid modeling;rolling shutterdistortion;post processing technique;image denoising;curve fitting;curve interpolation;algorithm design and analysis;cameras;videos	In videos shot by rolling shutter cameras, rapid camera or object motion may cause obvious image distortions. This paper proposes a rectification method that removes rolling shutter distortions by performing curve interpolation for each pixel. This method does not require camera calibration, and it can handle camera and object motion simultaneously. The method only assumes that the motion is smooth. If camera motion alone causes distortions, a variation of this method requiring less computation can be used. Experiments on synthetic datasets and real videos show that the methods can remove rolling shutter distortions effectively.	algorithm;camera resectioning;computation;computational complexity theory;distortion;experiment;image rectification;interpolation;movie projector;optical flow;pixel;rectifier;synthetic data	Yufen Sun;Gang Liu	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6311354	algorithm design;computer vision;simulation;distortion;interpolation;image sensor;mathematics;accuracy and precision;solid modeling;statistics;curve fitting;computer graphics (images)	Vision	52.59046990562528	-50.73113053691641	61192
52e83973b94f6105abf15f90e263489c3c32fed9	sunshine-change-tolerant moving object masking for realizing both privacy protection and video surveillance	video surveillance;privacy protection;background subtraction;real adaboost;sunshine change	Recently, video surveillance systems have been widely introduced in various places, and protecting the privacy of objects in the scene has been as important as ensuring security. Masking each moving object with a background subtraction method is an effective technique to protect its privacy. However, the background subtraction method is heavily affected by sunshine change, and a redundant masking by over-extraction is inevitable. Such superfluous masking disturbs the quality of video surveillance. In this paper, we propose a moving object masking method combining background subtraction and machine learning based on Real AdaBoost. This method can reduce the superfluous masking while maintaining the reliability of privacy protection. In the experiments, we demonstrate that the proposed method achieves about 78–94% accuracy for classifying superfluous masking regions and moving objects. key words: privacy protection, video surveillance, background subtraction, Real AdaBoost, sunshine change	adaboost;background subtraction;closed-circuit television;experiment;machine learning;privacy	Yoichi Tomioka;Hikaru Murakami;Hitoshi Kitazawa	2014	IEICE Transactions	10.1587/transinf.2013EDP7465	computer vision;background subtraction;computer science;video tracking;internet privacy;computer security	Vision	41.301071809290086	-45.053168314840164	61265
2d401f6c5b503c9b1d56e94740120ceb324dbece	application of motion-based visual servoing to target tracking	image stabilization;target tracking;visual servoing	In this paper, the classical task of mobile target tracking using a pan-and-tilt camera is considered. The authors use recent results in motion-based visual servoing to deal with complex targets for which shape and texture are unknown. The first method consists of designing a control law directly from the estimated image motion. This leads to the computation of the pan-and-tilt acceleration necessary to reduce the tracking error. A second method, more efficient for target tracking, consists of retrieving the target position in the image from its estimated motion. This leads to classical image-based visual servoing. For both methods, experimental results obtained at video rate are presented and discussed. KEY WORDS—visual servoing, 2D image motion, target tracking, image stabilization	algorithm;authorization;computation;control system;experiment;image processing;motion estimation;optimal control;sensor;video tracking;visual servoing	Armel Crétual;François Chaumette	2001	I. J. Robotics Res.	10.1177/02783640122068164	computer vision;tracking system;visual servoing;image stabilization	Robotics	52.60068495692345	-39.84542786024591	61493
1c7e3ba4809445d93e19b544850febdebb88e34e	affine-invariant anisotropic detector for soft tissue tracking in minimally invasive surgery	computer vision community;computer assisted minimally invasive surgical procedure;detectors;biological tissues;kernel;minimal invasive surgery;performance evaluation;spatial implementation based second moment matrix;minimally invasive;surgical scene visual appearance;soft tissue tracking;surgery biological tissues biomechanics biomedical optical imaging endoscopes feature extraction fourier transform optics image registration image sequences medical robotics robot vision;affine invariant anisotropic detector;feature tracking;biomechanics;minimally invasive surgery anisotropic magnetoresistance detectors biological tissues computer vision navigation application software layout in vivo video sequences;data mining;in vivo video sequence;robotic assisted invasive surgical procedure affine invariant anisotropic detector soft tissue tracking computer assisted minimally invasive surgical procedure reliable feature tracking accurate tissue deformation recovery 3d anatomical registration surgical navigation approach computer vision community surgical scene visual appearance persistent feature tracking anisotropic pattern intrinsic fourier properties spatial implementation based second moment matrix state of the art feature detector comparison in vivo video sequence performance evaluation;medical robotics;computer vision;robotic assisted invasive surgical procedure;scale space;robot vision;shape;intrinsic fourier properties;3d anatomical registration;feature extraction;image registration;persistent feature tracking;endoscopes;surgery;fourier transform optics;soft tissue;surgical procedure;biomedical optical imaging;reliable feature tracking;anisotropic pattern;surgical navigation approach;state of the art feature detector comparison;accurate tissue deformation recovery;invariant feature;image sequences	Reliable feature tracking is important for accurate tissue deformation recovery, 3D anatomical registration and navigation in computer assisted minimally invasive surgical procedures. Despite a wide range of feature detectors developed in the computer vision community, direct application of these approaches to surgical navigation has shown significant difficulties due to the paucity of reliable feature landmarks coupled with free-form tissue deformation and contrastingly different visual appearances of changing surgical scenes. The purpose of this paper is to introduce an affine-invariant feature detector based on anisotropic features to ensure reliable and persistent feature tracking. A novel scale-space representation is proposed for scale adaptation based on the strength of the anisotropic pattern whereas affine adaptation relies on its intrinsic Fourier properties with an efficient spatial implementation based on the second moment matrix. The proposed detector is compared against the current state-of-the-art feature detectors and their respective performance is evaluated with in vivo video sequences recorded from robotic assisted minimally invasive surgical procedures.	affine shape adaptation;computer vision;kadir–brady saliency detector;moment matrix;motion estimation;robot;scale space;sensor;video-in video-out	Stamatia Giannarou;Marco Visentini Scarzanella;Guang-Zhong Yang	2009	2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2009.5193238	computer vision;detector;kernel;scale space;simulation;feature extraction;shape;computer science;image registration;biomechanics;mathematics;soft tissue	Vision	50.53131955249706	-49.29920983162121	61519
8f232dade1935d932ce54e3bd8fdf1304c9c2011	robust patch-based tracking via superpixel learning		Aimed at tracking non-rigid objects with geometric appearance changes over time, we propose a novel patch-based appearance model to adapt to the changes of topology. Meanwhile, as an effective online updating scheme, superpixel learning is adopted to select and update the patches when a new frame arrives. We build a foreground-background vote map via superpixels to determine the confidence of the patches in case of drifting. Experimental results show the proposed approach enables tracking non-rigid targets robustly and accurately. Keyword: Non-rigid objects, Patch-based Model, Superpixel, Foreground-background vote map	foreground-background;online algorithm	Qianwen Li;Yue Zhou	2014		10.1117/12.2064635	computer vision;simulation;engineering;data mining	AI	42.8230544502892	-49.076505188653904	61803
bc1b460f6646a50781f70a99448f1b77cb846b0a	adaptive video-based algorithm for accident detection on highways		For the past few decades, automatic accident detection, especially using video analysis, has become a very important subject. It is important not only for traffic management but also, for Intelligent Transportation Systems (ITS) through its contribution to avoid the escalation of accidents especially on highways. In this paper a novel vision-based road accident detection algorithm on highways and expressways is proposed. This algorithm is based on an adaptive traffic motion flow modeling technique, using Farneback Optical Flow for motions detection and a statistic heuristic method for accident detection. The algorithm was applied on a set of collected videos of traffic and accidents on highways. The results prove the efficiency and practicability of the proposed algorithm using only 240 frames for traffic motion modeling. This method avoids to utilization of a large database while adequate and common accidents videos benchmarks do not exist.	arm architecture;algorithm;benchmark (computing);closed-circuit television;computation;database;display resolution;f1 score;heuristic;optical flow;privilege escalation;real-time computing;sensor;tunneling protocol;video content analysis	Boutheina Maaloul;Abdelmalik Taleb-Ahmed;Smaïl Niar;Naim Harb;Carlos Valderrama	2017	2017 12th IEEE International Symposium on Industrial Embedded Systems (SIES)	10.1109/SIES.2017.7993382	statistic;feature extraction;motion detection;algorithm;intelligent transportation system;computer science;heuristic;computer vision;artificial intelligence;optical flow	Embedded	41.178429883497934	-44.49712227894216	61915
14b63fc99624858d84893d5db1f73e8e7d346d32	depth-aware indoor staircase detection and recognition for the visually impaired	indoor navigation;optical imaging cameras image edge detection optical device fabrication navigation transforms support vector machines;support vector machines;support vector machines assisted living hough transforms image classification image colour analysis indoor navigation mobile computing object recognition;svm based multiclassifier depth aware indoor staircase detection depth aware indoor staircase recognition visually impaired mobile vision based navigation indoor navigation rgb d camera concurrent parallel line set extraction hough transform support vector machine based multiclassifier;visually impaired rgb d camera staircase detection indoor navigation;navigation;optical imaging;rgb d camera;image edge detection;transforms;staircase detection;visually impaired;optical device fabrication;cameras	A mobile vision-based navigation aid is capable of assisting the visually impaired to travel independently, especially in unfamiliar environments. Despite many effective navigation algorithms having been developed in recent decades, accurate, efficient, and reliable staircase detection in indoor navigation still remains to be a challenging problem. In this paper, we propose an effective indoor staircase detection algorithm based on an RGB-D camera. The candidates of staircases are first detected from RGB frames by extracting a set of concurrent parallel lines based on Hough transform. The complement depth frames are further employed to recognize the staircase candidates as upstairs, downstairs, and negatives (i.e., corridors). A support vector machine (SVM) based multi-classifier is trained and tested for the staircase recognition with our newly collected staircase dataset. The detection and recognition results demonstrate the effectiveness and efficiency of the proposed algorithm.	algorithm;hough transform;support vector machine	Rafael Muñoz;Xuejian Rong;Yingli Tian	2016	2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2016.7574706	support vector machine;computer vision;navigation;simulation;computer science;machine learning;optical imaging;computer graphics (images)	Robotics	41.0322943967572	-44.23237904809422	62384
0c1a8d7077cc7ba7de32daf763ab36846352a08e	camera-pose estimation via projective newton optimization on the manifold	lie algebra;riemannian manifold differential geometry newton method pose estimation;minimization;levenberg marquardt;optimisation;manifolds;cost function;levenberg marquardt camera pose estimation projective newton optimization computer vision euclidean group riemannian manifold lie algebra optimization direction homeomorphic parameterization newton minimization;differential geometry;lie algebras;pose estimation cameras computer vision lie algebras optimisation;three dimensional;computer vision;vectors;computational complexity;three dimensional displays;riemannian manifold;manifolds cameras three dimensional displays cost function minimization vectors;algorithms computer simulation image enhancement image interpretation computer assisted imaging three dimensional models theoretical motion reproducibility of results sensitivity and specificity;newton method;cameras;pose estimation	Determining the pose of a moving camera is an important task in computer vision. In this paper, we derive a projective Newton algorithm on the manifold to refine the pose estimate of a camera. The main idea is to benefit from the fact that the 3-D rigid motion is described by the special Euclidean group, which is a Riemannian manifold. The latter is equipped with a tangent space defined by the corresponding Lie algebra. This enables us to compute the optimization direction, i.e., the gradient and the Hessian, at each iteration of the projective Newton scheme on the tangent space of the manifold. Then, the motion is updated by projecting back the variables on the manifold itself. We also derive another version of the algorithm that employs homeomorphic parameterization to the special Euclidean group. We test the algorithm on several simulated and real image data sets. Compared with the standard Newton minimization scheme, we are now able to obtain the full numerical formula of the Hessian with a 60% decrease in computational complexity. Compared with Levenberg-Marquardt, the results obtained are more accurate while having a rather similar complexity.	approximation algorithm;arabic numeral 0;computation (action);computational complexity theory;computer vision;gradient;hessian;iteration;levenberg–marquardt algorithm;loss function;mathematical optimization;maxima and minima;motion;muscle rigidity;mycobacterium phage marquardt;newton;newton's method;nonlinear system;numerical analysis;probability;seagate st1;semantic parameterization;time complexity;exponential;manifold	Michel Sarkis;Klaus Diepold	2012	IEEE Transactions on Image Processing	10.1109/TIP.2011.2177845	local tangent space alignment;three-dimensional space;lie algebra;computer vision;mathematical optimization;pose;topology;levenberg–marquardt algorithm;line field;manifold;computer science;mathematics;geometry;newton's method;newtonian dynamics;computational complexity theory;pseudo-riemannian manifold;manifold alignment	Vision	51.06217461961091	-51.23795663106151	62392
db71bb6354dd9baca9e1ef310802ae1f164e310e	mean shift blob tracking with kernel histogram filtering and hypothesis testing	tracking system;adaptive kalman filter;real time;optimal estimation;mean shift;kalman filter;object tracking;hypothesis testing;mean shift algorithm;model updating;model update;object model;hypothesis test	We propose a new adaptive model update mechanism for the real-time mean shift blob tracking. Since the Kalman filter has been used mainly for smoothing the object trajectory in the tracking system, it is novel for us to use adaptive Kalman filters for filtering object kernel histogram so as to obtain the optimal estimate of object model. The acceptance of the object estimate for the next frame tracking is determined by a robust criterion, i.e. the result of hypothesis testing with the samples from the filtering residuals. Therefore, the tracker can not only update object model in time but also handle severe occlusion and dramatic appearance changes to avoid over model update. We have applied the proposed method to track real object under the changes of scale and appearance with encouraging results. 2004 Elsevier B.V. All rights reserved.	algorithm;blob detection;computational complexity theory;kalman filter;kernel (operating system);mean shift;real-time clock;self-tuning;smoothing;tracking system	Ningsong Peng;Jie Yang;Zhi Liu	2005	Pattern Recognition Letters	10.1016/j.patrec.2004.08.023	computer vision;statistical hypothesis testing;mean-shift;computer science;pattern recognition;mathematics;statistics	Vision	44.45490098139915	-48.279154780717654	62430
fa33df075701b0994f0b34fe8356132fdc974034	a graph theoretic approach to direct processing of sparse unwarped panoramic images	vision system;graph theory;real time robot vision graph theoretic approach sparse unwarped panoramic image omnidirectional camera video surveillance autonomous robot navigation image processing technique feature extraction;robot vision cameras graph theory image processing;video surveillance;image processing;real time;sparse images;image interpolation;robot vision systems image processing cameras interpolation computer vision navigation feature extraction robot sensing systems image reconstruction intelligent robots;indexing terms;omnidirectional camera;robot vision;feature extraction;sparse images feature extraction;image processing techniques;panoramic image;autonomous robot;cameras	The use of omnidirectional cameras has had a significant impact on the success of vision systems for video surveillance and autonomous robot navigation. Typically images obtained from such cameras are transformed to sparse panoramic images that are interpolated prior to low level image processing. We present a graph theoretic approach that enables image processing techniques, principally feature extraction, to be performed directly on sparse panoramic images, avoiding the need for image interpolation. We thus aim to reduce the computational overheads of processing images arising from omnidirectional cameras, whilst retaining accuracy sufficient for application to real-time robot vision.	autonomous robot;closed-circuit television;feature extraction;graph theory;image processing;interpolation;omnidirectional camera;real-time locating system;robotic mapping;sparse matrix;while	Bryan W. Scotney;Sonya A. Coleman;Dermot Kerr	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312604	computer vision;simulation;index term;image processing;feature extraction;computer science;graph theory;image scaling;computer graphics (images)	Robotics	48.367264344326685	-42.39769763464665	62467
1454ead95a69cd33e8d5ae5eac1be155a71b111a	tracking multiple persons based on a variational bayesian model		Object tracking is an ubiquitous problem in computer vision with many applications in human-machine and human-robot interaction, augmented reality, driving assistance, surveillance, etc. Although thoroughly investigated, tracking multiple persons remains a challenging and an open problem. In this paper, an online variational Bayesian model for multiple-person tracking is proposed. This yields a variational expectation-maximization (VEM) algorithm. The computational efficiency of the proposed method is due to closed-form expressions for both the posterior distributions of the latent variables and for the estimation of the model parameters. A stochastic process that handles person birth and person death enables the tracker to handle a varying number of persons over long periods of time. The proposed method is benchmarked using the MOT 2016 dataset.	augmented reality;benchmark (computing);calculus of variations;computer vision;expectation–maximization algorithm;human–robot interaction;latent variable;markov chain monte carlo;sampling (signal processing);sensor;stochastic process;variational principle	Yutong Ban;Sileye O. Ba;Xavier Alameda-Pineda;Radu Horaud	2016		10.1007/978-3-319-48881-3_5	computer vision;econometrics;simulation;computer science;machine learning;statistics	Vision	46.84240645932107	-47.33576353442655	62725
03fabf18086a3ae637ed71ef6713dac2c0b432a2	task-oriented generation of visual sensing strategies in assembly tasks	data mining assembly systems sensor phenomena and characterization robotic assembly machine vision uncertainty entropy layout information analysis feature extraction;probability;laser ranging;sensing planning;task oriented evaluation task oriented generation visual sensing strategies assembly tasks task analysis face contact relations predicted success probability laser range finder;task oriented vision;robot vision;laser range finder;laser ranging industrial manipulators assembling robot vision probability;assembling;task analysis;visual features;cad based vision;vision based assembly;industrial manipulators;active vision	This paper describes a method of systematically generating visual sensing strategies based on knowledge of the assembly task to be performed. Since visual sensing is usually performed with limited resources, visual sensing strategies should be planned so that only necessary information is obtained efficiently. The generation of the appropriate visual sensing strategy entails knowing what information to extract, where to get it, and how to get it. This is facilitated by the knowledge of the task, which describes what objects are involved in the operation, and how they are assembled. In the proposed method, using the task analysis based on face contact relations between objects, necessary information for the current operation is first extracted. Then, visual features to be observed are determined using the knowledge of the sensor, which describes the relationship between a visual feature and information to be obtained. Finally, feasible visual sensing strategies are evaluated based on the predicted success probability, and the best strategy is selected. Our method has been implemented using a laser range finder as the sensor. Experimental results show the feasibility of the method, and point out the importance of task-oriented evaluation of visual sensing	feedback;robot;sensor;task analysis	Jun Miura;Katsushi Ikeuchi	1998	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.659931	computer vision;simulation;active vision;computer science;machine learning;probability;task analysis;statistics	Robotics	49.138087985846795	-39.34542102264796	62783
1c3361b63d579d73bbafeb4ae9ea215b30cfaf83	simultaneous localization and mapping using invariant natural features	image features;ccd camera;monocular vision;marine technology;mobile robot;mobile robots;simultaneous localization and mapping mobile robots particle filters indoor environments cameras robot vision systems costs marine technology sonar biomimetics;visual landmarks;rao blackwellised particle filter;invariant natural feature;scale invariant feature transform;particle filter;indoor environment;simultaneous localization and mapping;position estimation;kd tree;indoor environments;particle filters;extended kalman filter;invariant natural feature mobile robot particle filter simultaneous localization and mapping monocular vision;robot vision systems;cameras;biomimetics;sonar	This paper presents a practical approach to solve mobile robot simultaneous localization and mapping (SLAM) problem using natural visual landmarks obtained from a monocular vision. The Rao-Blackwellised particle filter (RBPF) is used to extend the path posterior by sampling new poses that integrate the current observation. The landmark position estimation and update is implemented through extended Kalman filter (EKF). Furthermore, the number of resampling steps is determined adaptively, which seriously reduces the particle depletion problem. Single CCD camera tracks the 3D natural landmarks, which are structured with matching image features extracted through Scale Invariant Feature Transform (SIFT). The matching for highly distinctive SIFT features described with multi-dimension vector descriptor is implemented with a KD-Tree in the time cost of O(log2N). And the matches with large error are eliminated by epipolar line constraint approach. The dense metric maps of natural 3D point landmarks for indoor environments is constructed. Experiments on the robot Pioneer3 in our real indoor environment indicate superior performance.	charge-coupled device;depletion region;epipolar geometry;experiment;extended kalman filter;map;mobile robot;particle filter;sampling (signal processing);scale-invariant feature transform;simultaneous localization and mapping	Nan Zhang;Maohai Li;Bingrong Hong	2006	2006 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2006.340219	mobile robot;computer vision;simulation;particle filter;computer science;artificial intelligence;marine technology	Robotics	52.8050993644218	-39.281991547842374	63022
ca0532eca7efcd8eba87b3130a152a79468262e7	tracking of articulated pose and motion with a markerized grid suit	model matching articulated pose tracking motion tracking markerized grid suit motion capture technology unencumbered vision based motion recovery marker assisted motion capture solution multiple high speed cameras customized bodysuit guided pose reconstruction guided motion reconstruction feature tracking;image motion analysis;guided pose reconstruction;image matching;markerized grid suit;biological system modeling;feature tracking;target tracking feature extraction image matching image motion analysis image reconstruction image sensors pose estimation;image sensors;motion tracking;motion capture technology;motion capture;unencumbered vision based motion recovery;computational modeling;image edge detection;feature extraction;image reconstruction;model matching;solid modeling;marker assisted motion capture solution;robustness;target tracking;articulated pose tracking;tracking cameras robustness solid modeling optical noise geometry high speed optical techniques optical computing costs motion analysis;high speed;cameras;tracking;multiple high speed cameras;customized bodysuit;guided motion reconstruction;pose estimation	Despite leaps in motion capture technology, the dichotomy between unencumbered vision-based motion recovery and the prevailing marker-assisted motion capture solution remains largely pertinent. This paper bridges the gap by introducing an attachment-free, full-body motion capture technique that employs multiple high-speed cameras and a customized bodysuit. The flexible bodysuit allows free movements while providing marker data for guided reconstruction of pose and motion. Our method overcomes several practical problems in existing systems, including long preparation time and displaced markers. The network of markerized pattern provides connectivity information and enhances robustness in feature tracking, model matching and 3D reconstruction.	3d reconstruction;approximation algorithm;attachments;edge detection;motion capture;motion estimation;rejection sampling;relevance;traffic enforcement camera;wire-frame model	Jayashree Karlekar;Sang N. Le;Anthony C. Fang	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761158	computer vision;motion capture;simulation;computer science;computer graphics (images)	Vision	50.40072581910432	-47.14891581902467	63117
a12e437ff1e03c05b244a1b5c54080c3a661722a	efficient model-based object pose estimation based on multi-template tracking and pnp algorithms		Three-Dimensional (3D) object pose estimation plays a crucial role in computer vision because it is an essential function in many practical applications. In this paper, we propose a real-time model-based object pose estimation algorithm, which integrates template matching and Perspective-n-Point (PnP) pose estimation methods to deal with this issue efficiently. The proposed method firstly extracts and matches keypoints of the scene image and the object reference image. Based on the matched keypoints, a two-dimensional (2D) planar transformation between the reference image and the detected object can be formulated by a homography matrix, which can initialize a template tracking algorithm efficiently. Based on the template tracking result, the correspondence between image features and control points of the Computer-Aided Design (CAD) model of the object can be determined efficiently, thus leading to a fast 3D pose tracking result. Finally, the 3D pose of the object with respect to the camera is estimated by a PnP solver based on the tracked 2D-3D correspondences, which improves the accuracy of the pose estimation. Experimental results show that the proposed method not only achieves real-time performance in tracking multiple objects, but also provides accurate pose estimation results. These advantages make the proposed method suitable for many practical applications, such as augmented reality.		Chi-Yi Tsai;Kuang-Jui Hsu;Humaira Nisar	2018	Algorithms	10.3390/a11080122	mathematics;feature (computer vision);3d pose estimation;algorithm;template matching;augmented reality;pose;planar;solver;homography	Robotics	49.71159926119554	-46.89669976876685	63164
3ddf4ab0aae4f8b56c22f647abb69ae7bb9eabee	neural network-based multiple robot simultaneous localization and mapping	unsupervised learning;radon transforms;radon transform;relative orientation;neural nets;cross correlation;image matching;occupancy grid;kalman filters;laser ranging;matching feature neural network multiple robot simultaneous localization and mapping decentralized platform slam extended kalman filter laser ranger grid map fusion algorithm multistep process image preprocessing cross correlation radon transform relative translation extraction self organizing map learning method unsupervised process feature extraction occupancy grid map map fusion problem;robot vision;learning methods;feature extraction;simultaneous localization and mapping;multi robot systems;data mining databases factual feedback models theoretical neural networks computer robotics;simultaneous localization and mapping slam map fusion radon transform self organizing map;on the fly;self organized map;extended kalman filter;slam robots;unsupervised learning feature extraction image matching kalman filters laser ranging multi robot systems neural nets radon transforms robot vision slam robots;simultaneous localization and mapping multirobot systems self organizing feature maps neural networks robot kinematics;neural network	In this paper, a decentralized platform for simultaneous localization and mapping (SLAM) with multiple robots is developed. Each robot performs single robot view-based SLAM using an extended Kalman filter to fuse data from two encoders and a laser ranger. To extend this approach to multiple robot SLAM, a novel occupancy grid map fusion algorithm is proposed. Map fusion is achieved through a multistep process that includes image preprocessing, map learning (clustering) using neural networks, relative orientation extraction using norm histogram cross correlation and a Radon transform, relative translation extraction using matching norm vectors, and then verification of the results. The proposed map learning method is a process based on the self-organizing map. In the learning phase, the obstacles of the map are learned by clustering the occupied cells of the map into clusters. The learning is an unsupervised process which can be done on the fly without any need to have output training patterns. The clusters represent the spatial form of the map and make further analyses of the map easier and faster. Also, clusters can be interpreted as features extracted from the occupancy grid map so the map fusion problem becomes a task of matching features. Results of the experiments from tests performed on a real environment with multiple robots prove the effectiveness of the proposed solution.	algorithm;artificial neural network;biological neural networks;cluster analysis;cross-correlation;encoder;experiment;extended kalman filter;extraction;fuse device component;gain;histogram;image fusion;matching;map (higher-order function);map segmentation;natural science disciplines;network theory;neural network simulation;on the fly;organizing (structure);preprocessor;radon;robot (device);robotics;self-organization;self-organizing map;simultaneous localization and mapping;teaching method;unsupervised learning;verification of theories;ranger;statistical cluster	G. Saeedi SajadSaeedi;Liam Paull;Michael Trentini;Howard Li	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/TNN.2011.2176541	kalman filter;computer vision;generative topographic map;radon transform;self-organizing map;feature extraction;computer science;artificial intelligence;cross-correlation;machine learning;extended kalman filter;artificial neural network;simultaneous localization and mapping	Robotics	48.813537858262954	-38.6144356086889	63175
7a2bdaeb00964d917a071b7f70ad5215ebef655e	robust visual tracking using discriminative stable regions and k-means clustering	discriminative stable region;journal;期刊论文;visual tracking;k means clustering	This paper presents a method of extracting discriminative stable regions (DSRs) from image, and applies them for object tracking. These DSRs obtained by using the criterion of maximal entropy and spatial discrimination present high appearance stability and strong spatial discriminative power, which enables them to tolerate more appearance variations and to effectively resist spatial distracters. Meanwhile, the adaptive fusion tracking incorporated k-means clustering can handle severe occlusion as well as disturbance of motion noise during target localization. In addition, an effective local update scheme is designed to adapt to the object change for ensuring the tracking robustness. Experiments are carried out on several challenging sequences and results show that our method performs well in terms of object tracking, even in the presence of occlusion, deformation, illumination change, moving camera and spatial distracter.	cluster analysis;discriminative model;k-means clustering;video tracking	Canlong Zhang;Zhongliang Jing;Han Pan;Bo Jin;Zhixin Li	2013	Neurocomputing	10.1016/j.neucom.2012.12.020	computer vision;eye tracking;computer science;machine learning;pattern recognition;k-means clustering	ML	42.455509576368385	-49.957787029325516	63257
30862662623f6b70a4c631b7de02081a817d60e6	"""commentary paper 2 on """"multi-view access monitoring and singularization in interlocks"""""""	multiple background models;background modeling;multiview access monitoring;surveillance;image classification;bibliographies;background subtraction algorithm;multiple views;monitoring;feature extraction;background subtraction;classification algorithms;lighting;singularization detection;object detection;floors	This is a commentary paper on ldquoMulti-view access monitoring and singularization in interlocksrdquo. Stefano et al. (2008) presents a multi-view approach to access monitoring and classification of single or multiple occupants (singularization detection) in access restricted areas (interlocks). Two main contributions are made by Stefano et al. (2008). One is a robust background subtraction algorithm using multiple background models in various illuminations. This algorithm is used for floor area and floor border segmentation. The second contribution is to extract features from multiple views (two views in the experiments) for singularization detection.	algorithm;background subtraction;experiment	Gang Qian	2008	2008 IEEE Fifth International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2008.41	statistical classification;computer vision;contextual image classification;speech recognition;background subtraction;feature extraction;computer science;machine learning;data mining;lighting	Vision	40.404131797114374	-46.826028737819335	63341
617a06ebdfe4943374782b04ee6fb1d3af442aed	non-gibbsian markov random field models for contextual labelling of structured scenes	markov random field;conditional probability	In this paper we propose a non-Gibbsian Markov random field to model the spatial and topological relationships between objects in s tructured scenes. The field is formulated in terms of conditional probabilities le arned from a set of training images. A locally consistent labelling of new scen es is achieved by relaxing the Markov random field directly using these condit ional probabilities. We evaluate our model on a varied collection of several hundred handsegmented images of buildings.	graph coloring;loss function;markov chain;markov random field;monoid factorisation;optimization problem;robustness (computer science);sampling (signal processing);statistical classification	Daniel Heesch;Maria Petrou	2007		10.5244/C.21.88	markov chain;markov kernel;random field;conditional probability;markov property;computer science;machine learning;pattern recognition;mathematics;markov renewal process;markov process;markov model;statistics;variable-order markov model	Vision	45.790158151406104	-50.67262430128915	63434
a8e3357d86c332c8328116f816e64e8fa33cbf4f	head and facial action tracking: comparison of two robust approaches	3d;video sequence;eyebrow movements;hidden feature removal;occlusion;robust statistics facial action tracking 3d head movements tracking lip movements eyebrow movements video sequence occlusions facial adaptive texture model;image sequence analysis;robust statistics;head tracking;facial adaptive texture model;statistical analysis gesture recognition hidden feature removal image sequences image texture;image texture;3d head tracking;robust;3d model;adaptive appearance model;statistical analysis;mixture model;facial action tracking;lip movements;statistics;robustness statistics tracking video sequences active appearance model shape matrix decomposition eyebrows humans face;model;mixture models 3d head tracking facial action tracking 3d model adaptive appearance model occlusion robust statistics;image texture analysis;mixture models;3d head movements tracking;occlusions;gesture recognition;image sequences	In this work, we address a method that is able to track simultaneously 3D head movements and facial actions like lip and eyebrow movements in a video sequence. In a baseline framework, an adaptive appearance model is estimated online by the knowledge of a monocular video sequence. This method uses a 3D model of the face and a facial adaptive texture model. Then, we consider and compare two improved models in order to increase robustness to occlusions. First, we use robust statistics in order to downweight the hidden regions or outlier pixels. In a second approach, mixture models provides better integration of occlusions. Experiments demonstrate the benefit of the two robust models. The latter are compared under various occlusions	baseline (configuration management);database;experiment;extrapolation;high- and low-level;mixture model;pixel;polygonal modeling	Romain Hérault;Franck Davoine;Yves Grandvalet	2006	7th International Conference on Automatic Face and Gesture Recognition (FGR06)	10.1109/FGR.2006.63	computer vision;speech recognition;geography;pattern recognition	Vision	46.290834932459525	-49.20507139608139	63570
7bc96a4b8165e283bc25139da5d88333db6309d9	road geometry classification by adaptive shape models	appearance based methods road geometry classification adaptive shape models vision based road detection approach transportation autonomous driving vehicle collision warning pedestrian crossing detection low level road appearance scene geometry low level features road homogeneity lighting conditions scene composition analysis temporal coherence analysis prototypical road geometries large scale experiments road geometry classifier state of the art methods road shape information;geometry;image classification;shape recognition;classification;computer vision;pedestrians;computer models;traffic information systems;roads;roads geometry shape adaptation models hidden markov models lighting training;feature extraction;transportation;image analysis;support vector machine gist holistic representation illuminant invariant image classification road detection scene classifier scene recognition spatial pyramids;lighting;detection and identification systems;transportation computer vision feature extraction geometry image classification lighting pedestrians roads shape recognition traffic information systems;geometric segments	Vision-based road detection is important for different applications in transportation, such as autonomous driving, vehicle collision warning, and pedestrian crossing detection. Common approaches to road detection are based on low-level road appearance (e.g., color or texture) and neglect of the scene geometry and context. Hence, using only low-level features makes these algorithms highly depend on structured roads, road homogeneity, and lighting conditions. Therefore, the aim of this paper is to classify road geometries for road detection through the analysis of scene composition and temporal coherence. Road geometry classification is proposed by building corresponding models from training images containing prototypical road geometries. We propose adaptive shape models where spatial pyramids are steered by the inherent spatial structure of road images. To reduce the influence of lighting variations, invariant features are used. Large-scale experiments show that the proposed road geometry classifier yields a high recognition rate of 73.57% ± 13.1, clearly outperforming other state-of-the-art methods. Including road shape information improves road detection results over existing appearance-based methods. Finally, it is shown that invariant features and temporal information provide robustness against disturbing imaging conditions.	algorithm;autonomous car;coherence (physics);collision detection;experiment;high- and low-level;statistical classification;texture mapping	José Manuel Álvarez;Theo Gevers;Ferran Diego;Antonio M. López	2013	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2012.2221088	computer vision;transport;contextual image classification;image analysis;simulation;feature extraction;biological classification;computer science;machine learning;lighting	Vision	42.02616160589085	-45.54614407497012	63633
456bdf6a68973913774b2b381e1b8609c35d69e0	a generalized mean shift tracking algorithm	mean shift;video target tracking;camshift;mean shift camshift video target tracking similarity measure;similarity measure	CAMSHIFT algorithm and Comaniciu/Meer algorithm are two fundamental frameworks of mean shift procedure for video target tracking. This paper generalizes the two well-known mean shift tracking algorithms, originally due to Bradski and Comaniciu/Meer. A new general similarity function which defines the distance between the target model and target candidate is employed to calculate the pixel weights and the target location. The target size is iteratively estimated and updated based on the zeroth order moment of the pixel weights. Then we prove that both the CAMSHIFT algorithm and the Comaniciu/Meer algorithm can be included in the generalized mean shift tracking framework. The tracking performances of three mean shift algorithms in the unified framework are shown and compared in the experimental results.	algorithm;mean shift;performance;pixel;similarity measure;unified framework	Jianjun Chen;Suofei Zhang;Guocheng An;Zhenyang Wu	2011	Science China Information Sciences	10.1007/s11432-011-4359-8	computer vision;speech recognition;mean-shift;pattern recognition;mathematics	Vision	43.424440167130335	-51.04423772279281	63804
414522c51f3126aecf3b3648d2a3f2f3f8b55cda	human pose estimation using two rgb-d sensors	image segmentation;filling;indexes;decision support systems pattern analysis indexes image segmentation pose estimation filling calibration;rgb d cameras human pose estimation non rigid registration;decision support systems;pattern analysis;calibration;pose estimation	Accurate human pose estimation plays an important role in various applications such as sports analysis, health care and gaming. Even though recent approaches have shown that 3D positions of body joints can be estimated from a single depth sensor, the depth data often suffer from sensing noise and self-occlusion. In this paper, we present a novel pipeline to estimate the pose of a human body by using two depth sensors. The two sensors simultaneously capture the front and back of the body's movement. Using a wide-baseline RGB-D camera calibration algorithm, the two 3D scans are first geometrically aligned, and then registered to a generic human template using a Gaussian-mixture-model based point set registration procedure with local structure constraints. The new pose of person is finally estimated by a rigid bone-based pose transformation. Experimental results demonstrate the effectiveness of our system in estimating the body pose over other state-of-the-arts techniques.	3d pose estimation;algorithm;baseline (configuration management);camera resectioning;hidden surface determination;image noise;mixture model;point set registration;range imaging;sensor	Wanxin Xu;Po-Chang Su;Sen-Ching S. Cheung	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532564	database index;computer vision;calibration;simulation;pose;3d pose estimation;decision support system;computer science;pattern recognition;articulated body pose estimation;image segmentation	Robotics	49.12939896099865	-48.76566227730663	63830
e61f3f99479cfe062d7f94b748e9381ebd3ce03a	anchor node localization for wireless sensor networks using video and compass information fusion	computer communication networks;models theoretical;localization;wireless sensor network;magnetometer;geographic information systems;video recording;algorithms;triangulation;wireless technology;video camera;anchor node	Distributed sensing, computing and communication capabilities of wireless sensor networks require, in most situations, an efficient node localization procedure. In the case of random deployments in harsh or hostile environments, a general localization process within global coordinates is based on a set of anchor nodes able to determine their own position using GPS receivers. In this paper we propose another anchor node localization technique that can be used when GPS devices cannot accomplish their mission or are considered to be too expensive. This novel technique is based on the fusion of video and compass data acquired by the anchor nodes and is especially suitable for video- or multimedia-based wireless sensor networks. For these types of wireless networks the presence of video cameras is intrinsic, while the presence of digital compasses is also required for identifying the cameras' orientations.	computation (action);conflict (psychology);geographic coordinate system;global positioning system;gray platelet syndrome;mandatory - hl7definedroseproperty;mental orientation;multimedia;node - plant part;retinal cone;triangulation (geometry);magnetometers	Dan Pescaru;Daniel Curiac	2014		10.3390/s140304211	embedded system;magnetometer;wireless sensor network;internationalization and localization;telecommunications;triangulation;computer science;engineering;geographic information system;key distribution in wireless sensor networks;computer network	Mobile	50.62582623155839	-39.902176454592244	63922
5edd0660e5a12e8b9a2653edfe6504f2e62cadf0	robust tracking for interactive social video	video object;facebook robust tracking interactive social video video objects tracking interactive multimedia systems adaptive algorithm visual content tracking system interactive hypervideo system video content social network;tracking system;video signal processing;tracking color servers streaming media feature extraction robustness video sequences;color;video signal processing interactive systems multimedia computing object tracking social sciences computing;004 informatik;video sequences;interactive multimedia;multimedia computing;social network;adaptive algorithm;servers;streaming media;social sciences computing;feature extraction;object tracking;robustness;interactive systems;tracking	We propose a novel approach for tracking video objects in interactive multimedia systems. Instead of designing a single tracking algorithm that works well with some templates (video objects to be tracked) but fails with others, our adaptive algorithm uses three distinct tracking techniques with different strengths and weaknesses. The tracking technique that is selected for a given template depends on its visual content and the content of the video. Our approach is robust and allows tracking requests of users to be computed despite an imprecise selection of a template. In addition, our approach is designed to comply with high precision, speed, and scalability to support many users simultaneously. As an exemplary testing environment for this tracking system, an interactive hypervideo system was chosen. The system was integrated into the social network Facebook and used by more than 12,000 users.	adaptive algorithm;amazon elastic compute cloud (ec2);computation;graphical user interface;hidden surface determination;hypervideo;linear approximation;microsoft outlook for mac;particle filter;real-time computing;real-time locating system;requirement;robustness (computer science);scalability;social network;tracking system;video card	Stefan Wilk;Stephan Kopf;Wolfgang Effelsberg	2012	2012 IEEE Workshop on the Applications of Computer Vision (WACV)	10.1109/WACV.2012.6163022	computer vision;tracking system;feature extraction;computer science;video tracking;tracking;multimedia;interactive media;world wide web;server;robustness;social network	Vision	41.59734533894581	-47.9386044738008	64018
7ef8bbdc90520bedcadf4389fcd282cbc57bf459	dense disparity features for fast stereo vision	computer programming;algorithms;vision	A novel stereo vision algorithm suitable for real-time autonomous robot applications is proposed. The method extracts dense segments of constant disparity using a similarity metric based on the sum of absolute differences. The resolution of matching image segments can be defined adaptively to allow variable detail. The algorithm is tested on reference data-sets and self captured images and is shown to produce promising results. Handling of regions without texture is also discussed.	algorithm;autonomous robot;binocular disparity;experiment;machine vision;map;real-time clock;requirement;stereopsis;sum of absolute differences	John A. Kalomiros	2011	Proceedings of the 6th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems	10.1117/1.JEI.21.4.043023	3d reconstruction;computer stereo vision;vision;stereo cameras;computer vision;computer science;computer programming;computer graphics (images)	Robotics	52.05475476861684	-46.69386760154487	64045
3dfacbfd91d3db1934b4abf0c58817abdb546ea7	real-time skin color detection under rapidly changing illumination conditions	human computerinteraction skin color detection dynamic adaptation color correction illumination condition;image color analysis skin lighting face adaptation models face detection computational modeling;human computer interaction;real time systems face recognition human computer interaction image colour analysis;skin color detection;computer model;real time;skin;human computerinteraction;illumination condition;dynamic threshold;skin color;computational modeling;face recognition;image color analysis;image colour analysis;color correction;face;model updating;color correction strategy real time skin color detection illumination condition cue vision based human computer interaction bayesian decision framework face detection dynamic thresholding technique skin color model;lighting;face detection;adaptation models;dynamic adaptation;real time application;real time systems	Skin color provides a useful cue for vision based human-computer interaction (HCI). However, rapidly changing illumination conditions under HCI application environment make skin color detection a challenging task, as skin colors in an image highly depend on the illumination under which the image was taken. This paper presents a method for skin color detection under rapidly changing illumination conditions. Skin colors are modeled under the Bayesian decision framework. Face detection is employed to online sample skin colors and a dynamic thresholding technique is used to update the skin color model. When there is no face detected, color correction strategy is employed to convert the colors of the current frame to those as they appear under the same illuminant of the last model updated frame. Skin color detection is then applied on the color corrected image. Face detection is time-consuming and hence should not be applied to every frame in real-time applications on general consumer hardware. To improve efficiency, a novel method is proposed to detect illumination changes, and face detection is used to update the skin color model only if the illumination has changed. Experimental results show that the proposed method can achieve satisfactory performance for skin color detection under rapidly changing illumination conditions in real-time on general consumer hardware.	color;effective method;experiment;face detection;human–computer interaction;illumination (image);pixel;printing;real-time clock;real-time transcription;sampling (signal processing);thresholding (image processing)	Leyuan Liu;Nong Sang;Saiyong Yang;Rui Huang	2011	IEEE Transactions on Consumer Electronics	10.1109/TCE.2011.6018887	facial recognition system;face;computer vision;face detection;color normalization;computer science;lighting;skin;color balance;computational model;computer graphics (images)	Vision	44.021373018083004	-49.419078059974744	64174
b308706f194e273e19731711db6986863b2db798	uav, do you see me? establishing mutual attention between an uninstrumented human and an outdoor uav in flight	tracking cameras feature extraction motion estimation real time systems computer vision;motion estimation;computer vision;feature extraction;mobile robots autonomous aerial vehicles gesture recognition human robot interaction;cameras;tracking;human robot interaction outdoor uav autonomous normal flight uninstrumented human user periodic waving gesture uav wobble behavior;real time systems	We present the first demonstration of establishing mutual attention between an outdoor UAV in autonomous normal flight and an uninstrumented human user. We use the familiar periodic waving gesture as a signal to attract the UAV's attention. The UAV can discriminate this gesture from human walking and running that appears similarly periodic. Once a signaling person is observed and tracked, the UAV acknowledges that the user has its attention by hovering and performing a “wobble” behavior. Both parties are now ready for further interaction. The system works on-board the UAV using a single camera for input and is demonstrated working reliably in real-robot trials.	autonomous robot;computer vision;on-board data handling;relevance;stationary process;statistical classification;unmanned aerial vehicle;wiggle stereoscopy;with high probability	Valiallah Monajjemi;Jake Bruce;Seyed Abbas Sadat;Jens Wawerla;Richard T. Vaughan	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353882	computer vision;simulation;feature extraction;computer science;engineering;motion estimation;tracking	Robotics	46.64896719085143	-40.7010253178772	64376
1fe8f472c56a4d771f9fc6e057d0c23b2645936b	shape recovery of rotating object using weighted voting of spatio-temporal image	image sequences image reconstruction stereo image processing feature extraction optical tracking;information science;rotating object;computer graphics;shape recovery;computer industry;shape voting cameras data mining feature extraction information science computer graphics computer industry stereo vision charge coupled devices;data mining;charge coupled devices;shape;optical tracking;sequential images;voting;feature extraction;image reconstruction;stereo image processing;stereo vision;silhouettes;silhouettes 3d shape recovery rotating object spaciotemporal images sequential images feature extraction tracking;3d shape recovery;cameras;tracking;spaciotemporal images;image sequences	"""We propose a method to recover the 3D shape of an object rotating on a turnable. A spacio-temporal image is made of the sequential images taken by a single camera. Then, the trajectories which correspond to the 3D points on the surface are extracted in the spacio-temporal image by using """"weighted voting"""" of all intensity values on a constrained surface. Since the method consequently utilize intensity information as it is, the method can recover dense 3D positions compared with the one using feature extraction and tracking. Also, it can recover concave shapes unlike the one using silhouettes of the object. The experimental results with real images show the effectiveness of our method."""		Masatoshi Okutomi;Shigeki Sugimoto	2000		10.1109/ICPR.2000.905517	iterative reconstruction;computer vision;voting;feature extraction;information science;shape;computer science;stereopsis;machine learning;pattern recognition;geometry;tracking;computer graphics;computer graphics (images)	Vision	49.26663282464895	-49.19144367558095	64464
07886b21b45353e91be651681f29359707c1b8e4	detection of representative frames of a shot using multivariate wald-wolfowitz test	gunshot detection systems testing streaming media indexing information retrieval data mining histograms layout sampling methods motion analysis;histograms;video summarization;video streaming;image segmentation;video signal processing;video retrieval;testing;distance measurement;motion segmentation;streaming media;image representation;feature extraction;indexation;multimedia communication;video streaming feature extraction image representation image segmentation video retrieval video signal processing;algorithms;video stream segmentation representative frame detection multivariate wald wolfowitz test video retrieval video summarization;hypothesis test	For efficient indexing, browsing and retrieval of video data and also for video summarization, extraction of representative frames is essential. Once a video stream is segmented into shots, the representative frames or key-frames for the shot are selected. Automatic selection of suitable representatives for a wide variety of shots is still a challenge as the number of such frames in a shot may also vary depending on the variation in the content. In this work, we propose a novel scheme that relies on Wald-Wolfowitz runs test based hypothesis testing to detect the subshots within a shot and then for each subshot, the frame rendering the highest fidelity is extracted as the key-frame. Experimental result shows that the scheme works satisfactorily for a wide variety of shots.	digital video;emoticon;key frame;shot transition detection;streaming media	Partha Pratim Mohanta;Sanjoy Kumar Saha;Bhabatosh Chanda	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761403	computer vision;statistical hypothesis testing;feature extraction;computer science;histogram;multimedia;software testing;image segmentation;information retrieval;statistics	Vision	39.20407478637105	-51.6019096553397	64501
6a6b3ea0a10b7a27800b3db6fc83dff4890d57e1	cloud tracking for solar irradiance prediction		We propose a video analytic system to segment and track clouds for the purpose of solar irradiance prediction. Ground-based imaging sensors are used to monitor potential sun occlusions for solar irradiance drop prediction, which can be used to assist power ramp control strategies. Sky images are first rectified to remove fisheye artifacts. Cloud pixels are segmented and classified into low-to-high transparencies. Evolution of cloud boundaries are tracked using optical flow. Tracking extrapolation predicts future cloud movements and deformations as well as potential sun occlusions. To accurately estimate solar irradiance drop, we propose a novel scheme based on back-projecting the predicted sun occlusion onto the evolving cloud boundary to count for cloud transparency and irradiance drop estimation. Experimental results show that short-term solar irradiance drop is predictable with reasonable accuracy.	extrapolation;fisheye;image rectification;optical flow;pixel;ramp simulation software for modelling reliability, availability and maintainability;sensor;transparency (projection)	Ming-Ching Chang;Yi Yao;Guan Li;Yan Tong;Peter H. Tu	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8297111	pixel;computer vision;solar irradiance;extrapolation;image segmentation;artificial intelligence;irradiance;sky;cloud computing;computer science;optical flow	Vision	40.68448905699673	-41.29597564933602	64736
691c61981344faffaf3ad3366265e107baad9b9d	fire recognition based on correlation of segmentations by image processing techniques	image processing;fire detection;hsi;correlation;saturation	This article provides a novel method to recognize fires in areas monitored by a color video camera. Formulated with a graphical programming language, the developed software utilizes the HSI attributes to extract, statically and dynamically, the pixels of flames with high intensity (I value) as well as those of peri-flame regions within a specific range of saturation (S value). The other pixels that are filtered out are regarded as the backgrounds. Some sample clips of fires and pseudo fires are processed with the software in which the high intensity and the specific saturation regions can be effectively segmented from the images. For each fire clip, after analyzing the areas of foreground pixels for both I value and S value along the time axis, there exists a high correlation between both sequences. The pseudo fire clips, on the other hand, do not demonstrate high correlations after the same processing. Therefore, a fire incident can be identified according to the correlation of both extracted pixel areas. This promising achievement has laid down a firm basis for the development of a novel fire detecting alarm system in the near future.	apache axis;color;horizontal situation indicator;image processing;pixel;sensor;verification and validation;visual programming language	Yong-Ren Pu;Yung-Jen Chen;Su-Hsing Lee	2015	Machine Vision and Applications	10.1007/s00138-015-0698-6	computer vision;simulation;image processing;computer science;correlation;saturation;computer graphics (images)	ML	46.29805408928698	-41.88401196983405	64931
41f153c6910afc360a85ba0bf48b5ae21ec1969e	revisiting uncertainty in graph cut solutions	uncertainty handling computer vision graph theory inference mechanisms learning artificial intelligence probability;graph theory;probability;image segmentation;uncertainty;training;inference mechanisms;uncertainty handling;journal article;computer vision;computational modeling;graphical models;inference algorithms;optimization;learning artificial intelligence;maximum likelihood learning uncertainty assessment map assignment large scale graphical models computer vision inference algorithms loopy belief propagation min marginals generative probabilistic model training likelihood maximization dynamic graph cut operations multilabel marginal distributions;computational modeling training uncertainty inference algorithms graphical models optimization image segmentation	Graph cuts is a popular algorithm for finding the MAP assignment of many large-scale graphical models that are common in computer vision. While graph cuts is powerful, it does not provide information about the marginal probabilities associated with the solution it finds. To assess uncertainty, we are forced to fall back on less efficient and inexact inference algorithms such as loopy belief propagation, or use less principled surrogate representations of uncertainty such as the min-marginal approach of Kohli & Torr [8]. In this work, we give new justification for using min-marginals to compute the uncertainty in conditional random fields, framing the min-marginal outputs as exact marginals under a specially-chosen generative probabilistic model. We leverage this view to learn properly calibrated marginal probabilities as the result of straightforward maximization of the training likelihood, showing that the necessary subgradients can be computed efficiently using dynamic graph cut operations. We also show how this approach can be extended to compute multi-label marginal distributions, where again dynamic graph cuts enable efficient marginal inference and maximum likelihood learning. We demonstrate empirically that - after proper training - uncertainties based on min-marginals provide better-calibrated probabilities than baselines and that these distributions can be exploited in a decision-theoretic way for improved segmentation in low-level vision.	belief propagation;casio loopy;conditional random field;cut (graph theory);expectation–maximization algorithm;framing (world wide web);graph cuts in computer vision;graphical model;high- and low-level;marginal model;maxima and minima;min-max heap;multi-label classification;software propagation;statistical model;subderivative;theory	Daniel Tarlow;Ryan P. Adams	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247958	graph cuts in computer vision;computer vision;uncertainty;computer science;graph theory;machine learning;pattern recognition;probability;mathematics;graphical model;image segmentation;computational model;statistics	Vision	46.774663355312406	-51.907697772529225	65074
c11ea13d46b93bc895f316ccb0c3cd7203ccbb63	human detection in fish-eye images using hog-based detectors over rotated windows	detection algorithms;support vector machines;training;testing;support vector machine human detection fish eye camera histogram of oriented gradient;cameras training feature extraction support vector machines testing merging detection algorithms;feature extraction;human detection;overlapping windows fish eye images hog based detectors fish eye cameras omniview video recording human detection algorithm histogram of oriented gradient svm classifier hog features;merging;support vector machine;video recording cameras object detection;histogram of oriented gradient;fish eye camera;cameras	Fish-eye cameras are efficient means to provide an omni-view video recording over a large area using a single camera. Although effective algorithms for human detection in images captured by conventional cameras have been developed, human detection in fish-eye images remains an open challenge. Recognizing that humans typically appear on radial lines emitted from the center in fish-eye images, we propose to apply the popular human detection algorithm based on the Histogram of Oriented Gradient (HOG) features after rotating each search window on a radial line to the vertical reference line. We extract positive and negative examples by such rotations to train the SVM classifier using HOG features. To detect humans in a given image, we rotate the image successively and detect windows containing humans along the reference line after each rotation using the trained classifier. We use multiple window sizes to detect people with different appearance sizes. We further develop an algorithm to discover multiple overlapping windows covering the same person and identify the window that encloses the person the best. The proposed method has yielded highly accurate human detection in low-resolution, low-contrast images containing multiple people with varying poses and sizes.	algorithm;eye tracking;gradient;microsoft windows;radial (radio);sensor;video;window function	An-Ti Chiang;Yao Wang	2014	2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2014.6890553	support vector machine;computer vision;computer science;machine learning;pattern recognition;computer graphics (images)	Vision	39.83698726048653	-47.20914930766774	65145
2fa3260c377b47d76358f15e074e83128ca5d177	a 3d-point-cloud feature for human-pose estimation	trees mathematics feature extraction pose estimation robot vision;three dimensional displays feature extraction shape vegetation joints sensors estimation;trees mathematics;robot vision;feature extraction;geometric distribution 3d point cloud feature human pose estimation human motions cognitive capabilities geometric feature tree structure local properties global properties shape feature extraction;pose estimation	Estimating human poses is an important step towards developing robots that can understand human motions and improving their cognitive capabilities. This paper presents a geometric feature for estimating human poses from a 3D point cloud input. The proposed feature can be considered as an extension of the idea of visual features, such as color/edge, of color/grayscale images, and it contains the geometric structure of the point cloud. It is derived by arranging the 3D points into a tree structure, which preserves the global and local properties of the 3D points. Shown experimentally, the tree structure (spatial ordering) is particularly important for estimating human poses (i.e., articulated objects). The 3D orientation (pan, tilt and yaw angles) and shape features are then extracted from each node in the tree to describe the geometric distribution of the 3D points. The proposed feature has been evaluated on a benchmark dataset and compared with two existing geometric features. Experimental results show that the proposed feature has the lowest overall error in human-pose estimation.	3d pose estimation;benchmark (computing);experiment;feature extraction;grayscale;k-nearest neighbors algorithm;point cloud;preprocessor;real-time clock;real-time computing;robot;thresholding (image processing);time complexity;tree structure;vector field histogram;vish (game);yaws	Kai-Chi Chan;Cheng-Kok Koh;C. S. George Lee	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630787	computer vision;pose;feature extraction;computer science;machine learning;kanade–lucas–tomasi feature tracker;pattern recognition;feature	Robotics	43.820925095613134	-50.329006449229645	65232
b7ebb715bf5b3100f2aebe85e7847755e05cf2bc	semantic superpixel based vehicle tracking	video surveillance;automobiles;video surveillance automobiles object detection object tracking traffic engineering computing;object tracking;semantics target tracking vehicles detectors silicon vectors videos;traffic engineering computing;false target reduction semantic superpixel based multiple vehicle tracking real world traffic videos vehicle interactions vehicle occlusions object local appearance characteristics capturing object spatial relation capturing mid level feature offline trained semantic object detector superpixel temporal coherency false trajectory reduction;object detection	This paper focuses on tracking multiple vehicles in real-world traffic videos which is very challenging due to frequent interactions and occlusions between different vehicles. To address these problems, we fall back on superpixel which recently has received great attention in a wide range of vision problems, e.g. object segmentation, tracking and recognition, for its ability of capturing local appearance characteristics of objects and their spatial relations. As a mid-level feature, however, superpixel itself is unable to carry semantic information which may restricts their use in these problems. To this end, we introduce semantic information into superpixel from an offline trained semantic object detector and successfully deploy it into the multiple vehicle tracking problem. The benefits of semantic superpixel include: (1) it gains better temporal coherency of superpixel; (2) the effectiveness and robustness of occlusion handling are improved; (3) benefited from semantic analysis, false targets and false trajectories are significantly reduced. Experiments show significant accuracy improvements of our approach in comparison with existing tracking methods.	algorithm;belief propagation;experiment;high- and low-level;interaction;online and offline;sensor;software propagation;vehicle tracking system	Liwei Liu;Junliang Xing;Haizhou Ai;Shihong Lao	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		computer vision;simulation;computer science;video tracking;multimedia	Vision	41.2550984764973	-46.29855659350649	65238
c44274a643ace0005ece3f9630e50b667add9607	a robust monte carlo tracking algorithm based on feature adaptive selection	target tracking classification algorithms heuristic algorithms histograms monte carlo methods image edge detection;online learning;monte carlo samping;image colour analysis;feature extraction;object tracking;monte carlo samping object tracking online learning adaptive selection half forgotten update strategy;adaptive selection;half forgotten update strategy;learning artificial intelligence;particle filtering numerical methods feature extraction image colour analysis learning artificial intelligence monte carlo methods object tracking;object tracking robust monte carlo tracking algorithm dynamic scenario particle filter tracking tracking failure complex background feature adaptive selection mechanism online adaboost algorithm discriminative feature drift phenomena half forgotten sample set update strategy brand new construction mode candidate feature pool color feature pyramid gradient orientation histogram feature severe appearance change drift recovery;monte carlo methods;particle filtering numerical methods	"""We propose a novel Monte Carlo tracking algorithm which can work robustly under complex dynamic scenario. Firstly, for the problem that particle filter tracking framework is prone to make the tracking failure under complex background when the features have low discriminative abilities, we design a feature adaptive selection mechanism based on online Adaboost algorithm. This mechanism can choose the most discriminative features online. Secondly, considering that online Adaboost algorithm is easy to cause """"drift"""" phenomena as well as the features in the candidate feature pool are not reliable, we propose a novel half-forgotten sample set update strategy and a brand-new construction mode for the candidate feature pool which is based on color and pyramid gradient orientation histogram feature. Experimental results show that our tracker is able to handle severe appearance change and recover from drifts in realistic videos. The algorithm proposed in this paper can track the objects accurately and reliably compared with other existing object tracking algorithm."""	adaboost;clutter;genetic algorithm;gradient;metropolis;metropolis–hastings algorithm;monte carlo method;particle filter;principle of maximum entropy;sampling (signal processing)	Yuanchen Qi;Chengdong Wu;Dongyue Chen;Li Wang	2012	2012 IEEE 12th International Conference on Computer and Information Technology	10.1109/CIT.2012.180	computer vision;feature extraction;computer science;artificial intelligence;machine learning;video tracking;pattern recognition;feature;statistics;monte carlo method	Vision	42.95034517016968	-48.867920839095866	65276
448ded01ebc429a601bfd390bc9f73ae009602ae	three-dimensional planar model estimation using multi-constraint knowledge based on k-means and ransac	model extraction;ransac multi plane;three dimensional planes;info eu repo semantics article;computer vision	Plane model extraction from three-dimensional point clouds is a necessary step in many different applications such as planar object reconstruction, indoor mapping and indoor localization. Different RANdom SAmple Consensus (RANSAC)-based methods have been proposed for this purpose in recent years. In this study, we propose a novel method-based on RANSAC called Multiplane Model Estimation, which can estimate multiple plane models simultaneously from a noisy point cloud using the knowledge extracted from a scene (or an object) in order to reconstruct it accurately. This method comprises two steps: first, it clusters the data into planar faces that preserve some constraints defined by knowledge related to the object (e.g., the angles between faces); and second, the models of the planes are estimated based on these data using a novel multi-constraint RANSAC. We performed experiments in the clustering and RANSAC stages, which showed that the proposed method performed better than state-of-the-art methods.	cluster analysis;experiment;k-means clustering;point cloud;random sample consensus	Marcelo Saval-Calvo;Jorge Azorín López;Andrés Fuster Guilló;José García Rodríguez	2015	Appl. Soft Comput.	10.1016/j.asoc.2015.05.007	computer vision;ransac;simulation;computer science;machine learning;computer graphics (images)	Vision	50.58748899272807	-48.13796943392118	65346
44da2dfa4e332efb0518a36ed2ffa120b9f0a91d	a new framework for stereo sensor pose through road segmentation and registration	on board stereo camera pose;levenberg marquardt;differential evolution;evolutionary computation;image segmentation;lms algorithm;image processing;road detection;optimization technique;non linear optimization;real time;levenberg marquardt algorithm stereo sensor pose road segmentation road registration onboard stereo head feature extraction 3d space image domain featureless method optimization techniques differential evolution algorithm;indexing terms;road segmentation;road segmentation differential evolution algorithm featureless image registration illuminant invariant image non linear optimization on board stereo camera pose road detection;brightness;roads;feature extraction;roads cameras image segmentation image registration stereo image processing brightness algorithm design and analysis;image registration;stereo image processing;traffic engineering computing;traffic engineering computing evolutionary computation image registration image segmentation stereo image processing;illumination invariance;featureless image registration;illuminant invariant image;algorithm design;algorithm design and analysis;cameras;differential evolution algorithm	This paper proposes a new framework for real-time estimation of the onboard stereo head's position and orientation relative to the road surface, which is required for any advanced driver-assistance application. This framework can be used with all road types: highways, urban, etc. Unlike existing works that rely on feature extraction in either the image domain or 3-D space, we propose a framework that directly estimates the unknown parameters from the stream of stereo pairs' brightness. The proposed approach consists of two stages that are invoked for every stereo frame. The first stage segments the road region in one monocular view. The second stage estimates the camera pose using a featureless registration between the segmented monocular road region and the other view in the stereo pair. This paper has two main contributions. The first contribution combines a road segmentation algorithm with a registration technique to estimate the online stereo camera pose. The second contribution solves the registration using a featureless method, which is carried out using two different optimization techniques: 1) the differential evolution algorithm and 2) the Levenberg-Marquardt (LM) algorithm. We provide experiments and evaluations of performance. The results presented show the validity of our proposed framework.	computer performance;differential evolution;estimation theory;experiment;feature extraction;genetic algorithm;image registration;levenberg–marquardt algorithm;location-based service;mathematical optimization;optic axis of a crystal;pixel;real-time clock;reflections of signals on conducting lines;requirement;stereo camera;synthetic intelligence	Fadi Dornaika;José Manuel Álvarez;Angel Domingo Sappa;Antonio M. López	2011	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2011.2117420	algorithm design;computer vision;simulation;image processing;computer science;machine learning;evolutionary computation;computer graphics (images)	Vision	50.348632008452725	-42.27102146376226	65574
dabf52800dfece1108de279b297015f68bb8f2a4	structure from motion-a critical analysis of methods	equation non lineaire;ecuacion no lineal;estructura 3 dimensiones;movimiento;image processing;singular value;procesamiento imagen;picture processing;polynomial constraints structure recovery picture processing 2d image frames shape recovery critical analysis structure from motion polynomial essential parameter matrix;spatial structure;matrix algebra;motion;satisfiability;traitement image;polynomials matrix algebra picture processing;polynomials;structure 3 dimensions;algorithme;motion analysis nonlinear equations polynomials motion estimation helium shape artificial intelligence joining processes mirrors reflection;algorithm;polynomial method;mouvement;estimacion parametro;synthetic data;parameter estimation;estimation parametre;non linear equation;structure from motion;algoritmo	Abstruct-There are a multitude of equations, methods, formulations and solutions in the literature for solving the structure from motion problem. It is shown how such methods can be mathematically related by certain changes, some of these changes preserve the essential qualities of the original system, while other changes do not. Existing methods for structure from motion are classified and analyzed and mathematical relationships between them are shown. Their behavior on the same synthetic data is evaluated, examining the effect of noise, initial estimates, and the amount of motion. The missing necessary polynomial constraint on the singular values of Tsai and Huang's essential parameter matrix is shown, and derive related linear methods for less general SFM problems, and their necessary polynomial constraints. The experiments show that for data where the constraints are satisfied, linear methods work correctly and quickly, where they are not applicable, polynomial methods may work but are slower.	experiment;polynomial;structure from motion;synthetic data	Charles Jerian;Ramesh Jain	1991	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.97478	computer vision;mathematical optimization;combinatorics;discrete mathematics;structure from motion;image processing;artificial intelligence;motion;control theory;mathematics;geometry;estimation theory;singular value;statistics;polynomial;satisfiability;synthetic data	Robotics	53.046558006266224	-50.974127535863474	65726
455410aa8ccae23d06362175eb6848ad1d5c3c61	sdf tracker: a parallel algorithm for on-line pose estimation and scene reconstruction from depth images	robot vision image reconstruction motion estimation pose estimation;cost function sdf tracker parallel algorithm online pose estimation scene reconstruction depth images ego motion estimation environment mapping robotics depth camera truncated signed distance function;motion estimation;three dimensional displays cameras surface reconstruction image reconstruction simultaneous localization and mapping estimation vectors;robot vision;image reconstruction;datavetenskap datalogi;datavetenskap;computer science;pose estimation	Ego-motion estimation and environment mapping are two recurring problems in the field of robotics. In this work we propose a simple on-line method for tracking the pose of a depth camera in six degrees of freedom and simultaneously maintaining an updated 3D map, represented as a truncated signed distance function. The distance function representation implicitly encodes surfaces in 3D-space and is used directly to define a cost function for accurate registration of new data. The proposed algorithm is highly parallel and achieves good accuracy compared to state of the art methods. It is suitable for reconstructing single household items, workspace environments and small rooms at near real-time rates, making it practical for use on modern CPU hardware.	3d pose estimation;algorithmic efficiency;bittorrent tracker;central processing unit;computational complexity theory;display resolution;function representation;graphics processing unit;loss function;match moving;motion estimation;online and offline;parallel algorithm;real-time computing;real-time transcription;reflection mapping;robotics;six degrees of separation;workspace	Daniel Ricao Canelhas;Todor Stoyanov;Achim J. Lilienthal	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6696880	iterative reconstruction;computer vision;simulation;pose;3d pose estimation;computer science;motion estimation;computer graphics (images)	Robotics	52.83911368980105	-47.702940677432586	65728
59451ba0b2c8e60eb5dff3cc91b1456ac2ce2428	furniture layout ar application using floor plans based on planar object tracking	object recognition;human computer interaction;image matching;color rectangles recognition furniture layout ar application augmented reality system floor plans planar object tracking methods object pose estimation object position estimation matching procedure feature point descriptor furniture cg models human whistle sounds;furniture;layout humans image color analysis target tracking augmented reality pattern recognition cameras;user interfaces augmented reality feature extraction furniture human computer interaction image colour analysis image matching object recognition object tracking pose estimation;image colour analysis;feature extraction;object tracking;augmented reality;user interfaces;pose estimation	In this paper, we propose a new approach of Augmented Reality (AR) system for the furniture layout based on a planar object tracking. The planar object tracking methods using natural features are effective methods to estimate the object's pose and position in the AR applications because we are able to use the natural images. However, most of the feature descriptors have a lot of matching procedure. Therefore, by using an efficient feature point descriptor which is very fast both to build and to match, we track the planar objects. Especially, we use floor plans as the planar objects, and then furniture CG models are overlaid on the floor plans. This is because the floor plans are presented in the selection of rooms for rent or buy. Therefore, this system helps borrowers and buyers to select some mansion or apartment rooms. In this system, we propose to use human whistle sounds and color rectangles recognition to operate the furniture layout. In order to show the effectiveness of our proposed system, we perform some planar object tracking experiments when we applied the proposed system to some floor plans.	3d floor plan;augmented reality;bmc remedy action request system;book;color;experiment;graphics processing unit;head-mounted display;user interface	Taiki Fuji;Yasue Mitsukura;Toshio Moriya	2012	2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2012.6343828	computer vision;augmented reality;simulation;pose;feature extraction;computer science;cognitive neuroscience of visual object recognition;video tracking;user interface;computer graphics (images)	Robotics	47.795516273020105	-43.16449592981921	66000
7e4df1e05d76da459da97cc3d3a2a7fa4c36e70f	3-d motion estimation from motion field	closed form solution;linear system of equations;motion estimation;three dimensional;visual motion;motion perception;image sequence;optical flow;linear equations;nonlinear system	Abstract   Several experiments suggest that the first stage of motion perception is the measurement of visual motion. The result of this stage is called the  motion field , which assigns a velocity vector to each point in the image plane. The second stage involves interpreting the motion field in terms of objects and motion in the three-dimensional world. Recovering 3-D motion of the object from the motion field has been difficult owing to the nonlinear system of equations involved, and the sensitivity of the system to noise. The need for the stability of the system is essential as only the optical flow field can be recovered from a sequence of images, which is at best a crude approximation to the motion field.  We define two sets of “basic” parameters, which can be recovered from the motion field by solving a linear system of equations. The relationship between the basic parameters and the motion parameter being one-to-one and linear, we obtain a closed form solution for the 3-D motion parameter by solving a system of linear equations only. We prove the correctness, completeness and robustness of the approach and in that sense the problem of recovering the motion parameter from the motion field may be said to be “solved”. We present the results of extensive experimentation with real and simulated image sequences.	motion estimation;motion field	Naresh C. Gupta;Laveen N. Kanal	1995	Artif. Intell.	10.1016/0004-3702(95)00031-3	system of linear equations;three-dimensional space;computer vision;closed-form expression;mathematical optimization;structure from motion;hyperbolic motion;motion perception;nonlinear system;complex harmonic motion;motion estimation;constant of motion;optical flow;control theory;mathematics;motion system;linear equation;motion field;mechanics of planar particle motion;linear motion	Vision	53.261514856782725	-51.0027997226004	66102
109ca7fb05fbe89cc6f23d0e44c29141ed1f3b52	coupled multi-object tracking and labeling for vehicle trajectory estimation and matching	non linear object labelling;neural networks;motion based tracking;spectrum;detection and tracking of moving objects;automatic detection;object tracking;environmental change;neural network	Efficient detection and tracking of moving objects in real life conditions is a very challenging research issue, mainly due to occlusions, illumination variations, appearance (disappearance) of new (existing) objects and overlapping issues. In this paper, we address these difficulties by incorporating non-linear and recursive identification mechanisms in motion-based detection and tracking algorithms. Non-linearity allows correct identification of object of complex visual properties while the adaptability makes the proposed scheme able to update its behaviour to the dynamic environmental changes. In addition, in this paper, we introduce the concept of polar spectrum which is a measure for determining the deviation of a vehicle trajectory from an ideal trace. The proposed methods (object tracking and trajectory matching) are applied in survey engineering problems dealing with safe design road turns. In particular, the automatically detected trajectory of a moving vehicle is compared with the ideal trace, through the polar spectrum measure, to determine the safety of a road turn. This trace is also compared with the one manually derived using photogrammetric algorithms and a small error is obtained verifying the efficiency of the method.	algorithm;artificial neural network;automatic identification and data capture;autoregressive integrated moving average;comparison and contrast of classification schemes in linguistics and metadata;convolution;kalman filter;linear algebra;linear classifier;motion detector;nonlinear system;object detection;opening (morphology);particle filter;photogrammetry;real life;recursion;recursion (computer science);sensor;streaming media;verification and validation	Nikolaos D. Doulamis	2009	Multimedia Tools and Applications	10.1007/s11042-009-0370-0	spectrum;computer vision;simulation;environmental change;tracking system;computer science;machine learning;video tracking;artificial neural network	Robotics	42.110003625439056	-47.181114027918944	66466
58a9bd06dc3a1005b9fdb9178a3633966743e19e	an exemplar-based crf for multi-instance object segmentation	shape image segmentation labeling deformable models layout image color analysis object segmentation;shape appearance adaptation exemplar based crf multiinstance object segmentation joint detection joint segmentation multiple object instances scene understanding data driven method instance segmentation reference image shape mask crf framework object appearance shape deformation object occlusion map inference problem;conference paper;random processes image segmentation inference mechanisms object detection	We address the problem of joint detection and segmentation of multiple object instances in an image, a key step towards scene understanding. Inspired by data-driven methods, we propose an exemplar-based approach to the task of instance segmentation, in which a set of reference image/shape masks is used to find multiple objects. We design a novel CRF framework that jointly models object appearance, shape deformation, and object occlusion. To tackle the challenging MAP inference problem, we derive an alternating procedure that interleaves object segmentation and shape/appearance adaptation. We evaluate our method on two datasets with instance labels and show promising results.	algorithm;conditional random field;instance (computer science);pixel	Xuming He;Stephen Gould	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.45	active shape model;computer vision;computer science;viola–jones object detection framework;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	45.23294221248224	-51.21365941592937	66638
0320c3e2b31fb744d567b4fcd3a4866cb37e64eb	statistical background model-based target detection	kernel density estimation;statistical background model;self tuning spectral clustering;global and local information;self adaptive gaussian mixture model	This paper proposes a statistical background modeling framework to deal with the issue of target detection, where the global and local information is utilized to achieve more accurate detection of moving objects. Specifically, for the target detection problem under illumination change conditions, a novel self-adaptive Gaussian mixture model mixed with the global information is utilized to construct a statistical background model to detect moving objects; for the target detection problem under dynamic background conditions, the self-tuning spectral clustering method is first utilized to cluster background images, and then the kernel density estimation method mixed with the local information is utilized to construct a statistical background model to detect moving objects. Experimental results demonstrate that the proposed framework can improve the detection performance under illumination change conditions or dynamic background conditions.	algorithm;cluster analysis;kernel density estimation;mixture model;self-tuning;spectral clustering	Xiangxiang Li;Songhao Zhu;Lingling Chen	2015	Pattern Analysis and Applications	10.1007/s10044-015-0495-x	kernel density estimation;background subtraction;computer science;machine learning;pattern recognition;mathematics;statistics	AI	43.163179988823394	-49.420321897019775	66774
4ad9a38935c08cfb5d312d5578fbf3176a948f3b	dynamic indoor localization based on active rfid for healthcare applications: a shape constraint approach	average estimation error dynamic indoor localization rfid location tracking algorithm healthcare cost function optimal reference tag positions signal strength similarity signal strength disparity geometrical correlation properties shape constraint algorithm;signal strength;object recognition;shape constraints;location tracking;geometrical correlation properties;healthcare;cost function;radar tracking;telemedicine;optimal reference tag positions;shape recognition;telemedicine health care indoor radio object recognition radiofrequency identification shape recognition;signal strength similarity;shape constraint algorithm;accuracy;signal strength disparity;average estimation error;medical services;shape;estimation;healthcare system;heuristic algorithms;rfid;radiofrequency identification active rfid tags medical services shape hospitals rfid tags biomedical imaging cost function estimation error biomedical engineering;indoor radio;estimation error;location tracking algorithm;radiofrequency identification;dynamic indoor localization;health care	RFID-based localization has received considerable attention within the healthcare industry and has made a major impact on the healthcare system in hospitals. In this paper, we propose a new location tracking algorithm that aims to improve the accuracy of indoor localization for healthcare applications. In our approach, a cost function associated with a shape constraint factor is used to find the optimal reference tag positions that enclose the tracking tag. The cost function consists of the similarity and disparity of signal strength between the tracking and reference tags, as well as geometrical correlation properties. The experimental results indicate that the proposed shape constraint algorithm provides considerable improvement in average estimation error as compared with existing methods. We believe that this new algorithm is of potential value in dynamic location tracking of objects for healthcare applications.	binocular disparity;constraint algorithm;loss function;radio-frequency identification	Wei-Hong Chen;Herbert H. Chang;Tzong-Huei Lin;Po-Chou Chen;L. K. Chen;S. J. Hwang;David Hung-Tsang Yen;Hanna S. Yuan;Woei-Chyn Chu	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5304900	signal strength;computer vision;estimation;simulation;shape;mathematics;health care;statistics	Robotics	50.12923833697331	-50.82266817919489	66887
d48dd7d71cedf5735bdc756fb2213562e5600ff6	visual sensor based abnormal event detection with moving shadow removal in home healthcare applications	accidental falls;image processing computer assisted;abnormal event detection;delivery of health care;home care services;vision ocular;algorithms;humans;telemetry;ubiquitous healthcare surveillance;shape features variation shadow removal algorithm;movement;visual sensor	Vision-based abnormal event detection for home healthcare systems can be greatly improved using visual sensor-based techniques able to detect, track and recognize objects in the scene. However, in moving object detection and tracking processes, moving cast shadows can be misclassified as part of objects or moving objects. Shadow removal is an essential step for developing video surveillance systems. The goal of the primary is to design novel computer vision techniques that can extract objects more accurately and discriminate between abnormal and normal activities. To improve the accuracy of object detection and tracking, our proposed shadow removal algorithm is employed. Abnormal event detection based on visual sensor by using shape features variation and 3-D trajectory is presented to overcome the low fall detection rate. The experimental results showed that the success rate of detecting abnormal events was 97% with a false positive rate of 2%. Our proposed algorithm can allow distinguishing diverse fall activities such as forward falls, backward falls, and falling asides from normal activities.	accidental falls;algorithm;area striata structure;assisted living;biological science disciplines;closed-circuit television;computer vision;ground truth;minimum bounding box;object detection;physical object;sensor;the quality of life;usb;velocity (software development);biologic segmentation	Young-Sook Lee;Nguyen Trung Hau	2012		10.3390/s120100573	movement;computer vision;simulation;telecommunications;computer science;engineering;telemetry;computer security	Vision	40.06734613254849	-44.45100775545874	67106
4c94a8d811c31b87a2f6224d3bc2629485c4eb8e	on-vehicle video-based parking lot recognition with fisheye optics	parking lots;object recognition;image processing;sensors;video signal processing;video signal processing image sensors object recognition traffic engineering computing;image sensors;vehicles cameras estimation accuracy calibration sensors image processing;proximity detectors;video sensors on vehicle video based parking lot recognition fisheye optics parking space park assistance systems car park acoustic feedback visual feedback vehicle navigation;algorithms;traffic engineering computing;video imaging detectors;pattern recognition systems;cameras	The search for free parking space in a crowded car park is a time-consuming and tedious task. Today's park assistance systems provide the driver with acoustic or visual feedback when approaching an obstacle or semi-autonomously navigate the vehicle into the parking lot. However, finding a free parking lot is usually left to the driver. In this paper, we address this search problem via video sensors only. This can be used as a help to the driver to quickly pass a parking deck and, more important, can be regarded as a cornerstone to fully autonomously parking vehicles.	acoustic cryptanalysis;fisheye;multi-storey car park;search problem;semiconductor industry;sensor	Sebastian Houben;Matthias Komar;Andree Hohm;Stefan Lüke;Marcel Neuhausen;Marc Schlipsing	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728595	embedded system;computer vision;simulation;engineering;parking guidance and information	Robotics	45.01740030861342	-41.485974964439734	67133
2b401f00fd1f2ea8097971ee1421441a274b09e2	a framework for wrong way driver detection using optical flow	image quality;weather condition;traffic surveillance;optical flow;mixture of gaussians	In this paper a solution to detect wrong way drivers on highways is presented. The proposed solution is based on three main stages: Learning, Detection and Validation. Firstly, the orientation pattern of vehicles motion flow is learned and modelled by a mixture of gaussians. The second stage (Detection and Temporal Validation) applies the learned orientation model in order to detect objects moving in the lane’s opposite direction. The third and final stage uses an Appearance-based approach to ensure the detection of a vehicle before triggering an alarm. This methodology has proven to be quite robust in terms of different weather conditions, illumination and image quality. Some experiments carried out with several movies from traffic surveillance cameras on highways show the robustness of the proposed solution.	closed-circuit television;experiment;image quality;mixture model;optical flow	Gonçalo Monteiro;Miguel Ribeiro;João Marcos;Jorge Batista	2007		10.1007/978-3-540-74260-9_99	image quality;computer vision;simulation;computer science;mixture model;optical flow;computer security	Vision	41.91367187466418	-46.131235946002576	67187
8f85772d46c2d420e0889242a717fbcead71e7af	segmentation of dense depth maps using inertial data a real-time implementation	vision system;robot sensing systems;object recognition;sensor systems;sensor phenomena and characterization;image segmentation;real time;inertial navigation;robot navigation;mobile robots;data mining;geometric feature;road vehicles computer vision inertial navigation computerised navigation image segmentation real time systems stereo image processing mobile robots;computer vision;automated car driving systems;real time systems machine vision robot sensing systems sensor systems robot vision systems cameras data mining object recognition navigation sensor phenomena and characterization;navigation;stereo camera system real time system dense depth maps inertial data computer vision inertial sensor geometric features image segmentation autonomous robotics automated car driving systems;machine vision;stereo image processing;stereo camera system;inertial data;3d representation;real time implementation;real time system;physical environment;dense depth maps;geometric features;point of view;depth map;autonomous robotics;robot vision systems;inertial sensor;autonomous robot;cameras;road vehicles;real time systems;computerised navigation	We propose a real-time system that extracts information from dense relative depth maps. This method enables the integration of depth cues on higher level processes including segmentation of structures, object recognition, robot navigation or any other task that requires a 3D representation of the physical environment. Inertial sensors coupled to a vision system can provide important inertial cues for the ego-motion and system pose. In this work we explore the integration of inertial sensor data in vision systems. Depth maps obtained by vision systems, are very point of view dependant, providing discrete layers of detected depth aligned with the camera. We use inertial sensors to recover the camera pose, and rectify the maps to a reference ground plane, enabling the segmentation of vertical and horizontal geometric features. The aim of this work is a fast real-time system, so that it can be applied to autonomous robotic systems or to automated car driving systems, for modelling the road, identifying obstacles and roadside features in real-time.	map;real-time clock	Jorge Lobo;Luis Tadeu Almeida;Jorge Dias	2002		10.1109/IRDS.2002.1041368	mobile robot;embedded system;computer vision;navigation;simulation;machine vision;computer science;cognitive neuroscience of visual object recognition;image segmentation;inertial navigation system;depth map	Robotics	50.539739252332865	-39.40887945376255	67340
eb58b16380d18f02681d10240f13023d57746382	view planning of a multi-rotor unmanned air vehicle for tree modeling using silhouette-based shape estimation		The use of a multi-rotor unmanned air vehicle (UAV) in image acquisition tasks is promising for three-dimensional (3D) object modeling. Such an autonomous data acquisition system can be useful to handle the geometric complexity of objects such as trees and the inherent difficulties of image capture. In this paper, we address the problem of view planning for a camera-equipped multi-rotor UAV to acquire an adequate set of images that leads to more detailed and complete knowledge of the 3D tree model. The proposed algorithm based on shape-from-silhouette methods incorporates both expected new visual information and vehicle movement. Occupancy estimation for volumetric object model serves as a baseline measure of new information. The outlined approach determines next best views across the viewpoint space bounded by the sensor coverage and the capability of the UAV with minimal a priori knowledge of the object. Simulation studies conducted with virtual reality environments show the effectiveness of the algorithm.	r.o.t.o.r.;unmanned aerial vehicle	Dae-Yeon Won;Ali Göktogan;Salah Sukkarieh;Min-Jea Tahk	2013		10.1007/978-3-642-35485-4_15	computer vision;simulation;automotive engineering	Robotics	50.96845031680646	-40.148425250819	67591
043598095c039676f2185e47681848570cac6032	real-time estimation of head motion using weak perspective epipolar geometry	image recognition;facial expression recognition;perspective projection;real time;luminance distribution real time estimation head motion weak perspective epipolar geometry facial expression recognition local extremum saddle points;motion estimation;epipolar geometry;image recognition motion estimation feature extraction real time systems;feature extraction;motion estimation geometry magnetic heads face recognition tracking solid modeling eyes mouth equations workstations;real time systems;saddle point	For face and facial expression recognition, it is necessary to estimate head motion in order to track a head continuously. This paper proposes a new method for estimating head motion using the epipolar geometry of a weak perspective projection model. In this method, rst, the head region is segmented from the gradient of a luminance, by approximating the contour of the head as a circle. Then, feature points such as local ex-tremum or saddle points of a luminance distribution are tracked over successive frames. Finally, angles of rotation of the head during two successive frames are estimated from the coordinates of those feature points successfully tracked in these frames. Experiments were performed on a workstation in real time and the results showed that the method performs well in estimating head motion.	3d projection;epipolar geometry;gradient;real-time computing;real-time transcription;workstation	Takahiro Otsuka;Jun Ohya	1998		10.1109/ACV.1998.732883	computer vision;perspective;feature extraction;computer science;motion estimation;mathematics;geometry;saddle point;epipolar geometry;computer graphics (images)	Vision	47.906835465488825	-45.31983379866043	67617
5c19e4d67fcf8d09f4f95ece9bdb9aa0398dacbc	online video registration of dynamic scenes using frame prediction	moving object;dynamic texture;motion parallax;dynamic scenes	An online approach is proposed for Video registration of dynamic scenes, such as scenes with dynamic textures, moving objects, motion parallax, etc. This approach has three steps: (i) Assume that a few frames are already registered. (ii) Using the registered frames, the next frame is predicted. (iii) A new video frame is registered to the predicted frame. Frame prediction overcomes the bias introduced by dynamics in the scene, even when dynamic objects cover the majority of the image. It can also overcome many systematic changes in intensity, and the “brightness constancy” is replaced with “dynamic constancy”. This predictive online approach can also be used with motion parallax, where non uniform image motion is caused by camera translation in a 3D scene with large depth variations. In this case a method to compute the camera ego motion is described.	frame language;key frame;parallax	Alex Rav-Acha;Yael Pritch;Shmuel Peleg	2006		10.1007/978-3-540-70932-9_12	parallax;inter frame;computer vision;motion estimation;multimedia;computer graphics (images)	Vision	47.71399396207086	-47.78230396715702	67698
20c0715c5dcd6652c0c6e3ab78fa42a617ca68d2	a slit scanning depth of route panorama from stationary blur	street planar model;image motion analysis;image segmentation;scene archiving;spatial variables measurement;real time;route panorama;real time sensor;layout;feature matching;data representation;image representation;stationary blur;data visualization;cities and towns;matched filters;street planar model slit scanning depth route panorama stationary blur depth estimation data representation real time sensor scene archiving;pattern analysis;vehicles;computer science;depth estimation;layout cameras cities and towns videos vehicles data visualization image motion analysis pattern analysis computer science matched filters;spatial variables measurement image representation image segmentation;structure from motion;cameras;slit scanning depth;videos	This work achieves an efficient acquisition of scenes and their depths along streets. During the movement of a vehicle, a slit in the camera frame is set properly to sample scenes continuously for a route panorama. This paper proposes a novel method of depth estimation by analyzing a new phenomenon named stationary blur in the route panorama. We find its relation with the depth and evaluate its degree at local and global levels. The depth estimation through filtering avoids feature matching and tracking that are error-prone in the scanning of real and complex street scenes. Our method provides reliable results but requires much less data than that of the structure from motion. This keeps the elegance of the route panorama in data representation, and is suitable for real time sensor development. Utilizing the completeness of the route panorama in the scene archiving, we can generate planar models of streets, which can be used in city visualization.	algorithm;archive;cognitive dimensions of notations;computer data storage;data (computing);gaussian blur;global optimization;image stitching;stationary process;structure from motion	Min Shi;Jiang Yu Zheng	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.46	layout;computer vision;structure from motion;simulation;computer science;external data representation;image segmentation;matched filter;data visualization;computer graphics (images)	Vision	49.69781337807805	-48.344222266351046	67708
f8921a606a99f26337f60d6d167555397d17c018	landmark initialization for unscented kalman filter sensor fusion in monocular camera localization	journal article;camera pose;camera motion;trajectory recovery;unscented kalman filter	The determination of the pose of the imaging camera is a fundamental problem in computer vision. In the monocular case, difficulties in determining the scene scale and the limitation to bearing-only measurements increase the difficulty in estimating camera pose accurately. Many mobile phones now contain inertial measurement devices, which may lend some aid to the task of determining camera pose. In this study, by means of simulation and real-world experimentation, we explore an approach to monocular camera localization that incorporates both observations of the environment and measurements from accelerometers and gyroscopes. The unscented Kalman filter was implemented for this task. Our main contribution is a novel approach to landmark initialization in a Kalman filter; we characterize the tolerance to noise that this approach allows.	apache axis;approximation algorithm;book;british undergraduate degree classification;cloud database;computer graphics;computer science;control engineering;experiment;fuzzy logic;ground truth;image plane;image processing;inferring horizontal gene transfer;information engineering;information science;international journal of computer vision;iteration;kalman filter;landmark point;maxima and minima;microsoft azure;mobile phone;motion estimation;olap cube;ph (complexity);pipelines;sql;simulation;stacking;virtual reality	Gabriel Hartmann;Fay Huang;Reinhard Klette	2013	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2013.13.1.1	computer vision;camera auto-calibration;simulation;geography;control theory	Robotics	52.4991551779047	-41.57388790546805	67795
a1efd4ec1f30dd24f8429d0cf4be3023575efc28	shape matching for robot mapping	robot movil;model based reasoning;vision ordenador;raisonnement base sur modele;modele geometrique;path planning;robotics;similitude;computer vision;planification trajectoire;robot mobile;shape matching;shape similarity;pattern matching;similarity;robotica;vision ordinateur;geometric model;robotique;concordance forme;similitud;moving robot;geometrical model;modelo geometrico	We present a novel geometric model for robot mapping based on shape. Shape similarity measure and matching techniques originating from computer vision are specially redesigned for matching range scans. The fundamental geometric representation is a structural one, polygonal lines are ordered according to the cyclic order of visibility. This approach is an improvement of the underlying geometric models of today’s SLAM implementations, where shape matching allows us to disregard pose estimations. The object-centered approach allows for compact representations that are well-suited to bridge the gap from metric information needed in path planning to more abstract, i.e. topological or qualitative spatial knowledge desired in complex navigational tasks.	computer vision;cycle detection;geometric modeling;grammar-based code;microsoft outlook for mac;motion planning;robot;shape context;similarity measure;simultaneous localization and mapping	Diedrich Wolter;Longin Jan Latecki	2004		10.1007/978-3-540-28633-2_73	computer vision;similarity;computer science;artificial intelligence;similitude;geometric modeling;model-based reasoning;machine learning;pattern matching;shape analysis;mathematics;geometry;motion planning;robotics	Robotics	52.101875567242665	-38.77919249080229	67844
750604b594fd1c037ecd46bc15ef869ec8b42069	multi-view classifier swarms for pedestrian detection and tracking	image recognition;object recognition;image motion analysis;image recognition computer vision object recognition stereo vision laboratories particle swarm optimization decision theory geometrical optics optical sensors image motion analysis;multiple views;computer vision;optimization problem;multiple objectives;particle swarm optimizer;space use;particle swarm optimization;decision theory;pedestrian detection;stereo vision;object classification;optical sensors;geometric structure;geometrical optics	"""We describe a novel method for recognition and localization of objects in 3D space using multiple views. We pose the task of classifying and locating objects in 3D space as an optimization problem that combines 2D classifier scores from two separate views of the object. Our methods combine feature-based 2D object classification with efficient search mechanisms based on Particle Swarm Optimization (PSO) by implementing each particle as a local window classi- fier guided by PSO dynamics. We first formulate the localization problem in a multi-view framework where the domain of swarm classifier particles is extended from 2D image space to 3D space. In this scheme, swarm particles encapsulate the geometric structure that binds multiple views. Each particle is a self-contained classifier that """"flies"""" through the solution space seeking the most """"objectlike"""" region in space, thus effectively expanding the scope of an image based 2D classifier to 3D space. In a second formulation, we demonstrate how 2D swarm based search results obtained from individual 2D views can be used to perform 3D localization tasks. We outline a method to extend the framework from single object to multiple object localization and tracking scenarios."""	computer vision;feasible region;internationalization and localization;mathematical optimization;optimization problem;outline of object recognition;particle swarm optimization;pattern recognition;pedestrian detection;signal-to-noise ratio;synthetic intelligence	Payam Saisan;Swarup Medasani;Yuri Owechko	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops	10.1109/CVPR.2005.499	geometrical optics;optimization problem;computer vision;multi-swarm optimization;decision theory;computer science;stereopsis;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;particle swarm optimization	Vision	50.70350332170631	-39.1119000965449	67845
2a1c43ae3d3b4006500f04af606d51d619297c07	dense motion estimation for smoke		Motion estimation for highly dynamic phenomena such as smoke is an open challenge for Computer Vision. Traditional dense motion estimation algorithms have difficulties with non-rigid and large motions, both of which are frequently observed in smoke motion. We propose an algorithm for dense motion estimation of smoke. Our algorithm is robust, fast, and has better performance over different types of smoke compared to other dense motion estimation algorithms, including state of the art and neural network approaches. The key to our contribution is to use skeletal flow, without explicit point matching, to provide a sparse flow. This sparse flow is upgraded to a dense flow. In this paper we describe our algorithm in greater detail, and provide experimental evidence to support our claims.	algorithm;artificial neural network;comment (computer programming);computer graphics;computer vision;expectation propagation;framing (world wide web);maximum flow problem;motion estimation;oak (programming language);sparse matrix	Da Chen;Wenbin Li;Peter Hall	2016		10.1007/978-3-319-54190-7_14	computer vision;simulation;computer graphics (images)	Vision	51.95651304400363	-49.09794881810207	67950
e3b92cc14f2c33bfdc07b794292a30384f8d0ad1	local segmentation for pedestrian tracking in dense crowds		People tracking in dense crowds is challenging due to the high levels of inter-pedestrian occlusions occurring continuously. After each successive occlusion, the surface of the tracked object that has never been hidden reduces. If not corrected, this shrinking problem eventually causes the system to stop as the area to track become too small. In this paper we investigate how hidden parts of one target object can be recovered after occlusions and propose challenging data to evaluate such segmentation-tracking technique in dense crowds. The segmentation/tracking problem is particularly difficult to solve for non-rigid objects. Here, we focus on pedestrians whose limbs and lower body parts often get occluded in crowded scene. We first investigate the unmet challenges of pedestrian tracking in crowds and propose a challenging video to evaluate segmentation-tracking robustness to inter-pedestrian occlusions. We then detail a fast segmentation-based method to overcome some aspects of the tracking-under-occlusion problem. We finally compare our results with two existing tracking methods.	video tracking	Clement Creusot	2014		10.1007/978-3-319-04114-8_23	computer vision;simulation;multimedia	Vision	43.58669568185891	-46.68842474391021	68175
644592647ab53cbc43c0568f147dcc002dea5cf7	globally optimal estimates for geometric reconstruction problems	concave programming;estimation theory;global solution;non convex optimization;cost function;convex programming;nonconvex optimization;epipolar geometry estimation;optimal estimation;epipolar geometry estimation geometric reconstruction statistically optimal estimates statistically optimal cost function convex relaxations nonconvex optimization monotone sequence triangulation camera pose homography estimation;computational geometry;homography estimation;mesh generation computational geometry concave programming convex programming estimation theory;convex relaxations;camera pose;polynomials computer vision cameras geometry linear matrix inequalities computer science explosions reconstruction algorithms optimization methods;epipolar geometry;monotone sequence;geometric reconstruction;estimation;experimental validation;global optimization;triangulation;statistically optimal estimates;convex relaxation;matematik;local minima;mesh generation;structure from motion;lower bound;statistically optimal cost function;semidefinite program	We introduce a framework for computing statistically optimal estimates of geometric reconstruction problems. While traditional algorithms often suffer from either local minima or nonoptimality - or a combination of both - we pursue the goal of achieving global solutions of the statistically optimal cost-function. Our approach is based on a hierarchy of convex relaxations to solve nonconvex optimization problems with polynomials. These convex relaxations generate a monotone sequence of lower bounds and we show how one can detect whether the global optimum is attained at a given relaxation. The technique is applied to a number of classical vision problems: triangulation, camera pose, homography estimation and last, but not least, epipolar geometry estimation. Experimental validation on both synthetic and real data is provided. In practice, only a few relaxations are needed for attaining the global optimum	geometric median	Fredrik Kahl;Didier Henrion	2005		10.1109/ICCV.2005.109	optimal estimation;mesh generation;computer vision;mathematical optimization;estimation;combinatorics;structure from motion;monotonic function;triangulation;computational geometry;maxima and minima;mathematics;geometry;upper and lower bounds;estimation theory;statistics;epipolar geometry;global optimization	Theory	50.89389092755829	-50.86996171392012	68327
862ba6dd81c9ee1f0c299a05efea0c83eb496bd3	direct least square fitting of hyperellipsoids	fitting ellipsoids three dimensional displays surface fitting two dimensional displays estimation iterative methods;regularization calibration ellipsoid specific fitting ellipses ellipsoids least square fitting	This paper presents two new computationally efficient direct methods for fitting n-dimensional ellipsoids to noisy data. They conduct the fitting by minimizing the algebraic distance in subject to suitable quadratic constraints. The hyperellipsoid-specific (HES) method is an elaboration of existing ellipse and 3D ellipsoid-specific fitting methods. It is shown that HES is ellipsoid-specific in n-dimensional space. A limitation of HES is that it may provide biased fitting results with data originating from an ellipsoid with a large ratio between the longest and shortest main axis. The sum-of-discriminants (SOD) method does not have such a limitation. The constraint used by SOD rejects a subset of non-ellipsoidal quadrics, which enables a high tendency to produce ellipsoidal solutions. Moreover, a regularization technique is presented to force the solutions towards ellipsoids with SOD. The regularization technique is compatible also with several existing 2D and 3D fitting methods. The new methods are compared through extensive numerical experiments with n-dimensional variants of three commonly used direct fitting approaches for quadratic surfaces. The results of the experiments imply that in addition to the superior capability to create ellipsoidal solutions, the estimation accuracy of the new methods is better or equal to that of the reference approaches.	algorithmic efficiency;apache axis;axis vertebra;curve fitting;ellipsoid method;experiment;hypertext editing system;least-squares analysis;matrix regularization;numerical analysis;reference implementation;short;signal-to-noise ratio;solutions;subgroup	Martti Kes&#x00E4;niemi;Kai Matti Virtanen	2018	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2658574	mathematical optimization;computer science;mathematics;geometry;statistics	Vision	51.79014072948831	-51.9908351684972	68384
3089bd36d79dddcd96b09e4625eba4736e9d9425	learning 3d object templates by hierarchical quantization of geometry and appearance spaces	quantization;image segmentation;automobiles;geometry;gabor filters;trees mathematics;3d object representation 3d object template learning hierarchical quantization appearance space view labeled object image joint appearance geometry space deformable planar part template gabor filter and or tree bottom up pass top down pass 3d car template detection performance view estimation performance public car dataset;quantisation signal;computational modeling;shape;abstracts;image representation;solid modeling;geometry solid modeling shape image segmentation computational modeling quantization abstracts;trees mathematics automobiles gabor filters geometry image representation object detection quantisation signal;object detection	This paper presents a method for learning 3D object templates from view labeled object images. The 3D template is defined in a joint appearance and geometry space composed of deformable planar part templates placed at different 3D positions and orientations. Appearance of each part template is represented by Gabor filters, which are hierarchically grouped into line segments and geometric shapes. AND-OR trees are further used to quantize the possible geometry and appearance of part templates, so that learning can be done on a subsampled discrete space. Using information gain as a criterion, the best 3D template can be searched through the AND-OR trees using one bottom-up pass and one top-down pass. Experiments on a new car dataset with diverse views show that the proposed method can learn meaningful 3D car templates, and give satisfactory detection and view estimation performance. Experiments are also performed on a public car dataset, which show comparable performance with recent methods.	bottom-up proteomics;experiment;gabor filter;information gain in decision trees;kullback–leibler divergence;planar (computer graphics);quantization (signal processing);top-down and bottom-up design	Wenze Hu	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247945	computer vision;quantization;shape;computer science;machine learning;pattern recognition;mathematics;geometry;image segmentation;solid modeling;computational model	Vision	45.07491454216707	-51.94177128204427	68472
94c2823811525609ec4eb2a3feea344bdaf49daf	a vision-based location positioning system via augmented reality: an application in humanoid robot navigation	localization;augmented reality;humanoid robot navigation;mobile augmented reality	In this paper, we present a vision-based localization system using mobile augmented reality (MAR) and mobile audio augmented reality (MAAR) techniques, applicable to both humans and humanoid robots navigation in indoor environments. In the ̄rst stage, we propose a system that recognizes the location of a user from the image sequence of an indoor environment using its onboard camera. The location information is added to the user's view in the form of 3D objects and audio sounds with location information and navigation instruction content via augmented reality (AR). The location is recognized by using the prior knowledge about the layout of the environment and the location of the AR markers. The image sequence can be obtained using a smart phone's camera and the marker detection, 3D object placement and audio augmentation will be performed by the phone's operating processor and graphical/audio modules. Using this system will majorly reduce the hardware complexity of such navigation systems, as it replaces a system consisting of a mobile PC, wireless camera, head-mounted displays (HMD) and a remote PC with a smart phone with camera. In the second stage, the same algorithm is employed as a novel vision-based autonomous humanoid robot localization and navigation approach. The proposed technique is implemented on a humanoid robot NAO and improves the robot's navigation and localization performance previously done using an extended Kalman ̄lter (EKF) by presenting location-based information to the robot through di®erent AR markers placed in the robot environment.	algorithm;augmented reality;autonomous robot;display resolution;extended kalman filter;global positioning system;graphical user interface;head-mounted display;humanoid robot;nao (robot);robotic mapping;simultaneous localization and mapping;smartphone	Omid Mohareri;Ahmad B. Rad	2013	I. J. Humanoid Robotics	10.1142/S0219843613500199	embedded system;computer vision;augmented reality;simulation;internationalization and localization;computer science;mobile robot navigation	Robotics	48.302277205211674	-40.20407273188219	68634
5efc456ffbcc53dac61362535cf32b564e1a2cdf	front environment recognition of personal vehicle using the image sensor and acceleration sensors for everyday computing	moving object;acceleration sensor;segway;optical flow;image sensor	In this research, we propose the method for detecting moving objects in front of the Segway by detecting running state for the Segway. Running state of the personal vehicle Segway is detected with both an image sensor and an acceleration sensor mounted on the Segway. When objects are moving in front of the Segway, the image sensor can capture the motion while the acceleration sensor shows a different result. By analyzing the difference our method successfully recognizes moving objects from environment.	image sensor	Takahiro Matsui;Takeshi Imanaka;Yasuyuki Kono	2009		10.1007/978-3-642-02580-8_17	computer vision;simulation;computer science;image sensor;optical flow	Robotics	45.78234677297188	-41.444165196984244	68747
1a17487f252b4239718538860e358a887d5d9f1b	region of interest in disparity mapping for distance estimation on stereo vision application	software;sensors;stereo vision systems;stereoscopic cameras;mobile robots;vision;matlab;cameras	Stereo vision system is a useful method for deepness or depth gathering of objects and features in an environment. This paper presents the region of interest ROI in disparity mapping to be analyzed to get the estimate distance on stereo vision applications. The stereo vision application in this paper is using a mobile robot that navigates using a pair of camera. The cameras work as a stereo vision sensor for its navigation. The ROI is a reference sight of the stereo camera which the pixel intensities from the disparity mapping determine the distance or depth using an algorithm. The stereo vision baseline is based on horizontal configuration. The matching process is using block matching technique which briefly described with the performance of its output. The disparity mapping is generated by the algorithm with the reference to the left image coordinate. The algorithm uses Sum of Absolute Differences (SAD) which is developed using Matlab software.© (2012) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	binocular disparity;region of interest;stereopsis	R. A. Hamzah;Khairul Azha A. Aziz	2012		10.1117/12.946057	computer stereo vision;stereo cameras;stereo camera;computer vision;simulation;geography;epipolar geometry;computer graphics (images)	Vision	51.71274886733908	-41.24505019119223	68778
45fa39b764f2835bb9367733c1ef2e2b12a7fb59	graph matching via sequential monte carlo	object recognition;image matching;graph matching;feature correspondence;sequential monte carlo	Graph matching is a powerful tool for computer vision and machine learning. In this paper, a novel approach to graph matching is developed based on the sequential Monte Carlo framework. By constructing a sequence of intermediate target distributions, the proposed algorithm sequentially performs a sampling and importance resampling to maximize the graph matching objective. Through the sequential sampling procedure, the algorithm effectively collects potential matches under one-to-one matching constraints to avoid the adverse effect of outliers and deformation. Experimental evaluations on synthetic graphs and real images demonstrate its higher robustness to deformation and outliers.	algorithm;computer vision;extensibility;importance sampling;machine learning;matching (graph theory);modal logic;monte carlo method;one-to-one (data model);particle filter;sampling (signal processing);synthetic intelligence	Yumin Suh;Minsu Cho;Kyoung Mu Lee	2012		10.1007/978-3-642-33712-3_45	mathematical optimization;particle filter;3-dimensional matching;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;matching	Vision	45.85673926436396	-48.96114777870431	68832
cf30e9f089ffe9729712151ba28a96bcdda2c312	person following robot apriattenda	robot sensing systems;image recognition;robot sensing systems robot vision systems object detection image processing image recognition color data mining tracking motion control robust control;motion control;image processing;color;robust control;data mining;target detection;robot vision systems;tracking;object detection;ultrasonic sensor	"""We have developed the person following robot """"ApriAttenda(TM)"""". This robot can accompany a person using vision based target detection and avoid obstacles with ultrasonic sensors. The robot first identifies an individual with its image processing system by detecting a person's region and recognizing the registered color and texture of his/her clothes. Our newly developed algorithm allows the robot to extract a particular individual from a cluttered background, and to find and reconnect with the person if it loses visual contact. Tracking people with vision was realized by systematizing visual and motion control with a robust algorithm that utilizes various characteristics of the image data. ApriAttenda(TM) has been exhibited at Aichi EXPO 2005, and its robust functions and smooth person following capability were successfully demonstrated."""	algorithm;image processing;robot;sensor	Takashi Yoshimi;Manabu Nishiyama;Takafumi Sonoura;Hideichi Nakamoto;Seiji Tokura;Hirokazu Sato;Fumio Ozaki;Nobuto Matsuhira;Hiroshi Mizoguchi	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.282209	robust control;motion control;computer vision;simulation;image processing;computer science;engineering;control theory;tracking;ultrasonic sensor;mobile robot navigation	Robotics	49.75172656183585	-38.32374161345509	68908
05e13f2e710529a0cec8cfd3c9c58e5093075cc7	shape alignment by learning a landmark-pdm coupled model	shape noise shaping deformable models mathematical model biomedical imaging image analysis robustness computer vision data mining digital images;computational geometry;model based approach;shape contour;estimation algorithm;constrained point distribution model landmark pdm coupled model groupwise shape alignment shape contour;landmark pdm coupled model;groupwise shape alignment;constrained point distribution model;large deformation;point distribution model	This paper revisits the model-based approaches for groupwise shape alignment. The key contribution is modeling the landmarks instead of considering them as nodes sliding along the shape contour. The shape group is thus modeled by a landmark-PDM coupled model instead of a constrained point distribution model (PDM). This coupled model is estimated by a stable four-stage estimation algorithm. There are two significant achievements. First, shapes are aligned in a fully unsupervised manner - both the number and location of landmarks are automatically decided. Second, extremely noisy and largely deformed shapes can be robustly aligned. These are demonstrated using both synthesized and real data	algorithm;point distribution model;unsupervised learning	Yifeng Jiang;Jun Xie;Hung-Tat Tsui	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.1048	active shape model;point distribution model;computer vision;computational geometry;computer science;machine learning;geometry	Vision	49.048565680466965	-51.27297409694392	68983
f530fd7c20e377812ffe12cfcb1736cf881f715a	jaguar: low latency mobile augmented reality with flexible tracking		In this paper, we present the design, implementation and evaluation of Jaguar, a mobile Augmented Reality (AR) system that features accurate, low-latency, and large-scale object recognition and flexible, robust, and context-aware tracking. Jaguar pushes the limit of mobile AR's end-to-end latency by leveraging hardware acceleration with GPUs on edge cloud. Another distinctive aspect of Jaguar is that it seamlessly integrates marker-less object tracking offered by the recently released AR development tools (e.g., ARCore and ARKit) into its design. Indeed, some approaches used in Jaguar have been studied before in a standalone manner, e.g., it is known that cloud offloading can significantly decrease the computational latency of AR. However, the question of whether the combination of marker-less tracking, cloud offloading and GPU acceleration would satisfy the desired end-to-end latency of mobile AR (i.e., the interval of camera frames) has not been eloquently addressed yet. We demonstrate via a prototype implementation of our proposed holistic solution that Jaguar reduces the end-to-end latency to ~33 ms. It also achieves accurate six degrees of freedom tracking and 97% recognition accuracy for a dataset with 10,000 images.	augmented reality;bmc remedy action request system;central processing unit;cloud computing;context awareness;end-to-end principle;graphics processing unit;hardware acceleration;holism;image retrieval;jaguar;outline of object recognition;prototype;six degrees of separation	Wenxiao Zhang;Bo Han;Pan Hui	2018		10.1145/3240508.3240561	computer vision;latency (engineering);video tracking;artificial intelligence;real-time computing;latency (engineering);mobile edge computing;cloud computing;augmented reality;jaguar;hardware acceleration;computer science	Mobile	44.4501473430333	-39.12881136151532	69016
6bcf1235fdc95144cb552a08a6240cf1b204b1c4	view-invariant fall detection system based on silhouette area and orientation	geriatrics;kernel;video surveillance;inclination angle view invariant fall detection system silhouette area silhouette orientation old generation population healthcare system automatic visual surveillance systems feature extraction video sequences event classification multicamera;silhouette area;image classification;kernel feature extraction cameras support vector machine classification accuracy error analysis;fall detection;error analysis;accuracy;feature extraction;support vector machine classification;monocular system;video surveillance cameras feature extraction geriatrics health care image classification image sequences object detection;cameras;monocular system video surveillance fall detection feature extraction silhouette area;object detection;image sequences;health care	Population of old generation that live alone is growing in most countries. Surveillance systems help them stay home and reduce the burden on the healthcare system. Automatic visual surveillance systems have advantages over wearable devices. They extract features from video sequences and use them for event classification. But these features are dependent on the position of cameras relative to the person. Therefore they need multi-camera for more accuracy that increases cost and complexity. In this paper we propose using silhouette area combined with inclination angle as robust features that can be measured using only one camera with an arbitrary direction. Through rigorous simulations on a publicly available dataset the error rate of the system is found to be less than 1%.	bit error rate;complexity;population;simulation;wearable technology	Behzad Mirmahboub;S Abdolvahab Samavi;Nader Karimi;Shahram Shirani	2012	2012 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2012.193	computer vision;contextual image classification;kernel;speech recognition;feature extraction;computer science;machine learning;pattern recognition;accuracy and precision;geriatrics;health care;statistics	Robotics	39.6163198488772	-44.2205261631453	69024
99a73819cde46bdb15c82c30cea68d9e7bacaefb	improved hsv-based gaussian mixture modeling for moving foreground segmentation		It is crucial to get the moving foreground for variety video processing system in complex scenes. An improved GMM-based method is developed that can real-time segment moving foreground efficiently. The Gaussian mixture model is improved to effectively detect motion foreground objects even if the object moves slowly. Some relationships between H and S components in HSV space are adopted to suppress shadow caused by moving objects. The shortcoming in literature that more parameters are needed to remove shadow. Experimental results highlight that the proposed method is computationally cost-effective and robust to segment foreground by comparison.	google map maker;mixture model;real-time clock;video processing	Yepeng Guan;Jinhui Du;Changqi Zhang	2012		10.1007/978-3-642-34595-1_8	computer vision;mixture model;video processing;hsl and hsv;artificial intelligence;gaussian;segmentation;computer science;shadow	Vision	43.511565043603525	-49.230822871584294	69066
7ec27f35fbc58f80e5add6ffea939672198f7b9f	kalman filter-based facial emotional expression recognition	kalman filter;emotion recognition;hidden states;state space model	In this work we examine the use of State-Space Models to model the temporal information of dynamic facial expressions. The later being represented by the 3D animation parameters which are recovered using 3D Candide model. The 3D animation parameters of an image sequence can be seen as the observation of a stochastic process which can be modeled by a linear State-Space Model, the Kalman Filter. In the proposed approach each emotion is represented by a Kalman Filter, with parameters being State Transition matrix, Observation matrix, State and Observation noise covariance matrices. Person-independent experimental results have proved the validity and the good generalization ability of the proposed approach for emotional facial expression recognition. Moreover, compared to the state-of-the-art techniques, the proposed system yields significant improvements in recognizing facial expressions.	kalman filter	Ping Fan;Isabel Gonzalez;Valentin Enescu;Hichem Sahli;Dongmei Jiang	2011		10.1007/978-3-642-24600-5_53	psychology;kalman filter;computer vision;invariant extended kalman filter;speech recognition;computer science;state-space representation;machine learning;moving horizon estimation;communication	Vision	43.965350975861114	-47.75439209235076	69139
3e2513405221207819bcffded420c5ff23198699	safety for a robot arm moving amidst humans by using panoramic vision	moving object;manipulators;reliability;change detection;global illumination change detection;global illumination change detection robot arm panoramic vision panoramic cameras safety issues visual servoing manipulator moving object reliability robustness adaptive background modelling;panoramic camera;panoramic cameras;visual servoing computer vision manipulators safety;computer vision;safety robot vision systems humans cameras manipulators robotics and automation visual servoing lenses robustness lighting;global illumination;robot arm;panoramic vision;field of view;safety;robustness;safety issues;visual servoing;manipulator;adaptive background modelling	This paper describes how the use of panoramic cameras can dramatically simplify safety issues for a robot arm moving in close proximity to human beings, since they can simultaneously observe a 360deg field of view. We present in this context an approach to visual servoing in which both the manipulator as well as any other moving object are tracked. Reliability and robustness are enhanced by adaptative background modelling and global illumination change detection.	global illumination;robotic arm;visual servoing	Enric Cervera;Nicolás García Aracil;Ester Martínez-Martín;Leonardo Nomdedeu;Angel P. Del Pobil	2008	2008 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2008.4543530	computer vision;simulation;robotic arm;field of view;computer science;engineering;artificial intelligence;manipulator;reliability;visual servoing;global illumination;change detection;statistics;robustness;computer graphics (images)	Robotics	51.05165058900769	-40.38410833845147	69202
e3539807df286275b7ff3899097155f2b3df5ec1	design of soft robotic actuation for supporting eyelid closure movement		We have been developing a facial wearable robot to support the eyelid movements of patients with facial paralysis, especially on one side of the face [1]. This robot has a mechanism for supporting eyelid movements, made from a soft material, which is called the eyelid gating mechanism (ELGM). The ELGM deforms by simple rotational actuation inputs and its deformation is customized to the eyelid movements. Therefore, this robot can provide non-invasive and gentle support for eyelid movements. We herein describe the design rule of the ELGM, and based on this, we conducted a deformation analysis with a non-linear finite element method. We verified the deformation trend from the results, and developed three prototypes based on this trend. Using these prototypes, we conducted a clinical study with facial paralysis patients to evaluate if the ELGM is capable of assisting in closing the eyelid.	actuation dosing unit;blepharoptosis;blepharospasm;closing (morphology);customize;eyelid diseases;eyelid structure;facial paralysis;finite element method;nonlinear system;patients;robot;wearable computer;movement of eyelid	Yuta Kozaki;Naoki Matsushiro;Kenji Suzuki	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8512819	facial paralysis;computer vision;soft robotic;robot;artificial intelligence;wearable computer;finite element method;computer science;eyelid closure;eyelid	Robotics	40.248442414583074	-38.75162632894575	69212
d21da539b6f1e71f7500c4f94cc59e98dd946e15	visual simultaneous localization and mapping: a survey	visual slam;image matching;data association;salient feature selection;topological and metric maps	Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, construct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to provide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art.	algorithm;aliasing;autonomy;casio loopy;computation;computational complexity theory;computer vision;correspondence problem;database;day and night (cellular automaton);internationalization and localization;kidnapped robot problem;map;open research;real-time computing;sonar (symantec);sensor;simultaneous localization and mapping;spatial anti-aliasing	Jorge Fuentes-Pacheco;José Ruíz Ascencio;Juan M. Rendón-Mancha	2012	Artificial Intelligence Review	10.1007/s10462-012-9365-8	computer vision;artificial intelligence;machine learning	Robotics	51.26416484000965	-38.28955185707658	69983
35fe863eb02fe6f6c4159f3e86fac12a36fd2b96	itrack: image-based probabilistic tracking of people	complex dynamics;theoretical framework;probability;heuristic programming;heuristic programming optical tracking probability motion estimation;motion estimation;probablistic framework itrack image based probabilistic tracking people tracking person trackin image heuristics prediction estimation theoretical frameworks complex dynamical object models shape object models;optical tracking;shape computer vision bayesian methods application software tracking state estimation testing layout surveillance motion analysis;people tracking;profitability;shape modeling;object model	Real applications on people tracking are usually based on image heuristics. Real approaches do not use to apply recent prediction-estimation theoretical frameworks. These require the definition of complex dynamical and shape object models before the tracking process. We present a probabilistic framework that takes profit of these theories adapting them to real applications. The key idea of this work is to estimate the shape model and dynamical objects parameters using only image data. The flexibility of our algorithm makes it suitable to be used on different real applications. Some experiments have been done in order to test our method in outdoor scenes people tracking.	algorithm;experiment;heuristic (computer science)	Xavier Varona;Jordi Gonzàlez;F. Xavier Roca;Juan José Villanueva	2000		10.1109/ICPR.2000.903740	computer vision;simulation;complex dynamics;object model;computer science;machine learning;motion estimation;probability;statistics;profitability index	Vision	46.53689615291402	-48.74370353348659	70034
64f1b6873f6b8f06dcaf7594677f3c104b121b4a	human face reconstruction from a single input image based on a coupled statistical model		In this paper, the similar characteristics of human face has been used to relax the numbers of the input into one single face image, and reconstruct the 3D shape based on a couple statistical model. Moreover the lighting conditions of the single input image can be different from that of the training database. The experiment results have demonstrated the effectiveness of the proposed method.	statistical model	Yujuan Sun;Muwei Jian;Junyu Dong	2016		10.1007/978-981-10-3614-9_45	machine learning;artificial intelligence;computer vision;computer science;statistical model	Vision	47.360740936123825	-51.28403902462244	70368
a1a86efdca336437e248576e543d083d7f853625	evaluating error functions for robust active appearance models	least squares approximations;occlusion detection;hidden feature removal;video signal processing;image sequence analysis;video sequences;active appearance model;performance metric;iterative methods;gaussian distribution error function evaluation robust active appearance models video sequences iteratively reweighted least squares occlusion detection;face recognition;robustness active appearance model shape parametric statistics video sequences pixel parameter estimation robots measurement gaussian distribution;iteratively reweighted least squares;robust active appearance models;error function evaluation;video signal processing face recognition gaussian distribution hidden feature removal image sequences iterative methods least squares approximations;gaussian distributions;iteratively re weighted least squares;least squares methods;gaussian distribution;image sequences	Active appearance models (AAMs) are generative parametric models commonly used to track faces in video sequences. A limitation of AAMs is they are not robust to occlusion. A recent extension reformulated the search as an iteratively re-weighted least-squares problem. In this paper we focus on the choice of error function for use in a robust AAM search. We evaluate eight error functions using two performance metrics: accuracy of occlusion detection and fitting robustness. We show for any reasonable error function the performance in terms of occlusion detection is the same. However, this does not mean that fitting performance is the same. We describe experiments for measuring fitting robustness for images containing real occlusion. The best approach assumes the residuals at each pixel are Gaussianally distributed, then estimates the parameters of the distribution from images that do not contain occlusion. In each iteration of the search, the error image is used to sample these distributions to obtain the pixel weights	active appearance model;computer performance;experiment;hidden surface determination;iteration;least squares;pixel	Barry-John Theobald;Iain A. Matthews;Simon Baker	2006	7th International Conference on Automatic Face and Gesture Recognition (FGR06)	10.1109/FGR.2006.38	computer vision;pattern recognition;mathematics;statistics	Vision	48.03386866541158	-50.96076120661878	70385
5fba623f2852e7034e166b1f0592c2d9aa098404	conversation scene analysis with dynamic bayesian network basedon visual head tracking	belief networks;video sequence;teleconferencing;telecommunication network reliability;dynamic bayesian network conversation scene analysis probabilistic model video sequence face face communication conversation structure representation gaze direction head direction visual head tracking sparse template condensation;magnetic heads;visual head tracking;particle measurements;perforation;magnetic sensors;head tracking;bayesian methods;head direction;tracking belief networks image representation image sensors image sequences probabilistic logic;video sequences;image sensors;conversation structure representation;probabilistic model;dynamic bayesian network;image representation;image analysis;face face communication;humans;probabilistic logic;visual tracking;gaze direction;image analysis bayesian methods magnetic heads humans video sequences particle measurements magnetic sensors telecommunication network reliability teleconferencing robotics and automation;face to face;conversation scene analysis;robotics and automation;tracking;sparse template condensation;scene analysis;image sequences	A novel method based on a probabilistic model for conversation scene analysis is proposed that can infer conversation structure from video sequences of face-to-face communication. Conversation structure represents the type of conversation such as monologue or dialogue, and can indicate who is talking/listening to whom. This study assumes that the gaze directions of participants provide cues for discerning the conversation structure, and can be identified from head directions. For measuring head directions, the proposed method newly employs a visual head tracker based on sparse-template condensation. The conversation model is built on a dynamic Bayesian network and is used to estimate the conversation structure and gaze directions from observed head directions and utterances. Visual tracking is conventionally thought to be less reliable than contact sensors, but experiments confirm that the proposed method achieves almost comparable performance in estimating gaze directions and conversation structure to a conventional sensor-based method	dynamic bayesian network;experiment;sensor;sparse matrix;statistical model	Kazuhiro Otsuka;Junji Yamato;Yoshinao Takemae;Hiroshi Murase	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262677	statistical model;computer vision;image analysis;speech recognition;teleconference;eye tracking;bayesian probability;computer science;image sensor;tracking;probabilistic logic;dynamic bayesian network;statistics	Robotics	46.2100961171526	-47.29262690634908	70745
4d26db36d9e98c1591b4f78b4917a20deef90297	outlier removal using duality	large scale multiview reconstruction;image reconstruction geometry cameras computer science large scale systems image sequences statistical distributions motion estimation layout computer errors;computational methods;ransac method;number of datum;duality mathematics;convex programming;datorseende och robotik autonoma system;geometry;quasiconvex;l 1 optimization;duality;l 1 optimization outlier removal duality large scale multiview reconstruction ransac method sim hartley method quasiconvex problem error residual;motion estimation;layout;measurement uncertainty;computer vision;conference paper;keywords hartley;large scale;statistical distributions;estimation;image reconstruction;random processes;linear programming;error residual;sim hartley method;multi view reconstruction;quasiconvex problem;theoretical result;computer science;random processes convex programming duality mathematics image reconstruction;matematik;outlier removal;computer errors;cameras;large scale systems;noise;image sequences;post processing	In this paper we consider the problem of outlier removal for large scale multiview reconstruction problems. An efficient and very popular method for this task is RANSAC. However, as RANSAC only works on a subset of the images, mismatches in longer point tracks may go undetected. To deal with this problem we would like to have, as a post processing step to RANSAC, a method that works on the entire (or a larger) part of the sequence. In this paper we consider two algorithms for doing this. The first one is related to a method by Sim & Hartley where a quasiconvex problem is solved repeatedly and the error residuals with the largest error is removed. Instead of solving a quasiconvex problem in each step we show that it is enough to solve a single LP or SOCP which yields a significant speedup. Using duality we show that the same theoretical result holds for our method. The second algorithm is a faster version of the first, and it is related to the popular method of L1-optimization. While it is faster and works very well in practice, there is no theoretical guarantee of success. We show that these two methods are related through duality, and evaluate the methods on a number of data sets with promising results.1	algorithm;duality (optimization);hartley (unit);multiview video coding;quasiconvex function;random sample consensus;second-order cone programming;speedup	Carl Olsson;Anders P. Eriksson;Richard I. Hartley	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5539800	iterative reconstruction;probability distribution;layout;computer vision;mathematical optimization;estimation;combinatorics;ransac;duality;duality;quasiconvex function;linear programming;noise;machine learning;motion estimation;mathematics;geometry;video post-processing;statistics;measurement uncertainty	Vision	52.04664025977392	-49.11090904016399	71325
6466c4c463adb8c83d175d25ffd242e60f90fb18	a kinetic and 3d image input device	3d shape;frames per second;input device;image resolution;3d imaging;gesture;real time;motion;object segmentation;infrared;kinetics;image input device;gesture recognition	Gesture recognition in real time can bridge a gap between humans and computers. Object segmentation from the background is a critical problem in the conventional gesture recognition technology. We have developed a new input device which can detect a kinetic and 3D image of a hand in real time. We call it “Motion Processor”. The Motion Processor with infrared light sources and an area sensor can detect the reflected light image of a hand at 30 frames per second. The image resolution is 64 pixels by 64 pixels. It is easy to recognize gestures and motions in real time based on the detected hand images. This gesture recognition bridges a gap between humans and computers.	computer;gesture recognition;image resolution;input device;pixel	Shunichi Numazaki;Akira Morishita;Naoko Umeki;Minoru Ishikawa;Miwako Doi	1998		10.1145/286498.286723	stereoscopy;computer vision;image resolution;infrared;computer science;motion;gesture recognition;multimedia;gesture;frame rate;kinetics;input device;computer graphics (images)	Vision	45.690524001271726	-41.803436800945164	71392
122497e5f71c13ce733c83e9260466b873cbbb10	practical architectures for fused visual and inertial mobile sensing	mobile;real time;cloud;computer vision;dissertation;multimodal;computer science practical architectures for fused visual and inertial mobile sensing duke university romit roy choudhury;computer science;sensor fusion;landon cox jain puneet			Puneet Jain	2015			simulation;computer science;artificial intelligence;computer graphics (images)	HCI	52.378043303849516	-42.94844461577144	71715
06c01cf658a050df662251e87623db482dd2cf48	a simple vehicle segmentation approach for intelligent transportation system	binarization;binarization intelligent transportation system pre processing vehicle segmentation;pre processing;image segmentation;image processing;vehicle segmentation;sensors;intelligent transport system;intelligent transportation systems;intelligent transportation system;vehicle detection;intelligent transportation systems intelligent vehicles image segmentation vehicle detection shape computer vision pixel filling layout testing;testing;traffic information systems computer vision image segmentation;filling;layout;computer vision;object segmentation;inner region filling vehicle segmentation intelligent transportation system vehicle detection computer vision;shape;traffic information systems;intelligent vehicles;pixel;vehicles;inner region filling	In ITS, vehicle detection is very important research field, which can provide the relevant information, such as position, shape, size, and so on. How to segment the vehicle is the key in vehicle detection. A simple vehicle segmentation algorithm based on computer vision is proposed in this paper. Firstly, an effective preprocessing method to eliminate the affection of different scale gray pixels in different region, which can make the image be easily binarized; Secondly, a small threshold can be set to binarize the preprocessed image; After that, in order to avoid the small regions enclosed in big region, a inner region filling algorithm is presented; Thirdly, the image processed by first and second step can be easily segmented; Lastly, several transportation scene images are used to test the proposed algorithm, the result indicate the validity of the proposed segmentation algorithm.	algorithm;computer vision;pixel;preprocessor	Yongquan Xia;Baohua Jin;Yong Gan;Min Huang;Xiaolei Chen	2009	2009 International Conference on Digital Image Processing	10.1109/ICDIP.2009.40	embedded system;computer vision;simulation;engineering	Robotics	43.33799104696015	-44.52903205110434	71726
06f042a75f33ab7aae5bdf4b04e8e044bbd2b106	warping background subtraction	warping layers;background modeling;image processing;image resolution;training;biological system modeling;color histogram;pixel neighborhoods;motion;gray scale;computer vision;symposia;object detection monitoring pixel motion detection lighting statistical distributions histograms layout deformable models image generation;accuracy;background;background motion;monitoring;background subtraction warping;image colour analysis;pixel;occluding layer;background subtraction;pixel intensity;pixel locations;integrated circuit modeling;pixels;foreground objects;color histograms;image resolution image colour analysis;color histograms background subtraction warping background motion foreground objects pixel intensity pixel locations warping layers occluding layer pixel neighborhoods;layers	We present a background model that differentiates between background motion and foreground objects. Unlike most models that represent the variability of pixel intensity at a particular location in the image, we model the underlying warping of pixel locations arising from background motion. The background is modeled as a set of warping layers, where at any given time, different layers may be visible due to the motion of an occluding layer. Foreground regions are thus defined as those that cannot be modeled by some composition of some warping of these background layers. We illustrate this concept by first reducing the possible warps to those where the pixels are restricted to displacements within a spatial neighborhood, and then learning the appropriate size of that spatial neighborhood. Then, we show how changes in intensity/color histograms of pixel neighborhoods can be used to discriminate foreground and background regions. We find that this approach compares favorably with the state of the art, while requiring less computation.	background subtraction;computation;displacement mapping;geo warping;image warping;pixel;spatial variability;warp (information security)	Teresa Ko;Stefano Soatto;Deborah Estrin	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5539813	computer vision;background subtraction;image processing;computer science;pixel;computer graphics (images)	Vision	44.357272969943146	-51.90332551318938	72001
87e50442522e393d317b04fdf57507db4056e981	real-time diffuse behavior detection of pixels from outdoor image sequence	stereo augmented reality reflectance estimation;reflectivity;natural scenes image sequences;inbound z test real time diffuse behavior detection time lapsed low dynamic range image sequence outdoor scene unknown dynamic illumination pixel analysis standard deviation outlier value specular candidature failure;estimation;image color analysis;reflectance estimation;sun;stereo;lighting reflectivity image sequences image color analysis estimation sun calibration;lighting;augmented reality;calibration;image sequences	We have proposed a technique to detect diffuse reflectance behavior pixels in real-time from a time-lapsed low dynamic range image sequence of the outdoor scene under few assumptions, unknown dynamic illumination and without using calibration objects. Each pixel analysis is done at the arrival of the every input image of the image sequence. In this process we incrementally compute new mean and standard deviation of each pixel on the arrival of the new input image and perform Z-test on each pixel for its outlier value. Shadow and specular candidature failure of a pixel along with its inbound Z-test result signifies pixel is having diffuse behavior.	diffuse reflection;dynamic range;high-dynamic-range rendering;inbound marketing;pixel;range imaging;real-time clock	Brajesh B. Lal;Claus B. Madsen	2011	2011 Irish Machine Vision and Image Processing Conference	10.1109/IMVIP.2011.37	computer vision;feature detection;geography;optics;pixel connectivity;remote sensing	Vision	45.287322988325016	-45.94998655260707	72134
018dcb26e39c67a0c8bab8ef17ef358a4bfc0cf6	semi-autonomous learning of objects	image recognition;image segmentation;null;computer vision;training data;robot vision;automatic detection;machine vision;biological systems;humans;robot vision systems;autonomous learning;robotics and automation;humans image segmentation machine vision labeling robot vision systems robotics and automation image recognition biological systems computer vision training data;labeling	This paper presents a robotic vision system that can be taught to recognize novel objects in a semi-autonomous manner that does not require manual labeling or segmentation of any individual training images. Instead, unfamiliar objects are simply shown to the system in varying poses and scales against cluttered background and the system automatically detects, tracks, segments, and builds representations for these objects. We demonstrate the feasibility of our approach by training the system to recognize one hundred household objects, which are presented to the system for about a minute each. Our method resembles the way that biological organisms learn to recognize objects and it paves the way for a wealth of applications in robotics and other fields.	autonomous robot;robotics;semiconductor industry	Hyundo Kim;Erik Murphy-Chutorian;Jochen Triesch	2006	2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)	10.1109/CVPRW.2006.193	robot learning;computer vision;training set;labeling theory;simulation;machine vision;computer science;artificial intelligence;image segmentation	Vision	47.93673627120658	-38.13821253644623	72362
859374a7498180f1ca40cae70890f8aaee116d3c	video frame's background modeling: reviewing the techniques	moving object;background modeling	Background modeling is a technique for extracting moving objects in video frames. This technique can be used in machine vision applications, such as video frame compression and monitoring. To model the background in video frames, initially, a model of scene background is constructed, then the current frame is subtracted from the background. Eventually, the difference determines the moving objects. This paper evaluates a number of existing background modeling techniques in terms of accuracy, speed and memory requirement.	frame (video);machine vision;real-time clock;real-time computing	Hamid Hassanpour;Mehdi Sedighi;Ali Reza Manashty	2011	J. Signal and Information Processing	10.4236/jsip.2011.22010	video compression picture types;reference frame;computer vision;background subtraction;computer science;video tracking;block-matching algorithm;multimedia;motion compensation;computer graphics (images)	Vision	43.86601722790324	-45.172509907607214	72413
fd228f05dbc4b5523fb0fbaabf64f22425e63535	fast and accurate facial pose estimation by aligning a 3d appearance model	face recognition;image motion analysis;minimisation;3d appearance model;facial pose estimation;pose variations	This paper proposed a method to estimate pose including large rotation in depth by aligning the 3D appearance model with the target image captured under various illumination conditions. Pose estimation is formulated by the minimization of the error between the target image and an image reproduced by the model. In the experiments, the performances of our proposed method for static and realtime pose estimation has been evaluated with test images including pose variations up to 60 degrees even from frontal and drastic illumination variations. It has shown that the proposed method is fast enough.	3d pose estimation;experiment;gradient descent;illumination (image);local convergence;maxima and minima;pentium 4;performance;tracking system	Rui Ishiyama;Shizuo Sakamoto	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1333784	facial recognition system;computer vision;minimisation;simulation;pose;3d pose estimation;computer science;articulated body pose estimation;statistics;computer graphics (images)	Robotics	48.48893466442147	-48.7632901912361	72435
3f492a7ea9a2f0c31569921e04e480e24eaed562	solar irradiance now-casting with ramp-down event prediction via enhanced cloud detection and tracking	irradiance now casting;kalman filters;cloud classification irradiance now casting all sky image tracking cloud detection;noise measurement;cloud detection;current measurement;feature extraction;clouds;all sky image;sun;cloud classification;predictive models;multiscale neighborhood feature solar irradiance now casting system ramp down event prediction cloud detection algorithm cloud tracking cloud classification satellite image resolution all sky image feature;sun feature extraction clouds kalman filters predictive models current measurement noise measurement;weather forecasting clouds feature extraction geophysics computing image classification image resolution solar radiation;tracking	In this work, an accurate solar irradiance now-casting system is presented. The proposed system utilizes information from all-sky images to complement insufficient temporal and spatial resolution of satellite images and help improve prediction accuracy. Based on the history irradiance and the all-sky image features, irradiance are predicted ten to fifteen minutes ahead to allow Photovoltaics operators to schedule and allocate energy resources more effectively. To capture the relationship between clouds and the sun, an enhanced cloud detection algorithm using multi-scale neighborhood features is proposed. Then, feature points on the clouds are tracked to predict if the sun will be occluded by the clouds. According to the cloud tracking results, ramp-down events are forecasted and the predicted solar irradiance is refined. The proposed system is validated using a challenging dataset and exhibits superior performance compared with existing works.	algorithm;ramp simulation software for modelling reliability, availability and maintainability	Hsu-Yung Cheng;Chih-Chang Yu	2016	2016 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2016.7552863	kalman filter;computer vision;feature extraction;computer science;noise measurement;machine learning;predictive modelling;tracking	Robotics	40.65579490231075	-41.32807861093725	72456
70ed5bccb9a311af3e55573d8ad4c4a4ec0e9a27	a real time implementation of a neuromorphic optic flow algorithm			algorithm;maximum flow problem;neuromorphic engineering;optical flow	Jason Lee Dale	2003				EDA	52.38991603058124	-43.50480590759282	72464
c9b528fba502bdef958531e10bc6c29bd9a1c017	path recognition for agricultural robot vision navigation under weed environment		In this paper, a path recognition method for agricultural robot vision navigation under weed environment is proposed. The vision navigation is based on color images sampled by a high speed camera. First, the crop and weed information is extracted using an appropriate color feature model to separate the green crop from background; then the image is thresholded, and the noise caused by weed is filtered by deleting small-area objects in the image; the navigation path is extracted through Hough transformation. Experiments are carried out in corn seedling field, and results show that the method can recognize navigation path correctly under weed environment.	agricultural robot	Peidong Wang;Zhijun Meng;ChangHai Luo;Hebo Mei	2013		10.1007/978-3-642-54344-9_30	computer vision;simulation;agronomy;mobile robot navigation	Robotics	48.13317520259221	-41.59721155906661	72504
b714597ded1f03fa5934035d23305c93fb357ac5	real-time ball tracking in a semi-automated foosball table	image processing;perspective projection;real time;computer vision;kalman observer;a priori knowledge;object tracking;automated foosball table;positional information;visual servoing;ball segmentation;real time systems	In this article a method is proposed for ball tracking using 100 Hz computer vision in a semi-automated foosball table. In this application the behavior of the ball is highly dynamic with speeds up to 10 m/s and frequent bounces occur against the sides of the table and the puppets. Moreover, in the overhead camera view of the field the ball is often fully or partially occluded and there are other objects present that resemble the ball. The table is semi-automated to enable single user game play. This article shows that it is possible to perform fast and robust ball tracking by combining efficient image processing algorithms with a priori knowledge of the stationary environment and position information of the automated rods.	algorithm;ball project;bounce message;computer vision;image processing;overhead (computing);real-time transcription;semiconductor industry;stationary process	Rob Janssen;Jeroen de Best;René van de Molengraft	2009		10.1007/978-3-642-11876-0_12	computer vision;perspective;a priori and a posteriori;simulation;image processing;computer science;video tracking;visual servoing;computer graphics (images)	Vision	49.03935538516975	-43.71619823750568	72509
8917f67dddec290c5b90598692fee88dc3e66ed1	adaptive occlusion state estimation for human pose tracking under self-occlusions	3d human pose tracking;self occlusion;computer vision	Tracking human poses in video can be considered as the process of inferring the positions of the body joints. Among various obstacles to this task, one of the most challenging is to deal with ‘self-occlusion’, where one body part occludes another one. In order to tackle this problem, a model must represent the self-occlusion between different body parts which leads to complex inference problems. In this paper, we propose a method which estimates occlusion states adaptively. A Markov random field is used to represent the occlusion relationship between human body parts in terms an occlusion state variable, which represents the depth order. To ensure efficient computation, inference is divided into two steps: a body pose inference step and an occlusion state inference step. We test our method using video sequences from the HumanEva dataset. We label the data to quantify how the relative depth ordering of parts, and hence the self-occlusion, changes during the video sequence. Then we demonstrate that our method can successfully track ∗Corresponding Author: Department of Brain and Cognitive Engineering, Korea University, Anam-dong, Seongbuk-ku, Seoul 136-713, Korea. Tel: (+82)-2-3290-3197, Fax: (+82)-2-32903583 Email addresses: ngcho@image.korea.ac.kr (Nam-Gyu Cho), yuille@stat.ucla.edu (Alan L. Yuille), swlee@image.korea.ac.kr (Seong-Whan Lee ) Preprint submitted to Pattern Recognition September 10, 2012 human poses even when there are frequent occlusion changes. We compare our approach to alternative methods including the state of the art approach which use multiple cameras.	algorithm;cognitive engineering;computation;email;fax;festival of code;hidden surface determination;interaction;ku band;markov chain;markov random field;nam;pattern recognition;uniform resource identifier	Nam-Gyu Cho;Alan L. Yuille;Seong-Whan Lee	2013	Pattern Recognition	10.1016/j.patcog.2012.09.006	computer vision;simulation;computer science	Vision	43.70788277136855	-47.84046376784721	72743
df2b3cffc4798ff248c2a4b6375b8919832dbc9a	real-time visual tracking and identification for a team of homogeneous humanoid robots		The use of a team of humanoid robots to collaborate in completing a task is an increasingly important field of research. One of the challenges in achieving collaboration, is mutual identification and tracking of the robots. This work presents a real-time vision-based approach to the detection and tracking of robots of known appearance, based on the images captured by a stationary robot. A Histogram of Oriented Gradients descriptor is used to detect the robots and the robot headings are estimated by a multiclass classifier. The tracked robots report their own heading estimate from magnetometer readings. For tracking, a cost function based on position and heading is applied to each of the tracklets, and a globally optimal labeling of the detected robots is found using the Hungarian algorithm. The complete identification and tracking system was tested using two igus Humanoid Open Platform robots on a soccer field. We expect that a similar system can be used with other humanoid robots, such as Nao and DARwIn-OP.		Hafez Farazi;Sven Behnke	2016		10.1007/978-3-319-68792-6_19	histogram of oriented gradients;artificial intelligence;simulation;humanoid robot;computer vision;tracking system;robot;open platform;computer science;eye tracking;homogeneous	Robotics	47.96023267288161	-40.749294989727716	72865
0c4d803fee1eb4178f6f2363cc95b200e56a7fe8	matching of 3d curves using semi-differential invariants	three dimensional curve matching;differential invariant;3d curve matching;semidifferential invariant description;curve extraction;least mean squares methods;iterative algorithms;3d sensor 3d curve matching three dimensional curve matching euclidean motions semidifferential invariant description first derivatives reference point curve similarity measure spl epsiv reciprocal correspondence least median of squares motion estimation partially occluded curves curve registration curve extraction;euclidean motions;image matching;curve registration;least median of squares;psi_visics;motion estimation;spl epsiv reciprocal correspondence;data mining;reference point;3d sensor;computer vision;epsiv;feature extraction;image registration;stereo vision;robustness motion estimation robotics and automation computer vision laboratories data mining stereo vision robot kinematics iterative algorithms least squares methods;partially occluded curves;first derivatives;robustness;feature extraction motion estimation image matching least mean squares methods image registration;reciprocal correspondence;similarity measure;curve similarity measure;robotics and automation;least squares methods;robot kinematics	A method for matching 3-D curves under Euclidean motions is presented. Our approach uses a semi-differential invariant description requiring only first derivatives and one reference point, thus avoiding the computation of high order derivatives. A novel curve similarity measure building on the notion of /spl epsiv/-reciprocal correspondence is proposed. It is shown that by combining /spl epsiv/-reciprocal correspondence with the robust least median of squares motion estimation, the registration of partially occluded curves can be accomplished. An experiment with real curves extracted from 3-D surfaces demonstrates that curve matching can be successfully performed even on data from a simple and cheap 3-D sensor. >		Tomás Pajdla;Luc Van Gool	1995		10.1109/ICCV.1995.466913	computer vision;mathematical optimization;feature extraction;computer science;stereopsis;image registration;motion estimation;mathematics;geometry;robot kinematics;robustness	Vision	51.00635361975599	-51.79985121826611	72973
bca7288d6279aeb66b0d42e3b3fcf375a854d3bf	image processing acceleration for intelligent unmanned aerial vehicle on mobile gpu	parallel computing;image processing;intelligent uav;cuda;mobile gpu	In this paper, we present an algorithm for providing visually-guided unmanned aerial vehicle (UAV) control using visual information that is processed on a mobile graphic processing unit (GPU). Most real-time machine vision applications for UAVs exploit low-resolution images because the shortage of computational resources comes from size, weight and power issue. This leads to the limitation that the data are insufficient to provide the UAV with intelligent behavior. However, GPUs have emerged as inexpensive parallel processors that are capable of providing high computational power in mobile environments. We present an approach for detecting and tracking lines that use a mobile GPU. Hough transform and clustering techniques were used for robust and fast tracking. We achieved accurate line detection and faster tracking performance using the mobile GPU as compared with an x86 i5 CPU. Moreover, the average results showed that the GPU provided approximately five times speedup as compared to an ARM quad-core Cortex-A15. We conducted a detailed analysis of the performance of proposed tracking and detection algorithm and obtained meaningful results that could be utilized in real flight.	aerial photography;graphics processing unit;image processing;unmanned aerial vehicle	Dongwoon Jeon;Doo-Hyun Kim;Young-Guk Ha;Vladimir Tyan	2016	Soft Comput.	10.1007/s00500-015-1656-y	embedded system;computer vision;simulation;image processing;computer science	Robotics	44.49527274646047	-39.12479711458168	73510
c63a482c9edec4fe56ab1e837da3f14ee8aa9317	asymmetrical gauss mixture models for point sets matching	gaussian processes;image matching feature extraction gaussian processes;image matching;sgmr algorithm;point sets registration;scale invariant feature transforms;point sets matching;estimation;feature extraction;image nonrigid transformation;single gauss model for mismatch rejection;coherence;robustness;feature extraction noise data models robustness estimation coherence;sift feature points;image pair;feature similarity;asymmetrical gauss mixture models;gauss component;feature extraction asymmetrical gauss mixture models point sets matching agmm algorithm point sets registration image pair gauss component feature similarity sgmr algorithm single gauss model for mismatch rejection image nonrigid transformation sift feature points scale invariant feature transforms;noise;data models;agmm algorithm	The probabilistic methods based on Symmetrical Gauss Mixture Model(SGMM)[4, 13, 8] have achieved great success in point sets registration, but are seldom used to find the correspondences between two images due to the complexity of the non-rigid transformation and too many outliers. In this paper we propose an Asymmetrical GMM(AGMM) for point sets matching between a pair of images. Different from the previous SGMM, the AGMM gives each Gauss component a different weight which is related to the feature similarity between the data point and model point, which leads to two effective algorithms: the Single Gauss Model for Mismatch Rejection(SGMR) algorithm and the AGMM algorithm for point sets matching. The SGMR algorithm iteratively filters mismatches by estimating a non-rigid transformation between two images based on the spatial coherence of point sets. The AGMM algorithm combines the feature information with position information of the SIFT feature points extracted from the images to achieve point sets matching so that much more correct correspondences with high precision can be found. A number of comparison and evaluation experiments reveal the excellent performance of the proposed SGMR algorithm and AGMM algorithm.	coherence (physics);collaborative product development;comment (computer programming);data point;experiment;rejection sampling;scale-invariant feature transform;simplex algorithm;subspace gaussian mixture model	Wenbing Tao;Kun Sun	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.207	data modeling;mathematical optimization;estimation;coherence;feature extraction;computer science;noise;machine learning;pattern recognition;gaussian process;mathematics;statistics;robustness	Vision	48.55833727389662	-51.36091578682202	73636
b4e4927e588b0a63af6e2e45eea504c2b0b52b57	estimation of 3d structure and camera calibration from images under noisy conditions			camera resectioning	Michael Unger	2011				Vision	52.23541424035716	-43.062822834561224	73793
7341d73f9b1accc9b6b6e8d00125f8c8233a18a8	deep cnn-based visual target tracking system relying on monocular image sensing		The one-on-one target tracking problem is important in robot vision. Previous studies mainly focused on locating, depth information and control mechanism. In this study, we construct an autonomously visual tracking system called learn-to-track (LtT) by using a novel approach. This system only depends on a monocular camera. The main component is a deep convolutional neural network called the LtT, which trains a supervised image classifier by using images captured by the monocular camera in the follower robot. By operating merely on two adjacent frames, the network can predict the estimated velocity of the target, i.e., the velocity control for the follower. To verify the effectiveness of the LtT system, we construct a large-scale dataset that supports download l in the simulator, in which the LtT network is trained and the LtT system performance is evaluated. Furthermore, a remarkable tracking performance is achieved.	artificial neural network;convolutional neural network;download;information and computation;information theory;simulation;tracking system;velocity (software development);video tracking	Yawen Cui;Bo Zhang;Wenjing Yang;Xiaodong Yi;Yuhua Tang	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489650	task analysis;convolutional neural network;visualization;pattern recognition;eye tracking;tracking system;artificial intelligence;monocular;computer science;download	Robotics	46.959249198484905	-40.65106931908789	73832
a42422eef2f2f2f34446ae31b9b6946947b43edc	a novel bio-inspired tactile tumour detection concept for capsule endoscopy		Examination of the gastrointestinal(GI) tract has traditionally been performed using endoscopy tools that allow a surgeon to see the inside of the lining of the digestive tract. Endoscopes are rigid or flexible tubes that use fibre-optics or cameras to visualise tissues in natural orifices. This can be an uncomfortable and very invasive procedure for the patient.		Benjamin Winstone;Chris Melhuish;Sanja Dogramadzi;Anthony G. Pipe;Mark Callaway	2014		10.1007/978-3-319-09435-9_56	gastroenterology;medicine;pathology;surgery	Robotics	39.85017280792776	-38.948011631657806	73882
2b2b42d3ff0427736e68e79c80fdc8dff43edd83	research on dynamic time warping multivariate time series similarity matching based on shape feature and inclination angle	information systems applications incl internet;software engineering programming and operating systems;computer communication networks;multivariate time series;shape feature;special purpose and application based systems;computer system implementation;similarity matching;computer systems organization and communication networks;dynamic time warping	Different sets of research mainly focus on one variable time series now, while researches involving multivariate time series have been insufficient. In this paper, combined linear segments and fitting error for multivariate time series, we present a new method to reduce the time complexity of DTW distance metric algorithm. Based on the shape feature and the tilt angle, we propose a new approach for similarity matching of DTW multivariate time series. Experimental results demonstrate that this method is helpful for ensuring accuracy and for reducing the time complexity of similarity matching.	algorithm;dynamic time warping;time complexity;time series	Danyang Cao;Jie Liu	2016	Journal of Cloud Computing	10.1186/s13677-016-0062-z	speech recognition;computer science;theoretical computer science;machine learning;dynamic time warping	ML	42.65003096487539	-43.8950981291818	73948
34ee1b72e587dd72469fbd7d507eacb16735f5e4	plane equation features in depth sensor tracking		The emergence of depth sensors has made it possible to track not only monocular cues but also the actual depth values of the environment. This is especially useful in augmented reality solutions, where the position and orientation (pose) of the observer need to be accurately determined. Depth sensors have usually been used in augmented reality as mesh builders and in some cases as feature extractors for tracking. These methods are usually extensive and designed to operate by itself or in cooperation with other methods. We propose a systematic light-weight algorithm to supplement other mechanisms and we test it against a random algorithm	augmented reality;buildingsmart;computation;depth perception;emergence;feature extraction;glossary of computer graphics;ground truth;image noise;mixed reality;pixel;pose (computer vision);randomized algorithm;sensor	Mika Taskinen;Tero Säntti;Teijo Lehtonen	2017		10.5220/0006425700170024	artificial intelligence;computer vision;mathematical optimization;computer science	Visualization	52.747067636210446	-44.51761296786403	74013
a21f9d1191114bbf51cbfe4e4648a36e0dae9559	describing, navigating and recognising urban spaces - building an end-to-end slam system		This paper describes a body of work being undertaken by our research group aimed at extending the utility and reach of mobile navigation and mapping. Rather than dwell on SLAM estimation (which has received ample attention over past years), we examine sibling problems which remain central to the mobile autonomy agenda. We consider the problem detecting loop-closure from an extensible, appearance-based probabilistic view point and the use of visual geometry to impose topological constraints. We also consider issues concerning the intrinsic quality of 3D range data / maps and finally describe our progress towards substantially enhancing the semantic value of built maps through scene de-construction and labeling.	map;sensor;spaces	Paul Newman;Manjari Chandran-Ramesh;Dave Cole;Mark Joseph Cummins;Alastair Harrison;Ingmar Posner;Derik Schröter	2007		10.1007/978-3-642-14743-2_21	computer vision;simulation;geography;artificial intelligence	Robotics	50.13793490655709	-43.562690253040586	74104
3cb55b4e10e6eb9b2203cc0d1b7a8d7e4a08f032	maximum likelihood motion segmentation using eigendecomposition	eigendecomposition;eigenvalues and eigenfunctions;motion classification motion segmentation eigendecomposition iterative framework maximum likelihood method similarity matrix motion vectors pixel blocks clustering method real world motion sequences;pattern clustering;motion control;motion segmentation computer vision maximum likelihood detection maximum likelihood estimation clustering algorithms motion estimation object detection motion control computer science clustering methods;real world motion sequences;image segmentation;maximum likelihood;motion vectors;image classification;motion estimation;pixel blocks;maximum likelihood estimation;computer vision;iterative methods;motion segmentation;matrix decomposition;motion vector;clustering method;maximum likelihood detection;iterative framework;classification error;clustering algorithms;ground truth;similarity matrix;computer science;clustering methods;maximum likelihood method;object detection;motion classification;matrix decomposition image segmentation motion estimation maximum likelihood estimation eigenvalues and eigenfunctions iterative methods image sequences pattern clustering image classification;image sequences	This paper presents an iterative maximum likelihood framework for motion segmentation. Our representation of the segmentation problem is based on a similarity matrix for the motion vectors for pairs of pixel blocks. By applying eigendecomposition to the similarity matrix, we develop a maximum likelihood method for grouping the pixel blocks into objects which share a common motion vector. We experiment with the resulting clustering method on a number of real-world motion sequences. Here ground truth data indicates that the method can result in motion classification errors as low as 3%.		Antonio Robles-Kelly;Edwin R. Hancock	2001		10.1109/ICIAP.2001.956986	computer vision;quarter-pixel motion;computer science;machine learning;pattern recognition;motion estimation;mathematics;maximum likelihood;motion field	Vision	46.89745397797101	-49.52958308670935	74139
24348db884f02bbe431994cab574429ebbeb527d	people tracking system with lighting effect estimation in open environment			tracking system	Ching-Tang Hsieh;Yeh-Kuang Wu;Chia-Yi Chen	2005			tracking system;computer vision;artificial intelligence;computer science	HCI	51.00445467925677	-43.005707794379354	74364
9e1a28b14fb7ba469864ced6d704f6516fa343e5	depth-based target segmentation for intelligent vehicles: fusion of radar and binocular stereo	obstaculo;vision ordenador;obstacle detection;intelligent vehicles image segmentation radar imaging spatial resolution radar detection machine vision object detection robustness motion detection intelligent sensors;estimation mouvement;image segmentation;image processing;vision estereoscopica;road traffic;real time;estimacion movimiento;depth of field;vision stereoscopique;automated highways;in vehicle sensors;procesamiento imagen;motion estimation;segmentation;data fusion;accuracy image segmentation motion stereo obstacle detection sensor fusion intelligent vehicle systems segmentation robustness;indexing terms;traffic safety;profondeur champ;traitement image;computer vision;detection objet;dynamic environment;trafic routier;signal processing;fusion donnee;intelligent vehicles;stereo image processing;seguridad trafico;profundidad campo;trafico carretera;vision ordinateur;securite trafic;sensor fusion;fusion datos;stereopsis;high speed;radar signal processing;segmentacion;automated highways radar signal processing image segmentation sensor fusion stereo image processing motion estimation;obstacle;object detection;radar	Dynamic environment interpretation is of special interest for intelligent vehicle systems. It is expected to provide lane information, target depth, and the image positions of targets within given depth ranges. Typical segmentation algorithms cannot solve the problems satisfactorily, especially under the high-speed requirements of a real-time environment. Furthermore, the variation of image positions and sizes of targets creates difficulties for tracking. In this paper, we propose a sensor-fusion method that can make use of coarse target depth information to segment target locations in video images. Coarse depth ranges can be provided by radar systems or by a vision-based algorithm introduced in the paper. The new segmentation method offers more accuracy and robustness while decreasing the computational load.	binocular vision;radar	Yajun Fang;Ichiro Masaki;Berthold K. P. Horn	2002	IEEE Trans. Intelligent Transportation Systems	10.1109/TITS.2002.802926	computer vision;simulation;image processing;computer science;signal processing;sensor fusion	Robotics	52.395111740177114	-38.40246896697794	74457
735ad115009e3175fea45a419f9b659eba2bdf77	an adaptive people counting system with dynamic features selection and occlusion handling	image processing;computer vision;electronic and computer engineering;crowd counting;surveillance systems	A pair of collaborative Gaussian process models (GP) is used to handle occlusion.A principled technique is proposed to measure the level of occlusion in a frame.It proposes a method of choosing the best features depending on their environment.The system is evaluated using two benchmark datasets, the Mall and UCSD datasets. This paper presents an adaptive crowd counting system for video surveillance applications. The proposed method is composed of a pair of collaborative Gaussian process models (GP) with different kernels, which are designed to count people by taking the level of occlusion into account. The level of occlusion is measured and compared with a predefined threshold for regression model selection for each frame. In addition, the proposed method dynamically identifies the best combination of features for people counting. The Mall and UCSD datasets are used to evaluate the proposed method. The results show that the proposed method offers a higher accuracy when compared against state of the art methods reported in open literature. The mean absolute error (MAE), mean squared error (MSE) and the mean deviation error (MDE) for the proposed algorithm are 2.90, 13.70 and 0.095, respectively, for the Mall dataset and 1.63, 4.32 and 0.066, respectively, for UCSD dataset.	benchmark (computing);computation;distortion;gaussian process;hidden surface determination;high- and low-level;media source extensions;model-driven engineering;people counter	Zeyad Q. H. Al-Zaydi;David L. Ndzi;Yanyan Yang;Latifah Kamarudin	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2016.05.018	computer vision;simulation;image processing;computer science;machine learning;data mining;algorithm;statistics	Vision	42.73039420873668	-49.526605674477246	74657
21706fc4e619ca4749c39b27aa1f79eaa1f347e0	adaptive window strategy for high-speed and robust klt feature tracker	robust tracker;high speed tracking;klt feature tracker	The Kanade-Lucas-Tomasi tracking KLT algorithm is widely used for local tracking of features. As it employs a translation model to find the feature tracks, KLT is not robust in the presence of distortions around the feature resulting in high inaccuracies in the tracks. In this paper we show that the window size in KLT must vary to adapt to the presence of distortions around each feature point in order to increase the number of useful tracks and minimize noisy ones. We propose an adaptive window size strategy for KLT that uses the KLT iterations as an indicator of the quality of the tracks to determine near-optimal window sizes, thereby significantly improving its robustness to distortions. Our evaluations with a well-known tracking dataset show that the proposed adaptive strategy outperforms the conventional fixed-window KLT in terms of robustness. In addition, compared to the well-known affine KLT, our method achieves comparable robustness at an average runtime speedup of 7x.		Nirmala Ramakrishnan;Thambipillai Srikanthan;Siew Kei Lam;Gauri Ravindra Tulsulkar	2015		10.1007/978-3-319-29451-3_29	computer vision;machine learning;kanade–lucas–tomasi feature tracker;pattern recognition	HCI	42.882138883880536	-48.68093654097886	74669
04928f04dabb2f79ba1f7eab3fb6da11b3355d95	on geometric and photometric registration of images	change detection;stereo image processing computer vision feature extraction image registration;contrast invariant feature extraction;feature detection algorithm computer vision panoramic imaging dynamic range imaging stereo imaging change detection image photometric registration algorithms;indexing terms;photometry computer vision layout cameras motion estimation detectors dynamic range detection algorithms parameter estimation histograms;high dynamic range imaging;feature detection algorithm;computer vision;image photometric registration algorithms;panoramic imaging;stereo imaging;feature extraction;image registration;stereo image processing;detection algorithm;dynamic range imaging;panoramic image;photometric registration contrast invariant feature extraction;photometric registration;invariant feature	Finding geometric and photometric relation among images is crucial in many computer vision tasks such as panoramic imaging, high dynamic range imaging, stereo imaging, and change detection. Most photometric registration algorithms require accurate geometric registration of images. On the other hand, geometric registration may fail when images are not aligned photometrically. There are two contributions of this paper: (i) A contrast invariant feature detection algorithm is proposed. This would allow geometric registration of images without photometric registration, (ii) A photometric registration algorithm that can handle scene occlusions is presented.	algorithm;computer vision;dynamic range;feature detection (computer vision);feature detection (web development);high-dynamic-range imaging;image registration;range imaging	Murat Gevrekci;Bahadir K. Gunturk	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366144	computer vision;index term;feature extraction;computer science;image registration;pattern recognition;change detection;computer graphics (images)	Vision	50.9904704577659	-49.31773892538653	74693
d2b65e77ee1bd24c74de0cfe3984d96b7841b12c	genre-adaptive near-duplicate video segment detection	entropy based pixel selection scheme;video streaming;image segmentation;data compression;video signal processing;detection accuracy;vector space;video segmentation;compressed feature vector space;two step detection method;feature vector;video coding;entropy based pixel selection scheme near duplicate segments video stream detection accuracy brute force frame by frame comparison two step detection method compressed feature vector space;video streaming data compression image segmentation video coding video signal processing;false positive;brute force frame by frame comparison;video stream;high dimension;streaming media video compression image segmentation acceleration histograms information science principal component analysis multimedia communication broadcasting spatiotemporal phenomena;near duplicate segments	This paper proposes a fast and accurate method to detect all near-duplicate segments in a video stream. To reduce the computation time while ensuring the detection accuracy equivalent to that by brute-force frame-by-frame comparison, a two-step detection method is proposed; a fast but rough detection applied in a compressed feature vector space spanned by the result of a PC A, followed by confirmation of candidates in the original high dimension space. The results show that the proposed method accelerates the detection by more than 1,000 times while maintaining the detection accuracy. We also propose an entropy-based pixel selection scheme to generate feature vectors optimized for comparison of video segments within programs with mostly common pictures. The results show that the proposed scheme eliminates the false positives drastically, which should lead to even faster detection.	computation;feature vector;pixel;rough set;step detection;streaming media;time complexity	Ichiro Ide;Kazuhiro Noda;Tomokazu Takahashi;Hiroshi Murase	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284692	data compression;computer vision;feature vector;type i and type ii errors;vector space;computer science;theoretical computer science;machine learning;pattern recognition;image segmentation;statistics	Robotics	39.331582561589755	-51.8864074394779	74697
552a1bf2c924ebafc27c96e7a8b1c321c3f669e3	combination of foveal and peripheral vision for object recognition and pose estimation	image recognition;object recognition;high resolution;peripheral vision;image segmentation;layout;computer vision;engineering and technology;teknik och teknologier;object recognition cameras machine vision robustness layout image segmentation image recognition computer vision real time systems solid modeling;real time vision;machine vision;indoor environment;solid modeling;field of view;robustness;geometric model;experimental evaluation;cameras;real time systems;pose estimation	In this paper, we present a real-time vision system that integrates a number of algorithms using monocular and binocular cues to achieve robustness in realistic settings, for tasks such as object recognition, tracking and pose estimation. The system consists of two sets of binocular cameras; a peripheral set for disparity based attention and a foveal one for higher level processes. Thus the conflicting requirements of a wide field of view and high resolution can be overcome. One important property of the system is that the step from task specification through object recognition to pose estimation is completely automatic, combining both appearance and geometric models. Experimental evaluation is performed in a realistic indoor environment with occlusions, clutter, changing lighting and background conditions.	3d pose estimation;algorithm;binocular disparity;binocular vision;clutter;depth perception;digital single-lens reflex camera;image resolution;outline of object recognition;peripheral vision;real-time clock;requirement	Mårten Björkman;Danica Kragic	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1302532	layout;computer vision;simulation;pose;image resolution;peripheral vision;3d pose estimation;machine vision;field of view;computer science;geometric modeling;cognitive neuroscience of visual object recognition;articulated body pose estimation;3d single-object recognition;image segmentation;solid modeling;robustness;computer graphics (images)	Robotics	49.699739826964816	-46.30204521170276	74743
2c497af85d31efb9a0c5049891c50a71bde59a59	application of gibbs-markov random field and hopfield-type neural networks for detecting moving objects from video sequences captured by static camera	markov random field;hopfield neural network;map estimation	In this article, we propose a moving objects detection scheme using Gibbs–Markov random field (GMRF) and Hopfield-type neural network (HTNN) in expectation maximization (EM) framework for video sequences captured by static camera. In the considered technique, the background model is built by considering a running Gaussian average over few past frames. The change vector analysis (CVA) scheme is followed on the considered target frame and the constructed reference frame to generate a difference image. The moving objects in target frame are detected by segmenting the difference image into two classes: changed and unchanged, where the changed class represents moving object regions and the unchanged class the background regions. For segmentation, we have modeled the CVA generated difference image with GMRF and the segmentation problem is solved using the maximum a posteriori probability (MAP) estimation principle. The MAP estimator is found to be exponential in nature; and thus a modified HTNN is exploited for estimating the MAP. The parameters of the GMRF model are estimated using EM algoCommunicated by V. Loia. B. N. Subudhi Department of Electronics and Communication Engineering, National Institute of Technology, Farmagudi, Ponda 403401, Goa, India e-mail: subudhi.badri@gmail.com. S. Ghosh Department of Computer Science and Engineering, Jadavpur University, Kolkata 700032, India e-mail: susmitaghoshju@gmail.com A. Ghosh (B) Machine Intelligence Unit, Indian Statistical Institute, Kolkata 700108, India e-mail: ash@isical.ac.in rithm. Experiments are carried out on three video sequences. Results of the proposed change detection scheme are compared with those of the codebook-based background subtraction and GMRF model with graph-cut schemes. It is found that the proposed technique provides better results.	artificial neural network;background subtraction;change vector;codebook;cut (graph theory);email;expectation–maximization algorithm;ground truth;hopfield network;markov chain;markov random field;performance evaluation;reference frame (video);scott continuity;sensor;time complexity	Badri Narayan Subudhi;Susmita Ghosh;Ashish Ghosh	2015	Soft Comput.	10.1007/s00500-014-1440-4	computer vision;machine learning;pattern recognition;mathematics	Vision	43.665419637966366	-48.30996205587011	74916
52483078874130137d2c939df76d68eb1c698929	multiple moving person tracking based on the impresario simulator	subtraction;ccd camera;image processing;한국정보통신학회;vol 6 no 3;taeseok jin;detection;hyun deok kim;multiple moving person tracking based on the impresario simulator;the korea institute of information and communication engineering;person tracking;journal of information and communication convergence engineering 제6권 제3호	In this paper, we propose a real-time people tracking system with multiple CCD cameras for security inside the building. To achieve this goal, we present a method for 3D walking human tracking based on the IMPRESARIO framework incorporating cascaded classifiers into hypothesis evaluation. The efficiency of adaptive selection of cascaded classifiers has been also presented. The camera is mounted from the ceiling of the laboratory so that the image data of the passing people are fully overlapped. The implemented system recognizes people movement along various directions. To track people even when their images are partially overlapped, the proposed system estimates and tracks a bounding box enclosing each person in the tracking region. The approximated convex hull of each individual in the tracking area is obtained to provide more accurate tracking information. We have shown the improvement of reliability for likelihood calculation by using cascaded classifiers. Experimental results show that the proposed method can smoothly and effectively detect and track walking humans through environments such as dense forests.		Hyun Deok Kim;Tae-Seok Jin	2008	J. Inform. and Commun. Convergence Engineering		computer vision;simulation;tracking system;geography;computer graphics (images)	AI	46.519962277359014	-42.769857203837574	74966
0478fa23bd6dd617b79926c71519f30679ae653c	evaluation of maritime vision techniques for aerial search of humans in maritime environments	humans oceans target tracking sea surface cameras machine vision australia object detection search problems surface morphology;search and rescue;mathematical morphology;human fatigue;oceans;temporal tracking;point target detection;maritime search problem;radar tracking;hidden markov model;maritime;maritime search problem machine vision techniques aerial search human fatigue two phased approach point target detection temporal tracking target detection techniques hidden markov model;a priori knowledge;hidden markov models;image color analysis;machine vision;pixel;mathematical morphology search and rescue maritime machine vision hidden markov models;two phased approach;marine engineering;object detection hidden markov models marine engineering;target tracking;aerial search;target detection;object detection;machine vision techniques;target detection techniques;noise	Searching for humans lost in vast stretches of ocean has always been a difficult task. In this paper, a range of machine vision approaches are investigated as candidate tools to mitigate the risk of human fatigue and complacency after long hours performing these kind of search tasks. Our two-phased approach utilises point target detection followed by temporal tracking of these targets. Four different point target detection techniques and two tracking techniques are evaluated. We also evaluate the use of different colour spaces for target detection. This paper has a particular focus on Hidden Markov Model based tracking techniques, which seem best able to incorporate a priori knowledge about the maritime search problem, to improve detection performance.	aerial photography;color space;hidden markov model;humans;machine vision;markov chain;search problem	Paul Westall;Jason J. Ford;Peter O'Shea;Stefan Hrabar	2008	2008 Digital Image Computing: Techniques and Applications	10.1109/DICTA.2008.89	computer vision;radar tracker;a priori and a posteriori;simulation;mathematical morphology;computer science;noise;machine learning;pixel;hidden markov model	Vision	42.946085617206215	-42.966075079461234	74984
19542981dbf98e0d49d76ac0697f192825ccb9f1	tactile-based object center of mass exploration and discrimination		In robotic tasks, object recognition and discrimination can be realized according to their physical properties, such as color, shape, stiffness, and surface textures. However, these external properties may fail if they are similar or even identical. In this case, internal properties of the objects can be considered, for example, the center of mass. Center of mass is an important inherent physical property of objects; however, due to the difficulties in its determination, it has never been applied in object discrimination tasks. In this work, we present a tactile-based approach to explore the center of mass of rigid objects and apply it in robotic object discrimination tasks. This work comprises three aspects: (a) continuous estimation of the target object's geometric information, (b) exploration of the center of mass, and (c) object discrimination based on the center of mass features. Experimental results show that by following our proposed approach, the center of mass of experimental objects can be accurately estimated, and objects of identical external properties but different mass distributions can be successfully discriminated. Our approach is also robust against the textural properties and stiffness of experimental objects.	emoticon;hexahedron;modal logic;outline of object recognition;quantum number;robot;sensor;workspace	Kunpeng Yao;Mohsen Kaboli;Gordon Cheng	2017	2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids)	10.1109/HUMANOIDS.2017.8246975	computer science;center of mass;grippers;control theory;physical property;computer vision;stiffness;cognitive neuroscience of visual object recognition;robot kinematics;artificial intelligence	Robotics	49.33770944423743	-39.71394459817668	75267
89c215ddefb9afba2d876028f86056d55b0ec7dd	combined online and offline information for tracking facial feature points	facial feature;gabor wavelet;3d tracking;bundle adjustment	This paper proposes a novel real-time facial feature points tracking method. A 3D geometric face model is used to give a robust tracking which includes offline information that the movement constraints of facial feature points in 3D space. The iterative frame-to-frame tracking method with Gabor wavelet is used to give a high accuracy which is robust to homogeneous illumination changing and affine deformation of the face image. The former tracking method based offline information and the latter tracking method based on online information are integrated with the bundle adjustment method. We compare our method with three other typical methods. The experimental results show that it can be used for robust, real-time and wide-angle facial feature tracking.	online and offline	Xin Wang;Yequn Zhang;Chunlei Chai	2012		10.1007/978-3-642-33509-9_19	computer vision;machine learning;pattern recognition;mathematics;bundle adjustment;gabor wavelet	ML	49.23357651665659	-46.84010854659756	75302
455ada897daed072a5716e3205ca95cc6ea1f7c5	labeled rfs-based track-before-detect for multiple maneuvering targets in the infrared focal plane array	labeled multi bernoulli;labeled random finite sets;track before detect;maneuvering target;sequential monte carlo	The problem of jointly detecting and tracking multiple targets from the raw observations of an infrared focal plane array is a challenging task, especially for the case with uncertain target dynamics. In this paper a multi-model labeled multi-Bernoulli (MM-LMB) track-before-detect method is proposed within the labeled random finite sets (RFS) framework. The proposed track-before-detect method consists of two parts-MM-LMB filter and MM-LMB smoother. For the MM-LMB filter, original LMB filter is applied to track-before-detect based on target and measurement models, and is integrated with the interacting multiple models (IMM) approach to accommodate the uncertainty of target dynamics. For the MM-LMB smoother, taking advantage of the track labels and posterior model transition probability, the single-model single-target smoother is extended to a multi-model multi-target smoother. A Sequential Monte Carlo approach is also presented to implement the proposed method. Simulation results show the proposed method can effectively achieve tracking continuity for multiple maneuvering targets. In addition, compared with the forward filtering alone, our method is more robust due to its combination of forward filtering and backward smoothing.	acclimatization;adult fanconi syndrome;algorithm;bernoulli polynomials;conflict (psychology);experiment;focal (programming language);interaction;line mode browser;lithium;markov chain;monte carlo method;multiple personality disorder;numerous;qm/mm;quantum fluctuation;raw image format;remote file sharing;scott continuity;sensor;simulation;sm antibody;smoothing (statistical technique);staring array;tracer;track-before-detect	Miao Li;Jun Li;Yiyu Zhou	2015		10.3390/s151229829	track-before-detect;computer vision;mathematical optimization;simulation;particle filter;computer science;statistics	Robotics	46.08679224633854	-47.67241353182572	75321
79df08488d136401a32f49b7e12eea7a6c320d3e	team homer@unikoblenz - approaches and contributions to the robocup@home competition	robocup home;service robots;robotic architecture;homer unikoblenz;robot lisa	In this paper we present the approaches and contributions of team homer@UniKoblenz that were developed for and applied during the RoboCup@Home competitions. In particular, we highlight the different abstraction layers of our software architecture that allows for rapid application development based on the ROS actionlib. This architectural design enables us to focus on the development of new algorithms and approaches and significantly helped us in winning the RoboCup@Home competition in 2015. We further give an outlook on recently published open-source software for service robots that can be downloaded from our ROS package repository on http://wiki.ros.org/agas-ros-pkg.	3d single-object recognition;algorithm;computer vision;microsoft outlook for mac;object detection;open-source software;outline of object recognition;rapid application development;robot operating system;robotics;software architecture;software repository	Viktor Seib;Stephan Manthe;Raphael Memmesheimer;Florian Polster;Dietrich Paulus	2015		10.1007/978-3-319-29339-4_7	simulation;artificial intelligence	Robotics	45.71871447584013	-38.54677049054692	75328
a3a78ace7a83c529f65d426a95818779417b2c28	3d mesh-based representation of spherical images for dense rotation estimation	topology;shape;estimation;three dimensional displays;robustness;mesh generation;harmonic analysis	In this paper, we propose a dense approach for 3D rotation estimation between spherical images, which is simultaneously able to recover the large rotations, robust under clutter and small translations. The key idea is to represent the spherical images by 3D shapes of the triangular mesh surfaces based on image intensity signal. This allows to apply the spherical harmonics representation as 3D shape descriptor. The optimum rotation computation is recovered through the SVD decomposition of the cross covariance matrix, which is obtained from the two 3D shapes spherical harmonics coefficients. The performances of the proposed approach are examined using both synthetic and real image datasets. Experimental results show the effectiveness of our approach for rotation estimation, as well as its robustness against real conditions, image occlusions and small translations. The efficiency of the proposed approach is compared with that of competitive methods.	algorithm;clutter;coefficient;computation;continuation;cross-covariance;cross-validation (statistics);experiment;performance;polygon mesh;simultaneous localization and mapping;singular value decomposition;spectral density estimation;synthetic data;synthetic intelligence;visual odometry	Houssem-Eddine Benseddik;Hicham Hadj-Abdelkader;Brahim Cherki;Samia Bouchafa	2016	2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)	10.1109/ICARCV.2016.7838756	mesh generation;computer vision;mathematical optimization;estimation;shape;harmonic analysis;mathematics;geometry;statistics;robustness	Robotics	48.696497511756704	-51.33104186987143	75356
7762e832384789caf8a667dc32068e9fb554fe3f	3d motion estimation based on pitch and azimuth from respective camera and laser rangefinder sensing	vehicles cameras three dimensional displays trajectory motion estimation geometry roads;automobiles;sloping terrain 3d motion estimation car like structured motion model omnidirectional camera laser rangefinder vision sensor planar motion image point motion orientation absolute translation omnidirectional image based one point correspondence rotation components;motion estimation;image sensors;traffic engineering computing automobiles cameras image sensors motion estimation;traffic engineering computing;one point correspondence 3d motion estimation fusion multiple sensors laser range finder omnidirectional camera;cameras	This paper proposes a new method to estimate the 3D motion of a vehicle based on car-like structured motion model using an omnidirectional camera and a laser rangefinder. In recent years, motion estimation using vision sensor has improved by assuming planar motion in most conventional research to reduce requirement parameters and computational cost. However, for real applications in environment of outdoor terrain, the motion does not satisfy this condition. In contrast, our proposed method uses one corresponding image point and motion orientation to estimate the vehicle motion in 3D. In order to reduce requirement parameters for speedup computational systems, the vehicle moves under car-like structured motion model assumption. The system consists of a camera and a laser rangefinder mounted on the vehicle. The laser rangefinder is used to estimate motion orientation and absolute translation of the vehicle. An omnidirectional image-based one-point correspondence is used for combining with motion orientation and absolute translation to estimate rotation components of yaw, pitch angles and three translation components of Tx, Ty, and Tz. Real experiments in sloping terrain demonstrate the accuracy of vehicle localization estimation using the proposed method. The error at the end of travel position of our method, one-point RANSAC are 1.1%, 5.1%, respectively.	algorithmic efficiency;experiment;global positioning system;motion compensation;motion estimation;omnidirectional camera;pitch (music);random sample consensus;speedup;time complexity;tree accumulation;web application;yaws	Van-Dung Hoang;Danilo Cáceres Hernández;Le-My Ha;Kang-Hyun Jo	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6696433	computer vision;match moving;structure from motion;simulation;computer science;motion estimation;image sensor;optics;motion field	Robotics	53.75619242024432	-42.029507585774475	75829
7040bb78afc4c394e7275e9c6ea72b9240d2968b	multi-camera sensor system for 3d segmentation and localization of multiple mobile robots	health research;uk clinical guidelines;biological patents;sensor system;3d segmentation;europe pubmed central;citation search;mobile robots;intelligent space;3d positioning;motion segmentation;multiple mobile robots;uk phd theses thesis;multi camera sensor;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	This paper presents a method for obtaining the motion segmentation and 3D localization of multiple mobile robots in an intelligent space using a multi-camera sensor system. The set of calibrated and synchronized cameras are placed in fixed positions within the environment (intelligent space). The proposed algorithm for motion segmentation and 3D localization is based on the minimization of an objective function. This function includes information from all the cameras, and it does not rely on previous knowledge or invasive landmarks on board the robots. The proposed objective function depends on three groups of variables: the segmentation boundaries, the motion parameters and the depth. For the objective function minimization, we use a greedy iterative algorithm with three steps that, after initialization of segmentation boundaries and depth, are repeated until convergence.	greedy algorithm;image sensor;iterative method;loss function;mobile robot;numerous;optimization problem;robot (device);biologic segmentation	Cristina Losada;Manuel Mazo;Sira E. Palazuelos;Daniel Pizarro-Perez;Marta Marrón Romera	2010		10.3390/s100403261	mobile robot;embedded system;computer vision;simulation;computer science;bioinformatics;artificial intelligence;segmentation-based object categorization;scale-space segmentation	Robotics	51.421206255759174	-44.68253893408362	75836
80499c3b9b6460048e94ab782b7ffff7b942d3fd	robust fastener detection for autonomous visual railway track inspection	detectors;rails;rail transportation;support vector machines computer vision fasteners image classification railways statistical analysis;support vector machines;training;inspection;us northeast corridor robust fastener detection autonomous visual railway track inspection critical railway components computer vision methods intra class variation classification margin histogram of oriented gradients features linear svm classifiers probability;fasteners training detectors inspection rails support vector machines rail transportation;fasteners	Fasteners are critical railway components that maintain the rails in a fixed position. Their failure can lead to train derailments due to gage widening or wheel climb, so their condition needs to be periodically monitored. Several computer vision methods have been proposed in the literature for track inspection applications. However, these methods are not robust to clutter and background noise present in the railroad environment. This paper proposes a new method for fastener detection by 1) carefully aligning the training data, 2) reducing intra-class variation, and 3) bootstrapping difficult samples to improve the classification margin. Using the histogram of oriented gradients features and a combination of linear SVM classifiers, the system described in this paper can inspect ties for missing or defective rail fastener problems with a probability of detection of 98% and a false alarm rate of 1.23% on a new dataset of 85 miles of concrete tie images collected in the US Northeast Corridor (NEC) between 2012 and 2013. To the best of our knowledge, this dataset of 203,287 crossties is the largest ever reported in the literature.	clutter;computer vision;gradient;histogram of oriented gradients;wildlife corridor	Xavier Gibert;Vishal M. Patel;Rama Chellappa	2015	2015 IEEE Winter Conference on Applications of Computer Vision	10.1109/WACV.2015.98	support vector machine;detector;simulation;inspection;computer science;machine learning	Vision	40.87578599036306	-46.13369520617418	76116
545e76fc04b131bbafbee3937799d9c2815a8f12	real-time dance evaluation by markerless human pose estimation		This paper presents a unified framework that evaluates dance performance by markerless estimation of human poses. Dance involves complicated poses such as full-body rotation and self-occlusion, so we first develop a human pose estimation method that is invariant to these factors. The method uses ridge data and data pruning. Then we propose a metric to quantify the similarity (i.e., timing and accuracy) between two dance sequences. To validate the proposed dance evaluation method, we conducted several experiments to evaluate pose estimation and dance performance on the benchmark dataset EVAL, SMMC-10 and a large K-Pop dance database, respectively. The proposed methods achieved pose estimation accuracy of 0.9358 mAP, average pose error of 3.88 cm, and 98% concordance with experts’ evaluation of dance performance.	3d pose estimation;benchmark (computing);concordance (publishing);eval;experiment;hidden surface determination;real-time transcription;unified framework	Yeonho Kim;Daijin Kim	2018	Multimedia Tools and Applications	10.1007/s11042-018-6068-4	eval;computer vision;computer science;artificial intelligence;dance;pose;pattern recognition;invariant (mathematics)	Vision	49.99070357568952	-45.165841650878505	76209
4d0a31b0f9d1eb28473b09d3e0e01b6325bcb866	epinet: a fully-convolutional neural network using epipolar geometry for depth from light field images		Light field cameras capture both the spatial and the angular properties of light rays in space. Due to its property, one can compute the depth from light fields in uncontrolled lighting environments, which is a big advantage over active sensing devices. Depth computed from light fields can be used for many applications including 3D modelling and refocusing. However, light field images from hand-held cameras have very narrow baselines with noise, making the depth estimation difficult. Many approaches have been proposed to overcome these limitations for the light field depth estimation, but there is a clear trade-off between the accuracy and the speed in these methods. In this paper, we introduce a fast and accurate light field depth estimation method based on a fully-convolutional neural network. Our network is designed by considering the light field geometry and we also overcome the lack of training data by proposing light field specific data augmentation methods. We achieved the top rank in the HCI 4D Light Field Benchmark on most metrics, and we also demonstrate the effectiveness of the proposed method on real-world light-field images.		Changha Shin;Hae-Gon Jeon;Youngjin Yoon;In-So Kweon;Seon Joo Kim	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00499	computer vision;machine learning;convolutional neural network;light field;artificial neural network;artificial intelligence;computer science;image resolution;ray;training set;epipolar geometry	Vision	53.14317401636333	-45.29093601373793	76255
cd2737a96b0ad411067d94858ca4b8b1a5447dd5	corner proposals from hevc bitstreams		Corner-like features are important in computer vision problems such as object matching, tracking, recognition, and retrieval. Most corner detectors operate in the pixel domain, which means that they require image or video to be fully decoded and reconstructed before detection can start. In this paper we describe a method for generating corner proposals from compressed HEVC bitstreams without full decoding. Specifically, we utilize HEVC syntax and intra prediction directions to find the locations that are likely to contain corners. The proposed method is lightweight and can be applied to intra-coded frames or still images coded by HEVC. Experimental results illustrate that the proposed method is able to identify most regions where conventional pixel-domain corner detectors would find corners.	code;computer vision;corner detection;high efficiency video coding;intra-frame coding;iterative reconstruction;object detection;pixel;sensor	Hyomin Choi;Ivan V. Bajic	2017	2017 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2017.8050705	pixel;theoretical computer science;syntax;computer science;detector;decoding methods;benchmark (computing);integrated circuit;computer vision;artificial intelligence	Vision	44.77955696313933	-44.56715708793947	76303
9fb3ea72a22826774a5c95746e0237aebe84c8e0	real-time tracking of multiple objects by linear motion and repulsive motion		Successful multi-object tracking requires consistently maintaining object identities and real-time performance. This task becomes more challenging when objects are indistinguishable from one another. This paper presents a Bayesian framework for maintaining the identities of multiple objects. Our semi-independent joint motion model (SIMM) solves the coalescence and identity switching problem in real time. This joint motion model is a non-parametric mixture model that simultaneously captures linear motion and repulsive motion. Linear motion is a constant velocity model, while repulsive motion is described by a repulsive potential in MRF. By maintaining multimodality from multiple motion models, we can infer the appropriate motion model using image evidence and consequently avoid many identity switching errors. Moreover, we develop a new sampling method that does not suffer from the curse of dimensionality because of the availability of high-quality samples. Experimental results show that our approach can track numerous objects in real time and maintain identities under difficult situations.	algorithm;coalescing (computer science);cobham's thesis;curse of dimensionality;markov random field;mixture model;real-time locating system;simm;sampling (signal processing);scalability;semiconductor industry;velocity (software development)	Lejun Shen;Zhisheng You;Qing Liu	2014		10.1007/978-3-319-16817-3_24	motion estimation;motion field;linear motion	Vision	46.26004643388606	-47.78017251103152	76319
c48b917f97f80ac3051be40c7895fd3dc9006e45	developing rigid motion constraints for the registration of free-form shapes	computational geometry computer vision image registration motion estimation;computational geometry;motion estimation;computer vision;iterative closest point free form shapes image registration rigid motion constraints computer vision geometric algorithm;image registration;shape iterative closest point algorithm image registration machine vision technology management geology motion analysis image motion analysis robustness object recognition;iterative closest point;synthetic data;experience base	In this paper w e propose a novel method t o deal wi th sphere ambiguity, occlusion, appearance and disappearance of po in t s in image registration. W e have developed a n u m b e r of rigid mo t ion constraints through analysis of geometrical properties of reflected correspondence vectors synthesised in to a single coordinate frame. T h e properties are used as fu r ther constraints t o eliminate fa l se ma tches obtained by the ICP criterion. A n u m b e r of experiments based o n both synthetic data and real images demonstrate tha t t he proposed method is accurate, robust, and e f i c i e n t f o r t he registration of freeform shapes.	execution unit;experiment;image registration;synthetic data	Yonghuai Liu;Marcos A. Rodrigues;Ying Wang	2000		10.1109/IROS.2000.895308	point set registration;computer vision;mathematical optimization;computational geometry;computer science;image registration;motion estimation;mathematics;geometry;motion field;iterative closest point;synthetic data	Vision	52.396193681717534	-51.24064589050231	76407
319dfa833203db08db67444a6ecf726e53fb028d	real-time head tracking from the deformation of eye contours using a piecewise affine camera	human computer interaction;real time;head tracking;affine camera models;computer vision;3d human computer interaction;interface metaphor;contour tracking;immersive virtual reality;parameter estimation;pose estimation	A computer vision based approach for human±computer interaction through head movements is presented and evaluated in a non-immersive virtual reality context. Once intercepted and tracked in real-time using a piecewise ane camera model and ane-deformable eye contours, user head displacements are estimated and remapped onto the tridimensional graphic environment according to a natural interface metaphor. Both the real-time performance of the tracker and the improved head parameter estimation accuracy ± as compared to the one obtainable using globally ane camera models ± encourage the use of this approach to support diverse advanced interaction scenarios and applications. Ó 1999 Elsevier Science B.V. All rights reserved.	computer vision;estimation theory;immersion (virtual reality);interface metaphor;motion capture;real-time clock;real-time transcription;virtual reality	Carlo Colombo;Alberto Del Bimbo	1999	Pattern Recognition Letters	10.1016/S0167-8655(99)00036-7	computer vision;simulation;pose;interface metaphor;computer science;estimation theory;immersion;computer graphics (images)	Vision	49.35396269842878	-44.036678440510094	76622
b48ae107bf259036c746ca7b281c417844fac562	image correspondence from motion subspace constraint and epipolar constraint	image matching;correspondence problem;image sequence;geometric constraints	In this paper, we propose a novel method for inferring image correspondences on the pair of synchronized image sequences. In the proposed method, after tracking the feature points in each image sequence over several frames, we solve the image corresponding problem from two types of geometrical constraints: (1) the motion subspace obtained from the tracked feature points of a target sequence, and (2) the epipolar constraints between the two cameras. Dissimilarly to the conventional correspondence estimation based on image matching using pixel values, the proposed approach enables us to obtain the correspondences even though the feature points, that can be seen from one camera view, but can not be seen (occluded or outside of the view) from the other camera. The validity of our method is demonstrated through the experiments using synthetic and real images.	baseline (configuration management);constraint logic programming;epipolar geometry;experiment;image registration;pixel;random sample consensus;synthetic intelligence	Shigeki Sugimoto;Hidekazu Takahashi;Masatoshi Okutomi	2007		10.1007/978-3-540-76390-1_38	computer vision;mathematical optimization;feature detection;computer science;mathematics;geometry;fundamental matrix;correspondence problem;motion field;epipolar geometry	Vision	51.85505374008498	-49.744760365388686	76661
a8e12a353a60f2f0739c9abd98517c01cc1b58a5	multi-body icp: motion segmentation of rigid objects on dense point clouds	iterative closest point icp;principal component analysis computer graphics graph theory image segmentation image sequences iterative methods motion estimation pattern clustering;multibody icp motion clustering rigidly moving objects motions estimation gpca generalized principal component analysis icp algorithm iterative closest point graph cuts rigid motions point clouds sequence moving parts segmentation dynamic environment moving objects segmentation dense point clouds motion segmentation;motion clustering;motion segmentation;motion segmentation iterative closest point algorithm trajectory computer vision three dimensional displays labeling clustering algorithms;iterative closest point icp motion segmentation motion clustering	Motion is one of the crucial keys for segmenting moving objects in dynamic environment. This paper proposes an approach of segmenting moving parts in the sequence of point clouds. Different rigid motions in dense point clouds are found and clustered by applying segmentation schemes such as graph-cuts into Iterative Closest Point (ICP) algorithm with initial segmentation from Generalized PCA (GPCA). Result shows that we can estimate motions of rigidly moving objects and segment parts having different motions simultaneously.	algorithm;coexist (image);cut (graph theory);generalized least squares;iterative closest point;iterative method;matching (graph theory);point cloud	Youngji Kim;Hwasup Lim;Sang Chul Ahn	2015	2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2015.7358823	computer vision;mathematical optimization;segmentation-based object categorization;scale-space segmentation;iterative closest point	Robotics	48.69402449155409	-49.33907108937053	76725
07928c6faa1d974cdf4e38e0109c83573928ba8e	dense 3d motion capture from synchronized video streams	complex nonrigid motions dense 3d motion capture video stream synchronization fixed topology multiview stereo software rigid motion model;rigid motion model;image motion analysis;video streaming;fixed topology;video stream synchronization;motion estimation;deformable models;layout;video streaming cameras image motion analysis stereo image processing;surface reconstruction;motion capture;shape;streaming media;three dimensional displays;image reconstruction;pixel;stereo image processing;time use;multi view stereo;face;optimization;multiview stereo software;dense 3d motion capture;complex nonrigid motions;streaming media layout motion estimation cameras tracking shape image motion analysis deformable models surface reconstruction image reconstruction;cameras;tracking	This paper proposes a novel approach to non-rigid, markerless motion capture from synchronized video streams acquired by calibrated cameras. The instantaneous geometry of the observed scene is represented by a polyhedral mesh with fixed topology. The initial mesh is constructed in the first frame using the publicly available PMVS software for multi-view stereo [7]. Its deformation is captured by tracking its vertices over time, using two optimization processes at each frame: a local one using a rigid motion model in the neighborhood of each vertex, and a global one using a regularized nonrigid model for the whole mesh. Qualitative and quantitative experiments using seven real datasets show that our algorithm effectively handles complex nonrigid motions and severe occlusions.	algorithm;experiment;mathematical optimization;mesh networking;motion capture;polyhedron;streaming media	Yasutaka Furukawa;Jean Ponce	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587495	iterative reconstruction;face;layout;computer vision;motion capture;simulation;surface reconstruction;shape;computer science;motion estimation;mathematics;geometry;tracking;pixel;computer graphics (images)	Vision	52.06580859788719	-48.8901836710919	76763
2ea9722eb4778fe490625808ca7794655631f92d	olt: a toolkit for object labeling applied to robotic rgb-d datasets	robot sensing systems;labeling three dimensional displays robot sensing systems image reconstruction robot kinematics;software engineering decision making image colour analysis image segmentation image sequences mobile robots object recognition robot vision;three dimensional displays;image reconstruction;rgb d sequences olt object labeling toolkit robotic rgb d datasets software components mobile robot 3d reconstruction object segmentation depth images high level decision making semantic mapping contextual object recognition;labeling;robot kinematics	In this work we present the Object Labeling Toolkit (OLT), a set of software components publicly available for helping in the management and labeling of sequential RGB-D observations collected by a mobile robot. Such a robot can be equipped with an arbitrary number of RGB-D devices, possibly integrating other sensors (e.g. odometry, 2D laser scanners, etc.). OLT first merges the robot observations to generate a 3D reconstruction of the scene from which object segmentation and labeling is conveniently accomplished. The annotated labels are automatically propagated by the toolkit to each RGB-D observation in the collected sequence, providing a dense labeling of both intensity and depth images. The resulting objects' labels can be exploited for many robotic oriented applications, including high-level decision making, semantic mapping, or contextual object recognition. Software components within OLT are highly customizable and expandable, facilitating the integration of already-developed algorithms. To illustrate the toolkit suitability, we describe its application to robotic RGB-D sequences taken in a home environment.	3d reconstruction;algorithm;component-based software engineering;geometric primitive;high- and low-level;mobile robot;odometry;outline of object recognition;semantic mapper;sensor	José-Raúl Ruiz-Sarmiento;Cipriano Galindo;Javier González	2015	2015 European Conference on Mobile Robots (ECMR)	10.1109/ECMR.2015.7324214	iterative reconstruction;computer vision;labeling theory;simulation;computer science;artificial intelligence;robot kinematics	Robotics	49.533597407090134	-39.13374509137913	76768
d54db53f89b8fd989301faac5995baf9b1efe4a8	background subtraction in dynamic scenes with adaptive spatial fusing	background modeling;image resolution gaussian processes image classification;layout gaussian processes kernel gaussian distribution image segmentation laboratories computational efficiency color sun information science;image segmentation;background subtraction algorithms adaptive spatial fusing dynamic scenes pixel wise background models mixture of gaussians spatial correlation;image resolution;time complexity;gaussian processes;pixel wise background models;image classification;computational modeling;adaptation model;spatial correlation;dynamic texture;pixel;background subtraction;background subtraction algorithms;mixture of gaussians;adaptive spatial fusing;algorithm design and analysis;foreground background;noise;videos;dynamic scenes	Background subtraction in highly dynamic scenes has been a critical challenge for traditional pixel-wise background models which perform poorly when the background has dynamic textures. In this paper, we consider background modelling in a spatial perspective and make an attempt to exploit more information from the outputs of pixel-wise model. We propose a background subtraction scheme using adaptive spatial fusing to refine the output of typical pixel-wise background model — Mixture of Gaussians (MoG) and employ a MRF-MAP scheme to make foreground-background classification using the spatial correlation. Experiments on several challenge sequences show that our method is able to yield significantly better results than the traditional ones and is compelling with existing state of the art background subtraction algorithms. Additionally, we proved our algorithm has linear running time complexity and any pixel-wise background model could be easily integrated into our spatial fusing scheme which greatly enhanced its scalabilities and applications.	algorithm;background subtraction;experiment;foreground-background;google map maker;markov random field;median filter;mixture model;pixel;receiver operating characteristic;smoothing;time complexity	Zhiyu Wang;Hui Xu;Lifeng Sun;Shiqiang Yang	2009	2009 IEEE International Workshop on Multimedia Signal Processing	10.1109/MMSP.2009.5293304	time complexity;algorithm design;computer vision;contextual image classification;spatial correlation;image resolution;background subtraction;computer science;noise;machine learning;foreground-background;pattern recognition;mixture model;gaussian process;mathematics;image segmentation;computational model;pixel;statistics	Vision	44.146145819812254	-50.731003881367016	76857
431b0f111b61cd500a1f4eb22c6971680e30fd58	combining region-of-interest extraction and image enhancement for nighttime vehicle detection	regions of interest;vehicle detection;roi extraction;image enhancement;image edge detection;feature extraction;object proposals;intelligent systems;nighttime vehicle detection;pattern recognition;vehicle light detection;nighttime image enhancement;vehicles;proposals;score level multifeature fusion;nakagami distribution	In nighttime images, vehicle detection is a challenging task because of low contrast and luminosity. In this article, the authors combine a novel region-of-interest (ROI) extraction approach that fuses vehicle light detection and object proposals together with a nighttime image enhancement approach based on improved multiscale retinex to extract accurate ROIs and enhance images for accurate nighttime vehicle detection. Experimental results demonstrate that the proposed nighttime image enhancement method, score-level multifeature fusion, and the ROI extraction method are all effective for nighttime vehicle detection. But the proposed vehicle detection method demonstrates 93.34 percent detection rate and outperforms other models, detecting blurred and partly occluded vehicles, as well as vehicles in a variety of sizes, numbers, locations, and backgrounds.	image editing;region of interest;sensor	Hulin Kuang;Long Chen;Feng Gu;Jiajie Chen;Leanne Lai Chan;Hong Yan	2016	IEEE Intelligent Systems	10.1109/MIS.2016.17	computer vision;speech recognition;nakagami distribution;intelligent decision support system;feature extraction;computer science;artificial intelligence	Vision	41.67911035556593	-51.99255287396068	76946
419dc37ae00ff8bcf35da4b2ba60f714af811643	vision resolvability for visually servoed manipulation	visual servoing	This article introduces a sensor placement measure called vision resolvability. The measure provides a technique for estimating the relative ability of various visual sensors, including monocular systems, stereo pairs, multi-baseline stereo systems, and 3D rangefinders, to accurately control visually manipulated objects. The resolvability ellipsoid illustrates the directional nature of resolvability, and can be used to direct camera motion and adjust camera intrinsic parameters in real-time so that the servoing accuracy of the visual servoing system improves with camera-lens motion. The Jacobian mapping from task space to sensor space is derived for a monocular system, a stereo pair with parallel optical axes, and a stereo pair with perpendicular optical axes. Resolvability ellipsoids based on these mappings for various sensor configurations are presented. Visual servoing experiments demonstrate that vision resolvability can be used to direct camera-lens motion to increase the ability of a visually servoed manipulator to precisely servo objects. © 1996 John Wiley u0026 Sons, Inc.		Bradley J. Nelson;Pradeep K. Khosla	1996	J. Field Robotics	10.1002/(SICI)1097-4563(199602)13:2%3C75::AID-ROB2%3E3.0.CO;2-R	computer vision;simulation;computer science;engineering;stereopsis;control system;artificial intelligence;manipulator;optics;robotics;charge-coupled device;visual servoing	Robotics	52.958879902209354	-40.703333359168965	76997
44969af349cc65e50cbf5111ab0839648d896b44	modeling background activity for behavior subtraction	object path modeling;background luminance;background modeling;image motion analysis;background subtraction background modeling suspicious behavior detection motion detection;background activity;suspicious behavior detection;intrusion monitoring;surveillance;behavior subtraction;object path modeling background activity behavior subtraction camera based surveillance intrusion monitoring motion detection background luminance color modeling;color model;image sensors;surveillance image colour analysis image motion analysis image sensors;natural environment;image colour analysis;background subtraction;camera based surveillance;motion detection;color modeling	The detection of events that differ from what is considered normal is, arguably, the most important task for camera-based surveillance. Clearly, the definition of normal behavior differs from one application to another, and, therefore, approaches to its detection differ as well. In the case of intrusion monitoring, simple motion detection may be sufficient, such as based on background luminance/color modeling. However, in more complex scenarios, such as the detection of abandoned luggage, more advanced approaches have been developed, often relying on object path modeling. In this paper, we describe a new model for representing normality. Our model, that we call a behavior image, is low-dimensional and based on dynamics of luminance/color profiles, however it does not require explicit estimation of object paths. The process of estimating visual abnormality is then a simple comparison of training and observed behavior images, that we call behavior subtraction. We describe a new practical implementation of our model that is based on average activity. It is easy to program and requires little processing power and memory. Moreover, it is robust to motion detection errors, such as those resulting from parasitic background motion (e.g., heavy rain/snow, camera jitter). Most importantly, however, the method is not content-specific, and, therefore, is applicable to the monitoring of humans, cars or other objects in both uncluttered and highly-cluttered scenes. We support these claims by including various experimental results, from urban traffic, through sport scenes to natural environment.		Pierre-Marc Jodoin;Janusz Konrad;Venkatesh Saligrama	2008	2008 Second ACM/IEEE International Conference on Distributed Smart Cameras	10.1109/ICDSC.2008.4635683	computer vision;color model;simulation;background subtraction;computer science;image sensor;optics;natural environment;computer graphics (images)	Robotics	42.02322339656299	-44.8761265377841	77115
6c5a257840c84e54d16585c351472f12f6d67f4e	fast scale invariant feature detection and matching on programmable graphics hardware	scale invariant feature detection;frames per second;filtering;nvidia geforce 8800 gtx;kernel;interpolation;feature detection;paper;speeded up robust feature method;surf;computer graphics;image matching;real time;psi_visics;feature matching gpu surf feature extraction;gpu;natural images;feature matching;data mining;computer vision;speeded up robust features;cuda;feature vector;computer vision hardware computer graphics central processing unit robust stability data mining feature extraction image generation image databases spatial databases;graphics hardware;feature extraction;pixel;speeded up robust feature method scale invariant feature detection feature matching programmable graphics hardware graphics processing unit feature extraction;nvidia;graphic processing unit;programmable graphics hardware;computer science;graphics processing unit;processing speed;image matching computer graphics feature extraction;graphics;invariant feature	Ever since the introduction of freely programmable hardware components into modern graphics hardware, graphics processing units (GPUs) have become increasingly popular for general purpose computations. Especially when applied to computer vision algorithms where a Single set of Instructions has to be executed on Multiple Data (SIMD), GPU-based algorithms can provide a major increase in processing speed compared to their CPU counterparts. This paper presents methods that take full advantage of modern graphics card hardware for real-time scale invariant feature detection and matching. The focus lies on the extraction of feature locations and the generation of feature descriptors from natural images. The generation of these feature-vectors is based on the Speeded Up Robust Features (SURF) method [1] due to its high stability against rotation, scale and changes in lighting condition of the processed images. With the presented methods feature detection and matching can be performed at framerates exceeding 100 frames per second for 640 times 480 images. The remaining time can then be spent on fast matching against large feature databases on the GPU while the CPU can be used for other tasks.	algorithm;cuda;central processing unit;computation;computer graphics;computer vision;database;feature detection (computer vision);feature detection (web development);feature extraction;feature vector;general-purpose computing on graphics processing units;graphics hardware;graphics processing unit;matching (graph theory);mipmap;pixel;programming language;real-time transcription;simd;speeded up robust features;video card	Nico Cornelis;Luc Van Gool	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4563087	filter;computer vision;kernel;feature vector;feature extraction;surf;interpolation;computer science;graphics;theoretical computer science;feature detection;real-time computer graphics;computer graphics;graphics hardware;frame rate;feature;pixel;computer graphics (images)	Vision	41.10259966590539	-51.57054314786059	77250
7ebcb09e3f196ae961c69db1cbcd76565047d918	design of the high-speed a/d sampling control circuit basing on fpga	random sample consensus technique;eroded markings;road geometry;image processing;advanced driver assistance system;random sampling;automated highways;urban road;ransac;strong shadows;computer vision;lane extraction method;computational modeling;roads;image color analysis;hyperbola pairs;feature extraction;intelligent vehicles;pixel;symmetrical local threshold;roads pixel vehicles feature extraction cameras computational modeling image color analysis;lane distribution;high curvature;image processing techniques;country road;ground truth;geometric model;vision based lane detection;vehicles;hyperbola pairs lane detection intelligent vehicles computer vision ransac;country road intelligent vehicles image processing geometric model vision based lane detection advanced driver assistance system strong shadows occlusions eroded markings high curvature lane extraction method symmetrical local threshold hyperbola road model optimal model fit lane distribution random sample consensus technique road geometry vehicle position vehicle direction urban road;vehicle position;optimal model fit;hyperbola road model;driver information systems;occlusions;vehicle direction;lane detection;cameras;extraction method;optimization model;object detection;road vehicles;road vehicles automated highways computer vision driver information systems feature extraction object detection	This design describe a sample control process according to sampling sequence with MAX+PLUSII software and the VHDL language tool. The author takes TLC5510 as example, introduces a kind of designing method of the A/D sampling control circuit basing on the high-speed FPGA. The result of simulation expresses that the high-speed data collecting system have reached an expectant function.	analog-to-digital converter;fits;field-programmable gate array;linear system;pixel;random sample consensus;sampling (signal processing);simulation;tracking system;vhdl	Yasemin Timar;Fatih Alagöz	2010	2010 International Conference on Electrical and Control Engineering	10.1109/CICSyN.2010.60	sampling;computer vision;ransac;simulation;image processing;ground truth;feature extraction;computer science;geometric modeling;machine learning;computational model;pixel	EDA	44.076478590841155	-43.27050871617629	77581
97e95bd84be85428e99dd8957b58afffdd62f7eb	a scan matching method using euclidean invariant signature for global localization and map building	simultaneous localization and mapping robot sensing systems mobile robots shape histograms paper technology impedance matching bayesian methods indexing robustness;robot movil;alignement;path planning mobile robots;euclidean theory;map building;path planning;localization;mobile robots;localizacion;robotics;cartographie;cartografia;localisation;robot mobile;object oriented;scan matching;alineamiento;theorie euclidienne;euclidean invariant feature global localization and map building euclidean invariant signature scan matching method mobile robot localization geometric hashing scheme;robotica;mobile robot localization;invariante;cartography;oriente objet;robotique;orientado objeto;alignment;invariant;moving robot;invariant feature;teoria euclidiana	This work presents a new scan matching method for mobile robot localization and mapping. The proposed method is based on the geometric hashing scheme, which utilizes Euclidean invariant features in order to match an input scan with reference scans without an initial alignment The method is applicable to global localization in an environment having curved objects. Experimental results show that a map of a large cyclic environment was built with high accuracy using the proposed method.	geometric hashing;mobile robot;robotic mapping	Masahiro Tomono	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1307258	mobile robot;computer vision;topology;internationalization and localization;computer science;artificial intelligence;invariant;mathematics;geometry;motion planning;robotics;object-oriented programming	Robotics	52.013748443811934	-38.78851002814406	77735
15e7e244bded68313438c3ecf481bc1b325a6fab	space-variant vision for an active camera mount	vision system;moving object;log polar mapping;sensors;real time;geometric distribution;field of view;robotic systems;optical sensors;visual field;vision;measurement technique;cameras;active control	"""Robot systems that rely on vision as their main sensory capability need to be able to cope with changes in the visual environment and to manage a wide field of view. Moreover, in order not to loose real-time response capabilities, selective visual sensing is indeed highly desirable. The """"built-in"""" selection in space and time provided by space variant sensors acts as a filter on the visual field having considerable implications for robotic applications. This paper focuses the attention on log-polar vision in the context of active control of visual sensors. The geometnc distribution of sensing elements in the log-polar mapping provides visual task simplification and computational advantages. Correlation measurement techniques in the log-polar framework are formalized and two different uses are proposed. By performing global measurements on convergent log-polar images, a binocular mount can drive its cameras towards correct vergence configuration. It is also shown how image shifts can be detected by using one-dimensional correlation measurements in the log-polar domain, and a possible use of this technique aimed at stabilizing gaze or tracking moving objects is presented. Both the reduced algorithm complexity, due to space variant topology, and the computational advantages, due to the limited number of pixels, make log-polar mapping a good candidate image geometry to obtain real-time responses in the context of reactive vision systems."""	algorithm;binocular vision;computation;level of detail;motion estimation;pixel;real-time clock;real-time computing;robot;sensor;vergence	Francesco Panerai;Carla Capurro;Giulio Sandini	1995		10.1117/12.211981	vision;computer vision;geometric distribution;simulation;machine vision;field of view;sensor;optics;physics;computer graphics (images)	Robotics	51.98606772219565	-40.26740494381213	77749
105596c4311d1a788bb76a3d3d6e204acd340b89	automatic detection of pole-like structures in 3d urban environments	three dimensional displays roads feature extraction support vector machines semantics vehicles clustering algorithms;town and country planning feature extraction geophysical image processing image classification image sensors object detection pattern clustering;local stage automatic man made pole like structure detection 3d urban environments 3d sensor moving vehicle autonomous navigation facility damage detection city planning maintenance occlusions clutter noise local feature classification local feature clustering global stage	This work aims at automatic detection of man-made pole-like structures in scans of urban environments acquired by a 3D sensor mounted on top a moving vehicle. Pole-like structures, such as e.g. road signs and streetlights, are widespread in these environments, and their reliable detection is relevant to applications dealing with autonomous navigation, facility damage detection, city planning and maintenance. Yet, due to the characteristic thin shape, detection of man-made pole-like structures is significantly prone to both noise as well as occlusions and clutter, the latter being pervasive nuisances when scanning urban environments. Our approach is based on a “local” stage, whereby local features are classified and clustered together, followed by a “global” stage aimed at further classification of candidate entities. The proposed pipeline turns out effective in experiments on a standard publicly available dataset as well as on a challenging dataset acquired during the project for validation purposes.	algorithm;autonomous robot;cluster analysis;clutter;entity;experiment;markov random field;pervasive informatics;software deployment	Federico Tombari;Nicola Fioraio;Tommaso Cavallari;Samuele Salti;Alioscia Petrelli;Luigi di Stefano	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6943262	computer vision;feature detection;feature extraction;machine learning;pattern recognition;feature	Robotics	41.40048542778029	-46.1121719696296	77812
b0dfd52520abe882b1126c740f0fd8f65454d9b3	feature matching with bounded distortion	image matching;bounded distortion;feature correspondence	We consider the problem of finding a geometrically consistent set of point matches between two images. We assume that local descriptors have provided a set of candidate matches, which may include many outliers. We then seek the largest subset of these correspondences that can be aligned perfectly using a nonrigid deformation that exerts a bounded distortion. We formulate this as a constrained optimization problem and solve it using a constrained, iterative reweighted least-squares algorithm. In each iteration of this algorithm we solve a convex quadratic program obtaining a globally optimal match over a subset of the bounded distortion transformations. We further prove that a sequence of such iterations converges monotonically to a critical point of our objective function. We show experimentally that this algorithm produces excellent results on a number of test sets, in comparison to several state-of-the-art approaches.	algorithm;constrained optimization;constraint (mathematics);critical point (network science);distortion;experiment;iteration;least squares;mathematical optimization;maxima and minima;optimization problem;quadratic programming;test set	Yaron Lipman;Stav Yagev;Roi Poranne;David W. Jacobs;Ronen Basri	2014	ACM Trans. Graph.	10.1145/2602142	mathematical optimization;combinatorics;discrete mathematics;machine learning;mathematics;geometry;bounded function	Graphics	50.00619685601396	-51.44027209466626	78132
ed34c0d65fbf6e8a75ace7bd3d314cf1bf9eba27	a novel recursive bayesian learning-based method for the efficient and accurate segmentation of video with dynamic background	bayesian methods;computational modeling bayesian methods classification algorithms algorithm design and analysis mathematical model motion segmentation hidden markov models;video segmentation;dynamic background;motion segmentation;gaussian mixture model;computational modeling;hidden markov models;期刊论文;classification algorithms;recursive bayesian learning;mathematical model;algorithm design and analysis;video segmentation dynamic background gaussian mixture model recursive bayesian learning	Segmentation of video with dynamic background is an important research topic in image analysis and computer vision domains. In this paper, we present a novel recursive Bayesian learning-based method for the efficient and accurate segmentation of video with dynamic background. In the algorithm, each frame pixel is represented as the layered normal distributions which correspond to different background contents in the scene. The layers are associated with a confident term and only the layers satisfy the given confidence which will be updated via the recursive Bayesian estimation. This makes learning of background motion trajectories more accurate and efficient. To improve the segmentation quality, the coarse foreground is obtained via simple background subtraction first. Then, a local texture correlation operator is introduced to fill the vacancies and remove the fractional false foreground regions. Extensive experiments on a variety of public video datasets and comparisons with some classical and recent algorithms are used to demonstrate its improvements in both segmentation accuracy and efficiency.	algorithm;algorithmic efficiency;appendix;background subtraction;binary prefix;carrier-to-noise ratio;computer vision;decision theory;exptime;experiment;frame (physical object);gm(m);google map maker;image analysis;mbnl1 gene;mathematical morphology;multimodal interaction;pixel;population parameter;probability density;recursion (computer science);schisandra chinensis;the matrix;video post-processing;anatomical layer;biologic segmentation;contents - htmllinktype	Qingsong Zhu;Zhan Song;Yaoqin Xie;Lei Wang	2012	IEEE Transactions on Image Processing	10.1109/TIP.2012.2199504	statistical classification;algorithm design;computer vision;recursive bayesian estimation;variable-order bayesian network;bayesian probability;computer science;machine learning;segmentation-based object categorization;pattern recognition;mixture model;mathematical model;image segmentation;scale-space segmentation;computational model;hidden markov model;statistics	Vision	44.80036954756843	-50.84548789778078	78269
60821d447e5b8a96dd9294a0514911e1141ff620	real-time facial expression recognition with illumination-corrected image sequences	informatica;real time user independent computer vision system;face recognition image recognition image sequences lighting face detection deformable models computer vision humans data mining prototypes;facial expression recognition;front facing human face;real time systems face recognition image representation image sequences;manifolds;user independent appearance based model;real time;illumination corrected image sequences;data mining;face tracking;computer vision;face recognition;real time facial expression recognition;image representation;image sequence;face;lighting;facial expression;facial motion real time facial expression recognition illumination corrected image sequences real time user independent computer vision system front facing human face face tracking appearance based face tracker user independent appearance based model facial expression classification face image representation;appearance based face tracker;facial motion;facial expression classification;real time systems;image sequences;face image representation	We present a real-time user-independent computer vision system that processes a sequence of images of a front-facing human face and recognises a set of facial expressions at 30 fps. We track the face using an efficient appearance-based face tracker. We model changes in illumination with a user-independent appearance-based model. In our approach to facial expression classification, the image of a face is represented by a low dimensional vector that results from projecting the illumination corrected image onto a low dimensional expression manifold. In the experiments conducted we show that the system is able to recognise facial expressions in image sequences with large facial motion and illumination changes.	bittorrent tracker;computer vision;database;experiment;holism;real-time clock;real-time computing;real-time locating system;real-time transcription	He Li;José Miguel Buenaposada;Luis Baumela	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813328	facial recognition system;face;computer vision;facial motion capture;speech recognition;manifold;computer science;three-dimensional face recognition;lighting;facial expression;face hallucination;computer graphics (images)	Vision	39.856119870093266	-49.97783639762972	78448
160324b9fc8b64b39e5ba2b7b3319f1a8eb5e1d8	tumindoor: an extensive image and point cloud dataset for visual indoor localization and mapping	video retrieval content based retrieval image sequences pose estimation;video retrieval;point cloud indoor localization dataset location retrieval mapping content based image retrieval;6 dof pose estimation tumindoor point cloud dataset extensive image dataset visual indoor localization visual indoor mapping content based image retrieval cbir large image databases video sequences visual location recognition problem indoor dataset high resolution indoor dataset realistic query sequences;visualization cameras image retrieval image resolution google indoor environments;content based retrieval;image sequences;pose estimation	Recent advances in the field of content-based image retrieval (CBIR) have made it possible to quickly search large image databases using photographs or video sequences as a query. With appropriately tagged images of places, this technique can be applied to the problem of visual location recognition. While this task has attracted large interest in the community, most existing approaches focus on outdoor environments only. This is mainly due to the fact that the generation of an indoor dataset is elaborate and complex. In order to allow researchers to advance their approaches towards the challenging field of CBIR-based indoor localization and to facilitate an objective comparison of different algorithms, we provide an extensive, high resolution indoor dataset. The free for use dataset includes realistic query sequences with ground truth as well as point cloud data, enabling a localization system to perform 6-DOF pose estimation.	3d pose estimation;algorithm;content-based image retrieval;database;ground truth;image resolution;norm (social);point cloud	Robert Huitl;Georg Schroth;Sebastian Hilsenbeck;Florian Schweiger;Eckehard G. Steinbach	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467224	computer vision;visual word;pose;computer science;multimedia	Robotics	50.81774800165464	-44.11247008593717	78645
1365d179232048c2d80f6f7f10330edb069015f0	real-time depth enhanced monocular odometry	visual odometry methods monocular odometry depth information rgb d cameras visual images camera motion triangulation salient visual features motion estimates batch optimization 3d lidar kitti odometry sensing modality;cameras visualization three dimensional displays laser radar sensors transforms tracking;radar imaging cameras image colour analysis motion estimation optical radar optimisation	Visual odometry can be augmented by depth information such as provided by RGB-D cameras, or from lidars associated with cameras. However, such depth information can be limited by the sensors, leaving large areas in the visual images where depth is unavailable. Here, we propose a method to utilize the depth, even if sparsely available, in recovery of camera motion. In addition, the method utilizes depth by triangulation from the previously estimated motion, and salient visual features for which depth is unavailable. The core of our method is a bundle adjustment that refines the motion estimates in parallel by processing a sequence of images, in a batch optimization. We have evaluated our method in three sensor setups, one using an RGB-D camera, and two using combinations of a camera and a 3D lidar. Our method is rated #2 on the KITTI odometry benchmark irrespective of sensing modality, and is rated #1 among visual odometry methods.	benchmark (computing);bundle adjustment;convergence insufficiency;depth map;feature detection (computer vision);feature detection (web development);harris affine region detector;mathematical optimization;modality (human–computer interaction);newton's method;real-time clock;sensor;stereo cameras;triangulation (geometry);visual odometry	Ji Zhang;Michael Kaess;Sanjiv Singh	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6943269	computer vision;visual odometry;optics;remote sensing	Robotics	52.36440020836489	-45.706258417695814	78685
e7b366a6ef3a42b0146f8f582e160fad435de1fb	multiple faces tracking based on relevance vector machine		A multiple faces tracking system was presented based on Relevance Vector Machine (RVM) and Boosting learning. At the first frame, a face detector based on AdaBoost is used to detect faces, and the face motion models and face color models are created. The face motion model consists of a set of RVMs that learn the relationship between the motion of the face and its appearance in the image, and the face color model is the 2D histogram of the face region in CrCb color space. In the tracking process, different tracking methods are used according to different states of the faces and the states are changed according to the tracking results. When the full image search condition is satisfied, a full image search is started in order to find new coming faces and former occluded faces.	relevance vector machine	Wenxing Li;Liming Lian	2012	JSW		computer vision;face detection;machine learning;pattern recognition	ML	43.140081178348375	-48.497856466141776	78770
5493ecae1dfae1fa9f69d3567f9725cd8e27149a	augmented reality on robot navigation using non-central catadioptric cameras	mirrors;spherical mirror augmented reality robot navigation noncentral catadioptric camera system mobile robot noncentral catadioptric imaging device textured objects robot localization;cameras three dimensional displays robot vision systems mirrors augmented reality;three dimensional displays;augmented reality;robot vision systems;cameras;path planning augmented reality cameras control engineering computing mobile robots	In this paper we present a framework for the application of augmented reality to a mobile robot, using non-central camera systems. Considering a virtual object in the world with known local 3D coordinates, the goal is to project this object into the image of a non-central catadioptric imaging device. We propose a solution to this problem which allows us to project textured objects to the image in real-time (up to 20 fps): projection of 3D segments to the image; occlusions; and illumination. In addition, since we are considering that the imaging device is on a mobile robot, one needs to take into account the real-time localization of the robot. To the best of our knowledge this is the first time that this problem is addressed (all state-of-the-art methods are derived for central camera systems). To evaluate the proposed framework we test the solution using a mobile robot and a non-central catadioptric camera (using a spherical mirror).	augmented reality;computation;dbpedia;distortion;image plane;mobile robot;real-time computing;real-time locating system;real-time transcription;robotic mapping;time complexity	Tiago Dias;Pedro Miraldo;Nuno Gonçalves;Pedro U. Lima	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7354080	mobile robot;computer vision;augmented reality;simulation;computer science;mobile robot navigation;computer graphics (images)	Robotics	53.54600645615487	-45.62484437399281	78864
c468e843598f6578e3b7fb9f0b61badfce0b941f	a robotized data collection approach for convolutional neural networks		Convolutional Neural Networks are powerful tools in object classification which are widely used in Robot Vision. One of the basic requirements of this approach is the demand for a massive data set. However, in many scenarios, it is either economically expensive or difficult (impossible) to collect many valid data with few samples. To this end, in this paper we proposes an automatic approach to collecting data for food industry. First, a robotized data collection system is introduced which uses an industry robot with 6 Degree of Freedoms (DOF). Second, we analysis the key parameters of the proposed system in order to improve the quality of the training model. Finally, the effectiveness of our approach is demonstrated on real experimental platform.	convolutional neural network;neural networks	Yiming Liu;Shaohua Zhang;Xiaohui Xiao;Miao Li	2017		10.1007/978-3-319-65298-6_43	data mining;data collection;convolutional neural network;engineering;robot;data collection system	NLP	45.88443635261984	-38.83334205413994	78914
2873f1f86aa7099ebc813725e09c723b6ff2f02f	maneuvering head motion tracking by coarse-to-fine particle filter	color cue;coarse to fine;motion tracking;particle filter	Tracking a very actively maneuvering object is challenging due to the lack of state transition dynamics to describe the system's evolution. In this paper, a coarse-to-fine particle filter algorithm is proposed for such tracking, whereby one loop of the traditional particle filtering approach is divided into two stages. In the coarse stage, the particles adopt a uniform distribution which is parameterized by the limited motion range within each time step. In the following fine stage, the particles are resampled using the results of the coarse stage as the proposal distribution, which incorporates the most present observation. The weighting scheme is implemented using a partitioned color cue that implicitly embeds geometric information to enhance robustness. The system is tested by a publicly available dataset for tracking an intentionally erratic moving human head. The results demonstrate that the proposed system is capable of handling random motion dynamics with a relatively small number of particles.	particle filter	Yun-Qian Miao;Paul W. Fieguth;Mohamed S. Kamel	2011		10.1007/978-3-642-21593-3_39	computer vision;simulation;particle filter;computer science;control theory;mathematics	Robotics	45.38020361941531	-47.80623605331569	78955
9257bd70fc471409556d90938fa9ae1b7f673369	robust view-based visual tracking with detection of occlusions	robustness target tracking humans infrared detectors pixel parametric statistics robot sensing systems robot vision systems stereo vision indium tin oxide;image motion analysis;image motion analysis robot vision tracking ccd image sensors infrared imaging;ccd image sensors;robot vision;3d environment;infrared imaging;affine transformation;infrared images robust view based visual tracking occlusions detection 3d environment affine transformed templates cooperative handling correlation errors tessellated template;visual tracking;tracking	Visual tracking plays an important role in various robotic tasks. We have developed a view-based v isual tracking system to cope with change of t h e appearance o f the template in a 3D environment using ane transformed templates. In view-based visual tracking, however, there a r e s till critical situations in which the tracking fails because of occlusion by general objects and/or human hand in the environment, particularly in cases where h umans and robots perform cooperative handling of objects. This paper presents two methods to cope with such occlusion problems. The rst method detects the occlusion based o n a n e valuation of correlation errors in a tessellated t emplate. The second method uses infrared i m a ges to detect the oc-cluded r egion by human hand in the target template. The system then creates a mask of the occluded r e-gion and eliminates pixels in the mask from the calculation of correlation with the template image. In the prototype s ystem, the generation of the mask and the correlation can be p erformed, typically, in one TV frame. We have integrated t h e proposed methods into the view-based visual tracking system which we have already developed. Experimental results demonstrate the usefulness of the proposed m e t h o ds.	hidden surface determination;pixel;prototype;robot;sensor;tracking system;value (ethics);video tracking	Ken Ito;Shigeyuki Sakane	2001		10.1109/ROBOT.2001.932775	computer vision;simulation;tracking system;eye tracking;affine transformation;tracking;computer graphics (images)	Robotics	50.54383927316574	-40.89678328001863	79094
9b4af130b434c2ea18835b9e1ecffab54ed3dcea	face tracking in color video sequences	kanade lucas tomasi;real time;face tracking;skin color;skin color model;face detection;klt tracker;ellipse fitting	This paper presents a new extension of the KLT (Kanade--Lucas--Tomasi) face tracker, which is robust against loss of tracked points. Stochastic skin-color model is used for face segmentation in the frames contained in a video sequence. Ellipse-fitting is used for selection of the face candidates because of the elliptical face shape. The proposed method achieved real-time performance. The results seem very promising.	real-time clock;real-time computing;tomasi–kanade factorization	Peter Gejgus;Martin Sperka	2003		10.1145/984952.984992	computer vision;face detection;facial motion capture;speech recognition;computer science;computer graphics (images)	Vision	41.677363975729435	-48.838887280649985	79144
22ee43dbd2bdefbc8945d453c6cd453f49ab5eb7	urban traffic surveillance in smart cities using radar images		The Smart City concept arises from the need to provide more intelligent and optimized applications for the development of future urban centers. Traffic monitoring including surveillance is becoming a problem as cities are getting larger and crowded with vehicles. Intelligent video applications for outdoor scenarios need for good quality, stable and robust signal in every moment or climate condition. In this paper we present a radar signal surveillance application that works in real-time, in 360 degrees, with long range up to 400 meters away from the detector, with daylight or night, or even with adverse climatology like fog presence, detecting and tracking high speed vehicles in urban areas.	daylight;radar;real-time clock;sensor;smart city	Jesús Sánchez-Oro;David Fernández-López;Raúl Cabido;Antonio S. Montemayor;Juan José Pantrigo	2013		10.1007/978-3-642-38622-0_31	computer science;computer vision;daylight;smart city;artificial intelligence;radar imaging;radar;eye tracking;detector	Robotics	42.20237840701728	-41.82301051477428	79221
6fdd75e5e1d49b171276e97927a554e0cf87d0dd	vanishing point detection with convolutional neural networks		In a graphical perspective, a vanishing point (VP) is a 2D point (in the image plane) which is the intersection of parallel lines in the 3D world (but not parallel to the image plane). In other words, the vanishing point is the spot to which the receding parallel lines diminish. In principle, there can be more than one vanishing point in the image. VP can commonly be seen in fields, railroads, streets, tunnels, forest, buildings, objects such as ladder (from looking bottom-up), etc. It is an important visual cue useful in several applications (e.g., camera calibration, 3D reconstruction, autonomous driving). Inspired by the finding that vanishing point (road tangent) guides driver’s gaze [1, 2], in our previous work we showed that vanishing point attracts gaze during free viewing of natural scenes as well as in visual search [3]. We have also introduced improved saliency models using vanishing point detectors [4]. Here, we aim to predict vanishing points in naturalistic environments by training convolutional neural networks in an end-to-end manner. Traditionally, geometrical and structural features such as lines and corners (e.g., using Hough transform [5]) have been applied for detecting vanishing points in images. Here, we follow a data-driven learning approach by training two popular convolutional neural networks, Alexnet and VGG, for: 1) predicting whether a vanishing point exists in a scene (on a n × n grid map), and 2) If so, we then attempt to localize its exact location.	3d reconstruction;artificial neural network;autonomous car;bottom-up parsing;camera resectioning;convolutional neural network;end-to-end principle;hough transform;image plane;sensor;top-down and bottom-up design;vanishing point	Ali Borji	2016	CoRR		computer vision;simulation;artificial intelligence	Vision	42.302024948378886	-44.25826141588466	79579
29e451813a2c90f1e50b3e5133b60182015eeef8	stochastically optimal epipole estimation in omnidirectional images with geometric algebra	estimation method;omnidirectional image;epipolar geometry;stochastic optimization;rigid body motion;least square;geometric algebra	We consider the epipolar geometry between two omnidirectional images acquired from a single viewpoint catadioptric vision sensor with parabolic mirror. This work in particular deals with the estimation of the respective epipoles. We use conformal geometric algebra to show the existence of a 3×3 essential matrix, which describes the underlying epipolar geometry. The essential matrix is preferable to the 4×4 fundamental matrix, which comprises the fixed intrinsic parameters, as it can be estimated from less data. We use the essential matrix to obtain a prior for a stochastic epipole computation being a key aspect of our work. The computation uses the well-tried amalgamation of a least-squares adjustment (LSA) technique, called the Gauss-Helmert method, with conformal geometric algebra. The imaging geometry enables us to assign distinct uncertainties to the image points which justifies the considerable advantage of our LSA method over standard estimation methods. Next to the stochastically optimal position of the epipoles, the method computes the rigid body motion between the two camera positions. In addition, our text demonstrates the effortlessness and elegance with which such problems integrate into the framework of geometric algebra.	camera resectioning;computation;conformal geometric algebra;entity;epipolar geometry;essential matrix;fundamental matrix (computer vision);least squares;lecture notes in computer science;motion estimation;parabolic antenna;real-time locating system;springer (tank);uncertain data;whole earth 'lectronic link	Christian Gebken;Gerald Sommer	2008		10.1007/978-3-540-78157-8_7	mathematical optimization;topology;mathematics;geometry;conformal geometric algebra;fundamental matrix;epipolar geometry	Vision	52.97696834244189	-49.422707617682484	79655
4ae2326309a2c79f61839c8e703183c6924fef2e	stellar - a case-study on systematically embedding a traffic light recognition	traffic engineering computing field programmable gate arrays high definition video object recognition pedestrians power aware computing road traffic;case studies;real time information;traffic signals;hardware in the loop simulation;embedded tlr solution stellar systematically embedding a traffic light recognition tlr low cost fpga device systematic approach fpga board hardware in the loop hil power consumption real time behavior high definition video data automotive constraints;detection and identification systems;high risk locations;image color analysis automotive engineering hardware software cameras software algorithms robustness	In this paper we present an embedded implementation of a Traffic Light Recognition (TLR) on a low-cost FPGA device with low memory usage.We follow a systematic approach where we thoroughly investigate computational hot-spots, and systematically partition the system into hardware and software components which we both optimize. Our implementation is evaluated using an actual FPGA board as Hardware-in-the-Loop (HIL). In contrast to other approaches, we are not restricted to filled lights but also detect other types such as arrows, pedestrians or bicycle ones when provided with training data. With an average performance of 45 fps and minimum 12 fps with ~ 5 Watts of power consumption, our system shows real-time behavior even on high-definition video data with high comparable recognition rates while still obeying automotive constraints such as low power. As far as we know, we are the first ones presenting an embedded TLR solution.	best, worst and average case;component-based software engineering;computation;embedded system;field-programmable gate array;hdmi;hardware-in-the-loop simulation;obedience (human behavior);real-time clock;watts humphrey	Jan Micha Borrmann;Frederik Haxel;Dennis Nienhüser;Alexander Viehl;Johann Marius Zöllner;Oliver Bringmann;Wolfgang Rosenstiel	2014	17th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2014.6957860	embedded system;real-time computing;simulation;engineering	Embedded	42.26689536477219	-40.42324417050025	79728
fa93e027e795e98405dc72f123aadebafc5d80d8	explaining the ambiguity of object detection and 6d pose from visual data		3D object detection and pose estimation from a single image are two inherently ambiguous problems. Oftentimes, objects appear similar from different viewpoints due to shape symmetries, occlusion and repetitive textures. This ambiguity in both detection and pose estimation means that an object instance can be perfectly described by several different poses and even classes. In this work we propose to explicitly deal with this uncertainty. For each object instance we predict multiple pose and class outcomes to estimate the specific pose distribution generated by symmetries and repetitive textures. The distribution collapses to a single outcome when the visual appearance uniquely identifies just one valid pose. We show the benefits of our approach which provides not only a better explanation for pose ambiguity, but also a higher accuracy in terms of pose estimation.	3d pose estimation;autostereogram;class;object detection;physical object;pose (computer vision);published comment;benefit	Fabian Manhardt;Diego Martin Arroyo;Christian Rupprecht;Benjamin Busam;Nassir Navab;Federico Tombari	2018	CoRR			Vision	50.37887685395724	-47.63117108454952	79773
7ad4d49071f0567f9bf6f4beced56b656979f5c7	solution to the slam problem in low dynamic environments using a pose graph and an rgb-d sensor	simultaneous localization and mapping slam;rgb d red green blue depth;pose graph;low dynamic environment;robotics;environment;artificial intelligence;algorithms;movement	In this study, we propose a solution to the simultaneous localization and mapping (SLAM) problem in low dynamic environments by using a pose graph and an RGB-D (red-green-blue depth) sensor. The low dynamic environments refer to situations in which the positions of objects change over long intervals. Therefore, in the low dynamic environments, robots have difficulty recognizing the repositioning of objects unlike in highly dynamic environments in which relatively fast-moving objects can be detected using a variety of moving object detection algorithms. The changes in the environments then cause groups of false loop closing when the same moved objects are observed for a while, which means that conventional SLAM algorithms produce incorrect results. To address this problem, we propose a novel SLAM method that handles low dynamic environments. The proposed method uses a pose graph structure and an RGB-D sensor. First, to prune the falsely grouped constraints efficiently, nodes of the graph, that represent robot poses, are grouped according to the grouping rules with noise covariances. Next, false constraints of the pose graph are pruned according to an error metric based on the grouped nodes. The pose graph structure is reoptimized after eliminating the false information, and the corrected localization and mapping results are obtained. The performance of the method was validated in real experiments using a mobile robot system.	algorithm;anatomic node;closing (morphology);contain (action);experiment;graph - visual representation;map;mobile robot;movement;newton's method;object detection;physical object;prunes;repositioning (procedure);rule (guideline);slamf1 gene;sensor;simultaneous localization and mapping;while;algorithm	Donghwa Lee;Hyun Myung	2014		10.3390/s140712467	movement;computer vision;simulation;computer science;machine learning;natural environment;robotics	Robotics	52.98200960788851	-39.63589360975801	79809
a05d556db962cd78cc912c774216e423ccf40b6c	light field from smartphone-based dual video		In this work, we introduce a light field acquisition approach for standard smartphones. The smartphone is manually translated along a horizontal rail, while recording synchronized video with front and rear camera. The front camera captures a control pattern, mounted parallel to the direction of translation to determine the smartphones current position. This information is used during a postprocessing step to identify an equally spaced subset of recorded frames from the rear camera, which captures the actual scene. From this data we assemble a light field representation of the scene. For subsequent disparity estimation, we apply a structure tensor approach in the epipolar plane images. We evaluate our method by comparing the light fields resulting from manual translation of the smartphone against those recorded with a constantly moving translation stage.	binocular disparity;epipolar geometry;light field;linear stage;smartphone;structure tensor	Bernd Krolla;Maximilian Diebold;Didier Stricker	2014		10.1007/978-3-319-16181-5_46	artificial intelligence;computer science;computer vision;epipolar geometry;light field;structure tensor;video processing	Vision	49.17310936989967	-45.530668185560536	79929
b81970e576dbfeded78369ad2b257856db2baf4f	view extension for teleoperated mav	databases;weighted least squares microaerial vehicles teleoperated mav view extension field of view fov panorama frame camera frame fusing fast tracking feature descriptor matching stitching algorithm;google;cameras databases google accuracy visualization image edge detection machine vision;least squares approximations autonomous aerial vehicles cameras image matching;accuracy;visualization;image edge detection;machine vision;cameras	In this paper, we propose a method to extend the field of view (FoV) of cameras mounted on Micro Aerial Vehicles (MAVs). The idea is to stitch together appropriate sections of the panorama to the camera frame. The proposed system efficiently performs view extension by fusing fast tracking and feature descriptor matching into the stitching algorithm. The quality of the extended view is further improved by refining the transformation using weighted least squares. We demonstrate the success of our system in real cases where the distance from one panorama to the next panorama is 10m.	aerial photography;algorithm;control theory;field of view in video games;image stitching;least squares;transformation matrix;visual descriptor	Ren Foo Lim;Akihiko Torii;Masatoshi Okutomi	2015	2015 14th IAPR International Conference on Machine Vision Applications (MVA)	10.1109/MVA.2015.7153247	computer vision;simulation;visualization;machine vision;image stitching;computer science;accuracy and precision;statistics;computer graphics (images)	Robotics	51.33916252939258	-43.81218625068089	79988
ffb9c29ebe2a8dba1fc551ab54abc00c1b4f208a	track multiple objects with feature-correlation algorithms	feature matching;geometric feature;objective correlation;image sequence	We propose a method to compute objective contour feature. Conventional objective chain algorithms correlate the motion with area characteristics of the objectives to reach the goals. Nevertheless, they may not be efficient under certain circumstances of complicated tracking. In this paper, we incorporate the contour characteristic of the objective into conventional methods and thus construct a function to estimate the similarity of the objective. Experiments show that more reliable results are obtained when rigid objects are tracked.	algorithm;experiment	Xuan Nie;Kai Hu;Manhua Qi;Dengshan Huang;Bei Cao;Xiaodan Yuan	2017		10.1145/3055635.3056646	computer vision;mathematical optimization;pattern recognition;mathematics	DB	47.83736219818483	-48.908475641554055	80008
6172a45c2630ad7582b33304e942e4973757ec15	visual odometry [tutorial]	visual odometry;scene overlap visual odometry egomotion estimation single camera multiple cameras robotics wearable computing augmented reality automotive wheel odometry vehicle wheels pose estimation onboard cameras illumination image texture;apparent motion;motion estimation;image texture;wheels cameras image texture motion estimation pose estimation road vehicles robots;robots;wearable computer;augmented reality;cameras;wheels;road vehicles;pose estimation	Visual odometry (VO) is the process of estimating the egomotion of an agent (e.g., vehicle, human, and robot) using only the input of a single or If multiple cameras attached to it. Application domains include robotics, wearable computing, augmented reality, and automotive. The term VO was coined in 2004 by Nister in his landmark paper. The term was chosen for its similarity to wheel odometry, which incrementally estimates the motion of a vehicle by integrating the number of turns of its wheels over time. Likewise, VO operates by incrementally estimating the pose of the vehicle through examination of the changes that motion induces on the images of its onboard cameras. For VO to work effectively, there should be sufficient illumination in the environment and a static scene with enough texture to allow apparent motion to be extracted. Furthermore, consecutive frames should be captured by ensuring that they have sufficient scene overlap.	augmented reality;bundle adjustment;computation;download;propagation of uncertainty;robotics;software propagation;visual odometry;wearable computer;wheels	Davide Scaramuzza;Friedrich Fraundorfer	2011	IEEE Robotics & Automation Magazine	10.1109/MRA.2011.943233	robot;image texture;computer vision;augmented reality;simulation;pose;wearable computer;computer science;visual odometry;motion estimation;odometry;computer graphics (images)	Robotics	51.92028552128879	-44.134139654894	80045
a13398e8df63715d4442ed4d458a6948bd114dd7	dynamic stereoscopic selective visual attention (dssva): integrating motion and shape with depth in video segmentation	video sequences;video segmentation;stereovision;software reusability;problem solving method;visual attention;motion detection	Depth inclusion as an important parameter for dynamic selective visual attention is presented in this article. The model introduced in this paper is based on two previously developed models, dynamic selective visual attention and visual stereoscopy, giving rise to the socalled dynamic stereoscopic selective visual attention method. The three models are based on the accumulative computation problemsolving method. This paper shows how software reusability enables enhancing results in vision research (video segmentation) by integrating earlier works. In this article, the first results obtained for synthetic sequences are included to show the effectiveness of the integration of motion and shape features with depth parameter in video segmentation. 2007 Elsevier Ltd. All rights reserved.	ball project;computation;olap cube;problem solving;stereoscopy;synthetic intelligence	Antonio Fernández-Caballero;María T. López;Sergio Saiz-Valverde	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.01.007	computer vision;computer science;stereopsis;multimedia;computer graphics (images)	Vision	50.383893464228876	-43.44232433212004	80459
74b0c7c075788be7a63de30b2657a05bb9c93f6d	image processing for automatic reading of electro-mechanical utility meters	databases;computers;smart phones image segmentation meters;image enhancement image segmentation;meters;image segmentation;smart phones;image segmentation feature extraction cameras computers databases patents;image enhancement;patents;feature extraction;data analysis automatic reading electro mechanical utility meters utilities consumption analog meters rotary dials pointer dials automated meter reading image segmentation smart phone image processing functions energy consumption monitor energy saving;cameras	Electro-mechanical meters are commonly employed to measure the consumption of utilities. Basically there exist two types of analog meters: the ones that use rotary dials (like an odometer) and the ones with pointer dials (like a speedometer). Former approaches to automated meter reading have dealt with the first kind of meters. Considering that automated reading of the latter ones can be confusing, in this work we introduce a methodology based on image processing and segmentation to enable the image acquisition and processing of pointer dials to obtain efficiently and accurately readings. This methodology uses an image acquired with a smart phone and by applying a sequence of image processing functions it finds and extracts the dial images of such meter images. Then the methodology identifies the position of the pointers followed by a clever implementation that enables the reading. The database is composed with more than a hundred images taken under different light conditions, perspectives and angles. The method is able to extract the reading in an average of 3 seconds, with a 92 % accuracy with images taken in-field. Our method, enables the use of a common smart phone to acquire and automatically extract the reading of a pointer-type dial meter. This allows interesting applications that could help people to monitor their energy consumption and learn patterns to save energy. This could be one step ahead of energy saving policies that can be discovered through massive data analysis.	analog-to-digital converter;campus party;cloud computing;dial-up internet access;existential quantification;harris affine region detector;high-level programming language;image processing;mathematical morphology;pointer (computer programming);randomness extractor;rotary woofer;smartphone;structuring element	Ricardo Ocampo-Vega;Gildardo Sánchez-Ante;Luis Falcón-Morales;Juan Humberto Sossa Azuela	2013	2013 12th Mexican International Conference on Artificial Intelligence	10.1109/MICAI.2013.28	embedded system;computer vision;computer hardware;image processing;feature extraction;computer science;meter;machine learning;image segmentation	Robotics	44.15974622285251	-40.84619438589715	80553
917ea6a20c469b69685b41192c24676dcc92f26b	three-dimensional face orientation and gaze detection from a single image	human computer interaction;three dimensional;system evaluation;user cooperation;pattern recognition;gaze direction;human machine interaction	Gaze detection and head orientation are an important part of many advanced human-machine interaction applications. Many systems have been proposed for gaze detection. Typically, they require some form of user cooperation and calibration. Additionally, they may require multiple cameras and/or restricted head positions. We	human–computer interaction	Jeremy Yermiyahou Kaminski;Mina Teicher;Dotan Knaan;Adi Shavit	2004	CoRR		three-dimensional space;computer vision;simulation;computer science;pattern recognition	Robotics	46.849561605799366	-43.30848012844321	80555
f2b131c622f1ac7ad1de7661495cd1509f12f139	a statistic method of crop acreage based on image recognition	crop proportion;image recognition crop acreage crop proportion;ccd camera;electromotor control;image recognition;estimation theory;obliquity sensor;statistic method;statistical method;gis software;agriculture farming;statistical analysis;statistical analysis crops estimation theory farming government data processing image recognition remote sensing;remote sensing;remote sensing estimation;statistics crops image recognition global positioning system vehicles geographic information systems charge coupled devices charge coupled image sensors navigation frequency;crop acreage;off road vehicle;crops;crop proportion statistic method crop acreage image recognition gps receivers gis software ccd camera off road vehicle vehicle navigation vehicle protract tracking electromotor control obliquity sensor agriculture farming country farming remote sensing estimation;farming;vehicle navigation;country farming;gps receivers;vehicle protract tracking;government data processing	VGGVS is a system that was equipped with GPS receivers, GIS software and CCD camera on off-road vehicle. VGGVS can firstly implement navigation and protract track of vehicle, can secondly collect the image of video and calculate crop proportion by manual or automatic mode based on matching between GPS and video frequency and image recognition, at the same time saved the data of GPS and image, can thirdly fix the sampling width depend on controlling of electromotor through the single of obliquity sensor in order to improve precision of image recognized and statistic result. This based-image statistic method was putted forward aiming at some statistic methods shortage. It can accurately calculate crop acreage in time, which provides credibility data for government to make decision and provides technological method for serving to agriculture and country farmer together with remote sensing estimate	charge-coupled device;computer vision;geographic information system;global positioning system;sampling (signal processing)	Suxia Wang	2006	Sixth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2006.253873	crop;computer vision;simulation;geographic information system;charge-coupled device;estimation theory;statistics	Robotics	43.13199979390089	-41.216729741654994	80680
baeac2e4f037fc31b59d0bff5a76ee599391905c	a lss-based registration of stereo thermal-visible videos of multiple people using belief propagation	dense stereo matching;ir camera;visible camera;local self similarity;belief propagation;multimodal video registration	In this paper, we propose a novel stereo method for registering foreground objects in a pair of thermal and visible videos of close-range scenes. In our stereo matching, we use Local Self Similarity (LSS) as similarity metric between thermal and visible images. In order to accurately assign disparities to depth discontinuities and occluded Region Of Interest (ROI), we have integrated color and motion cues as soft constraints in an energy minimization framework. The optimal disparity map is approximated for image ROIs using a Belief Propagation (BP) algorithm. We tested our registration method on several challenging close-range indoor video frames of multiple people at different depths, with different clothing, and different poses. We show that our global optimization algorithm significantly outperforms the existing state-of-the art method, especially for disparity assignment of occluded people at different depth in close-range surveillance scenes and for relatively large camera baseline.	approximation algorithm;backpropagation;baseline (configuration management);belief propagation;binocular disparity;c++;categorization;closed-circuit television;color;computer stereo vision;desktop computer;energy minimization;global optimization;iteration;lookup table;markov random field;mathematical optimization;multi-core processor;openmp;parallel computing;pattern recognition;pixel;reflections of signals on conducting lines;region of interest;schema (genetic algorithms);self-similarity;software propagation	Atousa Torabi;Guillaume-Alexandre Bilodeau	2013	Computer Vision and Image Understanding	10.1016/j.cviu.2013.01.016	stereo camera;computer vision;simulation;computer science;machine learning;belief propagation;computer graphics (images)	Vision	48.61900672142706	-48.46343265550707	81030
71cfa27f7a36cc935310ccc46210516e919e70fc	robust real-time 3d person detection for indoor and outdoor applications		Fast and robust person detection is one of the most important tasks for robotic applications involving human interaction. Particularly in mobile robotics this task is still challenging. Though there are already reliable and real-time capable approaches, they are usually computationally expensive. They either require GPUs or multiple CPU cores in order to work properly. Furthermore, some of the approaches are designed for special environments and sensor types, which reduces general applicability. In this work, we present a robust, generic and lightweight solution for real-time 3D person detection. Since our approach requires only a single CPU thread, it can be run as a background process and is suitable for smaller robotic systems. We demonstrate applicability to indoor and outdoor scenarios using different 3D sensor types separately. Moreover, we are able to show that the proposed method outperforms other state-of-the-art approaches, including a DCNN.	3d scanner;analysis of algorithms;background process;central processing unit;cluster analysis;computation;graphics processing unit;mobile robot;real-time clock;real-time computing;real-time transcription;robotics;sensor;sparse matrix;voxel;web page	Richard Hanten;Philipp Kuhlmann;Sebastian Otte;Andreas Zell	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8461257	background process;visualization;control engineering;feature extraction;real-time computing;engineering;thread (computing);multi-core processor;artificial intelligence;robotics	Robotics	45.710294852173504	-39.24515754379105	81173
dea11a2b92fe8e1f806a5edcff4838b7e90e1e0b	image processing application in toll collection	toll collection;index terms—image processing;haar wavelet;hough transform.;indexing terms;real time;image processing;hough transform	The toll rate charged for the usage of facilities such as a tunnel or a bridge is usually proportional to the number of axles possessed by a vehicle. However, it is sometimes difficult to determine the number of axles of a vehicle by the toll-booth operator and therefore, an automatic system that can identify the number of axles is sought. Instead of detecting the axle, wheels of a vehicle are tested and a method based on the Hough transform for detecting circles is proposed. As the system must be able to detect the correct number of wheels in real-time, sub-sampling based on the Haar Wavelet transform is applied. Experimental results show that the system is able to identify the wheel correctly and to process the input images in real-time.	haar wavelet;hough transform;image processing;real-time clock;sampling (signal processing);sensor;tunneling protocol;wavelet transform;wheels	Yu-Fai Fung;Homan Lee;Muhammet Fikret Ercan	2006			stationary wavelet transform;image processing;computer science;haar wavelet;computer vision;discrete wavelet transform;speech recognition;hough transform;artificial intelligence	AI	43.69189010038748	-41.31101550690379	81265
833e873a80ed6428b558fe515405aa5dddae8f26	pedestrian association and localization in monocular fir video sequence	model based motion estimation framework;pedestrian height model pedestrian association pedestrian localization monocular fir video sequence frame to frame data association state estimation problems moving vehicle monocular far infra red video sequence model based motion estimation framework image appearance information pedestrian height ratio estimates multiple hypothesis mode height filtering algorithm;frame to frame data association;finite impulse response filter video sequences state estimation filtering algorithms motion estimation cameras vehicles matched filters image matching adaptive filters;pedestrian height ratio estimates;video signal processing;video signal processing filtering theory image sequences motion estimation sensor fusion state estimation traffic engineering computing;image matching;model based approach;motion estimation;pedestrian association;state estimation;infra red;data association;multiple hypothesis mode;ratio estimator;computational modeling;image appearance information;estimation;three dimensional displays;pedestrian height model;pedestrian localization;traffic engineering computing;moving vehicle;state estimation problems;vehicles;sensor fusion;height filtering algorithm;hierarchical model;filtering theory;cameras;monocular fir video sequence;monocular far infra red video sequence;image sequences	This paper addresses the frame-to-frame data association and state estimation problems in localization of a pedestrian relative to a moving vehicle from a monocular far infra-red video sequence. Using a novel application of the hierarchical model-based motion estimation framework, we are able to use the image appearance information to solve the frame-to-frame data association problem and estimate a sub-pixel accurate height ratio for a pedestrian in two frames. Then, to localize the pedestrian, we propose a novel approach of using the pedestrian height ratio estimates to guide an interacting multiple-hypothesis-mode/height filtering algorithm instead of using a constant pedestrian height model. Experiments on several IR sequences demonstrate that this approach achieves results comparable to those from a known pedestrian height thus avoiding errors from a constant height model based approach.	algorithm;can bus;correspondence problem;data quality;experiment;finite impulse response;hierarchical database model;interaction;motion estimation;pixel	Mayank Bansal;Shunguang Wu;Jayan Eledath	2009	2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2009.5204132	ratio estimator;computer vision;estimation;simulation;infrared;computer science;motion estimation;sensor fusion;computational model;hierarchical database model;statistics;computer graphics (images)	Vision	46.39112333053791	-47.34701060292723	81501
c0512b0da6bee8b5c914f0b0b7a77d1cc61a8b3e	recognition of indoor images employing supporting relation between objects	object recognition;image understanding;supporting relation;scene understanding;ordinary object recognition;image understanding system	In this paper, we describe a new design of a recognition system for a single image of an indoor scene including complex occlusions. In conventional works, the systems could not recognize images of an indoor scene including complex occlusions. Our system can treat them by employing supporting relation between objects. In our system, first, the system estimates the 3D structure of an object by fitting a 3D structure model to the image qualitatively. Next, by checking the supporting relation between objects, it eliminates object candidates that cannot exist and estimates real objects from their parts in the image. Finally, the system recognizes objects that are compatible with each other. We implemented the system as a multi-agent-based image understanding system. In this paper, we describe the design of the system and results of experiments. © 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(11): 14–26, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/scj.10142	agent-based model;autostereogram;computer cluster;computer vision;digi-comp i;digital camera;experiment;image resolution;john d. wiley;multi-agent system	Keiji Yanai;Koichiro Deguchi	2002	Systems and Computers in Japan	10.1002/scj.10142	computer vision;computer science;artificial intelligence;object-oriented design;cognitive neuroscience of visual object recognition	Robotics	46.598583709901504	-46.17195833968084	81588
1cf15bed2cb17b8c0389ee85caddb4efa0bb7e6e	new star pattern identification with vector pattern matching for attitude determination	image matching;stars astronomical image processing image matching;astronomical image processing;vectors catalogs cameras mathematical model correlation classification algorithms equations;reference star patterns star pattern identification algorithm attitude determination vector pattern matching technique celestial sphere robust identification performance captured star images onboard catalog star image positions target score function;stars	We propose a new star pattern identification algorithm using the vector pattern matching technique on the celestial sphere. The proposed algorithm provides more robust identification performance in cases where the captured star images are biased from the onboard catalog star image positions. The technique is based on the approach of maximizing the target score function, which is formed by the correlation between the original and the reference star patterns on the celestial sphere.	algorithm;approximation;celestial coordinate system;new star games;pattern matching;simulation;star trek:;star catalogue	Hyosang Yoon;Sung Wook Paek;Yeerang Lim;Byung-Hoon Lee;Hungu Lee	2013	IEEE Transactions on Aerospace and Electronic Systems	10.1109/TAES.2013.6494402	computer vision;stars;mathematics;geometry;optics	Vision	50.2391729777844	-42.77623737441884	81649
accc46a000d3f93fea9559e513ae1a97a86f25e5	intelligent electronic navigational aids: a new approach	classifier fusion;electronic chart system;pattern clustering;clutter;target tracking aerospace computing computer displays computerised navigation human factors image classification learning artificial intelligence pattern clustering sensor fusion;performance evaluation;image analysis tools;color space;smart clutter management capabilities;classifier fusion system;intelligent electronic navigational aids;electronic displays;image classification;image analysis tools intelligent electronic navigational aids intelligent devices smart clutter management capabilities user situational awareness electronic chart system adaboost electronic displays classifier fusion system;data fusion;situational awareness;multiple views;digital maps;user situational awareness;displays aerospace electronics aircraft navigation military aircraft biometrics laboratories object detection extraterrestrial measurements hazards image color analysis;symposia;human factors;aerospace computing;display systems;computer displays;intelligent devices;adaboost;situation awareness;pattern recognition;algorithms;visual perception;sensor fusion;learning artificial intelligence;target tracking;target detection;computerised navigation	Intelligent devices, with smart clutter management capabilities, can enhance a user's situational awareness under adverse conditions. Two approaches to assist a user with target detection and clutter analysis are presented, and suggestions on how these tools could be integrated with an electronic chart system are further detailed. The first tool, which can assist a user in finding a target partially obscured by display clutter, is a multiple-view generalization of AdaBoost. The second technique determines a meaningful measure of clutter in electronic displays by clustering features in both geospatial and color space. The clutter metric correlates with preliminary, subjective, clutter ratings. The user can be warned if display clutter is a potential hazard to performance. Synthetic and real data sets are used for performance evaluation of the proposed technique compared with recent classifier fusion strategies	aomedia video 1;adaboost;algorithm;cluster analysis;clutter;color space;display device;generalization error;international conference on machine learning;iteration;kernel method;oracle fusion middleware;performance evaluation;sampling (signal processing);semiconductor industry;semidefinite programming;stacking;synthetic data;test data	Costin Barbu;Maura Lohrenz;Geary Layne	2006	2006 5th International Conference on Machine Learning and Applications (ICMLA'06)	10.1109/ICMLA.2006.30	situation awareness;computer vision;simulation;digital mapping;computer science;human factors and ergonomics;machine learning;data mining;sensor fusion	Robotics	39.96935110447648	-41.22956818108172	81664
356de3f3da77bd54a10f3066012297066c24ec5b	tracking system to automate data collection of microscopic pedestrian traffic flow	automated data collection;tracking system;image processing;data collection;traffic flow;system performance;flow rate;object tracking	To deal with many pedestrian data, automatic data collection is needed. This paper describes how to automate the microscopic pedestrian flow data collection from video files. The study is restricted only to pedestrians without considering vehicular pedestrian interaction. Pedestrian tracking system consists of three sub-systems, which calculates the image processing, object tracking and traffic flow variables. The system receives input of stacks of images and parameters. The first sub-system performs Image Processing analysis while the second sub-system carries out the tracking of pedestrians by matching the features and tracing the pedestrian numbers frame by frame. The last sub-system deals with a NTXY database to calculate the pedestrian traffic-flow characteristic such as flow rate, speed and area module. Comparison with manual data collection method confirmed that the procedures described have significant potential to automate the data collection of both microscopic and macroscopic pedestrian flow variables.	color gradient;diagram;image processing;significant figures;tracking system;video file format;video processing	Kardi Teknomo;Yasushi Takeyama;Hajime Inamura	2003	CoRR		computer vision;simulation;tracking system;image processing;computer science;traffic flow;video tracking;volumetric flow rate;computer security;statistics;data collection	AI	43.764991290712636	-41.484357756796214	81776
35490b021dcdec12882870a31dce9a687205ab5c	learning bayesian networks with qualitative constraints	facial action unit recognition bayesian networks qualitative constraints graphical models computer vision problems geometric constraints model learning process generic qualitative knowledge bn parameter learning maximum likelihood estimation method expectation maximization algorithm;learning process;bayesian network;closed form solution;facial action unit recognition;maximum likelihood;estimation method;bayes methods;qualitative constraints;maximum likelihood estimation;computer vision;upper bound;training data;incomplete data;computational modeling;graphical models;estimation;expectation maximization;expectation maximization algorithm;learning artificial intelligence bayes methods computer vision expectation maximisation algorithm;model learning process;maximum likelihood estimation method;action unit;graphical model;generic qualitative knowledge;bn parameter learning;optimization;sparse data;learning artificial intelligence;geometric constraints;em algorithm;bayesian methods computer vision training data maximum likelihood estimation graphical models computer network reliability solid modeling closed form solution application software gold;computer vision problems;bayesian networks;expectation maximisation algorithm	Graphical models such as Bayesian networks (BNs) are being increasingly applied to various computer vision problems. One bottleneck in using BN is that learning the BN model parameters often requires a large amount of reliable and representative training data, which proves to be difficult to acquire for many computer vision tasks. On the other hand, there is often available qualitative prior knowledge about the model. Such knowledge comes either from domain experts based on their experience or from various physical or geometric constraints that govern the objects we try to model. Unlike the quantitative prior, the qualitative prior is often ignored due to the difficulty of incorporating them into the model learning process. In this paper, we introduce a closed-form solution to systematically combine the limited training data with some generic qualitative knowledge for BN parameter learning. To validate our method, we compare it with the maximum likelihood (ML) estimation method under sparse data and with the expectation maximization (EM) algorithm under incomplete data respectively. To further demonstrate its applications for computer vision, we apply it to learn a BN model for facial action unit (AU) recognition from real image data. The experimental results show that with simple and generic qualitative constraints and using only a small amount of training data, our method can robustly and accurately estimate the BN model parameters.	bayesian network;computer vision;expectation–maximization algorithm;graphical model;sparse matrix;subject-matter expert	Yan Tong;Qiang Ji	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587368	expectation–maximization algorithm;computer science;machine learning;pattern recognition;bayesian network;maximum likelihood;graphical model;statistics	Vision	46.19083158859062	-51.68361202174919	81820
7243d917c81c1fd2c41b30b260c3f8a45ac14514	a game-theoretic design for collaborative tracking in a video camera network	video surveillance;high resolution;game theory;camera handoff;image resolution;surveillance;collaborative tracking;camera handoff game theoretic design collaborative tracking moving target tracking video camera network surveillance border covering camera direction complete coverage failure reduction;border covering camera direction;game theoretic design;video cameras;video surveillance game theory image resolution target tracking video cameras;face;moving target tracking;humans;target tracking;camera network;complete coverage failure reduction;cameras;cameras target tracking surveillance humans face;video camera network surveillance	Tracking a moving target of interest at a high resolution with a dynamically designated network camera while ensuring complete-coverage of the area under surveillance of a video camera network can be a challenging task. Game theory can be applied to the situation, treating individual cameras as players and area coverage as utility. Camera collaboration is needed when one camera handoff the job of tracking a moving target of interest to another. By taking into account coverage shared by two cameras as secondary utility and giving precedence to border-covering camera directions, we can improve the performance of collaborative tracking, reducing complete-coverage failures as well as their associated uncovered blocks between handoffs. The cost of camera adjustment involved in a handoff can also be reduced this way.	game theory;ip camera;image resolution	Zongjie Tu;Prabir Bhattacharya	2011	2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2011.6027379	smart camera;game theory;computer vision;simulation;image resolution;computer science;video tracking;multimedia	Vision	45.84385433976696	-45.36815540337744	81843
b5319c0326e5f8939f975704650f49e5a1d1f847	ordering random object poses	photogrammetry;object recognition;computer aided design;double camera systems;image processing;manifolds;image matching;probability density function;camera selection;random object poses;multiple camera systems;data mining;camera selection objects reconstruction three dimensional reconstruction random object poses pose ordering pose angles principal component analysis automatic ordering multidimensional manifold pose estimation multiple camera systems double camera systems;multi camera image processing principal component analysis photogrammetry pose recognition pose estimation;machine vision;image reconstruction;principal component analysis;multi camera image processing;pose ordering;three dimensional reconstruction;automatic ordering;objects reconstruction;pose recognition;multidimensional manifold;principal component analysis image reconstruction cameras object recognition image recognition layout image matching application software machine vision design automation;cameras;principal component analysis image reconstruction object recognition pose estimation;pose angles;pose estimation	Complete or partial three-dimensional reconstruction of objects from multiple angle-views, or poses, is important in several applications such as photogrammetry, machine vision, and computer-aided design. Knowledge of the pose angles and their proper ordering are required for accurate reconstruction. When these multiple angle images are acquired in random order and the angle of view information is not available the poses have to be put into proper order. This work presents an approach based on principal component analysis (PCA) for automatic ordering of random object poses. A measure based on local curvature and correlation of the estimated pose trajectory in a multidimensional manifold is also developed to assess confidence in the ordering. In addition to providing a degree of confidence for pose ordering with single cameras, this measure enhances the pose estimation accuracy in double and multiple camera systems by providing a basis for camera selection for different poses. The paper presents theoretical development and experimental results.	computer-aided design;machine vision;photogrammetry;principal component analysis	James Massaro;Raghuveer Rao	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959846	iterative reconstruction;computer vision;probability density function;pose;machine vision;manifold;image processing;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;photogrammetry;statistics;principal component analysis	Robotics	51.05728152530666	-51.570894983815286	81997
14b75196862be505a86b805fc7293c1458836ac2	3d reconstruction of indoor scenes by casting visual rays in an occupancy grid	image segmentation;three dimensional displays cameras simultaneous localization and mapping image segmentation feature extraction stereo vision;sensors;occupancy grid;satisfiability;three dimensional;dense stereo matching visual rays occupancy grid range sensors automated 3d reconstruction system simultaneous localization and mapping vision based occupancy grid slam graph cut algorithm;stereo matching;three dimensional displays;graph cut;feature extraction;image reconstruction;indoor environment;field of view;simultaneous localization and mapping;stereo vision;ray tracing;augmented reality;slam robots;3d reconstruction;cameras;slam robots image reconstruction image segmentation ray tracing sensors	Three-dimensional maps are useful for many applications; from the gaming industry, to augmented reality, to the development of tour guides of important landmarks such as museums or university campuses. The generation of such maps is very labour intensive and has therefore justified its automation using robots with range sensors such as lasers or cameras. This paper presents an automated 3D reconstruction system for indoor environments, which relies on a vision-based occupancy-grid SLAM (Simultaneous Localization and Mapping) to detect the ground. The novelty in our work is the method in which 3D information is extracted and fed to SLAM. Initially, coherent sections in the scene are segmented using a graph-cut algorithm, next 3D points extracted via a stereo camera are used to fit planes to each section. The ground plane is then determined based on the orientation of its normal and virtual rays are cast into the field of view from the camera center to the intersection of each ray's 2D projection with the Ground boundaries. Dense depth information can then be suggested from these rays and inputted to SLAM. Walls and ceiling are also built in a heuristic manner by satisfying normality constraints and keeping within the boundaries of Ground. Our system produces high-quality maps and reduces the high computational cost of dense stereo matching by processing only a sparse set of highly reliable salient features. Experiments are conducted inside a lab setting and results prove the success of the system.	3d reconstruction;algorithm;algorithmic efficiency;augmented reality;bend minimization;coherence (physics);computation;computer stereo vision;converge;cut (graph theory);experiment;heuristic;image segmentation;map;pinhole camera model;point cloud;rendering (computer graphics);robot;sensor;simultaneous localization and mapping;sparse language;sparse matrix;stereo camera;vii	Samir Shaker;Daniel C. Asmar;Imad H. Elhajj	2010	2010 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2010.5723495	3d reconstruction;iterative reconstruction;three-dimensional space;ray tracing;computer vision;augmented reality;simulation;cut;field of view;feature extraction;computer science;sensor;stereopsis;image segmentation;satisfiability;computer graphics (images);simultaneous localization and mapping	Robotics	51.612300043278516	-47.18058421902457	82025
413f4df49a815ad3b922133f0da9117f9dfe6eff	a probabilistic model for recovering camera translation	confidence level;probability;posterior probability;probabilistic model;bayesian;wayfinding;optical flow;motion parallax	This paper describes the mathematical basis and application of a probabilistic model for recovering the direction of camera translation (heading) from optical flow. According to the theorem that heading cannot lie between two converging points in a stationary environment, one can compute the posterior probability distribution of heading across the image and choose the heading with maximum a posteriori (MAP). The model requires very simple computation, provides confidence level of the judgments, applies to both linear and curved trajectories, functions in the presence of camera rotations, and exhibited high accuracy up to 0.1◦–0.2◦ in random dot simulations. c © 1999 Academic Press	computation;course (navigation);optical flow;simulation;stationary process;statistical model	Ranxiao Frances Wang;James E. Cutting	1999	Computer Vision and Image Understanding	10.1006/cviu.1999.0798	parallax;statistical model;computer vision;simulation;confidence interval;bayesian probability;probability;optical flow;mathematics;posterior probability;statistics	Vision	53.41097051682935	-39.88528906837607	82072
b28346f6a962c6bbe309c891cfe04c90b97c1fc4	iterative online subspace learning for robust image alignment	jittered camera surveillance images iterative online subspace learning robust image alignment robust high dimensional data processing convex programming low rank component sparse outlier component robust pca computer vision image processing video processing face recognition massive image databases data quality data consistency t grasta transformed grasta grassmannian robust adaptive subspace tracking algorithm incremental gradient descent grassmann manifold low rank subspace foreground objects image rotation image translation state of the art algorithms memory requirement face images;video surveillance;video signal processing;convex programming;robustness vectors jacobian matrices cameras lighting databases sparse matrices;会议论文;computer vision;iterative methods;face recognition;video cameras;principal component analysis;object tracking;visual databases computer vision convex programming face recognition iterative methods learning artificial intelligence object tracking principal component analysis video cameras video signal processing video surveillance;learning artificial intelligence;visual databases	Robust high-dimensional data processing has witnessed an exciting development in recent years, as theoretical results have shown that it is possible using convex programming to optimize data fit to a low-rank component plus a sparse outlier component. This problem is also known as Robust PCA, and it has found application in many areas of computer vision. In image and video processing and face recognition, an exciting opportunity for processing of massive image databases is emerging as people upload photo and video data online in unprecedented volumes. However, the data quality and consistency is not controlled in any way, and the massiveness of the data poses a serious computational challenge. In this paper we present t-GRASTA, or “Transformed GRASTA (Grassmannian Robust Adaptive Subspace Tracking Algorithm)”. t-GRASTA performs incremental gradient descent constrained to the Grassmann manifold of subspaces in order to simultaneously estimate a decomposition of a collection of images into a low-rank subspace, a sparse part of occlusions and foreground objects, and a transformation such as rotation or translation of the image. We show that t-GRASTA is 4× faster than state-of-the-art algorithms, has half the memory requirement, and can achieve alignment for face images as well as jittered camera surveillance images.	algorithm;algorithmic efficiency;align (company);computation;computer vision;control theory;convex optimization;data quality;database;facial recognition system;foremost;iterative method;manifold regularization;mathematical optimization;real-time clock;sparse matrix;stochastic gradient descent;streaming media;upload;video processing;what if	Jun He;Dejiao Zhang;Laura Balzano;Tao Tao	2013	2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)	10.1109/FG.2013.6553759	computer vision;image processing;computer science;machine learning;video tracking;pattern recognition	Vision	51.639590915986545	-48.36905619926811	82143
6c19650800f0c4c52709b0df650b0665f70e6c6b	monocular localization in urban environments using road markings		Localization is an essential problem in autonomous navigation of self-driving cars. We present a monocular vision based approach for localization in urban environments using road markings. We utilize road markings as landmarks instead of traditional visual features (e.g. SIFT) to tackle the localization problem because road markings are more robust against changes in perspective, illumination, and across time. Specifically, we employ Chamfer matching to register edges of road markings against a lightweight 3D map where road markings are represented as a set of sparse points. By only matching geometry of road markings, our localization algorithm further gains robustness against photometric appearance changes in the environment. We take vehicle odometry and epipolar geometry constraints into account and formulate a non-linear optimization problem to estimate the 6 DoF camera pose. We evaluate the proposed method on data collected in the real world. Experimental results show that our method achieves sub-meter localization errors in areas with sufficient road markings.		Yan Lu;Jiawei Huang;Yi-Ting Chen;Bernd Heisele	2017	2017 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2017.7995762	robustness (computer science);simultaneous localization and mapping;scale-invariant feature transform;epipolar geometry;odometry;monocular;monocular vision;computer science;computer vision;artificial intelligence;optimization problem	Robotics	51.149580437734066	-45.782897113739416	82304
0b26208edb7faf93912e3e9fe6ab85ce758e4d6f	low-power embedded system for real-time correction of fish-eye automotive cameras	real-time correction;radial distortion;better view;on-board video system;fish-eye lens;low-power embedded system;cost-effective embedded system;causes distortion;fish-eye automotive camera;automotive application;real-time system;field of view;image sensor;embedded system;cost effectiveness;fov;lenses;real time systems;embedded systems;real time system;real time;image sensors	The design and the implementation of a flexible and cost-effective embedded system for real-time correction of fish-eye automotive cameras is presented. Nowadays many car manufacturers already introduced on-board video systems, equipped with fish-eye lens, to provide the driver a better view of the so-called blind zones. A fish-eye lens achieves a larger field of view (FOV) but, on the other hand, causes distortion, both radial and tangential, of the images projected on the image sensor. Since radial distortion is noticeable and dangerous, a real-time system for its correction is presented, whose low-power, low-cost and flexibility features are suitable for automotive applications.	distortion;embedded system;field of view in video games;image sensor;low-power broadcasting;on-board data handling;radial (radio);real-time clock;real-time computing	Mauro Turturici;Sergio Saponara;Luca Fanucci;Emilio Franchi	2012	2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;computer vision;engineering;image sensor	EDA	44.63795015063156	-38.197218733739156	82355
11ea239c256ca20662225897c2b800c7e690ef04	directing the attention of awearable camera by pointing gestures	cameras magnetic heads wearable sensors keyboards shape focusing humans robot vision systems scholarships computer science;wearable computers cameras computational complexity gesture recognition object detection;computational complexity wearable camera wearable visual sensor hand gesture recognition coarse to fine method shape detection 3d tracking method;wearable computers;wearable camera;shape detection;computational complexity;focus of attention;wearable visual sensor;image sequence;real time implementation;hand gesture recognition;3d tracking method;gesture recognition;cameras;coarse to fine method;object detection	Wearable visual sensors provide views of the environment which are rich in information about the wearer's location, interactions and intentions. In the wearable domain, hand gesture recognition is the natural replacement for keyboard input. We describe a framework combining a coarse-to-fine method for shape detection and a 3D tracking method that can identify pointing gestures and estimate their direction. The low computational complexity of both methods allows a real-time implementation that is applied to estimate the user's focus of attention and to control fast redirections of gaze of a wearable active camera. Experiments have demonstrated a level of robustness of this system in long and noisy image sequences	algorithmic efficiency;computation;computational complexity theory;experiment;finite-state machine;gesture recognition;interaction;plover;pointing device;real-time clock;robustness (computer science);sensor;wearable computer	Teófilo Emídio de Campos;Walterio W. Mayol-Cuevas;David W. Murray	2006	2006 19th Brazilian Symposium on Computer Graphics and Image Processing	10.1109/SIBGRAPI.2006.13	computer vision;computer science;communication;computer graphics (images)	Robotics	47.91744310305775	-42.61231974011042	82496
548953ca9ef4752c2931e3a4ff69dcfb98c87bdf	the fuzzy neural network based haul truck driver fatigue detection in surface mining		Haul trucks are one of the most important transportations in the surface mining. The traffic accidents involved haul trucks result in serious damages of the vehicles, even the loss of the drivers' lives. Drive fatigue is one of the key features that cause accidents. This paper analyzed the possible driver fatigue reasons based on the special features of the haul trucks. Then, it proposed the fuzzy neural network based fatigue detection for haul truck drivers. The CCD camera was amounted in the cabin of the haul truck to capture the key features of the drivers. The eyes of the driver could show most important information of fatigue. The head nod frequency, yawn frequency could also reflect the driver's fatigue degree. Each of these methods has its own limitation and detection errors. The results shows the fuzzy neural network detection method has more accurate detection by combining with PERCLOS (Percentage of eyelid closure over the pupil over time), AECS (Average eye closure speed), NodFreq (Nod frequency) and YawnFreq (Yawn frequency). It has great significance in reducing the accidents rate caused by the drive fatigue.	artificial neural network;camera phone;charge-coupled device;neuro-fuzzy	Enji Sun	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8392991	machine learning;automotive engineering;artificial neural network;artificial intelligence;truck;eyelid closure;computer science;eye closure;surface mining	ML	41.723798684181	-42.68723217134273	82566
e754608cb9ba536a0b50cb080092e3a6e1128221	target object tracking-based 3d object reconstruction in a multiple camera environment in real time		The visualization of a three-dimensional target object reconstruction from multiple cameras is an important issue in high-dimensional data representations with application for medical uses, sports scene analysis, and event creation for film. In this paper, we propose an efficient 3D reconstruction methodology to voxelize and carve the 3D scene in focus on 3D tracking of the object in a large environment. We applied sparse representation-based target object tracking to efficiently trace the movement of the target object in a background clutter and reconstruct the object based on the estimated 3D position captured from multiple images. The voxelized area is optimized to the target by tracking the 3D position and then effectively reduce the process time while keeping the details of the target. We demonstrate the experiments by carving the voxels within the 3D tracked area of the target object.		Jinjoo Song;Heeryon Cho;Sang Min Yoon	2017		10.1007/978-3-319-54472-4_56	camera auto-calibration;deep-sky object	Vision	50.07056341842736	-46.46758025725797	82624
0e7b5efe80af08b789a6c52c15cc6ded1db7d05b	estimating intrinsic parameters of cameras using two arbitrary rectangles	camera constraints;parameter estimation cameras computer vision geometry;camera intrinsic parameter estimation;parameter estimation cameras calibration robot vision systems layout robustness rendering computer graphics robot localization image sequences testing;geometry;computer vision;camera intrinsic parameter estimation camera calibration image warping camera constraints;parameter estimation;camera calibration;cameras;image warping;aspect ratio	In this paper, we propose new camera calibration methods assuming a static camera. Two corresponding imaged rectangles whose aspect ratios are unknown are sufficient to calibrate a camera. By warping the images properly, we show that the information from the imaged rectangles can be transformed to the form of camera constraints. Based on these results, we propose two methods, one for three or more images and the other for only two images. The proposed methods are verified with synthetic and real images, and the results are comparable with less assumption on cameras and on scenes	camera resectioning;homography (computer vision);synthetic intelligence;virtual camera system	Jun-Sik Kim;In-So Kweon	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.500	triangulation;image warping;stereo camera;computer vision;camera auto-calibration;camera matrix;aspect ratio;camera resectioning;simulation;computer science;estimation theory;three-ccd camera;pinhole camera model;epipolar geometry;computer graphics (images)	Vision	53.650327626996535	-50.09290867138652	82717
51647d9adb299719ac78a41e7502f12ae3226c10	automatic character recognition for moving and stationary vehicles and containers in real-life images	image recognition;neural nets;optical character recognition;container terminal;container depots automatic character recognition moving vehicles stationary vehicles containers real life images image distortion uneven lighting vecon vehicle and container number recognition car parks bus stations border checkpoints container terminals;neural nets image recognition optical character recognition;character recognition;character recognition containers paints fading image segmentation robustness layout lighting vehicle detection computer science	Many methods have been proposed for character recognition but they are often subjected to substantial constraints, due to unexpected difficulties encountered in real-life images. A real-life image may be complex for a variety of reasons. Rust, mud, peeling paint, or fading color may distort the images of the characters; uneven lighting may make them difficult to discern. This paper presents the VECON (Vehicle and Container Number Recognition) system, which takes into account a wide range of real-life considerations and aims at offering applicable solutions to some industries. After being tested under outdoor environment and 24-hour operations, the proposed methods proved to have accuracy higher than 95%. The system was commercially employed in car parks, bus stations, border checkpoints, container terminals and container depots.	24-hour clock;artificial neural network;character encoding;computer vision;distortion;image processing;mud;optical character recognition;real life;rust;stationary process;test set	John C. M. Lee	1999		10.1109/IJCNN.1999.833530	computer vision;simulation;computer science;machine learning;optical character recognition;artificial neural network	Robotics	42.22112066329514	-43.722667875540786	82748
330b7049e511c500a0e21c19ff62499bf5383a9d	tracking algorithm using background-foreground motion models and multiple cues [surveillance video applications]	probability propagation model;probability optical tracking tracking filters video signal processing motion estimation surveillance;multiple cues;probability;dim targets;video signal processing;surveillance;likelihood function estimation;nonlinear dynamical systems;low resolution targets;low resolution;tracking filter;motion estimation;tracking filters;surveillance target tracking videos particle tracking particle filters robustness educational institutions cameras nonlinear dynamical systems automation;surveillance videos;optical tracking;particle filter;multiple cue system observation model;motion models;stochastic tracking algorithm;background foreground motion estimation;likelihood function estimation tracking filter background foreground motion estimation motion models multiple cues stochastic tracking algorithm surveillance videos dim targets low resolution targets particle filter probability propagation model multiple cue system observation model;robustness;particle tracking;particle filters;target tracking;likelihood function;cameras;videos;automation	We present a stochastic tracking algorithm for surveillance videos where targets are dim and of low resolution. Our tracker is mainly based on the particle filter algorithm. Two important novel features of the tracker include: a motion model consisting of both background and foreground motion parameters; multiple cues are adaptively integrated in a system observation model when estimating the likelihood functions. Based on these features, the accuracy and robustness of the tracker has been improved, which is very important for surveillance problems. We present the results of applying the proposed algorithm to many videos.	closed-circuit television;image resolution;particle filter;peterson's algorithm	Jie Shao;Shaohua Kevin Zhou;Rama Chellappa	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415384	computer vision;simulation;particle filter;computer science;control theory;mathematics;statistics	Vision	45.84507458867783	-47.49396483792896	82877
49bcc21994f1df994795e3249aaf782765ce2567	optimal registration of object views using range data	3d free form objects;unweighted error criterion optimal registration object views range data robust registration uncertainties noise depth data view integration automatic construction minimum variance estimator view transformation parameters view transformation estimates;minimum variance;object recognition;range data;view integration;object recognition image registration motion estimation;motion estimation;multiple views;view transformation estimation;automatic object modeling;electrical capacitance tomography data acquisition solid modeling noise robustness uncertainty image registration object recognition buildings predictive models service robots;image registration;object model	This paper deals with robust registration of object views in the presence of uncertainties and noise in depth data. Errors in registration of multiple views of a 3D object severely aaect view integration during automatic construction of object models. We derive a minimum variance estimator (MVE) for computing the view transformation parameters accurately from range data of two views of a 3D object. The results of our experiments show that view transformation estimates obtained using MVE are signiicantly more accurate than those computed with an unweighted error criterion for registration.	experiment;glossary of computer graphics	Chitra Dorai;Juyang Weng;Anil K. Jain	1997	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.625115	computer vision;minimum-variance unbiased estimator;object model;computer science;image registration;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;motion estimation	Vision	52.026982265481614	-49.96039701635427	82887
3da157cb9b8a5ac625165930bd2ed6fdafbc54b5	accurate dense stereo matching of slanted surfaces using 2d integral images	disparity gradient;stereo matching;cross based support region;integral image	This paper presents an advanced algorithm providing accurate stereo correspondences of two frames through concise disparity gradient estimation at the per-pixel level and 2D integral images. The key contributions of this novel algorithm are twofold: First, combining an upright cross-based support region with disparity gradient estimation realizes the implicit construction of a 3D support region for each anchor pixel. This approach yields the disparity accuracy for slanted surfaces as well as fronto-parallel surfaces. Second, the 2D integral image technique leads to a speedup of matching cost aggregation in the implicit 3D support regions. The experimental results show that the proposed algorithm can successfully convey the correspondences of actual sequence of outdoor stereo images and Middlebury stereo images with high accuracy in near real time.		Gwnag Yul Song;Seong Ik Cho;Dong Yong Kwak;Joon Woong Lee	2013		10.1007/978-3-642-39402-7_29	computer vision;mathematical optimization;computer science;summed area table	Vision	51.96385091172096	-46.91098555306624	82946
3eb824c306335f62581c037ccee91d90e3d4ec1c	stereo vision enabling precise border localization within a scanline optimization framework	stereo matching;stereo vision	A novel algorithm for obtaining accurate dense disparity measurements and precise border localization from stereo pairs is proposed. The algorithm embodies a very effective variable support approach based on segmentation within a Scanline Optimization framework. The use of a variable support allows for precisely retrieving depth discontinuities while smooth surfaces are well recovered thanks to the minimization of a global function along multiple scanlines. Border localization is further enhanced by symmetrically enforcing the geometry of the scene along depth discontinuities. Experimental results show a significant accuracy improvement with respect to comparable stereo matching approaches.	algorithm;binocular disparity;computer stereo vision;correspondence problem;local variable;mathematical optimization;refinement (computing);scan line;stereopsis	Stefano Mattoccia;Federico Tombari;Luigi di Stefano	2007		10.1007/978-3-540-76390-1_51	computer stereo vision;computer vision;computer science;stereopsis;geometry	Vision	52.59780458478401	-47.73232818667805	82965
7669190d7e56359b2ce56eb8a135c0f6725a8717	articulated object recognition: a general framework and a case study	object recognition humans computer science image recognition object detection motion detection surveillance proposals leg kinematics;image recognition;object recognition;surveillance;null;constraint satisfaction;kinematics;standard model;humans;computer science;false positive;spatial configuration;proposals;motion detection;leg;domain specificity;object detection	We present in this paper a general-purpose approach for articulated object recognition. We split the recognition process in two distinct phases. In the former we use standard model-based techniques in order to recognize and localize in the input image the rigid components the articulated object is composed of. In the second phase the spatial configurations formed by the recognized components are analyzed and compared with the valid configurations of the object we are searching. The comparison is based on a constraint satisfaction method which can deal with both missing components and false positives. The proposed method is based on a redundant set of constraints which represent the valid spatial configurations of the object's components. Such constraints are not embedded in the system nor are domain-specific but they are learned during a suitable training phase. We show how this approach can be used in different scenarios with different kinds of articulated objects and we present a case study concerning a robotic application.	coat of arms;constraint satisfaction;embedded system;general-purpose modeling;gesture recognition;heuristic (computer science);online and offline;outline of object recognition;robot	Luigi Cinque;Enver Sangineto;Steven L. Tanimoto	2006	2006 IEEE International Conference on Video and Signal Based Surveillance	10.1109/AVSS.2006.26	standard model;computer vision;kinematics;simulation;constraint satisfaction;type i and type ii errors;computer science;cognitive neuroscience of visual object recognition	Robotics	48.7796313090842	-39.604065000956716	83122
4961be139889c933b8d6d198a68542d5ed71a8d4	motion recognition by higher order local auto correlation features of motion history images	dynamic programming;image recognition;hlac;image motion analysis;image resolution;image matching;pitching motions;image resolution motion recognition motion history images higher order local autocorrelation image sequences dynamic programming pitching motions;mhi hlac;higher order local autocorrelation;image sequences dynamic programming image matching image motion analysis image resolution;dynamic program;data mining;higher order;motion recognition;feature extraction;principal component analysis;pixel;image sequence;mhi;motion history images;correlation;feature extraction pixel image recognition image sequences correlation principal component analysis data mining;motion history image;image sequences	This paper proposes new features for motion recognition. Higher order local autocorrelation (HLAC) features are extracted from the motion history images (MHI). Since MHI calculated from the video images include important motion information, it is expected that HLAC features extracted from MHI have good properties for motion recognition. The proposed features were tested using image sequences of pitching in the baseball games. At first the pitchers were identified from the pitching motions by comparing the sequences of HLAC features using dynamic programming (DP) matching. The pitchers were recognized 100% correctly when the image size was 90times90 pixels. Then whether there was the runner on a base or not was identified. The recognition rate of the runners from the pitching motions was 96.7% when the image resolution was set to 25times25 pixels.	autocorrelation;dynamic programming;image resolution;motion history images;pixel	Kenji Watanabe;Takio Kurita	2008	2008 Bio-inspired, Learning and Intelligent Systems for Security	10.1109/BLISS.2008.15	computer vision;pattern recognition;mathematics;computer graphics (images)	Vision	39.4682222443453	-47.94802168716404	83156
cfd9b47b133fa6e48fe4fdca7ced64f172be6dbd	depth-from-motion estimation based on a multiscale analysis of motion constraint equation validity	motion analysis;image motion analysis;motion constraint equation;performance evaluation;image resolution;optical noise;confidence measure;multiscale analysis;statistical framework;motion estimation;layout;performance evaluation depth from motion estimation multiscale analysis motion constraint equation image sequence camera motion rigid scene pixels brightness change constraint equation depth value confidence measure local analysis image resolution statistical framework temporal information synthetic images real images;depth from motion estimation;brightness change constraint equation;temporal information;brightness;depth value;camera motion;navigation;statistical analysis;rigid scene;pixel;motion estimation motion analysis equations image motion analysis cameras layout brightness navigation optical noise pixel;synthetic images;image sequence;pixels;statistical analysis image sequences image resolution motion estimation brightness;real images;cameras;local analysis;image sequences	We present a new method of estimating the depth-from-motion in a image sequence. It is assumed that the camera motion is known and the scene is rigid. Our idea is to evaluate carefully the pixels where the brightness change constraint equation is valid, and consequently the depth value is reliable. A confidence measure is derived from local analysis. This analysis is performed at various levels of resolution, increasing the number of reliable pixels. The depth values obtained at each level are then fused within a statistical framework, allowing one to also integrate the temporal information. Results are shown for real and synthetic images used in the performance evaluation.	motion estimation	J.-L. Sune;Veronique Rebuffel;R. Samy	1995		10.1109/ICIP.1995.529731	computer vision;computer science;mathematics;pixel;computer graphics (images)	Vision	52.92394639639029	-50.510508796067086	83215
d855b902eccd344c1f65bbd12c2668cb5925e4b9	non-metric image-based rendering for video stabilization	metric space;stability;camera motion;image reconstruction;rendering computer graphics cameras image reconstruction video sequences layout software algorithms laboratories computer science calibration image sequences;stability rendering computer graphics image sequences image reconstruction;camera calibration;image based rendering;rendering computer graphics;image space non metric image based rendering unwanted image perturbation removal unstable camera motions image based rendering unstabilized video sequence stabilized camera trajectory euclidean reconstruction unstabilized camera trajectory ibr algorithm arbitrary video sequence non metric reconstructions rendering techniques video stabilization problem;image sequences	We consider the problem of video stabilization: removing unwanted image perturbations due to unstable camera motions. We approach this problem from an image-based rendering (IBR) standpoint. Given an unstabilized video sequence, the task is to synthesize a new sequence as seen from a stabilized camera trajectory. This task is relatively straightforward if one has a Euclidean reconstruction of the unstabilized camera trajectory and a suitable IBR al-	control theory;euclidean algorithm;image-based modeling and rendering;interpolation;quantum superposition;rendering (computer graphics);rollover (key);sensor;virtual camera system	Chris Buehler;Michael Bosse;Leonard McMillan	2001		10.1109/CVPR.2001.991019	iterative reconstruction;computer vision;camera auto-calibration;tiled rendering;camera resectioning;simulation;image-based modeling and rendering;stability;3d rendering;rendering;image processing;metric space;computer science;mathematics;real-time rendering;alternate frame rendering;video post-processing;statistics;image-based lighting;computer graphics (images)	Vision	53.60091828875857	-50.40836000125073	83229
33359dd0f1c7283cdfe10e90bbb164b349171793	interactive shadow analysis for camera heading in outdoor images	sun earth system interactive shadow analysis camera heading outdoor image image geolocalization augmented reality navigation metadata gps geotagged database open street map query image;cameras sun estimation mathematical model databases azimuth optical variables measurement;sun cameras earth geophysical image processing meta data	Image geo-localization is an important problem with many applications such as augmented reality and navigation. The most common ways to geo-localize an image are to use its meta-data such as GPS or to match it against a geotagged database. When neither of those is available, it is still possible to apply shadow analysis to determine the camera heading for outdoor images. This could be useful pruning the search space in geo-localization applications, for example by removing roads with incompatible orientations from a database such as Open Street Map. In this paper, we develop a novel interactive method for deducing the global heading of a query image using the shadows in it. We start by constructing a model of the sun-earth system to determine all shadows possible at a given approximate latitude, and compare shadows within the query to those possible under the model to determine the range of possible headings. We demonstrate this on 54 query images with known ground truth, and show that in 52 cases the ground truth lies in the computed range.	approximation algorithm;augmented reality;course (navigation);earth system science;geotagging;global positioning system;ground truth;openstreetmap	Matthew Clements;Avideh Zakhor	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025681	computer vision;computer graphics (images)	Vision	51.179568952222525	-43.68383114532282	83380
ec787cb291c77f95679f7bdb5e7f4b94c664382f	3d periodic human motion reconstruction from 2d motion sequences	motion analysis;humans image reconstruction motion analysis biological system modeling surveillance visual system pattern recognition computer vision image recognition shape;image recognition;surveillance;biological system modeling;three dimensional;computer vision;shape;image reconstruction;human motion;linear model;pattern recognition;principle component analysis;humans;visual system;3d reconstruction	In this report, we present and evaluate a method of reconstructing three-dimensional (3D) periodic human motion from two-dimensional (2D) motion sequences. Based on a Fourier decomposition of a training set of 3D data, we construct a linear, morphable representation. Using this representation a low-dimensional linear model is learned by means of Principle Component Analysis (PCA). Two-dimensional test data are now projected onto this model and the resulting 3D reconstructions are evaluated. We present two different simulations. In the first experiment, we assume the 2D projection matrix to be known. In the second experiment, the horizontal viewpoint is unknown and is being recovered from the data.		Zonghua Zhang;Nikolaus F. Troje	2004		10.1109/CVPR.2004.276	3d reconstruction;iterative reconstruction;three-dimensional space;computer vision;simulation;visual system;shape;computer science;linear model;motion estimation;mathematics;motion field;principal component analysis;computer graphics (images)	Vision	46.53107536936555	-49.52765267520162	83381
d9ac82f9e829cbd57daedfcb57326eaa45dccef0	roam: a rich object appearance model with application to rotoscoping		Rotoscoping, the detailed delineation of scene elements through a video shot, is a painstaking task of tremendous importance in professional post-production pipelines. While pixel-wise segmentation techniques can help for this task, professional rotoscoping tools rely on parametric curves that offer the artists a much better interactive control on the definition, editing and manipulation of the segments of interest. Sticking to this prevalent rotoscoping paradigm, we propose a novel framework to capture and track the visual aspect of an arbitrary object in a scene, given a first closed outline of this object. This model combines a collection of local foreground/background appearance models spread along the outline, a global appearance model of the enclosed object and a set of distinctive foreground landmarks. The structure of this rich appearance model allows simple initialization, efficient iterative optimization with exact minimization at each step, and on-line adaptation in videos. We demonstrate qualitatively and quantitatively the merit of this framework through comparisons with tools based on either dynamic segmentation with a closed curve or pixel-wise binary labelling.	benchmark (computing);iterative method;mathematical optimization;online and offline;pipeline (computing);pixel;programming paradigm;roam	Ondrej Miksik;Juan-Manuel Pérez-Rúa;Philip H. S. Torr;Patrick Pérez	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.785	machine learning;computer vision;minification;initialization;parametric equation;artificial intelligence;active appearance model;computer science	Vision	48.95201576186577	-50.948863182464024	83417
a885df2901537dd6fedbef43ed75f537e6c46f64	motion estimation by iterative 2-d features matching in range images	optimisation;cost function;image matching;path planning;motion estimation;optimisation mobile robots robot vision laser ranging motion estimation path planning computerised navigation feature extraction image matching iterative methods;mobile robots;laser ranging;feature matching;geometric feature;iterative algorithm;iterative methods;optimisation image matching 2d range images feature extraction ellipsoidal clusters motion estimation iterative method mobile robots robot vision navigation path planning;robot vision;range image;feature extraction;indoor environment;motion estimation iterative algorithms motion measurement indoor environments clustering algorithms cost function image segmentation robot kinematics laser radar robustness;computerised navigation	This article describes an iterative algorithm for relative motion estimation from 2D range images. The matching is achieved in the geometric feature domain represented by straight lines and ellipsoidal clusters. Using infinite length straight lines instead of line segments as features, the algorithm attempts to achieve robustness to partial occlusions. The motion estimates and the feature correspondence measures are determined in order to minimize a cost function. The performance of this algorithm was evaluated with experiments carried out in real cluttered indoor environments.	motion estimation	Geovany de Araújo Borges;Marie-José Aldon	2000		10.1109/ROBOT.2000.845155	computer vision;simulation;computer science;artificial intelligence;machine learning;iterative method	Robotics	49.971694621439404	-50.725662210049734	83511
4ae91e84eecf690c93fbdf4a4de867d68727e0e3	a real-time object tracker equipped with deep object recognizer	learning;real time;deep convolutional neural network;detection;object recognization;visual tracking	A novel system for intelligent visual tracking focus on the unknown object tracking in short or mid-term video stream of the object in unconstrained environment. Also it commonly focuses only few kind of objects for doing definite programmed mission like moving the object or opening the door. It means that many vision tracking did not consider the general category of the object. For more intelligent visual tracking system, long-term robust unknown object tracking and recognizer is important. We propose the real-time intelligent tracking system equipped with recognizer operating on long-term situation by using novel tracking framework Tracking-Learning-Detection (TLD) and deep convolutional neural network. The system is validated on long-term surveillance to tracking and recognizing object.	artificial neural network;convolutional neural network;finite-state machine;real-time locating system;real-time transcription;streaming media;tracking system;video tracking	Youngjoo Jo;Jun-Cheol Park;Jinwoo Jeon;Changmo Nam;Junghee Han;Yongin Park;Dae-Shik Kim	2016		10.1145/2857546.2857647	computer vision;simulation;speech recognition;tracking system;eye tracking;computer science;viola–jones object detection framework;video tracking	Robotics	39.46012335720682	-46.68803788768905	83766
091b78133b0bc95c7011093754ae3e9281f7e97e	modeling and simulation of upper brachial plexus injury	simulation animation force modeling rehabilitation robotics;muscles brachytherapy force injuries lesions joints;medicina;comunicacion de congreso;simulation;robotica e informatica industrial;joints;patient rehabilitation computer animation digital simulation injuries medical computing medical robotics muscle orthopaedics;force;lesions;rehabilitation robotics;brachytherapy;injuries;animation;modeling;arm abduction adduction upper brachial plexus injury simulation musculoskeletal simulation robotic exoskeleton musculoskeletal modeling motion simulation computer package open source package moment of inertia musculoskeletal modeling software muscles postsurgery rehabilitation protocol movement animation inertial measurement unit elbow flexion extension;muscles	This paper presents the first musculoskeletal model and simulation of an upper plexus brachial injury. From this model, it is possible to analyze forces and movement ranges in order to develop a robotic exoskeleton to improve rehabilitation. The software that currently exists for musculoskeletal modeling is varied and most have advanced features for proper analysis and study of motion simulations. While more powerful computer packages are usually expensive, there are other free and open source packages available that offer different tools to perform animations and simulations and obtain forces and moments of inertia. Among them, Musculoskeletal Modeling Software was selected to construct a model of the upper limb, which has 7° of freedom and 10 muscles. These muscles are important for two of the movements simulated in this paper that are part of the postsurgery rehabilitation protocol. We performed different movement animations that are made using the inertial measurement unit to capture real data from movements made by a human being. We also performed the simulation of forces produced in elbow flexion-extension and arm abduction-adduction of a healthy subject and one with an upper brachial plexus injury in a postoperative state to compare the force that is capable of being produced in both cases.	open-source software;robot;simulation;video game rehabilitation	Marie André Destarac;Cecilia E. Garcia Cena;Roque J. Saltarén;Monica J. Reyes Urbina;Javier López López;Ricardo Espinoza Gomez	2016	IEEE Systems Journal	10.1109/JSYST.2014.2387426	anime;simulation;systems modeling;computer science;engineering;force	Robotics	39.63354891539533	-38.53792374228585	83796
39ca65064f17486da1146bd29b747c68e2b815aa	human age estimation method robust to camera sensor and/or face movement	affective interface for entertainment;blurring effect of camera sensor;human age estimation	Human age can be employed in many useful real-life applications, such as customer service systems, automatic vending machines, entertainment, etc. In order to obtain age information, image-based age estimation systems have been developed using information from the human face. However, limitations exist for current age estimation systems because of the various factors of camera motion and optical blurring, facial expressions, gender, etc. Motion blurring can usually be presented on face images by the movement of the camera sensor and/or the movement of the face during image acquisition. Therefore, the facial feature in captured images can be transformed according to the amount of motion, which causes performance degradation of age estimation systems. In this paper, the problem caused by motion blurring is addressed and its solution is proposed in order to make age estimation systems robust to the effects of motion blurring. Experiment results show that our method is more efficient for enhancing age estimation performance compared with systems that do not employ our method.	blurred vision;body dysmorphic disorders;classification;distortion;elegant degradation;estimated;experiment;gabor filter;gaussian blur;image resolution;image sensor;physical object;real life;total peripheral resistance	Tien Dat Nguyen;So Ra Cho;Tuyen Danh Pham;Kang Ryoung Park	2015		10.3390/s150921898	computer vision;simulation;computer graphics (images)	Robotics	45.28551955962443	-43.779984108747755	84175
064c31f67c9af75e8291df43a5746c0baaf0e1bf	rent3d: floor-plan priors for monocular layout estimation	rooms aspect ratio rent3d floor plan priors monocular layout estimation 3d virtual tour monocular images 2d floor plan markov random field camera pose aspect ratio constraints semantic information room apartment alignment problem exact inference algorithm integral geometry;layout three dimensional displays cameras estimation semantics face geometry;virtual reality buildings structures geometry image sensors inference mechanisms markov processes pose estimation structural engineering computing	The goal of this paper is to enable a 3D “virtual-tour” of an apartment given a small set of monocular images of different rooms, as well as a 2D floor plan. We frame the problem as inference in a Markov Random Field which reasons about the layout of each room and its relative pose (3D rotation and translation) within the full apartment. This gives us accurate camera pose in the apartment for each image. What sets us apart from past work in layout estimation is the use of floor plans as a source of prior knowledge, as well as localization of each image within a bigger space (apartment). In particular, we exploit the floor plan to impose aspect ratio constraints across the layouts of different rooms, as well as to extract semantic information, e.g., the location of windows which are marked in floor plans. We show that this information can significantly help in resolving the challenging room-apartment alignment problem. We also derive an efficient exact inference algorithm which takes only a few ms per apartment. This is due to the fact that we exploit integral geometry as well as our new bounds on the aspect ratio of rooms which allow us to carve the space, significantly reducing the number of physically possible configurations. We demonstrate the effectiveness of our approach on a new dataset which contains over 200 apartments.	3d floor plan;algorithm;markov chain;markov random field;microsoft windows	Chenxi Liu;Alexander G. Schwing;Kaustav Kundu;Raquel Urtasun;Sanja Fidler	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298963	computer vision;simulation;machine learning	Vision	52.10943569412924	-46.741417587239695	84233
ea1946d4f9480119fe418af009abefa53f0ac094	robust recovery of piled box-like objects in range images				Dimitrios Katsoulas	2005			computer science	Robotics	51.867189462360535	-43.087745375832206	84247
3226cca8dad9b66abb503eddb546d780e604227a	articulated-pose estimation using brightness and depth-constancy constraints	motion estimation;nonlinear equations motion estimation brightness mathematics legged locomotion cameras electronic switching systems image sensors tracking motion analysis;linear constraint;first order;linear transformation;walking sequence articulated pose estimation stereo cameras depth measurements depth constraint equation joint constraints rotation matrix synthetic walking sequences moving camera views;matrix approximation;image sequences motion estimation;image sequences;pose estimation	This paper explores several approaches for articulated-pose estimation, assuming that video-rate depth information is available, from either stereo cameras or other sensors. We use these depth measurements in the traditional linear brightness constraint equation, as well as in a depth constraint equation. To capture the joint constraints, we combine the brightness and depth constraints with twist mathematics. We address several important issues in the formation of the constraint equations, including updating the body rotation matrix without using a first-order matrix approximation and removing the coupling between the rotation and translation updates. The resulting constraint equations are linear on a modified parameter set. After solving these linear constraints, there is a single closed-form non-linear transformation to return the updates to the original pose parameters. We show results for tracking body pose in oblique views of synthetic walking sequences and in moving-camera views of synthetic jumping-jack sequences. We also show results for tracking body pose in side views of a real walking sequence. 1 Introduction In this paper, we extend the head-pose tracking of Har-ville et al. [1] to articulated-pose tracking. We assume that we have video-rate depth images, from either stereo cameras or from other sensors. The depth images allow us to use depth-constancy constraint equations (ZCCE) that are similar to the classic brightness-constancy constraint equations (BCCE). The depth images also give us linear constraints , even when we use a perspective-camera model. In Section 3, we review these constraint equations and use twist mathematics [2] to capture the motion constraints imposed by the articulated joints. Our basic twist derivations are similar in spirit to the derivations of Bregler et al. [3]. The primary differences trace back to the approximations made within the derivations: Bregler approximates perspective constraints using scaled-orthographic constraints and he approximates the body-rotation matrix using an extra first-order Taylor-series expansion. We avoid this first-order approximation by solving our constraints on a transformed parameter set and by remapping our results into the original parameter set using a closed-form non-linear function (Section 3.5). Throughout Section 3, we assume that we know which limb each pixel corresponds to. To get this information, we	approximation algorithm;control theory;first-order predicate;glossary of computer graphics;interleaved memory;interpolation;iteration;iterative method;joint constraints;linear function;linear least squares (mathematics);nonlinear system;oblique projection;order of approximation;orthographic projection;pixel;requirement;sensor;series expansion;simulation;singular value decomposition;stereo cameras;synthetic intelligence;tree accumulation	Michele Covell;Ali Rahimi;Michael Harville;Trevor Darrell	2000		10.1109/CVPR.2000.854875	computer vision;mathematical optimization;pose;3d pose estimation;computer science;motion estimation;first-order logic;mathematics;geometry;linear map	Vision	52.7781052351357	-50.61761681346257	84304
efd08973ae9579848ef3dff16b345f5bc22154ac	joint random sample consensus and multiple motion models for robust video tracking	computer vision and robotics autonomous systems;computer engineering;publikationer;bildanalys;estimation method;random sampling;datorseende och robotik autonoma system;computer and information science;video tracking;konferensbidrag;model complexity;computer vision;deformable objects;natural sciences;multiple objectives;motion blur;estimation;artiklar;rapporter;multiple model;image analysis;datorteknik;video;matematik;conditional probability;tracking	We present a novel method for tracking multiple objects in video captured by a non-stationary camera. For low quality video, ransac estimation fails when the number of good matches shrinks below the minimum required to estimate the motion model. This paper extends ransac in the following ways: (a) Allowing multiple models of different complexity to be chosen at random; (b) Introducing a conditional probability to measure the suitability of each transformation candidate, given the object locations in previous frames; (c) Determining the best suitable transformation by the number of consensus points, the probability and the model complexity. Our experimental results have shown that the proposed estimation method better handles video of low quality and that it is able to track deformable objects with pose changes, occlusions, motion blur and overlap. We also show that using multiple models of increasing complexity is more effective than just using ransac with the complex model only.	emoticon;experiment;gaussian blur;match moving;mobile phone;motion estimation;random sample consensus;stationary process;video tracking	Petter Strandmark;Irene Y. H. Gu	2009		10.1007/978-3-642-02230-2_46	sampling;computer vision;estimation;ransac;image analysis;simulation;video;conditional probability;computer science;video tracking;motion estimation;mathematics;tracking;statistics;computer graphics (images)	Vision	45.32282944242992	-48.3402954315577	84332
741637b8b6e4fdd30cb40b3881f29c3377eb4749	figure-ground segmentation - object-based		Tracking with a moving camera is a challenging task due to the combined effects of scene activity and egomotion. As there is no longer a static image background from which moving objects can easily be distinguished, dedicated effort must be spent on detecting objects of interest in the input images and on determining their precise extent. In recent years, there has been considerable progress in the development of approaches that apply object detection and class-specific segmentation in order to facilitate tracking under such circumstances (“tracking-by-detection”). In this chapter, we will give an overview of the main concepts and techniques used in such tracking-by-detection systems. In detail, the chapter will present fundamental techniques and current state-of-the-art approaches for performing object detection, for obtaining detailed object segmentations from single images based on top–down and bottom–up cues, and for propagating this information over time.	autostereogram;closed-circuit television;mobile device;object detection;object-based language;sensor;tracking system;visual odometry	Bastian Leibe	2011		10.1007/978-0-85729-997-0_4		Vision	42.16779644190548	-46.614140852700444	84368
021189469321aa6fc34a5ee15f68ab46df8eccfe	estimating the 3d position of humans wearing a reflective vest using a single camera system	datalogi;datavetenskap datalogi;datavetenskap;computer science	This paper presents a novel possible solution for people detection and estimation of their 3D position in challenging shared environments. Addressing safety critical applications in industrial environments, we make the basic assumption that people wear reflective vests. In order to detect these vests and to discriminate them from other reflective material, we propose an approach based on a single camera equipped with an IR flash. The camera acquires pairs of images, one with and one without IR flash, in short succession. The images forming a pair are then related to each other through feature tracking, which allows to discard features for which the relative intensity difference is small and which are thus not believed to belong to a reflective vest. Next, the local neighbourhood of the remaining features is further analysed. First, a Random Forest classifier is used to discriminate between features caused by a reflective vest and features caused by some other reflective materials. Second, the distance between the camera and the vest features is estimated using a Random Forest regressor. The proposed system was evaluated in one indoor and two challenging outdoor scenarios. Our results indicate very good classification performance and remarkably accurate distance estimation especially in combination with the SURF descriptor, even under direct exposure to sunlight.	humans;motion estimation;random forest;safety engineering;speeded up robust features;succession	Rafael Mosberger;Henrik Andreasson	2012		10.1007/978-3-642-40686-7_10	computer vision;simulation;computer science;computer graphics (images)	Vision	44.95126509221037	-43.28232044332351	84459
194e9c36a97c911c4edc754cd059efcc42aa1c46	accurate localization in dense urban area using google street view images		Accurate information about the location and orientation of a camera in mobile devices is central to the utilization of location-based services (LBS). Most of such mobile devices rely on GPS data but this data is subject to inaccuracy due to imperfections in the quality of the signal provided by satellites. This shortcoming has spurred the research into improving the accuracy of localization. Since mobile devices have a camera, a major thrust of this research has been directed at acquiring the local scene and applying image retrieval techniques by querying a GPS-tagged image database to find the best match for the acquired scene. The techniques are however computationally demanding. To overcome the high complexity of those techniques, we investigated the use of inertial sensors as an aid in image-retrieval-based approach. Armed with information of media other than images, such as data from the GPS module along with orientation sensors such as accelerometer and gyro, we sought to limit the number of candidate images that should be considered for finding the best match. Specifically, data from the orientation sensors (heading) along with Dilution of Precision (DOP) from GPS are used to find the angle of view and the estimate of location. We present analysis of the reduction in the image set size for the search as well as simulations to demonstrate the effectiveness in a fast implementation with acceptable location error.	course (navigation);feature extraction;fundamental matrix (computer vision);gyro;global positioning system;google street view;homography (computer vision);image retrieval;internationalization and localization;inverted index;location-based service;mobile device;sensor;simulation;smartphone;tf–idf;thrust;visual odometry	Mahdi Salarian	2015	2015 SAI Intelligent Systems Conference (IntelliSys)		computer vision;computer science;data mining;mobile device;pound;tf–idf	Mobile	50.82369859929588	-44.030055040241145	84531
1a2d951587cd74aabdad4a45547a76b6771639fc	using object interactions to improve particle filter performance	high dimensionality;object interaction;real time;particle filter	This paper describes and evaluates a novel set of approaches to handle situations where multiple distinct and visually differing objects are tracked, such as tracking of people and objects they are manipulating. Unlike tracking of multiple similar objects, visually different interacting objects can provide an opportunity to improve the tracking accuracy. These approaches are designed for use with Condensation/Particle Filter based algorithms, and allow drop-in replacement of tracker modules for each object type tracked. They use information about the relationships and interactions between objects to improve the tracking, rather than in order to distinguish between the objects, as in current algorithms. They are also designed to be highly efficient, for real time use. The approaches are tested on a challenging set of real data and achieve tracking performance similar to using a single very high dimensional tracker, but with vastly reduced complexity and hence much better time performance.	arm architecture;algorithm;black box;correspondence problem;drop-in replacement;interaction;module file;music tracker;object type (object-oriented programming);particle filter;real-time computing;trackball	Joe Marshall;Steven Mills;Steve Benford	2006		10.5244/C.20.35	computer vision;simulation;particle filter;computer science;machine learning	Vision	45.47091994228881	-39.45937399658239	84537
292d453181ca9376a8b2feb2ffc2deedc178f65c	a neuromorphic smart camera for real-time 360°distortion-free panoramas	hierarchical;comparative analysis;image resolution;real time;edge extraction;temporal resolution;optimization;coverage;high dynamic range;high speed;environmental awareness	This paper presents a novel neuromorphic camera system rotating at high-speed (1 to 4 rotations/sec) to acquire 360° panoramas in real-time by exploiting the high temporal resolution, the high dynamic range and the sparse visual information representation using a neuromorphic vision sensor with address-event (AE) signaling mounted on a high-speed mechanical rotation device. Contrary to state-of-the-art panorama cameras (e.g. rotational cameras or catadioptric cameras), this camera system can delivers several distortion-free 360° panoramas per second at constant image resolution and efficient edge extraction of the scene under real illumination conditions without any further computation. This camera system could establish new sensing capabilities in challenging applications such as real-time environmental awareness for robotics and surveillance. After introducing panorama systems and the neuromorphic dual-line dynamic vision sensor, the new camera concept is presented. A comparative analysis of this system with state-of-the art cameras is given. The concept, the camera design and resulting images using an existing 256 pixel line sensor are presented.	computation;distortion;high dynamic range;image resolution;neuromorphic engineering;pixel;qualitative comparative analysis;real-time clock;robotics;smart camera;sparse matrix	Ahmed Nabil Belbachir;Roman P. Pflugfelder;Roman Gmeiner	2010		10.1145/1865987.1866022	smart camera;qualitative comparative analysis;computer vision;camera auto-calibration;camera resectioning;simulation;image resolution;computer science;temporal resolution;hierarchy;computer graphics (images)	Vision	51.335089525544504	-42.17682957926573	84585
e65b751ced5e7a7c3cd1c9b249e0d8e33bd093e8	a practical roadside camera calibration method based on least squares optimization	least squares approximations;video surveillance;least squares method;road traffic;intelligent transportation systems;camera translation vector estimation practical roadside camera calibration method least squares optimization traffic surveillance systems camera intrinsic parameter estimation camera rotation angles;vanishing line;computer vision;roadside camera;vanishing point;video cameras;video surveillance calibration cameras intelligent transportation systems least squares approximations road traffic;traffic surveillance;optimization;camera calibration;least squares ls optimization;cameras calibration roads optimization surveillance estimation vectors;calibration;vanishing point camera calibration least squares ls optimization roadside camera traffic surveillance vanishing line;cameras	In this paper, we propose a more practical and accurate method for calibrating the roadside camera used in traffic surveillance systems. Considering the characteristics of the traffic scenes, we propose a minimum calibration condition that consists of two vanishing points and a vanishing line, which can be easily satisfied in most traffic scenes. Based on the minimum calibration condition, we provide a calibration method to estimate camera intrinsic parameters and rotation angles, which employs least squares optimization instead of closed-form computation. Compared with the existing calibration methods, our method is suitable for more traffic scenes and is able to accurately determine more camera parameters including the principal point. By making full use of video information, multiple observations of the vanishing points are available from different objects. For more accurate calibration, we present a dynamic calibration method using these observations to correct camera parameters. As for the estimation of the camera translation vector, known lengths in the road or known heights above the road are exploited. The experimental results on synthetic data and real traffic images demonstrate the accuracy, robustness, and practicability of the proposed calibration method.	camera resectioning;computation;least squares;mathematical optimization;synthetic data	Yuan Zheng;Silong Peng	2014	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2013.2288353	computer vision;camera auto-calibration;intelligent transportation system;calibration;camera resectioning;simulation;vanishing point;computer science;least squares;computer graphics (images)	Vision	53.74410141893246	-49.525950501330556	84782
b3689ca1e5f25a3009d2d9d0aa71a7de56d087c2	saliency combined particle filtering for aircraft tracking	bayesian estimation;saliency;object tracking;aircraft tracking;uavs;particle filters;target tracking;vision;article	Vision-based aircraft tracking has been considered for emerging real-world applications, such as collision avoidance, air traffic surveillance, and target tracking for military use. However, conventional tracking methods often fail in following aircraft due to 1) variations of object shape, 2) continuously varying background, and 3) unpredictable flight motion. In this paper, we address the problems of vision-based aircraft tracking. To this ends, we propose a principled manner of improving color-based tracking algorithm by combining a biologically inspired saliency feature. More specifically, we exploit the integration of color distributions into particle filtering, which is a Monte Carlo method for general nonlinear filtering problems. To overcome the varying appearances which are usually from changing illumination and pose conditions, we update the target color model. Furthermore, we adopt a structure tensor based saliency algorithm to incorporate the saliency features into particle filter framework, which results in robustly assigning appropriate particle weights even in complex backgrounds. The rationale behind our approach is that color and saliency information are complementary, both mutually fulfilling and completing each other, especially when tracking aircraft in a harsh environment. Tests on real flight sequences reveal that the proposed system yields convincing tracking outcomes under both variations of background and sudden target motion changes.	particle filter	Seungwoo Yoo;Wonjun Kim;Changick Kim	2014	Signal Processing Systems	10.1007/s11265-013-0803-x	vision;computer vision;simulation;bayes estimator;particle filter;computer science;salience;video tracking;control theory;statistics	ML	44.71340671803401	-47.75630160861512	84985
9921a2141484e3e40de7e6cbd9a083bf1de869b3	perceiving clutter and surfaces for object placement in indoor environments	high clutter situation;humanoid robot;low profile object;human environment;histograms;manipulators;clutter;real world clutter;image segmentation;lidar perceiving clutter object placement indoor environment handheld manipulable object flat surface human environment natural clutter low profile object real world clutter laser range finder camera machine learning humanoid robot high clutter situation;data collection;proceedings;natural clutter;perceiving clutter;laser ranging;handheld manipulable object;robot vision cameras humanoid robots image segmentation laser ranging learning artificial intelligence manipulators optical radar radar clutter;robot vision;laser range finder;humanoid robots;machine learning;optical radar;three dimensional displays;image color analysis;clouds;object placement;indoor environment;clutter image color analysis clouds three dimensional displays histograms humanoid robots cameras;post print;cross validation;radar clutter;learning artificial intelligence;flat surface;cameras;camera;lidar	Handheld manipulable objects can often be found on flat surfaces within human environments. Researchers have previously demonstrated that perceptually segmenting a flat surface from the objects resting on it can enable robots to pick and place objects. However, methods for performing this segmentation can fail when applied to scenes with natural clutter. For example, low-profile objects and dense clutter that obscures the underlying surface can complicate the interpretation of the scene. As a first step towards characterizing the statistics of real-world clutter in human environments, we have collected and hand labeled 104 scans of cluttered tables using a tilting laser range finder (LIDAR) and a camera. Within this paper, we describe our method of data collection, present notable statistics from the dataset, and introduce a perceptual algorithm that uses machine learning to discriminate surface from clutter. We also present a method that enables a humanoid robot to place objects on uncluttered parts of flat surfaces using this perceptual algorithm. In cross-validation tests, the perceptual algorithm achieved a correct classification rate of 78.70% for surface and 90.66% for clutter, and outperformed our previously published algorithm. Our humanoid robot succeeded in 16 out of 20 object placing trials on 9 different unaltered tables, and performed successfully in several high-clutter situations. 3 out of 4 failures resulted from placing objects too close to the edge of the table.	algorithm;autonomous robot;clutter;cross-validation (statistics);humanoid robot;machine learning;smt placement equipment	Martin J. Schuster;Jason Okerman;Hai Nguyen;James M. Rehg;Charles C. Kemp	2010	2010 10th IEEE-RAS International Conference on Humanoid Robots	10.1109/ICHR.2010.5686328	computer vision;simulation;computer science;humanoid robot;artificial intelligence	Robotics	51.3940409056173	-40.22319372458417	85398
5e34ab4882649b0d61388d46764bf5b608545fe4	distributed visual processing for a home visual sensor network	personal service robot;home visual sensor network;mobile robot;self localization distributed visual processing home visual sensor network personal service robot intelligent home environment mobile robot visual functionalities action recognition gesture recognition;distributed visual processing;service robots gesture recognition home automation mobile robots robot vision;service robots;self localization;mobile robots;sensor network;robot vision;action recognition;robot sensing systems service robots cameras robot vision systems robot kinematics intelligent robots intelligent sensors intelligent networks home automation human robot interaction;visual functionalities;service robot;sensor nodes;intelligent home environment;visual processing;gesture recognition;home automation	We address issues dealing with distributed visual processing for a personal service robot in the Intelligent Home environment. We propose an efficient and reliable framework to organize and coordinate the vision sensor nodes: fixed cameras mounted on walls, and camera(s) on the mobile robot. We also propose key visual functionalities necessary for the robot to perform its activities. They include people detection and identification, action recognition, gesture recognition, and self-localization. We propose solutions to the different vision tasks, and present our implementation within this framework, validated with experimental results.	gesture recognition;home automation;mobile robot;service robot;visual sensor network	Kwangsu Kim;Gérard G. Medioni	2008	2008 IEEE Workshop on Applications of Computer Vision	10.1109/WACV.2008.4544043	mobile robot;embedded system;computer vision;simulation;computer science;gesture recognition;visual sensor network	Vision	47.277020788101616	-38.32143582075801	85842
2b3cd55d6de1dee524b59a17e311ef8452e7936a	minimal solvers for generalized pose and scale estimation from two rays and one point		Estimating the poses of a moving camera with respect to a known 3D map is a key problem in robotics and Augmented Reality applications. Instead of solving for each pose individually, the trajectory can be considered as a generalized camera. Thus, all poses can be jointly estimated by solving a generalized PnP (gPnP) problem. In this paper, we show that the gPnP problem for camera trajectories permits an extremely efficient minimal solution when exploiting the fact that pose tracking allows us to locally triangulate 3D points. We present a problem formulation based on one point-point and two point-ray correspondences that encompasses both the case where the scale of the trajectory is known and where it is unknown. Our formulation leads to closed-form solutions that are orders of magnitude faster to compute than the current stateof-the-art, while resulting in a similar or better pose accuracy.	augmented reality;estimation theory;generalized linear model;legacy plug and play;robotics;tango	Federico Camposeco;Torsten Sattler;Marc Pollefeys	2016		10.1007/978-3-319-46454-1_13	mathematical optimization;combinatorics;3d pose estimation;mathematics;geometry	Robotics	53.12606824914329	-48.46379566465456	85967
9276720b306f2fc867df2454c51d8c68cd9b8e17	recovering lshgcs and shgcs from stereo	image recognition;interpolation;image segmentation;intelligent robots;3d volumetric descriptions;computational geometry;stereo correspondences;shape measurement;layout;surface texture;computer vision;volumetric shape from stereo;indexing;stereo image processing;curved surfaces;shape models;intelligent systems;shape modeling;linear straight homogeneous generalized cones;computer science;shape models volumetric shape from stereo curved surfaces 3d volumetric descriptions stereo correspondences linear straight homogeneous generalized cones;stereo image processing computational geometry computer vision;layout intelligent robots image recognition image segmentation indexing shape measurement intelligent systems computer science surface texture interpolation	We examine the problem of computing volumetric shape from stereo. We argue that intermediate 2'3-D den.se or wire-fram.e descriptions may not be always possible from stereo, especially when Ihere are curved surfaces in the scene, and that 3 0 volumetric descriptions of objects may h.ave to be derived directly from stereo correspondences. We th,en presen,t m.ethods to recover volumetric shape using LSHGCs and SHGCs as the shape models, based on some invariant properties in their monocular an,d stereo projections. Experimental results on images of objects with curved surfaces are given.		Ronald C.-K. Chung;Ramakant Nevatia	1992		10.1109/CVPR.1992.223229	layout;surface finish;computer vision;search engine indexing;computational geometry;interpolation;computer science;mathematics;geometry;image segmentation;computer graphics (images)	Vision	52.987249920796	-51.67644167919386	86067
46f698dacdb5f76d6b4dae67cb1ae4da2b789398	deformable distributed multiple detector fusion for multi-person tracking		This paper addresses fully automated multi-person tracking in complex environments with challenging occlusion and extensive pose variations. Our solution combines multiple detectors for a set of different regions of interest (e.g., full-body and head) for multi-person tracking. The use of multiple detectors leads to fewer miss detections as it is able to exploit the complementary strengths of the individual detectors. While the number of false positives may increase with the increased number of bounding boxes detected from multiple detectors, we propose to group the detection outputs by bounding box location and depth information. For robustness to significant pose variations, deformable spatial relationship between detectors are learnt in our multi-person tracking system. On RGBD data from a live Intensive Care Unit (ICU), we show that the proposed method significantly improves multi-person tracking performance over state-of-the-art methods.	international components for unicode;loss function;minimum bounding box;mutual exclusion;optimization problem;region of interest;sensor;tracking system	Andy Jinhua Ma;Pong C. Yuen;Suchi Saria	2015	CoRR		computer vision;real-time computing;simulation;computer science	Vision	43.142217181362625	-46.841771220154016	86581
c803dc109544998282e04cde2788d5e10a6ed1fe	a system for automatic judgment of offsides in soccer games	video signal processing;player tracking automatic offside judgment system soccer game multicamera image formation analysis team classification;player tracking;multicamera image;image classification;formation analysis;soccer game;video signal processing image classification image sequences sport tracking;team classification;automatic offside judgment system;spatial relationships;sport;cameras image processing layout computer science image recognition trajectory image analysis image reconstruction multimedia communication tv broadcasting;tracking;image sequences	In this paper, we propose a system for automatic judgment of offsides in soccer games. We detect and track players in fixed multi camera images and calculate the world coordinates of them. Furthermore, we do a formation analysis by classifying uniforms and calculate the position of an offside line. On the other hand, we calculate the 3D coordinates and the trajectories of a ball in world coordinates from the plane coordinates of a ball in multi cameras and recognize the moment of a play from the 3D trajectories of a ball. In addition, we make a judge player's interfering with play by analyzing the spatial relationship between a ball and players. Finally, we make an offside judgment by integrating these results. We apply our system to a real soccer match and demonstrate the availability of this system by showing the experimental results	glossary of computer graphics;terminator 2: judgment day	Sadatsugu Hashimoto;Shinji Ozawa	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262924	computer vision;simulation;computer science;sport;tracking;multimedia	Robotics	42.854293785534935	-47.00863389460469	86597
37bd377e7148c0ab1f5a77ddedb96aaf6eb30dd0	online real boosting for object tracking under severe appearance changes and occlusion	online real boosting;convergence;occlusion;robust control;appearance changes;boosting;incremental learning;particle filter;object tracking;boosting target tracking particle tracking particle filters robustness laboratories robust control visual databases lighting convergence;particle filter framework;appearance changes online real boosting tracking;robustness;lighting;appearance change;particle tracking;particle filters;target tracking;visual tracking;tracking object detection particle filtering numerical methods;tracking;object detection;particle filtering numerical methods;particle filter framework occlusion appearance change object tracking online real boosting visual tracking;visual databases	Robust visual tracking is always a challenging but yet intriguing problem owing to the appearance variability of target objects. In this paper we propose a novel method to handle large changes in appearance based on online real-value boosting, which is utilized to incrementally learn a strong classifier to distinguish between objects and their background. By incorporating online real boosting into a particle filter framework, our tracking algorithm shows a strong adaptability for different target objects which undergo severe appearance changes during the tracking process.	algorithm;boosting (machine learning);particle filter;spatial variability;video tracking	Li Xu;Takayoshi Yamashita;Shihong Lao;Masato Kawade;Feihu Qi	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.366060	computer vision;particle filter;computer science;machine learning;pattern recognition;statistics	Vision	42.89970333860315	-49.332166753614025	86610
1c1a8d4928681f977880c294810ddb40cd8c4bb3	detection of harmful content using multilevel verification in visual sensor data	multilevel verification;harmful content;visual sensor data;average color filter	Various types of harmful content such as adult images and video clips have been increasingly distributed through wired or wireless visual sensor-based networks. In this paper, we propose a new algorithm for extracting human nipple regions, representing the harmfulness of the images, by using a multilevel verification technique in visual sensor-based image data. The proposed algorithm first detects human face regions including eyes and lips from input images. The method then generates a nipple map utilizing representative features that female nipples have and detects candidate nipple regions by applying the generated nipple map to segmented skin regions followed by morphological operations. Subsequently, the proposed method selects real nipple areas after eliminating non-nipple regions at multiple levels by applying geometrical information and an average color filter to the detected candidate nipple regions. Experimental results show that the proposed method can robustly extract female nipple regions in various types of input images captured in environments where certain constraints are not imposed on.	verification and validation	Seok-Woo Jang;Myunghee Jung	2016	Wireless Personal Communications	10.1007/s11277-015-2966-1	computer vision;computer science	HCI	41.79053159872487	-52.07470970118337	86641
5a28bd5443a34245976169758b2ca213b974e17b	robust dense depth maps generations from sparse dvs stereos			dynamic voltage scaling;sparse	Dongqing Zou;Feng Shi;Wei-Heng Liu;Jia Li;Qiang Wang;Paul K. J. Park;Hyunsurk Ryu	2017			computer science;computer vision;artificial intelligence	Theory	52.55196860562073	-44.16991944501852	86687
d43b42948b44ef4c6fca926a141761155a86b888	cooperative neural network background model for multi-modal video surveillance	video surveillance image classification image motion analysis image segmentation neural nets probability;background modeling;video surveillance;image motion analysis;probability;image segmentation;probabilistic neural network pnn multi modal video surveillance motion detection neural network nn;neural nets;fuses;computational modeling adaptation models neurons motion detection educational institutions image color analysis fuses;computer model;pixel classification;image classification;neural network nn;computational modeling;probabilistic neural network pnn;image color analysis;adaptive learning rate;image sequence;neurons;infrared;adaptation models;probabilistic neural network;multi modal video surveillance;motion detection;foreground background;pixel classification cooperative neural network background model multimodal video surveillance probability neural network motion regions segmented pixels frame motion difference;neural network	This paper proposed a new cooperative background model for multi-modal video surveillance based on probability neural network (PNN). Firstly, probability of being foreground was estimated in visible and infrared channel, and post processed separately. Then, every pixel was classified into foreground, background, and change pixels by fusing this information, and foreground pixels were segmented into motion regions. Thirdly, adaptive learning rate was computed for every frame and every pixel based on frame motion difference and pixel classification result, and background model for every channel was updated. Experimental results on well-known benchmark image sequences show that the proposed algorithm can detect motion region more precisely.	artificial neural network;closed-circuit television	Zhiming Wang;Hong Bao	2011		10.1109/CIS.2011.63	fuse;computer vision;contextual image classification;probabilistic neural network;infrared;computer science;machine learning;foreground-background;pattern recognition;probability;image segmentation;computational model;artificial neural network	Vision	39.49040767353729	-48.48822679282002	86742
209bb45fc5e9a0ee14721f911d01ecee042b4804	artificial vision in 3d perspective. for object detection on planes, using points clouds	depth;cluster;segmentation;point cloud;surfaces	"""In this paper, we talk about an algorithm of artificial vision for the robot Golem II + with which to analyze the environment the robot, for the detection of planes and objects in the scene through point clouds, which were captured with kinect device, possible objects and quantity, distance and other characteristics. Subsequently the """"clusters"""" are grouped to identify whether they are located on the same surface, in order to calculate the distance and the slope of the planes relative to the robot, and finally each object separately analyzed to see if it is possible to take them, if they are empty surfaces, may leave objects on them, long as feasible considering a distance, ignoring false positives as the walls and floor, which for these purposes are not of interest since it is not possible to place objects on the walls and floor are out of range of the robot's arms."""	algorithm;coat of arms;computer vision;kinect;machine vision;object detection;point cloud;preprocessor;robot;robotics;sensor;statistical classification	Catalina Alejandra Vázquez Rodriguez;Luis García Tovar	2013	IJCOPI		robot;point cloud;golem;object detection;computer vision;engineering;artificial intelligence	Robotics	50.40578621070677	-40.89536173062491	86815
f274fb13125d6322263cbecec28e762123ffa50b	lidar and panoramic camera extrinsic calibration approach using a pattern plane		Mobile platforms typically combine several data acquisition systems such as lasers, cameras and inertial systems. However the geometrical combination of the different sensors requires their calibration, at least, through the definition of the extrinsic parameters, i.e., the transformation matrices that register all sensors in the same coordinate system. Our system generate an accurate association between platform sensors and the estimated parameters including rotation, translation, focal length, world and sensors reference frame. The extrinsic camera parameters are computed by Zhang’s method using a pattern composed of white rhombus and rhombus holes, and the LIDAR with the results of previous work. Points acquired by the LIDAR are projected into images acquired by the Ladybug cameras. A new calibration pattern, visible to both sensors is used. Correspondence is obtained between each laser point and its position in the image, the texture and color of each point of LIDAR can be know.		Angel-Iván García-Moreno;José-Joel González-Barbosa;Francisco-Javier Ornelas-Rodriguez;Juan-Bautista Hurtado-Ramos;Marco-Neri Primo-Fuentes	2013		10.1007/978-3-642-38989-4_11	computer vision;camera auto-calibration	Vision	52.283844125498675	-41.527303097995464	86955
7024a92186b81e6133dc779f497d06877b48d82b	a coarse-to-fine indoor layout estimation (cfile) method		The task of estimating the spatial layout of cluttered indoor scenes from a single RGB image is addressed in this work. Existing solutions to this problems largely rely on hand-craft features and vanishing lines, and they often fail in highly cluttered indoor rooms. The proposed coarse-to-fine indoor layout estimation (CFILE) method consists of two stages: 1) coarse layout estimation; and 2) fine layout localization. In the first stage, we adopt a fully convolutional neural network (FCN) to obtain a coarse-scale room layout estimate that is close to the ground truth globally. The proposed FCN considers combines the layout contour property and the surface property so as to provide a robust estimate in the presence of cluttered objects. In the second stage, we formulate an optimization framework that enforces several constraints such as layout contour straightness, surface smoothness and geometric constraints for layout detail refinement. Our proposed system offers the state-of-the-art performance on two commonly used benchmark datasets.	artificial neural network;benchmark (computing);computer multitasking;convolutional neural network;ground truth;mathematical optimization;refinement (computing)	Yuzhuo Ren;Chen Chen;Shangwen Li;C.-C. Jay Kuo	2016		10.1007/978-3-319-54193-8_3	computer vision;simulation	Vision	52.52210395775802	-47.63799336227053	86995
52b1ad18c4bf2f8fd858b654a78b2b26e1d90f8b	multi-agent motion tracking using the particle filter in ispace with dinds	motion tracking;particle filter;vision system;object tracking	We present a method for representing, tracking and human following by fusing distributed multiple vision systems in ISpace, with application to pedestrian tracking in a crowd. And the article presents the integration of color distributions into particle filtering. Particle filters provide a robust tracking framework under ambiguity conditions. We propose to track the moving objects by generating hypotheses not in the image plan but on the top-view reconstruction of the scene. Comparative results on real video sequences show the advantage of our method for multi-object tracking. Simulations are carried out to evaluate the proposed performance. Also, the method is applied to the intelligent environment and its performance is verified by the experiments.	particle filter	Tae-Seok Jin;ChangHoon Park;Soo-Hong Park	2006		10.1007/11801603_134	filter;computer vision;simulation;particle filter;machine vision;tracking system;computer science;artificial intelligence;video tracking;motion estimation;tracking;magnetosphere particle motion	Vision	45.9791085514311	-46.5787729213103	87143
294bd27edd680c04cbd098b75763ce0193d4de70	real-time monocular segmentation and pose tracking of multiple objects		We present a real-time system capable of segmenting multiple 3D objects and tracking their pose using a single RGB camera, based on prior shape knowledge. The proposed method uses twist-coordinates for pose parametrization and a pixel-wise second-order optimization approach which lead to major improvements in terms of tracking robustness, especially in cases of fast motion and scale changes, compared to previous region-based approaches. Our implementation runs at about 50–100 Hz on a commodity laptop when tracking a single object without relying on GPGPU computations. We compare our method to the current state of the art in various experiments involving challenging motion sequences and different complex objects.	computation;experiment;general-purpose computing on graphics processing units;image segmentation;laptop;mathematical optimization;pixel;real-time computing;real-time locating system;real-time transcription;sensor;texture mapping	Henning Tjaden;Ulrich Schwanecke;Elmar Schömer	2016		10.1007/978-3-319-46493-0_26	computer vision	Vision	53.056404539064324	-46.290962946993005	87191
71cd1500908f6cb420641666b1772f9d365d36fb	video motion capture by silhouette analysis and pose optimization	motion analysis;human computer interaction video motion capture silhouette analysis pose optimization human motion camera;image motion analysis;human computer interaction;video signal processing;application software;silhouette analysis;skeleton;motion capture;video signal processing image motion analysis image reconstruction;image reconstruction;human motion;video motion capture;robustness;motion analysis humans cameras robustness skeleton user interfaces educational institutions computer science application software calibration;humans;computer science;camera calibration;user interfaces;calibration;3d reconstruction;cameras;pose optimization;camera	Video based 3D reconstruction of human motion plays an important role in many applications. We implement a system that robustly reconstructs 3D human motion from markerless videos taken by a single camera. Our system only requires a desktop PC and a mainstream camera, and doesn't involve complex camera calibration, making it easy to implement and widely accessible in daily uses such as human computer interaction or entertainment.	3d reconstruction;camera resectioning;desktop computer;human computer;human–computer interaction;kinesiology;motion capture;personal computer;shadow volume;silhouette (clustering)	Cheng Chen;Yueting Zhuang;Shicong Zhao;Yin Cheng	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284565	3d reconstruction;iterative reconstruction;smart camera;computer vision;camera auto-calibration;application software;motion capture;calibration;camera resectioning;simulation;computer science;programming language;user interface;skeleton;robustness;computer graphics (images)	Vision	47.952747915051866	-44.05113272628843	87311
4605973c2f8051ec80cf9c06cb435b10cb095dc9	rolling shutter pose and ego-motion estimation using shape-from-template		We propose a new method for the absolute camera pose problem (PnP) which handles Rolling Shutter (RS) effects. Unlike all existing methods which perform 3D-2D registration after augmenting the Global Shutter (GS) projection model with the velocity parameters under various kinematic models, we propose to use local differential constraints. These are established by drawing an analogy with Shape-from-Template (SfT). The main idea consists in considering that RS distortions due to camera ego-motion during image acquisition can be interpreted as virtual deformations of a template captured by a GS camera. Once the virtual deformations have been recovered using SfT, the camera pose and egomotion are computed by registering the deformed scene on the original template. This 3D-3D registration involves a 3D cost function based on the Euclidean point distance, more physically meaningful than the reprojection error or the algebraic distance based cost functions used in previous work. Results on both synthetic and real data show that the proposed method outperforms existing RS pose estimation techniques in terms of accuracy and stability of performance in various configurations.	3d pose estimation;aura;distortion;image registration;loss function;motion estimation;movie projector;reed–solomon error correction;reprojection error;roland gs;synthetic intelligence;velocity (software development);virtual reality headset;visual odometry	Yizhen Lao;Omar Ait-Aider;Adrien Bartoli	2018		10.1007/978-3-030-01216-8_29	shutter;id, ego and super-ego;rolling shutter;computer vision;motion estimation;artificial intelligence;computer science;euclidean geometry;analogy;pose;kinematics	Vision	53.35068662557698	-49.270329657744774	87408
686c6579e328c40da82dd7abb0fb0b18d265fbfe	rklt: 8 dof real-time robust video tracking combing coarse ransac features and accurate fast template registration	convergence;evaluation methods registration based tracker ransac klt;image sequence rklt 8dof homograph model video tracking ransac feature template registration motion state estimation object image translation object tracking inverse compositional tracker lena convergence benchmark;ransac;evaluation methods;klt;accuracy;integrated circuits robustness target tracking accuracy convergence robots;robots;robustness;target tracking;registration based tracker;integrated circuits;video surveillance feature extraction image registration image sequences iterative methods motion estimation object tracking state estimation	The performance of a tracker can be measured by two often conflicting criteria - robustness and accuracy. Recently researchers have focused on improving robustness, using adaptive appearance models. However updating the appearance model can cause drift and lower the accuracy of motion (state) estimation. These trackers generally compute 2 degree of freedom(DOF) image translation of the object, and are suited for applications such as surveillance. In contrast, we are interested in tracking objects using high DOF motion models - especially 8DOF homograph models that allow tracking of precise state information (projective 8D or calibrated 3Dworld translations and 3D rotations of the tracked object). Such precise state is required for visual motion control of e.g. robot arms, hands and UAV. To this end, we propose a novel tracking algorithm that combines KLT [8], RANSAC [21] and Inverse Compositional tracker [7]. First we sample a large patch into a set of small patches and track each one using frame-to-frame 2D KLT trackers. An 8 DOF homograph describing the large patch motion is then estimated from the current locations of these KLT trackers using RANSAC while also discarding lost trackers as outliers. Finally, using the RANSAC 8DOF motion estimate as the initial guess, we perform a few iterations of an IC registration tracker. This refines the patch motion to sub-pixel accuracy and avoids drift by registering to the original template. We perform three sets of experiments - one is the standard synthetic Lena convergence benchmark and two use real image sequences from recent datasets - to show that our tracker compares favourably with the state-of-the-art.	algorithm;approximation;benchmark (computing);coat of arms;consistency model;dhrystone;experiment;feature detection (web development);homography (computer vision);iteration;kanade–lucas–tomasi feature tracker;least squares;lenna;numerical analysis;pixel;random sample consensus;real-time transcription;robustness (computer science);similarity measure;unmanned aerial vehicle;video tracking	Xi Zhang;Abhineet Singh;Martin Jägersand	2015	2015 12th Conference on Computer and Robot Vision	10.1109/CRV.2015.18	robot;computer vision;ransac;simulation;convergence;computer science;evaluation;machine learning;kanade–lucas–tomasi feature tracker;accuracy and precision;robustness;computer graphics (images)	Vision	53.53298064453622	-41.13689954406814	87582
1b8753221b5acdc563250730b9868a43365fac9d	background estimation method with incremental iterative re-weighted least squares		The basic steps for computer vision-based automatic video analysis are to detect and track objects. In order to do these steps, the most important and commonly used methods are background subtraction methods. This paper proposes a novel background subtraction method, which is a member of estimation-based background model, involving robust regression technique. The method proposed can estimate backgrounds at enough precision even when there are foreground objects stationary for a long time, which is often the case in images belonging to urban traffic cameras. The method has been tested with existing datasets in the literature and proved its success compared with other known methods. Moreover, it has been tested also with the dataset prepared during this research, which involves images where vehicles stop in different periods and then move again.	iterative method;least squares	Muhammet Balcilar;A. Coskun Sonmez	2016	Signal, Image and Video Processing	10.1007/s11760-014-0705-9	computer vision;simulation;background subtraction;computer science;statistics	Vision	44.832454584647785	-46.51715572882786	87639
4a12225f27e968423225af578546ea3f0c901025	a factorization method in stereo motion for non-rigid objects	measurement matrix;image motion analysis;factorization based nonrigid shape modeling;singular value decomposition;data capture;stereo correspondences;stereo vision matrix decomposition shape measurement image reconstruction layout singular value decomposition geometry motion measurement tracking cameras;stereo rig;indexing terms;camera geometry;rigid motion;motion correspondences;stereo image processing image motion analysis image reconstruction image sequences singular value decomposition;configuration weights;factorization method;image reconstruction;stereo vision factorization method nonrigid objects factorization based nonrigid shape modeling measurement matrix stereo motion data stereo rig singular value decomposition 3d basis shapes configuration weights rigid motion camera geometry motion correspondences stereo correspondences rank constraints image sequences visual tracking;stereo vision visual tracking;nonrigid objects;image sequence;stereo image processing;stereo vision;3d basis shapes;rank constraints;shape modeling;stereo motion data;visual tracking;image sequences	In this paper we propose a framework of factorization-based non-rigid shape modeling and tracking in stereo-motion. We construct a measurement matrix with the stereo-motion data captured from a stereo-rig. Organized in a particular way this matrix could be decomposed by singular value decomposition (SVD) into the 3D basis shapes, their configuration weights, rigid motion and camera geometry. Accordingly, the stereo correspondences can be inferred from motion correspondences only requiring that a minimum of 3K point stereo correspondences (where K is the dimension of shape basis space) are created in advance. Basically this framework still keeps the property of rank constraints, meanwhile it owns other advantages such as simpler correspondence and accurate reconstruction even with short image sequences. Results with real data are given to demonstrate its performance.	singular value decomposition	Yu Huang;Jilin Tu;Thomas S. Huang	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517797	iterative reconstruction;euclidean group;computer vision;index term;eye tracking;computer science;stereopsis;automatic identification and data capture;mathematics;geometry;singular value decomposition;computer graphics (images)	Vision	53.1730594502526	-50.32920629834741	87760
c4789e04841f1ee36c008c2ab3502233521bd867	cylindrical object reconstruction with a moving camera embedded in a poor robotic platform	extended kalman filter;control system;kalman filter;surface reconstruction	This paper presents a method intended to reconstruct a scene composed of cylindrical objects, and to simultaneously estimate the position of the moving camera used to acquire the image sequence. The iterated extended Kalman filter, used to perform this task, is supplied with the discrete sequence of monocular images of the scene and a poor n priort knowledge of the camera motion between successive shooting positions. Two real scene reconstructions are presented. The first, conducted with the accurate IRlSA robotic platform, was planned to assess our image processing module. The second was performed to evaluate tlie estimator performances in more realistic running conditions (i.e. with a noisy a prtorr knowledge of the camera motion). 2 I N T R O D U C T I O N One of the major aims in computer vision is to extract scene structure information from a monocular image sequence. This information can be used in many applications, sucli as object recogniticn, motion planning, ven1ot.e controlled operations . . . There are two paradigms' for the computation of motion from an image sequence. The first is based on extracting, and tracking along the image sequence, a set of relatively sparse, but easily distinguishable 2-D features corresponding to 3-D object features of the scene (corners, surface markings, surface edges, etc). The second paradigm is based on computing the optical flow or the 2-D velocity field of some image patterns. Then, the 2-D features displacements or the optical flow is used to evaluate the camera motion. These two paradigms have shown accurate and robust results in the case of polyhedric scene^.^,^ However, most approaches based on features or optical flow fail as regards scenes with curved obiects. These met.hods use the basic hypothesis that object contours are viewpoint-independent. Unfortunately, this is not true for apparent contours of curved objects. For regular curved objects, an apparent contour corresponds to the projection of the curve for which the surface normals are perpendicular to the line of sight. Evolution of apparent contours in a sequence of images have already been studied by several a ~ t h o r s . ~ , ~ ~ ~ ~ ' Most of the proposed curved surface reconstruction methods require an accurate estimation of the camera motion, and sometimes use a camera model more simple than the standard perspective projection. Moreover, in the case of solids of revolution, no approach has taken advantage, as far as we know, of a left-right limb matching to improve the estimation perforniances. The proposed method consists in analyzing the dynamic of apparent contours in a sequence of monocular images. It is based on the discrete feature paradigm and on a cylindrical model to represent objects in a static scene. It enables the simultaneous estimation of the camera motion and cylindrical object parameters. In this work, the camera is considered to be embedded in a robotic platform, the control system of which provides a camera motion a prioriestimation with an error up to 20 percents. No assumptions are made concerning the cylindrical scene: number of cylinders, radii,. . .The matching of apparent contours in the sequence is performed by a visual module not considered herein. The dynamic and nonlinear features of the system, as well as the computation time burden led us to use the nonlinear Kalman filter (socalled extended Kalman filter). This research can be related to the work of Broida e l al.,' who presented a method for estimating the kinematics and structure of a rigid object from a long sequence of images.	3d projection;computation;computer vision;control system;embedded system;extended kalman filter;image processing;iteration;motion planning;nonlinear system;normal (geometry);optical flow;performance;programming paradigm;robot;scene graph;sparse matrix;time complexity;velocity (software development)	Marc Viala;Christian Faye;Jean-Pierre Guerin;Didier Juvin	1992				Robotics	52.868354246885424	-40.88986080666133	87781
c152df22867d0e3e242878c8551489903290e530	large scale indoor 3d mapping using rgb-d sensor		3D Mapping using RBG-D sensor is a hot topic in the robotic field. This paper proposes a sub-map stitching method to build map in the large scale indoor environment. We design a special landmark, and place it in the environment. Every sub-map contains those landmarks, and then can be stitched by BA optimization. The result shows that the map error is blow 1 % in a room with the dimensions of 13 m × 8 m.		Xiaoxiao Zhu;Qixin Cao;Hiroshi Yokoi;Yinlai Jiang	2016		10.1007/978-3-319-43506-0_27	engineering;computer vision;control engineering;rgb color model;image stitching;artificial intelligence	Robotics	52.78288752619142	-42.49707192962212	88073
045c178a3ce84e3693cc33d862f8652cc1700f48	contextual classification of 3d laser points with conditional random fields in urban environments	image segmentation;remotely operated vehicles feature extraction graph theory image classification image representation image restoration image segmentation image texture natural scenes;three dimensional displays image segmentation feature extraction solid modeling navigation real time systems computational modeling;navigation;computational modeling;three dimensional displays;feature extraction;solid modeling;kaist dataset 3d laser point contextual classification online 3d point cloud classification unmanned ground vehicle ugv multiple laser scanners 2d image representation optimal bearing angle model oba model image blurring on the fly navigation 3dslic based superpixel segmentation crf graph node 29 dimensional feature extraction dut2 dataset;real time systems	Online 3D point cloud classification and scene understanding are crucial tasks for Unmanned Ground Vehicles (UGVs) equipped with multiple laser scanners. Due to the poor performance of traditional 2D image representation model for 3D point clouds, a novel Optimal Bearing Angle (OBA) model is therefore proposed to overcome the limitations of texture information losing and image blurring caused by the UGV's on-the-fly navigation. With the result of 3DSLIC based super-pixel segmentation in OBA images, the center of points belonging to each segmented OBA image patch is assigned as CRF graph node, so that a simplified CRF graph structure is constructed for online contextual classification of 3D laser points in urban environments. Moreover, total 29-dimensional features are extracted from both the raw 3D laser points and the corresponding OBA images. A large number of urban scenes selected from both DUT2 dataset and KAIST dataset are used as testing data in our experiments, and the results show the validity and performance of the proposed method.	algorithmic efficiency;business architecture;computation;conditional random field;experiment;graph (discrete mathematics);out-of-band agreement;pixel;point cloud;range imaging;robustness (computer science);unmanned aerial vehicle	Yan Zhuang;Yisha Liu;Guojian He;Wei David Wang	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353927	image texture;computer vision;navigation;feature detection;feature extraction;computer science;machine learning;pattern recognition;image segmentation;solid modeling;computational model	Robotics	43.42518546935161	-44.516176459950536	88157
1c07207e20b97f8beb9770ebd30301a4eb34584f	an occlusion metric for selecting robust camera configurations	moving object;quality metric;tracking system;motion capture;probabilistic model	Vision based tracking systems for surveillance and motion capture rely on a set of cameras to sense the environment. The exact placement or configuration of these cameras can have a profound affect on the quality of tracking which is achievable. Although several factors contribute, occlusion due to moving objects within the scene itself is often the dominant source of tracking error. This work introduces a configuration quality metric based on the likelihood of dynamic occlusion. Since the exact geometry of occluders can not be known a priori, we use a probabilistic model of occlusion. This model is extensively evaluated experimentally using hundreds of different camera configurations and found to correlate very closely with the actual probability of feature occlusion.	experiment;motion capture;statistical model;tracking system	Xing Chen;James Davis	2007	Machine Vision and Applications	10.1007/s00138-007-0094-y	statistical model;computer vision;motion capture;simulation;tracking system;computer science;mathematics	Vision	51.95251571869816	-45.0317437398072	88318
d4c7be7ca90fbe39b0b1ff4b651a4646ce3d4bc1	robust struck tracker via color haar-like feature and selective updating		Recently, Struck—a tracker based on structured support vector machine, received great attention as a consequence of its superior performance on many challenging scenes. In this work, we present an improved Struck tracker by using color Haar-like features and effective selective updating. First, we integrate color information into Haar-like features in a simple way, which models the spatial and color information simultaneously without increasing the computational complexity. Second, we make selective model updates according to the tracking status of the object. This prevents inferior patterns resulted by occlusions, abrupt appearance or illumination changes from being added to object model, which decreases the risk of model drift problem. The experimental results indicate that the proposed tracking algorithm outperforms the original Struck by a remarkable margin in precision and accuracy, and it is competitive with other state-of-the-art trackers on a tracking benchmark of 50 challenging sequences.	haar wavelet	Shaojie Jiang;Jifeng Ning;Cheng Cai;Yunsong Li	2017	Signal, Image and Video Processing	10.1007/s11760-017-1059-x	computer vision;simulation;computer graphics (images)	Vision	42.22371323177555	-48.821066868549266	88539
625cdd2f864d5336b9c90710dec5145b1810eef2	a best view selection in meetings through attention analysis using a multi-camera network	activity analysis;video streaming;ambient intelligence;computer vision;technology and engineering;video cameras;meeting analysis;smart distributed cameras;sensor fusion;video communication;human activity recognition;pose estimation	Human activity analysis is an essential task in ambient intelligence and computer vision. The main focus lies in the automatic analysis of ongoing activities from a multi-camera network. One possible application is meeting analysis which explores the dynamics in meetings using low-level data and inferring high-level activities. However, the detection of such activities is still very challenging due to the often corrupted or imprecise low-level data. In this paper, we present an approach to understand the dynamics in meetings using a multi-camera network, consisting of fixed ambient and portable close-up cameras. As a particular application we are aiming to find the most informative video stream, for example as a representative view for a remote participant. Our contribution is threefold: at first, we estimate the extrinsic parameters of the portable close-up cameras based on head positions. Secondly, we find common overlapping areas based on the consensus of people's orientation. And thirdly, the most informative view for a remote participant is estimated using common overlapping areas. We evaluated our proposed approach and compared it to a motion estimation method. Experimental results show that we can reach an accuracy of 74% compared to manually selected views.	ambient intelligence;camera resectioning;computer vision;high- and low-level;information;motion estimation;streaming media	Sebastian Gruenwedel;Xingzhe Xie;Wilfried Philips;Chih-Wei Chen;Hamid K. Aghajan	2012	2012 Sixth International Conference on Distributed Smart Cameras (ICDSC)		embedded system;computer vision;simulation;pose;ambient intelligence;computer science;machine learning;sensor fusion;multimedia	Vision	47.70327659136073	-40.40828325874134	88662
dde6b987caa3aaf04ccff737e360bb46d40bb41c	obstacle detection by direct estimation of multiple motion and scene structure from a moving stereo rig	pattern clustering;obstacle detection;image segmentation;real time;motion estimation;cluster analysis;obstacle avoidance;image sequence;stereo image processing;stereo vision;parameter space;optical flow;collision avoidance;geometric constraints;structure and motion;image sequences obstacle detection multiple image motion estimation method scene structure stereo rig real time collision obstacle avoidance stereo vision optical flow reliability motion parameter space cluster analysis motion segmentation object motion;direct method;motion detection motion estimation layout image motion analysis stereo vision geometrical optics optical variables control photometry vehicles motion analysis;image segmentation collision avoidance motion estimation image sequences pattern clustering stereo image processing;image sequences	In the context ofreal-time collision /obstacle avoidance this paper investigates a direct -method of estimating dense scene structure and image motion. Our proposed method combines both stereo vision (invoIving geometric constraints) and optical flow (involving photometric constraints). It is applied to multiple motion images (i.e., images with vehicle ego-motion plus obstacle motion). Modijications to the direct method were introduced to improve the reliabili$ of the motion estimation. The mult~le-motion-estimation scheme comprises local motion estimations followed by a classification of these estimates in a simplified motion parameter space through cluster analysis. The classification enables segmentation of the dyerent motions that are used to estimate more accurately'.the dense structure and ,motion of objects in a scene. Experimental results on both synthetic and real image sequences demonstrate the potential ofthe method.	cluster analysis;direct method in the calculus of variations;motion estimation;optical flow;stereopsis;synthetic intelligence	Christophe Y. Vincent;Tardi Tjahjadi	2003		10.1109/ICSMC.2003.1244231	direct method;computer vision;match moving;structure from motion;simulation;quarter-pixel motion;computer science;stereopsis;motion estimation;optical flow;obstacle avoidance;image segmentation;cluster analysis;parameter space;motion field	Vision	49.18400939714687	-49.23236468331876	88669
72fb3a5823d3d0133a4719bf0362c84d78c0096c	real-time approach for adaptive object segmentation in time-of-flight sensors	kernel;time of flight;image segmentation;3d object tracking;real time tracking;sensors;real time;adaptive object segmentation;object segmentation;depth sensing;depth sensing depth segmentation 3d object tracking time of flight sensor;a priori knowledge;image edge detection;range image;tracking image segmentation object detection sensors;pixel;object tracking;depth segmentation;real time tracking adaptive object segmentation time of flight sensors adaptive depth segmentation method;time of flight sensors;high speed;time of flight sensor;tracking;object detection;adaptive depth segmentation method;object segmentation image segmentation image edge detection layout iterative algorithms application software robustness computer vision machine vision partitioning algorithms;real time systems	In this paper, a high-speed, adaptive depth segmentation method is proposed, which results in superior performance over current employed segmentation algorithms when applied in real-time tracking applications. Existing segmentation methods are difficult to implement in real-time due to their slow performance, whereby enhancing their run-time they are better applicable in real-time approaches, i.e., tracking. The proposed method leverages the depth distributions of range images for segmentation of objects of interest, without having a priori knowledge about the scene. This approach has been tested with real data in unconstrained environments, under varying conditions. The experimental results demonstrate the speed efficiency, as well as robustness of the proposed technique.	algorithm;edge detection;iteration;iterative method;real-time clock;real-time computing;real-time locating system;real-time transcription;requirement;sensor	Ehsan Parvizi;Q. M. Jonathan Wu	2008	2008 20th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2008.139	computer vision;time of flight;kernel;a priori and a posteriori;simulation;computer science;sensor;segmentation-based object categorization;video tracking;tracking;image segmentation;scale-space segmentation;pixel;computer graphics (images)	Robotics	45.29536513116074	-45.91130401912674	88675
93b2bb6b9e79fb65d7b4560a68996a5dbd3acfd8	registration of infrared and visible-spectrum imagery for face recognition	infrared imaging face recognition cameras registers humans head optical computing calibration skin temperature distribution;image motion analysis;gaussian mixture;parameter estimation face recognition image motion analysis principal component analysis infrared imaging;gaussian mixtures;skin;skin temperature;optical computing;spectrum;infra red;data uncertainty;pca model;face recognition;vanishing point;infrared imaging;registers;infrared spectrum imagery;principal component analysis;3d human head pose;humans;head;synthetic data;parameter estimation;facial expression;infrared;infrared camera;levenberg marquardt method;calibration;temperature distribution;cameras;visible spectrum imagery;pca model visible spectrum imagery infrared spectrum imagery face recognition 3d human head pose infrared camera gaussian mixtures	A method is proposed to register visible and infrared face images. The vanishing-point based approach [J.G. Wang and E. Sung, 2001] is applied to the visible image to determine the 3D pose of the human head. Then the corresponding pose with respect to the infrared camera is computed through the known relationship by calibration between the visible and infrared cameras. By doing so, the skin temperature range within the infrared image can be superimposed over the visible face image. We use the EM strategy to first compute the 3D pose using some initially learned (PCA) model parameters, and then update iteratively the parameters for individual persons and their facial expressions till convergence. The EM technique models data uncertainty using Gaussian mixtures defined over positions and orientation of facial plane. The resulting weighted parameters estimation problem is solved using the Levenberg-Marquardt method. The results on the synthetic data and real images have verified the performance of the approach.	estimation theory;facial recognition system;levenberg–marquardt algorithm;pose (computer vision);synthetic data	Jian-Gang Wang;Eric Sung;Ronda Venkateswarlu	2004	Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.	10.1109/AFGR.2004.1301605	facial recognition system;computer vision;infrared;computer science	Vision	47.974628897414	-45.163020290021066	88698
1954e35f3723fa44969bac9bc9b374269219d5ed	automatic planar shape segmentation from indoor point clouds	planer shape segmentation;scanned point clouds;gaussian map;indoor scene modeling	The use of a terrestrial laser scanner (TLS) has become a popular technique for the acquisition of 3D scenes in architecture and design. Surface reconstruction is used to generate a digital model from the acquired point clouds. However, the model often consists of excessive data, limiting real-time user experiences that make use of the model. In this study, we present a coarse to fine planar shape segmentation method for indoor point clouds, which results in the digital model of an indoor scene being represented by a small number of planar patches. First, the Gaussian map and region growing techniques are used to coarsely segment the planar shape from sampled point clouds. Then, the best-fit-plane is calculated by random sample consensus (RANSAC), avoiding the negative impact of outliers. Finally, the refinement of planar shape is produced by projecting point clouds onto the corresponding bestfit-plane. Our method has been demonstrated to be robust towards noise and outliers in the scanned point clouds and overcomes the limitations of over- and under-segmentation. We have tested our system and algorithms on real datasets and experiments show the reliability of the proposed method against existing region-growing methods.	algorithm;curve fitting;experiment;point cloud;random sample consensus;real-time transcription;refinement (computing);region growing;terrestrial television	Wuyang Shui;Jin Liu;Pu Ren;Steve Maddock;Mingquan Zhou	2016		10.1145/3013971.3014008	computer vision;simulation;computer graphics (images)	Vision	53.558408201244575	-43.733018864181766	88706
1663568ea85cf094a11c111be695defcae5710af	articulated human body pose tracking by suppression based immune particle filter	target tracking optimisation particle filtering numerical methods pose estimation stochastic processes;optimization articulated human body pose tracking suppression immune particle filter stochastic tracker object tracking;optimisation;articulated human body pose tracking;stochastic tracker;atmospheric measurements;particle measurements;pose tracking;immune particle filter;stochastic processes;particle filter;human body;object tracking;immune system;optimization;particle filters optimization particle measurements atmospheric measurements immune system target tracking;particle filters;target tracking;optimal algorithm;particle filtering numerical methods;suppression pose tracking particle filter;suppression;pose estimation	Particle filter is a popular stochastic tracker for object tracking. In articulated human body pose tracking, lots of work focuses on increasing sampling efficiency by incorporating optimization algorithm into particle filter. In this study, we propose a modified optimization based particle filter algorithm for pose tracking. The new algorithm can maintain the diversity of particle set by using a suppression scheme. Experimental results show that the proposed method can cope with multi-modality and can obtain more accurate estimation than other optimization based particle filter methods.	mathematical optimization;modality (human–computer interaction);multimodal interaction;particle filter;particle swarm optimization;peterson's algorithm;sampling (signal processing);state space;zero suppression	Min Jiang;Jinshan Tang;Li Chen;Tao Shang;Zhaohui Gan;Xiaoming Liu;Qin Xu	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5652190	adaptive filter;stochastic process;monte carlo localization;computer vision;particle filter;auxiliary particle filter;computer science;control theory;statistics	Robotics	46.57161204735974	-48.081504975134244	88908
2e99468757d66373710f7a5729f9704099c57123	estimating the pose and motion of a known object for real-time robotic tracking	moving object;three dimensions;real time;robot arm	An approach for estimating the pose and motion of a known moving object in three dimensions from a sequence of monocular images is considered. The principle is t o obtain initial estimates of the pose and motion parameters and to update them by using feature location measurements made from subsequent monocular ima.ge frames. The ultimate goal is to use the obtained estimates for controlling the movements of a robot arm.	computation;experiment;image analysis;kalman filter;online and offline;real-time locating system;robot;robotic arm;sampling (signal processing);velocity (software development)	Olli Silvén	1990			three-dimensional space;computer vision;simulation;pose;robotic arm;3d pose estimation;geodesy;computer science	Robotics	53.22089409470973	-39.94785507197287	89088
657b02cb2232f1d2353182d20599d6c10dd87f71	orthogonal-blendshape-based editing system for facial motion capture data	eigenvalues and eigenfunctions;eyebrows;solid modelling eigenvalues and eigenfunctions face recognition image motion analysis image sequences principal component analysis;pca decomposition;motion propagation motion editing blendshape animation facial animation motion capture data driven;image motion analysis;blendshape interference minimization;motion propagation;blendshape animation;motion capture data;eigenvector;computer graphics computer simulation face facial expression humans imaging three dimensional information storage and retrieval models biological software user computer interface;3d facial motion capture editing system;motion capture;face recognition;three dimensional displays;motion editing;image sequence 3d facial motion capture editing system data driven system orthogonal blendshape face model constrained weight propagation region based principle component analysis pca decomposition eigenvector anatomical region blendshape interference minimization;principal component analysis;solid modeling;image sequence;animation;orthogonal blendshape face model;facial animation;anatomical region;face modeling;principal component analysis facial animation motion analysis interference robustness costs application software biological system modeling humans face;face;data driven system;region based principle component analysis;constrained weight propagation;data driven;solid modelling;eigenvectors;image sequences	In this work, we present a data-driven 3D facial motion capture editing system that uses the automated construction of an orthogonal-blendshape face model and constrained weight propagation. Given a collected facial motion capture data set, we start by performing a region-based principal component analysis (PCA) decomposition and constructing a truncated PCA space spanned by the largest eigenvector for each anatomical region of the human face ( for example, the left eyebrow). We then construct an orthogonal-blendshape face model as follows: each eigenvector of an anatomical region corresponds to a blendshape basis, so we regard its PCA coefficient as its blendshape weight. Our orthogonal model also minimizes the blendshape interference issue that can affect the efficiency of the overall approach.Finally, we transform each frame of a new facial motion capture sequence into blendshape weights by projecting it into the PCA spaces mentioned earlier on a per-region basis. As such, modifying the blendshape weights (PCA coefficients) is equivalent to editing the underlying motion capture sequence.	catastrophic interference;coefficient;facial motion capture;interference (communication);largest;principal component analysis;software propagation	Qing Li;Zhigang Deng	2008	IEEE Computer Graphics and Applications	10.1109/MCG.2008.120	facial recognition system;computer vision;speech recognition;eigenvalues and eigenvectors;computer science;geometry;computer graphics (images)	Vision	47.919161948343664	-51.90753221944466	89183
ed7f0f415d7538094e617636204e6b2d7cea9e45	an original application of image recognition based location in complex indoor environments	database;rgb d images;image recognition bases location;mobile computing;indoor positioning;lidar;image retrieval	This paper describes the first results of an image recognition based location (IRBL) for a mobile application focusing on the procedure to generate a database of range images (RGB-D). In an indoor environment, to estimate the camera position and orientation, a prior spatial knowledge of the surroundings is needed. To achieve this objective, a complete 3D survey of two different environments (Bangbae metro station of Seoul and the Electronic and Telecommunications Research Institute (ETRI) building in Daejeon, Republic of Korea) was performed using a LiDAR (Light Detection and Ranging) instrument, and the obtained scans were processed to obtain a spatial model of the environments. From this, two databases of reference images were generated using specific software realised by the Geomatics group of Politecnico di Torino (ScanToRGBDImage). This tool allows us to generate synthetically different RGB-D images centred in each scan position in the environment. Later, the external parameters (X, Y, Z, ω, φ, and κ) and the range information extracted from the retrieved database images are used as reference information for pose estimation of a set of acquired mobile pictures in the IRBL procedure. In this paper, the survey operations, the approach for generating the RGB-D images, and the IRB strategy are reported. Finally, the analysis of the results and the validation test are described.	asea irb;acceptance testing;computer vision;database;geomatics;mobile app	Filiberto Chiabrando;Vincenzo Di Pietra;Andrea Lingua;Youngsu Cho;Ju-Il Jeon	2017	ISPRS Int. J. Geo-Information	10.3390/ijgi6020056	computer vision;simulation;geography;remote sensing	Robotics	51.69581509371426	-41.17243469722053	89199
e42bb0e2acfc936067535934d14f166f13e540e3	automated exterior inspection of an aircraft with a pan-tilt-zoom camera mounted on a mobile robot	sensors;zoom lenses;inspection;cameras	This paper deals with an automated preflight aircraft inspection using a pan-tilt-zoom camera mounted on a mobile robot moving autonomously around the aircraft. The general topic is image processing framework for detection and exterior inspection of different types of items, such as closed or unlatched door, mechanical defect on the engine, the integrity of the empennage, or damage caused by impacts or cracks. The detection step allows to focus on the regions of interest and point the camera toward the item to be checked. It is based on the detection of regular shapes, such as rounded corner rectangles, circles, and ellipses. The inspection task relies on clues, such as uniformity of isolated image regions, convexity of segmented shapes, and periodicity of the image intensity signal. The approach is applied to the inspection of four items of Airbus A320: oxygen bay handle, air-inlet vent, static ports, and fan blades. The results are promising and demonstrate the feasibility of an automated exterior inspection. © 2015 SPIE and IS&T [DOI: 10.1117/1.JEI.24.6.061110]	algorithm;binary classification;circuit complexity;computer fan;convex function;heuristic (computer science);illumination (image);image processing;mega man zx;mobile robot;pan–tilt–zoom camera;quasiperiodicity;region of interest;sensor;software bug;usability	Igor Jovancevic;Stanislas Larnier;Jean-José Orteu;Thierry Sentenac	2015	J. Electronic Imaging	10.1117/1.JEI.24.6.061110	embedded system;computer vision;inspection;sensor	Robotics	49.896910714568776	-40.77416282764375	89225
29de6b64f81f6240931336a3f6f33d3431645853	generalized weiszfeld algorithms for lq optimization	biological patents;optimisation;iterative methods optimisation;generalized weiszfeld algorithms;biomedical journals;cost function;text mining;multiple rotation averaging;multiple rotation averaging generalized weiszfeld algorithms lq optimization lq mean single rotation averaging;europe pubmed central;generalized weiszfeld algorithms lq optimization lq mean single rotation averaging multiple rotation averaging;optimisation iterative methods;citation search;single rotation averaging;weiszfeld algorithm;citation networks;journal article;computer vision;lq mean;iterative methods;computational modeling;research articles;abstracts;open access;life sciences;computer vision optimization computational modeling cost function measurement errors;lq optimization;clinical guidelines;optimization;full text;rest apis;rotation averaging;orcids;measurement errors;europe pmc;biomedical research;bioinformatics;literature search	In many computer vision applications, a desired model of some type is computed by minimizing a cost function based on several measurements. Typically, one may compute the model that minimizes the L2 cost, that is the sum of squares of measurement errors with respect to the model. However, the Lq solution which minimizes the sum of the qth power of errors usually gives more robust results in the presence of outliers for some values of q, for example, q = 1. The Weiszfeld algorithm is a classic algorithm for finding the geometric L1 mean of a set of points in Euclidean space. It is provably optimal and requires neither differentiation, nor line search. The Weiszfeld algorithm has also been generalized to find the L1 mean of a set of points on a Riemannian manifold of non-negative curvature. This paper shows that the Weiszfeld approach may be extended to a wide variety of problems to find an Lq mean for 1 ≤ q <; 2, while maintaining simplicity and provable convergence. We apply this problem to both single-rotation averaging (under which the algorithm provably finds the global Lq optimum) and multiple rotation averaging (for which no such proof exists). Experimental results of Lq optimization for rotations show the improved reliability and robustness compared to L2 optimization.	algorithm;computer vision;convergence (action);geometric median;line search;loss function;mathematical optimization;optimization problem;provable security;manifold	Khurrum Aftab;Richard I. Hartley;Jochen Trumpf	2015	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2014.2353625	computer vision;mathematical optimization;combinatorics;text mining;computer science;theoretical computer science;machine learning;mathematics;geometry;iterative method;computational model;statistics;observational error	Vision	50.6567503824881	-51.25087568741988	89243
20c71ee8275969a7df881de69b8d8baf88f1d120	a variational observation model of 3d object for probabilistic semantic slam		We present a Bayesian object observation model for complete probabilistic semantic SLAM. Recent studies on object detection and feature extraction have become important for scene understanding and 3D mapping. However, 3D shape of the object is too complex to formulate the probabilistic observation model; therefore, performing the Bayesian inference of the object-oriented features as well as their pose is less considered. Besides, when the robot equipped with an RGB mono camera only observes the projected single view of an object, a significant amount of the 3D shape information is abandoned. Due to these limitations, semantic SLAM and viewpoint-independent loop closure using volumetric 3D object shape is challenging. In order to enable the complete formulation of probabilistic semantic SLAM, we approximate the observation model of a 3D object with a tractable distribution. We also estimate the variational likelihood from the 2D image of the object to exploit its observed single view. In order to evaluate the proposed method, we perform pose and feature estimation, and demonstrate that the automatic loop closure works seamlessly without additional loop detector in various environments.	approximation algorithm;calculus of variations;closing (morphology);cobham's thesis;experiment;feature extraction;generative model;mathematical optimization;object detection;pose (computer vision);robot;simultaneous localization and mapping;smoothing;variational principle	Haoping Yu;B. H. Le	2018	CoRR		control engineering;engineering;feature extraction;probabilistic logic;rgb color model;object detection;exploit;pattern recognition;artificial intelligence;bayesian probability;bayesian inference	Robotics	50.831199087740316	-41.734838814764714	89254
f21bdc00f68a6758b51949474d350c4678d04030	3d scene reconstruction using omnidirectional vision and lidar: a hybrid approach	ladybug;iterative closest point icp;3d point cloud registration;surface reconstruction;technology and engineering;velodyne;loop closure;lidar scanning	In this paper, we propose a novel approach to obtain accurate 3D reconstructions of large-scale environments by means of a mobile acquisition platform. The system incorporates a Velodyne LiDAR scanner, as well as a Point Grey Ladybug panoramic camera system. It was designed with genericity in mind, and hence, it does not make any assumption about the scene or about the sensor set-up. The main novelty of this work is that the proposed LiDAR mapping approach deals explicitly with the inhomogeneous density of point clouds produced by LiDAR scanners. To this end, we keep track of a global 3D map of the environment, which is continuously improved and refined by means of a surface reconstruction technique. Moreover, we perform surface analysis on consecutive generated point clouds in order to assure a perfect alignment with the global 3D map. In order to cope with drift, the system incorporates loop closure by determining the pose error and propagating it back in the pose graph. Our algorithm was exhaustively tested on data captured at a conference building, a university campus and an industrial site of a chemical company. Experiments demonstrate that it is capable of generating highly accurate 3D maps in very challenging environments. We can state that the average distance of corresponding point pairs between the ground truth and estimated point cloud approximates one centimeter for an area covering approximately 4000 m 2 . To prove the genericity of the system, it was tested on the well-known Kitti vision benchmark. The results show that our approach competes with state of the art methods without making any additional assumptions.	3d reconstruction from multiple images;algorithm;alignment;autonomous robot;benchmark (computing);closure;color vision defect;cycle detection;drug vehicle;experiment;generic programming;ground truth;map;mobile mapping;modal logic;numerous;odometry;point cloud;scanner device component;weatherstar;whole earth 'lectronic link;centimeter;ladybug	Michiel Vlaminck;Hiêp Quang Luong;Werner Goeman;Wilfried Philips	2016		10.3390/s16111923	computer vision;simulation;chemistry;surface reconstruction;nanotechnology;optics;remote sensing	Vision	53.32681884491142	-42.5717940254119	89270
3cf595c4d4a3c0ab1db08579c4f52c45c19d967c	rgb-d camera-based parallel tracking and meshing	real time tracking;3d point cloud;real time;motion estimation;three dimensional;camera motion;three dimensional displays;image color analysis;feature extraction;image reconstruction;cameras tracking three dimensional displays image reconstruction real time systems feature extraction image color analysis;image sequence;ground truth;point cloud;augmented reality;cameras;quantitative evaluation;tracking;real time systems	Compared to standard color cameras, RGB-D cameras are designed to additionally provide the depth of imaged pixels which in turn results in a dense colored 3D point cloud representing the environment from a certain viewpoint. We present a real-time tracking method that performs motion estimation of a consumer RGB-D camera with respect to an unknown environment while at the same time reconstructing this environment as a dense textured mesh. Unlike parallel tracking and mapping performed with a standard color or grey scale camera, tracking with an RGB-D camera allows a correctly scaled camera motion estimation. Therefore, there is no need for measuring the environment by any additional tool or equipping the environment by placing objects in it with known sizes. The tracking can be directly started and does not require any preliminary known and/or constrained camera motion. The colored point clouds obtained from every RGB-D image are used to create textured meshes representing the environment from a certain camera view and the real-time estimated camera motion is used to correctly align these meshes over time in order to combine them into a dense reconstruction of the environment. We quantitatively evaluated the proposed method using real image sequences of a challenging scenario and their corresponding ground truth motion obtained with a mechanical measurement arm. We also compared it to a commonly used state-of-the-art method where only the color information is used. We show the superiority of the proposed tracking in terms of accuracy, robustness and usability. We also demonstrate its usage in several Augmented Reality scenarios where the tracking allows a reliable camera motion estimation and the meshing increases the realism of the augmentations by correctly handling their occlusions.	align (company);augmented reality;grayscale;ground truth;match moving;motion estimation;pixel;point cloud;real-time clock;usability	Sebastian Lieberknecht;Andrea Huber;Slobodan Ilic;Selim Benhimane	2011	2011 10th IEEE International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2011.6092380	iterative reconstruction;smart camera;three-dimensional space;computer vision;camera auto-calibration;augmented reality;match moving;real-time computing;camera resectioning;simulation;ground truth;feature extraction;computer science;motion estimation;point cloud;tracking;motion field;pinhole camera model;computer graphics (images)	Vision	53.72369427933505	-46.23226350105509	89320
dd0889e98266538ba7f4cab879cd43645d97a1ac	distributed object recognition in visual sensor networks	object recognition;cache placement;wireless sensor networks cameras centralised control feature extraction image capture image matching object recognition optimisation;completion time minimization distributed object recognition visual sensor network vsn image capture camera sensor node visual feature extraction image matching central controller optimization framework;visual sensor networks;visual sensor networks cache placement object recognition;cameras visualization databases relays object recognition feature extraction histograms	This work focuses on Visual Sensor Networks (VSNs) which perform visual analysis tasks such as object recognition. There, the goal is to find the image in a reference database which is the closest match to the image captured by camera sensor nodes. Recognition is performed by relying on visual features extracted from the acquired image, which are matched against a database of labeled features in order to find the closest image match. The matching functionalities are often implemented at a central controller outside the VSN. In contrast, we study the performance trade-offs involved in distributing the matching functionalities inside the VSN by letting sensor nodes performing parts of the matching process. We propose an optimization framework to optimally distribute the matching task to in-network sensor nodes with the goal of minimizing the overall completion time of the recognition task. The proposed optimization framework is then used to assess the performance of distributed matching, comparing it to a traditional, centralized approach in realistic VSN scenarios.	bibliographic database;centralized computing;computer data storage;distributed object;image sensor;mathematical optimization;outline of object recognition	Stefano Paris;Alessandro Enrico Cesare Redondi;Matteo Cesana;Marco Tagliasacchi	2015	2015 IEEE International Conference on Communications (ICC)	10.1109/ICC.2015.7249393	computer vision;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;visual sensor network	Robotics	46.869109383760716	-38.342468501191014	89413
8eabeb0bc97f60ee9b219a075877b2116daa748b	object tracking using point matching based on mcmc	image sampling;object video tracking method;mcmc algorithm;mcmc sampling method;video signal processing;image matching;optimal matching parameter;matrix decomposition optical distortion information science target tracking probability distribution bayesian methods optical filters image motion analysis pixel computer science;video tracking;posterior probability;computer vision;posterior probability distribution problem;statistical distributions;estimation;matrix decomposition;image color analysis;object tracking;video signal processing image matching image sampling markov processes monte carlo methods object detection statistical distributions tracking;robustness;markov processes;markov chain monte carlo algorithm object video tracking method mcmc point matching method posterior probability distribution problem optimal matching parameter mcmc sampling method;target tracking;markov chain monte carlo algorithm;mcmc point matching method;proposals;monte carlo methods;tracking;object detection;matching method	Most of the reported object tracking methods achieved by estimating the object position cannot suit for large image deformations. For this reason, a new object tracking base on MCMC point matching method is presented in this paper. This method applies MCMC algorithm to solve the posterior probability distribution problem and obtain the optimal matching parameters include position, rotation, scale information. The test results prove this method can successfully cope with target scale or orientation variations in video tracking	algorithm;markov chain monte carlo;optimal matching;video tracking	Hongbo Yang;Xia Hou	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.319	computer vision;computer science;video tracking;pattern recognition;mathematics;statistics	Vision	45.76869571676285	-48.964576270357185	89519
82c448ac01a96ab43d68cd6a3864bec6fdbc08db	an ant particle filter for visual tracking		Sequential Monte Carlo method (also named as particle filter) is now a standard framework for solving nonlinear/non-Gaussian problems, especially in computer vision fields. This paper proposes an ant colony optimization (ACO) based iterative particle filter for visual tracking. In the proposed tracking method, the basic idea of ACO is used to simulate the behavior of particle moving toward the posterior density. Such idea is incorporated into the particle filtering framework in order to overcome the well-known problem of particle impoverishment. We design an iterative proposal distribution for particle generation in order to generate better predicted sample states. The experimental results demonstrate that the proposed tracker shows better performance than the other trackers.	ant colony optimization algorithms;computer vision;iterative method;mathematical optimization;nonlinear system;particle filter;resampling (statistics);simulation;video tracking;whole earth 'lectronic link;lsh	Fasheng Wang;Baowei Lin;Xucheng Li	2017	2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2017.7960029	ant colony optimization algorithms;auxiliary particle filter;particle filter;monte carlo method;nonlinear system;eye tracking;mathematical optimization;ant;particle;computer science	Vision	45.67899767393852	-47.81267363978293	89752
f32b15d7c484465fd612a3757a3d055281c9bd61	color landmark based self-localization for indoor mobile robots	phase measurement;local algorithm;stochastic algorithm;mobile robot;real time;mobile robots;layout;navigation;stochastic processes;photometry;indoor environment;indoor environments;robustness;computer science;three dimensional structure;mobile robots navigation indoor environments phase measurement robustness computer science stochastic processes wheels layout photometry;wheels	We present a simple artificial landmark model and a robust tracking algorithm for the navigation of indoor mobile robots. The landmark model is designed to have a three-dimensional structure consisting of a multicolored planar pattern. A stochastic algorithm based on Condensation [1] tracks the landmark model robustly using the color distribution of the pattern . A new selflocalization algorithm computes the location of robot with the tracked single landmark . Experimental results show that the proposed landmark model is effective. Through extensive navigation experiments in a cluttered indoor environment, we demonstrate the feasibility of the single view based self-localization in real-time.	autostereogram;condensation algorithm;experiment;image resolution;internationalization and localization;line fitting;mobile robot;real-time clock;tracking system	Gi-jeong Jang;Sungho Kim;Wang-Heon Lee;In-So Kweon	2002		10.1109/ROBOT.2002.1013492	mobile robot;stochastic process;embedded system;computer vision;simulation;computer science;artificial intelligence	Robotics	51.692119871722255	-38.98020325155255	89809
2b06b4fc1ed78738a447a3a427375213a43197e3	robust video-based surveillance by integrating target detection with tracking	histograms;confidence level;surveillance;surveillance system;feeds;support vector regression;vehicle detection;adaptive optimization;null;robustness surveillance object detection target tracking shape vehicle detection target recognition feeds optimization methods histograms;shape;target recognition;shape matching;background subtraction;robustness;target tracking;target detection;object detection;on line learning;optimization methods	"""Target detection and tracking represent two fundamental steps in automatic video-based surveillance systems where the goal is to provide intelligent recognition capabilities by analyzing target behavior. This paper presents a framework for video-based surveillance where detection and tracking are addressed simultaneously in a unified framework (i.e., detection results trigger tracking, and tracking re-enforces detections)to improve detection results. In contrast to methods that apply target detection and tracking sequentially and independently from each other (i.e., """"detect-then-track""""), we feed the results of tracking back to the detection stage to adaptively optimize the threshold used in the detection stage and improve system robustness (i.e., """"detect-and-track""""). Specifically, the initial locations and representations of the targets are extracted by background subtraction. To model the background, we employ Support Vector Regression (SVR) along with an on-line learning scheme to update it efficiently over time. Target detection is performed by thresholding the outputs of the SVR model. Tracking uses shape projection histograms to iteratively localize the targets and achieve a high shape matching confidence level. Feeding back the results of tracking to the detection stage restricts the range of threshold values, suppress false alarms due to noise, and allows to continuously detect small targets as well as targets undergoing projection distortions. We have validated the proposed framework by detecting vehicles and pedestrians in traffic scenes using both visible and thermal video sequences. Experimental results and comparisons with framebased detection and kernel-based tracking methods illustrate the robustness of our approach."""	3d projection;background subtraction;coefficient;collision detection;distortion;experiment;online and offline;online machine learning;pixel;real-time computing;real-time transcription;sampling (signal processing);sensor;support vector machine;thresholding (image processing);unified framework;zero suppression	Junxian Wang;George Bebis;Ronald Miller	2006	2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)	10.1109/CVPRW.2006.180	adaptive optimization;support vector machine;computer vision;confidence interval;background subtraction;tracking system;shape;computer science;machine learning;video tracking;pattern recognition;histogram;robustness	Vision	43.042196875726646	-47.55993135616245	89876
9e76cc9e87f6dc8f0dc235f29315254ef4570a5c	using pattern recognition to automatically localize reflection hyperbolas in data from ground penetrating radar	opencv;adaboost;viola jones;haartraining;reflection hyperbola;gpr	Ground Penetrating Radar (GPR) is used for the localization of supply lines, land mines, pipes and many other buried objects. These objects can be recognized in the recorded data as reflection hyperbolas with a typical shape depending on depth and material of the object and the surrounding material. To obtain the parameters, the shape of the hyperbola has to be fitted. In the last years several methods were developed to automate this task during post-processing. In this paper we show another approach for the automated localization of reflection hyperbolas in GPR data by solving a pattern recognition problem in grayscale images. In contrast to other methods our detection program is also able to immediately mark potential objects in real-time. For this task we use a version of the Viola–Jones learning algorithm, which is part of the open source library “OpenCV”. This algorithm was initially developed for face recognition, but can be adapted to any other simple shape. In our program it is used to narrow down the location of reflection hyperbolas to certain areas in the GPR data. In order to extract the exact location and the velocity of the hyperbolas we apply a simple Hough Transform for hyperbolas. Because the Viola–Jones Algorithm reduces the input for the computational expensive Hough Transform dramatically the detection system can also be implemented on normal field computers, so on-site application is possible. The developed detection system shows promising results and detection rates in unprocessed radargrams. In order to improve the detection results and apply the program to noisy radar images more data of different GPR systems as input for the learning algorithm is necessary. & 2013 Elsevier Ltd. All rights reserved.	algorithm;computation;facial recognition system;fallout;grayscale;hough transform;jones calculus;kriging;microsoft outlook for mac;open-source software;opencv;pattern recognition;radar;real-time clock;real-time locating system;reflection (computer graphics);rugged computer;velocity (software development);video post-processing	Christian Maas;Jörg Schmalzl	2013	Computers & Geosciences	10.1016/j.cageo.2013.04.012	adaboost;computer vision;speech recognition;ground-penetrating radar;computer science;viola–jones object detection framework;machine learning;computer graphics (images)	Vision	42.55765364045619	-42.52381688497692	89964
7f8c853682bb7ebb6d9d801b7d2c2769c3f6e1ca	ten-fold improvement in visual odometry using landmark matching	real time visualization;wearable systems;visual odometry;robot visual odometry system;image motion analysis;image matching;wearable computers;pose computation;pose computation robot visual odometry system visual landmark matching wearable systems dynamic local landmark tracking technique landmark database;visual landmarks;inertial measurement unit;robot vision;dynamic local landmark tracking technique;visual landmark matching;wearable computers image matching image motion analysis pose estimation robot vision tracking;error rate;navigation system;landmark database;tracking;pose estimation;navigation layout error analysis stereo vision cameras sensor systems error correction robots visual databases measurement units	Our goal is to create a visual odometry system for robots and wearable systems such that localization accuracies of centimeters can be obtained for hundreds of meters of distance traveled. Existing systems have achieved approximately a 1% to 5% localization error rate whereas our proposed system achieves close to 0.1% error rate, a ten-fold reduction. Traditional visual odometry systems drift over time as the frame-to-frame errors accumulate. In this paper, we propose to improve visual odometry using visual landmarks in the scene. First, a dynamic local landmark tracking technique is proposed to track a set of local landmarks across image frames and select an optimal set of tracked local landmarks for pose computation. As a result, the error associated with each pose computation is minimized to reduce the drift significantly. Second, a global landmark based drift correction technique is proposed to recognize previously visited locations and use them to correct drift accumulated during motion. At each visited location along the route, a set of distinctive visual landmarks is automatically extracted and inserted into a landmark database dynamically. We integrate the landmark based approach into a navigation system with 2 stereo pairs and a low-cost inertial measurement unit (IMU) for increased robustness. We demonstrate that a real-time visual odometry system using local and global landmarks can precisely locate a user within 1 meter over 1000 meters in unknown indoor/outdoor environments with challenging situations such as climbing stairs, opening doors, moving foreground objects etc..	computation;real-time clock;robot;stationary process;visual odometry;wearable computer	Zhiwei Zhu;Taragay Oskiper;Supun Samarasekera;Rakesh Kumar;Harpreet S. Sawhney	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4409062	inertial measurement unit;computer vision;simulation;pose;wearable computer;word error rate;computer science;visual odometry;odometry;tracking;computer graphics (images)	Robotics	48.842498653905864	-43.44338374665004	90202
5b75382ff470309fc63d2b3c74975dfc89ab4950	sequential localisation and map-building in computer vision and robotics	map building;real time;robot navigation;first order;error propagation;open source software	Reviewing the important problem of sequential localisation and map-building, we emphasize its genericity and in particular draw parallels between the often divided fields of computer vision and robot navigation. We compare sequential techniques with the batch methodologies currently prevalent in computer vision, and explain the additional challenges presented by real-time constraints which mean that there is still much work to be done in the sequential case, which when solved will lead to impressive and useful applications. In a detailed tutorial on map-building using first-order error propagation, particular attention is drawn to the roles of modelling and an active methodology. Finally, recognising the critical role of software in tackling a generic problem such as this, we announce the distribution of a proven and carefully designed open-source software framework which is intended for use in a wide range of robot and vision applications: http://www.robots.ox.ac.uk/~ajd/	computer vision;robotics	Andrew J. Davison;Nobuyuki Kita	2000		10.1007/3-540-45296-6_15	computer vision;simulation;computer science;artificial intelligence;mobile robot navigation	Vision	43.87341216078681	-38.638742730256745	90386
51593801a95fd15bc40f2ac1b76185f617c397ef	geometric hypergraph learning for visual tracking	confidence aware sampling correspondence hypotheses deformation geometric hypergraph learning mode seeking occlusion visual tracking;target tracking visualization robustness laboratories computers data mining	Graph-based representation is widely used in visual tracking field by finding correct correspondences between target parts in different frames. However, most graph-based trackers consider pairwise geometric relations between local parts. They do not make full use of the target’s intrinsic structure, thereby making the representation easily disturbed by errors in pairwise affinities when large deformation or occlusion occurs. In this paper, we propose a geometric hypergraph learning-based tracking method, which fully exploits high-order geometric relations among multiple correspondences of parts in different frames. Then visual tracking is formulated as the mode-seeking problem on the hypergraph in which vertices represent correspondence hypotheses and hyperedges describe high-order geometric relations among correspondences. Besides, a confidence-aware sampling method is developed to select representative vertices and hyperedges to construct the geometric hypergraph for more robustness and scalability. The experiments are carried out on three challenging datasets (VOT2014, OTB100, and Deform-SOT) to demonstrate that our method performs favorably against other existing trackers.	area striata structure;computational complexity theory;experiment;frame (physical object);gamma glutamyl transferase measurement;graph - visual representation;holism;sampling (signal processing);scalability;silo (dataset);vertex (geometry);vertex (graph theory);video tracking	Dawei Du;Honggang Qi;Longyin Wen;Qi Tian;Qingming Huang;Siwei Lyu	2017	IEEE Transactions on Cybernetics	10.1109/TCYB.2016.2626275	computer vision;computer science;machine learning;pattern recognition;mathematics	Vision	46.45313628039511	-50.981564106944454	90399
0e3e1d8bdbd9f7c39d46c0283f791cf578154b7f	transition state clustering: unsupervised surgical trajectory segmentation for robot learning	trajectory segmentation;surgical robotics;robot learning	Demonstration trajectories collected from a supervisor in teleoperation are widely used for robot learning, and temporally segmenting the trajectories into shorter, less-variable segments can improve the efficiency and reliability of learning algorithms. Trajectory segmentation algorithms can be sensitive to noise, spurious motions, and temporal variation. We present a new unsupervised segmentation algorithm, transition state clustering (TSC), which leverages repeated demonstrations of a task by clustering segment endpoints across demonstrations. TSC complements any motion-based segmentation algorithm by identifying candidate transitions, clustering them by kinematic similarity, and then correlating the kinematic clusters with available sensory and temporal features. TSC uses a hierarchical Dirichlet process Gaussian mixture model to avoid selecting the number of segments a priori. We present simulated results to suggest that TSC significantly reduces the number of false-positive segments in dynamical systems observed with noise as compared with seven probabilistic and non-probabilistic segmentation algorithms. We additionally compare algorithms that use piecewise linear segment models, and find that TSC recovers segments of a generated piecewise linear trajectory with greater accuracy in the presence of process and observation noise. At the maximum noise level, TSC recovers the ground truth 49% more accurately than alternatives. Furthermore, TSC runs 100× faster than the next most accurate alternative autoregressive models, which require expensive Markov chain Monte Carlo (MCMC)-based inference. We also evaluated TSC on 67 recordings of surgical needle passing and suturing. We supplemented the kinematic recordings with manually annotated visual features that denote grasp and penetration conditions. On this dataset, TSC finds 83% of needle passing transitions and 73% of the suturing transitions annotated by human experts.	algorithm;autoregressive model;cluster analysis;dynamical system;ground truth;machine learning;markov chain monte carlo;mixture model;monte carlo method;noise (electronics);piecewise linear continuation;remote desktop services;robot learning;time stamp counter;unsupervised learning	Sanjay Krishnan;Animesh Garg;Sachin Patil;Colin Lea;Gregory D. Hager;Pieter Abbeel;Kenneth Y. Goldberg	2017	I. J. Robotics Res.	10.1177/0278364917743319	robot learning;market segmentation;mixture model;mathematics;linear dynamical system;dynamical systems theory;machine learning;hierarchical dirichlet process;cluster analysis;unsupervised learning;artificial intelligence	AI	47.21163152375692	-39.57908080922195	90463
3069115b0938e84ca0be44829158ad0d3feed8b7	wall estimation from stereo vision in urban street canyons		Geometric context has been recognised as important high-level knowledge towards the goal of scene understanding. In this work we present two approaches to estimate the local geometric structure of urban street canyons captured from a head-mounted stereo camera. A dense disparity estimation is the only input for both approaches. First, we show how the left and right building facade can be obtained by planar segmentation based on random sampling. In a second approach we transform the disparity into an elevation map from which we extract the main building orientation. We evaluate both approaches on a set of challenging inner city scenes and demonstrate how visual odometry can be incorporated to keep track of the estimated geometry.	binocular disparity;data point;high- and low-level;iterative method;least squares;mathematical optimization;monte carlo method;sampling (signal processing);stereo camera;stereopsis;the wall street journal;visual odometry	Tobias Schwarze;Martin Lauer	2013		10.5220/0004484600830090	engineering;control engineering;canyon;computer vision;artificial intelligence;stereopsis	Vision	52.31682887041406	-42.4430421346486	90853
b37563a750eef10830a03aeceb9c9d32e836b7b9	a robust pixel ecc based algorithm for occluded image alignment		The alignment of occluded images constitutes a common and difficult problem. In this paper we propose a new method based on ECC algorithm tailored to occluded image alignment problem which enjoy a simple closed-form solution with low computational cost. Moreover, the use of a proper subset of the region of interest that limits the impact of the outliers in the estimation of the parameters is proposed. The use of this set seems to make the proposed method insensitive to the use of the occluded image as template or as warped in the alignment process. The proposed method is compared against two well known Gradient Corellation based methods by its application in several image alignment problems and in all cases outperforms its rivals in terms of accuracy and percentage of convergence.	algorithm;algorithmic efficiency;approximation algorithm;computation;distortion;ecc memory;experiment;mathematical optimization;nonlinear programming;nonlinear system;pixel;region of interest;stochastic gradient descent	Nefeli Lamprinou;Emmanouil Z. Psarakis	2016		10.5220/0005788202790284	computer vision;computer graphics (images)	Vision	50.0379755818059	-51.537760746938105	90987
f39cc824fbc0324a447eca288163c6b572272d94	automatic detection of adverse weather conditions in traffic scenes	image recognition;ing inf 05 sistemi di elaborazione delle informazioni;video surveillance;gaussian mixture;video surveillance gaussian processes image recognition meteorology object detection road vehicles statistical analysis traffic control;snow;gaussian processes;gaussian mixture adverse weather condition automatic detection traffic scenes visual surveillance outdoor scene analysis vehicle traffic control statistical framework;traffic control;statistical framework;weather recognition visual surveillance gaussian mixture model;computer vision;visual surveillance;gaussian mixture model;computational modeling;statistical analysis;automatic detection;image color analysis;pixel;weather condition;vehicle traffic control;mixture of gaussians;weather recognition;adverse weather condition automatic detection;meteorology;outdoor scene analysis;traffic scenes;object detection;road vehicles;image sequences;layout surveillance vehicles traffic control signal analysis snow rain gaussian processes frequency meteorology	Visual surveillance in outdoor environments requires the monitoring of both objects and events. The analysis is generally driven by the target application which, in turn, determines the set of relevant events and objects to be analyzed. In this paper we concentrate on the analysis of outdoor scenes, in particular for vehicle traffic control. In this scenario, the analysis of weather conditions is considered to signal particular and potentially dangerous situations like the presence of snow, fog, or heavy rain. The developed system uses a statistical framework based on the mixture of Gaussians to identify changes both in the spatial and temporal frequencies which characterize specific meteorological events. Several experiments performed on standard databases and real scenes demonstrate the applicability of the proposed approach.	database;distance fog;experiment;mixture model	Andrea Lagorio;Enrico Grosso;Massimo Tistarelli	2008	2008 IEEE Fifth International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2008.50	computer vision;simulation;computer science;mixture model;statistics	Vision	39.92971455016466	-45.38426074958226	91519
0033d8e3662e2b4c4bed5ba68a397bd3f1619a4b	a novel robust approach for handling illumination changes in video segmentation	background modeling;activity detection;video segmentation;human silhouette detection;illumination changes	In this work we propose a novel algorithm for foreground segmentation in video sequences that achieves better accuracy, while maintaining low complexity and allows real time execution. The proposed algorithm combines selected features from a number of well-established and state of the art algorithms, such as the Gaussian mixture models, the Self Organizing Maps and the illumination sensitive method. The presented methodology is capable of efficiently handling sudden light changes, both from natural and multiple artificial light sources, which may have caused erroneous segmentation for other algorithms. Comparative results are presented utilizing user-defined ground truth segmentation for two different types of indoor video sequences, one of which was obtained by a hemispheric omnidirectional dioptric (fish-eye) camera, with and without illumination changes and the second by a plain camera. The behavior of the algorithm with respect to its controlling parameters is investigated and its computational burden is studied. A segmentation algorithm is proposed for video sequences with illumination changes.The proposed algorithm maintains low complexity and allows real time executionThe proposed algorithm combines features from state of the art algorithmsComparative results are presented for videos obtained by omnidirectional and by plain camera		Kostas Delibasis;Theodosios Goudas;Ilias Maglogiannis	2016	Eng. Appl. of AI	10.1016/j.engappai.2015.11.006	computer vision;simulation;scale-space segmentation;computer graphics (images)	AI	43.704779138667945	-49.14629429280634	91548
cbdda0fbbff54e976e80ba1e28e247a4cc284f0d	visual tracking using the joint inference of target state and segment-based appearance models	probability;image segmentation;features model robust visual tracking method joint inference target state model segment based appearance model casting tracking joint space estimation problem nonrigid appearance model discriminative power and lack method image restoring method particle markov chain monte carlo joint density approximation probability marginalization convergence image classification;image classification;inference mechanisms;visual servoing approximation theory computer vision feature extraction image classification image restoration image segmentation inference mechanisms markov processes monte carlo methods object tracking probability;image restoration;computer vision;approximation theory;feature extraction;object tracking;markov processes;visual servoing;monte carlo methods;target tracking computational modeling visualization color histograms computer vision joints	In this paper, a robust visual tracking method is proposed by casting tracking as an estimation problem of the joint space of non-rigid appearance model and state. Conventional trackers which use templates as the appearance model do not handle ambiguous samples effectively. On the other hand, trackers that use non-rigid appearance models have low discriminative power and lack methods for restoring methods from inaccurately labeled data. To address this problem, multiple non-rigid appearance models are proposed. The probabilities from these models are effectively marginalized by using the particle Markov chain Monte Carlo framework which provides an exact and efficient approximation of the joint density through marginalization and the theoretical evidences of convergence. An appearance model combines multiple classification results with different features and multiple models can infer an accurate solution despite the failure of several models. The proposed method exhibits high accuracy compared with nine other state-of-the-art trackers in various sequences and the result was analyzed both analyzed both qualitatively and quantitatively.	approximation;experiment;markov chain monte carlo;monte carlo method;particle filter;sampling (signal processing);video tracking;vii	Junha Roh;Dong Woo Park;Junseok Kwon;Kyoung Mu Lee	2013	2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference	10.1109/APSIPA.2013.6694177	computer vision;active appearance model;machine learning;pattern recognition;mathematics	Vision	45.45134183398004	-49.508778163908005	91549
1d0747b0f777b213d697497943a6ff9b6bdce34d	spark detection with thermal camera		Motion detection can be defined as the problem of detecting changes in the motion of objects depending on the environment with various cameras (RGB, thermal etc.). This problem has been studied by researchers for many years, taking into consideration the state of the camera, its capabilities and the physical structure of the objects to be detected. In this research, SPARK (i.e. sudden motion) changes are studied and focused on temperature change using thermal camera. The experimental results show that the proposed method achieves promising results in the proposed dataset.	spark;sensor	Omer Ozdil;Berkan Demirel;Yunus Emre Esin;Safak Ozturk	2018	2018 26th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2018.8404631	computer vision;image segmentation;computer science;artificial intelligence;rgb color model;spark (mathematics);motion detection	Robotics	42.61914276977318	-44.495291812389205	91694
4d4ebf24a532a3d20850498aa21eb7f8b34682a4	consensus-based cross-correlation	robust correlation;cross correlation;synchronization;signal processing;template matching;similarity measure	Cross-correlation is a classical similarity measure with broad applications in multimedia signal processing. While it is robust against uncorrelated noise in the input signals, it is severely affected by systematic disturbances which lead to biased results. To overcome this limitation, we propose in this paper consensus-based cross-correlation (ConCor) to deal with heavily corrupted signal parts that derail regular cross-correlation. ConCor builds upon the widely adopted RANSAC algorithm to reliably identify and eliminate corrupt signal parts at limited additional complexity. Our approach is universal in that it can be combined with existing cross-correlation variants. We apply ConCor in two example applications, namely video synchronization and template matching. Our experimental results demonstrate the improved robustness and accuracy when compared to classical cross-correlation.	algorithm;cross-correlation;random sample consensus;signal processing;similarity measure;template matching;white noise	Florian Schweiger;Georg Schroth;Michael Eichhorn;Eckehard G. Steinbach;Michael Fahrmair	2011		10.1145/2072298.2071996	synchronization;computer vision;template matching;computer science;cross-correlation;machine learning;signal processing;pattern recognition;data mining	ML	39.309001930969764	-41.17745053747004	91715
c745bd60dd952b82b25707cf589d31bab0d2e196	manifold-based fingerprinting for target identification	image segmentation;manifolds;fingerprint recognition manifolds target tracking cameras robustness image segmentation;computer graphics;statistical analysis computer graphics fingerprint identification object detection;occlusions manifold based fingerprinting analysis algorithm target identification robust signatures motion imagery camera handoff situations statistical manifold training data;statistical analysis;fingerprint recognition;robustness;target tracking;cameras;fingerprint identification;object detection	In this paper, we propose a fingerprint analysis algorithm based on using product manifolds to create robust signatures for individual targets in motion imagery. The purpose of target fingerprinting is to reidentify a target after it disappears and then reappears due to occlusions or out of camera view and to track targets persistently under camera handoff situations. The proposed method is statistics-based and has the benefit of being compact and invariant to viewpoint, rotation, and scaling. Moreover, it is a general framework and does not assume a particular type of objects to be identified. For improved robustness, we also propose a method to detect outliers of a statistical manifold formed from the training data of individual targets. Our experiments show that the proposed framework is more accurate in target reidentification than single-instance signatures and patch-based methods.	algorithm;antivirus software;electronic signature;experiment;fingerprint (computing);generalized linear model;image scaling;multimodal interaction;patch (computing);sensor;single-instance storage;statistical manifold;tracking system;type signature	Kang-Yu Ni;Terrell N. Mundhenk;Kyungnam Kim;Yuri Owechko	2012	2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2012.6239199	fingerprint;computer vision;manifold;computer science;machine learning;pattern recognition;image segmentation;computer graphics;fingerprint recognition;robustness	Vision	42.812916438427564	-51.7210309935787	91733
5025858610be98a985db5094f9e25076d4956bf3	robust sparse and dense nonrigid structure from motion		Modeling deformable three-dimensional (3-D) shape from a video sequence is a fundamental task in computer vision. Nonrigid structure from motion (NRS f M) refers to the problem of recovering the 3-D shape and pose of an object, deforming over time from a monocular video sequence. Presently, dense NRS f M is a research problem of great interest in academia and the industry due to the large demand for 3-D data in various contexts. We provide a robust system for the sparse and dense NRS f M. The strength of our approach comes with the ability to deal with the trajectories corrupted with outliers that serve as an input to NRS f M. To tackle this problem, the input trajectories are processed with a density-based spatial clustering approach that is combined with a RANSAC technique for the outlier’ s detection. Processing the trajectories with this process enhances the trajectories by removing the unwanted outliers. Also, extending the work from sparse to dense NRS f M substantially increases the difficulty of the optimization problem. Thus, the proposed system also provides asymptotic improvements to the current optimization approaches by providing an efficient and a novel supervised Gauss–Newton method. Extensive experiments have demonstrated that the proposed method outperforms most of the existing NRS f M methods. The results show that the proposed method reconstructs largely deforming objects accurately and efficiently.	cluster analysis;computer vision;experiment;gauss–newton algorithm;mathematical optimization;newton's method;optimization problem;random sample consensus;sparse matrix;structure from motion	Imran Khan	2018	IEEE Transactions on Multimedia	10.1109/TMM.2017.2758740	ransac;artificial intelligence;computer vision;pattern recognition;iterative reconstruction;robustness (computer science);computer science;outlier;structure from motion;cluster analysis;trajectory;optimization problem	Vision	51.7257512817949	-47.65081572110672	91818
50c89813eab745dc170bf440cba869d3999a2827	hierarchical pedestrian attribute recognition based on adaptive region localization		Learning to recognize pedestrian attributes (such as gender, hair style, take hat or not) in video surveillance scenarios is critical to a variety of tasks, such as crime prevention and border control. However, it is still challenging due to low resolution and highlight influence in the actual surveillance scenarios, in which traditional methods work not well. This paper aims at proposing a robust pedestrian attribute recognition framework which can be adaptive to the actual surveillance scenarios. Specifically, we first propose a hierarchical recognition strategy by heuristically classifying the pedestrian attributes as global ones (such as gender and age) and local ones (such as hair style and has glass). Then the whole region is used for the global attribute recognition, and the relevant regions are used for the local attribute recognition. To estimate the relevant regions above, we further propose an adaptive region localization scheme, including position estimation based on geometric human body and relevant region localization based on random expansion. Finally, experimental results on representative datasets and our actual surveillance scenarios both demonstrate the effectiveness of the proposed method.	closed-circuit television;heuristic;image resolution	Chunfeng Yao;Bailan Feng;Defeng Li	2017	2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2017.8026292	computer vision;artificial intelligence;computer science;pattern recognition;pedestrian;machine learning;heuristic;image resolution;crime prevention	Vision	40.90042268256629	-45.64980737284174	91938
b47a64e7b2cb75f64238dae02da25118466d09e3	a global linear method for camera pose registration	image coding;matrix algebra;tensors cameras image coding image registration matrix algebra pose estimation;image registration;bundle adjustment global linear method global camera pose registration pair wise relative pose encoding essential matrices approximate geometric error minimization camera triplets unbalanced scale problem pair wise translation direction constraints trifocal tensor multiple camera registration point triangulation;structure from motion;cameras;pose estimation;tensors;cameras image reconstruction equations three dimensional displays mathematical model robustness tensile stress	We present a linear method for global camera pose registration from pair wise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical `unbalanced scale' problem in linear methods relying on pair wise translation direction constraints, i.e. an algebraic error, nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.	approximation algorithm;baseline (configuration management);bundle adjustment;degeneracy (graph theory);homology-derived secondary structure of proteins;level of detail;linear approximation;robustness (computer science);trifocal tensor;triplet state;unbalanced circuit	Nianjuan Jiang;Zhaopeng Cui;Ping Tan	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.66	computer vision;mathematical optimization;structure from motion;pose;tensor;computer science;image registration;mathematics;geometry	Vision	51.542490292685926	-50.4952135472708	92162
3f3e116d3b67535ad4ea09055cdd096b98a92d3b	a 2d eye gaze estimation system with low-resolution webcam images	signal image and speech processing;deformable template methods;low cost 2d eye gaze estimation;eyeball detection;quantum information technology spintronics;stable approximate pupil center detection;robust eye movement detection	In this article, a low-cost system for 2D eye gaze estimation with low-resolution webcam images is presented. Two algorithms are proposed for this purpose, one for the eye-ball detection with stable approximate pupil-center and the other one for the eye movements’ direction detection. Eyeball is detected using deformable angular integral search by minimum intensity (DAISMI) algorithm. Deformable template-based 2D gaze estimation (DTBGE) algorithm is employed as a noise filter for deciding the stable movement decisions. While DTBGE employs binary images, DAISMI employs gray-scale images. Right and left eye estimates are evaluated separately. DAISMI finds the stable approximate pupil-center location by calculating the mass-center of eyeball border vertices to be employed for initial deformable template alignment. DTBGE starts running with initial alignment and updates the template alignment with resulting eye movements and eyeball size frame by frame. The horizontal and vertical deviation of eye movements through eyeball size is considered as if it is directly proportional with the deviation of cursor movements in a certain screen size and resolution. The core advantage of the system is that it does not employ the real pupil-center as a reference point for gaze estimation which is more reliable against corneal reflection. Visual angle accuracy is used for the evaluation and benchmarking of the system. Effectiveness of the proposed system is presented and experimental results are shown.	angularjs;approximation algorithm;binary image;cursor (databases);display size;grayscale;noise reduction;webcam	Ibrahim Ince;Jin Wook Kim	2011	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2011-40	computer vision;simulation;eye tracking;computer science	Vision	47.85731604149529	-45.79283155448495	92217
61764c068ad7d2ec988e6ec315d6ed2ed7489c2e	phd forum: dynamic camera positioning and reconfiguration for multi camera networks	object recognition;binary local visual features;visual sensor networks;object tracking;arm	The main objective of this paper is to present a generalized framework for automatic positioning and reconfiguration of the camera parameters in order to obtain maximum efficiency in a multi camera network. The proposed framework initially addresses the problem of camera positioning and configuration using a virtual 3D model of the environment, including light modeling as well as the deployment of the cameras. It then utilizes the same setup to address reconfiguration issues that may arise due to changes in the environment. In order to address dynamic task-specific real time reconfiguration, the algorithm relies on a quantitative and qualitative approach exploiting the information provided directly from the camera stream in the compressed domain. There is also a provision for addressing user specific requirements and restrictions at all stages.	algorithm;polygonal modeling;requirement;software deployment	Krishna Reddy Konda;Nicola Conci	2014		10.1145/2659021.2675059	smart camera;embedded system;computer vision;camera auto-calibration;real-time computing;simulation;computer science;operating system;cognitive neuroscience of visual object recognition;video tracking;arm architecture	Robotics	46.8143764596558	-38.3819681831605	92445
a1dd5523fb74d85d3f8cf4440288d4b8db63a6aa	hand gesture recognition using input-output hidden markov models	neural networks;hidden markov model;symbolic gestures hand gesture recognition input output hidden markov models dynamic gestures gesture extraction video image sequences tracking skin color blobs body face space deictic gestures;input output;skin color;hidden markov models;image colour analysis;feature extraction;image colour analysis gesture recognition hidden markov models tracking image sequences feature extraction;hand gesture recognition;hidden markov models face detection skin neural networks recurrent neural networks computer vision face recognition pixel image segmentation telecommunications;gesture recognition;tracking;image sequences	A new hand gesture recognition method based on Input– Output Hidden Markov Models is presented. This method deals with the dynamic aspects of gestures. Gestures are extracted from a sequence of video images by tracking the skin–color blobs corresponding to the hand into a body– face space centered on the face of the user. Our goal is to recognize two classes of gestures: deictic and symbolic.	artificial neural network;discriminant;face detection;face space;gesture recognition;hidden markov model;markov chain;microsoft windows;poor posture	Sébastien Marcel;Olivier Bernier;Jean-Emmanuel Viallet;Daniel Collobert	2000		10.1109/AFGR.2000.840674	input/output;computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;gesture recognition;tracking;hidden markov model	Vision	39.46138447418023	-48.74435743489988	92501
737b1bba27440c31c7422c6dd4367bd61acd1c8d	vehicle detection algorithm based on light pairing and tracking at nighttime	headlamps;detection and tracking algorithms;roads;video;calibration;reflection;cameras	Detecting a vehicle to obtain traffic information at nighttime is difficult. This study proposes a vehicle detection algorithm, called the headlight extraction, pairing, and tracking (HLEPT) algorithm, which can acquire traffic information in the rain at nighttime by identifying vehicles through the location of their headlights and other indicative lights. A knowledge-based connected-component procedure, in which vehicles are located by grouping their lights and estimating their features, is proposed. The features of a complex nighttime traffic scene were also analyzed. The HLEPT algorithm includes a headlight extraction algorithm, as well as regulations for the pairing and grouping of lights and light tracking using a Kanade–Lucas– Tomasi tracker to measure traffic flow and velocity. Experimental results demonstrate the feasibility and effectiveness of the proposed approach on vehicle detection in the rain at nighttime. © 2011 SPIE and IS&T. [DOI: 10.1117/1.3663961]	algorithm;kanade–lucas–tomasi feature tracker;knowledge-based systems;tomasi–kanade factorization;velocity (software development)	Weibing Wan;Tao Fang;Shuguang Li	2011	J. Electronic Imaging	10.1117/1.3663961	computer vision;calibration;simulation;reflection;video;optics	Vision	43.33051261933638	-43.403784413405084	92502
d1a9f71e5563a1bb2f956b9b805cfc6aafe4a6e6	robust methods for visual tracking and model alignment	hao;computer science robust methods for visual tracking and model alignment university of maryland;dissertation;college park rama chellappa wu;electrical engineering	Title of dissertation Robust Methods for Visual Tracking and Model Alignment Hao Wu, Doctor of Philosophy, 2009 Directed by Professor Rama Chellappa Department of Electrical and Computer Engineering The ubiquitous presence of cameras and camera networks needs the development of robust visual analytics algorithms. As the building block of many video surveillance tasks, a robust visual tracking algorithm plays an important role in achieving the goal of automatic and robust surveillance. In order to maintain a persistent tracking of objects, it is critical to know when and where the tracking algorithm fails so that remedial measures can be taken to resume tracking. However, most present evaluation methods are ogg-line and based on manually labeled ground truth data. Online evaluation methods in the absence of ground truth are of urgent need. We propose a novel performance evaluation strategy for tracking systems based on particle filter using a time-reversed Markov chain. The posterior density of the time-reversed chain is computed and the distance between the prior density used to initialize the tracking algorithm and the time-reversed posterior density function forms the decision statistic for evaluation. This backward tracking-based performance evaluation strategy is also general enough to be applied to many other tracking algorithms. In this dissertation, we also present a new bidirectional tracking strategy to achieve better performance. Instead of looking only forward in the time domain, we incorporate both forward and backward processing of video frames using a time-reversibility constraint. This leads to a new minimization criterion that combines the forward and backward similarity functions and the distances of the state vectors between the forward and backward states of the tracker. The bidirectional track strategy significantly improves tracking robustness and accuracy. We illustrate the improvements due to the proposed approach for the popular KLT tracker and a search-based tracker. Some objects of interest in surveillance applications like faces have relatively stable structures, which allows us to build parameterized shape models to localize the objects more precisely. There are some widely used algorithms for model alignment; however, most of them suffer from the problem of converging to local extrema when used in practice. In this dissertation, we present a machine learning method to learn a scoring function without local extrema to guide the gradient descent/accent algorithm and find the optimal parameters of the shape model. The method is called Boosted Ranking Model (BRM). By arranging the training samples in some special structure, we feed them pairwise into the rank training algorithm and learn a strong ranking function from a pool of weak features. Theoretically, this method can learn a function with arbitrarily few local extrema as long as the training samples are dense and the representation ability of the features are good enough. The extensive experimental results show that our proposed algorithm, BRM, outperforms existing algorithms. Robust Methods for Visual Tracking and Model Alignment	algorithm;closed-circuit television;computer engineering;gradient descent;ground truth;kanade–lucas–tomasi feature tracker;machine learning;markov chain;maxima and minima;particle filter;performance evaluation;principle of good enough;ranking (information retrieval);reversible computing;tracking system;video tracking;visual analytics	Hao Wu	2009			engineering;artificial intelligence;performance art;cartography	Vision	43.70702353810385	-48.42457187867804	92553
c8e1d80dc02d6b51eec0cd6be46da16b304c3e0d	water region detection supporting ship identification in port surveillance	autonomous port surveillance;water detection;multiple ship detection	In this paper, we present a robust and accurate water region detection technique developed for supporting ship identification. Due to the varying appearance of water body and frequent intrusion of ships, a region-based recognition is proposed. We segment the image into perceptually meaningful segments and find all water segments using a sampling-based Support Vector Machine (SVM). The algorithm is tested on 6 different port surveillance sequences and achieves a pixel classification recall of 97.5% and precision of 96.4%. We also apply our water region detection to support the task of multiple ship detection. Combined with our cabin detector, it successfully removes 74.6% false detections generated in the cabin detection process. A slight decrease of 5% in the recall value is compensated by a significant improvement of 15% in precision.	algorithm;dvd region code;pixel;sampling (signal processing);sensor;support vector machine	Xinfeng Bao;Svitlana Zinger;Rob G. J. Wijnhoven;Peter H. N. de With	2012		10.1007/978-3-642-33140-4_39	simulation;computer security	Vision	41.75019459251685	-43.42036339545207	92590
9850b643ccc55ee468793cb51743de4a2ae1de2f	censure: center surround extremas for realtime feature detection and matching	visual odometry;feature detection;scale space;image registration;real time implementation;scale invariance	We explore the suitability of different feature detectors for the task of image registration, and in particular for visual odometry, using two criteria: stability (persistence across viewpoint change) and accuracy (consistent localization across viewpoint change). In addition to the now-standard SIFT, SURF, FAST, and Harris detectors, we intro- duce a suite of scale-invariant center-surround detectors (CenSurE) that outperform the other detectors, yet have better computational charac- teristics than other scale-space detectors, and are capable of real-time implementation.	feature detection (computer vision)	Motilal Agrawal;Kurt Konolige;Morten Rufus Blas	2008		10.1007/978-3-540-88693-8_8	computer vision;scale space;simulation;computer science;image registration;visual odometry;scale invariance;feature detection;computer graphics (images)	Vision	45.19702520245553	-39.90273226041438	92741
ba3d489ac09b5da9c8532cc24e0f93ae7ca73cea	repmatch: robust feature matching and pose for reconstructing modern cities	correspondence;ransac;structure from motion	A perennial problem in recovering 3-D models from images is repeated structures common in modern cities. The problem can be traced to the feature matcher which needs to match less distinctive features (permitting wide-baselines and avoiding broken sequences), while simultaneously avoiding incorrect matching of ambiguous repeated features. To meet this need, we develop RepMatch, an epipolar guided (assumes predominately camera motion) feature matcher that accommodates both wide-baselines and repeated structures. RepMatch is based on using RANSAC to guide the training of match consistency curves for differentiating true and false matches. By considering the set of all nearest-neighbor matches, RepMatch can procure very large numbers of matches over wide baselines. This in turn lends stability to pose estimation. RepMatch’s performance compares favorably on standard datasets and enables more complete reconstructions of modern architectures.	epipolar geometry;procurement;random sample consensus	Wen-Yan Lin;Siying Liu;Nianjuan Jiang;Minh N. Do;Ping Tan;Jiangbo Lu	2016		10.1007/978-3-319-46448-0_34	computer vision;ransac;structure from motion;computer science;machine learning;pattern recognition	Vision	51.20620442253462	-47.202553107170566	92863
c725e8da99c187defd049de32ef96ef3d2e4196f	scene text recognition using a hough forest implicit shape model and semi-markov conditional random fields	semi markov crfs;part based character models;ism;scene text recognition;hough forests;character recognition	Most of the scene text recognition methods utilize character models only in the character recognition phase, the last stage of the process. In former phases such as text detection, only abstracted features of text regions are used, which might cause loss of information. In this paper, we propose a novel scene text recognition method which fully utilizes model of target characters throughout the process. Each of the target character set is modeled with a part-based object model called implicit shape model (ISM) to achieve robustness for the partial degradation of characters. Towards this end, we trained a Hough forest which localizes and aggregates character parts to detect character candidates in the image. The detected character candidates are verified by organizing the most plausible text lines in a semi-Markov conditional random field (semi-CRF) framework. As concrete character models are utilized throughout the process, even extremely deformed texts are detected and recognized. Graphical abstractDisplay Omitted HighlightsPart-based character models are built in a Hough forest.The Hough forest robustly detects characters even if characters are highly degraded.Checking text line properties effectively remove false detections.Utilizing character models throughout the process raises end-to-end text recognition accuracy.	conditional random field;hough transform;implicit shape model;markov chain;optical character recognition;random forest;semiconductor industry	Jae-Hyun Seok;Jin Hyung Kim	2015	Pattern Recognition	10.1016/j.patcog.2015.05.004	computer vision;speech recognition;computer science;machine learning;pattern recognition	Vision	42.88780622790562	-51.203609199000475	92890
1b057314ff0dd9deb47bfe709c758e09b01df96b	multi-reference object pose indexing and 3-d modeling from video using volume feedback	shape from silhouette;pose estimates;video sequence;point tracking;3d object tracking multireference object pose indexing 3d modelling volume feedback 3d object reconstruction rigid object video sequences flat depth map pose estimates shape from silhouette volume reconstruction pose index video sequence human head feature matching point tracking content based retrieval;3d object tracking;image matching;3d modelling;pose index;motion estimation;video sequences;feature matching;volume feedback;image texture;multireference object pose indexing;image reconstruction;content based retrieval image sequences image reconstruction image texture image matching motion estimation;indexation;object tracking;rigid object;image sequence analysis image reconstruction image texture analysis image matching motion analysis;quality measures;depth map;flat depth map;human head;content based retrieval;shape from silhouette volume reconstruction;3d object reconstruction;image sequences;pose estimation	A system for 3-D reconstruction of a rigid object from monocular video sequences is introduced. Initially an object pose is estimated in each image by locating similar (unknown) texture assuming flat depth map for all images. Shape-from-silhouette as stated in R. Szeliski (1993) is then applied to construct a 3-D model which is used to obtain better pose estimates using a model-based method. Before repeating the process by building a new 3-D model, pose estimates are adjusted to reduce error by maximizing a quality measure for shape-from-silhouette volume reconstruction. Translation of the object in the input sequence is compensated in two stages. The volume feedback is terminated when the updates in pose estimates become small. The final output is a pose index (the last set of pose estimates) and a 3-D model of the object. Good performance of the system is shown by experiments on a real video sequence of a human head. Our method has the following advantages: (1) No model is assumed for the object. (2) Feature points are neither detected nor tracked, thus no problematic feature matching or lengthy point tracking are required. (3) The method generates a high level pose index for the input images, these can be used for content-based retrieval. Our method can also be applied to 3-D object tracking in video.	3d modeling;depth map;experiment;high-level programming language	Alireza Nasiri Avanaki;Babak Hamidzadeh;Faouzi Kossentini;Rabab Kreidieh Ward	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)	10.1109/ISCAS.2004.1328891	iterative reconstruction;image texture;computer vision;pose;3d pose estimation;computer science;video tracking;pattern recognition;motion estimation;depth map;computer graphics (images)	Vision	48.95319603795891	-47.86468497712667	92909
2ce84465b9759166effc7302c2f5339766cc523d	sparsity-based joint gaze correction and face beautification for conferencing video	search space invariant sparsity based joint gaze correction face beautification video conferencing gaze mismatch facial components sparse linear combination pretrained dictionary atoms face reconstruction;training;sparse coding gaze correction face beautification;image reconstruction;dictionaries;transforms;video communication face recognition gaze tracking image coding image reconstruction;face;optimization;face dictionaries image reconstruction optimization transforms training cameras;cameras	"""A well-known problem in video conferencing is gaze mismatch. Instead of relying exclusively on online captured data for rendering, a recent work first trains offline dictionaries using a large image database of movie and TV stars to learn """"beautiful"""" features. During real-time conferencing, one can then simultaneously correct gaze and beautify the subject's facial components in single images by seeking sparse linear combination of pre-trained dictionary atoms for face reconstruction. Extending on this work, we focus on joint gaze correction / face beautification for video. First, we define a large search space invariant to scale, shift and rotation for facial feature beautification based on SIFT. We then address two practical issues unique to video: i) how beautified results can be temporally consistent across group of pictures (GOP), and ii) how blinking eyes can be beautified even though the training database contains only open-eye facial images. Experimental results show that our method achieves the desired temporal consistency, and the blinking process is smooth and natural."""	autostereogram;data dictionary;group of pictures;image;online and offline;real-time locating system;scale-invariant feature transform;sparse matrix	Xianming Liu;Gene Cheung;Deming Zhai;Debin Zhao	2015	2015 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2015.7457830	iterative reconstruction;face;computer vision;speech recognition;computer science;multimedia	Vision	49.1168404704195	-49.663778977727375	93027
732bf05a9a31e0b658cb9f4d29b9f8d5523294eb	3d shape perception from monocular vision, touch, and shape priors		Perceiving accurate 3D object shape is important for robots to interact with the physical world. Current research along this direction has been primarily relying on visual observations. Vision, however useful, has inherent limitations due to occlusions and the 2D-3D ambiguities, especially for perception with a monocular camera. In contrast, touch gets precise local shape information, though its efficiency for reconstructing the entire shape could be low. In this paper, we propose a novel paradigm that efficiently perceives accurate 3D object shape by incorporating visual and tactile observations, as well as prior knowledge of common object shapes learned from large-scale shape repositories. We use vision first, applying neural networks with learned shape priors to predict an object's 3D shape from a single-view color image. We then use tactile sensing to refine the shape; the robot actively touches the object regions where the visual prediction has high uncertainty. Our method efficiently builds the 3D shape of common objects from a color image and a small number of tactile explorations (around 10). Our setup is easy to apply and has potentials to help robots better perform grasping or manipulation tasks on real-world objects.		Shaoxiong Wang;Jiajun Wu;Xingyuan Sun;Wenzhen Yuan;William T. Freeman;Joshua B. Tenenbaum;Edward H. Adelson	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593430	artificial intelligence;computer science;computer vision;pattern recognition;iterative reconstruction;robot;artificial neural network;prior probability;perception;monocular vision;monocular;color image	Robotics	50.16082822541449	-41.873935883819165	93284
0de46e6768acaa71745406d646573644b975dc62	scene-domain active part models for object representation	3d landmark shape and viewpoint estimation scene domain active part models object representation 2d images 3d geometric statistics 3d scene domain learning algorithms inference algorithms 2d object localization image understanding tasks object detection;solid modeling three dimensional displays data models deformable models training data shape cameras;statistics computational geometry image representation inference mechanisms learning artificial intelligence object detection	In this paper, we are interested in enhancing the expressivity and robustness of part-based models for object representation, in the common scenario where the training data are based on 2D images. To this end, we propose scene-domain active part models (SDAPM), which reconstruct and characterize the 3D geometric statistics between object's parts in 3D scene-domain by using 2D training data in the image-domain alone. And on top of this, we explicitly model and handle occlusions in SDAPM. Together with the developed learning and inference algorithms, such a model provides rich object descriptions, including 2D object and parts localization, 3D landmark shape and camera viewpoint, which offers an effective representation to various image understanding tasks, such as object and parts detection, 3D landmark shape and viewpoint estimation from images. Experiments on the above tasks show that SDAPM outperforms previous part-based models, and thus demonstrates the potential of the proposed technique.	3d computer graphics;algorithm;computer vision;experiment;graphics processing unit;ibm notes;part-based models	Zhou Ren;Chaohui Wang;Alan L. Yuille	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.287	active shape model;computer vision;object model;computer science;viola–jones object detection framework;machine learning;pattern recognition	Vision	45.936276475786926	-51.70801774636166	93354
3c35a178e664a2ce339278dc6af7f63f2c5b38f2	object detection method in traffic by on-board computer vision with time delay neural network		Abstract A collision avoidance system play an important role in reducing incidents occurring on the road, which the object detection is crucial to enable obstacle avoidance for this system. The objective of this paper is to improve general object detection methods for vehicles in order to prevent a collision of the vehicles and the obstacles - of which we do not know the exact shape, size or color. A combined computer vision system with artificial neural networks can improve the performance of the vehicle has the ability to see and recognize the obstacles like human beings. In this paper, the authors present the algorithm for vehicles to detect general objects, which can classify obstacles that are real obstacles or fake obstacles, such as a painting or text on the road. The proposed method, we combined on-board computer vision system based on Histograms of Oriented Gradient (HOG) and Time Delay Neural Network (TDNN). We extract feature of the obstacles by HOG and using TDNN to recognize and classify the obstacles. The experimental results showed that this system can detect general objects, and is not restricted to vehicles, objects or pedestrians. It has provided good results along with high accuracy and reliability, which it is accurate enough to provide a warning to the driver when a collision is imminent.	computer vision;object detection;time delay neural network	Jittima Varagula;Panit-a-nong Kulproma;Toshio Itob	2017		10.1016/j.procs.2017.08.185	machine learning;artificial neural network;collision avoidance system;artificial intelligence;collision;time delay neural network;object detection;computer science;computer vision;obstacle avoidance	Vision	41.333161768917336	-44.22626315221964	93390
2fab93eee99272e2c7f92fc74070564921c1dc55	finding the optimal temporal partitioning of video sequences	dynamic programming;short length distribution optimal temporal partitioning video sequence bayesian principle probability dynamic programming sequential process;video sequence;adaptive thresholding;probability;image segmentation;video signal processing bayes methods dynamic programming image segmentation image sequences probability;motion pictures;video signal processing;video sequences gunshot detection systems production bayesian methods partitioning algorithms heuristic algorithms dynamic programming motion pictures streaming media performance analysis;efficient algorithm;bayes methods;gunshot detection systems;bayesian methods;080199 artificial intelligence and image processing not elsewhere classified;video sequences;dynamic program;sequential process;streaming media;heuristic algorithms;optimal temporal partitioning;performance analysis;production;global optimization;bayesian principle;short length distribution;partitioning algorithms;image sequences	The existing techniques for shot partitioning either process each shot boundary independently or proceed sequentially. The sequential process assumes the last shot boundary is correctly detected and utilizes the shot length distribution to adapt the threshold for detecting the next boundary. These techniques are only locally optimal and suffer from the strong assumption about the correct detection of the last boundary. Addressing these fundamental issues, in this paper, we aim to find the global optimal shot partitioning by utilizing Bayesian principles to model the probability of a particular video partition being the shot partition. A computationally efficient algorithm based on dynamic programming is then formulated. The experimental results on a large movie set show that our algorithm performs consistently better than the best adaptive-thresholding technique commonly used for the task	adaptive filter;algorithm;algorithmic efficiency;binary space partitioning;dynamic programming;local optimum;sensor;thresholding (image processing)	Ba Tu Truong;Svetha Venkatesh	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521638	computer vision;bayesian probability;computer science;theoretical computer science;machine learning;dynamic programming;pattern recognition;probability;thresholding;image segmentation;statistics;global optimization	Vision	44.98752032725366	-50.382298048212895	93434
ef320ca75db01fa515f38ba0b390b2925d968853	adapting generic trackers for tracking faces	detectors;computer science;target tracking;cameras;videos;image sequences	A novel system for adapting generic trackers for long-term face tracking in unconstrained videos is proposed. The system treats the tracker as a black box. The only requirement is that the tracker can be reinitialized when needed. The system consists of a generic face detector trained offline and a validator trained online which helps to distinguish the target face from other people's faces and the background. We demonstrate this method on three state-of-the-art generic trackers: OpenTLD, Struck and MIL. For the experiments we use public face videos as well as our own dataset. In all our experiments our face tracking adaptation method shows superior results in comparison with the original trackers.	black box;experiment;face detection;generic function;jones calculus;online and offline;validator	Maria Mikhisor;Geoff Wyvill;Brendan McCane;Steven Mills	2015	2015 International Conference on Image and Vision Computing New Zealand (IVCNZ)	10.1109/IVCNZ.2015.7761570	computer vision;detector;simulation;computer science	Vision	41.60193664708124	-47.95284665755733	93494
2676f398ce0526ee4164bb4dcd8c3f11d988e72e	video analysis of vehicles and persons for surveillance	emergency response;moving object;multiple perspectives;human interaction;video analysis;movement analysis;field of view;situation awareness;vehicle tracking;invariant feature	This chapter presents a multi-perspective vision-based analysis of the activities of vehicles and persons for the enhancement of situational awareness in surveillance. Multiple perspectives provide a useful invariant feature of the object in the image, i.e., the footage area on the ground. Moving objects are detected in the image domain, and the tracking results of the objects are represented in the projection domain using planar homography. Spatiotemporal relationships between human and vehicle tracks are categorized as safe or unsafe situation depending on the site context such as walkway and driveway locations. Semanticlevel information of the situation is achieved with the anticipation of possible directions of near-future tracks using piecewise velocity history. Crowd density is estimated from the footage on the homography plane. Experimental data show promising results. Our framework can be applied to broad range of situational awareness for emergency response, disaster prevention, human interactions in structured environments, and crowd movement analysis in a wide field of view.	categorization;homography (computer vision);interaction;invariant (computer science);velocity (software development)	Sangho Park;Mohan Manubhai Trivedi	2008		10.1007/978-3-540-69209-6_21	situation awareness;computer vision;interpersonal relationship;simulation;field of view;video tracking;multimedia	Vision	40.38051892491966	-44.16269125525891	93813
14a3194bb454f1f2e3fc1452045ac18c69959368	fast object detection using multistage particle window deformable part model	adaptive particle window generation deformable part model multistage particle window early jump;early jump;video signal processing object detection object tracking pedestrians;multimedia communication;on road videos fast object detection framework multistage particle window strategy cascade deformable part model early jump scheme adaptive particle window generation efficient preprocessing dpm vehicle detection pedestrian detection;multistage particle window;adaptive particle window generation;deformable part model	For object detection, evaluating all sliding windows at various scales draws a computational efficiency issue. In this paper, we propose a fast object detection framework using the multistage particle window strategy to accelerate the cascade deformable part model (DPM). Coupling this strategy with the proposed early jump scheme, adaptive particle window generation, and efficient preprocessing, we demonstrate that the proposed method runs 34.5 times faster than the conventional DPM to detect objects in images, and is able to efficiently detect vehicles and pedestrians in on-road videos.	microsoft windows;multistage amplifier;object detection;preprocessor;window function	Wei-Ta Chu;Ming-Hung Hsu	2014	2014 IEEE International Symposium on Multimedia	10.1109/ISM.2014.23	computer vision;simulation;computer science;computer graphics (images)	Vision	41.69613335585624	-49.904133199291465	93882
2bcf848939fe290356b715e42f2f5761bf5c4ada	target contour recovering for tracking people in complex environments	image processing computer assisted;algorithms;pattern recognition automated;humans;form perception	Recovering people contours from partial occlusion is a challenging problem in a visual tracking system. Partial occlusions would bring about unreasonable contour changes of the target object. In this paper, a novel method is presented to detect partial occlusion on people contours and recover occluded portions. Unlike other occlusion detection methods, the proposed method is only based on contours, which makes itself more flexible to be extended for further applications. Experiments with synthetic images demonstrate the accuracy of the method for detecting partial occlusions, and experiments on real-world video sequence are also carried out to prove that the method is also good enough to be used to recover target contours.	algorithm;area striata structure;contour line;entity name part qualifier - adopted;experiment;matching;morphing;obstruction;performance;physical object;principle of good enough;sensor;synthetic data;synthetic intelligence;thin plate spline;tracking system;video tracking;world file	Jianhua Zhang;Sheng Liu;Youfu Li;Jianwei Zhang	2012		10.1155/2012/506908	computer vision;form perception;computer science;communication;algorithm;computer graphics (images)	Vision	44.1450195636166	-47.00940520451113	93901
32825e0764be9864eb811d79b3ca5fe7798e2e33	adaptive driver assistance system based on traffic information saliency map	road safety accident prevention driver information systems intelligent transportation systems pedestrians road accidents;driver attention monitoring adaptive driver assistance system accident prevention careless driving inattentive driving driver traffic information cognitive information driver condition checking driver status detection driving ability tism external road information bottom up traffic information saliency map top down importance information pedestrian detection traffic light detection;traffic light recognition advanced driver assistance system gaze detection saliency map;vehicles monitoring roads image color analysis feature extraction magnetic heads head	In this paper, we propose a framework that can prevent accidents due to careless or inattentive driving by providing the necessary traffic information to the driver. The proposed system complements the driver by providing the missed cognitive information regarding the traffic. The proposed system is divided into three parts. First, the system checks the condition of the driver in real time, and detects the status of the driver in terms of driving ability. Second, we propose bottom-up and top-down processes based on Traffic Information Saliency Map (TISM) which contains the distribution corresponding to the external road information using bottom-up traffic information saliency map and top-down importance information such as pedestrian and traffic light detection results. Computer experimental results show that the proposed method works well for monitoring of internal situation for driver's attention as well as external environment.	algorithm;artificial neural network;bottom-up parsing;bottom-up proteomics;computation;embedded system;finite-state machine;human visual system model;neural networks;real-time computing;sensor;top-down and bottom-up design	Jihun Kim;Seonggyu Kim;Rammohan Mallipeddi;Gil-Jin Jang;Minho Lee	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727434	computer vision;simulation;advanced driver assistance systems;computer security	EDA	40.72558543632611	-44.77497498573034	93969
33b3958843980f1e755c6244ad54ca5ac304f4d7	evaluation of effectivity of omnidirectional image sensor copis in a real environment	mobile robot;real time;robot navigation;omnidirectional image;indoor environment;image processing methods;image sensor	"""We designed a new omnidirectional image sensor COPIS (COnic Projection Image Sensor) to guide the navigation of a mobile robot. Under the assumption of the known'motion of the robot (COPIS), an environmental map.of an indoor scene is generated by monitoring azimuth change in the image. We did several experiments in the simple indoor environment to examine the potential of COPIS against effects of observational errors in realtime navigation. The precision of obtained environmental maps was sufficient for robot navigation in such environment. In this paper, we improve the image processing method for extracting and tracking vertical edges, and also evaluate the effectivity of omnidirectional image sensor COPlS in a real indoor and outdoor environment. 1.Introduction While there has been much work on mobile robots with vision systems [I ][2][3], most robots view things only in front of them and avoid static obstacles and as a result, they may collide against objects moving from the side or from behind. To overcome these drawbacks, a sensor is needed to view the environment around the robot so that it may navigate safely. We have proposed an omnidirectional image sensor COPlS (COnic Projection Image Sensor) for guiding navigaiion of a mobile robot [4][5]. The key feature of COPIS is passive sensing of the omnidirectional environment in real-time (at the frame rate of a TV camera), using a conic mirror. Mapping of the scene onto the image by COPlS involves a conic projection and vertical edges in the environment appear as lines radiating from the image center. The main field viewed by COPlS is a side view and the resolution along a radial direction of conic mirror is sufficient to extract vertical edges. We also reported a method of map generation, which is based on azimuth information in the image sequence [6][7]. Under the assumption of known motion of the robot, locations of objects around the robot can be estimated by detecting their azimuth changes in the omnidirectional image. One can recognize the surface of objects and estimate the free space for a mobile robot from the geometrical relation between object points in the image. Using this method, the robot generates an environmental map of an indoor scene while i t is moving in the environment. The method used properties common to many indoor environments. The floor is almost flat and many visible vertical edges are present a b u t the room. heref fore, the method can be used for mobile robots work'ing in most buildings or plants. Using this method, we did several experiments in the simple indoor environment. The obtained precision of environmental maps was sufficient for robot navigation in such environment. However, in this experiment, we simplified the image processing method and the experimental environment to examine the potential of COPIS against effects of observational errors in real-time navigation. In this paper, we improve the image processing method for extracting and tracking vertical edges to discriminate these edges and evaluate the effectivity of omnidirectional image sensor COPlS in a real indoor and outdoor environment. Experiments done in an actual environment support the validity of our method. 2.Conic Projection Image Sensor (COPIS) COPIS mounted on a robot consists of a conic mirror and a TV camera, with its optical axis aligned with the cone's axis, in a cylindrical glass tube. A prototype of the COPlS system is shown in Fig.1. COPlS maps the scene onto the image plane through a conic mirror and a lens. We call this mapping """" conic projection """". We describe here how features of the scene appear in the image. Let us use the three dimensional coordinate system 0XYZ, aligned with the image coordinate system o-xy and the Z-axis pointed toward the cone's vertex (see Fig.2). We fix origin 0 at the camera center, thus the image plane is on a level off , where f is the focal length of the camera. A conic mirror yields the image of a point in space on a vertical plane through the point and its axis. Thus, the point P at (X,Y,Z) is projected onto the image point p at (x,y) such that Y Y tan0 = X -x ' (1) If a line is vertical, it appears radially in the image plane, as shown in Fig.2. Thus, COPIS can easily find and track the vertical edges by searching consecutive images for radial edges. For further details, we refer to reader to our preceding report [S]. 3.Assumptions The following properties of the environment and the mobile robot are assumed for image analysis. The floor is almost flat and horizontal while walls and static objects such as desks or shelves have vertical planes. The robot moves in a man-made environment such as a room or a corridor. Its motion parameters are two translational components (U,V) and one rotational 4.Ceneration of an Environmental Map projection. In this projection. the data are segmented into Let us denote the robot motion by (u(t),v(t)) and cp(t). regions of each edge and the azimuth of the edges is cp(t) is the pan angle at time t. Defining the position of P at estimated by calculating the average angle of each region. time t=O by PI(X(O),Y(O),Z(O)), the relative velocity of In the previous method, i t is difficult to distinguish the the point p in the environment at time t is represented by projection data Of all edges in the area where there are (-u(t). -v(t), 0). We get the location of point P at time t as dense vertical edges. Dense areas often appear in the real"""	apache axis;c date and time functions;experiment;focal (programming language);fractal dimension;image analysis;image plane;image processing;image sensor;map projection;mobile robot;optic axis of a crystal;pinhole camera model;prototype;radial (radio);real-time clock;robotic mapping;velocity (software development)	Ryota Hiura;Yasushi Yagi;Masahiko Yachida	1994			mobile robot;computer vision;simulation;computer science;image sensor;mobile robot navigation	Robotics	52.273239600876494	-41.07041329748699	94030
7b091eb006f26dc458746c9e241f8b66f278bf03	homography based monocular dense reconstruction for a mobile robot	constrained motion;obstacle avoidance;servo manipulator;assist force	Single camera mobile robots, wherein the single camera becomes the quintessential sensor for robotic tasks such as localization, mapping and obstacle avoidance are challenging. From such a standpoint, we demonstrate a dense reconstruction as conducted by a navigating robot with a monocular camera. Unlike most other dense reconstruction methods this approach first identifies planar areas through homography. These segments are tracked over multiple views with homography based dense correspondences. The tracked correspondences are reconstructed within a VSLAM formulation, wherein the dense reconstructed points get added to the existing SLAM computed structure. The dense structure is further refined using a modified Bundle Adjustment which minimizes projection error in 3D to align with a inferred model of the scene. A mobile robot can thus make use of the reconstructed ground plane and planar obstacles to compute a collision free trajectory.	align (company);bundle adjustment;homography (computer vision);internationalization and localization;mobile robot;obstacle avoidance;simultaneous localization and mapping	Laxit Gavshinde;K. Madhava Krishna	2013		10.1145/2506095.2506147	computer vision;simulation;computer science;artificial intelligence;obstacle avoidance	Robotics	52.178543467521656	-46.0933382014762	94033
02653556b757929a577fb6e4332e27fbb35d91a1	learning to track objects through unobserved regions	tracking system;learning;testing;tracking smart cameras testing learning computer science artificial intelligence laboratories robustness vehicles cities and towns;multiple objectives;smart cameras;cities and towns;artificial intelligence;robustness;vehicles;computer science;synthetic data;tracking;hypothesis test	As tracking systems become more effective at reliably tracking multiple objects over extended periods of time within single camera views and across overlapping camera views, increasing attention is being focused on tracking objects through periods where they are not observed. This paper investigates an unsupervised hypothesis testing method for learning the characteristics of objects passing unobserved from one observed location to another. This method not only reliably determines whether objects predictably pass from one location to another without performing explicit correspondence, but it approximates the likelihood of those transitions. It is robust to non-stationary traffic processes that result from traffic lights, vehicle grouping, and other non-linear vehicle-vehicle interactions. Synthetic data allows us to test and verify our results for complex traffic situations over multiple city blocks and contrast it with previous approaches.	closed-circuit television;information theory;interaction;nonlinear system;stationary process;synthetic data;tracking system	Chris Stauffer	2005	2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05) - Volume 1	10.1109/ACVMOT.2005.69	smart camera;computer vision;statistical hypothesis testing;simulation;tracking system;computer science;artificial intelligence;machine learning;tracking;software testing;robustness;synthetic data	Vision	44.56739513361428	-47.45364641492744	94141
4a44381f7c639451a797b2d3016b1d4cb54736dc	pedestrian travel time estimation in crowded scenes	legged locomotion correlation estimation trajectory feature extraction computer vision surveillance;abnormal pedestrian behavior detection pedestrian travel time estimation statistic global distributions crowd densities crowd velocities active regions walking regions source destination traffic flow scene features moving persons modeling stationary persons modeling crowd scene understanding pedestrian behavior analysis surveillance applications dynamic scene monitoring regions blocking traffics localization;video surveillance behavioural sciences feature extraction image motion analysis object detection pedestrians road traffic statistical analysis	In this paper, we target on the problem of estimating the statistic of pedestrian travel time within a period from an entrance to a destination in a crowded scene. Such estimation is based on the global distributions of crowd densities and velocities instead of complete trajectories of pedestrians, which cannot be obtained in crowded scenes. The proposed method is motivated by our statistical investigation into the correlations between travel time and global properties of crowded scenes. Active regions are created for each source-destination pair to model the probable walking regions over the corresponding source-destination traffic flow. Two sets of scene features are specially designed for modeling moving and stationary persons inside the active regions and their influences on pedestrian travel time. The estimation of pedestrian travel time provides valuable information for both crowd scene understanding and pedestrian behavior analysis, but was not sufficiently studied in literature. The effectiveness of the proposed pedestrian travel time estimation model is demonstrated through several surveillance applications, including dynamic scene monitoring, localization of regions blocking traffics, and detection of abnormal pedestrian behaviors. Many more valuable applications based on our method are to be explored in the future.	blocking (computing);stationary process	Shuai Yi;Hongsheng Li;Xiaogang Wang	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.359	computer vision;simulation	Vision	39.732536672921945	-45.7419223022228	94208
f055416d6b7dd8644148f5ca7043fff014975ac4	new objects seizure method in mobile robotic using a visual servoing and neural network classification	object recognition;manipulators;image motion analysis;seizure strategy robot manipulator visual servoing image processing neural network;image processing;neural nets;mobile robot;image matching;mobile robots visual servoing neural networks robot sensing systems cameras manipulators robot vision systems sonar humans object recognition;mobile robots;robot manipulator;manipulators mobile robots image motion analysis robot vision feature extraction image matching object recognition pattern classification neural nets;robot vision;feature extraction;robot manipulatory objects seizure mobile robotic visual servoing neural network classification robotic service manipulator arm sensors camera sonar human environment ultrasonic information object recognition distance information object image edge extraction image processing image matching object matching image information;pattern classification;visual servoing;neural network	This paper describes the development of a new objects seizure method in the robotic service framework. Its main objective is to make a manipulator arm equipped with a grip and two sensors (camera and sonar) able to handle objects in a human environment. Ultrasonic information is used by a neural classifier in order to give us object recognition and distance information. In the same time, a camera takes an image of the object. We extract its edges by image processing. So it is possible to match image center and object center. The object seizure strategy uses image and ultrasonic information both. This strategy is applied to two kinds of objects: sphere and cylinder.	artificial neural network;cylinder-head-sector;image processing;outline of object recognition;robot;sonar (symantec);sensor;visual servoing	Mohamed Trabelsi;Naima Aitoufroukh;Sylvie Lelandais	2005	2005 International Symposium on Computational Intelligence in Robotics and Automation	10.1109/CIRA.2005.1554344	mobile robot;computer vision;simulation;deep-sky object;computer science;object-oriented design;machine learning;artificial neural network	Robotics	48.584231619749424	-38.63825542133923	94254
d9ef23d77716db5502e90cbf2450451c88b455ac	computer vision based method for real-time fire and flame detection	fire detection in video;moving object;pedestrian safety;poison control;injury prevention;real time;safety literature;event detection;traffic safety;injury control;computer vision;home safety;injury research;safety abstracts;research paper;gaussian mixture model;wavelet transform;human factors;event detection in video;occupational safety;safety;detection algorithm;safety research;accident prevention;violence prevention;bicycle safety;poisoning prevention;falls;ergonomics;suicide prevention	This paper proposes a novel method to detect fire and/or flames in real-time by processing the video data generated by an ordinary camera monitoring a scene. In addition to ordinary motion and color clues, flame and fire flicker is detected by analyzing the video in the wavelet domain. Quasi-periodic behavior in flame boundaries is detected by performing temporal wavelet transform. Color variations in flame regions are detected by computing the spatial wavelet transform of moving fire-colored regions. Another clue used in the fire detection algorithm is the irregularity of the boundary of the fire-colored region. All of the above clues are combined to reach a final decision. Experimental results show that the proposed method is very successful in detecting fire and/or flames. In addition, it drastically reduces the false alarms issued to ordinary fire-colored moving objects as compared to the methods using only motion and color clues.	computer vision;real-time clock	B. Ugur Töreyin;Yigithan Dedeoglu;Ugur Güdükbay;A. Enis Çetin	2006	Pattern Recognition Letters	10.1016/j.patrec.2005.06.015	computer vision;simulation;computer science;suicide prevention;human factors and ergonomics;injury prevention;mixture model;computer security;wavelet transform	Vision	40.87634255419666	-45.7969415888391	94288
fc643993aabfc9fc2625b63ce7c19d0a58322e5a	2.5d multi-view gait recognition based on point cloud registration	health research;uk clinical guidelines;point cloud registration;biological patents;europe pubmed central;qa mathematics;gait;citation search;2 5d modeling;uk phd theses thesis;life sciences;person identification;uk research reports;medical journals;europe pmc;biomedical research;ta engineering general civil engineering general;bioinformatics	This paper presents a method for modeling a 2.5-dimensional (2.5D) human body and extracting the gait features for identifying the human subject. To achieve view-invariant gait recognition, a multi-view synthesizing method based on point cloud registration (MVSM) to generate multi-view training galleries is proposed. The concept of a density and curvature-based Color Gait Curvature Image is introduced to map 2.5D data onto a 2D space to enable data dimension reduction by discrete cosine transform and 2D principle component analysis. Gait recognition is achieved via a 2.5D view-invariant gait recognition method based on point cloud registration. Experimental results on the in-house database captured by a Microsoft Kinect camera show a significant performance gain when using MVSM.	2.5d;calibration;dimensionality reduction;discrete cosine transform;gait analysis;kinect;normal statistical distribution;numerous;point cloud;point set registration;principal component analysis;projections and predictions;voxel;registration - actclass	Jin Tang;Jian Luo;Tardi Tjahjadi;Yan Gao	2014		10.3390/s140406124	simulation;telecommunications;computer science;bioinformatics;engineering;electrical engineering;data mining;gait;multimedia	Vision	48.02368089642292	-50.31971071255165	94319
c4db68a16cc0fe798206049c2f5d8c1a4a658c37	nonlinear target tracking for threat detection using rssi and optical fusion	wireless communication;target tracking wireless communication wireless sensor networks cameras ieee 802 11 standard calibration mathematical model;target tracking information fusion dddas rssi;mathematical model;ieee 802 11 standard;target tracking;video surveillance image fusion object detection optical images target tracking video cameras;calibration;wireless sensor networks;cameras;threat detection video surveillance data analysis homeland security nonlinear target tracking distributed camera systems track obscuration problem optical measurements received signal strength indicator techniques rssi techniques wireless sensing systems continuous tracking line of sight rssi measurements location estimate accuracy optical fusion	Video surveillance data analysis plays a key role in homeland security where non-linear target tracking through distributed camera systems is often necessary. However, such a tracking problem poses a grand challenge because the subject of interest can be lost through obscuration. In this paper, we propose a novel approach to solving the track obscuration problem by f using optical measurements and Received-Signal-Strength-Indicator (RSSI) techniques. While the RSSI of wireless sensing systems is coordinated to allow for continuous tracking, a distributed camera system is applied to track a target in its line of sight. The video and RSSI measurements are fused to enhance the location estimate accuracy of the studied target. Our real-world experiments demonstrate the applicability and accuracy of the proposed approach.	experiment;grand challenges;nonlinear system;threat (computer)	Tommy Chin;Kaiqi Xiong;Erik Blasch	2015	2015 18th International Conference on Information Fusion (Fusion)		computer vision;tracking system;geography;video tracking;computer security;remote sensing	Robotics	47.5532694655931	-41.91012151595739	94337
b2516b4eb55fb95e60212a3e7a08590f658610f7	active target tracking: a simplified view aligning method for binocular camera model		Abstract While monitoring large-scale scenes, visual surveillance systems are often confronted with a dilemma between obtaining efficient coverage of the scene and getting sufficient resolution of the targets of interest. To alleviate the contradiction, a solution based on binocular cameras has been proposed. In the solution, an active pan–tilt–zoom (PTZ) camera and a fixed camera operate in a collaborative way by dynamically aligning their fields of view. However, due to the asymmetric camera distribution structure and fast-changing scenario, the accurate alignment is yet challenging. In this paper, a novel view aligning method is presented which introduces three reasonable and mild simplifications to transform the complex view aligning problem into a simple arctangent control function. These simplifications not only relax the restrictions of target depth and the intrinsic parameters of PTZ camera, but also constrain the relative external parameters with a coaxial structure. Meanwhile, a corresponding calibration method is designed to estimate the model parameters off-line. The proposed method was tested in comprehensive cases with relatively close target, wide-range scene and altitude variation. Both the simulation and real scene experiments demonstrate that our method outperforms other state-of-the-art methods on accuracy and effectiveness for active target tracking tasks.		Xinzhao Li;Yuanqi Su;Yuehu Liu;Shaozhuo Zhai;Ying Wu	2018	Computer Vision and Image Understanding	10.1016/j.cviu.2018.09.005	computer vision;mathematics;machine learning;dilemma;artificial intelligence;coaxial	Vision	52.735938537250476	-45.33414439696693	94446
c4609daf8976fd60a9fd9e1ccd639ba2302ee4a9	an intelligent system for road moving object detection		In this work, we propose a new application for road moving object detection in the goal to participate in reducing the big number of road accidents. Road moving object detection in a traffic video is a difficult task. Hence, in this work we present a new system in order to control the outside car risks by detecting and tracking of different road moving objects. This developed system is based on computer vision techniques that aim to solve this problem by using Haar like features and Background Subtraction technique. Experimental results indicate that the suggested method of moving object detection can be achieved with a high detection ratio.	artificial intelligence;object detection	Mejdi Ben Dkhil;Ali Wali;Adel M. Alimi	2015		10.1007/978-3-319-27221-4_16	computer vision;object detection;computer science;haar-like features;background subtraction;artificial intelligence	Robotics	41.386193757329764	-44.886732766578106	94506
7fc65d5a5ac69b031482d8c05d32fb1aa0cee441	real-time detection, tracking and classification of multiple moving objects in uav videos		Unnamed Aerial Vehicles (UAVs) are becoming increasingly popular and widely used for surveillance and reconnaissance. There are some recent studies regarding moving object detection, tracking, and classification from UAV videos. A unifying study, which also extends the application scope of such previous works and provides real-time results, is absent from the literature. This paper aims to fill this gap by presenting a framework that can robustly detect, track and classify multiple moving objects in real-time, using commercially available UAV systems and a common laptop computer. The framework can additionally deliver practical information about the detected objects, such as their coordinates and velocities. The performance of the proposed framework, which surpasses human capabilities for moving object detection, is reported and discussed.	aerial photography;geographic coordinate system;laptop;object detection;real-time clock;real-time computing;real-time locating system;real-time transcription;unmanned aerial vehicle	Huseyin Can Baykara;Erdem Biyik;Gamze Gul;Deniz Onural;Ahmet Safa Ozturk	2017	2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2017.00145	machine learning;artificial intelligence;laptop;object detection;image registration;computer science	Robotics	45.42103130691911	-39.88547504272566	94570
80cfcdc41ac2847363bafb8279ef05d26d6505eb	a fast and robust extrinsic calibration for rgb-d camera networks †	3d reconstruction;rgb-d camera;camera network calibration;spherical object	From object tracking to 3D reconstruction, RGB-Depth (RGB-D) camera networks play an increasingly important role in many vision and graphics applications. Practical applications often use sparsely-placed cameras to maximize visibility, while using as few cameras as possible to minimize cost. In general, it is challenging to calibrate sparse camera networks due to the lack of shared scene features across different camera views. In this paper, we propose a novel algorithm that can accurately and rapidly calibrate the geometric relationships across an arbitrary number of RGB-D cameras on a network. Our work has a number of novel features. First, to cope with the wide separation between different cameras, we establish view correspondences by using a spherical calibration object. We show that this approach outperforms other techniques based on planar calibration objects. Second, instead of modeling camera extrinsic calibration using rigid transformation, which is optimal only for pinhole cameras, we systematically test different view transformation functions including rigid transformation, polynomial transformation and manifold regression to determine the most robust mapping that generalizes well to unseen data. Third, we reformulate the celebrated bundle adjustment procedure to minimize the global 3D reprojection error so as to fine-tune the initial estimates. Finally, our scalable client-server architecture is computationally efficient: the calibration of a five-camera system, including data capture, can be done in minutes using only commodity PCs. Our proposed framework is compared with other state-of-the-arts systems using both quantitative measurements and visual alignment results of the merged point clouds.	3d reconstruction;algorithm;algorithmic efficiency;alignment;bundle adjustment;calibration;client–server model;code;customize;distortion;estimated;glossary of computer graphics;manifold alignment;merge;muscle rigidity;peripheral;physical object;point cloud;polynomial;polynomial-time reduction;real-time clock;refinement (computing);reprojection error;scalability;server (computer);server (computing);sparse matrix;transcutaneous electric nerve stimulation;virtual reality headset;manifold	Po-Chang Su;Ju Shen;Wanxin Xu;Sen-Ching S. Cheung;Ying Luo	2018		10.3390/s18010235		Vision	53.74428255588421	-47.3684017715373	94709
3ba77d6c586c3638b4dae9fc2b78d37660c48928	real-time pedestrian counting by active linear cameras	new technology;real time;traffic control;video processing;transport system;optical sensor;cameras	ch Abstract. Real-time counting of pedestrians traveling through a transport system are increasingly required for traffic control and management by the companies operating such systems. One of the most widely used systems for counting passengers consists of a mechanical gate equipped with a counter. Such simple systems, however, are not able to count passengers jumping above the gates. Moreover, passengers carrying large luggage or bags may meet some difficulties when going through such gates. The ideal solution is a contact-free counting system that would bring more comfort of use for the passengers. For these reasons, we propose to use a video processing system instead of these mechanical gates. The optical sensors discussed offer several advantages, including well-defined detection areas, fast response time, and reliable counting capability. A new technology is developed and tested, based on linear cameras. For the algorithms, thanks to the principle of our system, no assumption is made about the scene being analyzed and the nature of pedestrian movements to enable the system to run in real time. We also consider the problems presented by crowded scenes, when a high incidence of pedestrians occlusions occurs. Preliminary results have shown that this system is very efficient when the passengers crossing the optical gate are well separated. In other cases, such as in compact crowd conditions, good accuracy in terms of counting in real time is demonstrated. These results are illustrated by means of a number of sequences shot in field conditions. © 1996 SPIE and IS&T.	algorithm;incidence matrix;passive optical network;real-time transcription;response time (technology);sensor;video processing	Louahdi Khoudour;Luc Duvieubourg;Jean-Pierre Deparis	1996	J. Electronic Imaging	10.1117/12.245848	embedded system;stereo cameras;computer vision;simulation;computer science;sensor;video processing;three-ccd camera	Robotics	44.23819284113106	-41.06598927034929	94740
97e5503c4ef44f86ba6c97f2e728b979a416c353	an approach to detect crowd panic behavior using flow-based feature	image motion analysis;training;computer vision;qa75 electronic computers computer science;optical imaging;shape;feature extraction;adaptive optics	With the purpose of achieving automated detection of crowd abnormal behavior in public, this paper discusses the category of typical crowd and individual behaviors and their patterns. Popular image features for abnormal behavior detection are also introduced, including global flow based features such as optical flow, and local spatio-temporal based features such as Spatio-temporal Volume (STV). After reviewing some relative abnormal behavior detection algorithms, a brand-new approach to detect crowd panic behavior has been proposed based on optical flow features in this paper. During the experiments, all panic behaviors are successfully detected. In the end, the future work to improve current approach has been discussed.	algorithm;categorization;computational complexity theory;computer vision;crowd simulation;crowdsourcing;experiment;kernel panic;list of sega arcade system boards;network congestion;optical flow;sensor;vagueness	Yu Hao;Zhijie Xu;Jing Wang;Ying Liu;Jiulun Fan	2016	2016 22nd International Conference on Automation and Computing (ICAC)	10.1109/IConAC.2016.7604963	computer vision;simulation;feature extraction;shape;computer science;optical imaging;multimedia;adaptive optics;computer security	Robotics	39.526723494768525	-45.2761405693545	94780
39b3d33b73754819aea9fa6fe096eee1ac7b66c2	a hierarchical estimator for object tracking	signal image and speech processing;quantum information technology spintronics;object tracking	A closed-loop local-global integrated hierarchical estimator (CLGIHE) approach for object tracking using multiple cameras is proposed. The Kalman filter is used in both the local and global estimates. In contrast to existing approaches where the local and global estimations are performed independently, the proposed approach combines local and global estimates into one for mutual compensation. Consequently, the Kalman-filter-based data fusion optimally adjusts the fusion gain based on environment conditions derived from each local estimator. The global estimation outputs are included in the local estimation process. Closedloop mutual compensation between the local and global estimations is thus achieved to obtain higher tracking accuracy. A set of image sequences from multiple views are applied to evaluate performance. Computer simulation and experimental results indicate that the proposed approach successfully tracks objects.	algorithm;computer simulation;feedback;iterative method;kalman filter;mathematical optimization	Chin-Wen Wu;Yi-Nung Chung;Pau-Choo Chung	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/592960	computer vision;simulation;computer science;machine learning;video tracking	Robotics	45.62976992018839	-47.29081020083593	95139
8f974ff78494381dfeede1dd579dfa994c05d5a9	incident detection based on semantic hierarchy composed of the spatio-temporal mrf model and statistical reasoning	traffic accident;road accidents;road traffic;continuous variable;object detection statistical analysis road traffic road accidents;event detection;event detection image sensors accidents monitoring shape layout cameras road vehicles information analysis traffic control;statistical analysis;traffic monitoring;mortality rate;event class operators statistical reasoning incident detection semantic hierarchy traffic accidents automatic event detection logical reasoning individual behavior relative behavior coordinate class operators behavior class operators;object detection	Japan and EU governments aim to reduce the mortal rate from traffic accidents by 50% at the end of 2010. To achieve this goal, sufficient information about accidents need to be gathered, so investigators can evaluate for causes and how to prevent the future accidents. In this work, we develop three algorithms for automatic event detection, and provide video clip during accidents as our results. Three algorithms are (1) logical reasoning focusing on an individual behavior, (2) logical reasoning focusing on relative behavior and (3) classification with continuous variables by hyperplane. Our algorithms utilize a semantic hierarchy composed of three kinds of operators (coordinate-class, behavior class, event-class). The three operating classes provide the context of traffic events similar to the understanding of a human operator on traffic scenes. We evaluate our algorithms on actual traffic scene taken for 18 months. Our algorithms can detect more than 90%.	algorithm;markov random field;video clip	Shunsuke Kamijo;Masahiro Harada;Masao Sakauchi	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1398333	simulation;data mining;mortality rate;computer security;statistics	Robotics	39.761248943749976	-45.18691478365646	95249
4479f57a82af8fa2ff502a49a5e9b56edd7fcf85	calibration between color camera and 3d lidar instruments with a polygonal planar board	biological patents;biomedical journals;text mining;europe pubmed central;citation search;calibration board;citation networks;research articles;abstracts;calibration matrix;open access;life sciences;clinical guidelines;3d lidar;3d point clouds;sensor fusion;full text;camera calibration;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Calibration between color camera and 3D Light Detection And Ranging (LIDAR) equipment is an essential process for data fusion. The goal of this paper is to improve the calibration accuracy between a camera and a 3D LIDAR. In particular, we are interested in calibrating a low resolution 3D LIDAR with a relatively small number of vertical sensors. Our goal is achieved by employing a new methodology for the calibration board, which exploits 2D-3D correspondences. The 3D corresponding points are estimated from the scanned laser points on the polygonal planar board with adjacent sides. Since the lengths of adjacent sides are known, we can estimate the vertices of the board as a meeting point of two projected sides of the polygonal board. The estimated vertices from the range data and those detected from the color image serve as the corresponding points for the calibration. Experiments using a low-resolution LIDAR with 32 sensors show robust results.	calibration;camera resectioning;color image;correspondence problem;image resolution;instrument - device;monochrome;motherboard;projections and predictions;scanning;vertex (geometry);sensor (device)	Yoonsu Park;Seok Min Yun;Chee Sun Won;Kyungeun Cho;Kyhyun Um;Sungdae Sim	2014		10.3390/s140305333	computer vision;text mining;camera resectioning;simulation;computer science;sensor fusion;remote sensing	Robotics	52.79848903199252	-41.99984315366941	95288
747525cdb8792997090069db59c0ac104721e474	pedestrian detection for counting applications using a top-view camera	pedestrian detection;people counting;top-view camera	This paper presents a pedestrian detection framework using a top-view camera. The paper contains two novel contributions for the pedestrian detection task: 1. Using shape context method to estimate the pedestrian directions and normalizing the pedestrian regions. 2. Based on the locations of the extracted head candidates, system chooses the most adaptive classifier from several classifiers automatically. Our proposed methods may solve the difficulties on top-view pedestrian detection field. Experimental was performed on video sequences with different illumination and crowed conditions, the experimental results demonstrate the efficiency of our algorithm.	pedestrian detection	Xue Yuan;Xue-Ye Wei;Yongduan Song	2011	IEICE Transactions		computer vision;simulation;internationalization and localization;computer science;algorithm;computer graphics (images)	Vision	42.39070568242812	-46.797775479740096	95409
a6722d5361cf7b0887414f72b14547d5f5825120	recovering articulated object models from 3d range data	unsupervised learning;range data;graphical model;em algorithm;object model	We address the problem of unsupervised learning of complex articulated object models from 3D range data. We describe an algorithm whose input is a set of meshes corresponding to different configurations of an articulated object. The algorithm automatically recovers a decomposition of the object into approximately rigid parts, the location of the parts in the different object instances, and the articulated object skeleton linking the parts. Our algorithm first registers all the meshes using an unsupervised non-rigid technique described in a companion paper. It then segments the meshes using a graphical model that captures the spatial contiguity of parts. The segmentation is done using the EM algorithm, iterating between finding a decomposition of the object into rigid parts, and finding the location of the parts in the object instances. Although the graphical model is densely connected, the object decomposition step can be performed optimally and efficiently, allowing us to identify a large number of object parts while avoiding local maxima. We demonstrate the algorithm on real world datasets, recovering a 15-part articulated model of a human puppet from just 7 different puppet configurations, as well as a 4 part model of a flexing arm where significant non-rigid deformation was present.	biconnected component;bootstrapping (compilers);coupling (computer programming);expectation–maximization algorithm;graphical model;instance (computer science);iteration;maxima and minima;object file;unsupervised learning	Dragomir Anguelov;Daphne Koller;Hoi-Cheung Pang;Praveen Srinivasan;Sebastian Thrun	2004			unsupervised learning;computer vision;method;object model;expectation–maximization algorithm;computer science;machine learning;pattern recognition;graphical model	ML	46.75735744089326	-51.820287824703044	95481
06f9552cfc44c6b748959926cb0f79699520232d	k-hyperline clustering-based color image segmentation robust to illumination changes				Senquan Yang;Pu Li;Haoxiang Wen;Yuan Xie;Zhaoshui He	2018	Symmetry	10.3390/sym10110610		Vision	41.292078614001866	-48.71028983374751	95597
55125025df0a0fa50b288239a4f10ff2253ee556	6 dof slam using a tof camera: the challenge of a continuously growing number of landmarks	time of flight;image resolution;sensors;real time;cameras three dimensional displays robot kinematics simultaneous localization and mapping robot vision systems;service robots;3d landmark dof slam tof camera localization and mapping service robotics time of flight sensors image resolution computer vision feature extraction;service robotics;localization and mapping;computer vision;robot vision;3d landmark;three dimensional displays;feature extraction;indoor environment;simultaneous localization and mapping;service robot;visual features;dof slam;slam robots cameras feature extraction image resolution robot vision service robots;tof camera;slam robots;robot vision systems;cameras;robot kinematics	Localization and mapping are fundamental problems in service robotics since representations of the environment and knowledge about the own pose significantly simplify the implementation of a series of high-level applications.	high- and low-level;robotics;simultaneous localization and mapping;time-of-flight camera	Siegfried Hochdorfer;Christian Schlegel	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5651229	computer vision;time of flight;simulation;image resolution;feature extraction;computer science;sensor;artificial intelligence;robot kinematics;computer graphics (images);simultaneous localization and mapping	Robotics	51.616065338689836	-39.64827406160272	95772
2c27455df3d7b0dd110f78083345242d69aaafec	improving camera pose estimation via temporal ewa surfel splatting		Camera pose estimation is a fundamental problem of Augmented Reality and 3D reconstruction systems. Recently, despite the new better performing direct methods being developed, state-of-the-art methods are still estimating erroneous poses due to sensor noise, environmental conditions and challenging trajectories. Adding a back-end mapping process, SLAM systems achieve better performance and are more robust, but require higher computational resources, limiting their applicability. Therefore, lighter solutions to improve the accuracy of pose estimates are required. In this work we demonstrate the effectiveness of lighter data structures, namely surface elements, and exploit the temporality of sensor data streams to accumulate moving camera frames and improve tracking. This representation allows us to splat a photometric and geometric model simultaneously and use it to improve the performance of dense RGB-D camera pose estimation methods. Exploiting Elliptical Weighted Average splatting to produce high quality photometric results also allows us to detect erroneous poses through a novel visual quality analysis process. We show evidence of the EWA temporal model's effectiveness in publicly available datasets and argue that point-based representations are a good candidate for building lighter systems that should be further explored.	3d pose estimation;3d reconstruction;alpha compositing;anomaly detection;augmented reality;color image;computation;computational resource;control theory;data structure;display resolution;experiment;geometric modeling;image noise;key frame;mad;open research;simultaneous localization and mapping;surfel;synthetic data;texture splatting;time series;volume rendering;window function	Nikolaos Zioulis;Alexandros Papachristou;Dimitrios Zarpalas;Petros Daras	2017	2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)	10.1109/ISMAR.2017.17	computer vision;3d reconstruction;artificial intelligence;computer science;geometric modeling;simultaneous localization and mapping;surfel;data stream mining;pose;augmented reality;solid modeling	Vision	52.82638318854052	-45.52260059084604	95791
1cedd65cb907372cd2eefa5a6cee043c3820ab83	optical flow templates for mobile robot environment understanding	obstacle detection;mobile robots;dissertation;optical flow;generalized imaging systems			Richard Roberts	2015			mobile robot;computer vision;simulation;geography;communication	Robotics	52.169332459238454	-42.862713723172234	95839
ef939697a448ad42e9093e5c7125403a80582a31	3-d rigid models from partial views - global factorization	non linear optimization;factorization method;field of view;pattern recognition;structure from motion	The so-called factorization methods recover 3-D rigid stru cture from motion by factorizing an observation matrix that collects 2-D projections of features. These met hods became popular due to their robustness—they use a large number of views, which constrains adequately the solu tion—and computational simplicity—the large number of unknowns is computed through an SVD, avoiding non-linear optimization. However, they require that all the entries of the observation matrix are known. This is unlikel y to happen in practice, due to self-occlusion and limited field of view. Also, when processing long videos, regions tha t become occluded often appear again later. Current factorization methods process these as new regions, leadin g to less accurate estimates of 3-D structure. In this paper, we propose a global factorization method that infers comple te 3-D modelsdirectly from the 2-D projections in the entire set of available video frames . Our method decides whether a region that has become visible a region that was seen before, or a previously unseen region, in a glob al way, i.e., by seeking the simplest rigid object that describes well the entire set of observations. This glo bal approach increases significantly the accuracy of the estimates of the 3-D shape of the scene and the 3-D motion of th e camera. Experiments with artificial and real videos illustrate the good performance of our method.	glob (programming);hidden surface determination;linear programming;mathematical optimization;nonlinear programming;nonlinear system;singular value decomposition	Pedro M. Q. Aguiar;Rui F. C. Guerreiro;Bruno B. Gonçalves	2010	CoRR		computer vision;mathematical optimization;structure from motion;field of view;computer science;machine learning;pattern recognition;mathematics;geometry	Vision	51.93325544552419	-49.598317260453534	96018
2c2dfb566379f58cde65dc39b46f5109f308323e	simultaneous gesture segmentation and recognition based on forward spotting accumulative hmms	gesture segmentation;competitive differential observation probability;probability;image segmentation;smart home;probability gesture recognition hidden markov models image segmentation;accumulative hmm;hidden markov models forward contracts lighting control smart homes humans delay effects feature extraction pattern recognition face recognition face detection;hidden markov models;smart home environment gesture segmentation gesture recognition forward spotting competitive differential observation probability sliding window accumulative hmm;smart home environment;forward spotting;gesture recognition;sliding window	In this paper, we propose a forward spotting scheme that executes gesture segmentation and recognition simultaneously by detecting start point. By using competitive differential observation probability, sliding window and accumulative HMMs, we apply the proposed method to recognize the upper-body gestures for controlling the curtains and lights in a smart home environment.		Jinyoung Song;Daijin Kim	2006		10.1109/ICPR.2006.1056	sliding window protocol;computer vision;speech recognition;computer science;machine learning;pattern recognition;probability;gesture recognition;image segmentation;statistics	Vision	39.74154110438179	-48.198773620050666	96129
a7ac4b78d96eadd6742e328dbdbe2ed0967fbaa4	a robust image retrieval system for mobile guide applications	drntu engineering computer science and engineering;journal article	We describe the prototype of an image retrieval system used for mobile guide applications. It allows users to send image queries of urban scenes using camera phones. The system uses a database of views of scenes to determine the poses of query views provided by the users. Information are then mapped onto the query views. Our approach is based on a fully affine invariant descriptor, coined PRIUS, which can identify corresponding building facades across widely separated views despite the highly repetitive nature of the man-made environments. The system is substantiated by experiments which show that our approach outperforms the state-of-the-art approaches such as SIFT and MSER in image retrieval.	camera phone;database;experiment;image retrieval;maximally stable extremal regions;prototype;scale-invariant feature transform	Jimmy Addison Lee;Kin Choong Yow;Itsuo Kumazawa	2012	Int. J. Intell. Syst.	10.1002/int.21522	computer vision;computer science;artificial intelligence;machine learning;multimedia;information retrieval	Vision	50.54461206563272	-43.9692501192223	96485
f833ec13a384f53a6eeec27b499de392c8bcc2d1	human detection in intelligent video surveillance: a review				Li Hou;Qi Liu;Zhenhai Chen;Jun Xu	2018	JACIII	10.20965/jaciii.2018.p1056	computer science;machine learning;computer vision;background subtraction;artificial intelligence	Vision	40.77323691894352	-46.615011163836044	96496
8cb90f379308d18db6243b937a146e1b37052726	saliency detection with flash and no-flash image pairs		In this paper, we propose a new saliency detection method using a pair of flash and no-flash images. Our approach is inspired by two observations. First, only the foreground objects are significantly brightened by the flash as they are relatively nearer to the camera than the background. Second, the brightness variations introduced by the flash provide hints to surface orientation changes. Accordingly, the first observation is explored to form the background prior to eliminate background distraction. The second observation provides a new orientation cue to compute surface orientation contrast. These photometric cues from the two observations are independent of visual attributes like color, and they provide new and robust distinctiveness to support salient object detection. The second observation further leads to the introduction of new spatial priors to constrain the regions rendered salient to be compact both in the image plane and in 3D space. We have constructed a new flash/no-flash image dataset. Experiments on this dataset show that the proposed method successfully identifies salient objects from various challenging scenes that the state-of-the-art methods usually fail.	adobe flash;color;image plane;object detection;strongly regular graph	Shengfeng He;Rynson W. H. Lau	2014		10.1007/978-3-319-10578-9_8	computer vision	Vision	43.69037943675165	-52.022172623269306	96505
10bbf0c80f648b7cb919368c92901fe2c40bb83a	evaluation of spatiotemporal detectors and descriptors for facial expression recognition	video signal processing face recognition object detection object tracking;detectors;spatiotemporal;bu 4dfe expression recongition spatiotemporal detectors descriptors;video signal processing;bu 4dfe;bu_4dfe data set spatiotemporal detector spatiotemporal descriptor facial expression recognition video analysis spatial scale temporal scale space time detector bag of features framework spatiotemporal feature facial point tracking manual initialization;expression recongition;face recognition;spatiotemporal phenomena detectors face recognition feature extraction vectors humans computer vision;object tracking;descriptors;object detection	Local spatiotemporal detectors and descriptors have recently become very popular for video analysis in many applications. They do not require any preprocessing steps and are invariant to spatial and temporal scales. Despite their computational simplicity, they have not been evaluated and tested for video analysis of facial data. This paper considers two space-time detectors and four descriptors and uses bag of features framework for human facial expression recognition on BU_4DFE data set. A comparison of local spatiotemporal features with other non-spatiotemporal published techniques on the same data set is also given. Unlike spatiotemporal features, these techniques involve time consuming and computationally intensive preprocessing steps like manual initialization and tracking of facial points. Our results show that despite being totally automatic and not requiring any user intervention, local spacetime features provide promising and comparable performance for facial expression recognition on BU_4DFE data set.	analysis of algorithms;belief propagation;computation;preprocessor;rendering (computer graphics);sampling (signal processing);sensor;video content analysis	M. Hayat;Mohammed Bennamoun;Amar A. El-Sallam	2012	2012 5th International Conference on Human System Interactions	10.1109/HSI.2012.16	computer vision;computer science;pattern recognition;communication	Vision	40.10021911824093	-50.85429697928419	96506
c510a62bdb6a081a819fb2b3e8d389aa9e3b6144	people reidentification in a camera network	databases;video surveillance;colored noise;images matching process;color;interest points;reacquisition problem;object reidentification interest points color;video surveillance cameras closed circuit television identification telecommunication security;cameras surveillance charge coupled image sensors event detection humans computer networks computer science computerized monitoring application software data security;public video datasets;modern security systems;closed circuit television;accuracy;video surveillance people reidentification camera network reacquisition problem images matching process modern security systems security agents target localization public video datasets synthetic images noise;image color analysis;security agents;identification;target localization;synthetic images;telecommunication security;secure system;robustness;object reidentification;people reidentification;camera network;cameras;noise	This paper presents an approach to the object reidentification problem in a camera network system. The reidentification or reacquisition problem consists essentially on the matching process of images acquired from different cameras. This work is applied in a monitored environment by cameras. This application is important to modern security systems, in which the targets presence identification in the environment expands the capacity of action by security agents in real time and provides important parameters like localization for each target. We used target's interest points and target's color with features for reidentification. The satisfactory results were obtained from real experiments in public video datasets and synthetic images with noise.	3d modeling;algorithm;autonomous robot;experiment;high- and low-level;identification (psychology);internationalization and localization;overhead (computing);point of interest;real-time clock;real-time computing;software deployment;synthetic intelligence;web search engine	I. O. de Oliveira;J. L. de Souza Pio	2009	2009 Eighth IEEE International Conference on Dependable, Autonomic and Secure Computing	10.1109/DASC.2009.33	identification;computer vision;simulation;colors of noise;computer science;noise;computer security;statistics;robustness	Vision	43.89636554335822	-43.07792453999101	96549
825c09cf2ee39b03ecb4f9d262592d2895b0c0dd	optical flow based head movement and gesture analysis in automotive environment	image motion analysis;image segmentation;magnetic heads;head vehicles magnetic heads optical imaging cameras roads tracking;object detection driver information systems gesture recognition image motion analysis image segmentation image sequences;laboratory experiment optical flow based head movement and gesture analyzer gesture analysis automotive environment head gesture detection intelligent driver assistance systems ohmega head gesture segmentation no head motion states higher level semantic information fixation time head motion rate uncontrolled on road experiment;optical imaging;roads;head;vehicles;driver information systems;gesture recognition;cameras;tracking;object detection;image sequences	Head gesture detection and analysis is a vital part of looking inside a vehicle when designing intelligent driver assistance systems. In this paper, we present a simpler and constrained version of Optical flow based Head Movement and Gesture Analyzer (OHMeGA) and evaluate on a dataset relevant to the automotive environment. OHMeGA is user-independent, robust to occlusions from eyewear or large spatial head turns and lighting conditions, simple to implement and setup, real-time and accurate. The intuitiveness behind OHMeGA is that it segments head gestures into head motion states and no-head motion states. This segmentation allows higher level semantic information such as fixation time and rate of head motion to be readily obtained. Performance evaluation of this approach is conducted under two settings: controlled in laboratory experiment and uncontrolled on-road experiment. Results show an average of 97.4% accuracy in motion states for in laboratory experiment and an average of 86% accuracy overall in on-road experiment.	gesture recognition;optical flow;performance evaluation;real-time clock;uncontrolled format string	Sujitha Martin;Cuong Tran;Ashish Tawari;Jade Kwan;Mohan Manubhai Trivedi	2012	2012 15th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2012.6338909	computer vision;simulation;computer science;computer graphics (images)	Robotics	40.13408251549643	-47.61805653408749	96643
0443ed6f0bdbe787466a8b73744d8a8a4cd2aede	person detection, tracking and masking for automated annotation of large cctv datasets		In this paper we describe a real-time approach for person detection in video footage, joint with a privacy masking tool, in the framework of forensic applications in CCTV systems. Particularly, this paper summarizes our results in these domains within the European FP7 SAVASA and P-REACT projects. Our main contributions have been focused on real-time performance of detection algorithms using a novel perspective-based approach, and the creation of a methodology for privacy masking content such as the faces of the persons in the images.	algorithm;closed-circuit television;real-time locating system	Marcos Nieto;Peter Leskovský;Juan Diego Ortega	2014		10.1007/978-3-319-13102-3_82	computer vision;data mining;internet privacy	Vision	40.69557138297226	-47.02979517821781	96674
12ec2aadec6f8c569ac8206e68e8f4ed937de924	binocular hand tracking and reconstruction based on 2d shape matching	2d shape matching;stereo images;3d contour reconstruction;video streaming;stereoscopic system;image matching;real time;hand detection;target tracking computer vision feature extraction image colour analysis image matching image reconstruction image sequences object detection stereo image processing;computer vision;binocular hand tracking;skin color;shape matching;image colour analysis;feature extraction;image reconstruction;image sequence;stereo image processing;hand tracking;2d skin color tracking;shape image reconstruction image sequences cameras computer science stereo image processing streaming media face humans robustness;target tracking;real time 3d hand tracking;stereoscopic system binocular hand tracking image reconstruction 2d shape matching real time 3d hand tracking hand detection image sequences stereo images 3d contour reconstruction 2d skin color tracking;3d reconstruction;object detection;image sequences	This paper presents a method for real-time 3D hand tracking in images acquired by a calibrated, possibly moving stereoscopic rig. The proposed method consists of a collection of techniques that enable the modeling and detection of hands, their temporal association in image sequences, the establishment of hand correspondences between stereo images and the 3D reconstruction of their contours. Building upon our previous research on color-based, 2D skin-color tracking, the 3D hand tracker is developed through the coupling of the results of two 2D skin-color trackers that run independently on the two video streams acquired by a stereoscopic system. The proposed method runs in real time on a conventional Pentium 4 processor when operating on 320times240 images. Representative experimental results are also presented	3d reconstruction;binocular vision;pentium 4;real-time clock;stereoscopy;streaming media	Antonis A. Argyros;Manolis I. A. Lourakis	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.327	3d reconstruction;iterative reconstruction;computer vision;simulation;feature extraction;computer science;computer graphics (images)	Vision	48.159111284566926	-46.54340123584737	96755
47195f77f97b35b0c516107665bac9496f55f6ec	an efficient gmm and active contour based unsupervised person re-identification		This paper proposes a novel algorithm for static and single camera foreground detection and multi-person tracking using active contour and Gaussian Mixture Model (GMM) methods. A new unsupervised multi-person re-identification algorithm has been developed, which dynamically assigns labels to persons for recognition. Detection of persons that have ever been in motion but become stationary for a long time is a challenge in conventional motion-based foreground detection methods. The proposed algorithm overcomes this challenge using information from the bounding boxes, targeting persons from precedent frame where they last moved. Chan-Vese active contours method is used to get proper shape of persons and corresponding bounding boxes from the foreground extracted by using the traditional GMM method. The proper shape obtained from active contours method is used to minimize the area of background in the tracked bounding box, which increases the accuracy of person reidentification. Parallel fusion of color moments and structure tensors are proposed to solve the problem of person re-identification. For re-identification, distinctive color features of the persons are extracted and stored on their first appearance in the field of view. In the subsequent appearances, their corresponding features are compared with the stored features using sum of absolute difference and are properly labelled based on the similarity measure. Experiments were conducted on IIT Patna database and our experimental study shows robustness in multi person tracking and re-identification. The result shows that this approach leads to improvement in multi-person tracking with 26.79% increase in accuracy compared to GMM and 85.33% correct re-identification of person.	active contour model;chan's algorithm;color moments;experiment;google map maker;integrated information theory;minimum bounding box;mixture model;sensor;similarity measure;stationary process;unsupervised learning	Swati;Sparha Mishra;Devesh Devendra;Subhamoy Chatterjee;Maheshkumar H. Kolekar	2017	2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2017.8125996	robustness (computer science);control theory;computer science;mixture model;precedent;foreground detection;similarity measure;image segmentation;minimum bounding box;pattern recognition;artificial intelligence;active contour model	Vision	41.80140344758716	-49.974079533814944	96953
0bd45198d36dd719a7a9b398544f7ee297c89b72	object edge contour localisation based on hexbinary feature matching	histograms;image edge detection estimation standards robots feature extraction histograms labeling;standards;robot vision approximation theory edge detection feature extraction image matching object detection;estimation;image edge detection;feature extraction;robots;labeling;polar derivative approximation object edge contour localisation hexbinary feature matching cluttered background robotics task robot vision system coarse to fine matching edge localised descriptor hexhog descriptor structure binary string image descriptor hog base descriptor	This paper addresses the issue of localising object edge contours in cluttered backgrounds to support robotics tasks such as grasping and manipulation and also to improve the potential perceptual capabilities of robot vision systems. Our approach is based on coarse-to-fine matching of a new recursively constructed hierarchical, dense, edge-localised descriptor, the HexBinary, based on the HexHog descriptor structure first proposed in [1]. Since Binary String image descriptors [2]-[5] require much lower computational resources, but provide similar or even better matching performance than Histogram of Orientated Gradient (HoG) descriptors, we have replaced the HoG base descriptor fields used in HexHog with Binary Strings generated from first and second order polar derivative approximations. The ALOI [6] dataset is used to evaluate the HexBinary descriptors which we demonstrate to achieve a superior performance to that of HexHoG [1] for pose refinement. The validation of our object contour localisation system shows promising results with correctly labelling ~ 86% of edgel positions and mis-labelling ~ 3%.	approximation;computation;computational resource;edge detection;gradient;recursion;refinement (computing);robot;robotics;visual descriptor	Yuan Liu;Gerardo Aragon-Camarasa;J. Paul Siebert	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090314	robot;computer vision;estimation;labeling theory;feature extraction;gloh;computer science;machine learning;pattern recognition;histogram;mathematics;statistics	Robotics	43.45614673032434	-50.624777072564235	97012
2e532a833df8718492adf879d0284f13a2d2d12a	incremental estimation of dense depth maps from image sequences	low level vision pattern recognition picture processing pixel based algorithm iconic algorithm depth extraction dense depth maps image sequences feature based kalman filtering algorithm convergence rates outdoor scene model lateral camera translations;picture processing;kalman filter;convergence rate;image sequence;pattern recognition;depth map;picture processing pattern recognition;image sequences kalman filters filtering motion estimation uncertainty application software image analysis layout robot vision systems cameras	Kalman filtering has recently been proposed as a mechanism for obtaining on-line estimates of depth from motion sequences. Previous applications of Kalman filtering to depth from motion have been limited to estimating depth at the location of a sparse set of features. In this paper, we introduce a new, pixel-based (iconic) algorithm that estimates depth and depth uncertainty at each pixel and incrementally refines these estimates over time. We describe the algorithm for translations parallel to the image plane and contrast its formulation and performance to that of a feature-based Kalman filtering algorithm. We compare the performance of the two approaches by analyzing their theoretical convergence rates, by conducting quantitative experiments with images of a flat poster, and by conducting qualitative experiments with images of a realistic outdoor scene model. The results show that the new method is an effective way to extract depth from lateral camera translations and suggest that it will play an important role in low-level vision.	algorithm;depth map;experiment;high- and low-level;image plane;kalman filter;lateral thinking;online and offline;pixel;sparse language;sparse matrix	Larry H. Matthies;Richard Szeliski;Takeo Kanade	1988		10.1109/CVPR.1988.196261	kalman filter;computer vision;simulation;computer science;rate of convergence;depth map;computer graphics (images)	Vision	51.872667995330296	-45.934981830790534	97120
bf5d2b0972ce088e9ce70f9a0ecc36ce4a8a93a3	visual homing via guided locality preserving matching		This study proposes a simple yet surprisingly effective feature matching approach, termed as guided locality preserving matching (GLPM), for visual homing of panoramic images. The key idea of our approach is merely to preserve the neighborhood structures of potential true matches between two panoramic images. We formulate it into a mathematical model, and derive a simple closed-form solution with linearithmic time and linear space complexities. This enables our method to accomplish the mismatch removal from hundreds of putative correspondences in only a few milliseconds. To handle extremely large proportions of outliers, we further design a guided matching strategy based on the proposed method, using the matching result on a small putative set with a high inlier ratio to guide the matching on a large putative set. This strategy can also significantly boost true matches without sacrifice in accuracy. To apply our GLPM to the visual homing problem, we develop a method for dense motion flow estimation from sparse feature matches based on Tikhonov regularization. Moreover, the focus-of-contraction/focus-of-expansion is derived to determine homing directions. The effectiveness of our method is demonstrated on a panoramic database in both feature matching and visual homing.	interpolation;locality of reference;mathematical model;missile guidance;multiple homing;profile-guided optimization;real-time clock;sparse matrix;time complexity	Jiayi Ma;Ji Zhao;Junjun Jiang;Huabing Zhou;Yu Zhou;Zheng Wang;Xiaojie Guo	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8460935	control theory;linear space;locality;tikhonov regularization;feature extraction;visualization;outlier;engineering;homing (biology);artificial intelligence;pattern recognition	Robotics	50.93250492493558	-47.67459357418009	97205
39641843b58c79756cacdcb401fe6bdba5fc9b37	modeling and generating complex motion blur for real-time tracking	real time visualization;charge coupled image sensors;complex motion blur generation;nvidia geforce 8800 gtx;kernel;paper;real time gpu implementation;cost function;real time visual tracking;real time tracking;convolution;real time;tracking coprocessors motion estimation;video tracking;motion estimation;layout;coprocessors;computer vision;motion blur;camera exposure time estimation;streaming media;pixel;blur estimation complex motion blur generation real time visual tracking real time gpu implementation camera exposure time estimation;nvidia;algorithms;blur estimation;lighting;computer science;tracking kernel cameras charge coupled image sensors motion estimation computer vision layout lighting cost function digital images;structure and motion;jacobian matrices;digital images;cameras;tracking	This article addresses the problem of real-time visual tracking in presence of complex motion blur. Previous authors have observed that efficient tracking can be obtained by matching blurred images instead of applying the computationally expensive task of deblurring (H. Jin et al., 2005). The study was however limited to translational blur. In this work, we analyse the problem of tracking in presence of spatially variant motion blur generated by a planar template. We detail how to model the blur formation and parallelise the blur generation, enabling a real-time GPU implementation. Through the estimation of the camera exposure time, we discuss how tracking initialisation can be improved. Our algorithm is tested on challenging real data with complex motion blur where simple models fail. The benefit of blur estimation is shown for structure and motion.	algorithm;analysis of algorithms;box blur;deblurring;gaussian blur;graphics processing unit;real-time clock;real-time locating system;velocity (software development);video tracking	Christopher Mei;Ian D. Reid	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587535	layout;computer vision;kernel;simulation;computer science;video tracking;motion estimation;lighting;tracking;convolution;digital image;pixel;coprocessor;computer graphics (images)	Vision	51.29962563033599	-49.01407832437026	97214
5fdd9ce246161de61496947a9227bc5ba7a5ea0e	robust rgb-d visual odometry based on edges and points		Abstract Localization in unknown environments is a fundamental requirement for robots. Egomotion estimation based on visual information is a hot research topic. However, most visual odometry (VO) or visual Simultaneous Localization and Mapping (vSLAM) approaches assume static environments. To achieve robust and precise localization in dynamic environments, we propose a novel VO based on edges and points for RGB-D cameras. In contrast to dense motion segmentation, sparse edge alignment with distance transform (DT) errors is adopted to detect the states of image areas. Features in dynamic areas are ignored in egomotion estimation with reprojection errors. Meanwhile, static weights calculated by DT errors are added to pose estimation. Furthermore, local bundle adjustment is utilized to improve the consistencies of the local map and the camera localization. The proposed approach can be implemented in real time. Experiments are implemented on the challenging sequences of the TUM RGB-D dataset. The results demonstrate that the proposed robust VO achieves more accurate and more stable localization than the state-of-the-art robust VO or SLAM approaches in dynamic environments.	visual odometry	Erliang Yao;Hexin Zhang;Hui Xu;Haitao Song;Guoliang Zhang	2018	Robotics and Autonomous Systems	10.1016/j.robot.2018.06.009	computer vision;distance transform;artificial intelligence;simultaneous localization and mapping;computer science;bundle adjustment;rgb color model;visual odometry;pose	Robotics	51.4885380507153	-46.22365713770604	97221
3dfb7b12e8e122fc977a2e9ea0a84ce0ccba6633	robust ellipse detection with gaussian mixture models	ellipse detection;l2 distance;gmm;parameter estimation	The Euclidian distance between Gaussian Mixtures has been shown to be robust to perform point set registration (Jian and Vemuri, 2011). We propose to extend this idea for robustly matching a family of shapes (ellipses). Optimisation is performed with an annealing strategy, and the search for occurrences is repeated several times to detect multiple instances of the shape of interest. We compare experimentally our approach to other state-of-the-art techniques on a benchmark database for ellipses, and demonstrate the good performance of our approach. HighlightsWe extend the framework based on L2 to estimate a parametric family of curves.We propose a non-isotropic and multidimensional modeling for the density functions.We propose a method for detecting multiple instances of an ellipse.	mixture model	Claudia Arellano;Rozenn Dahyot	2016	Pattern Recognition	10.1016/j.patcog.2016.01.017	mathematical optimization;generalized method of moments;machine learning;pattern recognition;mathematics;estimation theory;statistics	Vision	48.00074765666376	-51.60116505897283	97297
dfca0e85835b25453fc6cd13240a87a4fd967dc6	an image-based visual-motion-cue for autonomous navigatio	image motion analysis;image texture active vision image sequences robot vision mobile robots path planning;rotation measurement;time measurement;path planning;feature tracking;visual navigation;mobile robots;surface texture;surface reconstruction;data mining;visual motion;image texture;navigation;collision avoidance visual motion cue autonomous navigation visual motion cue hybrid visual threat cue 3d textured surface sequence of images active vision visual navigation;3d textured surface;visual motion cue;robot vision;time to contact;3d environment;image reconstruction;sequence of images;a priori information;autonomous navigation;optical flow;collision avoidance;motion measurement;visual field;3d reconstruction;cameras;hybrid visual threat cue;navigation data mining surface reconstruction surface texture motion measurement rotation measurement time measurement cameras image motion analysis image reconstruction;active vision;image sequences	This paper presents a novel time-based visual motion cue called the Hybrid Visual Threat Cue (HVTC) that provides some measure for a change in relative range as well as absolute clearances, between a 3D surface and a moving observer. It is shown that the HVTC is a linear combination of Time-To-Contact (TTC), visual looming and the Visual Threat Cue (VTC). The visual field associated with the HVTC can be used to demarcate the regions around a moving observer into safe and danger zones of varying degree, which may be suitable for autonomous navigation tasks. The HVTC is independent of the 3D environment and needs almost no a-priori infomation about it. It is rotation independent, and is measured in [time-l] units Several approaches to extract the HVTC are suggested. Also a practical method to extract it from a sequence of images of a 3D textured surface obtzined by a visually fixating, fixed-focus monocular camera in motion is presented. This approach of extracting the HVTC is independent of the type of 31) surface tedure and needs no optical $ow informtion, 3D reconstruction, segmentation, feature tracking.	3d reconstruction;autonomous robot;motion estimation	Sridhar R. Kundur;Daniel Raviv;Ernest Kent	1997		10.1109/CVPR.1997.609289	3d reconstruction;iterative reconstruction;image texture;mobile robot;surface finish;computer vision;navigation;simulation;active vision;surface reconstruction;computer science;optical flow;motion planning;time;computer graphics (images)	Vision	51.48699335025997	-38.39319005963287	97379
f4b5fde136f26627918661cc7978710991a1fcb0	determine absolute soccer ball location in broadcast video using syba descriptor		This paper presents research work on the detection, tracking, and localization of the soccer ball in a broadcast soccer video and maps the ball locations to the global coordinate system of the soccer field. Because of the lack of reference points in these frames, the calculation of the global coordinates of the ball remains a very challenging task. This paper proposes to use an object- based algorithm and Kalman filter to detect and track the ball in such videos. Once the ball is located, frames are registered to static soccer field, and the absolute ball location is found in the field. The existing feature matching algorithms do not work well for frame registration, especially when involving lighting variations and large camera pan-tile-zoom change. To overcome this challenge, a new feature descriptor and matching algorithm that is robust to these deformations is developed and presented in this paper. Experimental results show the proposed algorithm is very effective and accurate.		Alok Desai;Dah-Jye Lee;Craig Wilson	2014		10.1007/978-3-319-14364-4_57	computer vision;mathematical optimization;simulation;mathematics	Vision	49.23048412607218	-46.525333190303215	97441
a12d052afae9d94b3f9d612cbfe3e637c195c4c7	interactive offline tracking for color objects	dynamic programming;video signal processing dynamic programming image colour analysis object detection;tracking system;interactive offline tracking;global optimization framework;video signal processing;dynamic programming interactive offline tracking color objects user interaction global optimization framework object detector boosted color bin temporal coherence;dynamic program;boosted color bin;multiple objectives;image colour analysis;color objects;global optimization;temporal coherence;user interaction;object detector;object detection;video compression object detection target tracking interactive systems shape image segmentation detectors application software surveillance trajectory	In this paper, we present an interactive offline tracking system for generic color objects. The system achieves 60- 100 fps on a 320 times 240 video. The user can therefore easily refine the tracking result in an interactive way. To fully exploit user input and reduce user interaction, the tracking problem is addressed in a global optimization framework. The optimization is efficiently performed through three steps. First, from user's input we train a fast object detector that locates candidate objects in the video based on proposed features called boosted color bin. Second, we exploit the temporal coherence to generate multiple object trajectories based on a global best-first strategy. Last, an optimal object path is found by dynamic programming.	coherence (physics);color;display resolution;dynamic programming;global optimization;haar wavelet;mathematical optimization;online and offline;tracking system	Yichen Wei;Jian Sun;Xiaoou Tang;Harry Shum	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4408949	computer vision;tracking system;computer science;dynamic programming;video tracking;mathematics;multimedia;global optimization;computer graphics (images)	Vision	49.04202646875765	-48.70399573025546	97463
37c7afcf2421d6ebe71f1f59e2a76e93c1567c1a	collaborative target tracking using multiple visual features in smart camera networks	monte carlo methods;cameras;image colour analysis;image texture;intelligent sensors;state estimation;target tracking;video signal processing;wireless sensor networks;monte carlo method;collaborative target tracking;color features;multiview histograms;probabilistic tracker;smart camera networks;target state estimation;texture features;wireless network	With the evolution and fusion of technologies from sensor networks and embedded cameras, smart camera networks are emerging as useful and powerful systems. Wireless networks, however, introduce new constraints of limited bandwidth, computation, and power. Existing camera network approaches for target tracking either utilize target handover mechanisms between cameras, or combine results from 2D trackers into 3D target state for continuous tracking. Such approaches suffer from the drawbacks associated with 2D tracking, such as scale selection, target rotation, and occlusion. In this paper, we present an approach for tracking multiple targets in 3D space using a wireless network of smart cameras. In our approach, we use multiview histograms in different feature-spaces to characterize targets in 3D space. We employ color and texture as the visual features to model targets. The visual features from each camera, along with the target models are used in a probabilistic tracker to estimate the target state. We demonstrate the effectiveness of our proposed tracker with results tracking people using a camera network deployed in a building.	bandwidth (signal processing);color;computation;embedded system;hidden surface determination;monte carlo method;multiview video coding;probabilistic automaton;smart camera;software deployment	Manish Kushwaha;Xenofon D. Koutsoukos	2010	2010 13th International Conference on Information Fusion		smart camera;computer vision;simulation;tracking system;computer science;visual sensor network;computer graphics (images)	Robotics	45.967258071464094	-46.824071811361144	97490
0efaf02c38bb48536452555e0408267d7467425c	rapid eye detection method for non-glasses type 3d display on portable devices	rapid eye detection;eye position;portable devices;eye detection;stereo displays;contrast features;iris three dimensional displays face feature extraction shape lighting glass;glass;contrast operator;face recognition;shape;computational complexity;three dimensional displays;feature extraction;eye glasses;detection rate;iris region;neighboring region;face;lighting;iris;three dimensional displays face recognition feature extraction;3d display;facial images;facial images rapid eye detection 3d display portable devices computational complexity eye glasses contrast features iris region neighboring region eye position contrast operator	Eye detection is a critical task in many applications, especially for portable single user 3D display systems of the non-glasses type. However, existing methods either have high computational complexity or a low detection rate. In addition, these methods do not work with those using eye glasses for portable devices. In this paper, we propose a rapid eye detection method for portable devices. We propose a novel operator that measures contrast features to compute the difference in intensity between the iris region and its neighboring regions. The probability of an identified area being the eye position is calculated by our proposed contrast operator and the a priori geometric information of the two eyes. Experimental results show that the proposed method successfully detects the eyes in facial images regardless of illumination changes or wearing glasses. The average accuracy of the proposed method is 92.7%, which is higher than conventional methods. In addition, the proposed method can be run at an average rate of 12 fps on a portable device.	computational complexity theory;eb-eye;illumination (image);mobile device;personal digital assistant;stereo display;stereoscopy	Byeoung-su Kim;Hyun Young Lee;Whoi-Yul Kim	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5681133	facial recognition system;face;computer vision;stereo display;feature extraction;shape;computer science;lighting;glass;computational complexity theory;computer graphics (images)	Visualization	40.944087566568506	-50.64574188544984	97539
e885eea04b0850a2961465010573df514ca62835	a dense stereo matching algorithm with occlusion and less or similar texture handling		Due to image noise, illumination and occlusion, to get an accurate and dense disparity with stereo matching is still a challenge. In this paper, a new dense stereo matching algorithm is proposed. The proposed algorithm first use cross-based regions to compute an initial disparity map which can deal with regions with less or similar texture. Secondly, the improved hierarchical belief propagation scheme is employed to optimize the initial disparity map. Then the left-right consistency check and mean-shift algorithm are used to handle occlu- sions. Finally, a local high-confidence strategy is used to refine the disparity map. Experiments with the Middlebury dataset validate the proposed algorithm.	algorithm;computer stereo vision	Hehua Ju;Chao Liang	2013		10.1007/978-3-642-39342-6_19	computer vision;machine learning;pattern recognition	Vision	43.82438215809769	-51.3361197742812	97703
e22894b681a89a7e2293d91e5d6ee826b3134155	multi-camera tracking system in a large area case	object motion;prediction method;tracking system;video signal processing;kalman filters;particle filter multicamera tracking system video tracking system object motion prediction method;video tracking system;video tracking;motion estimation;data mining;particle filter;pixel;video signal processing motion estimation particle filtering numerical methods;multicamera tracking system;humans;tracking humans shape security monitoring robots prediction methods particle filters surveillance streaming media;particle filters;target tracking;particle filtering numerical methods	A video tracking system raises a wide range of possibilities in today's society, particularly in security, monitoring, and robotics. The most important research in tracking systems is to discover and develop an available method and algorithm for tracking an object's motion. The objective of this paper is to propose a new method that combines a prediction method and particle filter to manage problems in a wide area of observation. The comparative study of the method is provided and its capabilities are evaluated.	algorithm;match moving;particle filter;robotics;tracking system;video tracking	Zalili Binti Musa;Junzo Watada	2009	2009 IEEE International Conference on Fuzzy Systems	10.1109/FUZZY.2009.5277294	computer vision;simulation;particle filter;computer science;artificial intelligence;video tracking;computer graphics (images)	Robotics	47.55896114282168	-42.34090073521236	97764
f9333ad50b867be56d7916744ca83982c93a34f4	action recognition system with the microsoft kinectv2 using a hidden markov model		This paper proposes the independent human action recognition for more practical use, using the depth image and the Joint Points from the Microsoft KinectV2. The threshold value not only for the binarization of the depth image but also for the others except for the circularity was chosen automatically. Moreover, the value of the background is not even value but some depth value. In my previous research, the background image was simple so that the foreground area could be easily extracted as the binary image from the depth image by the threshold value chosen automatically. The whole human silhouette is difficult to be get with the uneven background. This problem can be overcome by subtraction between the two depth information in the current image and the background image.	algorithm;apply;binary image;glossary of computer graphics;hidden markov model;markov chain;poor posture	Masato Fujino;Thi Thi Zin	2016	2016 Third International Conference on Computing Measurement Control and Sensor Network (CMCSN)	10.1109/CMCSN.2016.49	silhouette;binary image;hidden markov model;subtraction;training set;computer vision;background subtraction;pattern recognition;computer science;artificial intelligence	Vision	39.5038699730929	-48.778379838216026	97963
4eb226950d7f331598078810d3ed1f3d2851dd2f	a gaze prediction technique for open signed video content using a track before detect algorithm	track before detect algorithm;face detection algorithm;detectors;likelihood ratio;grid based likelihood ratio track before detect routine;video coding gaze prediction technique open signed video content track before detect algorithm face detection algorithm grid based likelihood ratio track before detect routine orientation prediction eye tracking;gaze tracking video contexts video coding;gaze tracking;indexing terms;orientation prediction;video coding;accuracy;handicapped aids;face recognition;open signed video content;mathematical model;track before detect;video contexts;face;handicapped aids face detection video coding multimedia communication broadcasting electronic mail predictive models modulation coding protection detectors;prediction model;video coding face recognition;eye tracking;face detection;gaze prediction technique	This paper proposes a gaze prediction model for open signed video content. A face detection algorithm is used to locate faces across each frame in both profile and frontal orientations. A grid-based likelihood ratio track before detect routine is used to predict the orientation of the signer's head, which allows the gaze location to be localised to either the signer or the inset. The face detections are then used to narrow down the gaze prediction further. The gaze predictor is able to predict the results of an eye tracking study with up to 95% accuracy, and an average accuracy of over 80%.	algorithm;digital video;eye tracking;face detection;kerrison predictor;sensor;track-before-detect	Sam J. C. Davies;Dimitris Agrafiotis;Cedric Nishan Canagarajah;David R. Bull	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711852	facial recognition system;computer vision;face detection;speech recognition;computer science;mathematics;multimedia;statistics	Vision	39.86734138844295	-49.96057676960598	98011
572189f75838989a2b78b1f732a56f890ef1bfbd	structured light based depth edge detection for object shape recovery	image edge detection shape layout gabor filters cameras computer vision object detection biometrics computer science humans;edge detection;biometrics;shape recovery;structured light;gabor filters;layout;computer vision;shape;image edge detection;humans;computer science;projective structure;body shape;cameras;object detection	This research features a novel approach that efficiently detects depth edges in real world scenes. Depth edges play a very important role in many computer vision problems because they represent object contours. We strategically project structured light and exploit distortion of light pattern in the structured light image along depth discontinuities to reliably detect depth edges. Distortion along depth discontinuities may not occur or be large enough to detect depending on the distance from the camera or projector. For practical application of the proposed approach, we have presented methods that guarantee the occurrence of the distortion along depth discontinuities for a continuous range of object location. Experimental results show that the proposed method accurately detect depth edges of human hand and body shapes as well as general objects.	3d reconstruction;algorithmic efficiency;analysis of algorithms;biometrics;computation;computer vision;distortion;edge detection;movie projector;structured light;video projector	Cheolhwon Kim;Jiyoung Park;Juneho Yi;Matthew Turk	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops	10.1109/CVPR.2005.536	layout;computer vision;edge detection;structured light;shape;body shape;computer science;mathematics;biometrics;computer graphics (images)	Vision	48.03284505777346	-49.81251752950764	98061
e18224dc430ac9f2c0a34f55ff8424baa35e88ee	non-rigid articulated point set registration for human pose estimation	estimation joints shape iterative closest point algorithm three dimensional displays topology;topology;joints;topology computer vision gaussian processes image motion analysis image registration iterative methods mixture models pose estimation set theory;shape;estimation;three dimensional displays;nonrigid articulated point set registration computer vision applications iterative closest point depth images 3d scan data gmm based formulation lle local linear embedding based topology constraint gltp global local topology preservation aicp initialization articulated icp motion coherence assumption articulated deformations gaussian mixture model based nonrigid registration method coherent point drift human pose estimation;iterative closest point algorithm	We propose a new non-rigid articulated point set registration framework for human pose estimation that aims at improving two recent registration techniques and filling the gap between the two. One is Coherent Point Drift (CPD) that is a powerful Gaussian Mixture Model (GMM)-based non-rigid registration method, but may not be suitable for articulated deformations due to the violation of motion coherence assumption. The other is articulated ICP (AICP) that is effective for human pose estimation but prone to be trapped in local minima without good correspondence initialization. To bridge the gap of the two, a new non-rigid registration method, called Global-Local Topology Preservation (GLTP), is proposed by integrating a Local Linear Embedding (LLE) -based topology constraint with CPD in a GMM-based formulation, which accommodates articulated non-rigid deformations and provides reliable correspondence estimation for AICP initialization. The experiments on both 3D scan data and depth images demonstrate the effectiveness of the proposed framework.	3d pose estimation;3d scanner;algorithm;coherent;collaborative product development;experiment;google map maker;kernel density estimation;maxima and minima;mixture model;nonlinear dimensionality reduction;point set registration	Song Ge;Guoliang Fan	2015	2015 IEEE Winter Conference on Applications of Computer Vision	10.1109/WACV.2015.20	computer vision;mathematical optimization;estimation;shape;pattern recognition;mathematics;geometry;statistics	Vision	52.108269773571884	-52.00949795806538	98181
aff9988f60617d37c9d2224dca3f0f227cbf61d7	how wrong can you be: perception of static orientation errors in mixed reality	augmented reality;mobile computing;ar scenarios;vr scenarios;classical augmented scenario;commodity hardware;indirect augmented scenario;local orientation axis;mixed reality;mobile devices;pitch orientation error;pitch perception;roll orientation errors;static orientation errors human perception;tracking system design;yaw orientation error;yaw perception;augmented reality;perception;tracking errors	Tracking technologies are becoming an affordable commodity due to the wide use in mobile devices today. However, all tracking technologies available in commodity hardware is error prone due to problems such as drift, latency and jitter. The current understanding of human perception of static tracking errors is limited. This information about human perception might be useful in designing tracking systems for the display of AR and VR scenarios on commodity hardware. In this paper we present the findings of a study on the human perception of static orientation errors in a tracking system, using two different setups leveraging a handheld viewfinder: a classical augmented scenario and an indirect augmented one. By categorizing static orientation errors by scenario and local orientation axis, new insights into the users' ability to register orientational errors in the system are found. Our results show that users are much more aware of errors in classical AR scenarios in comparison to indirect AR scenarios. For both scenarios, the users registered roll orientation errors differently from both pitch and yaw orientation errors, and pitch and yaw perception is highly dependent on the scenario. However, the users performance ranking for orientational errors in AR scenarios was unexpected.	3d modeling;apache axis;ar (unix);autoregressive model;categorization;cognitive dimensions of notations;commodity computing;experiment;handheld game console;mixed reality;mobile device;motion capture;optic axis of a crystal;pitch (music);roll-to-roll processing;tablet computer;tracking system;video tracking;yaws	Jacob B. Madsen;Rasmus Stenholt	2014	2014 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2014.6798847	computer vision;simulation;computer science;computer graphics (images)	Visualization	51.96712069877127	-44.67291158968398	98312
f4283384e195c3c83c8d1d3a06d1602ee5debf49	persistent tracking of static scene features using geometry	projective invariants;occlusion;feature tracking;projective geometry;persistent tracking;tracking	Despite many alternatives to feature tracking problem, iterative least squares solution solving the optical flow constraint has been the most popular approach used by many in the field. This paper attempts to leverage the former efforts to enhance feature tracking methods by introducing a view geometric constraint to the tracking problem. In contrast to alternative geometry based methods, the proposed approach provides a closed form solution to optical flow estimation from image appearance and view geometry constraints. We particularly use invariants in the projective coordinates generated from tracked features that results in a new optical flow equation. This treatment provides persistent tracking of features even when they are occluded. At the end of each tracking loop the quality of the tracked features is judged using both appearance similarity and geometric consistency. Our experiments demonstrate robust tracking performance even when the features are occluded or they undergo appearance changes due to projective deformation of the template.	computer vision;experiment;motion estimation;optical flow	Jinwei Jiang;Alper Yilmaz	2014	Computer Vision and Image Understanding	10.1016/j.cviu.2013.10.009	computer vision;projective geometry;simulation;mathematics;geometry;tracking	Vision	52.51181234480879	-49.657778591607844	98411
1922082de84afb69c966a7628e86012342b475fd	fast online tracking with detection refinement		Most of the existing multiple object tracking (MOT) methods employ the tracking-by-detection framework. Among them, the min-cost network flow optimization techniques become the most popular and standard ones. In these methods, the graph structure models the MOT problem and finds the optimal flow in a connected graph of detections to encode the accurate track trajectories. However, the existing network flow is not suitable for directly online tracking, where the tracking results depend too much on the initial detections. To solve these problems, we present a fast online MOT algorithm by introducing the minimum output sum of squared error filter. The proposed method can adaptively refine the tracking targets according to the proposed rules of correcting the detection mistakes. Furthermore, we introduce an alternative targets hypotheses to reduce the dependence on detections and adaptively refine the object detection boxes. The experimental results on the MOT 2015 benchmark demonstrate that our method achieves comparable or even better results than previous approaches.	adaptive algorithm;benchmark (computing);connectivity (graph theory);encode;flow network;kalman filter;least squares;mathematical optimization;multistage interconnection networks;object detection;sensor	Jianbing Shen;Dajiang Yu;Leyao Deng;Xingping Dong	2018	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2017.2750082	robustness (computer science);artificial intelligence;computer vision;flow network;connectivity;video tracking;visualization;object detection;algorithm;engineering;trajectory;mean squared error	Vision	42.89020553485674	-48.53610596920522	98412
660ba3001bc5c9601f5b0c5605709f54f4d731e3	stairway detection based on single camera by motion stereo	stair candidate region;line segments extraction;distance measure;stairway segmentation;motion tracking;gabor filter;maximum distance of ground plane;visual impairment	In this paper we are proposing a method for detecting the localization of indoor stairways. This is a fundamental step for the implementation of autonomous stair climbing navigation and passive alarm systems intended for the blind and visually impaired. Both of these kinds of systems must be able to recognize parameters that can describe stairways in unknown environments. This method analyzes the edges of a stairway based on planar motion tracking and directional filters. We extracted the horizontal edge of the stairs by using the Gabor Filter. From the specified set of horizontal line segments, we extracted a hypothetical set of targets by using the correlation method. Finally, we used the discrimination method to find the ground plane, using the behavioral distance measurement. Consequently, the remaining information is considered as an indoor stairway candidate region. As a result, testing was able to prove its effectiveness.	autonomous robot;gabor filter;sensor	Danilo Cáceres Hernández;Taeho Kim;Kang-Hyun Jo	2011		10.1007/978-3-642-21822-4_34	computer vision;match moving;simulation;computer science	Robotics	50.59160478111242	-38.479420739441906	98502
8ad1b1580e442338e45e4e66fcc22777d3200861	hierarchical structural stereo matching with simultaneous autonomous camera calibration	focusing;hierarchical structure;cameras calibration image edge detection stereo vision focusing computer vision robot vision systems layout image segmentation signal analysis;correspondence;image segmentation;camera pose estimation;signal analysis;correspondence problem;layout;hierarchical structural stereo matching;computer vision;multiresolution framework hierarchical structural stereo matching autonomous camera calibration computer vision correspondence stereo matching camera pose estimation interactive algorithm;stereo matching;image edge detection;interactive algorithm;stereo vision;autonomous camera calibration;camera calibration;multiresolution framework;robot vision systems;calibration;cameras;pose estimation	Two key issues in computer vision are: solving the correspondence and camera calibration problems in stereo. We present a novel approach to autonomous relative camera orientation and stereo matching that uses the relationship between the correspondence problem and the camera pose estimation problem and combines these two into a single interactive algorithm embedded in a multiresolution framework. Our results show that this technique exhibits advantages in accuracy and versatility over existing methods.	camera resectioning;computer stereo vision	Xanthippos C. Magnisalis;Kim L. Boyer	1994		10.1109/ICPR.1994.576414	computer stereo vision;layout;computer vision;camera auto-calibration;calibration;camera resectioning;simulation;pose;computer science;stereopsis;signal processing;image segmentation;correspondence problem;computer graphics (images)	Vision	49.99200372121575	-47.11562914857178	98603
81bea1aadbc405e3459d542b298afa41ca796d44	fast rigid motion segmentation via incrementally-complex local models	motion models fast rigid motion segmentation incrementally complex local models trajectory data orthography nondegenerate motions real trajectory data nontrivial representations computational cost cost reduction motion representation motion degeneracies;multi body structure from motion;model selection;motion degeneracies;motion model instantiation;image motion analysis;image segmentation;real trajectory data;cost reduction;multi body structure from motion motion segmentation motion model instantiation model selection;orthography;incrementally complex local models;nondegenerate motions;motion segmentation;nontrivial representations;trajectory data;computational modeling;trajectory;computational complexity;three dimensional displays;image representation;solid modeling;motion models;computational cost;predictive models;image denoising;trajectory computational modeling three dimensional displays motion segmentation noise predictive models solid modeling;image segmentation computational complexity cost reduction image denoising image motion analysis image representation;fast rigid motion segmentation;motion representation;noise	The problem of rigid motion segmentation of trajectory data under orthography has been long solved for non-degenerate motions in the absence of noise. But because real trajectory data often incorporates noise, outliers, motion degeneracies and motion dependencies, recently proposed motion segmentation methods resort to non-trivial representations to achieve state of the art segmentation accuracies, at the expense of a large computational cost. This paper proposes a method that dramatically reduces this cost (by two or three orders of magnitude) with minimal accuracy loss (from 98.8% achieved by the state of the art, to 96.2% achieved by our method on the standard Hopkins 155 dataset). Computational efficiency comes from the use of a simple but powerful representation of motion that explicitly incorporates mechanisms to deal with noise, outliers and motion degeneracies. Subsets of motion models with the best balance between prediction accuracy and model complexity are chosen from a pool of candidates, which are then used for segmentation.	algorithmic efficiency;computation;degenerate energy levels;rigid motion segmentation	Fernando Flores-Mangas;Allan D. Jepson	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.293	computer vision;mathematical optimization;simulation;orthography;quarter-pixel motion;computer science;noise;trajectory;machine learning;motion estimation;mathematics;predictive modelling;image segmentation;solid modeling;scale-space segmentation;computational complexity theory;computational model;model selection;statistics	Vision	51.26523640694774	-50.29558908433844	98711
671fd923eeb970698d03f4153772ae61ce1409ab	tracking a firefly -a stable likelihood estimation for variable appearance object tracking-	target tracking kalman filters parameter estimation particle filtering numerical methods probability;probability;parameter estimations variable appearance object tracking likelihood estimation particle filter probability distribution kalman filtering;temporal variability;kalman filters;kalman filter;state estimation;brightness;shape;estimation;particle filter;probability distribution;state space;object tracking;parameter space;target tracking particle filters parameter estimation particle tracking state estimation brightness probability distribution filtering state space methods shape;lighting;parameter estimation;target tracking;particle filtering numerical methods	The particle filter estimates a probability distribution of target objectpsilas state by sampled hypotheses and their weights. This method is more expressive than existing method such as Kalman filtering, because the object state is represented as a multi-modal distribution. However, the method canpsilat be directly applied to temporally variable appearance object tracking, for example, a firefly or a flickering neon-sign. For solving this problem, we propose a particle filter for a variable appearance object, which estimates a unique state parameter independent of targetpsilas position. Our method decomposes the state space into disjoint parameter spaces, i.e., object position and posture space and Appearance parameter space. In the appearance parameter space, the likelihood of each hypothesis is evaluated at the position parameters generated in the other space, and the best explain parameter is determined. Based on this parameter, likelihood in the position and posture space is evaluated. By interacting the parameter estimations in different spaces, we can successfully track blinking firefly in the darkness.	firefly (cache coherence protocol);interaction;kalman filter;modal logic;neon (light synthesizer);particle filter;poor posture;state space	Yoshihiko Tsukamoto;Yusuke Matsumoto;Toshikazu Wada	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761478	kalman filter;computer vision;control theory;mathematics;statistics	Robotics	45.48923625747467	-47.36691505308542	98741
3453027e1a07b318cf779492c16ca8f69c349c1f	a tracking framework for augmented reality tours on cultural heritage sites	cultural heritage;visual cues;camera tracking;multimedia tour guides;augmented reality;visual tracking;scene analysis	Visual tracking for augmented reality tours is still challenging for cultural heritage sites because of the great variation of tracking targets and environments on such sites. Even at today's state of the art, it is almost impossible to apply just one tracking method to all the various environments with any hope of success. This paper presents a tracking framework to overcome this problem. It consists of different tracking flows, each efficiently using robust visual cues of the target scene. Analysis of the tracking environment enables more practical tracking at the sites. The reliability of the tracking framework is verified through on-site demonstrations at Gyeong-bokgung, the most symbolic cultural heritage site in Korea.	augmented reality;robustness (computer science);video tracking	Byung-Kuk Seo;Kangsoo Kim;Jungsik Park;Jong-Il Park	2010		10.1145/1900179.1900215	computer vision;augmented reality;simulation;sensory cue;eye tracking;computer science;cultural heritage;video tracking;multimedia;computer graphics (images)	Vision	53.707030517922426	-44.65571988722771	98802
1947006807164c9beba024832babcd20ddb6cb82	single image face orientation and gaze detection	vision system;intelligent driver support;human interaction;real time;geometric model;visual tracking;human activity recognition;target detection;eye gaze	We introduce a system to compute both head orientation and gaze detection from a single image. The system uses a camera with fixed parameters and requires no user calibration. Our approach to head orientation is based on a geometrical model of the human face, and is derived form morphological and physiological data. Eye gaze detection is based on a geometrical model of the human eye. Two new algorithms are introduced that require either two or three feature points to be extracted from each image. Our algorithms are robust and run in real-time on a typical PC, which makes our system useful for a large variety of needs, from driver attention monitoring to machine-human interaction.	algorithm;autostereogram;real-time clock	Jeremy Yrmeyahu Kaminski;Dotan Knaan;Adi Shavit	2008	Machine Vision and Applications	10.1007/s00138-008-0143-1	computer vision;simulation;machine vision;eye tracking;computer science	Vision	46.9411285944327	-43.28082749128999	98824
eca0d102a0e5d48ac19f1c2e99c7169b29b0497d	robust human body segmentation based on part appearance and spatial constraint	spatial constraint based graph cuts scgc;weak structure property;human body segmentation;期刊论文;part appearance map pam;complementary property	Human body segmentation in images is desirable in various practical applications, e.g., content-based image retrieval. However, it remains a challenging problem due to various body poses and confusing background. To overcome these difficulties, two properties of human body are explored in this paper, i.e., complementary property and weak structure property. Complementary property means that different Part Appearance Map (PAM). PAM can effectively represent the appearance probability of what a pixel belong to a human body, even for inaccurate human pose obtained by pictorial structure model. Afterward, robust foreground and background seeds are acquired by PAM. To utilize the structure information of human body effectively, we propose a novel graph cuts method – spatial constraint based graph cuts (SCGC), which incorporates weak structure property of human body parts into the cost function. The weak structure property constrains the arms, legs and head to appear in limited space under the condition that the location of torso is ascertained. With this property, the SCGC can successfully remove false segmentations by traditional graph cuts methods due to their similar appearances to human body. Experimental results show that the proposed method achieves promising performance and outperforms many state-of-the-art methods over publicly available challenging datasets which contain arbitrary poses. & 2013 Elsevier B.V. All rights reserved.	algorithm;coat of arms;content-based image retrieval;cut (graph theory);gsc bus;grabcut;loss function;minimum bounding box;pixel	Lei Huang;Sheng Tang;Yongdong Zhang;Shiguo Lian;Shouxun Lin	2013	Neurocomputing	10.1016/j.neucom.2013.03.003	computer vision;artificial intelligence;machine learning;mathematics	Vision	43.47039723121316	-51.901045491629894	98862
0ca633decf16d29c81a619341c66606b789411a3	segmenting, modeling, and matching video clips containing multiple moving objects	image segmentation;three dimensional;object recognition;indexing terms	This paper presents a novel representation for dynamic scenes composed of multiple rigid objects that may undergo different motions and are observed by a moving camera. Multiview constraints associated with groups of affine-covariant scene patches and a normalized description of their appearance are used to segment a scene into its rigid components, construct three-dimensional models of these components, and match instances of models recovered from different image sequences. The proposed approach has been applied to the detection and matching of moving objects in video sequences and to shot matching, i.e., the identification of shots that depict the same scene in a video clip	childhood immunization;matching;motion;muscle rigidity;physical object;video clip	Fred Rothganger;Svetlana Lazebnik;Cordelia Schmid;Jean Ponce	2004	Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.	10.1109/CVPR.2004.222	three-dimensional space;computer vision;index term;computer science;cognitive neuroscience of visual object recognition;pattern recognition;mathematics;image segmentation;computer graphics (images)	Vision	43.5899386015443	-50.56533282230072	98974
6ae5bb90fa97acb7589ba7dce146c91d07a56c0c	pushing and grasping for autonomous learning of object models with foveated vision	grasp rotate release action cycle autonomous learning object model foveated vision visual appearance unknown object humanoid robot autonomous object discovery explorative manipulation action visual scene hypothetical object pushing action rigid body motion sequential application object appearance object recognition;visualization grasping cameras robot vision systems head three dimensional displays;robot vision humanoid robots mobile robots object recognition	In this paper we address the problem of autonomous learning of visual appearance of unknown objects. We propose a method that integrates foveated vision on a humanoid robot with autonomous object discovery and explorative manipulation actions such as pushing, grasping, and in-hand rotation. The humanoid robot starts by searching for objects in a visual scene and generating hypotheses about which parts of the visual scene could constitute an object. The hypothetical objects are verified by applying pushing actions, where the existence of an object is considered confirmed if the visual features exhibit rigid body motion. In our previous work we showed that partial object models can be learnt by a sequential application of several robot pushes, which generates the views of object appearance from different viewpoints. However, with this approach it is not possible to guarantee that the object will be seen from all relevant viewpoints even after a large number of pushes have been carried out. Instead, in this paper we show that confirmed object hypotheses contain enough information to enable grasping and that object models can be acquired more effectively by sequentially rotating the object. We show the effectiveness of our new system by comparing object recognition results after the robot learns object models by two different approaches: 1. learning from images acquired by several pushes and 2. learning from images acquired by an initial push followed by several grasp-rotate-release action cycles.	autonomous robot;humanoid robot;optic axis of a crystal;outline of object recognition;pose (computer vision);snapshot (computer storage)	Robert Bevec;Ales Ude	2015	2015 International Conference on Advanced Robotics (ICAR)	10.1109/ICAR.2015.7251462	computer vision;simulation;deep-sky object;pose;object model;computer science;3d single-object recognition;communication	Robotics	48.807113583684895	-38.945175046987025	99007
f7d0b5dd2fa417de3fe702ec9bf349f13bf2fe8c	lane mark segmentation and identification using statistical criteria on compressed video	motion compensation;motion vectors;compressed video domain;macroblocks;h264 avc	The detection and localization of road lane marks are relevant to many applications of driving assistance and road traffic surveillance. Usually, these techniques work by processing all the pixels in every image, making the computational cost too high. In these situations, the implementation of real-time detection applications is impossible. Processing the video directly in the compressed domain avoids this limitation because the data rate is much reduced and full decoding of the compressed images is unnecessary. The development of a real-time detection systems then becomes possible, even for resource-limited systems like mobile devices. In this paper an approach to the segmentation and recognition of lane marks using only H264/AVC motion vectors is proposed. A new representation of motion vectors is defined in order to detect efficiently the regions or blobs of interest in complex videos captured by moving cameras. Then, a set of mathematical filters are applied removing progressively the blobs detected, depending on their position in the scene, their size, and their shape; and obtaining finally the regions corresponding to the lane marks. The proposed method shows encouraging results in different road traffic video sequences.	data compression	Juan Giralt;Luis Rodriguez Benitez;Juan Moreno García;Cayetano J. Solana-Cipres;L. Jimenez	2013	Integrated Computer-Aided Engineering	10.3233/ICA-130424	computer vision;simulation;quarter-pixel motion;computer science;motion estimation;motion compensation;computer graphics (images)	EDA	44.33799435884492	-44.79470135699238	99026
a71a3061663d6c29c8ec3a99dbd25629cc6596d0	robust background segmentation using background models for surveillance application		Gaussian Mixture Models (GMM) is a very typical method for background subtraction because it possesses a strong resistibility to repetitive background motion. However when it comes to complex environment, some unexpected situations occur, e.g., when illumination changes, gradually or quickly, segmentation is generated with a poor result. Moreover, this method is not capable of distinguishing shadows of moving objects. In this paper features of intensity and texture information are utilized to eliminate the shadow of moving objects. Integrated with modified Gaussian mixture models by redefining the update criterion, proposed algorithm is adapted to the flexible illumination environment. To validate that the proposed algorithm is robust to apply on surveillance system, we provide a metric with set of variables for evaluation, a comparison had been made between proposal and original GMM, results show the accuracy improvement of models using our updated algorithm. Averagely at least of 34.8% decrease of false alarm rate proves the quality of segmentation has been significantly enhanced and proposal is more competent and stable for outdoor surveillance applications.	algorithm;background subtraction;definition;google map maker;mixture model	Tianci Huang;Jingbang Qiu;Takahiro Sakayori;Takeshi Ikenaga	2009			computer vision;simulation;background subtraction;machine learning	Vision	43.39701116343855	-49.04577018673102	99216
16ca3a95f683fc69e29a1b63da790f65b17b9cea	situated vision in a dynamic world: chasing objects		We describe a system that approaches and follows arbitrary moving objects in real time using vision as its only sense. The system uses multiple simple vision computations which, although individually unreliable complement each other in a manner mediated by a situated control network. The objects can move over a wide variety of backgrounds including those with strong secondary reflections from light sources. Previously unseen objects can be tracked against backgrounds that include other moving objects. Computations are carried out in image coordinates at roughly 5 frames per second on a Lisp machine. The camera need not be calibrated or aligned well, and the system can tolerate a wide range of dynamically changing actuator response characteristics.		Ian Horswill;Rodney A. Brooks	1988			computer vision;simulation	AI	43.02722176390771	-39.62792427222982	99237
0f31de31d76dadb8d83b71d468bbfa82d6df6efb	scalable active matching	graph theory;information theory bayes methods graph theory image matching;image reconstruction layout cameras calibration clouds geometry stereo image processing surface reconstruction stereo vision computer vision;image processing;real time tracking;image matching;bayesian updating;bayes methods;geometry;prior information;intermediate bayesian updates;layout;joints;surface reconstruction;computer vision;probabilistic model;sequential camera tracking scalable active matching computer vision ransac algorithm image processing information theory intermediate bayesian updates rigid probabilistic model graph theory clam variation subam variation;optical imaging;scalable active matching;image reconstruction;clouds;stereo image processing;stereo vision;sequential camera tracking;approximation methods;rigid probabilistic model;correlation;probabilistic logic;clam variation;subam variation;calibration;cameras;information theory;ransac algorithm	In matching tasks in computer vision, and particularly in real-time tracking from video, there are generally strong priors available on absolute and relative correspondence locations thanks to motion and scene models. While these priors are often partially used post-hoc to resolve matching consensus in algorithms like RANSAC, it was recently shown that fully integrating them in an ‘Active Matching’ (AM) approach permits efficient guided image processing with rigorous decisions guided by Information Theory. AM's weakness was that the overhead induced by intermediate Bayesian updates required meant poor scaling to cases where many correspondences were sought. In this paper we show that relaxation of the rigid probabilistic model of AM, where every feature measurement directly affects the prediction of every other, permits dramatically more scalable operation without affecting accuracy. We take a general graph-theoretic view of the structure of prior information in matching to sparsify and approximate the interconnections. We demonstrate the performance of two variations, CLAM and SubAM, in the context of sequential camera tracking. These algorithms are highly competitive with other techniques at matching hundreds of features per frame while retaining great intuitive appeal and the full probabilistic capability to digest prior information.	approximation algorithm;bottom-up proteomics;computer vision;cryptographic hash function;graph theory;hoc (programming language);image processing;image scaling;information theory;linear programming relaxation;match moving;matching (graph theory);mutual information;overhead (computing);random sample consensus;real-time locating system;scalability;statistical model;top-down and bottom-up design	Ankur Handa;Margarita Chli;Hauke Strasdat;Andrew J. Davison	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5539788	iterative reconstruction;layout;statistical model;computer vision;ransac;calibration;surface reconstruction;image processing;information theory;computer science;stereopsis;graph theory;machine learning;pattern recognition;optical imaging;mathematics;probabilistic logic;bayesian inference;correlation;statistics	Vision	51.06857071197791	-49.60089540940401	99278
ec5f3d29d2d25055819dae4b10809b707a6dc8b3	a fast minimal solver for absolute camera pose with unknown focal length and radial distortion from four planar points.		In this paper we present a fast minimal solver for absolute camera pose estimation from four known points that lie in a plane. We assume a perspective camera model with unknown focal length and unknown radial distortion. The radial distortion is modelled using the division model with one parameter. We show that the solutions to this problem can be found from a univariate six-degree polynomial. This results in a very fast and numerically stable solver.		Magnus Oskarsson	2018	CoRR		distortion (optics);focal length;artificial intelligence;pattern recognition;mathematical optimization;computer science;polynomial;pose;planar;solver;univariate	Vision	53.62429363560896	-49.130792975142015	99503
ef14393a9b6da680390461901f8be949e101a88c	enhanced human parsing with multiple feature fusion and augmented pose model	trees mathematics image fusion pose estimation sport;biological system modeling;sports image dataset human parsing method multiple feature fusion augmented pose model human pose estimation parallel line feature uniform lbp feature standard tree model kinematic tree model;kinematics;estimation;image edge detection;heuristic algorithms;feature extraction;inference algorithms;estimation kinematics image edge detection heuristic algorithms feature extraction biological system modeling inference algorithms	We address the problem of human pose estimation, which is a very challenging problem due to view angle variance, noise and occlusions. In this paper, we propose a novel human parsing method which can estimate diverse human poses from real world images. We merge the parallel lines feature and uniform LBP feature, thereby the new feature contains both shape and texture information, which can be used by discriminative body part detectors. The standard tree model is augmented by using virtual nodes in order to describe the correlations between originally unconnected nodes, which enhances the robustness of the traditional kinematic tree model. We test our method in a sports image dataset, and the experimental results demonstrate the advantages of the merged feature as well as the augmented pose model in real applications.	algorithm;belief propagation;dynamic programming;heuristic;parsing;sensor	Zhaoxiang Zhang;Jianliang Hao;Yunhong Wang;Yuhang Zhao	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.72	computer vision;kinematics;estimation;feature detection;3d pose estimation;feature extraction;computer science;machine learning;pattern recognition;feature;statistics	Vision	44.119116757392895	-50.312849686472646	99680
4e16804c5b2f56e793bd6c7bffe23df872382508	a combined approach for estimating patchlets from pmd depth images and stereo intensity images	time of flight;real time;photonic mixer device;dynamic environment;range image;stereo vision;least squares estimate;depth estimation;3d measurement	Real-time active 3D range cameras based on time-of-flight technology using the Photonic Mixer Device (PMD) can be considered as a complementary technique for stereo-vision based depth estimation. Since those systems directly yield 3D measurements, they can also be used for initializing vision based approaches, especially in highly dynamic environments. Fusion of PMD depth images with passive intensity-based stereo is a promising approach for obtaining reliable surface reconstructions even in weakly textured surface regions. In this work a PMD-stereo fusion algorithm for the estimation of patchlets from a combined PMD-stereo camera rig will be presented. As patchlet we define an oriented small planar 3d patch with associated surface normal. Least-squares estimation schemes for estimating patchlets from PMD range images as well as from a pair of stereo images are derived. It is shown, how those two approaches can be fused into one single estimation, that yields results even if either of the two single approaches fails.	algorithm;experiment;image resolution;normal (geometry);pmd;pixel;real-time clock;realms of the haunting;stereo camera;stereopsis;texture mapping;time-of-flight camera	Christian Beder;Bogumil Bartczak;Reinhard Koch	2007		10.1007/978-3-540-74936-3_2	computer vision;electronic engineering;geography;optics	Vision	53.066882175632074	-44.37425130231422	100054
c86cb23cb2b1a137bb7b6556cdda0dcfb748a492	case deletion for fundamental matrix computation	multi object;gpu;fpga;heterogeneous;visual tracking	This paper reevaluates the case deletion algorithm for fundamental matrix computation and compares it against two RANSAC variants. The case deletion algorithm has several advantages for use in low-power devices in real-time applications including guaranteed time performance and excellent accuracy. This paper shows that case deletion can be just as accurate, but much more efficient than RANSAC when the ratio of outliers is large.	algorithm;computation;fundamental matrix (computer vision);low-power broadcasting;numerical linear algebra;power semiconductor device;random sample consensus;real-time clock	Brendan McCane	2014		10.1145/2683405.2683416	mathematical optimization;real-time computing;eye tracking;computer science;theoretical computer science;field-programmable gate array	Embedded	46.24036160578686	-46.12307957349821	100057
5fa8ee3f2bf3763dc5fc8102b0281942cd60c4ad	stochastic meta-descent for tracking articulated structures	stochastic processes joints sampling methods cost function computer vision robustness deformable models particle filters laboratories convergence;convergence;cost function;psi_visics;deformable models;joints;computer vision;stochastic meta descent;stochastic processes;gradient descent;robustness;particle filters;sampling methods;visual tracking;local minima;likelihood function	Recently, an optimization approach for fast visual tracking of articulated structures based on Stochastic Meta-Descent (SMD) has been presented. SMD is a gradient descent with local step size adaptation that combines rapid convergence with excellent scalability. Stochastic sampling helps to avoid local minima in the optimization process. We have extended the SMD algorithm with new features for fast and accurate tracking by adapting the different step sizes between as well as within video frames and by introducing a robust likelihood function which incorporates both depths and surface orientations. A realistic deformable hand model reinforces the accuracy of our tracker. The advantages of the resulting tracker over state-of-the-art methods are corroborated through experiments.	2d computer graphics;algorithm;bittorrent tracker;experiment;global optimization;loss function;mathematical optimization;maxima and minima;sampling (signal processing);scalability;service mapping description;stochastic gradient descent;surface-mount technology;video tracking	Matthieu Bray;Esther Koller-Meier;Nicol N. Schraudolph;Luc Van Gool	2004	2004 Conference on Computer Vision and Pattern Recognition Workshop	10.1109/CVPR.2004.445	gradient descent;stochastic process;sampling;computer vision;mathematical optimization;convergence;particle filter;eye tracking;computer science;machine learning;maxima and minima;mathematics;likelihood function;statistics;robustness	Vision	47.30659466538773	-48.85194307532197	100150
acc8564ac0c171e9c1117a9be07d1f3436023f1b	fast moving region detection scheme in ad hoc sensor network	time complexity;search space;real time;aperture problem;sensor network;field of view;detection rate;processing speed;global motion;image sensor	In this paper we present a simple yet effective temporal differencing based moving region detection scheme which can be used in limited resource condition such as in ad-hoc sensor network. Our objective is to achieve realtime detection in these low-end image sensor nodes. By double-threshold temporal differencing we can exclude the effect of global motion as well as detect the real motion regions. Then to achieve fast processing speed and overcome foreground aperture problem, we scale down the searching space to a rather small size, 30x40, and apply our Scalable Teeterboard Template to locate moving regions’ bounding boxes. Resource requirement and time complex of our scheme are very low yet experiment result shows that our scheme yields a high detection rate. And our scheme’s speed and detection rate cannot be affected essentially by the number of objects in the field of view.	autoregressive integrated moving average;hoc (programming language);image processing;image scaling;image sensor;object detection;real-time computing;scalability	Yazhou Liu;Wen Gao;Hongxun Yao;Shaohui Liu;Lijun Wang	2004		10.1007/978-3-540-30126-4_64	time complexity;computer vision;real-time computing;simulation;wireless sensor network;field of view;motion perception;computer science;image sensor;mathematics	Mobile	45.134777129968874	-39.89588989945763	100240
4e3d307d5a1044095aeb7ff107b32581a0da38be	spatio-temporal context tracking algorithm based on master-slave memory space model		The spatio-temporal context (STC) tracking algorithm has the advantages of high tracking accuracy and speed, but it may update the target template incorrectly under complex background and interference conditions. A spatio-temporal context tracking algorithm based on master-slave memory space model is proposed in this paper. The algorithm introduces the memory mechanism of Human Visual System (HVS) into the template updating process of STC algorithm, and forms a memory-based update strategy by constructing the master and slave memory spaces. Meanwhile, a method for determining the target location from multi peak points of saliency is proposed. Experimental results indicate that the proposed algorithm has comparatively high accuracy and robustness in the case of the target under occlusion, attitude changes, the target missing and appearing, and illumination changes, etc.	algorithm;computational resource	Xu Li;Yong Song;Yufei Zhao;Yun Li;Shangnan Zhao;Guowei Shi;Xin Yang	2018		10.1007/978-981-13-1702-6_21	temporal context;robustness (computer science);master/slave;salience (neuroscience);algorithm;interference (wave propagation);human visual system model;computer science	Vision	39.61201157876754	-41.77513578386612	100285
8198fb0d403a6495df5b024afaee337192a7a071	robust calibration of a reconfigurable camera array for machine vision inspection (ramvi): using rule-based colour recognition	rule based;machine vision	This paper describes a Reconfigurable Arr ay for Machine Vision Inspection (RAMVI) that is ab le to produce spatially-accurate images combining informa tion obtained from several cameras. Automatic camer calibration is essential for minimizing the changeo v r time required to reconfigure the array. This pa per describes an automatic calibration method that uses a colour coded calibration grid (CCG) to determine th field of view of each camera relative to the other cameras. Since colour is integral to the calibratio n process, robust colour recognition is essential, particularl y since several cameras are involved. Hence, a rule -bas d colour recognition methodology is described. Result s are presented demonstrating the effectiveness of this approach under varying lighting conditions.	logic programming;machine vision	Patrick T Spicer;Kristin Bohl;Gil Abramovich;Jacob Barhak	2006			rule-based system;computer science;calibration;artificial intelligence;pattern recognition;computer vision;machine vision	Vision	49.709328021403515	-40.08027635186942	100307
16f13f45ef09e7e696591f85a6d5e19cdbda7b89	investigating optical flow and tracking techniques for recovering motion within image sequences	computer science and informatics	Analysing objects interacting in a 3D environment and captured by a video camera requires knowledge of their motions. Motion estimation provides such information, and consists of re-covering 2D image velocity, or optical flow, of the corresponding moving 3D objects. A gradient-based optical flow estimator is implemented in this thesis to produce a dense field of velocity vectors across an image. An iterative and parameterised approach is adopted which fits planar motion models locally on the image plane. Motion is then estimated using a least-squares minimisation approach. The possible approximations of the optical flow derivative are shown to differ greatly when the magnitude of the motion increases. However, the widely used derivative term remains the optimal approximation to use in the range of accuracies of the gradient-based estimators i.e. small motion magnitudes.#R##N#Gradient-based estimators do not estimate motion robustly when noise, large motions and multiple motions are present across object boundaries. A robust statistical and multi-resolution estimator is developed in this study to address these limitations. Despite significant improvement in performance, the multiple motion problem remains a major limitation. A confidence measurement is designed around optical flow covariance to represent motion accuracy, and is shown to visually represent the lack of robustness across motion boundaries.#R##N#The recent hyperplane technique is also studied as a global motion estimator but proved unreliable compared to the gradient-based approach. A computationally expensive optical flow estimator is then designed for the purpose of detecting at frame-rate moving objects occluding background scenes which are composed of static objects captured by moving pan and tilt cameras. This was achieved by adapting the estimator to perform global motion estimation i.e. estimating the motion of the background scenes. Moving objects are segmented from a thresholding operation on the grey-level differences between motion compensated background frames and captured frames. Filtering operations on small object dimensions and using moving edge information produced reliable results with small levels of noise. The issue of tracking moving objects is studied with the specific problem of data correspondence in occlusion scenarios.	optical flow	Etienne Corvée	2005			computer vision;mathematical optimization;match moving;structure from motion;simulation;quarter-pixel motion;motion estimation;optical flow;mathematics;motion field	Vision	52.02410739463919	-46.43142255189751	100441
607b014489d50e1dfeab4a89fd96d0aa8778e551	towards resource-aware hybrid camera systems		We investigate how hybrid camera systems---stationary and mobile cameras---allow to improve the observability of mobile objects and/or locations considering limited resources. This Static Camera System with Mobile robots (SCSM) allows mobile cameras to observe targets where no stationary cameras are deployed, where they fail, or their field of view (FOV) is blocked. By assigning dynamic priorities to targets, the self-coordination and control of the SCSM becomes an assignment problem. Furthermore, the coordination of the available resources, especially for the mobile part of the SCSM, is considered. We adapt a market-based approach based on dynamic clustering. The SCSM is evaluated through simulation studies with a graphical simulator for visual sensor networks (VSNs). The results show that the SCSM can achieve up to 30% higher observability of targets compared to a stationary camera system. Moreover, the resource consumption can be distributed among the cameras with the dynamic clustering protocol to not burden all cameras having the same object in their FOV.	assignment problem;cluster analysis;digital camera;field of view in video games;graphical user interface;mobile robot;network model;niche blogging;robot;simulation;stationary process;virtual camera system;visual sensor network	Melanie Schranz;Torsten Andre	2018		10.1145/3243394.3243701	real-time computing;assignment problem;computer science;wireless sensor network;resource consumption;observability;cluster analysis;mobile robot;field of view	Robotics	46.2488957465602	-46.19309679409222	100504
af2fba2a9aedd35ea2f5c938b33efbfcff1040b6	body-relative navigation guidance using uncalibrated cameras	cameras computer displays mice computer interfaces keyboards motion control humans face testing calibration;affine projection;real time;computer vision user interfaces;affine projections visual interface mouse pointer control user interface face motion gaze positions uncalibrated cameras;computer vision;visual interfaces;user interfaces	We present a vision-based method that assists human navigation within unfamiliar environments. Our main contribution is a novel algorithm that learns the correlation between user egomotion and feature matches on a wearable set of uncalibrated cameras. The primary advantage of this method is that it provides robust guidance cues in the user's body frame, and is tolerant to small changes in the camera configuration. We couple this method with a topological mapping algorithm that provides global localization within the traversed environment. We validate our approach with ground-truth experiments and demonstrate the method on several real-world datasets spanning two kilometers of indoor and outdoor walking excursions.		Olivier Koch	2000	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICPR.2000.905368	computer vision;computer science;multimedia;user interface;computer graphics (images)	Vision	49.09304180686553	-41.80322285183816	100634
f2f2d05f78188ddf73cf53d143fc904e1dcd1c07	a novel image registration algorithm for remote sensing under affine transformation	image sampling;geophysical image processing;image matching;image representation;feature extraction;remote sensing;image registration;affine transforms;random processes;triangle area representation tar histogram image registration remote sensing robust estimation;parameter estimation;image registration feature extraction remote sensing robustness histograms parameter estimation estimation	With the help of the histogram of triangle area representation (TAR) and feature matching strategy, a new effective image registration approach for remote sensing is proposed in this paper. This approach is based on a robust transformation parameter estimation algorithm called the histogram of TAR sample consensus (HTSC in short). The HTSC algorithm can replace the existing random sample consensus (RANSAC) and progressive sample consensus (PROSAC) methods that have been widely used in the transformation parameter estimation step of remote-sensing image registration, for it can efficiently calculate the consensus set with a higher accuracy. This paper lays down a new way to build a robust transformation parameter estimator based on the invariance constraint for remote-sensing image registration. Analogous to the two types of well-known existing transformation parameter estimation methods RANSAC and PROSAC, HTSC can serve as a new type (or the third type if we treat RANSAC and PROSAC as the first and the second types) of such methods, as it adopts the transformation-invariance information to find the consensus.	algorithm;estimation theory;image registration;mathematical model;random sample consensus;synthetic intelligence	Zhili Song;Shuigeng Zhou;Jihong Guan	2014	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2013.2285814	stochastic process;computer vision;feature extraction;image registration;machine learning;pattern recognition;mathematics;estimation theory;statistics;remote sensing	Vision	45.37074077338565	-49.86719598722086	100747
7fd27a166183097ba2c9119f16355058b4d70e09	robust guided matching and multi-layer feature detection applied to high resolution spherical images		We present a novel, robust guided matching technique. Given a set of calibrated spherical images along with the associated sparse 3D point cloud, our approach consistently finds matches across the images in a multilayer feature detection framework. New feature matches are used to refine existing 3D points or to add reliable ones to the point cloud, therefore improving scene representation. We use real indoor and outdoor scenarios to validate the robustness of the proposed approach. Moreover, we perform a quantitative evaluation of our technique to demonstrate its effectiveness.	3d reconstruction;feature detection (computer vision);feature detection (web development);jean;layer (electronics);marc (archive);matching (graph theory);point cloud;profile-guided optimization;sensor;sparse matrix;technical support	Christiano Couto Gava;Alain Pagani;Bernd Krolla;Didier Stricker	2013			computer science;computer vision;pattern recognition;artificial intelligence;feature detection	Vision	53.35763788637814	-44.3669500453235	101162
01fc731c2c2a7d52f429bc27578cb3db512d1636	improvement of mean shift tracking performance using a convex kernel function and extracting motion information	convex kernel function;object tracking;color feature;tracking process;ms tracking algorithm;motion information;image sequence;real-time object tracking;ms kernel;mean shift tracking performance;tracking algorithm	convex kernel function;object tracking;color feature;tracking process;ms tracking algorithm;motion information;image sequence;real-time object tracking;ms kernel;mean shift tracking performance;tracking algorithm	mean shift	Amir Hooshang Mazinan;A. Amir-Latifi	2012	Computers & Electrical Engineering	10.1016/j.compeleceng.2012.06.009	computer vision;machine learning;video tracking;pattern recognition;mathematics	ML	41.56350709171454	-48.9355639637506	101176
b481ea048b0574547cd5e0bdeaa9d915be57211a	real time interaction with mobile robots using hand gestures	neural network human robot interaction gesture detection;image processing;neural networks;neural nets;mobile robot;static hand gestures real time interaction mobile robots hand gestures interaction system outdoor environment hand gesture based commands depth images neural network;real time;robot vision gesture recognition human robot interaction mobile robots neural nets real time systems;mobile robots;human robot interaction;robot vision;interactive system;vehicles;lighting;gesture detection;gesture recognition;mobile robots vehicles real time systems neural networks tracking lighting;tracking;neural network;real time systems	We developed a robust real time hand gesture based interaction system to effectively communicate with a mobile robot which can operate in an outdoor environment. The system enables the user to operate a mobile robot using hand gesture based commands. In particular the system offers direct on site interaction providing better perception of environment to the user. To overcome the illumination challenges in outdoors, the system operates on depth images. Processed depth images are given as input to a convolutional neural network which is trained to detect static hand gestures.  The system is evaluated in real world experiments on a mobile robot to show the operational efficiency in outdoor environment.	artificial neural network;convolutional neural network;experiment;mobile robot	Kishore Reddy Konda;Achim Königs;Hannes Schulz;Dirk Schulz	2012	2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/2157689.2157743	mobile robot;computer vision;simulation;computer science;artificial intelligence;artificial neural network	Robotics	46.81068928537881	-40.655085545969904	101256
62d04f3de3d980ccea8686c6413831c82930101e	spatiotemporal algorithm for joint video segmentation and foreground detection	moving object;probability density;white noise gaussian distribution gaussian noise image segmentation image sequences markov processes mixture models object detection smoothing methods spatiotemporal phenomena video signal processing;slowly varying function;video segmentation;markov random field;probabilistic model;foreground detection smooth surface image plane segmentation spatial gibbs markov random field probability density distribution temporal changes gaussian mixture approach white gaussian noise spatial segmentation temporal probabilistic model moving object detection pixel classification image smoothness spatial continuity video sequence segmentation spatiotemporal algorithm joint video segmentation;white gaussian noise;image segmentation signal processing algorithms markov processes video sequences adaptation models transform coding motion segmentation;temporal change	We present a novel algorithm for segmenting video sequences into objects with smooth surfaces. The segmentation of image planes in the video is modeled as a spatial Gibbs-Markov random field, and the probability density distributions of temporal changes are modeled by a Mixture of Gaussians approach. The intensity of each spatiotemporal volume is modeled as a slowly varying function distorted by white Gaussian noise. Starting from an initial spatial segmentation, the pixels are classified using the temporal probabilistic model and moving objects in the video are detected. This classification is updated by Markov random field constraints to achieve smoothness and spatial continuity. The temporal model is updated using the segmentation information and local statistics of the image frame. Experimental results show the performance of our algorithm.	algorithm;interaction;markov chain;markov random field;mixture model;object-based language;pixel;scott continuity;statistical model	S. Derin Babacan;Thrasyvoulos N. Pappas	2006	2006 14th European Signal Processing Conference		computer vision;range segmentation;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	44.69080851973095	-50.54201453045852	101287
ad80cc42e4779f7a009af87518eb4c5619a5fbbb	efficient large-scale photometric reconstruction using divide-recon-fuse 3d structure from motion	two dimensional displays;estimation;three dimensional displays;image reconstruction;pipelines;solid modeling;cameras	We propose an efficient framework for large-scale 3D reconstruction from a large set of photos following the Structure-from-Motion (SfM) paradigm with divide-conquer and fusion. Our main novelty is to ensure commonality from overlaps between image sets corresponding to their reconstructions, which facilitates effective stitching and fusion. Specifically, such commonality is ensured by selecting a set of duplicated images (which are termed anchor images) in adjacent image sets prior to the 3D reconstruction. The anchor images can assist accurate fusion of the 3D point clouds. We describe an efficient RANSAC scheme for pairwise stitching. Our method is intuitively scalable to large site reconstruction via subdivision and fusion following a graph construct. We further describe another RANSAC algorithm to improve loop closure in our anchor image approach. Experimental results on reconstructing a large portion of a university campus demonstrate the efficacy of our method.	3d reconstruction;algorithm;image stitching;point cloud;programming paradigm;random sample consensus;scalability;structure from motion;subdivision surface	Yueming Yang;Ming-Ching Chang;Longyin Wen;Peter H. Tu;Honggang Qi;Siwei Lyu	2016	2016 13th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2016.7738070	iterative reconstruction;computer vision;estimation;pipeline transport;solid modeling;statistics;computer graphics (images)	Vision	53.16150774542776	-47.16358536059435	101504
e1221f9e0a5211fa9624aa3e83f45e6dd4b6a83e	robust monocular slam in dynamic environments	slam robots iterative methods robot vision;three dimensional displays cameras simultaneous localization and mapping feature extraction solid modeling robustness real time systems;iterative methods;robot vision;h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities i 4 8 image processing and computer vision scene analysis tracking;parsac robust monocular slam dynamic environments online keyframe representation method prior based adaptive ransac algorithm;slam robots	We present a novel real-time monocular SLAM system which can robustly work in dynamic environments. Different to the traditional methods, our system allows parts of the scene to be dynamic or the whole scene to gradually change. The key contribution is that we propose a novel online keyframe representation and updating method to adaptively model the dynamic environments, where the appearance or structure changes can be effectively detected and handled. We reliably detect the changed features by projecting them from the keyframes to current frame for appearance and structure comparison. The appearance change due to occlusions also can be reliably detected and handled. The keyframes with large changed areas will be replaced by newly selected frames. In addition, we propose a novel prior-based adaptive RANSAC algorithm (PARSAC) to efficiently remove outliers even when the inlier ratio is rather low, so that the camera pose can be reliably estimated even in very challenging situations. Experimental results demonstrate that the proposed system can robustly work in dynamic environments and outperforms the state-of-the-art SLAM systems (e.g. PTAM).	algorithm;key frame;random sample consensus;real-time clock;simultaneous localization and mapping	Wei Tan;Haomin Liu;Zilong Dong;Guofeng Zhang;Hujun Bao	2013	2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)	10.1109/ISMAR.2013.6671781	computer vision;simulation;iterative method;computer graphics (images)	Vision	47.487345664003584	-46.91695527471077	101671
18a64bae4a26ef037ff776caa741af111359c552	action recognition based on visual tracking and qualitative spatial modeling			video tracking	Tao Wang	2016				Vision	50.976508439640725	-43.163909791072115	101697
32c801cb7fbeb742edfd94cccfca4934baec71da	multi-source multi-scale counting in extremely dense crowd images	dense crowds;multisource multiscale counting count disparity scale invariant feature transforms head counts markov random field global consistency constraint image region frequency domain analysis sift texture elements repetition low confidence head detections extremely dense crowd images;frequency domain analysis;transforms frequency domain analysis image texture markov processes;multi scale analysis dense crowds counting markov random field;markov random field;image texture;transforms;counting;head videos reliability fourier transforms image reconstruction computer vision frequency domain analysis;markov processes;multi scale analysis	We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. Due to problems including perspective, occlusion, clutter, and few pixels per person, counting by human detection in such images is almost impossible. Instead, our approach relies on multiple sources such as low confidence head detections, repetition of texture elements (using SIFT), and frequency-domain analysis to estimate counts, along with confidence associated with observing individuals, in an image region. Secondly, we employ a global consistency constraint on counts using Markov Random Field. This caters for disparity in counts in local neighborhoods and across scales. We tested our approach on a new dataset of fifty crowd images containing 64K annotated humans, with the head counts ranging from 94 to 4543. This is in stark contrast to datasets used for existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance.	autostereogram;binocular disparity;clutter;domain analysis;experiment;hidden surface determination;markov chain;markov random field;multi-source;pixel;scale-invariant feature transform;sensor;texton	Haroon Idrees;Imran Saleemi;Cody Seibert;Mubarak Shah	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.329	image texture;computer vision;pattern recognition;mathematics;markov process;frequency domain;counting;statistics	Vision	42.40357191478135	-51.56785315383488	101958
2be1e2f2b7208fdf7a379da37a2097cfe52bc196	learning to recognize familiar faces in the real world	silicon;robotic platform embedded system;cluster algorithm;pattern clustering;image segmentation;image databases;learning curve;intelligent robots;familiar face recognition learning system;noisy data;working environment noise;training;public environment;realistic image;human robot interaction;orbital robotics;automatic generation;robot interaction;accuracy;face recognition;robot vision;real human environment;robot autonomous detection;robust performance;robot vision face recognition human robot interaction image segmentation intelligent robots pattern clustering realistic images;clustering algorithms;realistic images;public space;robustness;humans;face clustering algorithm;face detection;realistic image familiar face recognition learning system robotic platform embedded system real human environment robot autonomous detection image segmentation robot interaction public environment face clustering algorithm;robotics and automation;robot kinematics;face recognition face detection orbital robotics robotics and automation image segmentation clustering algorithms humans image databases robustness working environment noise	We present an incremental and unsupervised face recognition system and evaluate it offline using data which were automatically collected by Mertz, a robotic platform embedded in real human environment. In an eight-day-long experiment, the robot autonomously detects, tracks, and segments face images during spontaneous interactions with over 500 passersby in public spaces and automatically generates a data set of over 100,000 face images. We describe and evaluate a novel face clustering algorithm using these data (without any manual processing) and also on an existing face recognition database. The face clustering algorithm yields good and robust performance despite the extremely noisy data segmented from the realistic and difficult public environment. In an incremental recognition scheme evaluation, the system is correct 74% of the time when it declares “I don't know this person” and 75.1% of the time when it declares “I know this person, he/she is …” The latter accuracy improves to 83.8% if the system is allowed some learning curve delay in the beginning.	algorithm;cluster analysis;embedded system;experiment;facial recognition system;in the beginning... was the command line;interaction;online and offline;robot;signal-to-noise ratio;spontaneous order;unsupervised learning	Lijin Aryananda	2009	2009 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2009.5152362	facial recognition system;human–robot interaction;computer vision;face detection;simulation;computer science;artificial intelligence;machine learning;accuracy and precision;image segmentation;cluster analysis;silicon;learning curve;robot kinematics;robustness	Robotics	47.55913435626017	-38.61171919637001	102154
034276d4ee5bbfc6d4458bb6759c998a92a8b42b	tracking articulated bodies using generalized expectation maximization	video signal processing expectation maximisation algorithm image sequences principal component analysis;articulated bodies;monocular video sequence;video signal processing;generalized expectation maximization;video sequences;edge pixels articulated bodies generalized expectation maximization monocular video sequence moving camera low dimensional space principal component analysis;moving camera;image edge detection;space use;three dimensional displays;principal component analysis;pipelines;pixel;impedance matching;cameras video sequences principal component analysis target tracking pipelines robustness image edge detection impedance matching humans clothing;robustness;clothing;humans;edge pixels;target tracking;low dimensional space;cameras;tracking;image sequences;expectation maximisation algorithm	A generalized expectation maximization (GEM) algorithm is used to retrieve the pose of a person from a monocular video sequence shot with a moving camera. After embedding the set of possible poses in a low dimensional space using principal component analysis, the configuration that gives the best match to the input image is held as estimate for the current frame. This match is computed iterating GEM to assign edge pixels to the correct body part and to find the body pose that maximizes the likelihood of the assignments.	expectation–maximization algorithm;ground truth;pixel;preprocessor;principal component analysis	Andrea Fossati;Elise Arnaud;Radu Horaud;Pascal Fua	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4563073	computer vision;mathematical optimization;impedance matching;computer science;clothing;pattern recognition;mathematics;pipeline transport;tracking;pixel;robustness;principal component analysis	Vision	48.98039715096618	-49.13110516301391	102283
60b4581f1c7983c8d00ddb4ec0a88bbb268cd0b5	moving object detection via robust background modeling with recurring patterns voting		This paper proposes a simple yet effective background modeling method based on recurring patterns voting for moving object detection under challenging scenes. Our method performs the following two steps. First, we employ Gaussian Mixture Model (GMM) to generate the initial background probability map for each frame, in which the value of each pixel represents its probability belonging to the background. Second, we perform recurring patterns voting by employing the graph-based manifold ranking algorithm on the spatially constrained graph to refine the probability map. This prior bases on the observation that the same patterns tend to recur frequently in same semantic regions (background or foreground). We verify it in our problem by calculating the background-background, foreground-foreground and background-foreground densities on some video sequences with ground truths. Experimental results on public video sequences suggest that the proposed method significantly outperforms other moving object detection methods.	algorithm;google map maker;mixture model;object detection;pixel	Chenglong Li;Zhimin Bao;Xiao Wang;Jin Tang	2017	Multimedia Tools and Applications	10.1007/s11042-017-4975-4	artificial intelligence;computer science;mixture model;manifold;pixel;voting;pattern recognition;object detection;machine learning;ranking;graph	AI	45.02087275522046	-50.555354313276744	102287
265cf003f17554df0be30f26c372a6e2a95967c8	a robust bus detection and recognition method based on 3d model and lsd method for public security road crossing application		Bus detection and recognition in real transportation scenes is a fundamental task for public security road crossing application. In this paper, a novel system is proposed to overcome the high computation complexity and the hard task of training large set of 3D models of the current algorithms. In the proposed system, the 3D model is built according to the contour information of the vehicle itself so that the system is more robust and practical. Meanwhile, the line features of the vehicle are extracted using the LSD (line segment detector) method. Finally, the line features are matched with the 3D model using a combined matching algorithm which reduces the computational complexity of the matching process. Experiments on real videos show the proposed method has a good performance in terms of the high recall ratio and low fall-out ratio.	3d modeling;algorithm;computation;computational complexity theory;waist–hip ratio	Wenqi Ma;Hua Yang;Yingkun Wang	2012		10.1007/978-3-642-34595-1_11	embedded system;simulation;engineering;computer security	Vision	41.926312205058444	-44.520749839216236	102385
b1dcf92a02ea4e0de0fadc085b761f2a54ce516e	skin region extraction and person-independent deformable face templates for fast video indexing	detectors;video signal processing;face skin image color analysis detectors indexing face recognition streaming media;skin color detection;skin;multiple views;face tracking;skin color detection multimedia indexing video archive indexing face recognition face tracking deformable template matching;multimedia computing;video indexing;visual databases face recognition feature extraction image colour analysis image registration indexing multimedia computing object tracking video signal processing;skin color;left right;face recognition;deformable template matching;indexing;streaming media;image color analysis;image colour analysis;feature extraction;image registration;indexation;object tracking;multimedia indexing;face;deformable template;facial expression;multimedia archive indexing skin region extraction person independent deformable face templates video indexing face tracking face recognition variable face poses facial expressions skin color regions near frontal poses image registration bootstrap face images;video archive indexing;visual databases	"""We describe a face tracking and recognition system for video and multimedia indexing that handles face regions at variable face poses (left-right and up-down), and deformations due to facial expressions and speech, by employing person-independent deformable templates at multiple poses on the view-sphere. An earlier version of the system handled variable poses (left-right only) by employing person-specific templates registered for each target individual at multiple poses. The new system speeds up processing by (i) extracting and restricting attention to skin-color regions, (ii) performing recognition using person-specific templates at near-frontal poses only, and (iii) tracking at non-frontal poses using the person-independent templates. Registration is also simplified, since multiple views of each target individual are no longer required, at the cost of a loss of recognition functionality at poses far from frontal (the system instead """"remembers"""" the identity of each individual from near-frontal matches and tracks between them). We describe the skin region extraction process and the process by which the person-independent templates are constructed off-line from """"bootstrap"""" face images of multiple non-target individuals, and we present experimental results showing the system in operation. Finally we discuss remaining issues in the practical application of the system to video and multimedia archive indexing."""	archive;central processing unit;cluster analysis;computer cluster;facial motion capture;graphics processing unit;online and offline;real-time clock;real-time computing;shot transition detection;speech recognition;template matching;throughput;unified framework;workstation	Simon Clippingdale;Mahito Fujii	2011	2011 IEEE International Symposium on Multimedia	10.1109/ISM.2011.75	facial recognition system;face;computer vision;search engine indexing;detector;facial motion capture;feature extraction;computer science;image registration;video tracking;multimedia;skin;facial expression;computer graphics (images)	Vision	39.95343357046467	-50.33296749748869	102990
37a926091a40549a973656c1047633f6c2d06496	speedup the multi-camera video-surveillance system for elder falling detection	geriatrics;human shape based falling detection algorithm;falling accidents;thread level parallelism;pipelining multithread;human shape generation;video surveillance;pipelining technique multicamera video surveillance system elder falling detection falling accidents human shape based falling detection algorithm falling pattern recognition multivideo stream processing image fetch image processing human shape generation multiple video stream thread level parallelism;accidental falls;yarn;video streaming;image processing;multicamera video surveillance system;surveillance system;system performance;video surveillance cameras yarn monitoring partitioning algorithms throughput streaming media face detection accidents delay;shape;monitoring;accidents;image edge detection;streaming media;image fetch;elder falling detection;multivideo stream processing;detection algorithm;multiple video stream;video surveillance geriatrics pattern recognition;pattern recognition;falling pattern recognition;pipelining technique;humans;face detection;multiple cameras falling detection pattern recognition pipelining multithread video surveillance;multiple cameras;cameras;pipeline processing;partitioning algorithms;throughput;falling detection	Nowadays, all countries have to face the growing populations of elders. For most elders, unpredictable falling accidents may occur at the corner of stairs or a long corridor due to body functional decay. If we delay to rescue a falling elder who is likely fainting, more serious consequent injury may happen. Traditional secure or video surveillance systems need someone to monitor a centralized screen continuously, or need an elder to wear sensors to detect accidental falling signals, which explicitly require higher costs of care staffs or cause inconvenience for an elder.In this work, we propose a human-shape-based falling detection algorithm and implement this algorithm in a multi-camera video surveillance system. The algorithm uses multiple cameras to fetch the images from different regions required to monitor. It then uses a falling-pattern recognition approach to determine if an accidental falling has occurred. If yes, the system will send short messages to someone needs to alert.In addition, we propose a multi-video-stream processing algorithm to speedup the throughput for the video surveillance system having multiple cameras. We partition the workloads of each video-surveillance streaming into four tasks: image fetch, image processing, human-shape generation, and pattern recognition. Each task will be handled by a forked thread. When the system receives multiple video streams from cameras, there are four simultaneous threads executed for different tasks. The objective of this algorithm is to exploit large thread-level-parallelism among those video-stream operations, and apply pipelining technique to execute these threads.  All above algorithms have been implemented in a real-world environment for functionality proof. We also measure the system performance after multi-streaming speedup. The results show that the throughput can be improved by about 2.12 times for a four-camera surveillance system.	algorithm;centralized computing;closed-circuit television;error-tolerant design;fork (software development);image processing;parallel computing;pattern recognition;pipeline (computing);population;sensor;speedup;stream processing;streaming media;syncope (medicine);task parallelism;throughput	Wann-Yun Shieh;Ju-Chin Huang	2009	2009 International Conference on Embedded Software and Systems	10.1109/ICESS.2009.62	embedded system;computer vision;throughput;face detection;simulation;image processing;shape;computer science;geriatrics;task parallelism	Vision	44.539156946962315	-42.66774251927458	103073
f30aa563a1814ef839265e3d74c76fb911346690	a change information based fast algorithm for video object detection and tracking	video object;image motion analysis;image segmentation;image segmentation motion segmentation object detection image edge detection pixel video sequences estimation;video signal processing;edge detection;video sequences;simulated annealing;maximum likelihood estimation;video signal processing image colour analysis image segmentation iterative methods markov processes maximum likelihood estimation object detection object tracking random processes simulated annealing;iterative methods;motion segmentation;estimation;image edge detection;jseg method video object detection moving object detection object tracking spatio temporal spatial segmentation temporal segmentation markov random field mrf model image attribute model spatial distribution temporal color coherence edge map temporal frame pixel labeling maximum a posteriori probability map estimation mrf map framework heuristic initialization frame segmentation simulated annealing iterative conditional mode gray level change detection mask label difference;image colour analysis;fast algorithm;pixel;object tracking;random processes;map estimation;tracking image edge analysis image motion analysis image segmentation map estimation modeling object detection simulated annealing;markov processes;modeling;tracking;object detection;image edge analysis	In this paper, we present a novel algorithm for moving object detection and tracking. The proposed algorithm includes two schemes: one for spatio-temporal spatial segmentation and the other for temporal segmentation. A combination of these schemes is used to identify moving objects and to track them. A compound Markov random field (MRF) model is used as the prior image attribute model, which takes care of the spatial distribution of color, temporal color coherence and edge map in the temporal frames to obtain a spatio-temporal spatial segmentation. In this scheme, segmentation is considered as a pixel labeling problem and is solved using the maximum a posteriori probability (MAP) estimation technique. The MRF-MAP framework is computation intensive due to random initialization. To reduce this burden, we propose a change information based heuristic initialization technique. The scheme requires an initially segmented frame. For initial frame segmentation, compound MRF model is used to model attributes and MAP estimate is obtained by a hybrid algorithm [combination of both simulated annealing (SA) and iterative conditional mode (ICM)] that converges fast. For temporal segmentation, instead of using a gray level difference based change detection mask (CDM), we propose a CDM based on label difference of two frames. The proposed scheme resulted in less effect of silhouette. Further, a combination of both spatial and temporal segmentation process is used to detect the moving objects. Results of the proposed spatial segmentation approach are compared with those of JSEG method, and edgeless and edgebased approaches of segmentation. It is noticed that the proposed approach provides a better spatial segmentation compared to the other three methods.	care-of address;computation;grayscale;heuristic;hybrid algorithm;image segmentation;iterated conditional modes;iterative method;markov chain;markov random field;object detection;pixel;simulated annealing;trionic	Badri Narayan Subudhi;Pradipta Kumar Nanda;Ashish Ghosh	2011	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2011.2133870	stochastic process;computer vision;estimation;systems modeling;edge detection;simulated annealing;computer science;machine learning;segmentation-based object categorization;video tracking;pattern recognition;mathematics;iterative method;tracking;maximum likelihood;image segmentation;markov process;scale-space segmentation;pixel;statistics	Vision	44.971866524314954	-50.38316841839364	103088
10c077bf2dd1bed928926feb37837862ab786808	multiple target tracking and identity linking under split, merge and occlusion of targets and observations		Multiple object tracking in video sequences is a difficult problem when one has to simultaneously deal with the following realistic conditions: 1) all or most objects share an identical or very similar appearance, 2) objects are imaged at close positions so there is a data association problem which becomes worse when the number of targets is high, 3) the objects to be tracked may lack observations for a short or long interval, for instance because they are not well detected or are being temporally occluded by another non-target object, and 4) their observations may overlap in the images because the objects are very near or the image results from a 2D projection from the 3D scene, giving rise to the merging and subsequently splitting of tracks. This later condition poses the additional problem of maintaining the objects identity when their observations undergo a merge and split. We pose the tracking and identity linking problem as one of inference on a two-layer probabilistic graphical model and show how can it be efficiently solved. Results are assessed on three very different types of video sequences, showing a turbulent flow of particles, bacteria growth and on-coming traffic headlights.	3d computer graphics;correspondence problem;experiment;graphical model;meaning–text theory;merge algorithm;synthetic intelligence;turbulence	José C. Rubio;Joan Serrat;Antonio M. López	2012			computer vision;pattern recognition	Vision	47.17375852428249	-49.82551260971897	103230
a8e4567308de6b3d07f7eb09b6153a1a1a29a17b	performance analysis of gait recognition with large perspective distortion	legged locomotion;probes;distortion;three dimensional displays;feature extraction;solid modeling;cameras	In real security scenarios, gait data may be highly distorted due to perspective effects and there may be significant change in appearance, orientation and occlusion between different measurements. To deal with this problem, a new identification technique is proposed by reconstructing 3D models of the walking subject, which are then used to identify subject images from an arbitrary camera. 3D models in one gait cycle are aligned to match silhouettes in a 2D gait cycle by estimating the positions of a 3D and 2D gait cycles in a 3D space. This allows the gait data in a gallery and probe share the same appearance, perspective and occlusion. Generic Fourier Descriptors are used as gait features. The performance is evaluated using a new collected dataset of 17 subjects walking in a narrow walkway. A Correct Classification Rate of 98.8% is achieved. This high recognition rate has still been achieved using a modest number of features. The analysis indicate that the technique can handle truncated gait cycles of different length and is insensitive to noisy silhouettes. However, calibration errors have a negative impact upon recognition performance.	3d modeling;algorithm;computer performance;distortion;gait analysis;profiling (computer programming)	Fatimah Abdulsattar;John Carter	2016	2016 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)	10.1109/ISBA.2016.7477229	computer vision;simulation;geography;communication	Vision	48.24257174000768	-46.013795907989724	103272
026ec0fe925dbefe166929af3daf7461da4ff6d6	real-time lane detection and rear-end collision warning system on a mobile computing platform	acoustics;traffic engineering computing android operating system collision avoidance computer vision distance measurement mobile computing road safety smart phones;estimation;vehicles cameras roads image edge detection estimation alarm systems acoustics;image edge detection;roads;vehicles;alarm systems;acoustic warning real time lane detection rear end collision warning system mobile computing platform accident prevention systems safety systems higher end vehicle luxury vehicle economical method smartphone application rearend collision warning system android based application image processing algorithms monoscopic camera vehicle detections monocular vision distance estimation unintentional lane departure;cameras	Accident prevention systems have recently been a part of many modern cars to reduce injuries and casualties on the road. However, the high cost of components and equipment have limited such safety systems to higher-end and luxury vehicles. This paper proposes an economical method of using a smartphone application for real-time lane detection and rearend collision warning system for drivers on the road. The Android-based application uses image-processing algorithms coupled with the monoscopic camera on the smartphone as the main sensor to perform lane and vehicle detections. The novelty of this work lies in the use of the monocular vision of the camera to estimate the distance with the vehicle up ahead. The system is able to distinguish unintentional lane departure and if the driver is traveling too close to the vehicle ahead. An acoustic warning will notify the driver of a potential accident.	acoustic cryptanalysis;algorithm;android;canny edge detector;edge detection;fits;hough transform;image processing;linear equation;mobile app;mobile computing;real-time clock;real-time transcription;sensor;smartphone;thresholding (image processing)	Samuel Jia Wei Tang;Kok Yew Ng;Boon How Khoo;Jussi Parkkinen	2015	2015 IEEE 39th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2015.171	embedded system;estimation;simulation;engineering;computer security;statistics	Mobile	43.750222655216994	-41.7607776283621	103429
c87ffd3b9c6b814ab6a1cf8aaba11c97b488b8e8	multi-face tracking by extended bag-of-tracklets in egocentric videos				Maedeh Aghaei;Mariella Dimiccoli;Petia Radeva	2015	CoRR			Vision	50.888289159697806	-43.068185791857466	103460
96035376dab41f7770bb69bbb10ed3e2d3c2e38f	dynamic feature cascade for multiple object tracking with trackability analysis	multiple objectives;computational complexity;confusion matrix;trackability;multiple object tracking;feature cascade	In multiple object tracking, the confusion caused by occlusion and similar appearances is an important issue to be solved. In this paper, trackability is proposed to measure how well given features can be used to find the correspondence of any given object in videos with multiple objects. Based on the analysis of trackability and computational complexity of the features under various occlusion conditions, a dynamic feature method cascade is presented to match the objects in consecutive frames. The cascade is composed of three tracking features: appearance, velocity and position. These features are enabled or disabled online to reduce computational complexity while obtaining similar trackability.#R##N##R##N#Experiments are conducted on 27062 frame occlusion objects, in the cases of good trackability, our experiments can obtain high succussful tracking rate with low computation burden, and in the cases of poor trackability, our estimation of trackability and confusion matrix can explain why they can not be tracked well.		Zhijun Li;Haifeng Gong;Song-Chun Zhu;Nong Sang	2007		10.1007/978-3-540-74198-5_27	computer vision;confusion matrix;computer science;machine learning;computational complexity theory	Robotics	43.20285628089025	-47.792308160031325	103644
9b0064690177aece9d319929ca07c8a0f9e0a1f5	plane-feature based 3d outdoor slam with gaussian filters	gaussian local filters plane feature 3d outdoor slam 3d planar features laser range data lidar extended kalman filter ekf unscented kalman filter ukf probabilistic plane extraction method merging criteria;nonlinear filters;slam robots gaussian processes kalman filters nonlinear filters optical radar robot vision;gaussian processes;kalman filters;simultaneous localization and mapping feature extraction covariance matrix vectors laser radar merging;robot vision;optical radar;slam robots	In this paper, a novel method extracting 3D planar features from laser range data (LiDAR) and its adaptation to outdoor SLAM using respective Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF) is proposed. The paper is mainly divided into two parts such that the feature extraction algorithm and its application to SLAM problem are given. Firstly, the feature extraction from 3D LiDAR data using a probabilistic plane extraction method and merging criteria with a region growing algorithm is explained. Secondly, the extracted 3D planar features are used with the well-known Gaussian local filters such as EKF or UKF to solve SLAM problem. The plane-feature based SLAM method estimate the robot pose in 6D as well as the plane parameters in 4D, which are the normal of plane and its minimum distance to origin represented in the world frame. Although the feature extraction method is proposed for outdoor SLAM, since it is developed from an indoor feature extraction method it can be safely used in indoor, outdoor, and even in the complex environments. The method is evaluated with the real datasets, and the EKF and UKF based SLAM performances are compared. The results show that UKF has better performance than EKF and can be successfully used in SLAM problems.	algorithm;extended kalman filter;feature extraction;performance;region growing;simultaneous localization and mapping;whole earth 'lectronic link	Cihan Ulas;Hakan Temeltas	2012	2012 IEEE International Conference on Vehicular Electronics and Safety (ICVES 2012)	10.1109/ICVES.2012.6294326	computer vision;simulation;geography;control theory;extended kalman filter	Robotics	50.3771389312122	-42.01831814752228	103672
075526b0d14b9559155b09ae790f1b8ac7b36352	virtual environment generation by cad-based methodology for underwater vehicle navigation	image features;image recognition;design automation;reliability;underwater vehicles;virtual environments;aspect graph;feature extraction;solid modeling;optical sensors;virtual environment	Perception. We describe a recognition framework to generate a virtual environment through CAD-based vision techniques from optical data. Descriptions of objects of the environment in terms of aspect graphs, and suitable recognition strategies for them are compiled off-line. A relational graph of image features is obtained on-line by processing optical data, and matching occurs between such a graph, and descriptions of objects in the framed scene. Multiresolution techniques are used in order to adapt recognition strategies to the distance and relevance of objects within the field of view.	compiler;computer-aided design;graph (discrete mathematics);matching (graph theory);online and offline;relevance;virtual reality	Leila De Floriani;Vittorio Murino;Goffredo G. Pieroni;Enrico Puppo	1998	9th European Signal Processing Conference (EUSIPCO 1998)	10.5281/zenodo.36483	computer vision;simulation;computer science;communication	Vision	49.63873165529111	-39.443295375128855	103715
f30c53796b4a78e923c5dcbf6d5acdeecaa70010	robot-to-camera calibration: a generic approach using 6d detections	manipulators;estimation;robot vision systems;calibration;cameras	For vision controlled manipulation with a robotic arm, knowledge of the camera to end effector transformation is important. This is typically done by classical hand-eye calibration, where a transformation is estimated by observing a known calibration object under different robot poses. The same holds for mobile bases as well, where the mounted cameras' poses with respect to the robot need to be known. However, there are cases where this robot/hand-eye calibration is not possible, for example because of limited movability or reduced precision of the robot. We want to present a new calibration method, which allows for a precise calibration also in these situations. This is achieved by rotating the robot around one axis and sampling a high number of visual features, in our case AprilTag markers. By fitting a Bingham distribution to the sampled markers around several configurations of the robot and determining the rotation center and axis, a mathematical model connecting the kinematics of the robot to the camera frame of reference can be found. We show that this method can be applied to cases where a classical hand-eye calibration fails, e.g. a robot base with limited movability. Our method allows an estimation of the camera to robot transformation even under those challenging conditions, e.g. where no part of the robot is visible from the camera.		Christian Nissler;Zoltan-Csaba Marton	2017	2017 First IEEE International Conference on Robotic Computing (IRC)	10.1109/IRC.2017.66	monte carlo localization;computer vision;cartesian coordinate robot;simulation;computer science;arm solution;control theory;mobile robot navigation;robot calibration	Robotics	53.55638418277197	-40.0768811659021	103718
1b59190506a06bce8a29ca3c2b94a7580a87ef9d	step towards sequence-to-sequence alignment	video signal processing;image alignment;temporal variation;temporal information;synchronisation;video cameras;image sequence;moving object detection;sequence alignment;synchronisation video signal processing image sequences video cameras;layout cameras video sequences electrical capacitance tomography lighting spatial resolution pixel computer science space stations image resolution;direct method;dynamic scenes;multiple video cameras sequence to sequence alignment video sequences dynamic scene stationary uncalibrated video cameras spatial alignment temporal synchronization temporal alignment spatio temporal information temporal variations image frames moving objects scene illumination standard image to image alignment techniques temporal cues single alignment framework traditional image to image alignment methods temporal alignment parameter estimation measurable sequence quantities point correspondences frame correspondences moving object detection real image sequences;image sequences	This paper presents an approach for establishing correspondences in time and in space between two different video sequences of the same dynamic scene, recorded by stationary uncalibrated video cameras. The method simultaneously estimates both spatial alignment as well as temporal synchronization (temporal alignment) between the two sequences, using all available spatio-temporal information. Temporal variations between image frames (such as moving objects or changes in scene illumination) are powerful cues for alignment, which cannot be exploited by standard image-to-image alignment techniques. We show that by folding spatial and temporal cues into a single alignment framework, situations which are inherently ambiguous for traditional image-to-image alignment methods, are often uniquely resolved by sequence-to-sequence alignment. We also present a “direct” method for sequence-tosequence alignment. The algorithm simultaneously estimates spatial and temporal alignment parameters directly from measurable sequence quantities, without requiring prior estimation of point correspondences, frame correspondences, or moving object detection. Results are shown on real image sequences taken by multiple video cameras.	algorithm;object detection;sequence alignment;stationary process	Yaron Caspi;Michal Irani	2000		10.1109/CVPR.2000.854940	direct method;synchronization;computer vision;computer science;sequence alignment;multimedia;computer graphics (images)	Vision	49.18054953561339	-45.9465064359312	103820
0187ade841e374ce6293afdcc99ed6e3a2a89083	toward a vision based hand gesture interface for robotic grasping	skin color information;intelligent interfaces;image segmentation;robot hand;gestural interface;shape descriptor;robot vision systems grasping humans intelligent robots shape skin computer vision context taxonomy layout;skin;hand posture recognition system;inner distance shape context;vision based hand gesture interface;robotic grasping;computer vision;robot manipulator;dexterous manipulators;skin color;robot vision;shape;image color analysis;image colour analysis;human body;pixel;robots;manipulation tasks;robot vision dexterous manipulators gesture recognition image colour analysis image segmentation;dexterous robotic hands;grasp recognition;gesture recognition;context;skin color information vision based hand gesture interface robotic grasping manipulation tasks dexterous robotic hands computer vision hand posture recognition system inner distance shape context grasp recognition	The challenging problem of planning manipulation tasks for dexterous robotic hands can be significantly simplified if the robot system has the ability to learn manipulation skills by observing a human demonstrator. Toward this goal, we present a novel computer vision based hand posture recognition system to serve as an intelligent interface for skill transfer in robotic manipulation. We use the Inner Distance Shape Context (IDSC) as a hand shape descriptor to capture variations in the hand state (open or closed) under large in-plane rotations and considerable out-of-plane rotations. The proposed technique is further examined in applications involving grasp recognition and gesture based communications. The experiments show that the proposed approach can be generalized to recognizing a selected taxonomy of grasp types. At present, skin color is used to segment the hand region from the scene, but this method has its own limitations. We show preliminary results suggesting that the IDSC can be used to segment parts of the articulated object, including segmenting the hand from the human body silhouette without using skin color information.	color;computer vision;data descriptor;experiment;gesture recognition;poor posture;robot;robotic arm;sensor;shape context;taxonomy (general)	Raghuraman Gopalan;Behzad Dariush	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5354682	robot;computer vision;human body;simulation;shape;computer science;gesture recognition;skin;image segmentation;pixel	Robotics	48.851860115952896	-40.31238299182801	103821
279afe240e54e801867629dbff77211221a3befe	automatic road detection system for an air-land amphibious car drone		Abstract In recent years, unmanned aerial vehicle (UAV) technologies have rapidly developed. Drones, which are one type of UAV, are used in many industrial fields, such as photography, delivery and agriculture. However, a commercial drone can fly for only approximately 20 min on one charge. Furthermore, drones are prohibited from flying in some areas, and cannot be operated in bad weather. Due to the development of drone technologies, we must reduce energy consumption and achieve long-range movement. To overcome these limitations, we develop a new air–land amphibious car drone that can fly and requires less power consumption in land mode; this extends the range of mobility of the drone. Moreover, land mode can be used to pass through restricted areas or bad weather conditions by sliding. Furthermore, we develop a Convolutional Neural Network (CNN)-based algorithm for detecting the road in a captured scene. To more accurately segment the road region based on images from the equipped camera of the drone, we propose atrous spatial pyramid pooling (ASPP) ResNet blocks, instead of Resblocks, which were proposed by DeepLab. The experimental results demonstrate that the proposed method improves the pixel accuracy (PA) to 85.6% and achieves a mean Intersection over Union (mIoU) of 55.8%.		Yujie Li;Huimin Lu;Yoshiki Nakayama;Hyoungseop Kim;Seiichi Serikawa	2018	Future Generation Comp. Syst.	10.1016/j.future.2018.02.036	real-time computing;pixel;convolutional neural network;energy consumption;drone;pyramid;pooling;residual neural network;computer science;photography	Arch	42.09727538043022	-41.25810791486971	104067
7bb1549da0dcaab5af2c65e45befde9fd7ad1da9	improving the 3d perception of the pepper robot using depth prediction from monocular frames		The robot Pepper provides a bad depth estimation. In this paper, we present a method for improving that 3D estimation. The method is based on using the RGB image to predict monocular depth. As it will be shown, the combination of both, monocular and 3D depth, provides a better 3D data.	robot	Zuria Bauer;Félix Escalona;Edmanuel Cruz;Miguel Cazorla;Francisco Gomez-Donoso	2018		10.1007/978-3-319-99885-5_10	computer vision;perception;robot;sensor fusion;depth perception;monocular;rgb color model;artificial intelligence;computer science;pepper	Robotics	51.172800141677364	-42.69645440739111	104175
875319489403970223c50a2d10bd4758711572bc	newton geodesic optimization on special linear group	optimisation;convergence;critical point;manifolds;cost function;optimal method;data mining;optimisation convergence lie groups optimal control;optimal control;riemannian metric;algebra;image registration;pixel;quadratic convergence;lie group;special linear group;optimization;lie groups;algorithm design and analysis;optimization methods constraint optimization convergence geometry quaternions algebra visual servoing vectors algorithm design and analysis iterative algorithms;cost function newton geodesic optimization method special linear group riemannian exponential map noncompact lie group minimal geodesic equation locally quadratic convergence;exponential map	The Riemannian exponential map on a noncompact Lie Group, which is determined by a Riemannian metric, is different from the Lie group exponential map determined by one-parameter subgroups. The Riemannian exponential map which represents the geodesic of the optimal transformation is obtained in terms of the minimal geodesic equation on SL(n, R). Generally, the Newton optimization method on Lie group is independent of the connection but with the one-parameter group. Based on the parameterization of the manifold with the Riemannian exponential map, we propose an intrinsic Newton optimization method on special linear group and prove its locally quadratic convergence to critical point of the cost function. Our approach is slightly superior to the counterpart based on Lie group exponential map. We demonstrate this by an image registration example.	critical point (network science);image registration;loss function;mathematical optimization;newton;newton's method;rate of convergence;time complexity	Guangwei Li;Yunpeng Liu;Jian Yin;Zelin Shi	2009	Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference	10.1109/CDC.2009.5400115	isometry;exponential map;fundamental theorem of riemannian geometry;mathematical optimization;mathematical analysis;geodesic;vector flow;topology;geodesic map;representation of a lie group;mathematics;one-parameter group;lie group;adjoint representation of a lie algebra	Vision	50.83235511932456	-51.99181107498701	104267
71d8b144d41981d9f5529f4a90bf71b653b4fd78	real-time image recovery using temporal image fusion	image fusion;image classification;image registration image fusion fuzzy confidence spatial temporal fusion;image fusion computer vision fuzzy set theory image classification;fuzzy set theory;computer vision;image fusion image registration noise real time systems sensors image reconstruction hazards;fuzzy confidence maps real time image recovery temporal image fusion computer vision systems image corruption moving scenarios congregate information spatial image fusion fuzzy classification partial image recovery image alignment techniques	In computer vision systems an unpredictable image corruption can have significant impact on its usability. Image recovery methods for partial image damage, in particular in moving scenarios, can be crucial for recovering corrupted images. In these situations, image fusion techniques can be successfully applied to congregate information taken at different instants and from different points-of-view to recover damaged parts. In this article we propose a technique for temporal and spatial image fusion, based on fuzzy classification, which allows partial image recovery upon unexpected defects without user intervention. The method uses image alignment techniques and duplicated information from previous images to create fuzzy confidence maps. These maps are then used to detect damaged pixels and recover them using information from previous frames.	computer vision;fuzzy classification;image fusion;map;pixel;real-time transcription;usability	André Mora;José Manuel Fonseca;Rita Almeida Ribeiro	2013	2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2013.6622539	image quality;image warping;image texture;image restoration;computer vision;contextual image classification;feature detection;image analysis;binary image;image processing;computer science;machine learning;digital image processing;pattern recognition;fuzzy set;image fusion;automatic image annotation;image histogram	Vision	44.38663126431891	-45.885823362413454	104745
5532cc3ee63e0e9249a47ae07c46868f6729439e	image-based on-road vehicle detection using cost-effective histograms of oriented gradients	intelligent transportation systems;vehicle detection;histograms of oriented gradients;intelligent vehicles;image analysis	Image-based vehicle detection has received increasing attention in recent years in the framework of advanced driver assistance systems. However, the variability of vehicles in size, color, shape, etc. poses an enormous challenge, especially for the vehicle verification task. Histograms of Oriented Gradients (HOGs) have successfully been applied to image-based verification of objects. However, these descriptors are computationally demanding and are not affordable for real-time on-road vehicle detection. In this paper, less-demanding HOG descriptors are proposed and evaluated that significantly lighten the computation by exploiting the a priori known vehicle appearance. The proposed descriptors are evaluated on a large, public database and the experiments disclose that the computation times are reduced in a factor of more than 5, thus rendering HOG-based real-time vehicle detection affordable, while achieving detection rates of over 96%.	image gradient	Jon Arróspide Laborda;Luis Salgado;Massimo Camplani	2013	J. Visual Communication and Image Representation	10.1016/j.jvcir.2013.08.001	embedded system;computer vision;intelligent transportation system;image analysis;simulation;computer science	Vision	41.719798053407736	-44.9781362621659	104811
81f8aba18d4cbf2cb2091d6ec79139eeccdad633	nrsfm-flow: recovering non-rigid scene flow from monocular image sequences		Recovery of scene flow (a dense 3D velocity vector field) of a dynamic scene from monocular image sequences is an emerging field in computer vision. Being sensitive to occlusions, existing Monocular Scene Flow (MSF) methods are either limited in handling non-rigid deformations [5], or make strong assumptions on scene [2] and camera motion [1]. To overcome these limitations, we propose a framework for MSF estimation based on Non-Rigid Structure from Motion (NRSfM) [4] techniques — NRSfM-Flow. In the continuous domain, relation between a shape S(p, t), camera motion R(t) and scene flow Θ(p, t) can be expressed as	computer vision;microsoft solutions framework;structure from motion;velocity (software development)	Vladislav Golyanik;Aman S. Mathur;Didier Stricker	2016			computer vision;pattern recognition;artificial intelligence;computer science;monocular	Vision	52.120469716614366	-49.41689705924944	104896
01034dccda17fe9631e9fda01ff49eebb467419e	incremental reconstruction of urban environments by edge-points delaunay triangulation	manifolds;surface reconstruction;reconstruction from sparse point cloud;estimation;three dimensional displays image edge detection image reconstruction manifolds surface reconstruction estimation cameras;image edge detection;three dimensional displays;video signal processing image reconstruction mesh generation;image reconstruction;mapping;3d reconstruction;cameras;velodyne measurement incremental reconstruction edge points delaunay triangulation urban reconstruction automated mapping monocular video 3d delaunay triangulation online incremental mapping traversability analysis obstacle avoidance urban landscape inverse cone heuristic;manifold	Urban reconstruction from a video captured by a surveying vehicle constitutes a core module of automated mapping. When computational power represents a limited resource and, a detailed map is not the primary goal, the reconstruction can be performed incrementally, from a monocular video, carving a 3D Delaunay triangulation of sparse points; this allows online incremental mapping for tasks such as traversability analysis or obstacle avoidance. To exploit the sharp edges of urban landscape, we propose to use a Delaunay triangulation of Edge-Points, which are the 3D points corresponding to image edges. These points constrain the edges of the 3D Delaunay triangulation to real-world edges. Besides the use of the Edge-Points, a second contribution of this paper is the Inverse Cone Heuristic that preemptively avoids the creation of artifacts in the reconstructed manifold surface. We force the reconstruction of a manifold surface since it makes it possible to apply computer graphics or photometric refinement algorithms to the output mesh. We evaluated our approach on four real sequences of the public available KITTI dataset by comparing the incremental reconstruction against Velodyne measurements.	algorithm;cartography;computer graphics;cone;delaunay triangulation;experiment;harris affine region detector;heuristic;kanade–lucas–tomasi feature tracker;national lidar dataset;obstacle avoidance;parallax;preemption (computing);refinement (computing);sparse matrix;visual artifact	Andrea Romanoni;Matteo Matteucci	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7354012	computer vision;mathematical optimization;delaunay triangulation;manifold;mathematics;geometry;constrained delaunay triangulation;bowyer–watson algorithm;statistics	Robotics	53.13850622243479	-46.704785642916924	104946
6ec39b30d54cc5760a694976161f3b45885cad78	dynamic sensor planning	robots computer vision detectors;detectors;robotics;computer vision;motion planning sensor systems robot vision systems optical sensors robot sensing systems solid modeling sensor phenomena and characterization machine vision condition monitoring motion analysis;machine vision;robots;geometric model;robust view machine vision planning sensor planning abilities mvp pre planned robot task geometric models optical models swept volumes temporal interval search technique	In this paper, we describe a method of extending the sensor planning abilities of the “MVP” Machine Vision Planning system to plan viewpoints for monitoring a preplanned robot task. The dynamic sensor planning system presented here analyzes geometric models of the environment and of the planned motions of the robot, as well as optical models of the vision sensor. Using a combination of swept volumes and a temporal interval search technique, it computes a series of viewpoints, each of which provides a valid viewpoint for a different interval of the task. By mounting a camera on another manipulator, the viewpoints can be executed at appropriate times during the task so that there is always a robust view suitable for monitoring the task. Experimental results monitoring a simulated robot operation are presented, and directions for future research	automated planning and scheduling;computational geometry;machine vision;robot;sensor	Steven Abrams;Peter K. Allen;Konstantinos A. Tarabanis	1993		10.1109/ROBOT.1993.291892	robot;control engineering;computer vision;detector;simulation;machine vision;computer science;artificial intelligence;geometric modeling;robotics	Robotics	50.56424084802555	-39.91756920842298	105067
b302f4cb532b5575938ca340c5482256734f9dc0	adaptive camera calibration in an industrial robotic environment	spatial reasoning;computer vision;dynamic environment;industrial robots;camera calibration;linear equations	One of the fundamental difficulties that arises when attempting to use computer vision in dynamic environments is that camera calibration coefficients must be adjusted as the relative distances between camera and target object change, causing refocusing to occur. Such situations arise frequently in robotic environments in which the visual sensor is mobile or the target objects are in motion. This paper presents a method for computing camera calibration coefficients for cases in which it is known that the relative motion between camera and target object is a translation along the optical axis, as in cases for which the camera is moving directly toward or away from an object of interest. The calibration technique is straightforward, involving only the solution of linear equations. It is demonstrated that, within the context of a spatial reasoning system, inclusion of the calibration method can improve the relative accuracy of spatial inferences by one to two orders of magnitude.	apache axis;camera resectioning;coefficient;computer vision;industrial robot;linear equation;reasoning system;spatial–temporal reasoning	Michael J. Magee;William A. Hoff;Lance Gatrell;Martin Marietta;William J. Wolfe	1990		10.1145/98784.98825	smart camera;computer vision;camera auto-calibration;camera resectioning;simulation;computer science;linear equation;spatial intelligence;pinhole camera model	Robotics	53.231628021986985	-40.45029587480755	105105
f4b46bec9e35f76b70b971725bfb3b8b835f6ac7	visual tracking with automatic motion model switching	feature tracking;multiple hypothesis tracking;image sequence;multiple model;motion model switching;visual tracking;multiple model filtering;interacting multiple model	"""This paper provides a novel technique of e$ciently and reliably tracking features in a sequence of images. The method we provide for tracking features is based on the Bayesian multiple hypothesis tracking (MHT) technique coupled with a multiple model """"ltering (MMF) algorithm. We show the results of our work comparing it with some of the existing single-model-based trackers using a variety of video sequences. Initially, we demonstrate the ability of the MHT}MMF tracker, and later in the paper we extend the MMF-based tracker to the interacting multiple model (IMM) tracker and show the superiority of the latter in handling motion transition of features e$ciently. The primary purpose of this paper is to show how the IMM algorithm combined with an extension of the classical MHT framework can be used in a visual tracking scenario. The study shows that the method proposed can distinguish between di!erent motions depicted in an image sequence with good tracking results. ( 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."""	algorithm;incremental funding methodology;interaction;mhtml;pattern recognition;radar tracker;video tracking	Prithiraj Tissainayagam;David Suter	2001	Pattern Recognition	10.1016/S0031-3203(00)00019-4	computer vision;simulation;tracking system;eye tracking;control theory;mathematics	Vision	43.83520786427596	-48.28970284044379	105342
282ef89bf2f13df409727e53c000594d7f523294	robust and adaptive object tracking via correspondence clustering				Bo Wu;Yurui Xie;Wang Luo	2016	IEICE Transactions		correlation clustering;computer vision;active appearance model;eye tracking;computer science;machine learning;pattern recognition;cluster analysis	Vision	41.32829518215866	-48.86626806388099	105349
6bd3e7124d32826916fe169ad582522ef07aaf17	ready—aim—fly! hands-free face-based hri for 3d trajectory control of uavs		We present a novel user interface for aiming andlaunching flying robots on user-defined trajectories. The methodrequires no user instrumentation and is easy to learn by analogyto a slingshot. With a few minutes of practice users can sendrobots along a desired 3D trajectory and place them in 3D space, including at high altitude and beyond line-of-sight. With the robot hovering in front of the user, the robot tracksthe user's face to estimate its relative pose. The azimuth, elevationand distance of this pose control the parameters of the robot'ssubsequent trajectory. The user triggers the robot to fly thetrajectory by making a distinct pre-trained facial expression. Wepropose three different trajectory types for different applications:straight-line, parabola, and circling. We also describe a simple training/startup interaction to selecta trajectory type and train the aiming and triggering faces. Inreal-world experiments we demonstrate and evaluate the method. We also show that the face-recognition system is resistant to inputfrom unauthorized users.	authorization;experiment;facial recognition system;human–robot interaction;line-of-sight (missile);robot;robotics;unmanned aerial vehicle;user interface	Jake Bruce;J Taylor Perron;Richard T. Vaughan	2017	2017 14th Conference on Computer and Robot Vision (CRV)	10.1109/CRV.2017.39	artificial intelligence;computer science;computer vision;azimuth;robot;slingshot;facial recognition system;trajectory;user interface;instrumentation;human–robot interaction	Robotics	46.95895595288426	-40.52975255643388	105416
14bd98b8b2f6934bbd8c6fedac1d0c53fc48f8e0	second-order belief propagation and its application to object localization	second order;belief networks;particle filtering second order belief propagation object localization computer vision problems stereo matching object detection low level vision;local algorithm;image matching;computer vision;second order belief propagation;object localization;first order;stereo matching;belief propagation;particle filter;stereo image processing;particle filtering;stereo image processing belief networks computer vision image matching object detection;belief propagation computer vision stereo vision application software inference algorithms filtering performance analysis clustering algorithms face cybernetics;low level vision;object detection;computer vision problems	Belief propagation (BP) has been successfully used to solve many computer vision problems, such as stereo matching, object detection and low-level vision. Although the BP algorithm provides an efficient computation framework for general graph inference, the capability of BP is limited by the fact that it only considers the first-order constraints which can simply model the distance relation between two nodes. But in many computer vision problems, this limitation will bring on serious results when the differential or the angular constraints are inherent in the problems. To resolve this limitation, we generalize the BP algorithm to consider the second-order constraints, and integrate it into the particle filtering framework to speed up the computation. In addition, we apply the proposed method to develop a face localization algorithm to demonstrate its effectiveness.	algorithm;angularjs;backpropagation;belief propagation;computation;computer stereo vision;computer vision;face detection;first-order predicate;high- and low-level;mathematical optimization;message passing;object detection;particle filter;photometric stereo;software propagation	Yong-Dian Jian;Chu-Song Chen	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.384750	computer vision;particle filter;computer science;theoretical computer science;machine learning	Vision	52.56752826495122	-39.359597790503855	105577
db9f5e8b28a3674c34c8304910e8c0dd1337a954	multiple image matching in an automatic aerotriangulation system	image matching;least square method;affine transformation;least squares matching;geometric constraints	A crucial part of the aerotriangulation process is the identification and precise measurement of well-distributed points over areas with multiple image overlap. The selected point positions should be determined with sub-pixel accuracy on all the overlapping images. In this paper we describe an algorithm which combines gray-scale matching with geometrical constraints. The least squares method works in object space which is derived from the overlap and sensor geometry approximations. The model is based on the simultaneous use of multiple images which is fundamentally different from the traditional pairwise approach. Some practical data highlight the object-space-constrained, affine-transformation-based, gray-scale, least squares matching method.	image registration	Charles K. Toth;Toni Schenk	1993		10.1007/3-540-57233-3_103	mathematical optimization;combinatorics;discrete mathematics;template matching;affine transformation;mathematics;least squares	Vision	50.81389848284942	-51.69033246774327	105612
20d1e632da7d6f28a33a89aee89c5102947fe21f	precision tracking via joint detailed shape estimation of arbitrary extended objects	precision tracking joint detailed shape estimation arbitrary extended objects ego motion plane probabilistic inference techniques kinematic states autonomous vehicle research platform explicit surface based sensor models latent object state observability;robot sensing systems;shape robot sensing systems kinematics data models shape measurement three dimensional displays;extended objects object tracking probabilistic inference robot perception sensor fusion shape estimation;shape measurement;kinematics;estimation theory inference mechanisms object tracking shape recognition;shape;three dimensional displays;data models	A novel approach to estimating the detailed shape of arbitrary extended objects jointly with their kinematics in the absence of a priori information is presented. The proposed shape model represents the tightest enclosing bound of the object projected into the ego motion plane as a polygon with an unknown number of vertices. Probabilistic inference techniques are employed to overcome various sources of uncertainty by rigorously estimating the joint distribution over the object shape and kinematic states, rather than estimating these variables directly. Simulation and experimental results are presented for objects with complex shapes tracked from an autonomous vehicle research platform. In addition to providing a richer set of information for higher level reasoning about extended objects (e.g. about object type, or occupied space), the results demonstrate that detailed shape estimates enable efficient use of sensor information by way of explicit surface-based sensor models; this efficient use of sensor information improves observability of latent object states, thereby improving tracking precision.	arbitrary-precision arithmetic;autonomous car;autonomous robot;object type (object-oriented programming);simulation	Kevin Wyffels;Mark E. Campbell	2017	IEEE Transactions on Robotics	10.1109/TRO.2016.2630058	active shape model;data modeling;computer vision;kinematics;simulation;shape;computer science;machine learning;mathematics	Vision	53.27305042581628	-38.96200476561938	105653
c9a5b2f3a939f717accadc3f2584af933f2a1926	qualitative review of object recognition techniques for tabletop manipulation		This paper provides a qualitative review of different object recognition techniques relevant for near-proximity Human-Robot Interaction. These techniques are divided into three categories:2D correspondence, 3D correspondence and non-vision based methods. For each technique an implementation is chosen that is representative of the existing technology to provide a broad review to assist in selecting an appropriate method for tabletop object recognition manipulation. For each of these techniques we give their strengths and weaknesses based on defined criteria. We then discuss and provide recommendations for each of them.	human–robot interaction;outline of object recognition	Christopher D. Wallbridge;Séverin Lemaignan;Tony Belpaeme	2017		10.1145/3125739.3132593	computer science;object detection;strengths and weaknesses;computer vision;artificial intelligence;cognitive neuroscience of visual object recognition	HCI	47.35847038417615	-39.076974584083736	105731
2af0ab5b414aec1f50b39a5a3e83cd135ded1d1f	motion estimations based on invariant moments for frames interpolation in stereovision	motion estimation;frame interpolation;stereovision	Abstract At present, stereo video sequences are actively used in the movie industry, in geographical information systems, and in navigation systems, among others. A novel method improves the frames interpolation by forming an invariant set of local motion vectors. First, the motion in a scene is estimated by block-matching algorithm. Second, accurate estimations by using Hu moments (for a noisy video sequence Zernike moments) are calculated. Such approach provides smooth motion that significantly improves the resulting stereo video sequence. Experimental results show the efficiency of frames interpolation based on such approach. The detection of local motion vectors achieves 86% accuracy.	image moment;interpolation;stereopsis	Margarita N. Favorskaya;Dmitriy Pyankov;Aleksei Popov	2013		10.1016/j.procs.2013.09.196	velocity moments;computer vision;simulation;quarter-pixel motion;motion estimation;mathematics;geometry;block-matching algorithm;motion compensation	Logic	48.68427507141397	-46.559247029269834	105740
59400ba1f0b8af25e83ea858fb6b9ad405d1785d	object and shadow separation using fuzzy markov random field and local gray level co-occurence matrix based textural features	pattern clustering;image motion analysis;pattern clustering feature extraction fuzzy set theory image colour analysis image motion analysis image texture lighting markov processes maximum likelihood estimation object detection;state of the art techniques shadow separation fuzzy markov random field local gray level cooccurence matrix based textural features object detection technique moving object separation background model pixel values temporal direction color frequency variation rgb color local features background separation pixel absolute difference thirteen dimensional target image frame spatial markov random field constrained fuzzy clustering mrf constrained fuzzy clustering maximum aposteriori probability map binary image moving cast shadow three stage shadow analysis technique rgb color chrominance property local gray level feature based shadow processing boundary refinement;maximum likelihood estimation;fuzzy set theory;image texture;image colour analysis;feature extraction;photometric invariants background subtraction shadow removal temporal analysis color model;lighting;markov processes;object detection;estimation image color analysis video sequences correlation object detection feature extraction markov random fields	In this article, we propose a novel object detection technique that can separate the moving object from its shadow. In this regard, we initially built a background model by taking median of pixel values in the temporal direction. To suppress the effects of quick change in illumination, and color frequency variation of the textured background, we have extracted the RGB color and ten local features at each pixel location in the target image and background model. For background separation, a difference image is generated by considering pixel by pixel absolute difference of the thirteen dimensional target image frame and the constructed background model. This is followed by a spatial Markov Random Field (MRF) constrained fuzzy clustering to find the moving regions in the target frame. The maximum a'posteriori probability (MAP) of the MRF constrained fuzzy clustering provides a binary image, where the moving objects with the moving cast shadow are identified as one group and the background is obtained as another group. To segment the moving objects from its shadow we explore a three stage shadow analysis technique. It uses analysis of rg color chrominance property of shadow, local gray level feature based shadow processing followed by boundary refinement to separate out the moving objects from its shadows. The performance of the proposed scheme is evaluated by comparing it with the state-of-the-art techniques.	binary image;cluster analysis;fuzzy clustering;grayscale;markov chain;markov random field;object detection;pixel;refinement (computing)	Badri Narayan Subudhi;Susmita Ghosh;Ashish Ghosh	2012	2012 12th International Conference on Intelligent Systems Design and Applications (ISDA)	10.1109/ISDA.2012.6416519	image texture;computer vision;feature extraction;computer science;machine learning;pattern recognition;lighting;maximum likelihood;fuzzy set;markov process;statistics	Vision	44.16606926155407	-50.79173998977697	106514
83eca99a135c03d3f144880d47d382916d4c314b	fish monitoring and sizing using computer vision		This paper proposes an image processing algorithm, based in a non invasive 3D optical stereo system and the use of computer vision techniques, to study fish in fish tanks or pools. The proposed technique will allow to study biological variables of different fish species in underwater environments. This knowledge, may be used to replace traditional techniques such as direct observation, which are impractical or affect the fish behavior, in task such as aquarium and fish farm management or fishway evaluation. The accuracy and performance of the proposed technique has been tested, conducting different assays with living fishes, where promising results were obtained.	computer vision	Alvaro Rodríguez;Angel J. Rico-Diaz;Juan R. Rabuñal;Jerónimo Puertas;Luis Peña	2015		10.1007/978-3-319-18833-1_44	simulation	Vision	45.790162362671445	-42.36461293696115	106536
413b5b153e8f9a77f2d300a6bc922e12eadd4d41	line drawing interpretation using belief propagation	probabilistic formulation line drawing interpretation belief propagation trihedral planer objects bayesian inference problem line drawing image markov random field huffman clowes catalogue;keywords arc consistency;inference engines;edge detection;line drawing interpretation;bayesian inference;computational geometry;markov random fields;arc consistency;inference mechanisms;junctions;markov random field;computer vision;conference paper;line drawings;junctions image edge detection labeling mathematical model inference algorithms belief propagation educational institutions;image edge detection;probabilistic formulation;belief propagation;line drawing image labeling;mathematical model;inference algorithms;belief maintenance;markov processes belief maintenance computational geometry computer vision inference mechanisms;markov processes;clique potential;belief propagation algorithm;potential function;labeling;belief propagation line drawing image labeling markov random field;junction detection;bayesian networks	The interpretation of line drawings of trihedral planer objects is a classic problem. In this paper, it is formulated as a Bayesian inference problem. Given a line drawing image, a Markov random field can be built whose nodes represent the labels of edges. Its clique potential functions are designed to encode the valid junctions in the Huffman-Clowes catalogue. The belief propagation algorithm is used to find the most probable labeling of the edges. We find this algorithm closely related to the arc consistency methods. However our probabilistic formulation can accommodate uncertainty in junction detection and make use of various image cues. These advantages are demonstrated in the experiments.	bayesian network;belief propagation;computer vision;constraint (mathematics);encode;edge detection;experiment;huffman coding;line drawing algorithm;local consistency;markov chain;markov random field;monte carlo method;noise (electronics);polyhedron;robot;sensor;software propagation	Yansheng Ming;Hongdong Li;Jun Sun	2011	2011 International Conference on Digital Image Computing: Techniques and Applications	10.1109/DICTA.2011.26	computer vision;labeling theory;edge detection;computational geometry;computer science;theoretical computer science;machine learning;pattern recognition;bayesian network;mathematical model;mathematics;markov process;bayesian inference;inference engine;local consistency;statistics;belief propagation	Vision	45.80728917166736	-50.11746211137087	106559
7607458f3df7e10e77a3b4ec4e2cf4591bd03f9c	multi-view structure computation without explicitly estimating motion	graph theory;image motion analysis;tensile stress;semidefinite programming;structure computation step;convex programming;application software;semidefinite programming multiview structure computation structure from motion method structure computation step graph rigidity theory sfm problem convex relaxation;graph rigidity theory;computational geometry;relaxation theory convex programming graph theory image motion analysis;multiview structure computation;motion estimation;motion estimation cameras layout tensile stress large scale systems computational geometry equations australia computer vision application software;layout;sfm problem;structure computation;computer vision;camera motions;relaxation processes;conference paper;camera motion;large scale;keywords 3d structure;semi definite program;existing structure;structure from motion method;convex relaxation;relaxation theory;3d structure;structure from motion;multi views;cameras;australia;large scale systems;semi definite programming	Most existing structure-from-motion methods follow a common two-step scheme, where relative camera motions are estimated in the first step and 3D structure is computed afterward in the second step. This paper presents a novel scheme which bypasses the motion-estimation step, and goes directly to structure computation step. By introducing graph rigidity theory to Sfm problems, we demonstrate that such a scheme is not only theoretically possible, but also technically feasible and effective. We also derive a new convex relaxation technique (based on semi-definite programming) which implements the above scheme very efficiently. Our new method provides other benefits as well, such as that it offers a new way to looking at Sfm, and that it is naturally suited for handling sparse large-scale Sfm problems.	computation;convex optimization;linear programming relaxation;motion estimation;semiconductor industry;semidefinite programming;sparse matrix;structure from motion	Hongdong Li	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540005	layout;computer vision;mathematical optimization;combinatorics;application software;structure from motion;computer science;motion estimation;mathematics;geometry;stress;semidefinite programming	Vision	52.223968214398134	-50.6998914068077	106615
0326f4b6c38e7bc0f29668fe2fecf61bc893626b	probabilistic multiple cue integration for particle filter based tracking	computer vision;conference paper;cue integration;particle filter;visual tracking	Robust visual tracking has become an important topic in the field of computer vision. The integration of cues such as color, edge strength and motion has proved to be a promising approach to robust visual tracking in situations where no single cue is suitable. In this paper, an algorithm is presented which integrates multiple cues in a probabilistic manner. Specifically the likelihood of each cue is calculated and weighted before Bayes’ rule is applied to obtain the resultant posterior. This posterior is generally not well represented analytically, and is therefore represented as a set of weighted particles, which is updated at each frame by a particle filter. This paper demonstrates how the combination of multiple cue integration and particle filtering results in a robust tracking method. We also demonstrate how each cue’s weight can be adapted on-line during the tracking procedure.	algorithm;color;computer vision;online and offline;particle filter;resultant;video tracking	Chunhua Shen;Anton van den Hengel;Anthony R. Dick	2003			computer vision;simulation;particle filter;eye tracking;computer science;machine learning	Vision	45.140150792436636	-48.04926338003896	106754
22646e00a7ba34d1b5fbe3b1efcd91a1e1be3c2b	a database for person re-identification in multi-camera surveillance networks	height models multicamera surveillance networks individuals recognition cameras network ambient lighting conditions multicamera surveillance database design unscripted sequences building environment varying illumination conditions flexible xml based evaluation protocol highly configurable evaluation setup baseline person reidentification system texture models colour models;image recognition;surveillance;xml cameras image recognition lighting surveillance visual databases;xml;lighting;cameras;cameras image color analysis databases histograms lighting biological system modeling surveillance;visual databases	Person re-identification involves recognising individuals in different locations across a network of cameras and is a challenging task due to a large number of varying factors such as pose (both subject and camera) and ambient lighting conditions. Existing databases do not adequately capture these variations, making evaluations of proposed techniques difficult. In this paper, we present a new challenging multi-camera surveillance database designed for the task of person re-identification. This database consists of 150 unscripted sequences of subjects travelling in a building environment though up to eight camera views, appearing from various angles and in varying illumination conditions. A flexible XML-based evaluation protocol is provided to allow a highly configurable evaluation setup, enabling a variety of scenarios relating to pose and lighting conditions to be evaluated. A baseline person re-identification system consisting of colour, height and texture models is demonstrated on this database.	baseline (configuration management);color space;consistency model;database;viewing angle;xml	Alina Bialkowski;Simon Denman;Sridha Sridharan;Clinton Fookes;Patrick Lucey	2012	2012 International Conference on Digital Image Computing Techniques and Applications (DICTA)	10.1109/DICTA.2012.6411689	computer vision;xml;simulation;computer science;lighting;multimedia	Vision	41.16728219740647	-49.470968421461734	106829
1b1d87a3ab607cff48d5808058fbabfc435ddb08	real-time modeling and tracking manual workflows from first-person vision	image motion analysis;video signal processing;transforms;cameras image segmentation visualization tracking manuals optical distortion optical imaging;video signal processing augmented reality image motion analysis image sequences tracking transforms;2d region descriptors real time modeling manual workflow tracking first person vision video sequences augmented reality manuals user interaction camera motion piecewise homographic transform homographic model;augmented reality;tracking;image sequences	Recognizing previously observed actions in video sequences can lead to Augmented Reality manuals that (1) automatically follow the progress of the user and (2) can be created from video examples of the workflow. Modeling is challenging, as the environment is susceptible to change drastically due to user interaction and camera motion may not provide sufficient translation to robustly estimate geometry. We propose a piecewise homographic transform that projects the given video material onto a series of distinct planar subsets of the scene. These subsets are selected by segmenting the largest image region that is consistent with a homographic model and contains a given region of interest. We are then able to model the state of the environment and user actions using simple 2D region descriptors. The model elegantly handles estimation errors due to incomplete observation and is robust towards occlusions, e.g., due to the user's hands. We demonstrate the effectiveness of our approach quantitatively and compare it to the current state of the art. Further, we show how we apply the approach to visualize automatically assessed correctness criteria during run-time.	augmented reality;correctness (computer science);emergent;finger tracking;map;real-time transcription;region of interest	Nils Petersen;Alain Pagani;Didier Stricker	2013	2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)	10.1109/ISMAR.2013.6671771	computer vision;augmented reality;computer science;video tracking;tracking;multimedia;computer graphics (images)	Vision	49.651317905060004	-48.168143330568846	107062
07035e43ecef134de63a87aab556426d7c309d48	statistical recognition of motion patterns	automated surveillance;human robot interaction;people tracking;intention recognition	One key prerequisite for a machine to interact intelligently with people is its ability to recognize humans as interaction partners, and to understand their behaviors and the intentions and plans behind them. In this paper we are particularly interested in behaviors which are related to motion, e.g. intentionally moving towards a goal or obstructing somebody's path. More specifically, we address the problem of detecting and tracking people in natural dynamic environments in realtime, and extracting and classifying typical motion patterns. Robust detection and tracking of nonrigid objects in natural changing environments with a moving observer is a challenging problem. Although various sophisticated approaches have been presented, robustness with respect to changing light conditions, non-rigidness of the tracked objects, occlusion and motion of the observer is still a problem which needs for an effective solution. Recognizing and understanding the intention of motion would leverage applications in areas su...		Jörg Illmann;Boris Kluge;Erwin Prassler;Matthias Strobel	2003	Advanced Robotics	10.1163/156855303769157027	human–robot interaction;computer vision;simulation;computer science;engineering;artificial intelligence	Robotics	46.182781053932494	-40.56954612884027	107119
251087a5af75f6e5f253711069c64481b1d8389d	real-time template-based tracking of non-rigid objects using bounded irregular pyramids	target tracking histograms image segmentation robust stability telecommunications video sequences computer vision application software surveillance video compression;tracking system;hidden feature removal;image matching;real time;video sequence real time template based tracking nonrigid object bounded irregular pyramid object tracking template matching partial occlusion target distortion;object tracking;target tracking;template matching;object detection;hidden feature removal target tracking object detection image matching	In object tracking, change of objects aspect is the most important cause of failure. This paper proposes a modified template matching approach to solve this problem without a priori learning of object views. This method permits to track non-rigid objects in real-time by employing a weighted template, which is dynamically updated, and a hierarchical framework that integrates all the components of the tracker. The capability of the tracking system to handle partial occlusions and target distortions is demonstrated for several video sequences.	algorithm;algorithmic efficiency;computation;distortion;real-time locating system;template matching;tracking system	Rebeca Marfil;Antonio Bandera;Juan Antonio Rodríguez;Francisco Sandoval Hernández	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389368	computer vision;template matching;tracking system;computer science;artificial intelligence;machine learning;video tracking;pattern recognition	Robotics	44.9063549263768	-48.97687479681956	107158
8270d99382f7ecc3b838c035d91ca905a818077f	long-term object tracking for parked vehicle detection	vehicles lighting robustness tracking real time systems feature extraction surveillance;i lid object tracking parked vehicle detection vehicle tracking occlusions template matching;object tracking object detection	We develop a robust approach to detect parked vehicles in real time. Our approach particularly focuses on tracking vehicles in long term under challenging conditions such as lighting changes and occlusions. Vehicle tracking is performed by template matching based on fast-computed corner points. The template model is made self-adaptive over time to accommodate lighting changes. We also present an effective way to manage and track multiple vehicles when they are parked close together and occlude one another. We demonstrate the effectiveness of our approach on the challenging i-LIDs data set and another large one collected from real-world scenarios.	robustness (computer science);template matching;vehicle tracking system	Quanfu Fan;Sharath Pankanti;Lisa M. Brown	2014	2014 11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2014.6918672	embedded system;computer vision;simulation;tracking system	Robotics	43.04297962281408	-46.018155645892826	107206
224a95c4c8f6271c0dc6ebd801e587c5e2eb9be7	a study of virtual visual servoing sensitivity in the context of image/gis registration for urban environments		This paper studies the sensitivity of pose estimation to the 2D measure noise when using virtual visual servoing. Attempting to apply virtual visual servoing to image/Geographic Information System (GIS) registration, the robustness to the noise in images is an important factor to the accuracy of estimation. To analyze the impact of different levels of noise, a series of image/GIS registration tests based on synthetic input image are studied. Also, RANSAC is introduced to improve the robustness of the method. We also compare some different strategies in choosing geometrical features and in the treatment of projection error vector in virtual visual servoing, providing a guide for parametrization.	3d pose estimation;anomaly detection;geographic information system;random sample consensus;simulation;synthetic data;visual servoing	Hengyang Wei;Muriel Pressigout;Luce Morin;Myriam Servieres;Guillaume Moreau	2017	2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)	10.23919/MVA.2017.7986915	artificial intelligence;computer vision;robustness (computer science);pattern recognition;ransac;visual servoing;parametrization;pose;geographic information system;computer science	Visualization	53.21288651123138	-42.5416511415696	107286
7913efb8b7c5b7f49a879b24ab719ec0649f124e	enhancing the mixture of gaussians background model with local matching and local adaptive learning	features matching;foreground detection;mixture of gaussians	The Mixture of Gaussians (MoG) is a frequently used method for foreground-background separation. Although it is quite capable of handling gradual illumination changes and multi-model background, it cannot cope with dynamic changes such as the presence of paused objects, shadows, and sudden illumination changes. Furthermore, it is a parametric model and in general, its parameter tuning for different scenes remains a manual effort. In this paper, we propose an online learning framework that tackles these issues. Our main contributions are: local adaptive parameter learning, a feedback based updating method for stopped objects, hierarchical SURF features matching based ghosts suppression, and a new sudden illumination detection and handling technique. The proposed model is rigorously tested and compared with several previous models and has shown significant performance improvements.	foreground-background;mixture model;online machine learning;parametric model;performance tuning;speeded up robust features;zero suppression	Munir Shah;Jeremiah D. Deng;Brendon J. Woodford	2012		10.1145/2425836.2425859	computer vision;computer science;machine learning;pattern recognition;mixture model	Vision	42.60495867883988	-49.11033837497974	107388
14e878ec2d9b6c026339155d8ac53fae8d069c91	hierarchical architecture for robust people detection by fusion of infrared and visible video		Robust people detection systems are nowadays using heterogeneous cameras. This paper proposes an hierarchical architecture which is focused on robustly detecting people by fusion of infrared and visible video. The architecture covers all levels provided by the INT(^3)-Horus framework, initially designed to perform monitoring and activity interpretation tasks. Indeed, INT(^3)-Horus is used as the development environment where the approach starts with image segmentation in both infrared and visible spectra. Then, the results are fused to enhance the overall detection performance.		José Carlos Castillo;Juan Serrano-Cuerda;Antonio Fernández-Caballero;Arturo Martínez-Rodrigo	2015		10.1007/978-3-319-25017-5_32	computer vision;remote sensing	Vision	43.60229885794159	-43.05233114732522	107471
7e4a68c4be383c5cb1e7be1069e684ca4c711193	pose classification using support vector machines	eigenvalues and eigenfunctions;silhouette;support vector machines;neural nets;human arm pose;binary image;mobile robot;mobile robots;eigenvalues;navigation;software architecture;robot vision;position control;eigenvalues vector;covariance matrices;shape classification;navigation mobile robots robot vision gesture recognition pattern classification eigenvalues and eigenfunctions covariance matrices position control neural nets;pattern classification;robot vision pose recognition pattern classification support vector machines human arm pose mobile robot binary image silhouette eigenvalues vector covariance matrix shape classification;support vector machine classification;arm;humans;support vector machine;support vector machines support vector machine classification humans eigenvalues and eigenfunctions software architecture robotics and automation mobile robots arm covariance matrix usability;pose recognition;usability;gesture recognition;robotics and automation;covariance matrix	The field of human-computer interaction has been widely investigated in the last years, resulting in a variety of systems used in different application fields like virtual reality simulation environments, software user interfaces, and digital library systems. A very crucial part of all these systems is the input module which is devoted to recognize the human operator in terms of tracking and/or recognition of human face, arms position, hand gestures, and so on. In this work a software architecture is presented, for the automatic recognition of human arms poses. Our research has been carried on in the robotics framework. A mobile robot that has to find its path to the goal in a partially structured environment can be trained by a human operator to follow particular routes in order to perform its task quickly. The system is able to recognize and classify some different poses of the operator’s arms as direction commands like “turn-left”, “turn-right”, “go-straight”, and so on. A binary image of the operator silhouette is obtained from the gray-level input. Next, a slice centered on the silhouette itself is processed in order to compute the eigenvalues vector of the pixels co-variance matrix. This kind of information is strictly related to the shape of the contour of the operator figure, and can be usefully employed in order to assess the arms’ position. Finally, a support vector machine (SVM) is trained in order to classify different poses, using the eigenvalues array. A detailed description of the system is presented along with some remarks on the statistical analysis we used, and on SVM. The experimental results, and an outline of the usability of the system as a generic shape classification tool are also reported.	binary image;coat of arms;digital library;human–computer interaction;mobile robot;pixel;robotics;simulated reality;simulation;software architecture;support vector machine;usability;user interface;virtual reality	Edoardo Ardizzone;Antonio Chella;Roberto Pirrone	2000		10.1109/IJCNN.2000.859415	mobile robot;support vector machine;computer vision;computer science;machine learning;pattern recognition;gesture recognition;artificial neural network	Robotics	48.27271810942591	-38.88089073354894	107604
37c4541037b67e8f4c538b285efe80aa251a49b9	tracking as a whole: multi-target tracking by modeling group behavior with sequential detection		Video-based vehicle detection and tracking is one of the most important components for intelligent transportation systems. When it comes to road junctions, the problem becomes even more difficult due to the occlusions and complex interactions among vehicles. In order to get a precise detection and tracking result, in this paper we propose a novel tracking-by-detection framework. In the detection stage, we present a sequential detection model to deal with serious occlusions. In the tracking stage, we model group behavior to treat complex interactions with overlaps and ambiguities. The main contributions of this paper are twofold: 1) shape prior is exploited in the sequential detection model to tackle occlusions in crowded scene and 2) traffic force is defined in the traffic scene to model group behavior, and it can assist to handle complex interactions among vehicles. We evaluate the proposed approach on real surveillance videos at road junctions and the performance has demonstrated the effectiveness of our method.	algorithm;interaction;mesa;sensor	Yuan Yuan;Yuwei Lu;Qi Wang	2017	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2017.2686871	tracking system	Vision	41.92006192416151	-46.19291152197705	107713
6ce5b034234141fd967b733ed7454d4982622c44	a simple change comparison method for image sequences based on uncertainty coefficient		For identification of change information in image sequences, most studies focus on change detection in one image sequence, while few studies have considered the change level comparison between two different image sequences. Moreover, most studies require the detection of image information in details, for example, object detection. Based on Uncertainty Coefficient(UC), this paper proposes an innovative method “CCUC” for change comparison between two image sequences. The proposed method is computationally efficient and simple to implement. The change comparison stems from video monitoring system. The limited number of provided screens and a large number of monitoring cameras require the videos or image sequences ordered by change level. We demonstrate this new method by applying it on two publicly available image sequences. The results are able to show the method can distinguish the different change level for sequences.	algorithmic efficiency;object detection;type system;uncertainty coefficient	Ruzhang Zhao;Yajun Fang;Berthold K. P. Horn	2018	CoRR		object detection;change detection;computer science;uncertainty coefficient;pattern recognition;artificial intelligence	AI	42.19869107249289	-47.433226495135344	107751
770d4aa237bad420cf39eb8e607e447f31af2f75	scale matching of 3d point clouds by finding keyscales with spin images	principal component analysis image matching image reconstruction;3d point clouds scale matching;object recognition;spin images;keyscale;3d point sets;3d point cloud;image matching;three dimensional displays;image reconstruction;clouds;principal component analysis;solid modeling;pca keyscale 3d point sets spin images;principal component analysis 3d point clouds scale matching spin images 3d reconstruction techniques keyscale pca cumulative contribution rate;robustness;pca cumulative contribution rate;cumulant;pca;clouds three dimensional displays principal component analysis noise solid modeling robustness object recognition;3d reconstruction;3d reconstruction techniques;noise	In this paper we propose a method for matching the scales of 3D point clouds. 3D point sets of the same scene obtained by 3D reconstruction techniques usually differ in scales. To match scales, we propose a {\em keyscale} that characterizes the scale of a given 3D point cloud. By performing PCA of spin images over different scales, a keyscale is defined as the scale that gives the minimum of cumulative contribution rate of PCA at a specific dimension of eigen space. Simulations with the Stanford bunny and experimental results with 3D reconstructions of a real scene demonstrate that keyscales of any 3D point clouds can be uniquely found and effectively used for scale matching.	3d reconstruction;computer simulation;eigen (c++ library);point cloud;stanford bunny	Toru Tamaki;Shunsuke Tanigawa;Yuji Ueno;Bisser Raytchev;Kazufumi Kaneda	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.850	computer vision;computer science;machine learning;pattern recognition;mathematics;statistics;principal component analysis	Vision	48.38584493651648	-51.15820294269964	107888
ac9a0997267d7c46a396ec243a4903276d986b7b	intelligent video surveillance beyond robust background modeling		The increasing number of video surveillance cameras is challenging video control systems. Monitoring centers require tools to guide the process of supervision. Different video analysis methods have effectively met the main requirements from the industry of perimeter protection. High accuracy detection systems are able to process real time video on affordable hardware. However some problematic environments cause a massive number of false alerts. Many approaches in the literature do not consider this kind of environments while others use metrics that dilute their impact on results. An intelligent video solution for perimeter protection must select and show the cameras which are more likely witnessing a relevant event but systems based only on background modeling tend to give importance to problematic situations no matter if an intrusion is taking place or not. We propose to add a module based on machine learning and global features, bringing adaptability to the video surveillance solution, so that problematic situations can be recognized and given the right priority. Tests with thousands of hours of video show how good an intruder detector can perform but also how a simple fault in a camera can flood a monitoring center with alerts. The new proposal is able to learn and recognize events such that alerts from problematic environments can be properly handled.	adobe photoshop;algorithmic efficiency;analysis of algorithms;brute-force search;closed-circuit television;control system;experiment;global illumination;heuristic (computer science);machine learning;perimeter;real-time clock;requirement;sensitivity and specificity;sensor;video content analysis;word lists by frequency	Eduardo Cermeno;Ana Pérez;Juan A. Sigüenza	2018	Expert Syst. Appl.	10.1016/j.eswa.2017.08.052	simulation;machine learning;artificial intelligence;adaptability;video tracking;intrusion;computer science;control system	Vision	40.71210913314309	-43.92996823319088	107966
1655eef4162a7c84a253941b1eb17ed661dd0b4a	adapted windows detection of moving objects in video scenes	false alarm;moving object;cluster;62h35;62h15;94a08;video segmentation;real time detection;68t45;a contrario method;68u10	This paper presents a fast method for detecting textured objects moving in a video sequence. It is based on a known background estimation and a fixed camera position. The algorithm is able to detect the presence of moving objects and locates them on-line. It is a frame-by-frame method. First, a difference image is computed from the background and the current frame. This yields three classes of pixels, those for which something changed with respect to the background, those for which nothing changed, and finally the pixels for which no decision can be made. Then an a contrario model allows an automatic clustering, by using adapted rectangular windows, of the pixels for which changes have been detected. If necessary, these regions are corrected in order to better fit the moving objects’ boundaries. Experimental results show that the algorithm is very robust with respect to noise and to the quality of the background estimation. The choice of the model parameters is quite natural and user friendly. The algorithm has been successfully tested on video sequences coming from different databases, including indoor and outdoor sequences.	adaptive web design;algorithm;central processing unit;cluster analysis;color;database;mathematical model;microsoft windows;online and offline;performance evaluation;pixel;real-time clock;sensor;usability	Françoise Dibos;Georges Koepfler;Sylvain Pelletier	2009	SIAM J. Imaging Sciences	10.1137/070710500	computer vision;simulation;background subtraction;computer science;video tracking;cluster;computer graphics (images)	Vision	43.79418997501333	-49.19450128515266	108038
a1f01c58bbdc819e36488da625757ed315fc6cdb	active focus and zoom control used for scene analysis	software;image recognition;cameras visualization pixel robots lenses navigation software;neural networks;neural nets;zoom control;structural information;dynamic control;navigation;visualization;robot vision;depth perception;focus control vision depth perception scene analysis;humanoid robots;focus control;pixel;robots;sharpness plans;lenses;complex scenes active focus control zoom control scene analysis visual recognition system robotic application pan tilt camera structural information visual scene neural networks sharpness plans;visual scene;active focus control;pan tilt camera;complex scenes;visual field;robotic application;vision;robot vision cameras humanoid robots image recognition neural nets;visual recognition system;cameras;neural network;scene analysis	We propose a visual recognition system for robotic applications in which distance to the visual objects can change a lot (for instance, trying to recognize a distant object learned from a short distance). Our system takes advantage of a single pan-tilt camera controllable in zoom and focus. Focus control allows to detect plans of sharpness in the scene and indirectly to compute a distance. Hence, this information can be used to gain structural information of the visual scene (to segment objects from the ground, to count the number of depth plans in the visual field…) without complex computation. This distance information is then used to control either a software or a hardware zoom to keep the size of the object invariant. The image thus created can be used by view based recognition systems. In a second time we show how by using focus points and neural networks we can improve the detection of sharpness plans in complex scenes. Finally we present a simple method to dynamically control the focus and stabilize it on the plan of sharpness of an object in the scene.	artificial neural network;class invariant;computation;digital zoom;robot;visual objects	David Bailly;Philippe Gaussier	2010	2010 International Conference of Soft Computing and Pattern Recognition	10.1109/SOCPAR.2010.5686497	robot;vision;computer vision;navigation;simulation;visualization;depth perception;computer science;humanoid robot;machine learning;lens;artificial neural network;pixel;computer graphics (images)	Robotics	49.182565644040395	-38.757537344574594	108043
7f33541ace0269d7659309752841679f3127e8ab	a real-time hough-based method for segment detection in complex multisensor images	search space;real time;noise robustness;sar image;parameter space;low light;computational efficiency;spatial information	T his paper presents a real-time Hough-based algorithm for straight line segment extraction in complex multisensor images, which aims to avoid loss of spatial information as well as to eliminate spurious peaks and reduce discretization errors. A parameter space representation able to take into account spatial information during the voting phase is proposed. This representation allows the detection phase to be performed by focusing the algorithm on particular locations of the parameter space. The search space is consequently reduced, and a deeper decision strategy can be adopted, which takes into account the local distribution of segments along both a line and dierent lines, for comparable directions and positions. Experimental results on a large set of complex multisensor images (e.g. underwater images, lowlight outdoor images, SAR images, etc.) are presented. The main advantages of the proposed method over both feature and image-space methods are evaluated in terms of computational eciency, detection accuracy and noise robustness.	algorithm;computation;decision theory;discretization;hough transform;image processor;multiprocessing;real-time clock;real-time transcription;sparc;sensor	Gian Luca Foresti	2000	Real-Time Imaging	10.1006/rtim.1999.0179	computer vision;simulation;machine learning;spatial analysis;parameter space;statistics	Vision	41.3609074766012	-42.79970688923692	108091
75db724d3f0e3d3374f8425323d6ff29ac1ad4e8	visual vehicle tracking based on conditional random fields	video surveillance belief networks probability random processes tracking traffic engineering computing video signal processing;conditional random fields;automatic vehicle detection and identification systems;mathematical models;mathematical model matlab;vehicle localization visual vehicle tracking conditional random fields moving vehicle tracking surveillance videos uniform probabilistic framework crf model spatial contextual information temporal contextual information approximate inference algorithm loopy belief propagation vehicle region estimation background model nonstationary background processes monocular image sequences region level tracking;algorithms;traffic surveillance;vehicle tracking;region level tracking;automatic tracking;vehicle trajectories;region level tracking vehicle tracking conditional random fields	This paper proposes an approach to moving vehicle tracking in surveillance videos based on conditional random fields (CRF). The key idea is to integrate a variety of relevant knowledge about vehicle tracking into a uniform probabilistic framework by using the CRF model. In this work, the CRF model integrates spatial and temporal contextual information of vehicle motion, and the appearance information of the vehicle. An approximate inference algorithm, loopy belief propagation, is used to recursively estimate the vehicle region from the history of observed images. Moreover, the background model is updated adaptively to cope with non-stationary background processes. Experimental results show that the proposed approach is able to accurately track moving vehicles in monocular image sequences. Besides, region-level tracking realizes precise localization of vehicles.	approximation algorithm;background process;belief propagation;casio loopy;conditional random field;recursion;software propagation;stationary process;vehicle tracking system	Yuqiang Liu;Kunfeng Wang;Fei-Yue Wang	2014	17th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2014.6958189	computer vision;simulation;geography;machine learning;video tracking	Robotics	44.852064279856364	-47.370624799882705	108365
7a53c49a87689c8fa8ac3794bb57d13f4079f1f7	scale adaptation of mean shift based on graph cuts theory	graph theory;scale adaptive tracking graph cuts mean shift;image segmentation mean shift tracking graph cuts theory target tracking skin color gaussian mixture model tracking window size human computer interaction entertainment game video tracking algorithm;human computer interaction;image segmentation;video signal processing gaussian processes graph theory human computer interaction image colour analysis image segmentation target tracking;video signal processing;gaussian processes;real time;mean shift;skin color;gaussian mixture model;scale adaptive tracking;image colour analysis;graph cut;target tracking image color analysis skin image segmentation games face green products;target tracking;graph cuts	The classical Mean Shift can't change the scale of tracking window in real time while tracking target is changing in size. This paper adopts graph cuts theory to the problem of scale adaptation for Mean Shift tracking. According to the result of Mean Shift iteration in every frame, implementing graph cuts using skin color Gaussian mixture model(GMM) in a small area around it, and updating tracking window size through the largest skin lump among the result of graph cuts. Experimental results clearly demonstrate that the method can reflect the real scale change of tracking target, avoid the interference of other objects in background, and has good usability and robustness. Besides it enriches manipulation method of Human Computer Interaction by controlling entertainment games.	algorithm;cut (graph theory);google map maker;human computer;human–computer interaction;interference (communication);iteration;lumped element model;mathematical optimization;mean shift;mixture model;usability	Ling Zhao;Guocheng An;Fengjun Zhang;Hongan Wang;Guozhong Dai	2011	2011 12th International Conference on Computer-Aided Design and Computer Graphics	10.1109/CAD/Graphics.2011.36	graph cuts in computer vision;computer vision;cut;computer science;graph theory;machine learning;pattern recognition;mathematics	Vision	44.169094614398155	-49.66869732263718	108442
1da36539f8997d31a51f284a98076c6e8f31b03b	visual odometry using the global-appearance of omnidirectional images	visual odometry;robot navigation;global appearance descriptors;visualization robot kinematics navigation estimation databases measurement;robot navigation multi scale analysis global appearance descriptors visual odometry;multi scale analysis	This work presents a purely visual topologic odometry system for robot navigation. Our system is based on a Multi-Scale analysis that allows us to estimate the relative displacement between consecutive omnidirectional images. This analysis uses global appearance techniques to describe the scenes. The visual odometry system also makes use of global appearance descriptors of panoramic images to estimate the phase lag between consecutive images and to detect loop closures. When a previous mapped area is recognized during the navigation, the system re-estimates the pose of the scenes included in the map, reducing the error of the path. The algorithm is validated using our own database captured in an indoor environment under real dynamic conditions. The results demonstrate that our system permits estimating the path followed by the robot with accuracy comparing to the real route.	algorithm;displacement mapping;robotic mapping;visual odometry	Francisco Amorós;Luis Payá;David Valiente;Arturo Gil;Óscar Reinoso	2014	2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO)	10.5220/0005062604830490	computer vision;simulation;computer science;artificial intelligence;visual odometry;odometry;mobile robot navigation	Robotics	51.965630385243976	-38.66049024841019	108496
adf96a361bddb7a89fc17bf01389ce90bb4f54b6	unconstrained iris acquisition and recognition using cots ptz camera	signal image and speech processing;quantum information technology spintronics	Uniqueness of iris patterns among individuals has resulted in the ubiquity of iris recognition systems in virtual and physical spaces, at high security facilities around the globe. Traditional methods of acquiring iris patterns in commercial systems scan the iris when an individual is at a predetermined location in front of the scanner. Most state-of-the-art techniques for unconstrained iris acquisition in literature use expensive custom equipment and are composed of a multicamera setup, which is bulky, expensive, and requires calibration. This paper investigates a method of unconstrained iris acquisition and recognition using a single commercial off-the-shelf (COTS) pan-tilt-zoom (PTZ) camera, that is compact and that reduces the cost of the final system, compared to other proposed hierarchical multicomponent systems. We employ state-of-the-art techniques for face detection and a robust eye detection scheme using active shape models for accurate landmark localization. Additionally, our system alleviates the need for any calibration stage prior to its use. We present results using a database of iris images captured using our system, while operating in an unconstrained acquisition mode at 1.5 m standoff, yielding an iris diameter in the 150−200 pixels range.	algorithm;closed-circuit television;desktop computer;face detection;feature extraction;iris recognition;pan–tilt–zoom camera;pixel;workstation	Shreyas Venugopalan;Marios Savvides	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/938737	computer vision;simulation;computer science;computer graphics (images)	Vision	45.80009605299439	-43.3697205625252	108503
5e4db1a851d1a555829bbd5561a1b6175538f822	adm-hipar: an efficient background subtraction approach		This paper presents a novel background subtraction method that is flexible for various background scenarios. The method includes automated-directional masking (ADM) algorithm for adaptive background modeling and historical intensity pattern reference (HIPaR) algorithm for foreground segmentation. By selecting an appropriate mask in a set based on directional feature, ADM updates background smoothly and precisely following a boundary-based strategy with an intensity correction rule. In order to segment foreground, HIPaR refers intensity patterns of previous backgrounds and input frames and then compares their mean difference with a checking threshold to make foreground decision. Experimental results prove that our proposed ADM-HIPaR outperforms other state-of-the-art methods in terms of foreground detection accuracy.	algorithm;background subtraction;benchmark (computing);big data;smoothing	Thien Huynh-The;Sungyoung Lee;Cam-Hao Hua	2017	2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2017.8078536	computer vision;feature extraction;artificial intelligence;foreground detection;mean difference;pattern recognition;masking (art);computer science;background subtraction	Robotics	43.17330643770307	-49.608246709683826	108680
5c86513d43b8b9b0931356228960642bfb0a62ba	assisting minimally invasive surgery through exterior orientation to enhance perception	motion analysis;motor skills;image motion analysis;minimal invasive surgery;surgery image motion analysis image sequences medical image processing;body movements;sensory motor skills;human machine interface;medical image processing;surgery;minimally invasive surgery;image sequences minimally invasive surgery sensory motor skills body movements human machine interface motion analysis;minimally invasive surgery surges layout motion estimation instruments cameras mediation motion analysis robotics and automation geometry;exterior orientation;image sequences	A diversity of action-perception applications relies on 2D information to perform 3D tasks. Minimally invasive surgery (MIS) is one of these applications. It requires a high degree of sensory-motor skills to overcome the disengagement between action and perception caused by the physical separation of the surgeon with the operative site. The integration of body movements with visual information serves to assist the surgeon providing a sense of position. Our purpose in this paper is to present the exterior orientation as a tool in assisted interventions locating the instruments with respect to the surgeon. An enhanced perception is obtained by augmenting the 2D information imposing position and orientation data through a human-machine interface. Applying motion analysis in a sequence of images and having knowledge of the 3D transformations implemented to the instrument, we show it is possible to estimate its orientation with only two different rotations and also its position in the case its length information is supplied.	angularjs;camera resectioning;feature model;initial condition;machine perception;minimally invasive education;optical flow;pose (computer vision);robot;user interface	Agustin A. Navarro;Joan Aranda	2007	Proceedings 2007 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2007.363864	human–machine interface;computer vision;simulation;motor skill;engineering	Robotics	48.89321299754255	-40.69848141683647	108691
4daf6a0f181355058fecf7667185b0e34831ad30	direct estimation of multiple disparities for transparent multiple surfaces in binocular stereo	image matching;disparity estimation;stereo vision computer vision humans layout quantum computing image motion analysis laboratories information processing telecommunication computing signal processing;motion estimation;first order;stereo image processing;signal matching direct estimation multiple disparities transparent multiple surfaces binocular stereo closed form single shot stereo disparity estimation algorithm epipolar lines binocular image pair transparent stereo constraint equations mathematical technique principle of superposition computationally tractable single shot algorithm first order approximation motion transparency motion estimates sum of squared differences;sum of squared difference;image matching stereo image processing motion estimation	A closed-form single-shot stereo disparity estimation algorithm is proposed that can compute multiple disparities due to transparency directly from signal differences and variations on epipolar lines of a binocular image pair. The transparent stereo constraint equations have been derived by using a novel mathematical technique, the principle of superposition. A computationally tractable single-shot algorithm is derived by using the first-order approximation of the constraint equations with respect to disparities. The algorithm can compute multiple disparities from only two images, in contrast to the previous algorithms for motion transparency, which needed at least n+1 frames for n simultaneous motion estimates. The derived algorithm can be viewed as the SSD (sum of squared differences) for signal matching extended to deal with multiple disparities. However, the constraints are not dedicated solely to the SSD method and several other implementations are possible. >	binocular vision;stereoscopy	Masahiko Shizawa	1993		10.1109/ICCV.1993.378182	computer stereo vision;computer vision;mathematical optimization;computer science;motion estimation;first-order logic;mathematics	Vision	52.90994813277316	-51.03150602137801	108739
efcf285eae1e959b11097d8e80f2416314da18fd	introduction to coherent depth fields for dense monocular surface recovery			coherent	Vladislav Golyanik;Torben Fetzer;Didier Stricker	2017			computer vision;computer science;artificial intelligence;monocular	Robotics	52.214826747941046	-43.7535514798683	108782
5c5d1a496af377a7ed8c61da88cab308aab8c304	reduced epipolar cost for accelerated incremental sfm	iterative refinement;algebraic cost reduction;convergence;scene modelling;nonlinear programming;real time tracking;data normalization;barium;image database;large image databases epipolar cost reduction accelerated incremental structure from motion algebraic cost reduction iterative refinement multiple view 3d reconstruction data normalization true geometric epipolar distance geometric reprojection error final standard bundle adjustment refinement measurement matrix reduction nonlinear optimization memory usage reduction convergence improvements real time tracking applications scene modelling;convergence improvements;final standard bundle adjustment refinement;matrix algebra;multiple views;multiple view 3d reconstruction;large image databases;iterative methods;three dimensional displays;image reconstruction;epipolar cost reduction;accelerated incremental structure from motion;optimization;cameras jacobian matrices three dimensional displays convergence barium optimization sparse matrices;real time tracking applications;visual databases image reconstruction iterative methods matrix algebra nonlinear programming;measurement matrix reduction;memory usage reduction;nonlinear optimization;jacobian matrices;sparse matrices;3d reconstruction;cameras;true geometric epipolar distance;geometric reprojection error;visual databases	We propose a reduced algebraic cost based on pairwise epipolar constraints for the iterative refinement of a multiple view 3D reconstruction. The aim is to accelerate the intermediate steps required when incrementally building a reconstruction from scratch. Though the proposed error is algebraic, careful input data normalization makes it a good approximation to the true geometric epipolar distance. Its minimization is significantly faster and obtains a geometric reprojection error very close to the optimum value, requiring very few iterations of final standard BA refinement. Smart usage of a reduced measurement matrix for each pair of views allows elimination of the variables corresponding to the 3D points prior to nonlinear optimization, subsequently reducing computation, memory usage, and considerably accelerating convergence. This approach has been tested in a wide range of real and synthetic problems, consistently obtaining significant robustness and convergence improvements even when starting from rough initial solutions. Its efficiency and scalability make it thus an ideal choice for incremental SfM in real-time tracking applications or scene modelling from large image databases.	3d reconstruction;approximation;business architecture;computation;database;epipolar geometry;geometric median;iteration;iterative refinement;loss function;map projection;mathematical optimization;nonlinear programming;nonlinear system;real-time clock;real-time computing;real-time locating system;refinement (computing);reprojection error;scalability;serial digital video out;synthetic intelligence;virtual reality headset	Antonio L. Rodríguez;Pedro E. López-de-Teruel;Alberto Ruiz	2011	CVPR 2011	10.1109/CVPR.2011.5995569	3d reconstruction;iterative reconstruction;computer vision;mathematical optimization;convergence;sparse matrix;nonlinear programming;computer science;theoretical computer science;database normalization;machine learning;mathematics;geometry;iterative method;barium	Vision	52.24366675690187	-48.83258670068268	108794
27b0daba1beeb7d03e385267d32501178c1f097a	3d auto-calibration method for head-mounted binocular gaze tracker as human-robot interface	calibration planes;image representation;calibration;head-mounted binocular;forward-looking camera;calibration plane;attentive object;mapping point;eye-monitoring camera;user interfaces;attentive point;human-robot interaction;target tracking;3d autocalibration method;computerised monitoring;human-robot interface;previous method;3d space;mapping points;auto-calibration method;selective visual attention;object tracking;binocular gaze;cameras;human gaze point;3d position;visual marks;head-mounted binocular gaze tracker;novel calibration method;convenient calibration procedure;3d spatial domain;target object;gaze tracker;robot vision;human robot interaction;human robot interface;visualization;mobile robots	This paper presents a novel calibration method for a head-mounted binocular gaze tracker that enables the human gaze point, representing the selective visual attention of the user, to be tracked in 3D space. The proposed method utilizes two calibration planes with visual marks to calculate the mapping points between a forward-looking camera and two eye-monitoring cameras in an expanded 3D spatial domain. As a result, the visually attentive point of the user can be tracked, regardless of variations in the distance from the user to the target object. The proposed method also provides a more convenient calibration procedure and more accurate results in tests than the previous method suggested by the authors. The performance is tested when varying the 3D position of an attentive object, and the experimental results are discussed.	binocular vision	Su Hyun Kwon;Min Young Kim	2013	2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)		human–robot interaction;computer vision;calibration;simulation;computer science;artificial intelligence;video tracking;user interface;computer graphics (images)	Robotics	48.9524993679402	-41.85439468455865	108933
048fd541488610edc0f4c7bdb896f0bf696e581c	jacobian images of super-resolved texture maps for model-based motion estimation and tracking	moving object;super resolved estimate;projective surface tracker;planar surface patches;image resolution;time measurement;texture mapping;kalman filters;bayesian methods;kalman filter;motion estimation;layout;image texture;uncalibrated camera scenarios;jacobian matrices image resolution motion estimation tracking layout bayesian methods time measurement robots cameras streaming media;streaming media;robots;performance model;3d tracking;mosaicking kalman filter motion estimation tracking texture mapping super resolved estimate jacobian images 3d tracking planar surface patches projective surface tracker uncalibrated camera scenarios;jacobian images;extended kalman filter;jacobian matrices;mosaicking;cameras;image texture motion estimation kalman filters;tracking	We present a Kalman filter based approach to perform model-based motion estimation and tracking. Unlike previous approaches, the tracking process is not formulated as an SSD minimization problem, but is developed by using texture mapping as the measurement model in an extended Kalman filter. During tracking, a super-resolved estimate of the texture present on the object or in the scene is obtained. A key result is the notion of Jacobian images, which can be viewed as a generalization of traditional gradient images, and represent the crucial computation in the tracking process. The approach is illustrated with three sample applications: full 3D tracking of planar surface patches, a projective surface tracker for uncalibrated camera scenar ios, and a fast, Kalman filtered version of mosaicking with detection of independently moving objects.	apply;computation;extended kalman filter;gradient;hardware acceleration;image stitching;jacobian matrix and determinant;map;motion estimation;solid-state drive;super-resolution imaging;texture mapping;ios	Frank Dellaert;Sebastian Thrun;Charles E. Thorpe	1998		10.1109/ACV.1998.732850	kalman filter;computer vision;simulation;computer science;moving horizon estimation;computer graphics (images)	Robotics	51.43487220216529	-49.04947915160122	108975
05e156b2ebebd07b94b14bb0bd8afec377bbc82f	gpu-based multilayer invariant ekf for camera localization		Abstract This paper proposes a GPU-based camera localization approach using an RGBD camera and an inertial sensor. Data from the inertial sensor is input into an Iterative Closest Point (ICP) algorithm as initial information for obtaining the camera localization. German-McClure level-set function is set as the energy function of ICP for the sake of the complicated indoor environment. The covariance of ICP is generated by using a Fisher Information Matrix, and then the localization error of ICP can be quantized based on the covariance. This quantization approach is also extended to a multilayer ICP which operates a coarse to fine framework. An Invariant Extended Kalman Filter (IEKF) approach is employed to fuse the two poses which are obtained from inertial sensor and multilayer ICP respectively to reduce the localization error. The crucial steps of IEKF are implemented by GPU for fast computing massive vertexes. The Experiment results show that the proposed approach can track camera more accurately and perform better real-time performance.	extended kalman filter	Weijie Huang;Guoshan Zhang	2018	Computers & Graphics	10.1016/j.cag.2018.08.002	fisher information;computer science;computer vision;mathematical optimization;invariant extended kalman filter;artificial intelligence;inertial frame of reference;quantization (signal processing);invariant (mathematics);extended kalman filter;iterative closest point;covariance	Vision	53.39738782806612	-43.194488397247696	109163
66776be193a900f4208612589d506834aeaebfd2	real-time vehicle detection using equi-height mosaicking image	real time;vehicle detection;gpu;equi height;system;svm;mosaicking image	In this paper, we present a real-time forward vehicle detection warning system using a novel image representation called an equi-height mosaicking image. The proposed system uses a GPU (graphic processing unit) based approach for the real-time processing of a road scene image captured from a single camera. The equi-height mosaicking image improves the execution time of the existing GPU-based acceleration approach without decreasing the detection accuracy. The equi-height image is generated as follows. After a geometric analysis of a road scene using the vanishing point and horizon, we crop a set of image strips by sampling several positions on the road at uniform intervals. The height of each image strip is computed by projecting the predefined height of a vehicle at a distant position onto an image plane. After all the cropped images are resized to the uniform height required to build the equi-height image, we concatenate these resized images, similar to a panorama image, to create the equi-height mosaicking image. The concatenated image has a long width but the height of the image is uniform. The proposed system then performs a GPU-based vehicle detection on the concatenated image using a 1D search based support vector machine (SVM) classification. The proposed method is faster than the GPU-based OpenCV HOG detector because of the reduced search area.	concatenation;geometric analysis;graphics processing unit;image plane;image stitching;opencv;real-time clock;run time (program lifecycle phase);strips;sampling (signal processing);support vector machine;vanishing point	Min Woo Park;Jung Pil Park;Soon Ki Jung	2013		10.1145/2513228.2513288	image texture;computer vision;feature detection;simulation;geography;image processing;digital image processing;computer graphics (images)	Robotics	42.577067174402266	-43.41470839354915	109185
38ea967865d54e83f732e484e52c4cae0863faf3	reliable automatic calibration of a marker-based position tracking system	tracking system;nonlinear optimization reliable automatic calibration marker based position tracking system vision based position tracking system machine learning structure from motion solver;computer vision;machine learning;learning artificial intelligence;nonlinear optimization;calibration cameras robustness layout augmented reality target tracking detectors reliability engineering cost function conferences;structure from motion;calibration;learning artificial intelligence calibration cameras computer vision;cameras;bundle adjustment	This paper describes an accurate vision-based position tracking system which is significantly more robust and reliable over a wide range of environments than existing approaches. Based on fiducial detection for robustness, we show how a machine-learning approach allows the development of significantly more reliable fiducial detection than has previously been demonstrated. We calibrate fiducial positions using a structure-from-motion solver. We then show how nonlinear optimization of the camera position during tracking gives accuracy comparable with full bundle adjustment but at significantly reduced cost.	algorithm;augmented reality;bundle adjustment;cinema 4d;focal (programming language);fiducial marker;glossary of computer graphics;machine learning;match moving;mathematical optimization;nonlinear programming;nonlinear system;reduced cost;solver;structure from motion;tracking system	David Claus;Andrew W. Fitzgibbon	2005	2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05) - Volume 1	10.1109/ACVMOT.2005.101	computer vision;structure from motion;calibration;simulation;tracking system;nonlinear programming;computer science;machine learning;bundle adjustment	Vision	50.81405056275914	-46.28191843070772	109236
48ba361561da0c230a4095e2eeebac64987d6227	vehicle model based visual-tag monocular orb-slam		Monocular ORB-SLAM has been proved to be one of the best open-source SLAM method. However, it is still unsatisfying especially in low illumination indoor environment, which is caused by scale recovery and wrong feature matching. In this paper, we proposed a vehicle model based monocular ORBSLAM method supplemented by April-Tag to improve the performance of original algorithm. This approach is practical when autonomous driving in low-light and less-feature environment like garages and tunnels. We achieve this by proposing a vehicle model based initialization method fusing April-Tag measurement to recover scale. During tracking procedure, the outliers ORB feature points will be removed by checking reprojection error calculated from April-Tag. In addition, considering vehicle model can only obtain 2D motion, the vertical transition is estimated from camera model. Afterwards, a local Bundle Adjustment(BA) is applied to optimize camera pose both from frame to frame and frame to keyframe which will reduce accumulative error of the vehicle model. Finally, a convincing result is obtained from the testing drive in a garage.	algorithm;autonomous car;key frame;open-source software;reprojection error;simultaneous localization and mapping;virtual reality headset	Wenhao Zong;Longquan Chen;Changzhu Zhang;Zhuping Wang;Qijun Chen	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122816	computer vision;orb (optics);machine learning;visualization;artificial intelligence;computer science;simultaneous localization and mapping;initialization;outlier;bundle adjustment;monocular	Robotics	51.09257318852207	-45.898519440370485	109270
db0c81eb0f9708556eb38e1763d429caeacb3f65	computationally efficient path-following using adaptive color models	color model	The ability to follow man-made paths and roads is an important capability for a number of robotic tasks. To operate in outdoor environments designed for humans, autonomous robots must identify footpaths, and drive along them. In this paper, we describe a computationally efficient approach to identifying and following footpaths, using only a single camera. Our technique takes the robot’s kinematics into consideration when planning the best trajectory to follow the path that it is on. We show that our approach is highly robust to visual artifacts such as shadows, lighting changes, ground texture changes, and occlusions.	algorithmic efficiency;autonomous robot;humans;visual artifact	Stuart Glaser;Eitan Marder-Eppstein;William D. Smart	2008			optical imaging;situated;perpendicular;wedge (mechanical device);color model;optical axis;detector;computer vision;optics;artificial intelligence;physics	Robotics	51.38940888364575	-40.210745845486386	109311
22248649028fb1f15f09d5b95e4d8602bcdba754	learning a dictionary of shape epitomes with applications to image labeling	biological patents;biomedical journals;shape dictionaries labeling image segmentation image edge detection adaptation models training;text mining;europe pubmed central;edge detection;citation search;image patch shape shape epitomes dictionary image labeling edge detection image modeling conditional random field crf models;citation networks;statistical analysis;research articles;abstracts;image representation;open access;life sciences;clinical guidelines;statistical analysis edge detection image representation;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	The first main contribution of this paper is a novel method for representing images based on a dictionary of shape epitomes. These shape epitomes represent the local edge structure of the image and include hidden variables to encode shift and rotations. They are learnt in an unsupervised manner from ground truth edges. This dictionary is compact but is also able to capture the typical shapes of edges in natural images. In this paper, we illustrate the shape epitomes by applying them to the image labeling task. In other work, described in the supplementary material, we apply them to edge detection and image modeling. We apply shape epitomes to image labeling by using Conditional Random Field (CRF) Models. They are alternatives to the super pixel or pixel representations used in most CRFs. In our approach, the shape of an image patch is encoded by a shape epitome from the dictionary. Unlike the super pixel representation, our method avoids making early decisions which cannot be reversed. Our resulting hierarchical CRFs efficiently capture both local and global class co-occurrence properties. We demonstrate its quantitative and qualitative properties of our approach with image labeling experiments on two standard datasets: MSRC-21 and Stanford Background.	conditional random field;dictionary [publication type];encode;edge detection;electronic supplementary materials;experiment;ground truth;hidden variable theory;pixel;unsupervised learning	Liang-Chieh Chen;George Papandreou;Alan L. Yuille	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.49	computer vision;text mining;edge detection;computer science;machine learning;pattern recognition	Vision	44.78047176235875	-51.38446489968605	109483
ad866cc74ceba13ef7fb8de563f327eb1c881a2e	fast lifting for 3d hand pose estimation in ar/vr applications		We introduce a simple model for the human hand skeleton that is geared toward estimating 3D hand poses from 2D keypoints. The estimation problem arises in AR/VR scenarios where low-cost cameras are used to generate 2D views through which rich interactions with the world are desired. Starting with a noisy set of 2D hand keypoints (camera-plane coordinates of detected joints of the hand), the proposed algorithm generates 3D keypoints that are (i) compliant with human hand skeleton constraints and (ii) perspective-project down to the given 2D keypoints. Our work considers the 2D to 3D lifting problem algebraically, identifies the parts of the hand that can be lifted accurately, points out the parts that may lead to ambiguities, and proposes remedies for ambiguous cases. Most importantly, we show that the finger-tip localization errors are a good proxy for the errors at other finger joints. This observation leads to a look-up-table-based formulation that instantaneously determines finger poses without solving constrained trigonometric problems. The result is a fast algorithm running super real-time on a single core. When hand bone-lengths are unknown our technique estimates these and allows smooth AR/VR sessions where a user's hand is automatically estimated in the beginning and the rest of the session seamlessly continued. Our work provides accurate 3D results that are competitive with the state-of-the-art without requiring any 3D training data.		Onur G. Guleryuz;Christine Kaeser-Chen	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451559	computer vision;trigonometry;single-core;thumb;2d to 3d conversion;artificial intelligence;pose;pattern recognition;training set;computer science;hand skeleton	Robotics	50.61593032748958	-45.264989529121095	109604
733190db89fd81407812be0bc08d2f21ad644e1d	structure restriction for tracking through multiple views and occlusions	three dimensional;multiple views;state space;three dimensional structure	The last advances on multiple kernel tracking consider the kernels as estimators of target features. The state space of the target is defined by the individual state space of these features.#R##N##R##N#The aim of this work is to construct an algorithm robust against three dimensional rotations and partial occlusions. For this purpose, we take as the state space the two dimensional position of the features and an indicator of occlusions. We extract the three dimensional structure of the target from the first tracked frames and estimate the projection of this structure on each frame. By using this information, we are able to predict the position of a feature even when the kernel provides a wrong estimation, for example during an occlusion. The experimental results showed a good performance correcting errors and in presence of partial occlusions.		Brais Martínez;Alberto Márquez Pérez;Luis Ferraz;Xavier Binefa	2007		10.1007/978-3-540-72847-4_17	three-dimensional space;computer vision;mathematical optimization;computer science;state space;mathematics;geometry	Vision	46.70641700949301	-47.22858074532421	109644
45f355900cb66254b226559b956eb6b1e35d49ff	fast object detection for human-robot interaction control	fuzzy control;human robot interaction;distance measurement;robot vision;humanoid robots;infrared imaging;image colour analysis;robotic motions human robot interaction control structural light analysis human gesture detection human gesture recognition human pose detection human pose recognition rgb camera infrared light module distance estimation image perception objective skeleton detection laser source invisible infrared light filter reflected pattern detection infrared camera depth estimation human pose estimation fuzzy decision making system;joints robot kinematics legged locomotion three dimensional displays robot sensing systems;gesture recognition;robot vision cameras decision making distance measurement filtering theory fuzzy control gesture recognition humanoid robots human robot interaction image colour analysis infrared imaging object detection pose estimation;filtering theory;cameras;object detection;pose estimation	This paper proposes a fast object detection algorithm based on structural light analysis, which is used to detect and recognize human gesture and pose and then to conclude the respective commands for human-robot interaction control. The RGB camera and the infrared light module aim to do distance estimation of a body or several bodies. The module not only provides image perception but also objective skeleton detection. In which, a laser source in the infrared light module emits invisible infrared light which passes through a filter and is scattered into a semi-random but constant pattern of small dots which is projected onto the environment in front of the sensor. The reflected pattern is then detected by an infrared camera and analyzed for depth estimation. Since the depth of object is a key parameter for pose recognition, one can estimate the distance to each dot and then get depth information by calculation of distance between emitter and receiver. In this paper, the human poses are estimated and analyzed by the proposed scheme, and then the resultant data concluded by the fuzzy decision making system are used to launch respective robotic motions. The experimental results demonstrate the feasibility of the proposed system.	algorithm;control system;human–robot interaction;kinesiology;lambert's cosine law;object detection;remote control;resultant;robot;semiconductor industry;structured light	M. Y. Shieh;Y. H. Chen;J. H. Li;N. S. Pai;J. S. Chiou	2013	Proceedings of the 2013 IEEE/SICE International Symposium on System Integration	10.1109/SII.2013.6776622	computer vision;simulation;pose;3d pose estimation;engineering;communication	Robotics	48.810973919251886	-41.26028768944507	109655
07d08fb177230faa93e6040c12ff368e65492431	robust automatic target recognition in second generation flir images	image recognition;parts;radar tracking;flir;computer vision target tracking radar tracking image recognition infrared imaging;robustness target recognition image generation image recognition bayesian methods computer vision object detection weapons layout image sensors;segmentation;diffusion based approach robust automatic target recognition second generation flir images target detection forward looking infrared images;part identification;computer vision;bayesian;target;recognition;infrared imaging;target recognition;automatic target recognition;target tracking;infrared;target detection;diffusion	In this paper we present a system for the detection and recognition of targets in second generation forward looking infrared (FLIR) images. The system uses new algorithms for target detection and segmentation of the targets. Recognition is based on a methodology far target recognition by parts. A diffusion based approach for determining the parts of a target is also presented here. Experimental results on a large database of FLIR images validate the robustness of the system, and its applicability to FLIR imagery obtained from real scenes.	automatic target recognition	Dinesh Nair;Jake K. Aggarwal	1996		10.1109/ACV.1996.572055	computer vision;radar tracker;infrared;bayesian probability;computer science;pattern recognition;diffusion;segmentation;automatic target recognition	Vision	46.505426552315996	-49.87839598763734	109682
568e47b27731af6bcf843c49c049b487f37f8ea1	facial animation based on 2d shape regression		We present a facial animation system for ordinary single-cameral videos based on 2D shape regression. Unlike some prior facial animation techniques, our system doesn’t need complex equipment. The system consists of firstly a Cascade Multi-Channel Convolutional Neural Network (CMC-CNN) model to accurately detect facial landmarks from 2D video frames. Based on these detected 2D points, the facial motion parameters, including the head pose and facial expressions, are recovered. Then the system animates a bone-driven 3D avatar with the facial motion parameters. Experiments show that our system can accurately detect facial landmarks and the animation results are visually plausible and similar to the user’s facial motion.		Ruibin Bai;Qiqi Hou;Jinjun Wang;Yihong Gong	2016		10.1007/978-3-319-48896-7_4	artificial intelligence;convolutional neural network;computer vision;computer facial animation;computer science;pattern recognition;animation;video tracking;facial expression	Vision	46.934262825771974	-49.12859694650111	109735
dbb7fd3b3e54ed3e937d4212497cc67753a40e89	hierarchical fast mean-shift segmentation in depth images		Head position and head pose detection systems are very popular in recent times, especially with the rise of depth cameras like Microsoft Kinect and Intel RealSense. The goal is to recognize and segment a head in depth data. The systems could also detect the direction in which the head is pointing and we use these data to improve the gaze direction detection system and provide useful information to allow detectors to work properly. We present the Hierarchical Fast Blurring Mean Shift algorithm that is able to extract the data from depth images in real-time from above mentioned cameras. We also present some modifications for an effective reduction of the mean-shift dataset during the computation that allow us to increase the precision of the method. We use a hierarchical approach to reduce the dataset during the computation process and to improve the speed.		Milan Surkala;Radovan Fusek;Michael Holusa;Eduard Sojka	2016		10.1007/978-3-319-48680-2_39	segmentation-based object categorization;image segmentation;scale-space segmentation	Vision	41.36068557568889	-43.7882746734334	109929
e951bdf51210715867dcf15b6194347ce87c398b	suitability of real-time image under complicated environment based on contourlet in smn	suitability of real time image;scene matching navigation;contourlet;generalized gaussian distribution	Judging whether the real-time image under complicated environment is suitable is a challenging problem in scene matching navigation, which contributes to ensure the navigation precision and decrease computational complexity. This paper proposes a novel method for analyzing the suitability of real-time image under complicated environment based on Contourlet by taking advantage of the characteristic of multi-direction and multi-scale of Contourlet, where the complicated environment focus on motion blur, illumination variation, occlusion of cloud and fog. Firstly, real-time image is transformed on 4-layer Contourlet, and the obtained coefficients are parameterized by Generalized Gaussian Distribution, forming a 62 - dimension feature vector. Then the relationship between the feature vector and the objective evaluation index of suitability is trained by support vector machine, to build the prediction model of suitability of real-time image under complicated environment. Finally, experiments are performed on image database picked from Google Earth. The experiments clearly demonstrate that the proposed algorithm is simple but effective for real-time image quality assessment in scene matching navigation.	contourlet;real-time transcription;smn theorem	Lu Yu;Yongmei Cheng;Xialei Liu;Nan Liu	2015		10.1109/ISKE.2015.11	computer vision;contourlet;simulation;geography;computer graphics (images)	Robotics	42.59579272207116	-43.906684590074654	110511
f4cf4875cb89f57033b2c2e5524c1ddcd7e7be94	video mosaic with block matching and m-estimation	minimisation;video mosaic;levenberg marquardt;estimation theory;image segmentation;video signal processing;estimation method;image matching;m estimation video mosaic block matching phase correlation;iterative methods;levenberg marquardt iterative nonlinear minimization algorithm video mosaic block matching m estimation panoramic image video sequence individual overlapping video frames phase correlation alignment parameters;cameras layout video compression robustness video sequences iterative methods iterative algorithms parameter estimation transmission line matrix methods computer science;block matching;video signal processing estimation theory image matching image segmentation image sequences iterative methods minimisation;panoramic image;m estimation;phase correlation;image sequences	Video mosaic is to make a wide or a panoramic image from video sequence by assembling the individual overlapping video frames. In this paper a technique is presented for video mosaic, which employs block matching based on phase correlation and robust M-estimation method. Initial point correspondences between each pair of consecutive frames are computed by block matching method based on phase correlation, then M-estimation method is used to identify and reject outliers and the alignment parameters between the consecutive frames is subsequently estimated by Levenberg-Marquardt iterative nonlinear minimization algorithm. Some mosaic examples are constructed to testify the availability of this approach.	iterative method;levenberg–marquardt algorithm;motion estimation;ncsa mosaic;nonlinear system;phase correlation	Zhongxin Li;Wan-he Xu;Yue Zhang;Yaobin Mao;Zhiquan Wang	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.94	computer vision;minimisation;levenberg–marquardt algorithm;computer science;machine learning;pattern recognition;mathematics;iterative method;image segmentation;estimation theory;phase correlation;statistics	Vision	52.14542867106882	-50.64688394488367	110521
1389586cdf4288f916b67602e405c71c0e27394b	model-based face tracking for dense motion field estimation	face recognition image sequences motion estimation kalman filters;video sequence;filtering;dense motion field estimation;image motion analysis;optical noise;kalman filtering approach;optical filters;kalman filtering approach model based face tracking dense motion field estimation video sequence optical flow face tracking face model;kalman filters;facial modeling;kalman filter;motion estimation;video sequences;face tracking;multi stage noise shaping;face recognition;shape;motion vector;face modeling;field data;optical flow;tracking motion estimation video sequences optical filters image motion analysis optical noise kalman filters filtering shape multi stage noise shaping;face model;tracking;model based face tracking;image sequences	When estimating the dense motion field of a video sequence, if little is known or assumed about the content, a limited constraint approach such as optical flow must be used. Since optical flow algorithms generally use a small spatial area in the determination of each motion vector, the resulting motion field can be noisy, particularly if the input video sequence is noisy. If the moving subject is known to be a face, then we may use that constraint to improve the motion field results. This paper describes a method for deriving dense motion field data using a face tracking approach. A face model is manually initialized to fit a face at the beginning of the input sequence. Then a Kalman filtering approach is used to track the face movements and successively fit the face model to the face in each frame. The 2D displacement vectors are calculated from the projection of the facial model, which is allowed to move in 3D space and may have a 3D shape. We have experimented with planar, cylindrical, and Candide face models. The resulting motion field is used in multiple frame restoration of a face in noisy video.	3d modeling;algorithm;circuit restoration;displacement mapping;facial motion capture;kalman filter;machine vision;maximum flow problem;motion estimation;motion field;optical flow	Timothy F. Gee;Russell M. Mersereau	2001		10.1109/AIPR.2001.991218	computer vision;simulation;speech recognition;quarter-pixel motion;computer science;motion estimation;motion field	Vision	50.51865882232523	-48.43363862888025	110616
0a7dfec0ee8c45d587704681830ff2c02c291ed8	development of continuum shape constraint analysis (csca) for computer vision applications using range data	eigenvalues and eigenfunctions;shape constraints;continuum shape constraint analysis;range data;cost function;constraint analysis;registration algorithms;shape computer vision application software cost function laser radar performance analysis algorithm design and analysis instruments iterative closest point algorithm iterative algorithms;registration cost matrix;discrete point based constraint analysis;surface based self registration cost function;computer vision;iterative methods;continuous surface constraint analysis pose estimation continuum surface lidar range data;computational modeling;shape;estimation;noise amplification index;image registration;indexation;continuous surface;iterative closest point algorithm;iterative methods computer vision image registration;registration cost matrix continuum shape constraint analysis computer vision range data discrete point based constraint analysis registration algorithms surface based self registration cost function pose estimation lidar scanning iterative closest point algorithm noise amplification index;lidar scanning;continuum surface;data models;lidar;pose estimation	This paper further presents continuum shape constraint analysis (CSCA) of surfaces. CSCA is a generalization of discrete-point based constraint analysis which can be used to predict performance of registration algorithms. A surface-based self-registration cost function to which constraint analysis can be applied is introduced. This cost function takes into account a direction the object is viewed at. A sample study is provided to illustrate this approach applied to the problem of pose estimation using range-data taken from a scanning instrument such as LIDAR. Specifically, CSCA is used to assess an object feature for suitability for local LIDAR scanning and subsequent application of the ICP (iterative closest-point) algorithm to determine pose. In this study, the constraint analysis uses noise amplification index (NAI) as an output measure. The continuum nature of the CSCA approach renders the registration cost matrix and the derived NAI as pure shape properties of the feature with a dependence on viewpoint.	algorithm;apache continuum;computer vision;iterative closest point;iterative method;loss function;rendering (computer graphics);triune continuum paradigm	Galina Okouneva;D. J. McTavish;M. Gillespie;J. Enright	2008	2008 Canadian Conference on Computer and Robot Vision	10.1109/CRV.2008.44	lidar;data modeling;computer vision;mathematical optimization;estimation;pose;shape;computer science;image registration;machine learning;mathematics;iterative method;computational model	Vision	53.090301224971064	-51.5844583863404	110692
8b977431ddbff04d63d25f38ab759885cb306533	monocular reconstruction of vehicles: combining slam with shape priors	slam;three dimensional displays shape solid modeling vehicles deformable models image reconstruction pipelines;car pose recognition;structure from motion algorithm monocular reconstruction vehicles slam shape priors 3d representation computer vision scene understanding roads 3d vehicle detection vehicle tracking monocular video diverse vehicle skeletal structure multiple view setting image detection;scene understanding;video signal processing computer vision image motion analysis image reconstruction image representation object detection road traffic road vehicles slam robots solid modelling traffic engineering computing;3d reconstruction	Reasoning about objects in images and videos using 3D representations is re-emerging as a popular paradigm in computer vision. Specifically, in the context of scene understanding for roads, 3D vehicle detection and tracking from monocular videos still needs a lot of attention to enable practical applications. Current approaches leverage two kinds of information to deal with the vehicle detection and tracking problem: (1) 3D representations (eg. wireframe models or voxel based or CAD models) for diverse vehicle skeletal structures learnt from data, and (2) classifiers trained to detect vehicles or vehicle parts in single images built on top of a basic feature extraction step. In this paper, we propose to extend current approaches in two ways. First, we extend detection to a multiple view setting. We show that leveraging information given by feature or part detectors in multiple images can lead to more accurate detection results than single image detection. Secondly, we show that given multiple images of a vehicle, we can also leverage 3D information from the scene generated using a unique structure from motion algorithm. This helps us localize the vehicle in 3D, and constrain the parameters of optimization for fitting the 3D model to image data. We show results on the KITTI dataset, and demonstrate superior results compared with recent state-of-the-art methods, with upto 14.64 % improvement in localization error.	algorithm;autostereogram;computer vision;computer-aided design;feature extraction;mathematical optimization;national lidar dataset;programming paradigm;sensor;simultaneous localization and mapping;structure from motion;voxel;wire-frame model	Falak Chhaya;N. Dinesh Reddy;Sarthak Upadhyay;Visesh Chari;M. Zeeshan Zia;K. Madhava Krishna	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487799	3d reconstruction;computer vision;feature detection;simulation;computer science;computer graphics (images)	Robotics	49.97080643285965	-47.3984517921801	110698
66371f63f3dcabd723ddfe9272a753f2246e6601	development of real-time gesture recognition system using visual interaction		The aim of this paper is to present a methodology for hand detection, propose a finger detection method, and finally apply them to posture recognition. The detected hand and finger can be used to implement the non-contact mouse. This technology can be used to control home devices such as curtain and television. Skin color is used to segment the hand region from the background, and counter is extracted from the segmented hand. The counter analysis of gives us the location of fingertip in the hand. Fingertip tracking is performed assuming a constant velocity model and using a pixel labeling approach. From the tracking process, we extract several hand features that are fed to a finite state classifier that identifies the hand configuration. The hand can be classified into many gesture classes or several different movement directions. This method of skin segmentation assumes that the background does not contain any skin colored object beside hands. We have performed an extensive experiment and achieved a very encouraging result. Ultimately, this paper suggests an empirical application to verify the adequacy and validity of the proposed systems. Accordingly, the satisfaction and quality of services will improved gesture recognition.	gesture recognition;real-time transcription	Sung-Kwan Kang;Kyung-Yong Chung;Kee-Wook Rim;Jung-Hyun Lee	2011		10.1007/978-94-007-2911-7_25	gesture recognition;sketch recognition	HCI	46.09022257397218	-42.209367798926614	110895
3d4feeb390b040408377f49681f584189de0ab86	a new approach on spatio-temporal scene analysis for driver observation	graph theory;digital image processing;image processing;advanced driver assistance system;automotive industry;spatio temporal models;information fusion;scene analysis	Advanced Driver Assistance Systems are, due to their potentials regarding security and markets, in the focus of future developments within the automotive industry. The visual observation of the car interior is gaining attention due to the increasing efficiency of methods and technologies in digital image processing. Special weight is put on the visual driver observation, which measures diversion and fatigue of the driver and notifies about endangering behavior. This is accomplished by utilizing complex image-processing systems. The spatial positions and orientations of head and eyes are measured and evaluated. This report presents in detail and coherently the motivation and the current status of available approaches and systems. Following, a new concept for spatio-temporal modeling and tracking of partially rigid objects is developed and described. This concept is based on methods for spatio-temporal scene analysis, graph theory, adaptive information fusion and multi-hypothesis tracking. Our original contributions are the detailed representation of the available procedures and systems in this certain field and the development of a new concept and re-	curve fitting;digital image processing;graph theory;scene graph;stereopsis	Markus Steffens;Dominik Aufderheide;Stephan Kieneke;Werner Krybus;Christine Kohring;Danny Morton	2009		10.1007/978-3-642-02611-9_63	computer vision;simulation;image processing;computer science;automotive industry;graph theory;digital image processing;multimedia	Graphics	49.399646539174135	-39.93481709687171	111145
2f8b3e97fc07fe5055153cf3e49d073176dc4e28	simultaneous estimation of vanishing points and their converging lines using the em algorithm	non linear optimisation;vanishing points;projective plane;vanishing point;mixture model;mixture models;em algorithm;expectation maximisation	This paper introduces a new method for the simultaneous computation of sets of lines meeting at multiple vanishing points through the use of the Expectation-Maximisation (EM) algorithm. The proposed method is based on the formulation of novel error functions in the projective plane between lines and points which involve the use of non-linear optimisation procedures that allow to treat equally finite and infinite vanishing points. These functions are included into the EM framework, which handles the multi-modality of the problem naturally with mixture models. Results show that the proposed method of joint computation of vanishing points and lines converging at such points enhances the robustness and the accuracy of locating these points.	expectation–maximization algorithm	Marcos Nieto;Luis Salgado	2011	Pattern Recognition Letters	10.1016/j.patrec.2011.07.018	mathematical optimization;combinatorics;vanishing point;mixture model;mathematics;geometry	Vision	51.580176124849	-51.31730078574742	111284
26ac7ab7d4eb0f54606317f6a49bc15a1754e243	modeling object classes in aerial images using hidden markov models	unlabeled object identification object classes modelling aerial images hidden markov models hmm texture motifs geographic regions of interest geographic processes motifs spatial arrangement state transitions one dimensional approach computational complexity reduction spatial datasets;canonical model;hidden markov model;computational complexity image texture hidden markov models geography;image texture;aerial image;hidden markov models;hidden markov models image texture analysis graphics image retrieval statistical analysis airports computational complexity image analysis tree graphs parameter estimation;computational complexity;region of interest;state transition;geography	We propose a canonical model for object classes in aerial images. This model is motivated by the observation that geographic regions of interest are characterized by collections of texture motifs corresponding to geographic processes. Furthermore, the spatial arrangement of the motifs is an important discriminating characteristic. In our approach, the states of a Hidden Markov Model (HMM) correspond to the geographic processes and the state transitions correspond to the spatial arrangement of the processes. A novel onedimensional approach reduces the computational complexity. We show that the model is effective in characterizing objects of interest in spatial datasets, in terms of their underlying texture motifs. We also demonstrate the potential of the model for identifying the classes of unknown objects.	aerial photography;canonical model;computational complexity theory;hidden markov model;markov chain;region of interest	Shawn D. Newsam;Sitaram Bhagavathy;B. S. Manjunath	2002		10.1109/ICIP.2002.1038161	image texture;computer vision;computer science;machine learning;pattern recognition;canonical model;mathematics;computational complexity theory;hidden markov model;region of interest	Vision	45.1131752082467	-51.71313972524229	111387
2a2885028e36cb6ac250a3d01f2096c60c2a15bb	activity topology estimation for large networks of cameras	moving object;histograms;network topology cameras surveillance computer science australia computerized monitoring target tracking time measurement training data histograms;time measurement;surveillance;learning activities;network topology;training data;computerized monitoring;field of view;computer science;target tracking;cameras;australia	Estimating the paths that moving objects can take through the fields of view of possibly non-overlapping cameras, also known as their activity topology, is an important step in the effective interpretation of surveillance video. Existing approaches to this problem involve tracking moving objects within cameras, and then attempting to link tracks across views. In contrast we propose an approach which begins by assuming all camera views are potentially linked, and successively eliminates camera topologies that are contradicted by observed motion. Over time, the true patterns of motion emerge as those which are not contradicted by the evidence. These patterns may then be used to initialise a finer level search using other approaches if required. This method thus represents an efficient and effective way to learn activity topology for a large network of cameras, particularly with a limited amount of data.	closed-circuit television;synthetic intelligence	Anton van den Hengel;Anthony R. Dick;Rhys Hill	2006	2006 IEEE International Conference on Video and Signal Based Surveillance	10.1109/AVSS.2006.17	computer vision;training set;simulation;field of view;computer science;machine learning;data mining;histogram;network topology;time	Vision	47.20150868954415	-46.43335038761242	111463
204060ce8c3d7a550fc1a2895ad35725b2cf331c	super-resolution for an omnidirectional vision sensor	omnidirectional vision;omnidirctional vision sensor;consecutive images;super resolution		super-resolution imaging	Hajime Nagahara;Yasushi Yagi;Masahiko Yachida	2000	Advanced Robotics	10.1163/156855300741816	computer science;superresolution	Robotics	52.13253370264378	-43.00756289106878	111573
7a9b4b48ad25e16ea7e74e7cb7ddc02e225604df	current speaker detection system using lip motion information	ccd camera;image resolution;videoconference;zoom lenses;zoom;qualite image;resolucion imagen;internet;image transmission;video conferencing;image quality;camara ccd;camera ccd;videoconferencia;calidad imagen;video;ccd cameras;transmission image;resolution image;cameras;transmision imagen	In this paper, we propose a system that detects the current speaker in multi-speaker videoconferencing by using lip motion. First, the system detects the face and lip region of each of the candidate speakers using face color and shape information. Then, to detect the current speaker, it calculates the change between the current frame and the previous frame in lip region. To close-up the detected current speaker, we used two CCD cameras. One is a general CCD camera, the other is a PTZ camera controlled by RS-232C serial port. The experimental result is the proposed system capable of detecting the face of current speaker in a video feed with more than three people, regardless of orientation of the faces. With this system, it only takes 4 to 5 seconds to zoom in on the speaker from the initial reference image. Also, it is a more efficient image transmission system for such things as video conferencing and internet broadcasting because it offers a close up face image at a resolution of 320x240, while at the same time providing a whole background image.	algorithm;charge-coupled device;computer display standard;pan–tilt–zoom camera;pixel;rs-232;real-time computing;sensor;serial port;streaming media;tracking system;video	Heak Bong Kwon;Young-Jun Song;Un-Dong Chang;Jae-Hyeong Ahn	2005		10.1117/12.587578	computer vision;geography;multimedia;computer graphics (images)	Vision	46.668695060931256	-44.4867890590953	111610
31451f2ec32fc603b95805fdf6de65b4930881c7	adaptive segmentation for gymnastic exercises based on change detection over multiresolution combined differences	moving object;mathematical morphology;change detection;image segmentation;biomechanics;motion estimation;area of interest;markov random field;biomechanics adaptive segmentation gymnastic exercises change detection multiresolution combined differences sport sequences markov random fields mrf change detection analysis static image differences dynamic image differences morphological analysis moving objects quasi static background;morphological analysis;image segmentation image resolution motion estimation motion analysis image analysis image motion analysis information analysis biological system modeling lighting image color analysis;markov processes;biomechanics image segmentation image sequences markov processes mathematical morphology motion estimation;image sequences	A new adaptive segmentation strategy is proposed to segment gymnasts in sport sequences accurately. It is based on a Markov random fields (MRF) change detection analysis operating on a multiresolution combination of static and dynamic image differences. After a morphological analysis of the segmented masks, estimated motion information in the area of interest is incorporated to improve the efficiency of the segmentation process. Although presented in the particular context of gymnastic exercises, the new segmentation strategy could be applied to other applications where moving objects on a quasi-static background need to be segmented.	markov chain;markov random field;seven-segment display	María José Aramburu Cabo;Luis Salgado;Julián Cabrera	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1418759	image texture;computer vision;feature detection;simulation;mathematical morphology;morphological analysis;computer science;biomechanics;segmentation-based object categorization;motion estimation;image segmentation;markov process;scale-space segmentation;change detection;statistics;computer graphics (images)	Vision	43.18399418678642	-50.600287851746366	111646
42f4379092f38bdc7802607ab46ed736e2339698	fast object re-detection and localization in video for spatio-temporal fragment creation	image sampling;opencv;paper;video signal processing;image matching;gpu processing object re detection surf descriptors ransac algorithm shot segmentation;surf descriptors;algorithm design and analysis accuracy streaming media robustness real time systems feature extraction acceleration;computer vision;gpu processing;image matching fast object redetection object localization video collection object based spatio temporal fragment creation surf descriptor extraction surf descriptor matching video frames gpu based processing video instance based labeling;feature extraction;graphics processing units;nvidia;algorithms;computer science;video signal processing feature extraction graphics processing units image matching image sampling object detection;object re detection;nvidia geforce gtx 560;shot segmentation;object detection;ransac algorithm	This paper presents a method for the detection and localization of instances of user-specified objects within a video or a collection of videos. The proposed method is based on the extraction and matching of SURF descriptors in video frames and further incorporates a number of improvements so as to enhance both the detection accuracy and the time efficiency of the process. Specifically, (a) GPU-based processing is introduced for specific parts of the object re-detection pipeline, (b) a new video-structure-based sampling technique is employed for limiting the number of frames that need to be processed and (c) improved robustness to scale variations is achieved by generating and employing additional instances of the object of interest based on the one originally provided by the user. The experimental results show that the algorithm achieves high levels of detection accuracy while the overall needed processing time makes the algorithm suitable for quick instance-based labeling of video and the creation of object-based spatio-temporal fragments.	algorithm;emoticon;graphics processing unit;internationalization and localization;object-based language;sampling (signal processing);speeded up robust features	Evlampios E. Apostolidis;Vasileios Mezaris;Yiannis Kompatsiaris	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618349	computer vision;ransac;feature extraction;computer science;theoretical computer science;machine learning;video tracking;computer graphics (images)	Robotics	40.239600390883254	-51.066785001158806	111688
3e00131c12258547a57b15f4b7e86b20fdf2f923	multiple transition mode multiple target track-before-detect with partitioned sampling	target tracking radar tracking proposals psnr partitioning algorithms vectors;markov chain monte carlo metropolis hastings method multiple transition mode multiple target track before detect partitioned sampling low signal to noise ratios probability density function particle filter;probability markov processes monte carlo methods object detection particle filtering numerical methods	In this paper, we extend the multiple model track-before-detect method to track all possible target combinations at low signal-to-noise ratios. Given a maximum number of targets, the method estimates the posterior probability density function of the multitarget state vector, the corresponding target existence probabilities, and the probabilities of all possible target combinations. As the particle filter implementation of this method requires a large number of particles to achieve high tracking performance, we propose an efficient partition based proposal function method by partitioning the multiple target space into a set of single target spaces. We also integrate the Markov chain Monte Carlo Metropolis-Hastings method into the particle proposal process to improve sample diversity. The proposed algorithm is validated by tracking five targets in very low signal-to-noise ratios (SNRs).	markov chain monte carlo;metropolis;metropolis–hastings algorithm;monte carlo method;particle filter;sampling (signal processing);signal-to-noise ratio;track-before-detect	Samuel P. Ebenezer;Antonia Papandreou-Suppappola	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6855160	econometrics;mathematical optimization;hybrid monte carlo;particle filter;markov chain monte carlo;mathematics;rejection sampling;statistics;monte carlo method	Robotics	46.043735508325376	-48.01565822274747	111762
1929e60b67551099166c70d676bff634d9b9dcc3	cell motion analysis without explicit tracking	motion analysis;vector field representation;mice;liver;motion analysis tracking cells biology in vivo motion estimation layout microscopy biomedical imaging roads mice;vivo imagery;microscopy;biomedical imaging;motion estimation;layout;erratic motion patterns;radial flow transform;temporally disjoint motion events;motion estimates;boosting;roads;automated cell tracking;medical image processing;motion estimation cellular biophysics medical image processing;pixel;mouse liver cell motion analysis automated cell tracking vivo imagery erratic motion patterns radial flow transform motion estimates temporally disjoint motion events vector field representation;transforms;synthetic data;vector field;mouse liver;in vivo;cellular biophysics;tracking;cells biology;cell motion analysis	Automated cell tracking using in vivo imagery is difficult, in general, due to the noise inherent in the imaging process, occlusions, varied cell appearance over time, motion of other tissue (distractors), and cells traveling in and out of the image plane. For certain types of cells these problems are exacerbated due to erratic motion patterns. In this paper, we introduce the Radial Flow Transform, which provides motion estimates for objects of interest in a scene without explicitly tracking each object. The transform is robust to misdetected objects, temporally-disjoint motion events, and can represent multiple directions of flow at a single location. We provide operations to convert to and from a vector field representation. This allows for intuitive reasoning about the motion patterns in a scene. We demonstrate results on synthetic data and in vivo microscopy video of a mouse liver.	algorithm;image plane;pattern matching;radial (radio);request for tender;robustness (computer science);synthetic data;video-in video-out	Richard Souvenir;Jerrod P. Kraftchick;Sangho Lee;Mark G. Clemens;Min C. Shin	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587398	layout;computer vision;vector field;simulation;computer science;microscopy;machine learning;motion estimation;tracking;in vivo;motion field;boosting;pixel;synthetic data;computer graphics (images)	Vision	43.606250476633114	-50.840283962771906	111973
2599623866e1affef45c6c562a6a5a4ead1dd3cb	automatic 3d facial modelling with deformable models	theses and dissertations;surface reconstruction;facial expression transferring;template fitting;facial expression tracking;registration;texture reconstruction;range scans;deformable model			Guofu Xiang	2012			computer vision;computer science;pattern recognition;computer graphics (images)	Vision	48.51232185137147	-50.54923437205794	112125
859ca2f76ba12d026b72ce792a5c4e6aecfac02b	semantic structure from motion with object and point interactions	image motion analysis;training;three dimensional displays cameras correlation object detection feature extraction semantics training;semantics;pose estimation image motion analysis image reconstruction object detection;three dimensional;three dimensional displays;object manipulation;feature extraction;image reconstruction;autonomous navigation;correlation;structure from motion;cameras;object detection;structure from motion semantic structure scene geometry recovery ford car dataset kinect office dataset camera pose estimation point based sfm algorithm 3d object detection 2d object detection multiple semicalibrated images;point interaction;pose estimation	We propose a new method for jointly detecting objects and recovering the geometry of the scene (camera pose, object and scene point 3D locations) from multiple semi-calibrated images (camera internal parameters are known). To achieve this task, our method models high level semantics (i.e. object class labels and relevant characteristics such as location and pose) and the interaction (correlations) of objects and feature points within the same view and across views. We validate our algorithm against state-of-the-art baseline methods using two public datasets - Ford Car dataset and Kinect Office dataset [1] - and show that we: i) significantly improve the camera pose estimation results compared to point-based SFM algorithm; ii) achieve better 2D and 3D object detection accuracy than using single images separately. Our algorithm is critical in many application scenarios including object manipulation and autonomous navigation.	algorithm;autonomous robot;autostereogram;baseline (configuration management);high- and low-level;high-level programming language;interaction;kinect;object detection;semiconductor industry;sensor;structure from motion	Sid Ying-Ze Bao;Mohit Bagra;Silvio Savarese	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130358	iterative reconstruction;three-dimensional space;computer vision;structure from motion;pose;3d pose estimation;feature extraction;computer science;pattern recognition;mathematics;semantics;correlation;computer graphics (images)	Vision	49.16225340407666	-48.47776092879131	112180
86f1990caf628f6790c4ea15177984f1a33df93d	tightly-coupled robust vision aided inertial navigation algorithm for augmented reality using monocular camera and imu	mahalanobis distance;tracking system;degree of freedom;mems imu;kalman filters;feature tracking;inertial navigation;cameras current measurement cloning feature extraction tracking kalman filters vectors;kalman filter;cloning;inertial measurement unit;monocular camera;camera motion;current measurement;vectors;feature extraction;ekf mems imu monocular camera inertial navigation sensor fusion;ekf;sensor fusion;augmented reality;extended kalman filter;visual tracking;tight coupling;cameras;tracking;pose estimation	Odometry component of a camera tracking system for augmented reality applications is described. The system uses a MEMS-type inertial measurement unit (IMU) with 3-axis gyroscopes and accelerometers and a monocular camera to accurately and robustly track the camera motion in 6 degrees of freedom (with correct scale) in arbitrary indoor or outdoor scenes. Tight coupling of IMU and camera is achieved by an error-state extended Kalman filter (EKF) which performs sensor fusion for inertial navigation at a deep level such that each visually tracked feature contributes as an individual measurement as opposed to the more traditional approaches where camera pose estimates are first extracted by means of feature tracking and then used as measurement updates in a filter framework. Robustness, on the other hand, is achieved by using a geometric hypothesize-and-test architecture based on the five-point relative pose estimation method, rather than a Mahalanobis distance type gating mechanism derived from the Kalman filter state prediction, to select the inlier tracks and remove outliers from the raw feature point matches which would otherwise corrupt the filter since tracks are directly used as measurements.	algorithm;augmented reality;extended kalman filter;inertial navigation system;match moving;microelectromechanical systems;motion estimation;odometry;tracking system	Taragay Oskiper;Supun Samarasekera;Rakesh Kumar	2011	2011 10th IEEE International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2011.6143485	kalman filter;computer vision;camera auto-calibration;augmented reality;simulation;computer science;extended kalman filter	Robotics	53.10749451300173	-41.85120807575679	112424
c6058218b48a243482fc2bd8995dbde744a80b11	robocup 2002: robot soccer world cup vi	robot soccer	We present an approach to the landmark-based robot localization problem for environments, such as RoboCup middle-size soccer, that provide limited or low-quality information for localization. This approach allows use of different types of measurements on potential landmarks in order to increase landmark availability. Some sensors or landmarks might provide only range (such as field walls) or only bearing measurements (such as goals). The approach makes use of inexpensive sensors (color vision) using fast, simple updates robust to low landmark visibility and high noise. This localization method has been demonstrated in laboratory experiments and RoboCup 2001. Experimental analysis of the relative benefits of the approach is provided.	color vision;experiment;robotic mapping;sensor	Gal A. Kaminka;Pedro Urbano Lima;Raúl Rojas	2002		10.1007/b11927	simulation;computer science;artificial intelligence	Robotics	53.00499407269044	-39.04592675242917	112702
46ed775a301f01770e3d8ab55ae1db37c98608f1	vision-based topological mapping and localization methods: a survey	visual slam;localization;image descriptors;loop closure;bag of words;topological mapping	Topological maps model the environment as a graph, where nodes are distinctive places of the environment and edges indicate topological relationships between them. They represent an interesting alternative to the classic metric maps, due to their simplicity and storage needs, what has made topological mapping and localization an active research area. The different solutions that have been proposed during years have been designed around several kinds of sensors. However, in the last decades, vision approaches have emerged because of the technology improvements and the amount of useful information that a camera can provide. In this paper, we review the main solutions presented in the last fifteen years, and classify them in accordance to the kind of image descriptor employed. Advantages and disadvantages of each approach are thoroughly reviewed and discussed.	graph (discrete mathematics);map;sensor;visual descriptor	Emilio Garcia-Fidalgo;Alberto Ortiz	2015	Robotics and Autonomous Systems	10.1016/j.robot.2014.11.009	computer vision;internationalization and localization;computer science;bag-of-words model	Robotics	49.935259558269415	-43.208429921038714	112712
d32d5e3efa2c0419238d0982f3003fffa9ac9fb2	high precision road segmentation for cover level of forward view estimation via stereo camera	cover level of forward view;seed selection;image segmentation;roads image segmentation image color analysis estimation vehicles cameras regions;stereo camera;road segmentation;regions;estimation;road likelihood model;roads;image color analysis;vehicles;illuminant invariant image;cameras;stereo camera road segmentation illuminant invariant image cover level of forward view road likelihood model seed selection;obstacle detection road segmentation cover level forward view estimation stereo camera intelligent transportation system its road edge invariant space conversion shadow region removal illuminant invariant image road probability image mathematical morphology operation curve estimation;stereo image processing collision avoidance edge detection image segmentation intelligent transportation systems mathematical morphology probability	To develop a safe Intelligent Transportation System (ITS) while driving on unpredictable curves or road regions, high precision road segmentation and cover level of forward view estimation for drivers is necessary. Cover level of forward view is defined as the level of difficulty in predicting the dangerousness of road edge or incoming object near road edge especially at a curve due to the obstacles at the surrounding. We focus on road segmentation as it is one of the fundamental steps in developing ITS. According to the previous studies, road region is not segmented precisely; hence a new method of road segmentation is introduced. Input images had undergone illuminant invariant space conversion to remove shadow regions effectively. Next, the bottom part of illuminant invariant image (assumed to be road surface) is sampled to get a road model, which is then applied to obtain the road probability image. Lastly, dilation and erosion using mathematical morphology operation is applied to obtain road region. Our method of road segmentation shows precision of 0.73, recall as 0.82 and F measure as 0.81. High Precision road segmentation is very important for better cover level of forward view estimation. Segmented road region can be fully utilized for curve estimation and obstacles detection and hence lead to a better performance of cover level of forward view. Based on the results obtained, further improvement in several aspects, especially from the input image or illuminant invariant image to road probability image, has to be done in order to obtain perfectly segmented road region.	dilation (morphology);f1 score;image segmentation;mathematical morphology;stereo camera;trust region	Siti Nor Khuzaimah Binti Amit;Yoshimitsu Aoki	2015	2015 10th Asian Control Conference (ASCC)	10.1109/ASCC.2015.7244743	computer vision;simulation;geography;remote sensing	Vision	42.87909631105208	-44.769756408271206	112760
98509920af110498d700b1307e4d89c8924c7cc4	a novel camera parameters auto-adjusting method based on image entropy	vision system;omnidirectional vision;color constancy;robot vision	How to make vision system work robustly under dynamic light conditions is still a challenging research focus in robot vision community. In this paper, a novel camera parameters auto-adjusting method based on image entropy is proposed. Firstly image entropy is defined and its relationship with camera parameters is verified by experiments. Then how to optimize the camera parameters based on image entropy is proposed to make robot vision adaptive to the different light conditions. The algorithm is tested using the omnidirectional vision system in indoor RoboCup Middle Size League environment and outdoor RoboCup-like environment, and the results show that our method is effective and color constancy to some extent can be achieved.	algorithm;computer graphics lighting;experiment	Huimin Lu;Hui Zhang;Shaowu Yang;Zhiqiang Zheng	2009		10.1007/978-3-642-11876-0_17	computer vision;simulation;machine vision;computer science;artificial intelligence;color constancy	Vision	47.10931458468537	-41.78177969564959	112822
3551679ba6d437486747ce7d21046450bf82cd79	an effective rigidity constraint for improving ransac in homography estimation	statistical approach;degeneration;point of view;system of equations	  A homography is a projective transformation which can relate two images of the same planar surface taken from two different  points of view. Hence, it can be used for registering images of scenes that can be assimilated to planes. For this purpose  a homography is usually estimated by solving a system of equations involving several couples of points detected at different  coordinates in two different images, but located at the same position in the real world. A usual and efficient way of obtaining  a set of good point correspondences is to start from a putative set obtained somehow and to sort out the good correspondences  (inliers) from the wrong ones (outliers) by using the so-called RANSAC algorithm. This algorithm relies on a statistical approach  which necessitates estimating iteratively many homographies from randomly chosen sets of four-correspondences. Unfortunately,  homographies obtained in this way do not necessarily reflect a rigid transformation. Depending on the number of outliers,  evaluating such degenerated cases in RANSAC drastically slows down the process and can even lead to wrong solutions. In this  paper we present the expression of a lightweight rigidity constraint and show that it speeds up the RANSAC process and prevents  degenerated homographies.    	homography (computer vision);random sample consensus	David Monnin;Etienne Bieber;Gwenaél Schmitt;Armin L. Schneider	2010		10.1007/978-3-642-17691-3_19	system of linear equations;computer vision;mathematical optimization;mathematics;geometry	Vision	52.86904384783167	-49.80215322780082	113044
2d93bbabca272a4f1b697792dc8274af44211b5f	branch-and-price global optimization for multi-view multi-target tracking	temporal correlation;graph theory;optimisation;3d human pose tracking;computer vision;optimisation computer vision graph theory object tracking;graph structure;trajectory;spatial correlation;image edge detection;image reconstruction;multiview multitarget tracking;object tracking;linear programming;cameras trajectory image edge detection optimization correlation image reconstruction linear programming;branch and price global optimization;min cost problem;optimization;correlation;combinatorial optimization;3d human pose tracking branch and price global optimization multiview multitarget tracking min cost problem graph structure temporal correlation spatial correlation combinatorial optimization dantzig wolfe decomposition;dantzig wolfe decomposition;cameras	We present a new algorithm to jointly track multiple objects in multi-view images. While this has been typically addressed separately in the past, we tackle the problem as a single global optimization. We formulate this assignment problem as a min-cost problem by defining a graph structure that captures both temporal correlations between objects as well as spatial correlations enforced by the configuration of the cameras. This leads to a complex combinatorial optimization problem that we solve using Dantzig-Wolfe decomposition and branching. Our formulation allows us to solve the problem of reconstruction and tracking in a single step by taking all available evidence into account. In several experiments on multiple people tracking and 3D human pose tracking, we show our method outperforms state-of-the-art approaches.	algorithm;angularjs;assignment problem;branch and price;combinatorial optimization;computer vision;dantzig–wolfe decomposition;experiment;global optimization;mathematical optimization;maxima and minima;optimization problem;time complexity	Laura Leal-Taixé;Gerard Pons-Moll;Bodo Rosenhahn	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247901	iterative reconstruction;optimization problem;computer vision;mathematical optimization;spatial correlation;combinatorial optimization;generalized assignment problem;linear programming;dantzig–wolfe decomposition;graph theory;trajectory;machine learning;video tracking;mathematics;correlation	Vision	50.269223394672636	-49.42105149336305	113090
25391439b8f9b0f92164763e3e1bdd3af139e627	adaptive foreground and shadow detection inimage sequences		This paper presents a novel method of foreground segmentation that distinguishes moving objects from their moving cast shadows in monocular image sequences. The models of background, edge information, and shadow are set up and adaptively updated. A Bayesian belief network is proposed to describe the relationships among the segmentation label, background, intensity, and edge information. The notion of Markov random field is used to encourage the spatial connectivity of the segmented regions. The solution is obtained by maximizing the posterior possibility density of the segmentation field.	algorithm;bayesian network;color image;computation;information source;markov chain;markov random field;mathematical optimization;second source;sensor	Yang Wang;Tele Tan	2002				Vision	45.17816410048778	-50.350655890552	113271
6002cc943eada17176f21c3c01a6ee6e213823bc	3-d-laser-based scene measurement and place recognition for mobile robots in dynamic indoor environments	feature extraction lasers measurement by laser beam mobile robots robustness spatial databases sensors;optical scanners;smartrob2 3d laser based scene measurement mobile robot dynamic indoor environment active environment perception autonomous place recognition cluttered indoor environment random disturbance speeded up robust feature extraction speeded up robust feature matching bearing angle image self built rotating 3d laser scanner global metric information;image matching;mobile robots;3 d laser based scene measurement autonomous place recognition bearing angle image speeded up robust features surfs 3 d laser scanning;qa75 electronic computers computer science;robot vision;feature extraction;robot vision feature extraction image matching mobile robots optical scanners	Active environment perception and autonomous place recognition play a key role for mobile robots to operate within a cluttered indoor environment with dynamic changes. This paper presents a 3-D-laser-based scene measurement technique and a novel place recognition method to deal with the random disturbances caused by unexpected movements of people and other objects. The proposed approach can extract and match the Speeded-Up Robust Features (SURFs) from bearing-angle images generated by a self-built rotating 3-D laser scanner. It can cope with the irregular disturbance of moving objects and the problem of observing-location changes of the laser scanner. Both global metric information and local SURF features are extracted from 3-D laser point clouds and 2-D bearing-angle images, respectively. A large-scale indoor environment with over 1600 m2 and 30 offices is selected as a testing site, and a mobile robot, i.e., SmartROB2, is deployed for conducting experiments. Experimental results show that the proposed 3-D-laser-based scene measurement technique and place recognition approach are effective and provide robust performance of place recognition in a dynamic indoor environment.	autonomous robot;experiment;mobile robot;point cloud;speeded up robust features	Yan Zhuang;Nan Jiang;Huosheng Hu;Fei Yan	2013	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2012.2216475	mobile robot;computer vision;simulation;feature extraction;computer science;machine learning	Robotics	51.03418985404234	-38.12626611279356	113288
cdac61da0a3debcd1dd456237b85a797edc31240	segmentation of moving pedestrians within the compressed domain	image segmentation;data compression;decoding;surveillance;motion estimation;code standards;closed circuit television;video coding;discrete cosine transforms;motion vector;fast algorithm;median filters image segmentation surveillance motion estimation discrete cosine transforms data compression video coding decoding code standards image denoising closed circuit television;background subtraction;surveillance moving pedestrians video encoding standards mpeg 2 compression motion vector filtering dct coefficients accurate segmentation reliable motion estimation background subtraction fine segmentation step blob characteristics segmentation noise reduction decoding occlusion problems underground station cctv cameras;image denoising;video compression encoding transform coding decoding filtering discrete cosine transforms motion estimation noise reduction cameras surveillance;median filters	Video encoding standards, namely MPEG-2, store large amounts of information obtained for compression purposes that can be accessed with minimal decoding. This paper shows that, with proper filtering of motion vectors and DCT coefficients, accurate segmentation results can be achieved by combining both reliable motion estimation and background subtraction. We further present a fine segmentation step that exploits specific blob characteristics to reduce segmentation noise and solve some occlusion problems. Examples using real videos from underground station CCTV cameras show that compressed domain information can be the key for successful surveillance applications where very fast algorithms with high accuracy are required.	algorithm;autostereogram;background subtraction;closed-circuit television;coefficient;computation;discrete cosine transform;information;kalman filter;mpeg-2;motion estimation;refinement (computing);time complexity	Miguel Tavares Coimbra;Mike Davies	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326617	data compression;computer vision;speech recognition;background subtraction;computer science;motion estimation;image segmentation;scale-space segmentation;computer graphics (images)	Robotics	44.430657473426024	-44.70074387596033	113564
9bdbde9afedef05c6a1301e03f216ef81b6d4d17	a new approach to visual-based sensory system for navigation into orange groves	agricultural robotics;ccd camera;neural networks;robotics;image sensors;image processing computer assisted;info eu repo semantics article;ensembles;virtual sensor;algorithms;sensory system;humans;citrus sinensis;neural networks computer;autonomous robot;outdoor navigation;neural network	One of the most important parts of an autonomous robot is to establish the path by which it should navigate in order to successfully achieve its goals. In the case of agricultural robotics, a procedure that determines this desired path can be useful. In this paper, a new virtual sensor is introduced in order to classify the elements of an orange grove. This proposed sensor will be based on a color CCD camera with auto iris lens which is in charge of doing the captures of the real environment and an ensemble of neural networks which processes the capture and differentiates each element of the image. Then, the Hough's transform and other operations will be applied in order to extract the desired path from the classification performed by the virtual sensory system. With this approach, the robotic system can correct its deviation with respect to the desired path. The results show that the sensory system properly classifies the elements of the grove and can set trajectory of the robot.	agricultural robot;artificial neural network;autonomous robot;boosting (machine learning);bézier curve;central core myopathy (disorder);charge-coupled device;cross infection;cross-validation (statistics);extraction;genetic heterogeneity;hl7publishingsubsection <operations>;hough transform;neural network simulation;robotics;s transform;statistical classification	Joaquín Torres-Sospedra;Patricio Nebot	2011		10.3390/s110404086	sensory system;embedded system;computer vision;simulation;computer science;engineering;artificial intelligence;image sensor;charge-coupled device;artificial neural network;physics	Robotics	48.32634189129362	-38.487007113864486	113742
f48885ab1a51bfa120b740726316ef801d91f992	fast techniques for monocular visual odometry		In this paper, fast techniques are proposed to achieve real time and robust monocular visual odometry. We apply an iterative 5point method to estimate instantaneous camera motion parameters in the context of a RANSAC algorithm to cope with outliers efficiently. In our method, landmarks are localized in space using a probabilistic triangulation method utilized to enhance the estimation of the last camera pose. The enhancement is performed by multiple observations of landmarks and minimization of a cost function consisting of epipolar geometry constraints for far landmarks and projective constraints for close landmarks. The performance of the proposed method is demonstrated through application to the challenging KITTI visual odometry dataset.	algorithm;epipolar geometry;iterative method;loss function;random sample consensus;visual odometry	Mohammad Hossein Mirabdollah;Bärbel Mertsching	2015		10.1007/978-3-319-24947-6_24	visual odometry	Vision	51.73002823134786	-47.35713083703812	113787
1bf01e83fba634bab085ec5f0ab86a1a67da8577	an equalized global graph model-based approach for multicamera object tracking	cameras object tracking visualization trajectory optimization target tracking;non overlapping visual object tracking multi camera multi object tracking global graph model	Nonoverlapping multicamera visual object tracking typically consists of two steps: single-camera object tracking (SCT) and inter-camera object tracking (ICT). Most of tracking methods focus on SCT, which happens in the same scene, while for real surveillance scenes, ICT is needed and single-camera tracking methods cannot work effectively. In this paper, we try to improve the overall multicamera object tracking performance by a global graph model with an improved similarity metric. Our method treats the similarities of single-camera tracking and inter-camera tracking differently and obtains the optimization in a global graph model. The results show that our method can work better even in the condition of poor SCT.	feature learning;machine learning;match moving;mathematical optimization	Weihua Chen;Lijun Cao;Xiaotang Chen;Kaiqi Huang	2017	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2016.2589619	artificial intelligence;computer vision;tracking system;visualization;video tracking;computer science;trajectory;graph	Vision	41.67556535059579	-47.066340288078344	113885
9922b7d2f1fce1302abe2baeee74a8f3447c785b	depth analysis for surveillance videos in the h.264 compressed domain	video surveillance;image motion analysis;scene geometry h 264 compressed domain camera pose estimation;video streaming;camera pose estimation;data compression;cameras roads estimation image coding geometry surveillance image segmentation;image classification;equidistant lines depth analysis video surveillance h 264 compressed domain object detection object tracking object classification schemes scene geometry properties camera pose unsupervised method single view video sequences traffic surveillance h 264 encoding compressed video streams compressed domain motion vectors motion maps camera orientation angle estimation image plane;video coding;compressed domain;scene geometry;object tracking;h 264;video surveillance data compression image classification image motion analysis image sequences object detection object tracking video coding video streaming;object detection;image sequences	Knowledge about the distance of moving objects can be used to enhance the performance of object detection, tracking and classification schemes. However, such information is usually not known a priori. We present an unsupervised method to approximate basic scene geometry properties such as the camera pose in single-view video sequences. At present, the method is working in constraint environments such as traffic surveillance. The proposed approach is solely based on the motion information present in H.264 encoded, compressed video streams and does not rely on object tracking results. We start by constructing motion maps from compressed domain motion vectors. These maps are used to estimate the orientation angle of the camera, which allows to add a depth measure in the form of equidistant lines to the image plane.	approximation algorithm;data compression;effective method;h.264/mpeg-4 avc;image plane;map;object detection;statistical classification;streaming media;time-of-flight camera;unsupervised learning	H. Nicolas	2012	2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)		computer vision;computer science;video tracking;motion estimation;multimedia;computer graphics (images)	Vision	44.71384790190032	-45.000305941136226	114002
8dede36668e5aaf80f307762dbec4353502976ed	free-space detection with fish-eye cameras		Advance Driver Assistance Systems (ADAS) have gained huge attention in the last decades. One of the fundamental steps of the video processing chain is the detection of areas where the car can drive through, i.e. free-space. In this paper we present an approach for the detection of free-space which is based on image segmentation and classification of the obtained image segments. For the image segmentation step we use several state-of-the-art approaches. The classification is done by a random-forest classifier trained to label the image segments with one of three geometric classes (ground, sky, vertical) based on spatial, color and shape features. Segments labelled as ground are used to detect the free-space area in front of the car. Furthermore, a comparison of the results obtained by using different segmentation approaches is provided.	architecture design and assessment system;coherence (physics);distortion;edge case;fisheye;graphics processing unit;image segmentation;mathematical optimization;mean shift;parsing;random forest;real-time clock;real-time computing;real-time locating system;sensor;software deployment;test set;video processing	Simon Hanisch;Rubén Heras Evangelio;Hadj Hamma Tadjine;Michael Pätzold	2017	2017 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2017.7995710	range segmentation;binary image;computer vision;scale-space segmentation;region growing;image texture;image segmentation;segmentation-based object categorization;feature detection (computer vision);computer science;artificial intelligence;pattern recognition	Vision	42.322457036225686	-43.5806687834946	114129
996199e1ee8525f451fa600d93b7f6339d56b09b	dugma: dynamic uncertainty-based gaussian mixture alignment		Accurately registering point clouds from a cheap low resolution sensor is a challenging task. Existing rigid registration methods failed to use the physical 3D uncertainty distribution of each point from a real sensor in the dynamic alignment process. It is mainly because the uncertainty model for a point is static and invariant and it is hard to describe the change of these physical uncertainty models in different views. Additionally, the existing Gaussian mixture alignment architecture cannot efficiently implement these dynamic changes. This paper proposes a simple architecture combining error estimation from sample covariances and dual dynamic global probability alignment using the convolution of uncertainty-based Gaussian Mixture Models (GMM) from point clouds. Firstly, we propose an efficient way to describe the change of each 3D uncertainty model, which represents the structure of the point cloud much better. Unlike the invariant GMM (representing a fixed point cloud) in traditional Gaussian mixture alignment, we use two uncertainty-based GMMs that change and interact with each other in each iteration. In order to have a wider basin of convergence than other local algorithms, we design a more robust energy function by convolving efficiently the two GMMs over the whole 3D space. Tens of thousands of trials have been conducted on hundreds of models from multiple datasets to demonstrate the proposed method’s superior performance compared with the current state-of-the-art methods. All the materials including our code is available from https://github. com/Canpu999/DUGMA.	algorithm;approximation algorithm;convolution;experiment;fixed point (mathematics);google map maker;image resolution;iteration;mathematical optimization;mixture model;point cloud;time complexity	Can Pu;Nanbo Li;Radim Tylecek;Robert B. Fisher	2018	2018 International Conference on 3D Vision (3DV)	10.1109/3DV.2018.00092	architecture;mixture model;point cloud;pattern recognition;probabilistic logic;artificial intelligence;invariant (mathematics);computer science;gaussian;fixed point;solid modeling	Vision	47.75999379033916	-51.03265235609616	114135
24c0bb4d34e7e0ed6be90bbaffec3cdb4bb91a71	3d shape and motion analysis from image blur and smear: a unified approach	motion analysis;focusing;spatial blur;shutter;shape recovery;motion estimation;motion;realistic camera model;computer vision;aperture;image reconstruction motion estimation;finite aperture;shape;image reconstruction;temporal smear;mathematical model;inference algorithms;computer science;motion shape recovery motion estimation realistic camera model aperture shutter spatial blur temporal smear finite aperture shutter speed shape;cameras;shutter speed;apertures;shape motion analysis cameras apertures motion estimation focusing mathematical model inference algorithms computer science computer vision	This paper addresses 3D shape recovery and motion estimation using a realistic camera model with an aperture and a shutter. The spatial blur and temporal smear e ects induced by the camera's nite aperture and shutter speed are used for inferring both the shape and motion of the imaged objects.	box blur;gaussian blur;motion estimation;movie projector;smear campaign	Yuan-Fang Wang;Ping Liang	1998		10.1109/ICCV.1998.710843	aperture;computer vision;computer graphics (images)	Vision	53.53155502579078	-51.833630108103925	114245
2c7baff9c74f7eb565b3a72d0371f3547d9de99b	faceforge: markerless non-rigid face multi-projection mapping		Recent publications and art performances demonstrate amazing results using projection mapping. To our knowledge, there exists no multi-projection system that can project onto non-rigid target geometries. This constrains the applicability and quality for live performances with multiple spectators. Given the cost and complexity of current systems, we present a low-cost easy-to-use markerless non-rigid face multi-projection system. It is based on a non-rigid, dense face tracker and a real-time multi-projection solver adapted to imprecise tracking, geometry and calibration. Using this novel system we produce compelling results with only consumer-grade hardware.	algorithm;artifact (software development);depth map;morphologic artifacts;movie projector;muscle rigidity;numerous;performance;projections and predictions;real-time clock;real-time computing;real-time locating system;solver;tracking system	Christian Siegl;Vanessa Lange;Marc Stamminger;Frank Bauer;Justus Thies	2017	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2017.2734428	computer vision;artificial intelligence;theoretical computer science;computer science;projection mapping;solver	Visualization	53.758182153908855	-46.32779089410235	114341
3cf6dd179a7b5ed25535a155a84b5865d12c81f8	yolo3d: end-to-end real-time 3d oriented object bounding box detection from lidar point cloud		Object detection and classification in 3D is a key task in Automated Driving (AD). LiDAR sensors are employed to provide the 3D point cloud reconstruction of the surrounding environment, while the task of 3D object bounding box detection in real time remains a strong algorithmic challenge. In this paper, we build on the success of the oneshot regression meta-architecture in the 2D perspective image space and extend it to generate oriented 3D object bounding boxes from LiDAR point cloud. Our main contribution is in extending the loss function of YOLO v2 to include the yaw angle, the 3D box center in Cartesian coordinates and the height of the box as a direct regression problem. This formulation enables real-time performance, which is essential for automated driving. Our results are showing promising figures on KITTI benchmark, achieving real-time performance (40 fps) on Titan X GPU.	autonomous car;benchmark (computing);bird's-eye view;graphics processing unit;heuristic (computer science);loss function;minimum bounding box;object detection;opaque binary blob;operating point;pipeline (computing);point cloud;real-time clock;sensor;thinking outside the box;titan (supercomputer);titan rain;yaws	Waleed Ali;Sherif Abdelkarim;Mohamed Zahran;Mahmoud Zidan;Ahmad El Sallab	2018	CoRR		artificial intelligence;computer vision;point cloud;euler angles;cartesian coordinate system;computer science;bounding overwatch;object detection;minimum bounding box;end-to-end principle;lidar	Vision	52.22339262085457	-44.34708770526359	114371
1710d3fe464b804b016b26a86794acc61c5d6d57	a real-time 3d workspace modeling with stereo camera	integrated approach;object recognition;manipulators;image registration manipulators robot vision real time systems stereo image processing octrees feature extraction object recognition;3d workspace modeling;real time;3d object recognition 3d workspace modeling planar feature stereo vision sift;robot stereo vision real time 3d workspace modeling stereo camera robot manipulation workspace geometric configuration feature identification object recognition image registration multiresolution octree representation stereo sis scale invariant feature transform scene registration behavior oriented 3d modeling;3d object recognition;robot manipulator;cameras iterative closest point algorithm solid modeling object recognition humans clouds layout working environment noise mobile robots intelligent sensors;planar feature;3d model;robot vision;sift;feature extraction;image registration;stereo image processing;stereo vision;on the fly;point cloud;multi resolution;octrees;real time systems	This paper presents a novel approach to real-time 3D modeling of workspace for manipulative robotic tasks. First, we establish the three fundamental principles that human uses for modeling and interacting with environment. These principles have led to the development of an integrated approach to real-time 3D modeling, as follows: 1) It starts with a rapid but approximate characterization of the geometric configuration of workspace by identifying global plane features. 2) It quickly recognizes known objects in workspace and replaces them by their models in database based on in-situ registration. 3) It models the geometric details on the fly adaptively to the need of the given task based on a multi-resolution octree representation. SIFT features with their 3D position data, referred to here as stereo-sis SIFT, are used extensively, together with point clouds, for fast extraction of global plane features, for fast recognition of objects, for fast registration of scenes, as well as for overcoming incomplete and noisy nature of point clouds. The experimental results show the feasibility of real-time and behavior-oriented 3D modeling of workspace for robotic manipulative tasks.	3d modeling;approximation algorithm;entity;experiment;interaction;octree;on the fly;planar (computer graphics);point cloud;real-time clock;real-time computing;real-time locating system;real-time transcription;robot;scale-invariant feature transform;stereo camera;workspace	Sukhan Lee;Daesik Jang;Eunyoung Kim;Suyeon Hong;JungHyun Han	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545105	computer vision;simulation;feature extraction;computer science;stereopsis;image registration;cognitive neuroscience of visual object recognition;scale-invariant feature transform;point cloud;computer graphics (images)	Robotics	50.79224692016079	-40.288996641918956	114568
2f6014b5c431ce211d14e710db569c907f05485d	road network fusion for incremental map updates		In the recent years a number of novel, automatic map-inference techniques have been proposed, which derive road-network from a cohort of GPS traces collected by a fleet of vehicles. In spite of considerable attention, these maps are imperfect in many ways: they create an abundance of spurious connections, have poor coverage, and are visually confusing. Hence, commercial and crowd-sourced mapping services heavily use human annotation to minimize the mapping errors, and their response to changes in the road network is inevitably slow. In this paper we describe MapFuse, a system which fuses the humanannotated map (in our case OpenStreetMap) with any automatically inferred map, thus effectively allowing quick map updates. In addition to new road creation, we study in depth the road closures which have not been examined in the past. By leveraging solid, human-annotated, maps with minor corrections we derive maps which minimize the trajectory matching errors due to both road network change and imperfect map inference of fully-automatic approaches.	algorithm;crowdsourcing;global positioning system;map (higher-order function);openstreetmap;programming paradigm;tracing (software)	Rade Stanojevic;Sofiane Abbar;Saravanan Thirumuruganathan;Gianmarco De Francisci Morales;Sanjay Chawla;Fethi Filali;Ahid Aleimat	2018		10.1007/978-3-319-71470-7_5	theoretical computer science;global positioning system;computer science;data mining;spurious relationship;spite;inference;annotation	Mobile	41.76887841569097	-39.12517193512293	114736
f52efc206432a0cb860155c6d92c7bab962757de	mugshot database acquisition in video surveillance networks using incremental auto-clustering quality measures	automatic control;image recognition;lighting control;pattern clustering;face target tracking;video surveillance;image databases;video surveillance image databases face recognition face detection image recognition automatic control lighting control target tracking cameras target recognition;video signal processing;surveillance;real time;mugshot database acquisition;face recognition;incremental auto clustering quality measures;optical tracking;target recognition;mugshot extraction;face target tracking mugshot database acquisition video surveillance networks incremental auto clustering quality measures face recognition face image matching mugshot extraction face detection;pattern clustering optical tracking target tracking surveillance visual databases face recognition video signal processing object detection;quality measures;target tracking;face detection;video surveillance networks;camera network;cameras;object detection;visual databases;face image matching	Face recognition has primarily focused on recognizing and matching face images against large, controlled databases of frontal views. Many of these techniques perform well against databases that have been collected from a reduced set of viewpoints, under controlled lighting, and are normalized for scale. Acquisition of these databases, however, particularly in unconstrained environments, remains a challenge. We present a real-time technique to automatically acquire a mugshot database from a video surveillance network. Mugshot extraction is a twofold problem. First, faces are detected and tracked in all cameras of the network. Face targets are analyzed to determine which frames represent actual mugshots capable of supporting subsequent matching and recognition. Next, mugshot candidates are evaluated based on their ability to improve the quality of the incrementally constructed database. We introduce a database quality measure, which assigns high value to mugshots of previously unseen subjects or mugshots that do not decrease separability of existing clusters. The quality measure is discounted for mugshots that are redundant or increase the intra-cluster spread. Results demonstrate that automatic acquisition of a high-quality database from a twelve-camera network is feasible. The quality of these databases is demonstrated using traditional methods to accurately match faces against the acquired database.	closed-circuit television;database;facial recognition system;linear separability;mugshot;real-time locating system	Quanren Xiong;Christopher O. Jaynes	2003		10.1109/AVSS.2003.1217921	facial recognition system;computer vision;face detection;computer science;automatic control;pattern recognition;data mining	Vision	40.99863220156201	-50.102754054186654	114743
e7fad605f2f3d51147719f74c4a9d3ea5ba2b757	vertical edge-based mapping using range-augmented omnidirectional vision sensor	pioneer 3dx mobile robot;invariant image features;fastslam algorithm;visio per ordinador;image processing;vertical edge based mapping simultaneous localisation and mapping problem parabolic mirror urg 04lx laser range finder pioneer 3dx mobile robot indoor environments fastslam algorithm data association unified spherical model three dimensional position extraction sensor model texture information invariant image features vertical lines horizontal lines catadioptric projection vanishing points textured plane extraction environmental information omnidirectional cameras range augmented omnidirectional vision sensor;vertical lines;sensor model;vertical edge based mapping;vanishing points;range augmented omnidirectional vision sensor;catadioptric projection;data association;info eu repo semantics article;computer vision;imatges processament;unified spherical model;textured plane extraction;three dimensional position extraction;simultaneous localisation and mapping problem;omnidirectional cameras;horizontal lines;indoor environments;texture information;urg 04lx laser range finder;computer algorithms;slam robots cameras edge detection feature extraction image sensors image texture indoor environment laser ranging mobile robots robot vision sensor fusion;algorismes computacionals;parabolic mirror;environmental information	Laser range finder and omnidirectional cameras are becoming a promising combination of sensors to extract rich environmental information. This information includes textured plane extraction, vanishing points, catadioptric projection of vertical and horizontal lines, or invariant image features. However, many indoor scenes do not have enough texture information to describe the environment. In these situations, vertical edges could be used instead. This study presents a sensor model that is able to extract three-dimensional position of vertical edges from a range-augmented omnidirectional vision sensor. Using the unified spherical model for central catadioptric sensors and the proposed sensor model, the vertical edges are locally projected, improving the data association for mapping and localisation. The proposed sensor model was tested using the FastSLAM algorithm to solve the simultaneous localisation and mapping problem in indoor environments. Realworld qualitative and quantitative experiments are presented to validate the proposed approach using a Pioneer-3DX mobile robot equipped with a URG-04LX laser range finder and an omnidirectional camera with parabolic mirror.	algorithm;catadioptric sensor;correspondence problem;experiment;mobile robot;omnidirectional camera;parabolic antenna;simultaneous localization and mapping;spherical model	Bladimir Bacca;Xavier Cufí;Joaquim Salvi	2013	IET Computer Vision	10.1049/iet-cvi.2011.0214	computer vision;simulation;vanishing point;image processing;computer science;parabolic reflector	Robotics	51.352544019590866	-39.07209128652228	114777
0b6c1a2d1374a0279de588725693f51fb040fda8	trajectory analysis in natural images using mixtures of vector fields	analytical models;object recognition;image motion analysis;image analysis hidden markov models layout image sequences switches motion estimation parameter estimation image sequence analysis telecommunications animals;hidden markov model;natural images;vectors expectation maximisation algorithm image motion analysis image sequences object recognition;trajectories;vector fields;hidden markov models;trajectory;vectors;stochastic processes;expectation maximization;image sequence;mathematical model;expectation maximization algorithm trajectory analysis natural images vector fields object trajectories image sequences motion patterns;em algorithm trajectories vector fields hidden markov models;vector field;switches;em algorithm;algorithm design and analysis;image sequences;expectation maximisation algorithm	This work introduces a new approach to modeling object trajectories in image sequences. Trajectories performed by natural objects (e.g., people, animals) typically depend on the position of each object in the scene and can change in an unpredictable way. Despite this diversity, there is often a small number of typical motion patterns based on which it is possible to explain all the observed trajectories. To achieve this goal, we model each of these motion patterns using a motion field and allow objects to switch between fields in a space-varying, possible probabilistic, way. Our approach provides a space-dependent motion model which can be estimated using an expectation-maximization (EM) algorithm. Experiments with both synthetic and real data are presented to illustrate the ability of the proposed approach in modeling different motion patterns.	expectation–maximization algorithm;experiment;motion field;synthetic intelligence	Jacinto C. Nascimento;Mário A. T. Figueiredo;Jorge S. Marques	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413541	computer vision;vector field;expectation–maximization algorithm;computer science;trajectory;machine learning;pattern recognition;motion estimation;mathematics;motion field;hidden markov model;statistics	Robotics	46.29009094322211	-49.48231819796175	114849
c610db0ee2d111452f70ce4854e48ab9d5c2b1ab	fast and robust multi-people tracking from rgb-d data for a mobile robot		This paper proposes a fast and robust multi-people tracking algorithm for mobile platforms equipped with a RGB-D sensor. Our approach features an efficient point cloud depth-based clustering, an HOGlike classification to robustly initialize a person tracking and a person classifier with online learning to manage the person ID matching even after a full occlusion. For people detection, we make the assumption that people move on a ground plane. Tests are presented on a challenging real-world indoor environment and results have been evaluated with the CLEAR MOT metrics. Our algorithm proved to correctly track 96% of people with very limited ID switches and few false positives, with an average frame rate of 25 fps. Moreover, its applicability to robot-people following tasks have been tested and discussed.	algorithm;cluster analysis;cobham's thesis;decimation (signal processing);kinect;mobile device;mobile robot;motion planning;network switch;point cloud;population;sensor;tracking system	Filippo Basso;Matteo Munaro;Stefano Michieletto;Enrico Pagello;Emanuele Menegatti	2012		10.1007/978-3-642-33926-4_25	mobile robot;tracking system	Vision	46.954407479402754	-40.69442667603159	114853
ab17f52cda8387c7f6f815c0d9b3e27a3dbee4fc	alignment of three-dimensional point clouds using combined descriptors	histograms;three dimensional displays histograms iterative closest point algorithm accuracy computational modeling robots biomedical imaging;biomedical imaging;accuracy;computational modeling;surface descriptors point clouds 3d object registration 3d modelling point correspondence;three dimensional displays;robots;sampling methods computational geometry;iterative closest point algorithm;inliers three dimensional point cloud alignment three dimensional object model alignment 3d object model alignment point correspondences 3d point cloud modelling point cloud down sampling processing time improvement point allocation point cloud processing combinational descriptor scheme global transformation estimation	This paper presents a new methodology for aligning three-dimensional (3D) models of objects, based on point correspondences. In this case, objects are modelled as 3D point clouds. The proposed methodology considers pairs of such point clouds and firstly down-samples them in order to further improve processing time. Then, corresponding points are allocated between the processed point clouds, by using a novel combinational descriptor scheme. Finally, a global transformation is estimated from the inliers of the obtained correspondences. This transformation is used to align the two point clouds. The proposed methodology was applied to five pairs of large scale 3D point clouds. Results indicate that the proposed scheme achieved satisfactory alignment accuracy for all tested data pairs.	align (company);combinational logic;cyber-shot;point cloud	George K. Matsopoulos;Theodore L. Economopoulos;Irene S. Karanasiou;Maria Koutsoupidou;Errikos M. Ventouras	2014	2014 4th International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2014.7001945	medical imaging;robot;computer vision;mathematical optimization;computer science;theoretical computer science;histogram;mathematics;accuracy and precision;computational model;iterative closest point;statistics	Robotics	50.58434312718131	-51.73720612974497	114958
42fa64788b29ce8b8a93161580f47e14bab44da4	using geometric constraints through parallelepipeds for calibration and 3d modeling	geometric constraints index terms 3d modeling calibration;contraste;vision ordenador;3d modeling;modele geometrique;calibration solid modeling layout cameras image reconstruction parallel processing shape computational modeling computer vision internet;duality mathematics;duality mathematics cameras calibration image reconstruction computational geometry;modelo 3 dimensiones;index terms 3d modeling;paralelepipedo;modele 3 dimensions;computational geometry;three dimensional model;algorithms artificial intelligence calibration image enhancement image interpretation computer assisted imaging three dimensional photogrammetry reproducibility of results sensitivity and specificity;parameterization;indexing terms;universiteitsbibliotheek;parametrizacion;computer vision;reconstruction image;factorization;3d model;relative pose estimation geometric constraints parallelepipeds images geometric information camera calibration semiautomatic 3d modeling geometric primitives duality factorization based algorithm self calibration constraints scene primitive reconstruction;reconstruccion imagen;image reconstruction;indexation;shape parameter;3d scene analysis;vision ordinateur;etalonnage;information geometrique;geometrical constraints;parallelepipede;camera calibration;geometric constraints;calibration;contrainte geometrique;parametrisation;cameras;geometrical model;parallelepiped;parallelepipeds;modelo geometrico	This paper concerns the incorporation of geometric information in camera calibration and 3D modeling. Using geometric constraints enables more stable results and allows us to perform tasks with fewer images. Our approach is motivated and developed within a framework of semi-automatic 3D modeling, where the user defines geometric primitives and constraints between them. In this paper, first a duality that exists between the shape parameters of a parallelepiped and the intrinsic parameters of a camera is described. Then, a factorization-based algorithm exploiting this relation is developed. Using images of parallelepipeds, it allows us to simultaneously calibrate cameras, recover shapes of parallelepipeds, and estimate the relative pose of all entities. Besides geometric constraints expressed via parallelepipeds, our approach simultaneously takes into account the usual self-calibration constraints on cameras. The proposed algorithm is completed by a study of the singular cases of the calibration method. A complete method for the reconstruction of scene primitives that are not modeled by parallelepipeds is also briefly described. The proposed methods are validated by various experiments with real and simulated data, for single-view as well as multiview cases.	3d modeling;algorithm;calibration;camera resectioning;entity;experiment;semiconductor industry;singular	Marta Wilczkowiak;Peter F. Sturm;Edmond Boyer	2005	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2005.40	parametrization;computer vision;mathematical optimization;calibration;computational geometry;mathematics;geometry;shape parameter;factorization	Vision	53.224960987790276	-51.41826428080275	115034
6665970c95f70c15419b38a9bd84c286bc6e9240	fusing appearance and spatio-temporal features for multiple camera tracking	multiple camera tracking;metric learning;feature fusion	Multiple camera tracking is a challenging task for many surveillance systems. The objective of multiple camera tracking is to maintain trajectories of objects in the camera network. Due to ambiguities in appearance of objects, it is challenging to re-identify objects when they re-appear in other cameras. Most research works associate objects by using appearance features. In this work, we fuse appearance and spatio-temporal features for person re-identification. Our framework consists of two steps: preprocessing to reduce the number of association candidates and associating objects by using the probabilistic relative distance. We set up an experimental environment including 10 cameras and achieve a better performance than using appearance features only.	match moving	Nam Trung Pham;Karianto Leman;Richard Chang;Jie Zhang;Hee Lin Wang	2014		10.1007/978-3-319-04114-8_31	computer vision;camera auto-calibration;simulation;computer graphics (images)	Vision	42.36461136633251	-47.5457066255903	115044
8dbc084473722ee4cb1d9d7cfd240723ad023eef	crowd target positioning under multiple cameras based on block correspondence		In the research of crowd analysis in a multi-camera environment, the key problem is how to get target correspondence between cameras. Two main popular methods are epipolar geometric constraint and homography matrix constraint. For large view-angle and wide baseline, these two methods exist obvious disadvantages and have a low performance. The paper utilizes a new correspondence algorithm based-on the constraint of line-of-sight for the crowd positioning. Since the target area is discrete, the paper proposes to use blocking policy: dividing the target regions into blocks with certain size. The approach may provide appropriate redundancy information for each object and decrease the risk of objects missing which is caused by large view-angle and wide baseline between different perspective images. The experimental results show that the method has a high accuracy and a lower computational complexity.	global positioning system	Qiuyu Zhu;Sai Yuan;Bo Chen;Guowei Wang;Jianzhong Xu;Lijun Zhang	2014		10.1007/978-3-319-07788-8_47	computer vision;simulation;communication	Vision	50.39809735665557	-44.6260773430765	115090
74953f3ec4fdcc1d8a0a45f3f0f84eaa003ae4c3	online photometric calibration of auto exposure video for realtime visual odometry and slam	calibration;cameras;tracking;feature extraction;simultaneous localization and mapping;optimization	Recent direct visual odometry and SLAM algorithms have demonstrated impressive levels of precision. However, they require a photometric camera calibration in order to achieve competitive results. Hence, the respective algorithm cannot be directly applied to an off-the-shelf-camera or to a video sequence acquired with an unknown camera. In this letter, we propose a method for online photometric calibration that enables to process auto exposure videos with visual odometry precisions that are on par with those of photometrically calibrated videos. Our algorithm recovers the exposure times of consecutive frames, the camera response function, and the attenuation factors of the sensor irradiance due to vignetting. Gain robust Kanade-Lucas-Tomasi (KLT) feature tracks are used to obtain scene point correspondences as input to a nonlinear optimization framework. We show that our approach can reliably calibrate arbitrary video sequences by evaluating it on datasets for which full photometric ground truth is available. We further show that our calibration can improve the performance of a state-of-the-art direct visual odometry method that works solely on pixel intensities, calibrating for photometric parameters in an online fashion in realtime.	algorithm;color mapping;frequency response;ground truth;kanade–lucas–tomasi feature tracker;mathematical optimization;nonlinear programming;nonlinear system;pixel;simultaneous localization and mapping;tomasi–kanade factorization;visual odometry	Paul Bergmann;Rui Wang;Daniel Cremers	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2017.2777002	pixel;photometry (optics);camera resectioning;pattern recognition;attenuation;computer science;odometry;computer vision;artificial intelligence;ground truth;visual odometry;vignetting	Vision	53.72500226974961	-48.77552904604787	115200
78216cd51e6e1cc014b83e27e7e78631ad44b899	tracking facial features under occlusions and recognizing facial expressions in sign language	bayesian framework;belief networks;feedback mechanism;kanade lucas tomasi;update scheme;facial expression recognition;sign language recognition;image motion analysis;facial feature tracking;probability;probabilistic principal component analysis;subspace learning;facial features face recognition handicapped aids tracking shape bayesian methods feedback karhunen loeve transforms principal component analysis neural networks;american sign language;sign language;bayes methods;head motion;distance measurement;artificial neural networks;handicapped aids;face recognition;hidden markov models;temporal visual cue;shape;kanade lucas tomasi tracker;machine learning;feature extraction;principal component analysis;visual cues;facial features;face;probability bayes methods belief networks face recognition feature extraction gesture recognition handicapped aids image motion analysis learning artificial intelligence principal component analysis;facial expression;learning artificial intelligence;gesture recognition;head motion facial feature tracking facial expression recognition sign language recognition temporal visual cue bayesian framework feedback mechanism kanade lucas tomasi tracker probabilistic principal component analysis machine learning update scheme temporary facial occlusion;tracking;temporary facial occlusion	This paper presents work towards recognizing facial expressions that are used in sign language recognition. Facial features are tracked to effectively capture temporal visual cues on the signer's face during signing. A Bayesian framework is proposed as a feedback mechanism to the Kanade-Lucas-Tomasi (KLT) tracker for reliably tracking facial features in the presence of head motions and temporary occlusions by hand. This mechanism relies on a set of face shape subspaces learned by probabilistic principal component analysis with an update scheme to adapt to persons with different face shapes. The results show that the proposed tracker can track facial features with large head motions, substantial facial deformations, and temporary facial occlusions by hand. The tracked results were input to a recognition system comprising HMMs and a NN to recognize four common American sign language facial expressions.	bayesian network;bayesian programming;digital signature;feedback;hidden markov model;kanade–lucas–tomasi feature tracker;principal component analysis;tomasi–kanade factorization	Tan Dat Nguyen;Surendra Ranganath	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813464	computer vision;speech recognition;computer science;pattern recognition;face hallucination	Vision	39.812828743075706	-49.364554954031625	115494
c2fd9915afa5a1b71697fd4a72df9b0a4f398f50	model-based object tracking using stereo vision	histograms;monocular vision;object recognition;image segmentation;target tracking stereo image processing computer vision image segmentation image reconstruction object recognition image sequences;image sensors;noise robustness;clusterization;range image segmentation;stereo vision image segmentation visual servoing object recognition image reconstruction image sequences clustering algorithms histograms noise robustness image sensors;computer vision;histogram;image reconstruction;clusterization object tracking stereo vision object recognition visual servoing image reconstruction image segmentation image sequences condensation algorithm histogram;object tracking;image sequence;stereo image processing;stereo vision;condensation algorithm;clustering algorithms;target tracking;visual servoing;region growing;image sequences	Robust and accurate object recognition and tracking is a necessary pre-requisite for visual servoing tasks. While most approaches use monocular vision in order to solve this problem, this paper presents a new approach using disparity images provided by a stereo vision system from which 3-D models are reconstructed. We use a segmentation procedure based on a simple region growing process to segment the non-dense 3-D images and we compare it to a classical dense range image segmenter. Obtained regions are classified according to a simple model of the target object and multiple candidates are handled over image sequences by application of the CONDENSATION algorithm. We then use a histogram based clusterization technique to identify the target object pose for visual servoing commands. Experimental results in localizing a door-handle show an accuracy of about 10 cm for the estimated position, while being robust to clutter and noise in the sensor images.	algorithm;binocular disparity;clutter;computer vision;experiment;image segmentation;internationalization and localization;iteration;map;outline of object recognition;range imaging;region growing;stereo camera;stereo cameras;stereopsis;time complexity;visual servoing	Romuald Ginhoux;Jens-Steffen Gutmann	2001		10.1109/ROBOT.2001.932778	computer vision;computer science;machine learning;pattern recognition;histogram	Vision	46.97292297704823	-49.660113583453835	115569
8b1ec1313312c6edb176532f781d6e565ec3857f	motion propagation detection association for multi-target tracking in wide area aerial surveillance	target tracking trajectory vehicles videos linear programming optimization;trajectory;linear programming;aerial video sequence motion propagation detection association multi target tracking wide area aerial surveillance propagating motion information sliding temporal window optimal short track dynamic programming background subtraction motion detection;optimization;vehicles;target tracking;target tracking dynamic programming motion estimation;videos	We propose a novel approach to track multiple targets with weak appearance in low frame rate wide area aerial videos. In real world scenarios, non-linear motion such as sharp turns after slowing down or U-shape trajectories occur. Performing accurate matching without introducing undesired trajectories is very challenging. To tackle various motion patterns, we sequentially optimizing an objective function and propagating motion information at each time step in a sliding temporal window. We show how to exploit an optimal short track (tracklet) for each detection in the first frame of each window using dynamic programming. Tracklets obtained in the window are then associated with existing tracks iteratively to form final tracks. We reduce false alarms in background subtraction motion detection with the aid of optical flow. Our system is tested on two challenging datasets. The quantitative evaluation on a long annotated aerial video sequence shows that the proposed approach outperforms state-of-the-art detection and tracking methods in all common axes of evaluation metrics.	aerial photography;aerial video;association rule learning;background subtraction;dynamic programming;loss function;nonlinear system;optical flow;optimization problem;recursion;sensor;software propagation;transmitter;wide area augmentation system	Bor-Jeng Chen;Gérard G. Medioni	2015	2015 12th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2015.7301766	computer vision;simulation;linear programming;trajectory;motion estimation;computer graphics (images)	Vision	47.74657715149727	-47.320422896210395	115617
cf421c1aff997a2041217980f59906f08f13e229	robust visual tracking via coupled randomness		Tracking algorithms for arbitrary objects are widely researched in the field of computer vision. At the beginning, an initialized bounding box is given as the input. After that, the algorithms are required to track the objective in the later frames on-the-fly. Tracking-by-detection is one of the main research branches of online tracking. However, there still exist two issues in order to improve the performance. 1) The limited processing time requires the model to extract low-dimensional and discriminative features from the training samples. 2) The model is required to be able to balance both the prior and new objectives’ appearance information in order to maintain the relocation ability and avoid the drifting problem. In this paper, we propose a real-time tracking algorithm called coupled randomness tracking (CRT) which focuses on dealing with these two issues. One randomness represents random projection, and the other randomness represents online random forests (ORFs). In CRT, the grayscale feature is compressed by a sparse measurement matrix, and ORFs are used to train the sample sequence online. During the training procedure, we introduce a tree discarding strategy which helps the ORFs to adapt fast appearance changes caused by illumination, occlusion, etc. Our method can constantly adapt to the objective’s latest appearance changes while keeping the prior appearance information. The experimental results show that our algorithm performs robustly with many publicly available benchmark videos and outperforms several state-of-the-art algorithms. Additionally, our algorithm can be easily utilized into a parallel program. key words: online tracking, feature compression, online random forests	algorithm;benchmark (computing);cathode ray tube;computer vision;grayscale;minimum bounding box;random forest;random projection;randomness;real-time clock;relocation (computing);sparse matrix	Chao Zhang;Yo Yamagata;Takuya Akashi	2015	IEICE Transactions		computer vision;computer science;machine learning;pattern recognition	Vision	42.44790109975931	-48.43543776967873	115631
14542bebb519de0982455b5bb5579030562b2e08	a fast object segmentation method for mobile robots based on improved depth information	color image object segmentation mobile robots depth information object recognition real time performance robustness cluttered environments robot vision object candidate region depth clue depth filtering edge information foreground extraction;robot vision feature extraction image colour analysis image filtering image segmentation mobile robots;object segmentation image segmentation cameras sensors image color analysis filtering color	Due to the rapid development of mobile robots technology, the object recognition is of great practical significance. The real-time performance and robustness of object segmentation in cluttered environments is a considerable problem in robot vision. In this paper, a new object segmentation method using depth information is presented. Firstly, this approach obtains the object candidate region using the depth clue, then accomplished the depth filtering in the object candidate region. Next, the object region is extended to get the better edge information. Finally, the foreground is extracted and the segmentation results is realized on the color image. This method of object segmentation was tested on a real mobile robot platform and the results of experiments confirmed the excellent performance of the proposed method.	algorithm;color image;computer vision;experiment;graphics display resolution;kinect;mobile robot;outline of object recognition;pixel;range imaging;real-time clock;real-time computing	Hong Liang;Fan Xu;Yanlin Ji;Chengpeng Du;Sihao Deng;Chunnian Zeng	2015	2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2015.7418922	computer vision;machine learning;segmentation-based object categorization;region growing;image segmentation;scale-space segmentation;computer graphics (images)	Robotics	45.974002027882314	-44.16583182275572	115735
6efb923dd06f383ac7d764f6df02ff663b7b8402	early prediction of a pedestrian's trajectory at intersections	video signal processing gait analysis object detection pedestrians prediction theory risk management road vehicles;intersections;video signal processing;risk analysis;acceleration welding trajectory green products legged locomotion;risk management;time 2 4 s pedestrian short time trajectory prediction gait initiation public urban intersection 3d triangulation stationary video based marker detection data head detection data piecewise linear model sigmoid model mean prediction error vehicle based collision risk estimation autonomous emergency braking evasive steering distance 26 cm;pedestrians;trajectory;prediction theory;gait analysis;mathematical prediction;object detection;road vehicles	This paper focuses on the early prediction of a pedestrian's short time trajectory in the course of gait initiation at a crosswalk. We present a comprehensive study on trajectories of adults measured at a public urban intersection using 3D triangulation of stationary video-based marker- and head-detection data. Based on the results of this study we propose two models, a piecewise linear model and a sigmoid model, for predicting the trajectory starting at heel-off. At heel-off, the mean prediction error of the absolute walking distance in real intersection scenarios increases from 12 cm to 26 cm for prediction times of 600 ms to 2.4 s, respectively, for the piecewise linear model. The results provide a basis for vehicle based collision risk estimation resulting in a possible warning of the driver, autonomous emergency braking or evasive steering.	autonomous robot;linear model;piecewise linear continuation;sigmoid function;stationary process	Michael Goldhammer;Matthias Gerhard;Stefan Zernetsch;Konrad Doll;Ulrich Brunsmann	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728239	computer vision;simulation;geography;computer security	Visualization	39.886652792492185	-43.78663778645115	115737
98000a4ec8757ca4d907ba2988fd715bdb313e4d	stochastic framework for symmetric affine matching between point sets	least squares approximations;matching matrices;cost function;image matching;point matching problem;source target symmetric property;stochastic framework;iterative methods;total least square;symmetric affine matching;iterative generalized total least square;stochastic processes;image registration;ground truth;stochastic processes image matching image registration iterative methods least squares approximations;iterative generalized total least square stochastic framework symmetric affine matching point matching problem matching cost functions source target symmetric property matching matrices;stochastic processes cost function symmetric matrices biomedical engineering iterative algorithms biomedical computing educational institutions simultaneous localization and mapping least squares methods iterative closest point algorithm;matching cost functions	This paper presents a new approach to obtain symmetry in point matching problem. Here, symmetric matching means the essential property that the choices of source and target should not determine the eventual matching results. Most earlier approaches to achieve symmetric matching have been in deterministic fashions, where symmetry constraints are added into the matching cost functions to impose source-target symmetric property during the matching process. Nevertheless, these modified cost functions cannot generally converge to real ground truth, and further, the perfect source-target symmetry cannot be achieved. Given initial forward and backward matching matrices pair, computed from any reasonable matching strategies, our approach yields perfectly symmetric mapping matrices from a stochastic framework that simultaneously considers the errors underneath the initial matching matrices and the imperfectness of the symmetry constraint. An iterative generalized total least square (GTLS) strategy has been developed such that perfect source-target symmetry is imposed	converge;essence;ground truth;iterative method	Sai Kit Yeung;Pengcheng Shi	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.1080	stochastic process;mathematical optimization;combinatorics;discrete mathematics;ground truth;computer science;image registration;3-dimensional matching;optimal matching;mathematics;iterative method	Vision	51.082237640867916	-51.64671903269737	115850
311887aea8ad1a3223cc4ab58f50608020ec2576	a novel binocular vision system for surveillance application	occlusion;ptz pan tilt zoom camera;stereo rectification;binocular vision system	Visual surveillance system has been well studied in past decades. Because of the conflict between surveillance range and resolution of single-camera system, visual surveillance using multiple cameras has attracted increasing interest in recent years. In multi-camera systems, dualcamera system which contains active camera such as PTZ (pan-tilt-zoom) camera is the simplest and typical one. The superiority of this system lies in that it can obtain multiresolution information and depth information. But as far as we know, few of the dual systems make full use of the two superiorities. In this paper, we propose a new binocular vision system which is composed of two PTZ cameras. Different from other systems, stereo rectification is used to establish correspondence between two image coordinates. Then, both global and detailed image information can be obtained. Furthermore, we use the depth information to help solve the occlusion problem in tracking. The experimental results on indoor surveillance environments have demonstrated the effectiveness and robustness of our system.	binocular disparity;binocular vision;image rectification;pan–tilt–zoom camera;rectifier	Zhigao Cui;Aihua Li	2013	Journal of Multimedia	10.4304/jmm.8.4.307-314	computer stereo vision;smart camera;stereo camera;computer vision;computer graphics (images)	Robotics	50.143996779469276	-45.69676441158936	115911
3c5dba1e8455f397cbbff3132a175a3331c09ac1	people localization in a camera network combining background subtraction and scene-aware human detection	video surveillance;scene aware human detection;people localization;human localization;convex optimization;three dimensional;probabilistic occupancy map;background subtraction;multi camera surveillance;camera calibration;camera network	In a network of cameras, people localization is an important issue. Traditional methods utilize camera calibration and combine results of background subtraction in different views to locate people in the three dimensional space. Previous methods usually solve the localization problem iteratively based on background subtraction results, and high-level image information is neglected. In order to fully exploit the image information, we suggest incorporating human detection into multi-camera video surveillance. We develop a novel method combining human detection and background subtraction for multi-camera human localization by using convex optimization. This convex optimization problem is independent of the image size. In fact, the problem size only depends on the number of interested locations in ground plane. Experimental results show this combination performs better than background subtraction-based methods and demonstrate the advantage of combining these two types of complementary information.	analysis of algorithms;background subtraction;camera resectioning;closed-circuit television;convex optimization;ground sample distance;high- and low-level;image resolution;internationalization and localization;mathematical optimization;optimization problem;pattern-oriented modeling;sensor;the matrix	Tung-Ying Lee;Tsung-Yu Lin;Szu-Hao Huang;Shang-Hong Lai;Shang-Chih Hung	2011		10.1007/978-3-642-17832-0_15	three-dimensional space;computer vision;convex optimization;camera resectioning;simulation;background subtraction;computer science;machine learning	Vision	49.735750676194456	-47.63536784113191	115990
037e7e2941239cc35782b8023f421799bc71bddc	pedestrian localization and trajectory reconstruction in a surveillance camera network		In this paper, we propose a high accuracy solution for locating pedestrians from video streams in a surveillance camera network. For each camera, we formulate the vision-based localization service as detecting foot-points of pedestrians in the ground plane. We address two critical issues that strongly affect the foot-point's detection results: casting shadows and pruning detection results due to occlusion. For the first issue, we adopt a removing shadow technique based on a learning-based approach. For the second issue, a regression model is proposed to prune the wrong foot-point detection results. The regression model plays a role in estimating the position by using the human factors such as height, width and its ratio. A correlation of the detected foot-points and the results estimated from the regression model is examined. Once a foot-point is missed due to uncorrelated problem, a Kalman filter is deployed to predict the current location. To link the trajectory of the human in the camera network, we base on an observation about the same ground-plane/floor in view of cameras then the transformation between a pair of cameras could be computed offline. In the experiments, a high accuracy performance for locating the pedestrians and a real-time computation are achieved.	closed-circuit television;computation;experiment;hidden surface determination;human factors and ergonomics;kalman filter;multi-user;online and offline;real-time clock;sensor;streaming media	Hai Vu;Van Giap Nguyen;Anh-Tuan Pham;Thanh-Hai Tran	2017		10.1145/3155133.3155149	kalman filter;regression analysis;computer vision;streams;pedestrian;ground plane;correlation;trajectory;artificial intelligence;shadow;computer science	Vision	44.39055616103179	-43.910351224572345	116037
86b9e665b653db8c19d85b71fea677efdaa1370a	building the view graph of a category by exploiting image realism	three dimensional displays shape solid modeling training detectors estimation labeling;detectors;maximum spanning tree view graph image realism camera object geometry 3d plane hypotheses image matching greedy method;training;shape;estimation;three dimensional displays;solid modeling;image matching cameras graph theory greedy algorithms;labeling	We propose a weakly supervised method to arrange images of a given category based on the relative pose between the camera and the object in the scene. Relative poses are points on a sphere centered at the object in a given canonical pose, which we call object viewpoints. Our method builds a graph on this sphere by assigning images with similar viewpoint to the same node and by connecting nodes if they are related by a small rotation. The key idea is to exploit a large unlabeled dataset to validate the likelihood of dominant 3D planes of the object geometry. A number of 3D plane hypotheses are evaluated by applying small 3D rotations to each hypothesis and by measuring how well the deformed images match other images in the dataset. Correct hypotheses will result in deformed images that correspond to plausible views of the object, and thus will likely match well other images in the same category. The identified 3D planes are then used to compute affinities between images related by a change of viewpoint. We then use the affinities to build a view graph via a greedy method and the maximum spanning tree.	file spanning;greedy algorithm;pose (computer vision);spanning tree;supervised learning	Attila Szabó;Andrea Vedaldi;Paolo Favaro	2015	2015 IEEE International Conference on Computer Vision Workshop (ICCVW)	10.1109/ICCVW.2015.109	computer vision;estimation;labeling theory;detector;shape;machine learning;mathematics;geometry;solid modeling;statistics	Vision	46.3638453748602	-51.52775938670766	116048
1b58dc84c5ba18ca1564101fc45a05dfa99a58fd	hybrid particle filter and mean shift tracker with adaptive transition model	image sampling;probability;video signal processing;particle filters particle tracking target tracking iterative algorithms filtering algorithms histograms robustness kernel laboratories predictive models;mean shift;low complexity;tracking filters;adaptive estimation adaptive filters tracking filters video signal processing probability image sampling target tracking;adaptive filters;particle filter;position estimation hybrid particle filter mean shift tracker adaptive state transition model partial occlusions total occlusions multi modal pdf samples local maximum adaptive variances target size estimation;target tracking;adaptive estimation;state transition	We propose a tracking algorithm based on a combination of particle filter and mean shift, and enhanced with a new adaptive state transition model. The particle filter is robust to partial and total occlusions, can deal with multi-modal pdf and can recover lost tracks. However, its complexity dramatically increases with the dimensionality of the sampled pdf. Mean shift has a low complexity, but is unable to deal with multi-modal pdf. To overcome these problems, the proposed tracker first produces a smaller number of samples than the particle filter and then shifts the samples toward a close local maximum using mean shift. The transition model predicts the state based on adaptive variances. Experimental results show that the combined tracker outperforms the particle filter and mean shift in terms of accuracy in estimating the target size and position while generating 80% less samples than the particle filter.	algorithm;maxima and minima;mean shift;modal logic;particle filter;portable document format	Emilio Maggio;Andrea Cavallaro	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415381	adaptive filter;computer vision;kernel adaptive filter;particle filter;mean-shift;computer science;probability;control theory;mathematics;statistics	Vision	46.3237136045675	-47.76898242820879	116235
e57eb4ade28ae72edd09619279e2c38c4791b72c	object segmentation using independent motion detection	neck integrated optics robot kinematics robot sensing systems optical imaging optical sensors;robot sensing systems;neck;robot vision cameras humanoid robots image motion analysis image segmentation object detection;integrated optics;optical imaging;icub humanoid robot object segmentation independent motion detection robot egomotion orientation sensors sparse visual motion detection disparity map;optical sensors;robot kinematics	Independent motion detection aims at identifying elements in the scene whose apparent motion is not due to the robot egomotion. In this work, we propose a method that learns the input-output relationship between the robot motion - described by the position and orientation sensors embedded on the robot - and the sparse visual motion detected by the cameras. We detect independent motion by observing discrepancies (anomalies) between the perceived motion and the motion that is expected given the position and orientation sensors on the robot. We then perform a higher level analysis based on the available disparity map, where we obtain dense profile of the objects moving independently from the robot. We implemented the proposed pipeline on the iCub humanoid robot. In this work, we report a thorough experimental analysis that covers typical laboratory settings, where the effectiveness of the method is demonstrated. The analysis shows in particular the robustness of the method to scene and object variations and to different kinds of robot's movements.	computation;depth map;embedded system;humanoid robot;icub;map;parallax;sensor;sparse approximation;sparse matrix;spatial variability;visual odometry	Sriram Kumar;Francesca Odone;Nicoletta Noceti;Lorenzo Natale	2015	2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)	10.1109/HUMANOIDS.2015.7363537	computer vision;simulation;computer science;humanoid robot;artificial intelligence;optical imaging;mobile robot navigation;robot kinematics;robot calibration	Robotics	51.40442386182986	-40.78648920031381	116264
07fcbae86f7a3ad3ea1cf95178459ee9eaf77cb1	large scale unconstrained open set face database	databases face face recognition cameras lighting protocols face detection;face recognition algorithms large scale unconstrained open set face database operational face recognition scenarios closed set recognition face images natural variability;visual databases face recognition;face recognition;visual databases	This paper addresses large scale, unconstrained, open set face recognition, which exhibits the properties of operational face recognition scenarios. Most of the existing face recognition databases have been designed under controlled conditions or have been constructed from the images collected from the web. Face images collected from the web are less constrained than a mug-shot like collection. However, they lack information about the imaging conditions and have no operational paradigm. In either case, most of the databases and evaluation algorithms have taken the form of “closed set” recognition, in which all testing classes are assumed to be known at training time. A more realistic scenario in face recognition is an “open set,” where limited knowledge is available at training time and unknown classes can be present at test time. The database we provide supports the open set paradigm, which more closely mimics actual usage than classic closed set testing. The database also exhibits the natural variability among the face images such as pose, illumination, scale, expressions, occlusion, etc. Our goal is to provide around 100,000 images of more than 1,000 people. Also, with this paper, we release part 1 of the database, which consists of 6,337 images from 308 subjects. The paper discusses the details of the database followed by the challenges and results of baseline algorithms.	algorithm;baseline (configuration management);brian;database;facial recognition system;gimp;pose (computer vision);programming paradigm;spatial variability	Archana Sapkota;Terrance E. Boult	2013	2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2013.6712756	facial recognition system;computer vision;face detection;object-class detection;computer science;pattern recognition;data mining;three-dimensional face recognition;face recognition grand challenge;3d single-object recognition	Vision	41.039384595590995	-49.530606431009964	116302
fe730d3bb9300a21d980f12bc6e9d79c3772e3be	a new approach to hand tracking and gesture recognition by a new feature type and hmm	camshift method p2 dhmms hand tracking tower method;poles and towers;hand gesture recognition system;p2 dhmms;hidden markov models real time systems handicapped aids poles and towers robustness skin standards development cameras system testing vocabulary;standard camera hand gesture recognition system vietnamese sign language system real time hand tracking module training gesture module gesture recognition module pseudo 2d hidden markov models tower method skin color hand gesture tracking;two dimensions;hidden markov model;sign language;real time;training;hand gesture tracking;tracking gesture recognition hidden markov models image colour analysis natural language processing;vietnamese sign language system;real time hand tracking module;pseudo 2d hidden markov models;accuracy;skin color;tower method;hidden markov models;camshift method;image color analysis;image colour analysis;training gesture module;standard camera;hand tracking;hand gesture recognition;natural language processing;gesture recognition;cameras;tracking;gesture recognition module;real time systems	In this paper, we introduce a hand gesture recognition system to recognize real time gestures in Vietnamese sign language system. In our system, there are three modules: real time hand tracking, training gesture and gesture recognition using pseudo two dimension hidden Markov models (P2-DHMMs). In the hand tracking module, we introduce a new robust algorithm to obtain hand region, called Tower method, and use skin color for hand gesture tracking and recognition. Next, a gesture recognition system is developed, which can reliably recognize single hand gesture on a standard camera. Furthermore, we propose a new feature type in gesture recognition to improve the accuracy of overall system. In the experiments, we have tested our system to vocabulary of 29 gestures in Vietnamese sign language system (VSL), and show the effectiveness of the system and Tower method.	algorithm;experiment;gesture recognition;hidden markov model;markov chain;vocabulary	Pham The Bao;Thanh Binh Nguyen;Tu Duy Khoa	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.276	computer vision;two-dimensional space;speech recognition;sign language;computer science;machine learning;gesture recognition;accuracy and precision;tracking;hidden markov model	Vision	39.60513171484203	-49.00103763711146	116308
22502f1e63a7a04128dd0e2354c29ff334d916d0	a recognition method for overlapped objects using multiple rgb-d sensors		When some objects are overlapped each other, it becomes difficult to recognize. Upper object changes its posture angle and lower object partially hidden by upper object, so the appearance dynamically changes from single object sensing. Some earlier study employs an RGB-D sensor to treat this problem, but its accuracy improvement is unsatisfactory. In this paper we employ two RGB-D sensors to obtain rich object information to handle dynamically appearance changes. After alignment each viewpoint data, we can achieve improvement of recognition for overlapped objects. We also analyze the performance variation against to two sensor alignment, and clarify the tolerance of sensor settings.	poor posture;sensor	Masakazu Morimoto;Mitsuhiro Yukitou	2018	2018 World Automation Congress (WAC)	10.23919/WAC.2018.8430447	artificial intelligence;computer vision;engineering;rgb color model;cognitive neuroscience of visual object recognition	Robotics	43.964610227889764	-46.70399409077135	116347
1956795fea32d4fedbba7e5376ab440ecd9b3bcf	object tracking and segmentation in a closed loop	image segmentation;bayesian inference;environmental conditions;color histogram;object segmentation;object tracking;image sequence;color image	We introduce a new method for integrated tracking and segmentation of a single non-rigid object in an monocular video, captured by a possibly moving camera. A closed-loop interaction between EM-like color-histogram-based tracking and Random Walker-based image segmentation is proposed, which results in reduced tracking drifts and in fine object segmentation. More specifically, pixel-wise spatial and color image cues are fused using Bayesian inference to guide object segmentation. The spatial properties and the appearance of the segmented objects are exploited to initialize the tracking algorithm in the next step, closing the loop between tracking and segmentation. As confirmed by experimental results on a variety of image sequences, the proposed approach efficiently tracks and segments previously unseen objects of varying appearance and shape, under challenging environmental conditions.	amiga walker;bayesian approaches to brain function;closing (morphology);closing the loop;color histogram;color image;image segmentation;pixel;random walker algorithm	Konstantinos E. Papoutsakis;Antonis A. Argyros	2010		10.1007/978-3-642-17289-2_39	color histogram;image texture;computer vision;range segmentation;color image;computer science;machine learning;segmentation-based object categorization;video tracking;pattern recognition;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;bayesian inference	Vision	44.09770903790656	-49.188248387099314	116349
833ad0feab9b44da5994d8d7ebd6bef01df11c79	mcmc-based human tracking with stereo cameras under frequent interaction and occlusion	video surveillance;occlusion;video signal processing;interaction;mcmc based human tracking markov chain monte carlo interaction model occupancy map 3d information security purpose civilian surveillance video sequence occlusion frequent interaction stereo camera;video surveillance image sequences markov processes monte carlo methods security stereo image processing tracking video signal processing;stereo image processing;human;stereo;mcmc;markov processes;security;mcmc human tracking stereo interaction occlusion;monte carlo methods;tracking;humans cameras tracking computational modeling markov processes video sequences bayesian methods;image sequences	Human Tracking in a video sequence is an important task in civilian surveillance. Successful human tracking provides data for security-purposes. However, human tracking in video sequences is always a challenging problem. Due to the rapid changes in shape with irregular motion, typical methods may not have good results, especially under occlusion and interaction. Recently, methods based on multiple cameras have been proposed. However, this requires a high computation cost. In the stereo cameras approach, 3D information is obtained and projected onto an occupancy map. In this paper, we propose an algorithm combining the occlusion and interaction model and Markov Chain Monte Carlo (MCMC) such that humans can be tracked under frequent interaction and occlusion. We have successfully reduced the number of failures by 78%. We present an efficient and effective algorithm under frequent interaction and occlusion.	algorithm;computation;hidden surface determination;markov chain monte carlo;monte carlo method;pixel;realization (probability);stereo camera;stereo cameras;tracking system;vertical blanking interval	Pak-Ming Cheung;Kam-Tim Woo	2012	2012 IEEE Symposium on Computational Intelligence for Security and Defence Applications	10.1109/CISDA.2012.6291515	computer vision;interaction;simulation;markov chain monte carlo;computer science;information security;tracking;markov process;stereophonic sound;statistics;monte carlo method;computer graphics (images)	Vision	45.86106779337216	-48.27192591490054	116406
93769ed9b19f2f4e2560728d485fe2eff91885f5	fusion of laser and visual data for robot motion planning and collision avoidance	vector field histogram;laser scanner;data fusion;robot motion planning;stereo vision;motion planning;collision avoidance;sensor fusion;visual field;3d structure;laser range scanner	In this paper, a method for inferring scene structure information based on both laser and visual data is proposed. Common laser scanners employed in contemporary robotic systems provide accurate range measurements, but only in 2D slices of the environment. On the other hand, vision is capable of providing dense 3D information of the environment. The proposed fusion scheme combines the accuracy of laser sensors with the broad visual fields of cameras toward extracting accurate scene structure information. Data fusion is achieved by validating 3D structure assumptions formed according to 2D range scans of the environment, through the exploitation of visual information. The proposed methodology is applied to robot motion planning and collision avoidance tasks by using a suitably modified version of the vector field histogram algorithm. Experimental results confirm the effectiveness of the proposed methodology.	3d computer graphics;3d scanner;algorithm;calibration (statistics);collision detection;computation;image sensor;mobile robot;motion planning;pixel;real-time clock;stereopsis;stereoscopy;vector field histogram;visual odometry	Haris Baltzakis;Antonis A. Argyros;Panos E. Trahanias	2003	Machine Vision and Applications	10.1007/s00138-003-0133-2	computer vision;structure from motion;simulation;computer science;sensor fusion;computer graphics (images)	Robotics	52.8439409622841	-40.861251983991494	116451
4cdb723adc1be7d78621b60a713db873858e2830	a fully automatic 3d reconstruction method based on images	features matching;image matching;random sampling;texture mapping;fundamental matrix;matrix algebra;feature matching;data mining;delauny triangularization;image texture;reconstruction algorithms image reconstruction robust stability feature extraction layout cameras data mining rendering computer graphics large scale systems computer vision;epipolar geometry;large scale;3d model;image rendering automatic 3d reconstruction method 2d image feature extraction image point match fundamental matrix delauny triangulation texture mapping;delauny triangulation;2d image;solid modelling feature extraction image matching image reconstruction image texture matrix algebra mesh generation rendering computer graphics;three dimensional displays;feature extraction;image reconstruction;random sampling consensus ransac 3d reconstruction epipolar geometry features matching delauny triangularization fundamental matrix;image point match;image rendering;mathematical model;random sampling consensus ransac;automatic 3d reconstruction method;correlation;rendering computer graphics;mesh generation;3d reconstruction;cameras;solid modelling	A fully automatic 3D reconstruction method based on images without manual modeling is presented in this paper. The method extracts 3D information from 2D images directly. Firstly, the feature points must be extracted from the images and then we match these corresponding points. Secondly, to determine the coordinates of 3D points, we should calculate the fundamental matrix, along with the extrinsic and intrinsic parameters of camera, which is to get the essential matrix. Consequently, we can implement the 3D reconstruction of the real scene after establishing the 3D model by Delauny triangulation and texture mapping and rendering in the late stage. Experiments with real images show that the method can reduce the reconstruction time of large-scale virtual scene greatly. It is characterized as efficiency, high stability and robustness.	3d reconstruction	Zetao Jiang;Bina Zheng;Min Wu;Wenhuan Wu	2009		10.1109/CSIE.2009.144	3d reconstruction;iterative reconstruction;image texture;texture mapping;mesh generation;sampling;computer vision;feature extraction;computer science;pattern recognition;mathematical model;fundamental matrix;correlation;epipolar geometry;computer graphics (images)	Vision	52.48359422259421	-50.03625688983859	116491
7a4af841c3342de84cde7b0d422b99b36625de9f	distributed compression and fusion of nonnegative sparse signals for multiple-view object recognition	minimisation;active camera nodes;histograms;minimization;multiple view object recognition;image recognition;object recognition;nonnegative sparse signals;image coding;base stations;data compression;hybrid intelligent systems;fusion;surveillance;fusion scheme;object images;compressive sensing sparse representation distributed object recognition fusion;object images multiple view object recognition visual surveillance intelligent system distributed cameras hybrid sensor modalities sift based visual histograms distributed data compression fusion scheme nonnegative sparse signals random projection sensor modality multi hop protocol active camera nodes minimization berkeley citric camera motes feature extraction;visual surveillance;visualization;spread spectrum communication;target tracking data compression feature extraction image coding image recognition minimisation sensor fusion;hybrid sensor modalities;compressive sensing;feature extraction;berkeley citric camera motes;smart cameras;intelligent system;distributed object recognition;robustness;intelligent networks;sensor fusion;distributed cameras;target tracking;sparse representation;random projection;intelligent sensors;cameras;multi hop protocol;sensor modality;distributed data compression;sift based visual histograms;object recognition intelligent sensors spread spectrum communication surveillance hybrid intelligent systems intelligent networks smart cameras robustness sensor fusion data compression	Visual surveillance in complex urban environments requires an intelligent system to automatically track and identify multiple objects of interest in a network of distributed cameras. The ability to perform robust object recognition is critical to compensate adverse conditions and improve performance, such as multi-object association, visual occlusion, and data fusion with hybrid sensor modalities. In this paper, we propose an efficient distributed data compression and fusion scheme to encode and transmit SIFT-based visual histograms in a multi-hop network to perform accurate 3-D object recognition. The method harnesses an emerging theory of (distributed) compressive sensing to encode high-dimensional, nonnegative sparse signals via random projection, which is unsupervised and independent to the sensor modality. A multi-hop protocol then transmits the compressed visual data to a base-station computer, which preserves a constant bandwidth regardless of the number of active camera nodes in the network. Finally, the multiple-view object features are simultaneously recovered via ℓ1-minimization as an efficient decoder. The efficacy of the algorithm is validated using up to four Berkeley CITRIC camera motes deployed in a realistic indoor environment. The substantial computation power on the CITRIC mote also enables fast compression of SIFT-type visual features extracted from object images.	algorithm;artificial intelligence;compressed sensing;computation;data compression;encode;modality (human–computer interaction);object composition;outline of object recognition;random projection;scale-invariant feature transform;sensor node;sparse matrix;unsupervised learning;visual basic[.net]	Allen Y. Yang;Subhransu Maji;Kirak Hong;Posu Yan;S. Shankar Sastry	2009	2009 12th International Conference on Information Fusion		computer vision;computer science;machine learning;pattern recognition;visual sensor network	Robotics	46.66547720379473	-38.220792852463035	116580
29f519471f54aaff6efcff6d35483ae22280fe66	3d human tracking with gaussian process annealed particle filter.	human tracking;particle filter;gaussian process	We present an approach for tracking human body parts with prelearned motion models in 3D using multiple cameras. We use an annealed particle filter to track the body parts and a Gaussian Process Dynamical Model in order to reduce the dimensionality of the problem, increase the tracker's stability and learn the motion models. We also present an improvement for the weighting function that helps to its use in occluded scenes. We compare our results to the results achieved by a regular annealed particle filter based tracker and show that our algorithm can track well even for low frame rate sequences.	algorithm;approximation algorithm;dynamical system;gaussian process;hidden surface determination;particle filter;weight function	Leonid M. Raskin;Ehud Rivlin;Michael Rudzsky	2007			particle filter;auxiliary particle filter;computer science;gaussian process;gaussian filter;statistics	Vision	45.547840291297916	-47.75821612838104	116662
08321852ab30cf4d4344c2c30b3c55d119a53c29	real-time recognition of body motion for virtual dance collaboration system	virtual planes;modeling illuminations;computer graphics;real time;luminance patterns;geometrical calibrations;virtual reality;collaborative system;motion templates real time recognition body motion virtual dance collaboration system fourteen feature values principal component analysis motion recognition;ununiform illumination conditions;computer graphic;spherical marker;photo metrical calibrations;multiple camera based 3d object reconstruction;image reconstruction;real time systems motion analysis principal component analysis humans feature extraction data mining motion measurement virtual reality international collaboration animation;location dependent illumination modeling;3d object modeling;bundled adjustment method;virtual reality image motion analysis image recognition principal component analysis;minimize re projection errors;object reconstruction;multiple camera based marker observations;bundle adjustment;solid modelling	A method of real-time recognition of body motion for virtual dance collaboration system is described. Fourteen feature values are extracted from motion captured body motion data, and the dimension of data is reduced by using principal component analysis (PCA). In the training phase, templates for motion recognition are constructed from training samples of several types of motion. In the recognition phase, feature values obtained from a real dancer's motion data are projected to the subspace obtained by PCA, and the system recognizes the real dancer's motion by comparing with the motion templates. In this paper, the method and the experiments using seven kinds of basic motions are presented. The recognition experiment proved that the method could be used for motion recognition. A preliminary experiment in which a real dancer and a virtual dancer collaborate with body motion was also carried out.	experiment;principal component analysis;real-time transcription	Seiya Tsuruta;Yamato Kawauchi;Woong Choi;Kozaburo Hachimura	2007	17th International Conference on Artificial Reality and Telexistence (ICAT 2007)	10.1109/ICAT.2007.37	iterative reconstruction;computer vision;simulation;computer science;virtual reality;bundle adjustment;computer graphics;computer graphics (images)	Robotics	48.03992216623722	-44.978160480228674	116843
4c6ac0e051d2897bc003b6d30037132ec7f82b04	mixture random hypersurface models for tracking multiple extended objects	shape tracking;random hypersurface model;gaussian processes;radar tracking;shape target tracking vectors noise measurement shape measurement radar tracking noise;shape measurement;target tracking gaussian processes;noise measurement;shape;vectors;random hypersurface model multiple extended object tracking shape tracking;multiple extended target tracking mixture random hypersurface model multiple extended object single extended object shape boundaries gaussian assumed bayesian tracking;object tracking;target tracking;multiple extended object tracking;noise	This paper presents a novel method for tracking multiple extended objects. The shape of a single extended object is modeled with a recently developed approach called Random Hypersurface Model (RHM) that assumes a varying number of measurement sources to lie on scaled versions of the shape boundaries. This approach is extended by introducing a so-called Mixture Random Hypersurface Model (Mixture RHM), which allows for modeling multiple extended targets. Based on this model, a Gaussian-assumed Bayesian tracking method that provides the means to track and estimate shapes of multiple extended targets is derived. Simulations demonstrate the performance of the new approach.	algorithm;cluster analysis;clutter;computer simulation;gaussian blur;mixture model;prospective search;wave packet	Marcus Baum;Benjamin Noack;Uwe D. Hanebeck	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6161522	computer vision;mathematical optimization;radar tracker;shape;noise measurement;noise;video tracking;gaussian process;mathematics;geometry	Vision	45.67509076013636	-48.97013124041289	117074
93459de6ac6a6511ffdd0fa87c24bb6065d5a419	remarks on markerless human motion capture from voxel reconstruction with simple human model	human silhouette images extraction;three dimensional displays pixel image reconstruction humans cameras image color analysis solid modeling;estimation method;real time;client server systems;human body posture estimation method;voxel 3d reconstruction;pose estimation client server systems feature extraction image reconstruction;three dimensional displays;image color analysis;feature extraction;image reconstruction;human motion;human body;pixel;solid modeling;server client system;multicamera images;humans;knee joint;markerless human motion capture;3d reconstruction;cameras;voxel 3d reconstruction markerless human motion capture human body posture estimation method human silhouette images extraction multicamera images server client system;pose estimation	This paper investigates a human body posture estimation method based on the back projection of human silhouette images extracted from multi-camera images. The multi-camera system is based on a server-client system with local network of 1000 Base-T to achieve a voxel 3D reconstruction of human body posture in real-time. In order to extract significant points of the human body such as head, neck, shoulders, elbow joints, hands, waist, knee joints, and toes in 3D, an articulated cylindrical human model is applied to the voxel reconstruction of human body. To evaluate the proposed human body posture estimation method, 3D reconstruction experiments of human body posture and extraction experiments of human bodypsilas significant points are carried out. The system runs in real time (9 frames/sec with 50 times 50 times 50 voxel resolution) and the experimental results confirm both the feasibility and effectiveness of the proposed system in 3D human body posture estimation.	3d reconstruction;approximation algorithm;charge-coupled device;experiment;experimental system;gesture recognition;kinesiology;motion capture;personal computer;poor posture;real-time clock;server (computing);voxel	Kazuhiko Takahashi;Yusuke Nagasawa;Masafumi Hashimoto	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4650577	3d reconstruction;iterative reconstruction;computer vision;human body;simulation;pose;feature extraction;computer science;solid modeling;pixel;computer graphics (images)	Robotics	48.57609167382649	-44.83162901220356	117165
9baa302e3ea97d49f68ab265d672d4c1e9f8316f	unsupervised abnormality detection in video surveillance	video surveillance;prior knowledge;higher order;subspace method	The detection of abnormal movements is an important problem in video surveillance applications. We propose an unsupervised method for abnormal movement detection in scenes containing multiple persons. Our method uses cubic higher-order local auto-correlation (CHLAC) to extract movement features. We show that the additive property of CHLAC in combination with a linear subspace method is well suited to simplify the learning of normal movements and to detect abnormal movements even in scenes containing multiple persons. One particular advantage of this method is that it does not necessitate the object segmentation and tracking and also any prior knowledge about objects. Some experimental results are shown to exhibit the validity of the method.	autocorrelation;closed-circuit television;cubic function;unsupervised learning;utility functions on indivisible goods	Takuya Nanri;Nobuyuki Otsu	2005			computer vision;higher-order logic;computer science;machine learning;pattern recognition	Vision	39.76992964568988	-49.26178639205577	117166
62d9750adb300cd53fb107b174cb6a07fb8b96b5	using 3d models to recognize 2d faces in the wild	image sampling;image resolution;solid modelling data acquisition face recognition feature extraction image reconstruction image resolution image sampling pose estimation rendering computer graphics;ptz camera;2d rendered probe images 2d face recognition ptz cameras high resolution 3d model rendered face images face pose sampling sparse reconstruction sift feature extraction high speed 3d acquisition systems face recognition system synthetic view generation;face recognition;ptz camera 3d models face recognition;feature extraction;image reconstruction;probes three dimensional displays face recognition solid modeling image recognition image reconstruction feature extraction;rendering computer graphics;3d models;data acquisition;solid modelling;pose estimation	In this paper we consider the problem of face recognition in imagery captured in uncooperative environments using PTZ cameras. For each subject enrolled in the gallery, we acquire a high-resolution 3D model from which we generate a series of rendered face images of varying viewpoint. The result of regularly sampling face pose for all subjects is a redundant basis that over represents each target. To recognize an unknown probe image, we perform a sparse reconstruction of SIFT features extracted from the probe using a basis of SIFT features from the gallery. While directly collecting images over varying pose for all enrolled subjects is prohibitive at enrollment, the use of high speed, 3D acquisition systems allows our face recognition system to quickly acquire a single model, and generate synthetic views offline. Finally we show, using two publicly available datasets, how our approach performs when using rendered gallery images to recognize 2D rendered probe images and 2D probe images acquired using PTZ cameras.	2d computer graphics;3d modeling;concatenation;discriminative model;experiment;facial recognition system;feature vector;image resolution;lasso;online and offline;pan–tilt–zoom camera;pattern recognition;polygonal modeling;sampling (signal processing);scale-invariant feature transform;sparse matrix;synthetic intelligence;visual descriptor	Iacopo Masi;Giuseppe Lisanti;Andrew D. Bagdanov;Pietro Pala;Alberto Del Bimbo	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2013.116	iterative reconstruction;facial recognition system;computer vision;pose;image resolution;feature extraction;computer science;pattern recognition;three-dimensional face recognition;data acquisition;computer graphics (images)	Vision	48.58020571146826	-49.67242359743519	117175
8e5d27309743dfb3a6d2a959f6c94c014febe6e5	siamese network for underwater multiple object tracking		For underwater videos, the performance of object tracking is greatly affected by illumination changes, background disturbances and occlusion. Hence, there is a need to have a robust function that computes image similarity, to accurately track the moving object. In this work, a hybrid model that incorporates the Kalman Filter, a Siamese neural network and a miniature neural network has been developed for object tracking. It was observed that the usage of the Siamese network to compute image similarity significantly improved the robustness of the tracker. Although the model was developed for underwater videos, it was found that it performs well for both underwater and human surveillance videos. A metric has been defined for analyzing detections-to-tracks mapping accuracy. Tracking results have been analyzed using Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP)metrics.	artificial neural network;kalman filter;sensor	M. V. Rahul;Revanur Ambareesh;G. Shobha	2017		10.1145/3055635.3056579	robustness (computer science);machine learning;tracking system;artificial neural network;computer science;video tracking;kalman filter;artificial intelligence;underwater;computer vision	Robotics	47.02228111670305	-41.691242210860274	117255
e5d051868956a981ec4cd1fa102d4ac6d7f58bbe	shooting for smarter motion detection in cameras: improvements for the visual background extractor algorithm using optical flow	heuristic algorithms;computer vision;motion detection;optical imaging;image motion analysis;robustness	Motion detection is one of the hot-and difficult-topics in the field of computer vision and image processing. The visual background extractor (ViBe) algorithm has a high calculation speed and a simple implementation, which is suitable for embedded systems. However, it still has some defects with respect to ghost suppression and dynamic background interferences. In this article, a new motion detection method called optical flow ViBe (OF-ViBe) is proposed, which combines the improved ViBe algorithm and motion information. We not only introduce the speed information for removing the ghost and noise points more accurately but also reduce the dynamic background interferences using direction information. The detection results in different videos demonstrate that our method improves the accuracy and robustness of the ViBe algorithm with little additional time cost.	algorithm;computer vision;embedded system;image processing;optical flow;randomness extractor;vibe;zero suppression	Linlin Zhao;Yimin Chen;Yibo Zou;Qingqun Ye	2017	IEEE Consumer Electronics Magazine	10.1109/MCE.2017.2715521	robustness (computer science);image processing;optical imaging;computer science;motion detection;algorithm;computer vision;extractor;artificial intelligence;optical flow	Vision	44.8607846172309	-44.74109446943494	117324
961faaf35a4376b80384ad033d93126d16eff996	motion observability analysis of the simplified color correlogram for visual tracking	object representation;color histogram;estimation algorithm;object oriented;visual tracking;spatial information	Compared with the color histogram, where the position information of each pixel is ignored, a simplified color correlogram (SCC) representation encodes the spatial information explicitly and enables an estimation algorithm to recover the object orientation. This paper analyzes the capability of the SCC (in a kernel based framework) in detecting and estimating object motion and presents a principled way to obtain motion observable SCCs as object representations to achieve more reliable tracking. Extensive experimental results demonstrate the reliability of the tracking procedure using the proposed algorithm.	algorithm;color histogram;observable;pixel;sensor	Qi Zhao;Hai Tao	2007		10.1007/978-3-540-76386-4_32	color histogram;computer vision;color normalization;eye tracking;computer science;machine learning;pattern recognition;mathematics;spatial analysis;object-oriented programming	Vision	44.917062462223605	-48.34185990367047	117374
31916a01ed87623c8be194cd5f04d987300a108d	a robust occlusion judgment scheme for target tracking under the framework of particle filter		In traffic surveillance system, it is still a challenging issue to track an occluded vehicle continuously and accurately, especially under total occlusion situations. Occlusion judgment is critical in occluded target tracking. An occlusion judgment scheme with joint parameters is proposed for target tracking method based on particle filter. A corner matching method is utilized to improve the accuracy of target position and velocity estimation due to structure information, thus obtain a more accurate weight value which can reflect the real target’s status. By analyzing the internal relation between the weight value and the particles distribution region based on resample function of particle filter, a new parameter with good performance is proposed to improve the occlusion detection efficiency.		Kejia Liu;Bin Liu;Nenghai Yu	2015		10.1007/978-3-319-21978-3_37	computer vision;pattern recognition;control theory	Vision	43.99872444932505	-48.3079177221053	117378
2a4c4ed0b6ad6970dd3d1b58da7671aac754d2fa	experts-shift: learning active spatial classification experts for keyframe-based video segmentation	learning active spatial classification experts;generative modeling;image segmentation;generic model;video signal processing;image classifiers;experts shift;image classification;video sequences;video segmentation;joints;color model;image color analysis image segmentation pixel joints video sequences computer vision;mcmc sampler;learning activities;discriminative learning experts shift learning active spatial classification experts keyframe based video segmentation color models probability mixture model coupling image classifiers mcmc sampler generative modeling;computer vision;probability mixture model coupling;color models;mixture model;image color analysis;image colour analysis;discrimination learning;pixel;relational model;discriminative learning;markov processes;learning artificial intelligence;video signal processing image classification image colour analysis image segmentation learning artificial intelligence markov processes monte carlo methods;spatial configuration;monte carlo methods;keyframe based video segmentation	Experts-Shift is a novel statistical framework for keyframe-based video segmentation. Compared to existing video segmentation techniques with simple color models, our method proposes a probability mixture model coupling strong image classifiers (experts) with latent spatial configuration. In order to propagate image labels to the successive frames, our algorithm track all experts jointly by a efficient MCMC sampler with their relations modeled by MRFs. This algorithm is capable to handle overlapping color distribution, ambiguous image boundaries, large displacement in challenging scenario with a solid foundation of both generative modeling and discriminative learning. Experiment shows our algorithm achieves high quality results and need less supervision than previous work.	algorithm;displacement mapping;display resolution;experiment;generative modelling language;key frame;markov chain monte carlo;mixture model;sampling (signal processing)	Yibiao Zhao;Yanbiao Duan;Xiaohan Nie;Yaping Huang;Siwei Luo	2011	2011 IEEE Workshop on Applications of Computer Vision (WACV)	10.1109/WACV.2011.5711562	computer vision;color model;computer science;machine learning;pattern recognition;scale-space segmentation;statistics	Vision	44.965541576716205	-51.060734974388275	117600
0b4d2ea8f0235aba98cdc167a7a04d8926c03234	an approach for 2d visual occupancy grid map using monocular vision	monocular vision;image segmentation;visual occupancy grid maps	This paper presents an approach that uses planar information (homography matrix) to build a visual 2D occupancy grid map from monocular vision. Initially, a segmentation step is necessary to classify parts of the image in floor or non floor. From this classification is possible to determine which parts of the image are free and which parts of the image are obstacles. Practical results are presented to validate the proposal.	algorithm;artificial neural network;computation;experiment;homography (computer vision);neural network software;rapid application development;service robot;time complexity	Andre M. Santana;Kelson Rômulo Teixeira Aires;Rodrigo M. S. Veras;Adelardo A. D. Medeiros	2011	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2011.11.033	computer vision;computer science;monocular vision;image segmentation;computer graphics (images)	Robotics	50.01155944162886	-39.997039808201514	117620
596760d5743f78220804231cc9691945bcbc501a	geometric direct search algorithms for image registration	transformation group image registration mutual information nelder mead optimization;optimisation;convergence;algoritmo busqueda;rigid body;image processing;optimizacion;transformation group;convergence property geometric direct search algorithm image registration linear transformation mutual information maximization nelder mead optimization algorithm coordinate based algorithm;algorithme recherche;search algorithm;computational geometry;procesamiento imagen;informacion mutual;traitement image;search problems computational geometry convergence image registration optimisation;registro imagen;information mutuelle;recalage image;grupo transformacion;nelder mead optimization algorithm;image registration;transformation lineaire;joining processes;linear transformation;nelder mead;mutual information maximization;mutual information;geometric algorithm;coordinate based algorithm;optimization;convergence property;geometric direct search algorithm;search problems;image registration mutual information intelligent robots robot kinematics convergence of numerical methods image fusion fitting robot sensing systems orbital robotics mechanical engineering;direct search;computational efficiency;optimal algorithm;algorithms artificial intelligence image enhancement image interpretation computer assisted imaging three dimensional pattern recognition automated reproducibility of results sensitivity and specificity subtraction technique;groupe transformation;algorithm design and analysis;geometric structure;transformacion lineal	A widely used approach to image registration involves finding the general linear transformation that maximizes the mutual information between two images, with the transformation being rigid-body [i.e., belonging to SE(3)] or volume-preserving [i.e., belonging to SL(3)]. In this paper, we present coordinate-invariant, geometric versions of the Nelder-Mead optimization algorithm on the groups SL(3), SE(3), and their various subgroups, that are applicable to a wide class of image registration problems. Because the algorithms respect the geometric structure of the underlying groups, they are numerically more stable, and exhibit better convergence properties than existing local coordinate-based algorithms. Experimental results demonstrate the improved convergence properties of our geometric algorithms.	algorithm;convergence (action);image registration;mathematical optimization;measure-preserving dynamical system;muscle rigidity;mutual information;nelder–mead method;numerical analysis;numerical stability;version;registration - actclass	Seok Lee;Minseok Choi;Hyungmin Kim;Frank Chongwoo Park	2007	IEEE Transactions on Image Processing	10.1109/TIP.2007.901809	algorithm design;computer vision;mathematical optimization;rigid body;convergence;geometric transformation;image processing;computational geometry;computer science;image registration;theoretical computer science;machine learning;mathematics;linear map;mutual information;nelder–mead method;algorithm;search algorithm	Vision	50.83743106770884	-51.990176085996865	117975
135554c22c9527089a24185fe63e8d402092320b	augmented reality in large environments: application to aided navigation in urban context	camera localization augmented reality urban context vision based localization;image sensors;computer vision;image sensors augmented reality computer vision;three dimensional displays cities and towns databases image reconstruction solid modeling augmented reality cameras;augmented reality	This paper addresses the challenging issue of vision-based localization in urban context. It briefly describes our contributions in large environments modeling and accurate camera localization. The efficiency of the resulting system is illustrated through Augmented Reality results on large trajectory of several hundred meters.	3d city models;augmented reality;online and offline	Vincent Gay-Bellile;Pierre Lothe;Steve Bourgeois;Eric Royer;Sylvie Naudet-Collette	2010	2010 IEEE International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2010.5643579	computer vision;augmented reality;simulation;computer science;image sensor;computer graphics (images)	Visualization	52.71005420830014	-43.09950536648755	117985
e5ef7b1a983a272ef45449c9969f3a410a5c4437	stereo vision with arbitrary camera arrangement and with camera calibration	camera calibration;stereo vision		camera resectioning;stereopsis	Hiroshi Kano;Takeo Kanade	1998	Systems and Computers in Japan	10.1002/(SICI)1520-684X(199802)29:2%3C47::AID-SCJ6%3E3.0.CO;2-Q	computer stereo vision;stereo cameras;stereo camera;computer vision;camera auto-calibration;camera matrix;camera resectioning;computer science;stereopsis;pinhole camera model;epipolar geometry	Vision	52.24091058618984	-42.13628838307871	118099
0e33bb2ed974c33e3e17a7464705610d85a5e74f	joint subspace learning for reconstruction of 3d facial dynamic expression from single image	training;two dimensional displays;joints;shape;three dimensional displays;image reconstruction;principal component analysis	Recently, the synthesis of 3D dynamic expressions has become an important concern in computer graphics, facial recognition, etc. In this study, we propose a regression based joint subspace learning method for the automatic synthesis of 3D dynamic expression images. This method synthesizes 3D dynamic expression images from a single 2D facial image. We use two subspaces (the view subspace and the frame subspace) to synthesize a 3D image. First, we use the view subspace to estimate multi-view facial images from a front image. Next, we construct a 3D image using the estimated multi-view facial images. Finally, we estimate the 3D images in different frames by using the frame subspace to synthesis 3D dynamic expression images. This approach is unlike the conventional joint subspace learning in which, the coefficients estimated by the input image are directly used for synthesis. Furthermore, we propose using textural information to improve the accuracy of synthesized images.	autostereogram;coefficient;computer graphics;facial recognition system;frame language;image;stereoscopy	Masataka Seo;Yen-Wei Chen	2016	2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2016.7852823	iterative reconstruction;computer vision;shape;computer science;machine learning;pattern recognition;mathematics;principal component analysis	Vision	47.54579807101846	-51.64112309959096	118267
26919156cec1cc5bec03f63f566c934b55b682cd	from pictorial structures to deformable structures	probability;torso;training;deformable models;joints;probability image representation;shape joints deformable models training torso solid modeling vectors;nonparametric belief propagation pictorial structures deformable structures probabilistic model 2d articulated objects ps models pairwise constraints nonrigid articulated objects image likelihood models 2d projections human poses;shape;vectors;image representation;solid modeling;abt black	Pictorial Structures (PS) define a probabilistic model of 2D articulated objects in images. Typical PS models assume an object can be represented by a set of rigid parts connected with pairwise constraints that define the prior probability of part configurations. These models are widely used to represent non-rigid articulated objects such as humans and animals despite the fact that such objects have parts that deform non-rigidly. Here we define a new Deformable Structures (DS) model that is a natural extension of previous PS models and that captures the non-rigid shape deformation of the parts. Each part in a DS model is represented by a low-dimensional shape deformation space and pairwise potentials between parts capture how the shape varies with pose and the shape of neighboring parts. A key advantage of such a model is that it more accurately models object boundaries. This enables image likelihood models that are more discriminative than previous PS likelihoods. This likelihood is learned using training imagery annotated using a DS “puppet.” We focus on a human DS model learned from 2D projections of a realistic 3D human body model and use it to infer human poses in images using a form of non-parametric belief propagation.	belief propagation;casio loopy;generative model;human–computer interaction;image;shape context;software propagation;statistical model	Silvia Zuffi;Oren Freifeld;Michael J. Black	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6248098	active shape model;computer vision;torso;shape;machine learning;pattern recognition;probability;mathematics;geometry;solid modeling;statistics	Vision	46.216543130241256	-51.59483999218515	118380
a0383e35693b6f2d75d5ea8dc23bda35d7a690a7	depth errors analysis and correction for time-of-flight (tof) cameras	tecnologia electronica telecomunicaciones;particle filter;error correction;error modeling;svm;tecnologias;tof camera;depth error	Time-of-Flight (ToF) cameras, a technology which has developed rapidly in recent years, are 3D imaging sensors providing a depth image as well as an amplitude image with a high frame rate. As a ToF camera is limited by the imaging conditions and external environment, its captured data are always subject to certain errors. This paper analyzes the influence of typical external distractions including material, color, distance, lighting, etc. on the depth error of ToF cameras. Our experiments indicated that factors such as lighting, color, material, and distance could cause different influences on the depth error of ToF cameras. However, since the forms of errors are uncertain, it's difficult to summarize them in a unified law. To further improve the measurement accuracy, this paper proposes an error correction method based on Particle Filter-Support Vector Machine (PF-SVM). Moreover, the experiment results showed that this method can effectively reduce the depth error of ToF cameras to 4.6 mm within its full measurement range (0.5-5 m).	3d reconstruction;color;experiment;magnetic resonance imaging;particle filter;peterson's algorithm;support vector machine;time-of-flight camera;error correction;sensor (device)	Ying He;Bin Liang;Yu Zou;Jin He;Jun Yang	2017		10.3390/s17010092	support vector machine;computer vision;simulation;error detection and correction;particle filter;computer science;computer graphics (images)	Vision	47.17463821430539	-41.878580333272225	118511
25f3c8c029a17fb813017644f3e8c5d97dd9ed65	local difference binary for ultrafast and distinctive feature description	databases;detectors;object recognition;robustness training real time systems face training data detectors databases;augmented reality binary feature descriptor mobile devices object recognition tracking;training;tracking tasks local difference binary distinctive feature description ultrafast feature description computer vision binary descriptor ldb binary string image patch gradient difference tests intensity tests pairwise grid cells multiple gridding strategy salient bit selection method mobile object recognition;computer vision;training data;object tracking;robustness;face;augmented reality;object tracking computer vision object recognition;mobile devices;binary feature descriptor;tracking;real time systems	The efficiency and quality of a feature descriptor are critical to the user experience of many computer vision applications. However, the existing descriptors are either too computationally expensive to achieve real-time performance, or not sufficiently distinctive to identify correct matches from a large database with various transformations. In this paper, we propose a highly efficient and distinctive binary descriptor, called local difference binary (LDB). LDB directly computes a binary string for an image patch using simple intensity and gradient difference tests on pairwise grid cells within the patch. A multiple-gridding strategy and a salient bit-selection method are applied to capture the distinct patterns of the patch at different spatial granularities. Experimental results demonstrate that compared to the existing state-of-the-art binary descriptors, primarily designed for speed, LDB has similar construction efficiency, while achieving a greater accuracy and faster speed for mobile object recognition and tracking tasks.	adaboost;analysis of algorithms;benchmark (computing);computation (action);computer vision;gradient;local area networks;mobile app;mobile operating system;name;outline of object recognition;real-time computing;real-time locating system;scalability;user experience;visual descriptor;cell transformation	Xin Yang;Kwang-Ting Cheng	2013	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2013.150	face;computer vision;augmented reality;training set;detector;computer science;cognitive neuroscience of visual object recognition;machine learning;video tracking;pattern recognition;mobile device;tracking;robustness	Vision	40.84735896813353	-50.75618855127891	118619
f4eade51057d108c32d80dacdd3e4e8d9c997e15	5th issue of real-time image processing		It is now about 1 year that the Journal of Real-Time Image Processing (JRTIP) has been launched. So far four volumes have been published, one of which being a special issue on multispectral imaging. After this fifth regular issue, there will be a special issue on ‘‘Markerless Realtime Tracking for Augmented Reality Image Synthesis’’, which will address the growing needs of determining the position, orientation and focal length of cameras in realtime. The papers in this special issue are clustered around the results of the EU-funded collaborative project MATRIS (http://www.ist-matris.org) toward the development of a system to estimate the movement of a camera in real-time, by fusing tracked natural features from video data using unobtrusive inertial motion sensors. It is encouraging to see that the number of submissions has steady grown and we are on our way toward becoming a very successful journal. This is partly due to our attempts to cluster papers in recent and future special issues on subjects that are current hot topics in real-time image and video processing. Experts working in this area can support JRTIP by being guest editors. The fifth issue comprises six original research articles, four of which deal with video and two with image data. There are two papers from the same author team. The first one presents a real-time implementation of 2D to 3D video conversion using compressed video by extracting motion vectors. Depth maps are derived from motion vector maps to enable object-wise depth ordering to synthesize stereo pairs that can be viewed on stereoscopic displays. The second one is on a real-time algorithm that compensates image distortions due to atmospheric turbulence in video sequences conserving the real movements of objects in the video scene unharmed. Video abstracting is an emerging field that demands for advanced shot detection. The third article addresses the shot change detection as illumination changes. Adaptive threshold and derivative measures in a hue-saturation color space is used to segment and obtain video sequences. The fourth article deals with the detection and tracking of human heads and faces in video sequences as used in human–computer interaction and gesture recognition. Article number five addresses a real-time feature in digital still cameras consisting of zoom tracking. Based on two prediction models for trace curve estimation, autoregressive and recurrent neural networks, a more effective zoom tracking solution is introduced. The last article describes a reversible method capable of associating the patient diagnostic embedded in a medical image to achieve safe data transfer simultaneously deploying lossless compression, data hiding and partial encryption in one single processing step to considerably decrease the computational cost. In this editorial, we would like to bring to your attention the SPIE Conference on Real-Time Image Processing, which is to be held as part of the SPIE Electronic Imaging Symposium in Jan 2008 in San Jose, CA, USA. This is the only conference, which is solely dedicated to the subject of real-time image processing. For more information on this conference, please refer to N. Kehtarnavaz Department of Electrical Engineering, EC33, University of Texas at Dallas, 2601 N. Floyd Rd, Richardson, TX 75080, USA e-mail: kehtar@utdallas.edu	algorithm;algorithmic efficiency;artificial neural network;augmented reality;color space;data compression;distortion;electrical engineering;email;embedded system;encryption;focal (programming language);gesture recognition;human–computer interaction;image processing;lossless compression;map;multispectral image;real-time clock;real-time locating system;real-time transcription;recurrent neural network;richardson number;sensor;shot transition detection;stereoscopy;turbulence;video processing	Nasser Kehtarnavaz;Matthias F. Carlsohn	2007	Journal of Real-Time Image Processing	10.1007/s11554-007-0046-9		Vision	44.03815212766877	-44.840633836473636	118685
e9dd56abd8244dc58206a5dde6a4133310c6586f	multiclient identification system using adaptive probabilistic model	signal image and speech processing;probabilistic model;quantum information technology spintronics;article	This paper aims at integrating detection and identification of human faces in a more practical and real-time face recognition system. The proposed face detection system is based on the cascade Adaboost method to improve the precision and robustness toward unstable surrounding lightings. Our Adaboost method innovates to adjust the environmental lighting conditions by histogram lighting normalization and to accurately locate the face regions by a region-based-clustering process as well. We also address on the problem of multi-scale faces in this paper by using 12 different scales of searching windows and 5 different orientations for each client in pursuit of the multi-view independent face identification. There are majorly two methodological parts in our face identification system, including PCA (principal component analysis) facial feature extraction and adaptive probabilistic model (APM). The structure of our implemented APM with a weighted combination of simple probabilistic functions constructs the likelihood functions by the probabilistic constraint in the similarity measures. In addition, our proposed method can online add a new client and update the information of registered clients due to the constructed APM. The experimental results eventually show the superior performance of our proposed system for both offline and real-time online testing.	adaboost;advanced power management;cluster analysis;control theory;face detection;facial recognition system;feature extraction;microsoft windows;online and offline;principal component analysis;real-time clock;real-time locating system;statistical model	Chin-Teng Lin;Linda Siana;Yu-Wen Shou;Chien-Ting Yang	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/983581	statistical model;computer vision;computer science;artificial intelligence;machine learning;algorithm;statistics	Vision	43.300863121059265	-48.75371486078296	118784
a312599f9a842cf686c6cf80b770e05840d32a5a	mts: a multiple temporal scale tracker handling occlusion and abrupt motion variation		We propose visual tracking over multiple temporal scales to handle occlusion and non-constant target motion. This is achieved by learning motion models from the target history at different temporal scales and applying those over multiple temporal scales in the future. These motion models are learned online in a computationally inexpensive manner. Reliable recovery of tracking after occlusions is achieved by extending the bootstrap particle filter to propagate particles at multiple temporal scales, possibly many frames ahead, guided by these motion models. In terms of the Bayesian tracking, the prior distribution at the current time-step is approximated by a mixture of the most probable modes of several previous posteriors propagated using their respective motion models. This improved and rich prior distribution, formed by the models learned and applied over multiple temporal scales, further makes the proposed method robust to complex target motion through covering relatively large search space with reduced sampling effort. Extensive experiments have been carried out on both publicly available benchmarks and new video sequences. Results reveal that the proposed method successfully handles occlusions and a variety of rapid changes in target motion.	approximation algorithm;benchmark (computing);bootstrapping (statistics);experiment;hidden surface determination;high-level programming language;particle filter;particle swarm optimization;sampling (signal processing);video tracking	Muhammad Haris Khan;Michel F. Valstar;Tony P. Pridmore	2014		10.1007/978-3-319-16814-2_31	computer vision;control theory	Vision	44.75668907375831	-47.89817307838534	118892
867fc60cb9b7cb14ec3019ea6716d45cbcf2b496	an operational system for estimating road traffic information from aerial images	orthorectification;sequences;photogrammetrie und bildanalyse;vehicle detection;monitoring;pattern recognition;georeferencing;tracking	Given that ground stationary infrastructures for traffic monitoring are barely able to handle everyday traffic volumes, there is a risk that they could fail altogether in situations arising from mass events or disasters. In this work, we present an alternative approach for traffic monitoring during disaster and mass events, which is based on an airborne optical sensor system. With this system, optical image sequences are automatically examined on board an aircraft to estimate road traffic information, such as vehicle positions, velocities and driving directions. The traffic information, estimated in real time on board, is immediately downlinked to a ground station. The airborne sensor system consists of a three-head camera system, a real-time-capable GPS/INS unit, five industrial PCs and a downlink unit. The processing chain for automatic extraction of traffic information contains modules for the synchronization of image and navigation data streams, orthorectification and vehicle detection and tracking modules. The vehicle detector is based on a combination of AdaBoost and support vector machine classifiers. Vehicle tracking relies on shape-based matching operators. The processing chain is evaluated on a large number of image sequences recorded during several campaigns, and the data quality is compared to that obtained from induction loops. In summary, we can conclude that the achieved overall quality of the traffic data extracted by the airborne system is in the range of 68% and 81%. Thus, it is comparable to data obtained from stationary ground sensor networks. Remote Sens. 2014, 11 11316	adaboost;aerial photography;airborne ranger;algorithm;capability maturity model;data quality;experiment;franz lisp;global positioning system;image;inter-process communication;internet;machine learning;mathematical induction;mathematical optimization;on-board data handling;operational system;orthophoto;plausibility structure;real-time clock;sensor;simulation;stationary process;support vector machine;telecommunications link;template matching;traffic analysis;vehicle tracking system;website monitoring	Jens Leitloff;Dominik Rosenbaum;Franz Kurz;Oliver Meynberg;Peter Reinartz	2014	Remote Sensing	10.3390/rs61111315	orthophoto;georeference;computer vision;simulation;floating car data;sequence;tracking;physics;remote sensing	Robotics	42.23067019691141	-41.606678180622396	118910
23b18a4824cfca05f9858923bd5dad67f64ba574	motion compensation based fast moving object detection in dynamic background		This paper investigates robust and fast moving object detection in dynamic background. A motion compensation based approach is proposed to maintain an online background model, then the moving objects are detected in a fast fashion. Specifically, the pixel-level background model is built for each pixel, and is represented by a set of pixel values drawn from its location and neighborhoods. Given the background models of previous frame, the edge-preserving optical flow algorithm is employed to estimate the motion of each pixel, followed by propagating their background models to the current frame. Each pixel can be classified as foreground or background pixel according to the compensated background model. Moreover, the compensated background model is updated online by a fast random algorithm to adapt the variation of background. Extensive experiments on collected challenging videos suggest that our method outperforms other state-of-the-art methods, and achieves 8 fps in efficiency.	motion compensation;object detection	Wei Zhang;Chenglong Li;Aihua Zheng;Jin Tang;Bin Luo	2015		10.1007/978-3-662-48570-5_24	pixel;computer vision;motion compensation;object detection;randomized algorithm;artificial intelligence;computer science;optical flow	Vision	42.606999462230995	-48.557938497302835	119019
0f0338574a1e4abac10d0e1dac33e075e2ef7a7c	mirage: an o(n) time analytical solution to 3d camera pose estimation with multi-camera support			3d pose estimation	Semih Dinç;Farbod Fahimi;Ramazan Savas Aygün	2017	Robotica	10.1017/S0263574716000874	computer vision;simulation	Robotics	51.416427885441706	-43.30522949433167	119273
94703417723818b0105489aa4dde9d8373bf5879	an intelligent robot with cognition and decision-making ability	three dimensional objects;object recognition;computer manipulation;assembly drawing;drawing recognition;intelligent robot;three view plan;assembly procedure;polyhedra	An in te l l igent robot that recognizes and assembles three-dimensional objects is described. The instruction to the robot is fed through a vidicon camera as a three-view plan of the assemblage whose overall spatial configuration is then recognized and decomposed into component parts by a computer. Another camera looks at the real world for the specific parts required for the assembly and confirms their geometric features. The computer further makes the decisions on the procedure for the manipulation of the parts, and orders the art iculated mechanical hand to proceed the assembly sequence. The information processing is so far restr icted to simple polyhedral objects and their co-relationships.	cognition;information processing;polyhedron;robot;video camera tube	Masakazu Ejiri;Takeshi Uno;Haruo Yoda;Tatsuo Goto;Kiyoo Takeyasu	1971			computer vision;simulation;cognitive neuroscience of visual object recognition;polyhedron	Robotics	48.203013224591636	-39.18373081672473	119274
64d49dd122ee98f146bdb6799b48695279da50db	a tracking based manipulation system built on stereo vision	moving object;object recognition;manipulators;target coordinates;geometric model representation;event detection;stereo vision target tracking machine vision object recognition target recognition object detection event detection solid modeling robustness robot kinematics;pick and insertion task;recognition;robot vision;optical tracking;target recognition;machine vision;partial occlusion;solid modeling;stereo image processing;stereo vision;pick and insertion task tracking based manipulation system stereo vision visual tracking tracking function recognition geometric model representation partial occlusion short cycle monitoring target coordinates;robustness;geometric model;tracking based manipulation system;target tracking;computerised monitoring;visual tracking;short cycle monitoring;tracking function;object detection;computerised monitoring manipulators robot vision stereo image processing optical tracking;robot kinematics	This paper presents a manipulation system which is based on visual tracking. A tracking function is useful not only for tracking moving objects but also to verify the result of a task and to detect events in various phases of a task. In our system the tracking function is provided by a stereo vision system developed in the ETL. Recognition and tracking procedures are based on a geometric model representation that has generality for a wide range of solid objects. The implemented tracking method is robust against partial occlusion of the tracked object and enables short cycle monitoring of the target coordinates. These features make it possible to build a tracking-based manipulation system. A pick-and-insertion task was executed to show various uses of the tracking function.	geometric modeling;stereopsis;video tracking	Toshio Matsushita;Yasushi Sumi;Yutaka Ishiyama;Fumiaki Tomita	1998		10.1109/IROS.1998.724617	computer vision;finger tracking;simulation;machine vision;tracking system;eye tracking;computer science;stereopsis;artificial intelligence;geometric modeling;cognitive neuroscience of visual object recognition;video tracking;solid modeling;robot kinematics;robustness;computer graphics (images)	Robotics	50.38351151627845	-40.138404031766335	119341
f87ff6b66d4d537a2c076e0016f666387e3dad49	object state recognition for automatic ar-based maintenance guidance	maintenance engineering;visualization;training;histograms;manufacturing;databases;industries	This paper describes a component of an Augmented Reality (AR) based system focused on supporting workers in manufacturing and maintenance industry. Particularly, it describes a component responsible for verification of performed steps. Correct handling is crucial in both manufacturing and maintenance industries and deviations may cause problems in later stages of the production and assembly. The primary aim of such support systems is making the training of new employees faster and more efficient and reducing the error rate. We present a method for automatically recognizing an object's state with the objective of verifying a set of tasks performed by a user. The novelty of our approach is that the system can automatically recognize the state of the object and provide immediate feedback to the operator using an AR visualization enabling fully automatic step-by-step instructions.	algorithm;augmented reality;bit error rate;data pre-processing;futures studies;histogram of oriented gradients;image gradient;region of interest;requirement;verification and validation	Pavel Dvorák;Radovan Josth;Elisabetta Delponte	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.164	operator (computer programming);computer vision;visualization;word error rate;novelty;augmented reality;artificial intelligence;computer science;maintenance engineering;machine learning	Vision	48.4847307221732	-43.76308724469461	119536
069f895322fbf6ce5d2e689541c9ced21ca97e83	laser trail shape identification technique for robot navigation based on genetic programming	image motion analysis;edge detection;path planning;image classification;shape recognition;mobile robots;genetic programming;computer vision;computer aided software engineering;shape genetic programming vectors computer aided software engineering image motion analysis computer vision;robot vision;shape;vectors;image representation;feature extraction;shape recognition edge detection feature extraction genetic algorithms image classification image representation image sequences laser beam applications mobile robots path planning robot vision;genetic algorithms;laser beam applications;shape identification performance laser trail shape identification technique meta heuristic image processing application mobile robot navigation laser pointer optical flow extraction laser beam trail vector representation shape edges geometric characteristics classification algorithm genetic programming parameters;image sequences	This paper proposes a meta-heuristic image processing application for mobile robot navigation. It classifies figures that are drawn on a wall by hand with a laser pointer. Image processing technique extracts optical flow of the laser beam trail, which represents vectors along edges of shapes. Genetic programming learns geometric characteristics of laser trail shapes and creates classification algorithm. Three typical figures, such as a circle, a triangle, and a square, are evaluated and identified in high accuracy. We have investigated the effects of genetic programming parameters on the performance of shape identification. As a result, proposal system makes it possible to command robots by easy and intuitive action of drawing a figure only with a laser pointer.	algorithm;charge-coupled device;experiment;genetic programming;heuristic;image processing;mobile robot;optical flow;pointer (computer programming);robotic mapping	Takeshi Tsujimura;Hiroki Fukushima;Toshihiro Minato;Kiyotaka Izumi	2012	2012 Federated Conference on Computer Science and Information Systems (FedCSIS)		mobile robot;genetic programming;computer vision;contextual image classification;simulation;genetic algorithm;edge detection;feature extraction;shape;computer science;artificial intelligence;machine learning;motion planning;computer-aided software engineering	Robotics	48.85926533401595	-41.096314222334776	119560
71ef8a7ec742a19d0e1b6f76b8378dd5f109d08b	feasibility of hough-transform-based iris localisation for real-time-application	shadow mapping;frames per second;localisation error;localisation error hough transform based iris localisation frontal face images robustness shadowing partial occlusion;hough transform based iris localisation;image classification;testing;medical image processing hough transforms image classification;iris face detection testing robustness motion measurement velocity measurement waveguide discontinuities computer science shadow mapping tracking;medical image processing;partial occlusion;shadowing;hough transforms;robustness;hough transform;frontal face images;waveguide discontinuities;computer science;face detection;iris;motion measurement;velocity measurement;processing speed;real time application;tracking	We present a fast method for locating iris features in frontal face images based on the Hough transform. It consists of an initial iris detection step and a tracking step which uses iris features from initialisation for speeding up computation. The purpose of research was to evaluate the feasibility of the method for tracking at 200 frames per second or higher. Processing speed of the prototypical implementation on a 266Mhz Pentium II PC is approximately 6 seconds for initial iris detection and about 0.05 seconds for each tracking step. The algorithm was applied to images of subjects taken under normal room lighting conditions. Tests showed robustness with respect to shadowing and partial occlusion of the iris. The localisation error was below two pixels. Accuracy for tracking was within one pixel. A reduction of the number of pixels which are processed in the tracking step by 90% showed a modest degradation of the results.	algorithm;computation;elegant degradation;hough transform;pixel;real-time clock	Klaus D. Tönnies;Frank Behrens;Melanie Aurnhammer	2002		10.1109/ICPR.2002.1048486	hough transform;computer vision;contextual image classification;face detection;computer science;tracking;software testing;shadow mapping;frame rate;robustness;computer graphics (images)	Vision	46.120814107301555	-44.73049018561016	119577
a6f7a2c724d1ca438760cec6fceedb124811f750	a non-iterative greedy algorithm for multi-frame point correspondence	greedy algorithms computer vision;optimisation;occlusion;tracking image sequences algorithm theory feature extraction optimisation;image sequence analysis;monocular image sequences;polynomial time algorithm;np hard problem;tracking noniterative greedy algorithm multiframe point correspondence monocular image sequences np hard problem polynomial time algorithm occlusion feature extraction optimisation;algorithm theory;feature extraction;multiframe point correspondence;algorithms;noniterative greedy algorithm;tracking;optimization methods;image sequences	This paper presents a framework for finding point correspondences in monocular image sequences over multiple frames. The general problem of multi-frame point correspondence is NP Hard for three or more frames. A polynomial time algorithm for a restriction of this problem is presented and is used as the basis of the proposed greedy algorithm for the general problem. The greedy nature of the proposed algorithm allows it to be used in real time systems for tracking and surveillance etc. In addition, the proposed algorithm deals with the problems of occlusion, missed detections and false positives by using a single non-iterative greedy optimization scheme, and hence reduces the complexity of the overall algorithm as compared to most existing approaches where multiple heuristics are used for the same purpose. While most greedy algorithms for point tracking do not allow for entry and exit of the points from the scene, this is not a limitation for the proposed algorithm. Experiments with real and synthetic data over a wide range of scenarios and system parameters are presented to validate the claims about the performance of the proposed algorithm.	experiment;greedy algorithm;heuristic (computer science);iterative method;mathematical optimization;np-hardness;p (complexity);sensor;synthetic data	Khurram Shafique;Mubarak Shah	2003		10.1109/ICCV.2003.1238321	computer vision;greedy randomized adaptive search procedure;mathematical optimization;greedy algorithm;ramer–douglas–peucker algorithm;feature extraction;computer science;machine learning;np-hard;mathematics;tracking	Vision	50.15460726934645	-49.64823489771357	119612
b8f6ab05e147811da7038711aebc931d061ccffc	a robust point detection algorithm based on wavelet transform for visual tracking	image resolution;robustness;lighting;target tracking;wavelet coefficients	Visual tracking is one of the hot research topics in computer vision in recent years. It has been widely used in many vision applications, such as traffic surveillance, anti-terrorism. However, there are still challenges for visual tracking, like illumination change, object occlusion, appearance deformation, etc. This paper proposes a robust point detection algorithm based on wavelet transform for visual tracking. First, the input image patch that includes the tracking object is decomposed by wavelet transform with several levels and the wavelet coefficients are obtained. The wavelet coefficients are then analyzed and the points that hold the local maximal wavelet coefficients are determined as the robust points for tracking. Finally, the proposed method is integrated to the Tracking Learning Detection (TLD) framework, which not only improves the tracking precision, but also reduces the false detection. Experimental results showed that the new algorithm outperformed the TLD method with respect to the precision, recall, and f-measure.	algorithm;coefficient;computer vision;f1 score;maximal set;video tracking;wavelet transform	Rongfeng Zhang;Huanhou Xiao;Ting Deng;Wei Qiu;Jinglun Shi	2016	2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2016.7852672	wavelet;computer vision;simulation;image resolution;second-generation wavelet transform;computer science;lighting;cascade algorithm;mathematics;stationary wavelet transform;discrete wavelet transform;lifting scheme;robustness;computer graphics (images)	Vision	42.239854889678675	-49.562785865078496	119658
92f036aaf7d8a8743e305f97a38b86340627794c	feature matching using co-inertia analysis for people tracking	histograms;hog;co inertia;computer vision;mobile communication;correlation;target tracking;physical objects;tracking	Robust object tracking is a challenging computer vision problem due to dynamic changes in object pose, illumination, appearance and occlusions. Tracking objects between frames requires accurate matching of their features. We investigate real time matching of mobile object features for frame to frame tracking. This paper presents a new feature matching approach between objects for tracking that incorporates one of the multi-variate analysis method called Co-Inertia Analysis abbreviated as COIA. This approach is being introduced to compute the similarity between Histogram of Oriented Gradients (HOG) features of the tracked objects. Experiments conducted shows the effectiveness of this approach for mobile object feature tracking.	computer vision;histogram of oriented gradients;image gradient;local binary patterns;motion estimation	Srinidhi Mukanahallipatna Simha;Duc Phu Chau;François Brémond	2014	2014 International Conference on Computer Vision Theory and Applications (VISAPP)	10.5220/0004669502800287	computer vision;simulation;mobile telephony;tracking system;computer science;video tracking;histogram;geometry;tracking;correlation;computer graphics (images)	Vision	42.63592069700071	-48.08892449209481	119944
1126e1f1637be9f771a29b28ed2f77a9b2755e32	vision-based fast and reactive monte-carlo localization	vision system;monte carlo localization;path planning monte carlo methods feature extraction robot vision image processing mobile robots;image processing;path planning;robot sensing systems robot vision systems cameras feature extraction mobile robots image resolution orbital robotics head machine vision stability;mobile robots;robot vision;monte carlo localization vision based self localization robocup vision system feature extraction lighting condition independence;feature extraction;monte carlo methods	This paper presents a fast approach for vision-based self-localization in RoboCup. The vision system extracts the features required for localization without processing the whole image and is a first step towards independence of lighting conditions. In the field of self-localization, some new ideas are added to the well-known MonteCarlo localization approach that increase both stability and reactivity, while keeping the processing time low.	internationalization and localization;monte carlo localization;whole earth 'lectronic link	Thomas Röfer;Matthias Jüngel	2003		10.1109/ROBOT.2003.1241700	mobile robot;monte carlo localization;computer vision;simulation;image processing;feature extraction;computer science;engineering;artificial intelligence;motion planning;monte carlo method;computer graphics (images)	Robotics	51.97763264649848	-39.39561275242565	120020
8de776ea582a651d15babb120ee9b2b96f130186	applying multi level processing for robust geometric lane feature extraction	lane feature extraction;false lane mark detections;complexity theory;lane keeping;state estimation driver information systems feature extraction filtering theory image classification object detection road traffic;road traffic;multilevel feature extraction;image classification;lane departure warning;lane estimation algorithm;solid lane marks;state estimation;geometric feature;estimation algorithm;autonomous driving systems;state estimation filter;image edge detection complexity theory driver circuits;image edge detection;multilevel processing;white lane markers;feature extraction;driver assistance systems;multilevel classification;robust geometric lane feature extraction;false lane mark detections multilevel processing robust geometric lane feature extraction lane feature extraction autonomous driving systems driver assistance systems lane departure warning lane keeping lane estimation algorithm white lane markers dashed lane marks solid lane marks state estimation filter lane mark measurement extraction multilevel feature extraction multilevel classification;driver circuits;lane mark measurement extraction;driver information systems;dashed lane marks;filtering theory;object detection;driver assistance system	Lane feature extraction is a function which is needed for example for autonomous driving and driver assistance systems. For example Lane Departure Warning and Lane Keeping rely on information provided by a lane estimation algorithm. One important step of the lane estimation procedure is the extraction of measurements or detections which can be used to estimate the shape of the road or lane. These detections are generated by white lane markers or the road border itself. Additionally traffic rules can be derived if a system is able to distinguish for example between solid and dashed lane marks as well as between the different types of lane marks itself (length, thickness). Every state estimation filter needs a properly defined model and reliable measurements to work correctly. This paper presents an approach to extract reliable lane mark measurements using multi level feature extraction [1] and classification. Geometric features will be generated for lane mark candidates and used for the distinction between true and false lane mark detections.	algorithm;autonomous car;autonomous robot;estimation theory;feature extraction;multi-storey car park;sensor;thickness (graph theory)	Philipp Lindner;Stephan Blokzyl;Gerd Wanielik;Ulrich Scheunert	2010	2010 IEEE Conference on Multisensor Fusion and Integration	10.1109/MFI.2010.5604446	computer vision;simulation;geography;computer security	Robotics	42.557561918149624	-44.632088553230304	120048
ce22c34fc99881f6e1e932921edef30005b06fad	part-based initialization for hand tracking	hand tracking initialization;silicon;joints silicon estimation thumb tracking indexes least squares approximation;tracking system;degree of freedom;least squares approximation;hand finger articulation;joints;thumb;phalange joint angles;overlapping subsets;indexes;search problems gesture recognition object tracking;estimation;articulation estimator;object tracking;hand tracking;articulation estimator hand tracking initialization hand finger articulation nearest neighbor search hand gestures phalange joint angles overlapping subsets;search problems;nearest neighbor search;hand gestures;gesture recognition;tracking	Initializing hand/finger articulation for tracking is a very challenging problem, mainly because hand articulation is complicated and it has a large number of degrees of freedom. Most existing algorithms initialize tracking manually, or use a nearest-neighbor search with restricting the number of possible hand gestures. This paper presents a new solution to this problem by increasing the dimensionality but taking advantage of the sparseness. The basic idea is to divide the set of phalange joint angles into many overlapping subsets. As each subset has a much smaller number of joint angles, it is much easier to design a smaller-scale articulation estimator. The estimation of the whole hand is done by the collaboration of a network of dependent smaller-scale estimators. This paper describes a novel way of designing the smaller-scale estimators as well as a principled way of fusing the estimates. A tracking system is also shown by using this initialization technique.	algorithm;biconnected component;nearest neighbor search;neural coding;tracking system	Jiang Xu;Ying Wu;Aggelos K. Katsaggelos	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5651179	database index;computer vision;estimation;speech recognition;tracking system;computer science;video tracking;gesture recognition;tracking;nearest neighbor search;degrees of freedom;silicon;least squares;statistics	Robotics	47.9583926478215	-47.67382334571048	120078
71447d7842e8db16ab55858d29fc5c7fb12ec3a1	a hypothesize-and-bound algorithm for simultaneous object classification, pose estimation and 3d reconstruction from a single 2d image	optimal solution;image segmentation;image classification;prior knowledge;joints;three dimensional;graphical models;shape;three dimensional displays;image reconstruction;graphical model;upper and lower bounds;object classification;pose estimation image classification image reconstruction object detection;shape three dimensional displays cameras image reconstruction image segmentation joints graphical models;simultaneous object classification hypothesize and bound algorithm 3d reconstruction 2d image pose estimation mathematical framework 3d shapes h and b algorithm;3d reconstruction;cameras;object detection;pose estimation	We consider the problems of 3D reconstruction, pose estimation and object classification, from a single 2D image. In sharp contrast with state of the art methods that solve each of these problems separately or iteratively, we propose a mathematical framework that solves these problems jointly and simultaneously. Since the joint problem is ill posed unless “prior knowledge” is considered, the proposed framework incorporates “prior knowledge” about the 3D shapes of different object classes. This knowledge is used to define a function L(H) that encodes how well each hypothesis H (object class and pose) “explains” the input image. To efficiently maximize L(H) without having to exactly evaluate it for each hypothesis H, we propose a H&B algorithm that computes and refines upper and lower bounds for L(H) at a much lower cost. In this way suboptimal hypotheses are disregarded with little computation. The resulting algorithm integrates information from the 2D image and the 3D prior, is efficient, and is guaranteed to find the optimal solution.	3d modeling;3d pose estimation;3d reconstruction;3d reconstruction from multiple images;acm siggraph;algorithm;autostereogram;background subtraction;cvpr;categorization;computation;han unification;human dynamics;iccv;image segmentation;information processing;internationalization and localization;jones calculus;linear algebra;machine learning;machine perception;object detection;pattern recognition;simple features;springer (tank);time complexity;variable kernel density estimation;wang tile;well-posed problem;world wide web	Diego Rother;René Vidal	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130292	computer vision;3d pose estimation;computer science;machine learning;pattern recognition;mathematics;graphical model	Vision	46.53450566736153	-51.174462372163894	120101
60a72cdcf433cc25df2b35cf827168f8afb5337f	a generic process chain to extract key-objects from video shots	image segmentation;motion compensation;video cameras image segmentation image sequences motion compensation;key object;video representation key object;indexing terms;motion compensated;data mining cameras motion compensation motion detection fusion power generation shape filtering gunshot detection systems feature extraction clustering methods;video cameras;video representation;motion compensation generic process chain video shots moving camera;image sequences	This paper discusses object-based representation of video shots acquired by a moving camera. Our approach uses an extraction of foreground regions capable of representing semantic objects of interest. However, foreground regions extracted by motion compensation are not always representative of the entity they depict. A filtering and a clustering of these regions allow us to retain only the most representative of each real object in the shot, i.e., the key-object.	cluster analysis;motion compensation;object-based language	Jérémy Huart;Pascal Bertolino	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379582	computer vision;index term;quarter-pixel motion;computer science;video tracking;motion estimation;block-matching algorithm;multimedia;image segmentation;motion field;motion compensation;computer graphics (images)	Vision	39.5566971780004	-51.18950195373431	120282
a642e11125e6a5e87c4da7cd0aac00075499745d	fast smoke detection for video surveillance using cuda		Smoke detection is a key component of disaster and accident detection. Despite the wide variety of smoke detection methods and sensors that have been proposed, none has been able to maintain a high frame rate while improving detection performance. In this paper, a smoke detection method for surveillance cameras is presented that relies on shape features of smoke regions as well as color information. The method takes advantage of the use of a stationary camera by using a background subtraction method to detect changes in the scene. The color of the smoke is used to assess the probability that pixels in the scene belong to a smoke region. Due to the variable density of the smoke, not all pixels of the actual smoke area appear in the foreground mask. These separate pixels are united by morphological operations and connected-component labeling methods. The existence of a smoke region is confirmed by analyzing the roughness of its boundary. The final step of the algorithm is to check the density of edge pixels within a region. Comparison of objects in the current and previous frames is conducted to distinguish fluid smoke regions from rigid moving objects. Some parts of the algorithm were boosted by means of parallel processing using compute unified device architecture graphics processing unit, thereby enabling fast processing of both low-resolution and high-definition videos. The algorithm was tested on multiple video sequences and demonstrated appropriate processing time for a realistic range of frame sizes.	algorithm;background subtraction;cuda;closed-circuit television;color;computer graphics;connected-component labeling;graphics processing unit;hdmi;mathematical morphology;parallel computing;pixel;sensor;stationary process	Alexander Filonenko;Danilo C&#x00E1;ceres Hern&#x00E1;ndez;Kang-Hyun Jo	2018	IEEE Transactions on Industrial Informatics	10.1109/TII.2017.2757457	pixel;frame rate;computer vision;computer science;architecture;real-time computing;graphics processing unit;smoke;artificial intelligence;parallel processing;cuda;background subtraction	Vision	42.65703117500749	-44.61169302216462	120314
1527799b5487a448a82e1b139c8fe7a5bf79d4f1	freeway traffic incident detection from cameras: a semi-supervised learning approach		Early detection of incidents is a key step to reduce incident related congestion. State Department of Transportation (DoTs) usually install a large number of Close Circuit Television (CCTV) cameras in freeways for traffic surveillance. In this study, we used semi-supervised techniques to detect traffic incident trajectories from the cameras. Vehicle trajectories are identified from the cameras using state-of-the-art deep learning based You Look Only Once (YOLOv3) classifier and Simple Online Realtime Tracking (SORT) is used for vehicle tracking. Our proposed approach for trajectory classification is based on semi-supervised parameter estimation using maximum-likelihood (ML) estimation. The ML based Contrastive Pessimistic Likelihood Estimation (CPLE) attempts to identify incident trajectories from the normal trajectories. We compared the performance of CPLE algorithm to traditional semi-supervised techniques Self Learning and Label Spreading, and also to the classification based on the corresponding supervised algorithm. Results show that approximately 14% improvement in trajectory classification can be achieved using the proposed approach.		Pranamesh Chakraborty;Anuj Sharma;Chinmay Hegde	2018	2018 21st International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2018.8569426		Robotics	41.347342994857115	-45.40512571308257	120603
d7b46583de33615cb1c98bff18422f1c4f31a42a	simultaneous scene reconstruction and auto-calibration using constrained iterative closest point for 3d depth sensor array	robot sensing systems;environment building iterative closest point scene reconstruction auto calibration overhead depth camera array;auto calibration;warehouse automation image reconstruction image sensors iterative methods remotely operated vehicles robot vision;scene reconstruction;arrays;microsoft kinect cameras simultaneous scene reconstruction auto calibration constrained iterative closest point 3d depth sensor array intelligent warehouse automation automated guided vehicles agv icp based algorithm euclidean distance depth maps common field of view constraint;environment building;three dimensional displays;iterative closest point;cameras iterative closest point algorithm three dimensional displays robot sensing systems transmission line matrix methods arrays;transmission line matrix methods;iterative closest point algorithm;cameras;overhead depth camera array	Being able to monitor a large area is essential for intelligent warehouse automation. Complete depth map of a plant floor allows Automated Guided Vehicles (AGV) to navigate the environment and safely interact with nearby people and equipment, eliminating the need for installation of guide tracks and range sensors on individual robots. A single camera does not have sufficient field of view irresolution to monitor a large scene, and a camera mounted on a moving platform introduces delays and blind spots that could put people at risk in busy areas. Multi-camera arrays are needed in order to reconstruct the scene from simultaneous captures. Existing iterative closest point (ICP) based algorithms fail to produce meaningful results due to ICP attempting to minimize Euclidean distance between non-matching pairs. This paper describes a method for accurate and computationally efficient simultaneous scene reconstruction and auto-calibration using depth maps captured with multiple downward looking overhead cameras. The proposed method extends upon standard ICP algorithm by incorporating constraints imposed by the camera setup. The common field of view constraint imposed on the ICP algorithm matches a subset of points that are simultaneously in two camera's field of view. The planar constraint restricts the search space for closest points between 2point clouds to be on a projected planar surface. To simulate a typical warehouse environment, depth maps captured from two overhead Microsoft Kinect cameras were used to evaluate the effectiveness of the proposed algorithm. The results indicate the proposed algorithm successfully reconstructed the scene and produced auto-calibrated extrinsic camera matrix, where as standard ICP algorithm did not generate meaningful results.	algorithm;algorithmic efficiency;camera matrix;depth map;euclidean distance;iterative closest point;iterative method;kinect;overhead (computing);proximity problems;robot;sensor;simulation	Meng Xi Zhu;Christian Scharfenberger;Alexander Wong;David A. Clausi	2015	2015 12th Conference on Computer and Robot Vision	10.1109/CRV.2015.13	computer vision;simulation;computer science;iterative closest point;computer graphics (images)	Vision	51.524456899021374	-44.639565169959795	120662
71967bfa9780dd258084261328dfae2a087d1c3f	integrating object detection with 3d tracking towards a better driver assistance system	2d detections;paper;image segmentation;2d segmentation;road traffic;real time;level set;psi_visics;gpu;traffic sign;detection;region based 3d tracking;computer vision;road sign;recognition;3d model;single view detection;roads;three dimensional displays;traffic sign recognition;pixel;road sign recognition;three dimensional displays tracking robustness roads real time systems driver circuits pixel;adaboost;3d tracking;pattern recognition;driver circuits;gpu tracking detection recognition 3d tracking level set traffic sign road sign real time;robustness;tracking driver information systems image segmentation learning artificial intelligence object detection pose estimation road traffic;svm;computer science;3d pose tracking object detection driver assistance system single view detection region based 3d tracking road sign recognition adaboost 2d detections svm 2d segmentation;learning artificial intelligence;driver information systems;tracking;object detection;driver assistance system;real time systems;pose estimation;3d pose tracking	Driver assistance helps save lives. Accurate 3D pose is required to establish if a traffic sign is relevant to the driver. We propose a real-time system that integrates single view detection with region-based 3D tracking of road signs. The optimal set of candidate detections is found, followed by AdaBoost cascades and SVMs. The 2D detections are then employed in simultaneous 2D segmentation and 3D pose tracking, using the known 3D model of the recognised traffic sign. We demonstrate the abilities of our system by tracking multiple road signs in real world scenarios.	adaboost;device driver;gaussian blur;object detection;real-time clock;real-time computing;sensor;support vector machine	Victor Adrian Prisacariu;Radu Timofte;Karel Zimmermann;Ian D. Reid;Luc Van Gool	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.816	adaboost;support vector machine;computer vision;simulation;pose;computer science;level set;machine learning;traffic sign recognition;tracking;image segmentation;pixel;robustness;computer graphics (images)	Vision	41.98373021466659	-45.43887314616037	120964
4840bdaa8b2d13818f71baf02bd048081a266d0a	kullback-leibler divergence-based global localization for mobile robots	differential evolution;kullback leibler divergence;mobile robots;global localization;optimization methods	The global localization problem formobile robots is addressed in this paper. In this field, themost common approaches solve this problembased on theminimization of a quadratic loss function or themaximization of a probability distribution. The distances obtained from the perceptive sensors are used together with the predicted ones (from the estimates in the known map) to define a cost function or a probability to optimize. In our previouswork, we developed an optimization-based global localizationmodule that used evolutionary computation concepts. In particular, the algorithm engine was the Differential Evolution method. In thiswork, this algorithmhas beenmodified including theminimization of theKullback–Leibler divergence between true observations and estimates. This divergence is used to calculate the cost function of the localization module. The algorithm has been tested in different situations and the most important improvement is the ability to cope with different types of occlusions. © 2013 Elsevier B.V. All rights reserved.	algorithm;computational resource;differential evolution;elegant degradation;evolutionary computation;experiment;fitness function;image noise;iteration;kullback–leibler divergence;loss function;mathematical optimization;mobile robot;nonlinear system;sensor;state space	Fernando Martín Monar;Luis Moreno;Dolores Blanco;María Luisa Muñoz	2014	Robotics and Autonomous Systems	10.1016/j.robot.2013.11.006	differential evolution;mobile robot;mathematical optimization;simulation;computer science;machine learning;kullback–leibler divergence	AI	48.100389053238686	-51.17955187528129	121005
22275ea024b182b1f40fb9835fb2088aaf378793	robust fusion of color and depth data for rgb-d target tracking using adaptive range-invariant depth models and spatio-temporal consistency constraints		This paper presents a novel robust method for single target tracking in RGB-D images, and also contributes a substantial new benchmark dataset for evaluating RGB-D trackers. While a target object’s color distribution is reasonably motion-invariant, this is not true for the target’s depth distribution, which continually varies as the target moves relative to the camera. It is therefore nontrivial to design target models which can fully exploit (potentially very rich) depth information for target tracking. For this reason, much of the previous RGB-D literature relies on color information for tracking, while exploiting depth information only for occlusion reasoning. In contrast, we propose an adaptive range-invariant target depth model, and show how both depth and color information can be fully and adaptively fused during the search for the target in each new RGB-D image. We introduce a new, hierarchical, two-layered target model (comprising local and global models) which uses spatio-temporal consistency constraints to achieve stable and robust on-the-fly target relearning. In the global layer, multiple features, derived from both color and depth data, are adaptively fused to find a candidate target region. In ambiguous frames, where one or more features disagree, this global candidate region is further decomposed into smaller local candidate regions for matching to local-layer models of small target parts. We also note that conventional use of depth data, for occlusion reasoning, can easily trigger false occlusion detections when the target moves rapidly toward the camera. To overcome this problem, we show how combining target information with contextual information enables the target’s depth constraint to be relaxed. Our adaptively relaxed depth constraints can robustly accommodate large and rapid target motion in the depth direction, while still enabling the use of depth data for highly accurate reasoning about occlusions. For evaluation, we introduce a new RGB-D benchmark dataset with per-frame annotated attributes and extensive bias analysis. Our tracker is evaluated using two different state-of-the-art methodologies, VOT and object tracking benchmark, and in both cases it significantly outperforms four other state-of-the-art RGB-D trackers from the literature.	ambiguous grammar;benchmark (computing);extraction;frame (physical object);fuse device component;matching;obstruction;part dosing unit;relaxation;sensor;silo (dataset);small;algorithm;anatomical layer	Jingjing Xiao;Rustam Stolkin;Yuqing Gao;Ale&#x0161; Leonardis	2018	IEEE Transactions on Cybernetics	10.1109/TCYB.2017.2740952	robustness (computer science);video tracking;mathematics;computer vision;exploit;bittorrent tracker;benchmark (computing);invariant (mathematics);rgb color model;artificial intelligence	Vision	47.300055926207	-50.33383381712228	121031
4af28b6104649d9965a38ecf117fb115f970b810	reactive obstacle avoidance for highly maneuverable vehicles based on a two-stage optical flow clustering	cameras optical imaging optical sensors biomedical optical imaging dynamics adaptive optics collision avoidance;fahrzeug systemdynamik;optimization collision avoidance image processing optical feedback intelligent vehicles	This paper proposes a reactive obstacle avoidance approach based solely on image data from a monocular camera stream. By clustering and analyzing the optical flow, this approach is able to identify potential collisions with dynamic obstacles. Epipolar geometry is exploited to derive velocity commands that ensure a collision-free path for a highly maneuverable autonomous vehicle via a real-time optimizer. First, the underlying image processing and optimization principles are explained in detail, before simulation results show the general feasibility of the approach. Finally, real-world tests with the ROboMObil, the German Aerospace Center’s robotic electric vehicle, are provided to demonstrate its applicability.	aerial photography;autonomous robot;cluster analysis;cubic function;direct coupling;epipolar geometry;evasion (network security);feature detection (computer vision);feature detection (web development);holographic principle;image processing;lateral thinking;loss function;mathematical optimization;noise shaping;obstacle avoidance;open-source software;optical flow;real-time clock;sensor;simulation;unmanned aerial vehicle;velocity (software development)	Alexander Schaub;Daniel Baumgartner;Darius Burschka	2017	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2633292	control engineering;computer vision;simulation	Robotics	53.593246971073825	-38.49867672302031	121061
aa17b953790b2e323ec2c457abb9f0b5da9a7975	multi-object tracking driven event detection for evaluation	performance evaluation;real time tracking;real time;pixel classification;psi_visics;event detection;multiple objectives;object tracking;ground truth;tracking	This paper describes a monocular object tracker, able to detect and track multiple object classes in non-controlled environments. Our tracking framework uses Bayesian per-pixel classification to segment an image into foreground and background objects, based on observations of object appearances and motions in real-time. Furthermore, semantically high level events are automatically extracted from the tracking data for performance evaluation. The reliability of the event detection is demonstrated by applying it to state-of-the-art methods and comparing the results to human annotated ground truth data for multiple public datasets.	ground truth;high-level programming language;performance evaluation;pixel;real-time locating system	Daniel Roth;Esther Koller-Meier;Luc Van Gool	2008		10.1145/1463542.1463546	computer vision;real-time computing;tracking system;ground truth;viola–jones object detection framework;video tracking;data mining;tracking	Vision	43.621231172463695	-47.175414294749416	121205
fbe0dc1b753e8e7f5840f873b297abe1b49d396c	robust fingertip detection in a complex environment	hand region segmentation fingertip detection human computer interaction hand appearance model;image motion analysis;fingertip identification robust fingertip detection gesture recognition finger tracking human computer interaction system appearance model dense optical flow skin filter complete hand region segmentation local centroid distance centroid circles;human computer interaction;optical filters;skin;computer vision;object detection human computer interaction image segmentation image sequences;image color analysis;robustness;computer vision optical filters image motion analysis image color analysis human computer interaction skin robustness	Fingertip detection has a broad application in gesture recognition and finger tracking. It is also an important foundation of human-computer interaction systems. However, most algorithms are suitable for simple conditions with low accuracy because the hand is a nonrigid object, and its appearance model is complex. To address the challenging problem of accurately detecting fingertips in a complex environment, we propose a novel and robust fingertip detection algorithm in this paper. Unlike existing methods, our study requires no special device or mark, and users are free to move their hands. Via dense optical flow and a skin filter, we perform complete hand region segmentation in a complex environment. We find the maximum value of the local centroid distance outside the centroid circles and identify fingertips. Our algorithm performs favorably compared with common hand region segmentation and fingertip detection methods. Thorough experimentation proves that our proposed algorithm is effective and robust.	algorithm;energy citations database;finger tracking;gesture recognition;human–computer interaction;maxima and minima;optical flow;robustness (computer science);sensor	Guile Wu;Wenxiong Kang	2016	IEEE Transactions on Multimedia	10.1109/TMM.2016.2545401	computer vision;simulation;computer science;optical filter;skin;programming language;robustness;computer graphics (images)	Vision	46.096475637233766	-44.69529238724785	121225
77f1c87d8ad030be67666028065dfa4a109e0ed6	bafs: bundle adjustment with feature scale constraints for enhanced estimation accuracy	cameras;feature extraction;optical imaging;estimation;adaptive optics	We propose to incorporate within bundle adjustment (BA) a new type of constraint that uses feature scale information, leveraging the scale invariance property of typical image feature detectors (e.g., SIFT). While feature scales play an important role in image matching, they have not been utilized thus far for estimation purposes in a BA framework. Our approach exploits the already-available feature scale information and uses it to enhance the accuracy of BA, especially along the optical axis of the camera in a monocular setup. Importantly, the mentioned feature scale constraints can be formulated on a frame to frame basis and do not require loop closures. We study our approach in synthetic environments and the real-imagery KITTI dataset, demonstrating significant improvement in positioning error.	apache axis;bundle adjustment;business architecture;feature (computer vision);image registration;scale-invariant feature transform;sensor;synthetic intelligence	Vladimir Ovechkin;Vadim Indelman	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2018.2792141	optical imaging;control theory;feature extraction;engineering;optical axis;computer vision;monocular;feature detection;scale invariance;bundle adjustment;adaptive optics;artificial intelligence	Robotics	51.09317497481561	-46.21747294980957	121267
3ff21b9e52b1cc3f60075e010e872f768c260673	a csp solution to multi-camera surveillance and target tracking	robot sensing systems;cameras target tracking surveillance algorithm design and analysis robot sensing systems;surveillance;constraint satisfaction problems;target tracking cameras constraint satisfaction problems military systems;military systems;security surveillance;target tracking;security;surveillance algorithm multicamera surveillance target tracking security operation constraint satisfaction problem solution region of interest virtual environment average linear uncovered length;algorithm design and analysis;cameras	Surveillance is an essential part of any security operation, with cameras playing an important role. Often these cameras are controlled by a single individual, but this can lead to inefficient surveillance that leaves areas uncovered and misses targets. We propose a constraint satisfaction problem (CSP) solution to surveilling an area that maximizes the area seen in a region of interest (ROI), minimizes the time individual cells in the ROI goes unseen, and prioritizes targets for tracking. Cells in the ROI are weighted based on the time since they were last seen and whether or not a target is predicted to be in the cell. A virtual environment with cameras and targets is surveilled based on various parameters affecting camera actions. Several metrics are used to measure the performance of each set of parameters, including cell time unseen, average linear uncovered length (ALUL), and time a target is seen with results compared to another surveillance algorithm. The CSP was able to view all cells while greatly increasing the target tracking ability of rarely viewed targets at the cost of slightly fewer viewing steps for the more often viewed targets.	algorithm;constraint satisfaction problem;region of interest;virtual reality	Anton Dukeman;Monica Anderson	2011	2011 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/ICSMC.2011.6083649	algorithm design;computer vision;simulation;computer science;information security;constraint satisfaction problem;computer security	Robotics	45.8764464188443	-40.583060149854106	121333
1e14dbd0c5d6dc950603f2ef88f03e65cce0d165	utility-based dynamic camera assignment and hand-off in a video network	tracking camera network camera selection detection multi camera surveillance;assignment problem;video network;game theory;video signal processing;camera selection;detection;image sensors;user supplied criteria;error analysis;games;pixel;game theory utility based dynamic camera assignment video network multi camera multi person seamless tracking user supplied criteria;cameras games face game theory cathode ray tubes error analysis pixel;face;multi camera surveillance;utility based dynamic camera assignment;camera network;cathode ray tubes;cameras;multi camera multi person seamless tracking;tracking;video signal processing game theory image sensors	In this paper we propose an approach for multi-camera multi-person seamless tracking that allows camera assignment and hand-off based on a set of user-supplied criteria. The approach is based on the application of game theory to camera assignment problem. Bargaining mechanisms are considered for collaborations as well as for resolving conflicts among the available cameras. Camera utilities and person utilities are computed based on a set of criteria. They are used in the process of developing the bargaining mechanisms. Experiments for multi-camera multi-person tracking are provided. Several different criteria and their combination of them are carried out and compared with each other to corroborate the proposed approach.	assignment problem;game theory;seamless3d	Yiming Li;Bir Bhanu	2008	2008 Second ACM/IEEE International Conference on Distributed Smart Cameras	10.1109/ICDSC.2008.4635677	face;cathode ray tube;games;game theory;computer vision;simulation;computer science;image sensor;assignment problem;tracking;multimedia;pixel	Robotics	45.897522830906986	-45.34280748963549	121367
1387648b0edd8b2f2d1d39df93154fb3fb433450	multiple-view-based tracking of multiple humans	kalman filtering;feature extraction multiple view based tracking multiple human motions computer vision human tracking occlusions viewpoint selection image sequences kalman filtering motion estimation;image features;multiple view based tracking;kalman filters;viewpoint selection;kalman filter;motion estimation;multiple views;human tracking;computer vision;optical tracking;feature extraction;human motion;multiple human motions;humans;feature extraction computer vision optical tracking target tracking motion estimation image sequences kalman filters;target tracking;occlusions;image sequences	Wepropose a multiple-view-based tracking algorithm for multiple-human motions. In vision-based human tracking, self-occlusions and human-human occlusions are a part of the more signi®cant problems. Employing multiple viewpoints and a viewpoint selection mechanism, however, can reduce these problems. In our system, human positions are tracked with a sequence of multiple-viewpoint images. This tracking is based on the Kalman ®ltering approach. The estimation results are utilized to select proper viewpoints in other sub-tasks (rotation angle detection and body-side detection). Each sub-task has a different criterion for selecting viewpoints. We also describe the criterions for accomplishing individual sub-tasks and relationships between sub-tasks. We have already built an experimental system based on a small number of reliable image features. We con®rmed the stability of our algorithm through simulations and also performed fundamental examinations on the experimental system.	algorithm;apache axis;best-effort delivery;experimental system;feature extraction;kalman filter;kinesiology;naruto shippuden: clash of ninja revolution 3;sensor;simulation;type system	Akira Utsumi;Hiroki Mori;Jun Ohya;Masahiko Yachida	1998		10.1109/ICPR.1998.711214	kalman filter;computer vision;simulation;computer science;machine learning;control theory	Vision	43.84610287736793	-47.5935075370302	121528
69532d930980b35043f67c13a93955345ee1bc1b	a robust fall detection system for the elderly in a smart room	g740 computer vision;3 d head velocity;gaussian mixture;gaussian processes;robust fall detection system;head tracking;human shape information;single gaussian;density method;shape recognition;fall detection;video feature;conference contribution;handicapped aids;fall detection code book background subtraction motion based particle filtering head tracking density method;robustness senior citizens head humans shape feature extraction data mining cameras filtering particle tracking;particle filter;shape recognition feature extraction gaussian processes handicapped aids;feature extraction;unified modeling language;background subtraction;fall event;fall event robust fall detection system smart room density method video feature 3 d head velocity human shape information feature extraction single gaussian parzen window method;smart room;humans;head;computational efficiency;motion based particle filtering;code book background subtraction;parzen window method;cameras;tracking	In this paper, we propose a novel and robust fall detection system by using a density method for modeling a fall event as a function of certain video feature.3-D head velocity and human shape information are extracted as feature and three types of density model, single Gaussian, mixture of Gaussians and Parzen window method, are constructed for modeling the density of fall with respect to the extracted video feature. Falls are then detected according to the corresponding obtained density model and the success of the method is confirmed on real video sequences.	kernel density estimation;mixture model;velocity (software development);window function	Miao Yu;Syed Mohsen Naqvi;Jonathon A. Chambers	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495512	unified modeling language;computer vision;speech recognition;background subtraction;particle filter;feature extraction;computer science;pattern recognition;gaussian process;tracking;head;statistics	Robotics	39.78862849658333	-48.681337237605014	121628
80d78b777fdaabc278e827efe02607f961efcdd8	variational inference for background subtraction in infrared imagery		We propose a Gaussian mixture model for background subtraction in infrared imagery. Following a Bayesian approach, our method automatically estimates the number of Gaussian components as well as their parameters, while simultaneously it avoids over/under fitting. The equations for estimating model parameters are analytically derived and thus our method does not require any sampling algorithm that is computationally and memory inefficient. The pixel density estimate is followed by an efficient and highly accurate updating mechanism, which permits our system to be automatically adapted to dynamically changing operation conditions. Experimental results and comparisons with other methods show that our method outperforms, in terms of precision and recall, while at the same time it keeps computational cost suitable for real-time applications.	algorithm;algorithmic efficiency;background subtraction;computation;mixture model;pixel density;precision and recall;real-time clock;sampling (signal processing);variational principle	Konstantinos Makantasis;Anastasios D. Doulamis;Konstantinos Loupos	2015		10.1007/978-3-319-27857-5_62	computer vision;machine learning;mathematics;statistics	ML	44.904508469249244	-48.68674237758564	121642
e1a7446bc6f8267d4378977f8f39674f6932b628	change detection in crowded underwater scenes - via an extended gaussian switch model combined with a flux tensor pre-segmentation		In this paper a new approach for change detection in videos of crowded scenes is proposed with the extended Gaussian Switch Model in combination with a Flux Tensor pre-segmentation. The extended Gaussian Switch Model enhances the previous method by combining it with the idea of the Mixture of Gaussian approach and an intelligent update scheme which made it possible to create more accurate background models even for difficult scenes. Furthermore, a foreground model was integrated and could deliver valuable information in the segmentation process. To deal with very crowded areas in the scene – where the background is not visible most of the time – we use the Flux Tensor to create a first coarse segmentation of the current frame and only update areas that are almost motionless and therefore with high certainty should be classified as background. To ensure the spatial coherence of the final segmentations, the N2Cut approach is added as a spatial model after the background subtraction step. The evaluation was done on an underwater change detection datasets and showed significant improvements over previous methods, especially in the crowded scenes.	algorithm;anomaly detection;background subtraction;coherence (physics);flux limiter	Martin Radolko;Fahimeh Farhadifard;Uwe von Lukas	2017		10.5220/0006258504050415	computer vision;simulation	Vision	41.97847579659796	-47.023808949175425	121923
c7f607d868069c589ede14f6db50050ca69d9bc9	a feasibility study of ballet education using measurement and analysis on partial features of still scenes	ballet education;kinect;partial feature extraction;lower body joint point estimation;closest foot point estimation	There have been a number of dance and ballet education systems using different multimedia devices. One of the well-known multimedia devices is Kinect which uses multiple built-in sensors. We focus on Kinect-based dance and ballet education systems. Existing systems that use Kinect cannot properly recognize the turnout movement. We propose the use of a lower body joint point estimation algorithm and a closest foot points estimation algorithm that can efficiently perform image localization for recognition of basic ballet movements. In addition, in order to evaluate correct ballet movements, we propose a method that extracts partial features from still scenes and performs measurements for knee and foot positions. The proposed method is the first ballet education system that properly measures movements of a ballet dancer.		So-Hyun Park;Gwang-Soo Hong;Sun-Woo Park;Aziz Nasridinov;Injae Park;Byungkyu Kim;Young-Ho Park	2016	IJDSN	10.1177/1550147716681794	computer vision;simulation;multimedia	NLP	48.606216101435095	-44.45148202942318	122189
1879589535b78b574a0e15570a2634f31728090c	an automatic approach to online color training in robocup environments	color space;camera image;mobile robots;three dimensional;robocup;automatic online color training;image colour analysis;feature extraction;robotics and automation orbital robotics table lookup cameras robustness robot vision systems robot sensing systems lighting object detection pixel;object extraction;multi robot systems;object extraction robocup camera image automatic online color training landmark extraction;landmark extraction;multi robot systems feature extraction image colour analysis mobile robots	Many approaches for extracting landmarks and objects from a camera image based on their color coding were published in the RoboCup domain. They are quite sophisticated and tuned to the typical RoboCup scenario of constant bright lighting using a static subdivision of the color space into different color classes. However, such algorithms would soon be of limited use, as the future requirements of RoboCup include the possibility to play under changing and finally natural lighting. This paper presents an algorithm for automatic online color training, which is able to robustly adapt the mapping of colors to color classes onto different lighting situations online. Using the ACT algorithm a robot will be able to play a RoboCup match while the illumination of the field varies	3d pose estimation;algorithm;color space;colour look-up table;control system;embedded system;lookup table;map;pixel;requirement;robot;subdivision surface	Patrick Heinemann;Frank Sehnke;Felix Streichert;Andreas Zell	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.282444	mobile robot;three-dimensional space;computer vision;simulation;feature extraction;computer science;engineering;color space;computer graphics (images)	Robotics	50.02251725378051	-40.19379496320252	122288
0ca335065ef4eef18cd061acdd543f0ee5ee9f89	bayesian structure from motion using inertial information	3d scene reconstruction bayesian structure from motion inertial information sequential importance sampling inertial sensors camera motion synthetic images real images image sequence;image motion analysis;inertial information;bayesian methods cameras image sequences robustness sampling methods motion estimation costs layout visual system monte carlo methods;video signal processing;bayes methods;bayesian methods;motion estimation;prior knowledge;layout;camera motion;sequential importance sampling;image reconstruction;synthetic images;image sequence;3d scene reconstruction;robustness;video signal processing bayes methods image motion analysis inertial systems importance sampling parameter estimation image sequences image reconstruction;inertial systems;parameter estimation;importance sampling;sampling methods;real images;visual system;structure from motion;inertial sensor;monte carlo methods;cameras;inertial sensors;bayesian structure from motion;image sequences	In this papec a novel approach to Bayesian structure from motion (SM) using inertial informafion and sequential importance sampling (3s) bpresented. The inertial information is obtainedfmm camera-mountedinertial sensors and is used in the Bayesian SfMapproach as prior knowledge of the camera motion in the sampling algorithm. Experimental results using both synthetic and realimages show thatmore accurate results can be obtained when inertial information is used or same estimation accuracy can be obtained using inertialinformation at a lower cost.	algorithm;importance sampling;particle filter;sampling (signal processing);sensor;structure from motion;synthetic intelligence	Gang Qian;Rama Chellappa;Qinfen Zheng	2002		10.1109/ICIP.2002.1038996	iterative reconstruction;layout;inertial measurement unit;sampling;computer vision;structure from motion;simulation;visual system;bayesian probability;importance sampling;computer science;pattern recognition;motion estimation;mathematics;estimation theory;real image;statistics;robustness;monte carlo method	Vision	46.66465501655869	-48.36942669434757	122382
52cba0165f7aa498854dd07415cb4b1ee8a9fd5f	atlanta world: an expectation maximization framework for simultaneous low-level edge grouping and camera calibration in complex man-made environments	object part;object parts;corresponding part;corresponding region;invariant object recognition;different viewing condition;class-based matching;dense set;different image;corresponding feature;natural object class;computer vision;image recognition;object recognition;mathematics;computer science;affine transformation;stereo vision;lighting	"""Edges in man-made environments, grouped according to vanishing point directions, provide single-view constraints that have been exploited before as a precursor to both scene understanding and camera calibration. A Bayesian approach to edge grouping was proposed in the """"Manhattan World"""" paper by Coughlan and Yuille, where they assume the existence of three mutually orthogonal vanishing directions in the scene. We extend the thread of work spawned by Coughlan and Yuille in several significant ways. We propose to use the expectation maximization (EM) algorithm to perform the search over all continuous parameters that influence the location of the vanishing points in a scene. Because EM behaves well in high-dimensional spaces, our method can optimize over many more parameters than the exhaustive and stochastic algorithms used previously for this task. Among other things, this lets us optimize over multiple groups of orthogonal vanishing directions, each of which induces one additional degree of freedom. EM is also well suited to recursive estimation of the kind needed for image sequences and/or in mobile robotics. We present experimental results on images of """"Atlanta worlds"""", complex urban scenes with multiple orthogonal edge-groups, that validate our approach. We also show results for continuous relative orientation estimation on a mobile robot."""	camera resectioning;expectation–maximization algorithm;high- and low-level;iterative reconstruction;markov random field;mobile robot;model selection;recursion;robotics;vanishing point	Grant Schindler;Frank Dellaert	2004	Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.	10.1109/CVPR.2004.38	mobile robot;computer vision;mathematical optimization;expectation–maximization algorithm;computer science;machine learning;mathematics;group theory	Vision	46.19020336241359	-51.090871148726116	122391
90e1e64c515ebfe7adff4f2256b97e42b0792a72	a temporal filter approach for detection and reconstruction of curbs and road surfaces based on conditional random fields	road traffic;kalman filters;curb detection;kalman filter;road surface detection;labeling image reconstruction surface reconstruction three dimensional displays computational modeling cameras estimation;conditional random fields;surface reconstruction;computational modeling;estimation;three dimensional displays;image reconstruction;traffic engineering computing image reconstruction kalman filters object detection road traffic solid modelling;kalman filter temporal filter approach conditional random fields curb detection road surface detection curb reconstruction road surface reconstruction 3d curb model;road surface reconstruction;traffic engineering computing;curb reconstruction;temporal filter approach;cameras;labeling;object detection;solid modelling;3d curb model	A temporal filter approach for real-time detection and reconstruction of curbs and road surfaces from 3D point clouds is presented. Instead of local thresholding, as used in many other approaches, a 3D curb model is extracted from the point cloud. The 3D points are classified to different parts of the model (i.e. road and sidewalk) using a temporally integrated Conditional Random Field (CRF). The parameters of curb and road surface are then estimated from the respectively assigned points, providing a temporal connection via a Kalman filter.	computation;conditional random field;experiment;gradient descent;kalman filter;lateral computing;lateral thinking;point cloud;real-time clock;temporal logic;texture mapping;thresholding (image processing)	Jan Siegemund;Uwe Franke;Wolfgang Förstner	2011	2011 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2011.5940447	computer vision;simulation;engineering;remote sensing	Vision	43.61172096356979	-44.57508861201795	122469
01e8dcfb8bb7001535ee133b5963d960db8c51ec	random bounce algorithm: real-time image processing for the detection of bats and birds - algorithm description with application examples from a laboratory flight tunnel and a field test at an onshore wind energy plant		Wind energy plants generate an impact on wildlife with significant fatality rates for various bat and bird species, e.g. due to a collision with the rotor blades. Monitoring approaches, such as vision-based systems, are needed to reduce their mortality by means of an optimized turbine control strategy as soon as flying animals are detected. Since manual analysis of the video data is ineffective, automatic video processing with real-time capabilities is required. In this paper, we propose the random bounce algorithm (RBA) as a novel real-time image processing method for vision-based detection of bats and birds. The RBA is combined with object tracking in order to extract flight trajectories. Its performance is compared with connected components object detection. Results from a laboratory flight tunnel as well as from a field study at a 2 MW wind energy plant in Southern Germany will be presented and discussed. We have successfully detected and tracked objects both in laboratory experiments with many animals and in field experiments with individual animals at a frame rate of 10 fps.	algorithm;image processing;real-time clock	Nikolas Scholz;Jochen Moll;Moritz Mälzer;Konstantin Nagovitsyn;Viktor Krozer	2016	Signal, Image and Video Processing	10.1007/s11760-016-0951-0	simulation;electrical engineering	Robotics	42.846421468648074	-42.56001754086129	122591
8abe61b3b63230eabbcfb59371e2bf02f3e6e657	natural feature tracking on a mobile handheld tablet	computer graphics;image classification;databases robustness;registered synthetic graphics natural feature tracking mobile handheld tablet object recognition local keypoint descriptor method salient regions gallery object features nearest neighbor classification real time performance;object tracking;notebook computers;mobile computing;object tracking computer graphics image classification mobile computing notebook computers	This paper presents a natural feature tracking system for object recognition in real-life environments. The system is based on a local keypoint descriptor method optimized and adapted to extract salient regions within the image. Each object in the gallery is characterized by keypoints and corresponding local descriptors. The method first identifies gallery object features in new images using nearest neighbor classification. It then estimates camera pose and augments the image with registered synthetic graphics. We describe the optimizations necessary to enable real-time performance on a mobile tablet. An experimental evaluation of the system in real environments demonstrates that the method is accurate and robust.	graphics;handheld game console;motion estimation;outline of object recognition;real life;real-time clock;synthetic data;tablet computer;tracking system	Madjid Maidi;Marius Preda;Matthew N. Dailey;Sirisilp Kongsilp	2013	2013 IEEE International Conference on Signal and Image Processing Applications	10.1109/ICSIPA.2013.6708012	computer vision;contextual image classification;computer science;video tracking;multimedia;computer graphics;mobile computing;computer graphics (images)	Robotics	40.82912814049649	-50.59659769402344	122613
1702cf6d1c9315cd928a4171983813dfea3cd357	voting spaces cooperation for 3d plane detection from monocular image sequences	histograms vectors optical imaging buildings cameras roads vehicles;image motion analysis;iterative methods;iterative histogram splitting monocular image sequence analysis structure from motion 3d plane detection c velocity;statistical analysis;image reconstruction;statistical analysis driver information systems image motion analysis image reconstruction image sequences iterative methods object detection;histogram splitting approach voting space cooperation 3d plane detection monocular image sequence 3d scene reconstruction automatic driver assistance system parameterized surface moving camera camera calibration vehicle egomotion lateral plane orientation horizontal plane orientation frontal plane orientation iterative voting process iso velocity curve;driver information systems;object detection;image sequences	This paper deals with 3D scene reconstruction from an on-board moving camera in the context of automatic driver assistance systems. The aim of our study is to detect any kind of parameterized surface from a moving camera without camera calibration or any prior knowledge about the vehicle egomotion. We assume that the 3D scene is a set of 3D planes that are classified into three categories according to their orientation: lateral planes (buildings), horizontal planes (the road) and frontal planes (moving cars or crossing pedestrians). We propose an iterative voting process that takes advantages of some specific iso-velocity curves properties in order to build a set of appropriate voting spaces. Each of them facilitates the detection of a specific plane model. A tough problem as the detection of a parameterized surface from a moving camera is reduced to an easy maxima finding in several voting spaces. We focus in this paper on the iterative scheme that allows to deal with several spaces at the same time. We choose to adapt an histogram splitting approach in order to achieve a complete plane detection process.	algorithm;approximation;camera resectioning;hough transform;image histogram;iterative method;lateral thinking;maxima;on-board data handling;real-time computing;synthetic data;velocity (software development);visual odometry	Qiong Nie;Samia Bouchafa;Alain Mérigot	2012	2012 3rd International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2012.6469547	iterative reconstruction;computer vision;simulation;mathematics;iterative method;image plane;statistics;computer graphics (images)	Vision	51.23416940056039	-47.8275606719585	122655
53da3e8a0ffe578b9719b515c1772c14d2e0d3e8	feasibility study of textureless object detection and pose estimation based on a model with 3d edgels and surfaces			3d pose estimation;object detection;texture mapping	Kimitoshi Yamazaki;Kiyohiro Sogen;Takashi Yamamoto;Masayuki Inaba	2015	Paladyn		object detection;computer vision;pose;artificial intelligence;computer science	Vision	51.94313646751975	-43.46824383677039	122812
f2e08b182dde12e8eb4a76df083f7c761ce4d5fb	robotic vision: understanding improves the geometric accuracy		. Paraphrasing Olivier Faugeras in the foreword of [1], making a robot see is still an unsolved and challenging task after several decades of research. The traditional research has been based on the geometric models of multiple views of a scene, estimating a sparse 3D map of the scene and the camera pose. Recent advances have led to fully dense and real-time 3D reconstructions. Also, there are relevant recent works on the semantic annotation of the 3D maps. This extended abstract summarizes the work of [2], [3], [4], [5], [6] in this direction; in particular using mid and high-level features to improve the accuracy of dense maps.	3d reconstruction from multiple images;high- and low-level;map;real-time clock;robot;sparse matrix	Javier Civera	2015			computer vision;artificial intelligence;computer science;annotation	Vision	53.588108575137916	-45.35221064262761	122924
89e9bac45df03a3ac6539b47496296eb05318c62	quantum radar: snake oil or good idea?		Quantum radar has been proposed in the quantum information science literature as a novel approach to sensing that can offer a substantial gain over existing, classical sensors. In particular, a quantum radar based on quantum entanglement, termed quantum illumination radar, has been theoretically shown to demonstrate superiority over the optimal classical sensor, and there have been some experimental validation. However, all of the existing literature in the subject is in language, terminology and metrics that are completely unfamiliar (or relevant) to classical radar experts, and as a result there is understandable skepticism among the radar experts. This paper attempts to bridge the large gap between practical radar scientists and quantum information science (QIS) experts working in the field. We review the theoretical state-of-the-art in field and examine some of the claims in the literature in terms of metrics of relevance to radar detection, tracking and sensor fusion. We also discuss some possible technological routes to building such a radar across the electromagnetic spectrum.		Bhashyam Balaji	2018	2018 International Carnahan Conference on Security Technology (ICCST)	10.1109/CCST.2018.8585474	radar imaging;artificial intelligence;quantum illumination;computer vision;quantum entanglement;radar;signal processing;quantum radar;quantum information science;sensor fusion;computer science	EDA	43.36078865596111	-39.46973774094945	123022
552c802fefcf41d505936de8b730ea93205f3c77	an omni-rgb+d camera rig calibration and fusion using unified camera model for 3d reconstruction (withdrawal notice)	vision system;3d modeling;camera calibration;calibration;3d reconstruction;cameras;fisheye camera	The perfect vision system could be a system which can obtain surround images or information at once. We present a vision system which can view the image in 360◦ with highresolution depth information. The proposed vision system is compact and rigid with two fisheye cameras that provide a 360-degree field of view(FoV). Alongside, a high-resolution stereo vision camera is mounted to monitor anterior FoV for precise depth perception of the scene. To effectively calibrate the proposed camera system, we offer a novel camera calibration approach taking the advantages of Unified Camera Model representation. The proposed calibration method outperforms the state-of-the-art methods. Moreover, we proposed a more effective algorithm in fusing the two fisheye images into a single unified sphere, which offers seamless stitching results. This new omni-vision rig system is designed to obtain sufficient information to be used on a robot for object detection and recognition. A large scale SLAM and dense 3D reconstruction can be achieved taking the advantage of the large FoV.	3d reconstruction;algorithm;camera resectioning;depth perception;fisheye;image resolution;object detection;seamless3d;simultaneous localization and mapping;stereopsis;xbox live vision	Ahmad Zawawi Jamaluddin;Osama Mazhar;Cansen Jiang;Ralph Seulin;Olivier Morel;David Fofi	2017		10.1117/12.2266945	smart camera;stereo camera;computer vision;camera auto-calibration;camera matrix;camera resectioning;geography;optics;pinhole camera model;computer graphics (images)	Vision	53.63157989787837	-45.4753060353189	123101
0b5ea863c0a2cec3da11206f792963ad6f7145e9	semi-automatic interactive modeling for model-based camera tracking	estimation theory;cramer rao lower bound model based camera tracking semiautomatic interactive modeling pre processing 3d object models simple primitives feature based modeling 3d point clouds target scenes graphic modeling tools user participation video sequences;computational modeling cameras solid modeling analytical models feature extraction sensitivity video sequences;feature extraction;image sequences cameras estimation theory feature extraction;error analysis camera tracking model based tracking object modeling;cameras;image sequences	Object modeling is an important task as a pre-processing of model-based camera tracking. In this paper, we present a semi-automatic interactive modeling method to create 3D object models with simple primitives such as points and lines. Our approach is based on feature-based modeling that uses 3D point clouds of target scenes. Instead of using graphic modeling tools, the modeling is intuitively performed by user participation in video sequences. Finally, we analyze sensitivity of camera tracking according to modeling errors using Cramer-Rao Lower Bound.	graph (discrete mathematics);match moving;point cloud;preprocessor;semiconductor industry	Eun Joo Rhee;Kangsoo Kim;Byung-Kuk Seo;Jong-Il Park	2012	2012 3rd IEEE International Conference on Network Infrastructure and Digital Content	10.1109/ICNIDC.2012.6418788	computer vision;simulation;computer science;video tracking;computer graphics (images)	Robotics	50.57629842272624	-48.42760822806697	123154
1deab4a5e5a333b6cbfcda5e86f7e418e237cc7f	cyber surveillance for flood disasters	early warning system;biological patents;video surveillance;biomedical journals;image segmentation;pedestrian safety;flood;text mining;poison control;europe pubmed central;injury prevention;citation search;safety literature;traffic safety;injury control;citation networks;home safety;injury research;safety abstracts;human factors;research articles;abstracts;open access;occupational safety;safety;life sciences;clinical guidelines;safety research;accident prevention;first warning;violence prevention;bicycle safety;full text;poisoning prevention;falls;ergonomics;rest apis;suicide prevention;orcids;europe pmc;biomedical research;bioinformatics;literature search	Regional heavy rainfall is usually caused by the influence of extreme weather conditions. Instant heavy rainfall often results in the flooding of rivers and the neighboring low-lying areas, which is responsible for a large number of casualties and considerable property loss. The existing precipitation forecast systems mostly focus on the analysis and forecast of large-scale areas but do not provide precise instant automatic monitoring and alert feedback for individual river areas and sections. Therefore, in this paper, we propose an easy method to automatically monitor the flood object of a specific area, based on the currently widely used remote cyber surveillance systems and image processing methods, in order to obtain instant flooding and waterlogging event feedback. The intrusion detection mode of these surveillance systems is used in this study, wherein a flood is considered a possible invasion object. Through the detection and verification of flood objects, automatic flood risk-level monitoring of specific individual river segments, as well as the automatic urban inundation detection, has become possible. The proposed method can better meet the practical needs of disaster prevention than the method of large-area forecasting. It also has several other advantages, such as flexibility in location selection, no requirement of a standard water-level ruler, and a relatively large field of view, when compared with the traditional water-level measurements using video screens. The results can offer prompt reference for appropriate disaster warning actions in small areas, making them more accurate and effective.	computer and network surveillance;dental intrusion;flood;how true feel alert right now;image processing;intrusion detection system;natural disasters;physical object;projections and predictions;sensorineural hearing loss (disorder);telling untruths;verification of theories	Shi-Wei Lo;Jyh-Horng Wu;Fang-Pang Lin;Ching-Han Hsu	2015		10.3390/s150202369	simulation;telecommunications;computer science;bioinformatics;engineering;suicide prevention;human factors and ergonomics;injury prevention;data mining;image segmentation;computer security	AI	41.996932760346425	-42.845442460555155	123222
3201ac41ad7b815dec387f0bd07ad164165e97ba	a vision-based system supports mapping services for visually impaired people in indoor environments	visualization robots kalman filters navigation buildings indoor environments vehicles;kalman filters;blind people vision based system visually impaired people indoor environments visual based system evaluation map task localization task mapping service assistance visual data self designed image acquisition system robust visual odometry method fab map algorithm places learning offline phase matching place procedure kalman filter robot state estimation kinematic model image to map matching blind pupils navigation place recognition mapping service support;navigation;visualization;slam robots distance measurement handicapped aids human robot interaction image matching kalman filters learning artificial intelligence mobile robots object recognition robot kinematics robot vision;robots;indoor environments;vehicles;navigations visual odometry place recognition fab map algorithms;buildings	This paper describes and extensively evaluates a visual-based system that autonomously operators for both building a map and localization tasks. The proposed system is to assist mapping services to the visually impaired/blind people in small or mid-scale environments such as inside a building or campus of school, hospital. Toward this end, the proposed approaches solely rely on visual data thanks to a self-designed image acquisition system. On one hand, a robust visual odometry method is utilized to create a map of the environments. On the other hand, the proposed approaches utilize FAB-MAP algorithm that is maybe the most successful for learning places in the environments. Map building and learning places in an environment are processed in an off-line phase. Through a matching place procedure, online captured images are continuously positioned on the map. Furthermore, we utilize a Kalman Filter that combines the matching results of current observation and the estimation of robot states based on its kinematic model. We evaluate performances of the proposed system through experimental schemes. The results show that the constructed map coincides with ground truth, and matching image-to-map is high confidence. The evaluations also contain scenarios which the blind pupils move following Robot. The experimental results confirmed that proposed system feasibly navigating blind pupils in indoor environments.	algorithm;ground truth;internationalization and localization;kalman filter;online and offline;performance;pervasive informatics;robot;visual odometry	Quoc-Hung Nguyen;Hai Vu;Thanh-Hai Tran;Quang-Hoan Nguyen	2014	2014 13th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2014.7064541	robot;kalman filter;computer vision;navigation;simulation;visualization;computer science;artificial intelligence;mobile robot navigation	Robotics	48.278590788927865	-40.798534446293246	123252
a222d28b7ce4d766047cf633e17d720cec0d82a1	a multiple velocity fields approach to the detection of pedestrians interactions using hmm and data association filters		This paper addresses the diagnosis of interactions between pairs of pedestrians in outdoor scenes, using a generative model for the trajectories. It is assumed that pedestrians' motions are driven by a set of velocity fields, learned from the video signal. This model is extended to account for the interaction among pedestrians, using attractive/repulsive velocity components. An inference algorithm is provided to estimate the attraction/repulsion velocity from the pedestrian trajectory and characterize pedestrians' interaction. Since we consider multiple motion models switched according to space-varying probabilities, inference is performed by combining a data association filter with a HMM-like forward algorithm. The proposed algorithm is denoted I-PDAF and is tested with synthetic data and pedestrians trajectories.	hidden markov model;interaction;velocity	Ricardo A. Ribeiro;Jorge S. Marques;João Miranda Lemos	2013		10.1007/978-3-642-41914-0_15	computer vision;simulation;machine learning;mathematics	Vision	45.108236079432956	-47.24486961315093	123302
9d24973b5eb0c89b4e57dcbb117b24c142e3806c	a kernel particle filter multi-object tracking using gabor-based region covariance matrices	kernel;manifolds;computer graphics;gabor filters;multiple objectives;estimation;optical tracking;spatial distribution;particle filter;covariance matrices;feature extraction;object tracking;rcm kernel particle filter multiobject tracking gabor based region covariance matrices spatially significant occlusions object trajectory adaptive appearance models spatial distributions inter occlusion relationships gabor functions;kernel particle filters particle tracking covariance matrix target tracking filtering object detection image sequences layout chromium;particle filtering numerical methods computer graphics covariance matrices gabor filters object detection optical tracking;particle filters;target tracking;object detection;particle filtering numerical methods;covariance matrix	This paper presents an approach to label and track multiple objects through both temporally and spatially significant occlusions. To this end, tracking is performed at both the region level and the object level. At the region level, a kernel based particle filter method is used to search for optimal region tracks which limits the scope of object trajectories. At the object level, each object is located based on adaptive appearance models, spatial distributions and inter-occlusion relationships. Region covariance matrices are used to model objects appearance. We analyzed the advantages of using Gabor functions as features and embedded them in the RCMs to get a more accurate descriptor. The proposed architecture is capable of tracking multiple objects even in the presence of periods of full occlusions. Results from experiments with real video data show the effectiveness of the approach hereby proposed.	data transfer object;embedded system;experiment;gabor atom;kernel (operating system);particle filter;temporal logic	Hélio Palaio;Jorge P. Batista	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413703	computer vision;particle filter;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	44.980330613621206	-48.938450970602496	123379
0d652d7221c8e7f38492ff3edbf885a24e38f449	lietricp: an improvement of trimmed iterative closest point algorithm	trimmed iterative closest point;anisotropic scale transformation;registration;lie group;lietricp	We propose a robust registration method for two point sets using Lie group parametrization. Our algorithm is termed as LieTrICP, as it combines the advantages of the Trimmed Iterative Closest Point (TrICP) algorithm and Lie group representation. Given two low overlapped point sets, we first find the correspondence for every point, then select the overlapped point pairs, and use Lie group representation to estimate the geometric transformation from the selected point pairs. These three steps are conducted iteratively to obtain the optimal transformation. The novelties of this algorithm are twofold: (1) it generalizes the TrICP to the anisotropic case; and (2) it gives a unified Lie group framework for point set registration, which can be extended to more complicated transformations and high dimensional problems. We conduct extensive experiments to demonstrate that our algorithm is more accurate and robust than several other algorithms in a variety of situations, including missing points, perturbations and outliers. & 2014 Elsevier B.V. All rights reserved.	algorithm;experiment;image scaling;iterative closest point;iterative method;perturbation theory;point set registration	Jianmin Dong;Yaxin Peng;Shihui Ying;Zhiyu Hu	2014	Neurocomputing	10.1016/j.neucom.2014.03.035	mathematical optimization;combinatorics;discrete mathematics;mathematics;lie group;iterative closest point	Vision	50.364558231809156	-51.53465162059334	123649
ef373c46a0b064ce018b3a8c59b931479447cea9	point registration via efficient convex relaxation	point registration;shape matching;convex relaxation	Point cloud registration is a fundamental task in computer graphics, and more specifically, in rigid and non-rigid shape matching. The rigid shape matching problem can be formulated as the problem of simultaneously aligning and labelling two point clouds in 3D so that they are as similar as possible. We name this problem the Procrustes matching (PM) problem. The non-rigid shape matching problem can be formulated as a higher dimensional PM problem using the functional maps method. High dimensional PM problems are difficult non-convex problems which currently can only be solved locally using iterative closest point (ICP) algorithms or similar methods. Good initialization is crucial for obtaining a good solution.  We introduce a novel and efficient convex SDP (semidefinite programming) relaxation for the PM problem. The algorithm is guaranteed to return a correct global solution of the problem when matching two isometric shapes which are either asymmetric or bilaterally symmetric.  We show our algorithm gives state of the art results on popular shape matching datasets. We also show that our algorithm gives state of the art results for anatomical classification of shapes. Finally we demonstrate the power of our method in aligning shape collections.	algorithm;computer graphics;isometric projection;iterative closest point;iterative method;linear programming relaxation;map;point cloud;semidefinite programming;shape context;sockets direct protocol	Haggai Maron;Nadav Dym;Itay Kezurer;Shahar Z. Kovalsky;Yaron Lipman	2016	ACM Trans. Graph.	10.1145/2897824.2925913	mathematical optimization;combinatorics;3-dimensional matching;mathematics;geometry	Graphics	50.8771612945657	-51.34157858807503	123881
c04b804e20f0571c43627d5dd849808029aaa7cd	real time two-step inspection system for assembled valve key in automobile cylinder heads	false reject rate;vision based inspection;edge detection;vision based inspection assembled valve key inspection machine vision circle detection two step inspection real time inspection;real time;false negative;two step inspection;real time systems inspection assembly systems valves automobiles image edge detection head detectors noise shaping streaming media;inspection;real time inspection;machine vision;automotive components;hough transforms real time systems inspection valves automotive components edge detection gradient methods statistics automobile industry assembling;assembling;statistics;gradient methods;hough transforms;assembled valve key inspection;hough transform;valves;circle detection;automobile industry;circular hough transform real time two step inspection system assembled valve key automobile cylinder head two step vision based approach sobel edge detector gradient pair vector method circular shaped valve key canny edge detector;real time systems	This paper presents a two-step vision-based approach for real time inspection of the assembled valve keys in automobile cylinder heads. In the first step a combination of Sobel edge detector with gradient pair vector method is used for the detection of circular shaped valve keys. This method is very fast but also highly sensitive to noise, resulting in a relatively high percentage of false negatives. The second inspection step employs Canny edge detector with circular Hough transform. The images of the valve keys rejected in the first step pass through the second inspection step. This procedure differentiates between the false negatives and the true negatives. The cylinder heads rejected in the second step are separated afterwards from the main stream of the transportation line. In the statistics obtained, the first inspection step successfully rejected all of the wrongly assembled valve keys. The false rejection rate was around 4.6% of the total number of the valve keys rejected. The second inspection step successfully differentiated between true negatives and false negatives, reducing false negatives to 0%.	canny edge detector;cylinder seal;edge detection;gradient;hough transform;key (cryptography);rejection sampling;sobel operator	Faisal Shafait;Muhammad Saleem Pervez;Arish Asif Qazi;J. Wollnack;Thomas Trittin	2004	Third International Conference on Image and Graphics (ICIG'04)	10.1109/ICIG.2004.117	hough transform;computer vision;edge detection;inspection;machine vision;computer science	Visualization	44.7638805069082	-43.0205454548034	123990
9a266f54b00327794443c983ce5fce00ba6a07f2	topological map building and path estimation using global-appearance image descriptors		Visual-based navigation has been a source of numerous researches in the field of mobile robotics. In this paper we present a topological map building and localization algorithm using wide-angle scenes. Global-appearance descriptors are used in order to optimally represent the visual information. First, we build a topological graph that represents the navigation environment. Each node of the graph is a different position within the area, and it is composed of a collection of images that covers the complete field of view. We use the information provided by a camera that is mounted on the mobile robot when it travels along some routes between the nodes in the graph. With this aim, we estimate the relative position of each node using the visual information stored. Once the map is built, we propose a localization system that is able to estimate the location of the mobile not only in the nodes but also on intermediate positions using the visual information. The approach has been evaluated and shows good performance in real indoor scenarios under realistic illumination conditions.	content-based image retrieval;graph (discrete mathematics);internationalization and localization;interpolation;mobile robot;quantum phase estimation algorithm;robotics;topological graph;visual descriptor;weight function	Francisco Amorós;Luis Payá;Óscar Reinoso;Walterio W. Mayol-Cuevas;Andrew Calway	2013		10.5220/0004485203850392	topology	Robotics	51.565856555067995	-38.51044185076191	124135
13b66c577a0b6bb7b07778654b0c07eacbd64351	stereovision-based head tracking using color and ellipse fitting in a particle filter	real time;head tracking;color histogram;particle filter;image sequence;weight function;mobile agent;similarity measure;scale invariance;spatial information;ellipse fitting	This paper proposes the use of a particle filter combined with color, depth information, gradient and shape features as an efficient and effective way of dealing with tracking of a head on the basis of image stream coming from a mobile stereovision camera. The head is modeled in the 2D image domain by an ellipse. A weighting function is used to include spatial information in color histogram representing the interior of the ellipse. The lengths of the ellipse’s minor axis are determined on the basis of depth information. The dissimilarity between the current model of the tracked object and target candidates is indicated by a metric based on Bhattacharyya coefficient. Variations of the color representation as a consequence of ellipse’s size change are handled by taking advantage of the scale invariance of the similarity measure. The color histogram and parameters of the ellipse are dynamically updated over time to discriminate in the next iteration between the candidate and actual head representation. This makes possible to track not only a face profile which has been shot during initialization of the tracker but in addition different profiles of the face as well as the head can be tracked. Experimental results which were obtained on long image sequences in a typical office environment show the feasibility of our approach to perform tracking of a head undergoing complex changes of shape and appearance against a varying background. The resulting system runs in real-time on a standard laptop computer installed on a real mobile agent.	algorithm;apache axis;coefficient;color depth;color histogram;computation;curve fitting;experiment;facial recognition system;gradient;image noise;internationalization and localization;iteration;jaccard index;laptop;mobile agent;nonlinear system;overhead (computing);particle filter;real-time clock;real-time computing;robot control;robotics;similarity measure;stereopsis;weight function	Bogdan Kwolek	2004		10.1007/978-3-540-24673-2_16	color histogram;computer vision;simulation;weight function;particle filter;computer science;scale invariance;mobile agent;mathematics;spatial analysis;statistics;computer graphics (images)	Vision	48.306833413890786	-43.41013571739647	124411
2c798ba6bde4649ac415bf96ac6f4fcd5ee8d5e5	human activity recognition with 2d and 3d cameras		This presentation will cover human activity recognition by using conventional 2D video cameras as well as the recently developed 3D depth cameras. I’ll first give an overview on the interest- point based approach which has become a popular research direction in the past few years for 2D based activity recognition. In addition to the conventional classification problem, I’ll discuss the problem of detection (spacetime localization) as well as the example-based search where the amount of labelled data is extremely small. The second part of the talk will focus on activity recognition with 3D depth cameras. I’ll describe some of the recently developed visual representations and machine learning frameworks for 3D data analysis.	activity recognition	Zicheng Liu	2012		10.1007/978-3-642-33275-3_3	artificial intelligence;computer science;computer vision;activity recognition	Vision	39.86121048708896	-43.09135208342103	124432
72a88fe919da5e4d8fbfa746be9ac16bf16a6422	a low-complexity sensor fusion algorithm based on a fiber-optic gyroscope aided camera pose estimation system	visual tracking camera pose estimation fiber optic gyroscope low complexity sensor fusion	Visual tracking, as a popular computer vision technique, has a wide range of applications, such as camera pose estimation. Conventional methods for it are mostly based on vision only, which are complex for image processing due to the use of only one sensor. This paper proposes a novel sensor fusion algorithm fusing the data from the camera and the fiber-optic gyroscope. In this system, the camera acquires images and detects the object directly at the beginning of each tracking stage; while the relative motion between the camera and the object measured by the fiber-optic gyroscope can track the object coordinate so that it can improve the effectiveness of visual tracking. Therefore, the sensor fusion algorithm presented based on the tracking system can overcome the drawbacks of the two sensors and take advantage of the sensor fusion to track the object accurately. In addition, the computational complexity of our proposed algorithm is obviously lower compared with the existing approaches (86% reducing for a 0.5 min visual tracking). Experiment results show that this visual tracking system reduces the tracking error by 6.15% comparing with the conventional vision-only tracking scheme (edge detection), and our proposed sensor fusion algorithm can achieve a long-term tracking with the help of bias drift suppression calibration.	3d pose estimation;algorithm;computational complexity theory;computer vision;edge detection;fibre optic gyroscope;image processing;image sensor;maxima and minima;optical fiber;sensor web;tracking system;video tracking;zero suppression	Zhongwei Tan;Chuanchuan Yang;Yuliang Li;Yan Yan;Changhong He;Xinyue Wang;Ziyu Wang	2015	Science China Information Sciences	10.1007/s11432-015-5516-2	computer vision;simulation;tracking system;computer science;video tracking;control theory;visual sensor network	Robotics	47.507880071995054	-45.97814091008197	124517
45c7f90bf0cca8881861d98e4671f354b77767c8	sector based scanning and adaptive active tracking of multiple objects	zooming;object tracking;object dynamics;object detection;object scanning	This paper presents an adaptive active tracking system with sector based scanning for a single PTZ camera. Dividing sectors on an image reduces the search space to shorten selection time so that the system can cover many targets. Upon the selection of a target, the system estimates the target trajectory to predict the zooming location with a finite amount of time for camera movement. Advanced estimation techniques using probabilistic reason suffer from the unknown object dynamics and the inaccurate estimation compromises the zooming level to prevent tracking failure. The proposed system uses the simple piecewise estimation with a few frames to cope with fast moving objects and/or slow camera movements. The target is tracked in multiple steps and the zooming time for each step is determined by maximizing the zooming level within the expected variation of object velocity and detection. The number of zooming steps is adaptively determined according to target speed. In addition, the iterative estimation of a zooming location with camera movement time compensates for the target prediction error due to the difference between speeds of a target and a camera. The effectiveness of the proposed method is validated by simulations and real time experiments.		Shung Han Cho;Yunyoung Nam;Sangjin Hong;We-Duke Cho	2011	TIIS	10.3837/tiis.2011.06.005	computer vision;simulation;computer science;operating system;video tracking;zoom;computer graphics (images)	Vision	45.74689202596806	-45.25616675600998	124535
7063e1e337a6efbdebdf22cb0de8dad542f455d3	combining tactile sensing and vision for rapid haptic mapping	robot sensing systems;haptic interfaces robot sensing systems image color analysis chlorine three dimensional displays;three dimensional displays;image color analysis;chlorine;tactile sensors humanoid robots iterative methods manipulators robot vision;haptic interfaces;tactile sensing tactile only mapping method goal locations cluttered foliage environment darci humanoid robot haptically labeled pixels indoor cluttered scenes robot manipulation color based similarity measure rgb d image datasets color plus depth image dense haptic labels iterative algorithm visible surroundings dense haptic map rapid haptic mapping	We consider the problem of enabling a robot to efficiently obtain a dense haptic map of its visible surroundings using the complementary properties of vision and tactile sensing. Our approach assumes that visible surfaces that look similar to one another are likely to have similar haptic properties. We present an iterative algorithm that enables a robot to infer dense haptic labels across visible surfaces when given a color-plus-depth (RGB-D) image along with a sequence of sparse haptic labels representative of what could be obtained via tactile sensing. Our method uses a color-based similarity measure and connected components on color and depth data. We evaluated our method using several publicly available RGBD image datasets with indoor cluttered scenes pertinent to robot manipulation. We analyzed the effects of algorithm parameters and environment variation, specifically the level of clutter and the type of setting, like a shelf, table top, or sink area. In these trials, the visible surface for each object consisted of an average of 8602 pixels, and we provided the algorithm with a sequence of haptically-labeled pixels up to a maximum of 40 times the number of objects in the image. On average, our algorithm correctly assigned haptic labels to 76.02% of all of the object pixels in the image given this full sequence of labels. We also performed experiments with the humanoid robot DARCI reaching in a cluttered foliage environment while using our algorithm to create a haptic map. Doing so enabled the robot to reach goal locations using a single plan after a single greedy reach, while our previous tactile-only mapping method required 5 or more plans to reach each goal.	clutter;color;connected component (graph theory);experiment;greedy algorithm;haptic technology;humanoid robot;iterative method;kinect;map;pixel;relevance;similarity measure;sparse matrix	Tapomayukh Bhattacharjee;Ashwin A. Shenoi;Daehyung Park;James M. Rehg;Charles C. Kemp	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353522	chlorine;computer vision;computer graphics (images)	Robotics	51.54873742813081	-39.92687654878367	124983
56b67b9d5d1454bdcf5461236e5baefaf13e81cb	visual correction for mobile robot homing	monocular vision;lines and homographies;mobile robot;fundamental matrix;mobile robots;robot homing;teaching by doing;vision	We present a method to send a mobile robot to locations specified by images previously taken from these positions, which sometimes has been referred as homing. Classically this has been carried out using the fundamental matrix, but the fundamental matrix is ill conditioned with planar scenes, which are quite usual in man made environments. Many times in robot homing, small baseline images with high disparity due to rotation are compared, where the fundamental matrix also gives bad results. We use a monocular vision system and we compute motion through an homography obtained from automatically matched lines. In this work we compare the use of the homography and the fundamental matrix and we propose the correction of motion directly from the parameters of the 2D homography, which only needs one calibration parameter. It is shown that it is robust, sufficiently accurate and simple.	baseline (configuration management);binocular disparity;condition number;fundamental matrix (computer vision);homography (computer vision);missile guidance;mobile robot;multiple homing;robustness (computer science)	Carlos Sagüés;Josechu J. Guerrero	2005	Robotics and Autonomous Systems	10.1016/j.robot.2004.08.005	homography;mobile robot;computer vision;simulation;computer science;artificial intelligence	Robotics	53.50928650485711	-39.98029906244097	125025
607350abc3f097744400bcae418591d50d966798	upper body tracking using klt and kalman filter	kalman filter;klt;upper body tracking;affine transformation;articulated human body	Abstract   Human monitoring system based on image and sequence analysis is employed in security surveillance systems. For this purpose upper-body tracking is often needed. However, tracking challenges such as variations and similarity in appearance can mislead limbs tracker system. This paper describes a novel framework for visual tracking of human upper body parts in an indoor environment which can handle the tracking challenges. It is based on Kanade-Lucas-Tomasi (KLT) and motion model Kalman filter approach. In our approach, different upper body limbs are tracked by the KLT methods and then the motion model is imposed to the Kalman filter to predict and estimate the best tracked patch of KLT tracking results. These characteristics make our approach suitable for visual surveillance applications. Experiments on different datasets show the effciency of our approach on tracking the human upper body limbs.	kalman filter	Pouya Bagherpour;Seyed Ali Cheraghi;Musa Mohd Mokji	2012		10.1016/j.procs.2012.09.127	kalman filter;computer vision;simulation;tracking system;computer science;control theory;affine transformation	Robotics	45.929028047636045	-45.43691884233528	125080
20acbddc83b10a4576b602f6ab77e0e9919c2fba	a robust hough transform technique for description of multiple line segments in an image	ccd camera;computer vision edge detection image segmentation hough transforms;image segmentation;autonomous vehicle;edge detection;computer vision;robustness image segmentation voting robot vision systems testing machine vision robot kinematics manufacturing systems vehicle detection roads;machine vision;hough transforms;hough transform;manufacturing system;autonomous vehicles robust hough transform multiple line segments image lines detection accumulator array input image peak formation end points coordinates accuracy algorithm machine vision robotics manufacturing systems lane markers curve signs road scene ccd camera	The process of using the Hough transform (HT) to detect lines in an image involves the computation of the HT for the entire image, accumulating votes in an accumulator array and searching the array for peaks which hold information of potential lines present in the input image. The process of peak formation generates a butterfly shaped spread of votes in the accumulator array. The authors have used this property to adaptively define windows of interest around a detected peak to facilitate the description of multiple line segments within an image in terms of the coordinates of their end points. The developed technique has been employed to test several images composed of multiple line segments and the results in terms of accuracy of the determination of line segment mid points are presented. While most methods which employ the HT to detect line segments cannot handle the case of separate line segments formed by a colinear set of points, it is shown that the developed method can successfully do so. This algorithm would find applications in different areas of machine vision like robotics and manufacturing systems. Results of the application of the developed method, to detect lane markers and curve signs from a road scene captured by a CCD camera, to aid in the maneuvering of autonomous vehicles are presented.	hough transform;image	Varsha Kamat;Subramaniam Ganesan	1998		10.1109/ICIP.1998.723460	hough transform;computer vision;simulation;edge detection;machine vision;computer science;image segmentation;charge-coupled device;computer graphics (images)	Vision	50.01123418525008	-39.50600255449655	125308
73c2ac494fb919afb0330f7551cfd89bf667a91a	2d-to-3d visual human motion converting system for home optical motion capture tool and 3-d smart tv	three dimensional television image reconstruction image sequences motion estimation;nonlinear optics;2d universal joint model 2d visual human motion converting system 3d visual human motion converting system home optical motion capture tool 3d smart tv image sequences single channel 3d reconstruction processes human body joint;joints;visualization;visualization optical imaging three dimensional displays cameras nonlinear optics;optical imaging;yttrium;three dimensional displays;2d to 3d visual human motion converter analytical solution type process human motion capture single channel smart tv;cameras	Although 2-D visual human motion analyzers that work with a sequence of images have been widely studied, some consumer electronics equipment, such as home optical motion capture tools and smart TVs, require 3-D motion data. For this purpose, this paper proposes a 2D-to-3D visual human motion converter that reconstructs 3-D visual motion from the 2-D visual human motion in a sequence of images. Because home optical motion capture tools and smart TVs have a single channel, this paper designs a proposed 2D-to-3D converter that has a single channel. A common difficulty for the single channel is that most contemporary 3-D reconstruction processes based on 2-D motion are nonlinear iterative types. For more convenient use in real-time applications, this paper proposes an analytical solution type of 3-D reconstruction process instead of the nonlinear iterative types. To derive an analytical solution, this paper proposes an idea that formulates a human body joint as a 2-D universal joint model instead of the more common 3-D spherical joint models. To overcome the major limitation of analytical solution-type processes, i.e., the low accuracy, this paper formulates the estimation process as an optimization problem. The process thus designed is then applied to each joint in the human body, one after another. Performance evaluation shows that the proposed 2D-to-3D visual human motion converter has nearly the same high accuracy as a contemporary nonlinear iterative-type approach.1.	iteration;iterative method;kinesiology;mathematical optimization;motion capture;nonlinear system;optimization problem;performance evaluation;real-time clock;smart tv	Youngmo Han	2015	IEEE Systems Journal	10.1109/JSYST.2014.2322253	nonlinear optics;computer vision;match moving;structure from motion;simulation;visualization;telecommunications;quarter-pixel motion;engineering;yttrium;motion estimation;optical imaging;motion field;computer graphics (images)	Vision	50.32548369958075	-45.92626025577454	125359
5dc6a1be53718f48a4bebee2c608cfb6b6211f60	independent motion detection in 3d scenes	moving object;motion estimation;motion detection layout cameras shape geometry object detection video surveillance computerized monitoring tracking geometrical optics;camera motion;constraint theory;constraint verification independent motion detection sparse 3d scenes independently moving objects multi view camera motion constraint epipolar trilinear constraint shape constancy constraint correspondences optical flow plane plus parallax decomposition progressive constraint introduction;motion detection;constraint theory motion estimation	This paper presents an algorithmic approach to the problem of detecting independently moving objects in 3D scenes that are viewed under camera motion. There are two fundamental constraints that can be exploited for the problem : (i) two/multi-view camera motion constraint (for instance, the epipolar/trilinear constraint) and (ii) shape constancy constraint. Previous approaches to the problem either use only partial constraints, or rely on dense correspondences or ow. We employ both the fundamental constraints in an algorithm that does not demand a priori availability of correspondences or ow. Our approach uses the plane-plusparallax decomposition to enforce the two constraints. It is also demonstrated that for a class of scenes, called sparse 3D scenes in which genuine parallax and independent motions may be confounded, how the plane-plus-parallax decomposition allows progressive introduction and veri cation of the fundamental constraints. Results of the algorithm on some di cult sparse 3D scenes are promising.	algorithm;epipolar geometry;parallax;sensor;sparse matrix	Harpreet S. Sawhney;Yanlin Guo;Jane C. Asmuth;Rakesh Kumar	1999		10.1109/ICCV.1999.791281	computer vision;mathematical optimization;computer science;motion estimation;mathematics;geometry	Vision	52.09703909882931	-50.80826376199842	125378
26131a75b12312ecf1d6b3467d7adf2ba3b60588	3d structure from motion with fourier descriptor transformation	image sequence;structure from motion sfm;2d motion shift rotation	The simultaneous recovery of three-dimensional (3D) structure from motion (SfM) for the sequences of images, is one of the more difficult problems in computer vision. Classical approaches to the problem rely on using algebraic techniques to solve for these unknowns given two or more image. Motion analysis and 3D shape estimation based on the estimated motion is an important problem in computer vision. The correspondence problem is an important tool in SfM where in this paper a general 3D motion based on a simple rotation, tilt, roll and translation is proposed, and then is used for 3D shape estimation. The current work expands the 2D motion estimation (shift, rotation) to accommodate general 3D motion (shift, rotation, tilt and roll). The proposed work in this paper of motion estimation of moving object is based on Fourier Descriptor Transformation (FDT) analysis before and after motion. The FDT is also used to resolve the correspondence problem in a sequence of images. We test our method on several large-scale photo collections, show the efficacy of the introduced approach to improve reconstruction accuracy.	structure from motion	Gamal F. Elhady	2013	IJPRAI	10.1142/S0218001413550069	computer vision;match moving;structure from motion;quarter-pixel motion;motion estimation;mathematics;geometry;motion field;linear motion	Vision	53.08990138169671	-50.29815964618467	125431
8b9e94fb3bb64389e9765ffde365862231b5972c	fast eye tracking and feature measurement using a multi-stage particle filter		Eye trackers – systems that measure the activity of the eyes – are nowadays used in creative ways into a variety of domains: medicine, psychology, automotive industry, marketing etc. This paper presents a real time method for tracking and measuring eye features (iris position, eye contour, blinks) in video frames based on particle filters. We propose a coarse-to-fine approach to solve the eye tracking problem: a first particle filter is used to roughly estimate the position of the iris centers. Next, this estimate is analysed to decide the state of the eyes: opened or half-opened/closed. If the eyes are opened, two independent particles filters are used to determine the contour of each eye. Our algorithm takes less than 11 milliseconds on a regular PC.	approximation algorithm;camera resectioning;experiment;eye tracking;multiface;optical flow;particle filter;personal computer;support vector machine;thresholding (image processing)	Radu Danescu;Adrian Sergiu Darabant;Diana Borza	2017		10.5220/0006130202580265	tracking system	Vision	46.7298332924612	-42.49183092497375	125519
195e588dd249a8a806ac5b5df350270d22fdd723	using temporal covariance of motion and geometric features via boosting for human fall detection	health and well-being;human fall detection;intelligent surveillance systems;safety and security	Fall induced damages are serious incidences for aged as well as young persons. A real-time automatic and accurate fall detection system can play a vital role in timely medication care which will ultimately help to decrease the damages and complications. In this paper, we propose a fast and more accurate real-time system which can detect people falling in videos captured by surveillance cameras. Novel temporal and spatial variance-based features are proposed which comprise the discriminatory motion, geometric orientation and location of the person. These features are used along with ensemble learning strategy of boosting with J48 and Adaboost classifiers. Experiments have been conducted on publicly available standard datasets including Multiple Cameras Fall (with 2 classes and 3 classes) and UR Fall Detection achieving percentage accuracies of 99.2, 99.25 and 99.0, respectively. Comparisons with nine state-of-the-art methods demonstrate the effectiveness of the proposed approach on both datasets.	ability to sit question;accidental falls;adaboost;class;closed-circuit television;conceptualization (information science);conflict (psychology);data curation;digital curation;dispensing medication;ensemble learning;experiment;hearing impaired persons;lambertian reflectance;numerous;obstruction;patients;real-time computing;real-time locating system;real-time transcription;repository;run time (program lifecycle phase);sample variance;tissue damage;traumatic injury;ur, ur/web;uridine;video content analysis;algorithm;videocassette	Syed Farooq Ali;Reamsha Khan;Arif Mahmood;Malik Tahir Hassan;Moongu Jeon	2018		10.3390/s18061918	electronic engineering;adaboost;boosting (machine learning);machine learning;engineering;damages;ensemble learning;covariance;artificial intelligence;medication care;c4.5 algorithm	Vision	39.77361081164553	-44.763014608871025	125575
135e62f623f95d932b0073eaab94288ce3fc34c0	robust visual tracking based on simplified biologically inspired features	image sampling;object representation;bayes methods;particle filter visual tracking sbif;biological system modeling;sbif;indexing terms;robustness target tracking biological system modeling bayesian methods particle tracking sampling methods particle filters inference algorithms lighting immune system;visualization;computational modeling;particle filter;image representation;pixel;robustness;sampling importance resampling particle filter robust visual tracking simplified biologically inspired features object representation bhattacharyya coefficient bayesian state inference tracking;lighting;numerical experiment;target tracking;visual tracking;particle filtering numerical methods bayes methods image representation image sampling;particle filtering numerical methods	We address the problem of robust appearance-based visual tracking. First, a set of simplified biologically inspired features (SBIF) is proposed for object representation and the Bhattacharyya coefficient is used to measure the similarity between the target model and candidate targets. Then, the proposed appearance model is combined into a Bayesian state inference tracking framework utilizing the SIR (sampling importance resampling) particle filter to propagate sample distributions over time. Numerous experiments are conducted and experimental results demonstrate that our algorithm is robust to partial occlusions and variations of illumination and pose, resistent to nearby distractors, as well as possesses the state-of-the-art tracking accuracy.	algorithm;coefficient;experiment;jaccard index;particle filter;robustness (computer science);sampling (signal processing);video tracking	Min Li;Zhaoxiang Zhang;Kaiqi Huang;Tieniu Tan	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413456	computer vision;visualization;index term;particle filter;eye tracking;computer science;machine learning;pattern recognition;lighting;mathematics;computational model;pixel;robustness	Vision	45.38301027142246	-49.29713092776414	125630
14a0fd098092d7b66113981dffc25efbef3353bd	trajectory-driven point cloud compression techniques for visual slam	trajectory control data compression pareto analysis robot vision sampling methods slam robots;surface topography;splines mathematics;trajectory cameras three dimensional displays visualization splines mathematics simultaneous localization and mapping surface topography;visualization;trajectory;6d visual relocalisation performance trajectory driven point cloud compression techniques visual slam data compression strategy traveled trajectory analysis compressing scene structure compact map representations statistical evaluation pareto analysis baseline compression methods point geometry random sampling;three dimensional displays;simultaneous localization and mapping;cameras	We develop and evaluate methods based on a novel data compression strategy for visual SLAM that uses traveled trajectory analysis. Beyond compressing scene structure based purely on geometry, we aim at developing compact map representations that are useful for re-exploration while preserving scene structure. Our work is evaluated on data collected from a visual sensor and exploits the information intrinsic to the trajectory of exploration together with the visual information of map points. We perform rigorous statistical evaluation and Pareto analysis to show how this approach compares with three widely used baseline compression methods: k-means on point geometry, keyframes and random sampling. Results indicate that compressing maps to levels of 25% or even less of the original data is possible, while preserving good 6D visual relocalisation performance.	baseline (configuration management);data compression;k-means clustering;key frame;map;pareto efficiency;point cloud;sampling (signal processing);simultaneous localization and mapping	Luis Contreras;Walterio W. Mayol-Cuevas	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353365	computer vision;simulation;visualization;computer science;artificial intelligence;trajectory;mathematics;computer graphics (images);simultaneous localization and mapping	Robotics	53.09272087087461	-44.895423409356994	125939
87e664d3f330284b037a74d0380ec02a8daee7e0	clips - a camera and laser-based indoor positioning system	6 dof;optical;real time tracking;positioning;clips;indoor;indoor environment;indoor positioning;pose estimation	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	algorithm;clips;ct scan;francis;function model;graphical user interface;graphics processing unit;indoor positioning system;microsoft outlook for mac;nl (complexity);primary source;real-time clock;skolem normal form;switzerland;video projector	Sebastian Tilch;Rainer Mautz	2013	J. Location Based Services	10.1080/17489725.2012.688643	computer vision;camera auto-calibration;simulation;pose;computer science;six degrees of freedom;computer graphics (images)	Robotics	51.92333589363822	-43.47362357032655	125953
7b05012cccb687725984af91bcb0bd82ff6d26de	enhanced wifi localization system based on soft computing techniques to deal with small-scale variations in wireless sensors	robot localization;fuzzy rule based system;signal strength;wireless sensor;access point;partially observed markov decision process;soft computing;wifi signal strength sensor;test bed;robotics;wireless localization;local system;fuzzy logic;fuzzy modeling;large scale;research group;path loss;uncertainty relation;navigation system;department of electronics;esafety;fuzzy model;university of alcala	The framework of this paper is robot localization inside buildings by means of wireless localization systems. Such kind of systems make use of the Wireless Fidelity (WiFi) signal strength sensors which are becoming more and more useful in the localization stage of several robotic platforms. Robot localization is usually made up of two phases: training and estimation stages. In the former, WiFi signal strength of all visible Access Points (APs) are collected and stored in a database or WiFi map. In the latter, the signal strengths received from all APs at a certain position are compared with the WiFi map to estimate the robot location. Hence, WiFi localization systems exploit the well-known path loss propagation model due to large-scale variations of WiFi signal to determine how closer the robot is to a certain AP. Unfortunately, there is another kind of signal variations called small-scale variations that have to be considered. They appear when robots move under the wavelength . In consequence, a chaotic noise is added to the signal strength measure yielding a lot of uncertainty that should be handled by the localization model. While lateral and orientation errors in the robot positioning stage are well studied and they remain under control thanks to the use of robust low-level controllers, more studies are needed when dealing with small-scale variations. Moreover, if the robot can not use a robust low-level controller because, for example, the environment is not organized in perpendicular corridors, then lateral and orientation errors can be significantly increased yielding a bad global localization and navigation performance. The main goal of this work is to strengthen the localization stage of our previous WiFi Partially Observable Markov Decision Process (POMDP) Navigation System with the aim of dealing effectively with small-scale variations. In addition, looking for the applicability of our system to a wider variety of environments, we relax the necessity of having a robust low-level controller. To do that, this paper proposes the use of a Soft Computing based system to tackle with the uncertainty related to both the small-scale variations and the lack of a robust low-level controller. The proposed system is actually implemented in the form of a Fuzzy Rule-based System and it has been evaluated in two real test-beds and robotic platforms. Experimental results show how our system is easily adaptable to new environments where classical an no localization techniques c	calculus of variations;fuzzy rule;high- and low-level;internationalization and localization;lateral thinking;markov chain;partially observable markov decision process;regular language description for xml;robot;robotic mapping;rule-based system;sensor;signal-to-noise ratio;soft computing;software propagation;whole earth 'lectronic link	José M. Alonso;Manuel Ocaña;Noelia Hernández;Fernando Herranz;Angel Llamazares;Miguel Ángel Sotelo;Luis Miguel Bergasa;Luis Magdalena	2011	Appl. Soft Comput.	10.1016/j.asoc.2011.07.015	fuzzy logic;signal strength;embedded system;real-time computing;simulation;computer science;artificial intelligence;path loss;machine learning;soft computing;robotics;local system;testbed	Robotics	44.01088612427184	-40.24757006322073	126100
32705c587bebc43df1a3cf02587f1d9444d7903c	self-adapting part-based pedestrian detection using a fish-eye camera		Nowadays, fish-eye cameras play an increasingly important role in intelligent vehicles because of its wide field of view. Using fish-eye camera, pedestrians around the vehicles could be monitored expediently, but the problem of pedestrian distortion has always existed. This paper creates a new warping pedestrian benchmark using imaging principle of the fish-eye camera based on ETH pedestrian benchmark. With this practical benchmark, warping pedestrians are trained differently according to the position in fish-eye images. A self-adapting part-based algorithm is proposed to detect pedestrian with different degrees of deformation. Moreover, GPU is used to accelerate the whole algorithm to guarantee the real-time performance. Experiments show that the algorithm has competitive accuracy.	algorithm;benchmark (computing);distortion;experiment;graphics processing unit;image warping;mathematical optimization;pedestrian detection;real-time clock	Yeqiang Qian;Ming Yang;Chunxiang Wang;Bing Wang	2017	2017 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2017.7995695	image warping;pedestrian;simulation;computer vision;artificial intelligence;distortion;benchmark (computing);engineering;pedestrian detection;field of view	Robotics	42.51377792278362	-40.56184380946234	126113
b1aa9e9c634dbc5cac50fac645c0778a0232c794	applying visual image and satellite positioning for fast vehicle detection and lane marking recognition	satellite positioning;driver assistance;image recognition;traffic accident;automobiles;road accidents;visual image;road traffic;vision gps sensing;road traffic accident;automated highways;vehicle detection;fast vehicle detection;image plane coordinate;data mining;satellites vehicle detection image recognition road accidents vehicles delay image converters vehicle driving global positioning system alarm systems;visualization;gps;roads;global positioning system;car;coordinate transformation;satellite positioning vision gps sensing vehicle detection lane marking recognition;road traffic automated highways automobiles driver information systems global positioning system image recognition object detection road accidents;collision warning system visual image satellite positioning fast vehicle detection lane marking recognition road traffic accident coordinate transformation image plane coordinate gps car driver assistance;vehicles;lane marking recognition;collision warning system;driver information systems;cameras;object detection;real time systems	Vision difference or inattention to the traffic condition often causes the unnecessary traffic accidents while driving. Therefore, this study is to fast detect both the preceding vehicle and lane marking at the least response time 0.5972 seconds. After coordinate transformation converts image plane coordinates of the preceding vehicle to GPS coordinates, the headway between cars is obtained to assist driver in successfully identifying traffic situation. The reduced accident rate for the proposed system is about 70~82%, which performance is superior to the current collision warning system built in VOLVO, Mercedes-Benz, or BMW cars.	image plane;item unique identification;mercedes-euklid;response time (technology)	Bao Rong Chang;Hsiu Fen Tsai	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.160	computer vision;global positioning system;computer science;computer security	Robotics	41.7777901864089	-43.49720049466081	126131
004a1a6e42b3683ffd1c3a9a05442292ae9be5a1	towards geographical referencing of monocular slam reconstruction using 3d city models: application to real-time accurate vision-based localization	trajectory;real time systems;geographic information systems;image reconstruction;real time;solid modeling;geometry;simultaneous localization and mapping;computational geometry;bundle adjustment;computer vision;layout;cost function	In the past few years, lots of works were achieved on Simultaneous Localization and Mapping (SLAM). It is now possible to follow in real time the trajectory of a moving camera in an unknown environment. However, current SLAM methods are still prone to drift errors, which prevent their use in large-scale applications. In this paper, we propose a solution to reduce those errors a posteriori. Our solution is based on a postprocessing algorithm that exploits additional geometric constraints, relative to the environment, to correct both the reconstructed geometry and the camera trajectory. These geometric constraints are obtained through a coarse 3D modelisation of the environment, similar to those provided by GIS database. First, we propose an original articulated transformation model in order to roughly align the SLAM reconstruction with this 3D model through a non-rigid ICP step. Then, to refine the reconstruction, we introduce a new bundle adjustment cost function that includes, in a single term, the usual 3D point/ID observation consistency constraint as well as the geometric constraints provided by the 3D model. Results on large-scale synthetic and real sequences show that our method successfully improves SLAM reconstructions. Besides, experiments prove that the resulting reconstruction is accurate enough to be directly used for global relocalization applications.	3d city models;3d modeling;algorithm;align (company);augmented reality;bundle adjustment;experiment;geographic information system;loss function;polygonal modeling;real-time clock;real-time locating system;simultaneous localization and mapping;synthetic intelligence	Pierre Lothe;Steve Bourgeois;Fabien Dekeyser;Eric Royer;Michel Dhome	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206662	iterative reconstruction;layout;computer vision;simulation;computational geometry;computer science;trajectory;geometry;bundle adjustment;solid modeling;computer graphics (images);simultaneous localization and mapping	Vision	52.409008122714035	-48.248015840480186	126137
6698d766cb3ca2f240cf1815fb0a246435792108	appearance and geometry fusion for enhanced dense 3d alignment	histograms;three dimensional mapping;memory management;feature extraction shape histograms iterative closest point algorithm vectors robustness memory management;image matching;reconstruction;geometry;rgb d camera;shape;vectors;image colour analysis;feature extraction;image registration;registration;three dimensional mapping descriptor rgb d camera reconstruction registration;robustness;solid modelling geometry image colour analysis image matching image registration;image registration geometry fusion enhanced dense 3d alignment novel rgb d feature descriptor binary appearance and shape elements base shape information intensity information image matching rgb point clouds scene alignment indoor textured depth maps cloud alignment;iterative closest point algorithm;descriptor;solid modelling	This work proposes a novel RGB-D feature descriptor called Binary Appearance and Shape Elements (BASE) that efficiently combines intensity and shape information to improve the discriminative power and enable an enhanced and faster matching process. The new descriptor is used to align a set of RGB point clouds to generate dense three dimensional models of indoor environments. We compare the performance of state-of-the-art feature descriptors with the proposed descriptor for scene alignment through the registration of multiple indoor textured depth maps. Experimental results show that the proposed descriptor outperforms the other approaches in computational cost, memory consumption and match quality. Additionally, experiments based on cloud alignment show that the BASE descriptor is suitable to be used in the registration of RBG-D data even when the environment is partially illuminated.	3d scanner;algorithmic efficiency;align (company);central processing unit;computation;displacement mapping;experiment;feature model;field of view in video games;image noise;map;point cloud;sensor;visual descriptor	Erickson Rangel do Nascimento;William Robson Schwartz;Gabriel L. Oliveira;Antônio Wilson Vieira;Mario Fernando Montenegro Campos;Daniel Balbino de Mesquita	2012	2012 25th SIBGRAPI Conference on Graphics, Patterns and Images	10.1109/SIBGRAPI.2012.16	computer vision;geography;gloh;pattern recognition;computer graphics (images)	Vision	48.21905877182629	-50.13007952837248	126257
4e5dc457434ea6083f4badb01bbc4c1d34fb365f	a robust vision-based sensor fusion approach for real-time pose estimation	nonlinear filters;extended kalman filter robust vision based sensor fusion approach real time pose estimation object pose estimation augmented reality motion capture visual servoing monocular camera multicamera sensor fusion techniques sensor defects kalman based sensor fusion approach camera motion image occlusion vision based pose estimation algorithms;sensor fusion 3 d object pose estimation adaptive extended kalman filter iterative robust estimation;image fusion;kalman filters;pose estimation cameras computer vision image fusion kalman filters nonlinear filters;robust estimation;computer vision;adaptive;iterative;sensor fusion;extended kalman filter;cameras;3 d object pose estimation;pose estimation	Object pose estimation is of great importance to many applications, such as augmented reality, localization and mapping, motion capture, and visual servoing. Although many approaches based on a monocular camera have been proposed, only a few works have concentrated on applying multicamera sensor fusion techniques to pose estimation. Higher accuracy and enhanced robustness toward sensor defects or failures are some of the advantages of these schemes. This paper presents a new Kalman-based sensor fusion approach for pose estimation that offers higher accuracy and precision, and is robust to camera motion and image occlusion, compared to its predecessors. Extensive experiments are conducted to validate the superiority of this fusion method over currently employed vision-based pose estimation algorithms.	3d pose estimation;algorithm;augmented reality;computational complexity theory;computer;computers;concentrate dosage form;experience;experiment;extended kalman filter;internationalization and localization;iteration;iterative method;motion capture;movement;overhead (computing);population parameter;real-time clock;real-time transcription;visual servoing	Akbar Assa;Farrokh Janabi-Sharifi	2014	IEEE Transactions on Cybernetics	10.1109/TCYB.2013.2252339	kalman filter;computer vision;simulation;pose;iteration;3d pose estimation;computer science;adaptive behavior;control theory;sensor fusion;extended kalman filter;moving horizon estimation;image fusion	Robotics	46.15026739659863	-46.574717023879636	126280
16ea25c9fba29056e018c66b2fa00a08491b18bc	optical flow based head movement and gesture analyzer (ohmega)	image segmentation;magnetic heads vectors optical imaging integrated optics estimation face;object tracking;higher level semantic information optical flow based head movement and gesture analyzer ohmega head gesture segmentation fixation states moving states optical flow tracking intermittent head pose estimation elemental forms;gesture recognition;object tracking gesture recognition image segmentation image sequences;image sequences	Automatically identifying and analyzing head gestures is useful in many situations like smart meeting rooms and intelligent driver assistance. In this paper, we show that head movements can be broken into its elemental forms (i.e. moving and fixation states) and combinations of these elemental forms give rise to various head gestures. Our approach which we term, Optical flow based Head Movement and Gesture Analyzer (OHMeGA), segments head gestures into moving and fixation states using optical flow tracking and intermittent head pose estimation. OHMeGA runs in real-time, is simple to implement and set up, is robust and is accurate. Furthermore, segmenting head gestures into its elemental forms gives access to higher level semantic information such as fixation time and rate of head motion. Experimental analysis shows promising results.	elemental;gesture recognition;optical flow;real-time clock;real-time computing	Sujitha Martin;Cuong Tran;Mohan Manubhai Trivedi	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		computer vision;speech recognition;computer science;video tracking;gesture recognition;image segmentation	Robotics	40.00407949068396	-47.90378629011452	126311
b06782ccf26b5054a5aa7a8fa57cd3bf4d966c7b	detection of motorcyclists without helmet in videos using convolutional neural network		In order to ensure the safety measures, the detection of traffic rule violators is a highly desirable but challenging task due to various difficulties such as occlusion, illumination, poor quality of surveillance video, varying whether conditions, etc. In this paper, we present a framework for automatic detection of motorcyclists driving without helmets in surveillance videos. In the proposed approach, first we use adaptive background subtraction on video frames to get moving objects. Later convolutional neural network (CNN) is used to select motorcyclists among the moving objects. Again, we apply CNN on upper one fourth part for further recognition of motorcyclists driving without a helmet. The performance of the proposed approach is evaluated on two datasets, IITH_Helmet_1 contains sparse traffic and IITH_Helmet_2 contains dense traffic, respectively. The experiments on real videos successfully detect 92.87% violators with a low false alarm rate of 0.5% on an average and thus shows the efficacy of the proposed approach.	artificial neural network;background subtraction;closed-circuit television;convolutional neural network;deep learning;experiment;sparse matrix	C. Vishnu;Dinesh Singh;C. Krishna Mohan;Sobhan Babu	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966233	artificial intelligence;convolutional neural network;computer science;pattern recognition;feature extraction;artificial neural network;computer vision;constant false alarm rate;background subtraction	Vision	41.101251495526924	-44.983401324932565	126349
214e36347560bf00d55b7a21d52a4b15b3c86a14	automatic face tracking system using quadrotors: control by goal position thresholding	senior citizens;face;robot vision systems;cameras;tracking	This paper proposes a human face tracking system for obtaining elderly people's facial images, which can be used to estimate their individual emotion. Quadrotors are used to overcome occlusion and obtain closer facial images, while Kinect sensors provide human detection and quadrotor navigation. Noise from the measured head position results in vibration of the goal position, and subsequently the quadrotor, which can cause blurred images and safety problem. In order to improve the stability of the quadrotor, we propose an algorithm using threshold to fix the quadrotor's goal position. Performance of the algorithm is evaluated by using the detected positions of the quadrotor and is compared with tracking without threshold algorithm, as well as with different threshold values. Based on these positions, face tracking results are also calculated by simulating projection of the face in real world onto the image plane and evaluating the quality of the obtained face.	algorithm;facial motion capture;image plane;kinect;sensor;simulation;thresholding (image processing);tracking system	Veerachart Srisamosorn;Noriaki Kuwahara;Atsushi Yamashita;Taiki Ogata;Jun Ota	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090515	face;computer vision;simulation;tracking	Robotics	47.20085772801518	-42.01627238072099	126387
004ff13e81764de0c4a51e0c2d707a75a367bd12	generative face alignment through 2.5d active appearance models	image alignment;lucas kanade;aam;robust fitting;2 5d models;2 5d aam;normalization forwards additive;active appearance models;simultaneous forwards additive	This work addresses the matching of a 3D deformable face model to 2D images through a 2.5D Active Appearance Models (AAM). We propose a 2.5D AAM that combines a 3D metric Point Distribution Model (PDM) and a 2D appearance model whose control points are defined by a full perspective projection of the PDM. The advantage is that, assuming a calibrated camera, 3D metric shapes can be retrieved from single view images. Two model fitting algorithms and their computational efficient approximations are proposed: the Simultaneous Forwards Additive (SFA) and the Normalization Forwards Additive (NFA), both based on the Lucas-Kanade framework. The SFA algorithm searches for shape and appearance parameters simultaneously whereas the NFA projects out the appearance from the error image and searches only for the shape parameters. SFA is therefore more accurate. Robust solutions for the SFA and NFA are also proposed in order to take into account the selfocclusion or partial occlusion of the face. Several performance evaluations for the SFA, NFA and theirs efficient approximations were performed. The experiments include evaluating the frequency of converge, the fitting performance in unseen data and the tracking performance in the FGNET Talking Face sequence. All results show that the 2.5D AAM can outperform both the 2D+3D combined models and the 2D standard methods. The robust extensions to occlusion were tested on a synthetic sequence showing that the model can deal efficiently with large head rotation. Preprint submitted to CVIU Computer Vision and Image Understanding	2.5d;3d projection;active appearance model;additive model;algorithm;approximation;computer vision;converge;curve fitting;experiment;kanade–lucas–tomasi feature tracker;nondeterministic finite automaton;point distribution model;synthetic intelligence	Pedro Martins;Rui Caseiro;Jorge P. Batista	2013	Computer Vision and Image Understanding	10.1016/j.cviu.2012.11.010	lucas–kanade method;computer vision;active appearance model;simulation;computer science;machine learning;mathematics	Vision	48.16962052003981	-50.1457314957906	126404
1e7af1378a56b5d643b13015f2ed822107e35631	multi-resolution surfel maps for efficient dense 3d modeling and tracking	dense indoor scene mapping;visual odometry;real time rgb d image registration;on line loop closure detection;3d multi resolution rgb d image representation;real time simultaneous localization and mapping;real time pose tracking;dense object modeling	Building consistent models of objects and scenes from moving sensors is an important prerequisite for many recognition, manipulation, and navigation tasks. Our approach integrates color and depth measurements seamlessly in a multi-resolution map representation. We process image sequences from RGB-D cameras and consider their typical noise properties. In order to align the images, we register view-based maps efficiently on a CPU using multiresolution strategies. For simultaneous localization and mapping (SLAM), we determine the motion of the camera by registering maps of key views and optimize the trajectory in a probabilistic framework. We create object models and map indoor scenes using our SLAM approach which includes randomized loop closing to avoid drift. Camera motion relative to the acquired models is then tracked in real-time based on our registration method. We benchmark our method on publicly available RGB-D datasets, demonstrate accuracy, efficiency, and robustness of our method, and compare it with state-of-theart approaches. We also report on several successful public demonstrations where it was used in mobile manipulation tasks.	3d modeling;align (company);benchmark (computing);central processing unit;closing (morphology);color;graphics processing unit;image registration;map;mathematical optimization;mobile manipulator;multiresolution analysis;object detection;randomized algorithm;real-time clock;real-time locating system;robustness (computer science);run time (program lifecycle phase);sensor;simultaneous localization and mapping;surfel;system image;trajectory optimization	Jörg Stückler;Sven Behnke	2014	J. Visual Communication and Image Representation	10.1016/j.jvcir.2013.02.008	computer vision;simulation;computer science;visual odometry;computer graphics (images)	Robotics	52.47690895399888	-45.8115342679286	126409
aec05359e31ed40a7929c634256ea6c3678ed135	fast autofocus of microscopy images based on depth-from-defocus	focusing;look up table;optics;depth from focus;function relation microscopy images microscopic computer vision autofocusing techniques automated micromanipulation depth from focus based autofocusing method look up table;real time control;microscopy;autofocusing techniques;computer vision;microscopy table lookup optical microscopy equations distance measurement focusing optics;difference scheme;distance measurement;automated micromanipulation;microscopic computer vision;microscopy images;table lookup computer vision;depth from focus based autofocusing method;table lookup;optical microscopy;function relation;depth from defocus	Microscopic computer vision differs significantly from macroscale computer vision. Autofocusing techniques is of fundamental importance to automated micromanipulation in providing high level task understanding, task planning and real time control. Depth-from-Focus (DFF) based autofocusing method is widely used in many microscope systems, while the low efficiency limited its application in micromanipulation system which needs fast autofocus for real time control. In this paper, depth-from-defocus (DFD) algorithms are proposed to improve autofocusing performance and robustness for microscopic optics. Two different schemes based on look-up table and function relation are investigated and evaluated through actual experiments. Experimental results validate the performances of the two proposed autofocusing methods.	algorithm;computer vision;data flow diagram;digital forensics framework (dff);experiment;high-level programming language;lookup table;performance	Liguo Chen;Zhiliang Yang;Lining Sun	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4650658	computer vision;real-time control system;lookup table;computer science;microscopy;optical microscope;optics;computer graphics (images)	Robotics	52.28408685294665	-41.1210389790514	126446
5ce37821eb18e182311b6a7987212237e253c527	are we done with object recognition? the icub robot's perspective		Abstract We report on an extensive study of the benefits and limitations of current deep learning approaches to object recognition in robot vision scenarios, introducing a novel dataset used for our investigation. To avoid the biases in currently available datasets, we consider a natural human–robot interaction setting to design a data-acquisition protocol for visual object recognition on the iCub humanoid robot. Analyzing the performance of off-the-shelf models trained off-line on large-scale image retrieval datasets, we show the necessity for knowledge transfer. We evaluate different ways in which this last step can be done, and identify the major bottlenecks affecting robotic scenarios. By studying both object categorization and identification problems, we highlight key differences between object recognition in robotics applications and in image retrieval tasks, for which the considered deep learning approaches have been originally designed. In a nutshell, our results confirm the remarkable improvements yield by deep learning in this setting, while pointing to specific open challenges that need be addressed for seamless deployment in robotics.	categorization;deep learning;humanoid robot;human–robot interaction;icub;image retrieval;online and offline;outline of object recognition;robotics;seamless3d;software deployment	Giulia Pasquale;Carlo Ciliberto;Francesca Odone;Lorenzo Rosasco;Lorenzo Natale	2017	CoRR	10.1016/j.robot.2018.11.001	humanoid robot;computer vision;computer science;categorization;robot;deep learning;icub;image retrieval;robotics;artificial intelligence;knowledge transfer	Robotics	45.63675236017445	-38.92807826950913	126585
53d5754e18ff6d2d27d6b4caa884b9885747c52f	online re-calibration for robust 3d measurement using single camera- pantoinspect train monitoring system		Vision-based inspection systems measures defects accurately with the help of a checkerboard calibration CBC method. However, the 3D measurements of such systems are prone to errors, caused by physical misalignment of the object-of-interest and noisy image data. The PantoInspect Train Monitoring System PTMS, is one such system that inspects defects on pantographs mounted on top of the electric trains. In PTMS, the measurement errors can compromise railway safety. Although this problem can be solved by re-calibrating the cameras, the process involves manual intervention leading to large servicing times.#R##N##R##N#Therefore, in this paper, we propose Feature Based Calibration FBC in place of CBC, to cater an obvious need for online re-calibration that enhances the usability of the system. FBC involves feature extraction, pose estimation, back-projection of defect points and estimation of 3D measurements. We explore four state-of-the-art pose estimation algorithms in FBC using very few feature points.#R##N##R##N#This paper evaluates and discusses the performance of FBC and its robustness against practical problems, in comparison to CBC. As a result, we identify the best FBC algorithm type and operational scheme for PTMS. In conclusion, we show that, by adopting FBC in PTMS and other related 3D systems, better performance and robustness can be achieved compared to CBC.		Deepak Dwarakanath;Carsten Griwodz;Pål Halvorsen;Jacob Lildballe	2015		10.1007/978-3-319-20904-3_45	embedded system;computer vision;simulation;artificial intelligence	Robotics	48.18200093028491	-44.09508692296519	126900
04d8e03564ff5be33931312e404b164541b93391	benchmark datasets for pose estimation and tracking	mpi fur intelligente systeme;abt black	This chapter discusses the needs for standard datasets in the articulated pose estimation and tracking communities. It describes the datasets that are currently available and the performance of state-of-the-art methods on them. We discuss issues of ground-truth collection and quality, complexity of appearance and poses, evaluation metrics and partitioning of data. We also discusses limitations of current datasets and possible directions in developing new datasets for future use.	3d pose estimation;benchmark (computing)	Mykhaylo Andriluka;Leonid Sigal;Michael J. Black	2011		10.1007/978-0-85729-997-0_13	simulation;geography;artificial intelligence;cartography	Vision	52.37830656886563	-43.51606980405238	127274
2b9850267ced7b3c156d9f9b4ee70050e1f50893	real-time object recognition: hierarchical image matching in a parallel virtual machine environment	object recognition;feature detection;image matching;interest points;object recognition image matching virtual machining image edge detection feature extraction detectors computer vision image converters information technology microelectronics;computer vision;virtual machines;feature extraction;object recognition computer vision image matching feature extraction parallel machines virtual machines real time systems;hausdorff distance;parallel machines;feature points object recognition real time systems hierarchical image matching parallel virtual machine parallel feature extraction hausdorff distance;parallel virtual machine;high performance;real time systems	This paper describes an approach to high performance image matching, which includes parallel feature detection and hierarchical image matching. To improve the performance of the traditional image matching algorithms, we adopted interesting points as feature points to reduce the redundant edge points and proposed a parallel guided image matching scheme by using Hausdorff distance. A series of experiments have been conducted and the results indicate the effectiveness of the proposed approach in terms of speedup and matching accuracy.	algorithm;distributed computing;distributed shared memory;experiment;feature detection (computer vision);feature detection (web development);hausdorff dimension;image registration;outline of object recognition;parallel virtual machine;real-time clock;real-time transcription;speedup	Jane You;Prabir Bhattacharya;Suresh Hungenahally	1998		10.1109/ICPR.1998.711134	hausdorff distance;computer vision;feature detection;scale space;template matching;edge detection;feature extraction;computer science;virtual machine;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;feature detection;feature	Vision	41.503238220009976	-51.35109084046175	127467
38b524a22ce160f39917b62d9b009652c8ad7326	effective object tracking framework using weight adjustment of particle swarm optimization		This paper proposes an effective object tracking framework to compensate the lack of temporal information in the existing particle swarm optimization based object trackers. The object tracker in this paper considers the trajectory of the target object. Based on the trajectories information and the distraction set, a rule based approach with adaptive parameters is utilized for occlusion detection and determination of the target position. Compare to existing frameworks, the proposed approach provides more comprehensive use of available information and does not require manual adjustment of threshold values. Moreover, an effective weight adjustment function is proposed to alleviate the diversity loss and pre-mature convergence problem in particle swarm optimization. The proposed weight function ensures particles to search thoroughly in the frame before convergence to an optimum solution. In the existence of multiple objects with similar feature composition, this framework is tested to significantly reduce convergence to nearby distractions compared to the other existing swarm intelligence based object trackers.	algorithm;mathematical optimization;particle swarm optimization;swarm intelligence;weight function	Changseok Bae;Henry Wing Fung Teung;Yuk Ying Chung	2018	2018 International Conference on Information Networking (ICOIN)	10.1109/ICOIN.2018.8343236	rule-based system;mathematical optimization;swarm intelligence;video tracking;distributed computing;weight function;bittorrent tracker;trajectory;computer science;convergence (routing);particle swarm optimization	Robotics	43.63486077465035	-48.90277328557934	127616
7fa3d4be12e692a47b991c0b3d3eba3a31de4d05	efficient online spatio-temporal filtering for video event detection		We propose a novel spatio-temporal filtering technique to improve the per-pixel prediction map, by leveraging the spatio-temporal smoothness of the video signal. Different from previous techniques that perform spatio-temporal filtering in an offline/batch mode, e.g., through graphical model, our filtering can be implemented online and in real-time, with provable lowest computational complexity. Moreover, it is compatible to any image analysis module that can produce per-pixel map of detection scores or multi-class prediction distributions. For each pixel, our filtering finds the optimal spatio-temporal trajectory in the past frames that has the maximum accumulated detection score. Pixels with small accumulated detection score will be treated as false alarm thus suppressed. To demonstrate the effectiveness of our online spatio-temporal filtering, we perform three video event tasks: salient action discovery, walking pedestrian detection, and sports event detection, all in an online/causal way. The experimental results on the three datasets demonstrate the excellent performances of our filtering scheme when compared with the state-of-the-art methods.	algorithm;batch processing;causal filter;computational complexity theory;graphical model;image analysis;object detection;online and offline;pedestrian detection;performance;pixel;provable security;real-time clock;real-time locating system;streaming media;time complexity;video content analysis	Xinchen Yan;Junsong Yuan;Hui Liang	2014		10.1007/978-3-319-16178-5_54	artificial intelligence;machine learning;batch processing;false alarm;filter (signal processing);computer science;smoothness;computational complexity theory;pixel;pedestrian detection;graphical model	Vision	41.76822182530119	-48.44913366521195	127720
ecc8fe6d63d319d043f565485d4167ba9f2f6029	contour tracking in echocardiographic sequences without learning stage: application to the 3d reconstruction of the beating left ventricule	image sequence;3d reconstruction	In this paper we present a contour tracker on echographic image sequences. To do this, we use a hierarchical approach: we first compute a global estimation of the ventricular motion. Then we use a fine tuning algorithm to adjust the detection of the ventricular wall. The global estimation is based on a parametric motion model with a small number of parameters. This allows us to compute the motion in a robust way from the velocity computed at each point of the contour. Results are presented demonstrating tracking on various echographic sequences. We conclude by discussing some of our current research efforts.	3d reconstruction;algorithm;bittorrent tracker;contour line;robustness (computer science);velocity (software development)	Marie-Odile Berger;Goetz Winterfeldt;Jean-Paul Lethor	1999		10.1007/10704282_55	3d reconstruction;computer vision;computer science	Vision	51.952963544971986	-48.02108587422088	127884
2a725b002dfacc566a83c8096aa28e0af0eca8b1	towards macro- and micro-expression spotting in video using strain patterns	automatic facial expression segmentation;in plane plane motion;micro expressions detection;image motion analysis;image segmentation;video signal processing;micro expressions detection micro expression spotting macro expression spotting temporal segmentation strain patterns automatic spotting optical flow field automatic facial expression segmentation facial strain in plane plane motion out of plane motion;facial strain;micro expression spotting;automatic spotting;computer vision;optical imaging;temporal segmentation;out of plane motion;video signal processing image segmentation object detection;face;optical flow;macro expression spotting;capacitive sensors biomedical optical imaging face recognition nonlinear optics robustness image motion analysis face detection strain measurement facial animation video sequences;facial expression;optical flow field;strain;object detection;optical buffering;strain patterns	This paper presents a novel method for automatic spotting (temporal segmentation) of facial expressions in long videos comprising of continuous and changing expressions. The method utilizes the strain impacted on the facial skin due to the non-rigid motion caused during expressions. The strain magnitude is calculated using the central difference method over the robust and dense optical flow field of each subjects face. Testing has been done on 2 datasets (which includes 100 macro-expressions) and promising results have been obtained. The method is robust to several common drawbacks found in automatic facial expression segmentation including moderate in-plane and out-of-plane motion. Additionally, the method has also been modified to work with videos containing micro-expressions. Micro-expressions are detected utilizing their smaller spatial and temporal extent. A subject's face is divided in to sub-regions (mouth, cheeks, forehead, and eyes) and facial strain is calculated for each of these regions. Strain patterns in individual regions are used to identify subtle changes which facilitate the detection of micro-expressions.	finite difference;optical flow	Matthew Shreve;Sridhar Godavarthy;Vasant Manohar;Dmitry B. Goldgof;Sudeep Sarkar	2009	2009 Workshop on Applications of Computer Vision (WACV)	10.1109/WACV.2009.5403044	face;computer vision;computer science;optical imaging;optical flow;strain;image segmentation;facial expression;computer graphics (images)	Vision	43.155924356581544	-50.74202746588295	127908
5a1dd1a0ea96a4b2b1ec7e6890f4eb9d4f574d8d	lipacts: efficient representations for visual speakers	verification;verification learning systems feature extraction video analysis;activity analysis;video signal processing;video based lip activity analysis;video analysis;hog;histograms of oriented gradients;temporal sampling;youtube content;face vocabulary feature extraction quantization detectors visualization histograms;eer lipacts video based lip activity analysis speech recognition visual speaker retrieval tracking complexity histogram of oriented gradients hog quantization techniques temporal sampling spatial partitioning youtube content mobile video camera equal error rate;mobile video camera;learning systems;learning system;histogram of oriented gradients;computational complexity;feature extraction;equal error rate;quantization techniques;visual speaker retrieval;eer;speech recognition;lipacts;near real time;video signal processing computational complexity speech recognition;multi resolution;mobile video;tracking complexity;spatial partitioning	Video-based lip activity analysis has been successfully used for assisting speech recognition for almost a decade. Surprisingly, this same capability has not been heavily used for near real-time visual speaker retrieval and verification, due to tracking complexity, inadequate or difficult feature determination, and the need for a large amount of pre-labeled data for model training. This paper explores the performance of several solutions using modern histogram of oriented gradients (HOG) features, several quantization techniques, and analyzes the benefits of temporal sampling and spatial partitioning to derive a representation called LipActs. Two datasets are used for evaluation: one with 81 participants derived from varying quality YouTube content and one with 3 participants derived from a forward-facing mobile video camera with 10 varied lighting and capture angle environments. Over these datasets, LipActs with a moderate number of pooled temporal frames and multi-resolution spatial quantization, offer an improvement of 37–73% over raw features when optimizing for lowest equal error rate (EER).	angular momentum operator;enhanced entity–relationship model;gradient;histogram of oriented gradients;real-time clock;real-time computing;sampling (signal processing);space partitioning;speech recognition	Eric Zavesky	2011	2011 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2011.6012102	computer vision;verification;speech recognition;feature extraction;histogram of oriented gradients;computer science;space partitioning;machine learning;multimedia;computational complexity theory;algorithm	Vision	40.86525100424686	-51.48919183910544	128041
7ee868f9789193f19885391ccf21885344f52024	a traffic-congestion detection method for bad weather based on traffic video		In order to solve the problem that the result of traffic congestion detection in bad weather is inaccurate, we analyzed current vehicle identification algorithms and image processing algorithms. After that, we proposed a detection method of traffic congestion based on histogram equalization and discrete-frame difference. Firstly, this method uses discrete-frame difference algorithm to extract the images that have vehicle information. Secondly, this method uses the histogram equalization algorithm to eliminate the noise of the images. Finally, this method recognizes the vehicle from the video and computes the traffic congestion index by the calculation method based on discrete-frame difference. It has proved by experiments and theoretical analysis that this method decreases false-negative rate and increases the accuracy rate of automatic traffic congestion detection in bad weather.	network congestion	Jieren Cheng;Boyi Liu;Xiangyan Tang	2015		10.1007/978-981-10-0356-1_54	internet privacy;computer security;remote sensing	Vision	41.77014325168431	-43.39097533529269	128251
d254f168aa679ff7bd3fb2e4e1e61d65ab204f03	physics-based ball tracking and 3d trajectory reconstruction with applications to shooting location estimation in basketball video	sports video analysis;location estimation;video analysis;multimedia systems;domain knowledge;physical characteristic;statistical analysis;shot classification;object tracking;camera calibration;3d trajectory reconstruction;article;semantic analysis;multimedia system	The demand for computer-assisted game study in sports is growing dramatically. This paper presents a practical video analysis system to facilitate semantic content understanding. A physics-based algorithm is designed for ball tracking and 3D trajectory reconstruction in basketball videos and shooting location statistics can be obtained. The 2D-to-3D inference is intrinsically a challenging problem due to the loss of 3D information in projection to 2D frames. One significant contribution of the proposed system lies in the integrated scheme incorporating domain knowledge and physical characteristics of ball motion into object tracking to overcome the problem of 2D-to-3D inference. With the 2D trajectory extracted and the camera parameters calibrated, physical characteristics of ball motion are involved to reconstruct the 3D trajectories and estimate the shooting locations. Our experiments on broadcast basketball videos show promising results. We believe the proposed system will greatly assist intelligence collection and statistics analysis in basketball games. 2008 Elsevier Inc. All rights reserved.	algorithm;artificial intelligence;computation;computer;emoticon;experiment;parabolic antenna;tracking system;video content analysis	Hua-Tsung Chen;Ming-Chun Tien;Yi-Wen Chen;Wen-Jiin Tsai;Suh-Yin Lee	2009	J. Visual Communication and Image Representation	10.1016/j.jvcir.2008.11.008	computer vision;camera resectioning;simulation;computer science;video tracking;multimedia;domain knowledge	Vision	50.325606301977544	-45.673953334040135	128287
6bfe5f0ee40f16b651a6f569f43170b380c68d0d	planar object tracking in the wild: a benchmark		Planar object tracking is an actively studied problem in vision-based robotic applications. While several benchmarks have been constructed for evaluating state-of-the-art algorithms, there is a lack of video sequences captured in the wild rather than in constrained laboratory environment. In this paper, we present a carefully designed planar object tracking benchmark containing 210 videos of 30 planar objects sampled in the natural environment. In particular, for each object, we shoot seven videos involving various challenging factors, namely scale change, rotation, perspective distortion, motion blur, occlusion, out-of-view, and unconstrained. The ground truth is carefully annotated semi-manually to ensure the quality. Moreover, eleven state-of-the-art algorithms are evaluated on the benchmark using two evaluation metrics, with detailed analysis provided for the evaluation results. We expect the proposed benchmark to benefit future studies on planar object tracking.	algorithm;benchmark (computing);distortion;futures studies;gaussian blur;ground truth;planar (computer graphics);robot;semiconductor industry	Pengpeng Liang;Yifan Wu;Hu Lu;Liming Wang;Chunyuan Liao;Haibin Ling	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8461037	motion blur;machine learning;perspective distortion;artificial intelligence;video tracking;computer science;ground truth;benchmark (computing);planar	Robotics	53.02551046328757	-45.093982052157074	128310
129d2bee9a1942e605116e4d83ec93db67c492dd	a versatile low cost arterial simulator	computers;coagulation;mechanical thrombectomy device;arteries;training;surgical procedures arterial simulator surgeon training;hospitals;surgical procedures;costs arteries computational modeling engines catheters computer simulation coagulation surgery humans hospitals;medical computing;surgery medical computing;blood clot removal;computational modeling;engines;solid modeling;catheters;surgery;humans;mechanical thrombectomy device arterial simulator blood clot removal;surgeon training;surgical procedure;arterial simulator;computer simulation;numerical simulation	Numerous simulators have been developed for the training of surgeons concerning many surgical procedures. In many cases, the simulators are both demanding of space and are costly. We have designed a low cost versatile simulator that can be used almost anywhere on most computer systems.	simulation	M. Rai;Gillian Pearce;Neal D. Perkinson;Paul Brookfield;John Asquith;Changez Jadun;Julian Wong;Matthew Burley	2009	2009 11th International Conference on Computer Modelling and Simulation	10.1109/UKSIM.2009.71	simulation;medicine;biological engineering;surgery	Vision	39.624905977900646	-38.122616231055616	128417
60bdad8bfab8133ad13d113362b3c0ec395cfd5f	eigen-points: control-point location using principle component analyses	shape estimation;point estimation;training data eigen points control point location principal component analyses fiduciary points multiple locations affine manifold model recovering shape image appearance estimation equations;point location;eigen analysis;estimating equation;image reconstruction;automatic control noise shaping shape measurement gray scale milling machines training data uncertainty animation image databases deformable models;image analysis;feature location;control point location	Eigen-points estimates the image-plane locations of fiduciary points on an objects. By estimating multiple locations simultaneously, eigen-points exploits the interdependence between these locations. This is done by associating neighboring, inter-dependent control-points with a model of the local appearance. The model of local appearance is used to find the feature in new unlabeled images. Control-point locations are then estimated from the appearance of this feature in the unlabeled image. The estimation is done using an affine manifold model of the coupling between the local appearance and the local shape. Eigen-points uses models aimed specifically at recovering shape from image appearance. The estimation equations are solved non-iteratively, in a way that accounts for noise in the training data and the unlabeled images and that accounts for uncertainty in the distribution and dependencies within these noise sources. original original original automatic morph automatic morph Figure 1: Examples of image morphs using automatically placed correspondences The control-point locations for these morphs were estimated automatically by eigen-points. Constraints were placed around eyes, nose, mouth, chin and ears. No constraints were placed on the hair. Interval Research Corporation Technical Report # 1996-060 Copyright 1996 IEEE. Published in the Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognitio n, O t 14-16, 1996. Killington, VT. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotion al purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtain ed fr m the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-5623966. them to take advantage of example-based learning to constrain the estimated locations of these control points. However, there is no direct link between the image appearance (the external-energy term) and the shape constraints (the internal-energy term). This makes the discovery of “correct” energy functional an error-prone process. Shape-plus-texture models [8][9] describe appearance using two separate reconstructive models: one for shape (e.g. contour locations) and one for shape-free texture. The shape-free texture descriptions model the grayscale values under object-centric sampling. Thus, the texture models do not describe the observed grayscale data, but instead describe the grayscale data resampled according to the estimated shape description. These shape-plus-texture approaches give simultaneous estimates for many controlpoint locations. They have well-defined example-based training methods and an error criteria derived from training. However, the texture models use an estimate of shape. Thus, they are forced to rely on iterative solutions to find consistent shape and texture estimates. Another drawback to shape-plus-texture approaches is their use of reconstructive as opposed to discriminative models. The texture model capture the principal variations of the (shape-normalized) appearance, giving the minimum mean-square error reconstruction for a given description length. However, our goal is to find a good estimate for the true shape, not to find a good estimate for the true appearance (shape-normalized or otherwise). Instead of a reconstructive texture model, we need a “shape-discriminant” model of texture. That is, we need the model that best captures the principal variations of shape, as manifested in appearance. The next section describes our approach to discriminating between shapes based on the observed image data. 3 Eigen-point approach to placing control points Using eigen-points, the problem of locating fiduciary points on an unmarked image is solved in two stages. First, the location of features * are estimated; then control points are placed around that feature. The first stage locates the feature of interest—for example, the actors’ lips. This can be done using templateor model-based matching. The feature location defines both the subimage and the image-plane origin that are used in the second stage. The second stage places the control points around the feature—for example, marking the locations in the image that show the outer boundary of the lips. The locations of the fiduciary points are estimated using an affine manifold model that couples the grayscale values within the feature to the control-point locations associated with the feature. This approach effectively assumes that there is a single K dimensional vector, , which drives both the feature grayscale vector and the control-point locations. The functions which transform this vector into appearance and shape are assumed to be affine. Assuming a coupled, affine model for image-plane shape and appearance, the defining equations for the grayscale values and the control-point locations are:	automatic differentiation;cognitive dimensions of notations;contour line;control point (mathematics);discriminant;discriminative model;eigen (c++ library);estimation theory;gesture recognition;grayscale;higher-order programming;hoare logic;inline linking;interdependence;item unique identification;iterative method;manifold regularization;motorola 68060;point location;proceedings of the ieee;sampling (signal processing);shape context	Michele Covell	1996		10.1109/AFGR.1996.557253	computer vision;mathematical optimization;active appearance model;pattern recognition;mathematics	Vision	48.13610644624763	-50.949220853868674	128558
3006e665aa6f725bc20baf1957e716b816c093a7	fast and reliable active appearance model search for 3-d face tracking	computer vision and robotics autonomous systems;real time tracking three dimensional tracking human face animation monocular image sequences active appearance model search algorithm 3d face tracking analysis by synthesis approaches facial feature tracking human computer interaction;optimisation;facial feature tracking;human computer interaction;real time tracking;algorithms artificial intelligence biometry computer simulation face facial expression humans image interpretation computer assisted imaging three dimensional models biological movement pattern recognition automated reproducibility of results sensitivity and specificity subtraction technique;model analysis;real time;datorseende och robotik autonoma system;search algorithm;computational geometry;active appearance model;face tracking;face recognition;computational complexity tracking human computer interaction image sequences computer animation face recognition solid modelling optimisation computational geometry;active appearance model face detection facial animation deformable models biological system modeling active shape model humans image sequences facial features tracking;computational complexity;image sequence;computer animation;tracking;solid modelling;image sequences	This paper addresses the three-dimensional (3D) tracking of pose and animation of the human face in monocular image sequences using active appearance models. The major problem of the classical appearance-based adaptation is the high computational time resulting from the inclusion of a synthesis step in the iterative optimization. Whenever the dimension of the face space is large, a real-time performance cannot be achieved. In this paper, we aim at designing a fast and stable active appearance model search for 3D face tracking. The main contribution is a search algorithm whose CPU-time is not dependent on the dimension of the face space. Using this algorithm, we show that both the CPU-time and the likelihood of a nonaccurate tracking are reduced. Experiments evaluating the effectiveness of the proposed algorithm are reported, as well as method comparison and tracking synthetic and real image sequences.	3d computer graphics;acclimatization;active appearance model;addresses (publication format);animation;cpu (central processing unit of computer system);central processing unit;computation;face space;iterative method;mathematical optimization;real-time clock;search algorithm;synthetic intelligence;time complexity	Fadi Dornaika;Jörgen Ahlberg	2004	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2004.829135	computer vision;facial motion capture;active appearance model;simulation;computational geometry;computer science;computer animation;tracking;computational complexity theory;search algorithm;computer graphics (images)	Vision	47.67695212434194	-49.08587815745613	128667
fc9d1b1f39ca4aff39c97266d66244d836959c79	development of a gross motor skill by combining primitive motions in repetitive exercise	instruments;image processing;motion pictures;motion segmentation;correlation;cameras;conferences	This study proposes a new system for learners to master double-under skill. In terms of proposing methodology, it assumes four training phases. First and second phases deal with the primitive motion training but the latter two phases do their combination. This paper focuses mainly on the third one that treats combination of primitive dyad of the former phases without a rope instrument. The prototype is equipped with analyzed results of the monitored video. Then, this paper describes the designing issue and its implementation.	prototype	Kohta Sugawara;Hiroshi Toyooka;Kenji Matsuura;Stephen Karungaru;Naka Gotoda	2016	2016 IEEE 5th Global Conference on Consumer Electronics	10.1109/GCCE.2016.7800489	computer vision;simulation;computer science;computer graphics (images)	Robotics	48.101547110541155	-44.75689201206735	128957
3a7a4f62408f147422a59f028d54f855158a42f9	video summarization using geometric primitives	surveillance;displacement measurement;shape;image edge detection;feature extraction;cameras;object detection	Video summarization is the process to extract informative events of a video and represent in the condensed form. The paper proposes a new method for extracting important contents of a video for summarization using geometric primitives, such as line segments, angles, and conic parts. The primitives have the capabilities to represent complex shapes and structures of objects in a video frame. Therefore, they are indeed powerful features to localize objects in complex environments. After localizing objects, a cost function is applied to measure the dissimilarity of locations of geometric primitives to detect the movement of objects between consecutive frames. After detecting the dissimilar geometric primitives, amount of object movements are calculated based on the number and length of each geometric primitives. Utilizing this information, each video frame is assigned a probability score to become a key frame. Finally, a set of key frames are selected as per user preference. The proposed approach is evaluated using BL-7F, Office, and Lobby datasets and obtained better performance than the intra-view results of the recently proposed state of the art GMM based method.	automatic summarization;digital video;geometric primitive;google map maker;information;key frame;loss function;object detection;sensor	Md. Musfequs Salehin;Manoranjan Paul	2016	2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2016.7797094	computer vision;feature extraction;shape;computer science;video tracking;pattern recognition;mathematics;geometry	Robotics	42.08506951690278	-50.806656832920126	129304
af1fca5532f72d416cd7cb3d57cb5e83e1133672	spacetime stereo: a unifying framework for depth from triangulation	moving object;laser;exploracion;structured light;dynamic scenes spacetime stereo depth from triangulation algorithm depth estimation;mesh generation stereo image processing;triangulacion;spacetime stereo;stereo image processing;stereo temporospatiale;balayage;depth from triangulation;stereo;index terms depth from triangulation;laser scanning;triangulation;triangulation a partir profondeur;spacetime stereo index terms depth from triangulation stereo;depth estimation;scanning;layout lighting computer society performance analysis classification algorithms pixel power measurement geometry time domain analysis;mesh generation;dynamic scenes;algorithms artificial intelligence computer simulation image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval pattern recognition automated photogrammetry reproducibility of results sensitivity and specificity subtraction technique video recording	Depth from triangulation has traditionally been investigated in a number of independent threads of research, with methods such as stereo, laser scanning, and coded structured light considered separately. We propose a common framework called spacetime stereo that unifies and generalizes many of these previous methods. To show the practical utility of the framework, we develop two new algorithms for depth estimation: depth from unstructured illumination change and depth estimation in dynamic scenes. Based on our analysis, we show that methods derived from the spacetime stereo framework can be used to recover depth in situations in which existing methods perform poorly.		James Davis;Diego F. Nehab;Ravi Ramamoorthi;Szymon Rusinkiewicz	2005	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/TPAMI.2005.37	laser scanning;mesh generation;computer vision;structured light;laser;triangulation;computer science;mathematics;stereophonic sound;computer graphics (images)	Vision	53.1948191642893	-52.00044693656592	129316
f4680cd4a87cec56e65f85d46b94827985b52759	improving motion state change object detection by using block background context	learning algorithms;object recognition;background modeling;video surveillance gaussian processes object detection;conference;semantics;gaussian mixture background model motion state change object detection video surveillance systems block background context based background model learning rate context information foreground detection;motion state change block background context background modeling;context context modeling adaptation models mathematical model vehicles computational modeling roads;artificial intelligence;motion state change;block background context	Motion state change object detection, such as stopped objects detection, is one of important topics in Video Surveillance Systems. Generally, backgrounds in the most Video Surveillance Systems have the property of pureness and self-similarity. In this paper, we propose a block background context based background model to solve the motion state change problem. Unlike the classical background model, our approach first models blocks of background, and then determines the learning rate of each block background model by using the block background context information. There are two main advantages. First, the model adaptively selects the learning rate for each block of background model, and that is more flexible than the adaptive learning rate for the whole background. Second, context information helps the determination of true foreground and brings in more reliable information in foreground detection. Our experiments results show that our model outperforms the higher and lower learning rate Gaussian mixture background model in motion state change object detection.	experiment;object detection;self-similarity	Dazhen Lin;Donglin Cao;Hualin Zeng	2014	2014 14th UK Workshop on Computational Intelligence (UKCI)	10.1109/UKCI.2014.6930187	computer vision;background subtraction;computer science;machine learning;pattern recognition	AI	43.532886032451216	-49.17576904935351	129484
4fe1595cc1087aa6945419279f3da939e992afb4	iterative multi-planar camera calibration: improving stability usingmodel selection.	model selection;real time;camera calibration;augmented reality	Tracking, or camera pose determination, is the main technical challenge in numerous applications in computer vision and especially in Augmented Reality. However, pose computation processes commonly exhibit some fluctuations and lack of precision in the estimation of the parameters. This leads to unpleasant visual impressions when augmented scenes are considered. In this paper, we propose an efficient and reliable method for real time camera tracking which avoid unpleasant statistical fluctuations. This method is based on the knowledge of a piecewise planar structure in the scene and makes use of model selection to reduce fluctuations. Videos are attached to this paper which proved the effectiveness of our approach (hallTrack.mpg, hallCamera.mpg, hallAugmented.mpg, roomWithoutMS.mpg and roomWithMS.mpg).	augmented reality;camera resectioning;computation;computer vision;eisenstein's criterion;experiment;focal (programming language);genetic selection;match moving;mathematical model;model selection;numerous;published comment;used quit cigarette smoking videos;impression (attitude);videocassette	Javier-Flavio Vigueras;Marie-Odile Berger;Gilles Simon	2003			computer vision;camera auto-calibration;simulation;computer graphics (images)	Vision	53.43969773582134	-46.53120358341557	129524
f20223ac68fb7925c5292a3c84b47c798a424cfc	visual grasp point localization, classification and state recognition in robotic manipulation of cloth: an overview	garment classification;deformable object manipulation;cloth state recognition;grasp point localization;robotic vision;clothing	Cloth manipulation by robots is gaining popularity among researchers because of its relevance, mainly (but not only) in domestic and assistive robotics. The required science and technologies begin to be ripe for the challenges posed by the manipulation of soft materials, and many contributions have appeared in the last years. This survey provides a systematic review of existing techniques for the basic perceptual tasks of grasp point localization, state estimation and classification of cloth items, from the perspective of their manipulation by robots. This choice is grounded on the fact that any manipulative action requires to instruct the robot where to grasp, and most garment handling activities depend on the correct recognition of the type to which the particular cloth item belongs and its state. The high interand intraclass variability of garments, the continuous nature of the possible deformations of cloth and the evident difficulties in predicting their localization and extension on the garment piece are challenges that have encouraged the researchers to provide a plethora of methods to confront such problems, with some promising results. The present review constitutes for the first time an effort in furnishing a structured framework of these works, with the aim of helping future contributors to gain both insight and perspective on the subject.	internationalization and localization;relevance;robot;robotics;spatial variability;statistical classification;systematic review	P. Jiménez	2017	Robotics and Autonomous Systems	10.1016/j.robot.2017.03.009	computer vision;simulation;artificial intelligence;clothing	Robotics	43.790508310952006	-39.548666257015185	129538
877a1aa2b0ba54df5171991ac0b7abcc176064b0	3d object pose estimation for robotic packing applications		Given the growth of internet-based trading on a global level, there are several expected logistic challenges regarding the optimal transportation of large volumes of merchandise. With this in mind, the application of technologies such as computer vision and industrial robotics in facing these challenges presents significant advantages regarding the speed and reliability with which palletization tasks, a critical point in the merchandise transportation chain, can be performed. This paper presents a computer vision strategy for the localization and recognition of boxes in the context of a palletization process carried out by a robotic manipulator. The system operates using a Kinect 2.0 depth camera to capture a scene and processing the resulting point cloud. Obtained results permit the simultaneous recognition of up to 15 boxes, their position in space and their size characteristics within the workspace of the robot, with an average error of approximately 3 cm.		C. H. Rodríguez-Garavito;Guillermo A. Camacho-Munoz;David Álvarez-Martínez;Karol Viviana Cardenas;David M. Rojas;Andrés Grimaldos	2018		10.1007/978-3-030-00353-1_40	convex hull;point cloud;workspace;robot;pose;computer vision;mean-shift;artificial intelligence;robotics;computer science;bin packing problem	Robotics	50.40095922280005	-41.20152474206167	129645
cf0f1eb77e7eaa9698c563a2dcc728e1ab2e465d	camera calibration from orthogonally projected coordinates with noisy-ransac	cameras calibration transmission line matrix methods equations robustness layout global positioning system parameter estimation condition monitoring object detection;focal length camera calibration plane homography homography matrix orthogonal calibration noisy ransac method orthogonally projected coordinates camera parameters random sample consensus;fundamental matrix;matrix algebra;noise measurement;feature extraction;orthogonal projection;aerial photograph;robustness;transmission line matrix methods;parameter estimation;camera calibration;parameter estimation calibration cameras feature extraction matrix algebra;calibration;cameras;noise	We introduce a formulation for “orthogonal calibration” which is to extract camera calibration parameters from the world coordinates with unknown heights. Such coordinates can be obtained from an aerial photograph or a GPS device. A typical approach would be to introduce a planar surface assumption and apply the plane homography. However, the planar assumption often fails; moreover, the camera parameters recovered from a homography matrix is highly sensitive to noise. We introduce a formulation for the orthogonal calibration which is similar to the fundamental matrix equation. Based on the formulation, we introduce a 6-point algorithm to recover the rotation and translation and a 7-point algorithm to recover the focal lengths in addition. Then, we introduce a noisy-RANSAC method which enables robust parameter recovery from a small number of point correspondences. The noisy-RANSAC naturally incorporates the orthogonal calibration and any available information on the scene structure, which delivers much improved estimates. Experimental results on synthetic datasets and an example application to a real image are presented.	aerial photography;algorithm;camera resectioning;focal (programming language);fundamental matrix (computer vision);global positioning system;glossary of computer graphics;homography (computer vision);noisy-channel coding theorem;random sample consensus;synthetic intelligence	ZuWhan Kim	2009	2009 Workshop on Applications of Computer Vision (WACV)	10.1109/WACV.2009.5403107	homography;computer vision;mathematical optimization;calibration;camera resectioning;feature extraction;computer science;noise measurement;noise;mathematics;fundamental matrix;orthographic projection;estimation theory;aerial photography;robustness	Vision	53.53947739526018	-49.59950417637432	129791
300a6f3bcfcc1771f3f272e38f7d6d8f4c103ce6	calibration-free eye gaze direction detection with gaussian processes	wireless head mounted camera;gaussian processes;young children;wearable camera wearcam;gaussian process regression;eye gaze detection;direct detection;gaussian process;support vector machine;gaze direction;eye gaze;appearance based	In this paper we present a solution for eye gaze detection from a wireless head mounted camera designed for children aged between 6 months and 18 months. Due to the constraints of working with very young children, the system does not seek to be as accurate as other state-of-the-art eye trackers, however it requires no calibration process from the wearer. Gaussian Process Regression and Support Vector Machines are used to analyse the raw pixel data from the video input and return an estimate of the child’s gaze direction. A confidence map is used to determine the accuracy the system can expect for each coordinate on the image. The best accuracy so far obtained by the system is 2.34◦ on adult subjects, tests with children remain to be	eye tracking;gaussian process;kriging;pixel;support vector machine	Basilio Noris;Karim Benmachiche;Aude Billard	2008			computer vision;simulation;eye tracking;computer science;gaussian process;statistics	HCI	39.23518091957134	-43.56437840590323	129864
829bf35bdf4c00ddc02748ba770433ac43f06a03	extraction of vehicle groups in airborne lidar point clouds with two-level point processes	qa75 electronic computers computer science szamitastechnika;szamitogeptudomany;vehicle aerial laser scanning light detection and ranging lidar marked point process mpp urban;t2 technology general műszaki tudomanyok altalaban;vehicles laser radar three dimensional displays roads feature extraction shape vehicle detection	In this paper, we present a new object-based hierarchical model for the joint probabilistic extraction of vehicles and groups of corresponding vehicles-called traffic segments-in airborne light detection and ranging (Lidar) point clouds collected from dense urban areas. First, the 3-D point set is classified into terrain, vehicle, roof, vegetation, and clutter classes. Then, the points with the corresponding class labels and echo strength (i.e., intensity) values are projected to the ground. In the obtained 2-D class and intensity maps, we approximate the top view projections of vehicles by rectangles. Since our tasks are simultaneously the extraction of the rectangle population which describes the position, size, and orientation of the vehicles and grouping the vehicles into the traffic segments, we propose a hierarchical two-level marked point process (MPP) (L2MPP) model for the problem. The output vehicle and traffic segment configurations are extracted by an iterative stochastic optimization algorithm. We have tested the proposed method with real data of a discrete-return Lidar sensor providing up to four range measurements for each laser pulse. Using manually annotated ground-truth information on a data set containing 1009 vehicles, we provide quantitative evaluation results showing that the L2MPP model surpasses two earlier grid-based approaches, a 3-D point-cloud-based process and a single-layer MPP solution. The accuracy of the proposed method measured in F-rate is 97% at object level, 83% at pixel level, and 95% at group level.	airborne ranger;approximation algorithm;cloud computing;clutter;goodyear mpp;ground truth;hierarchical database model;iterative method;map;mathematical optimization;object-based language;pixel;point cloud;point process;stochastic optimization;windows 95	Attila Bo&#x0308;rcs;Csaba Benedek	2015	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2014.2344438	meteorology;computer vision;simulation;optics;remote sensing	Vision	43.7481700243417	-44.373193791695385	130190
9d2040554c891236f4637ee177fc2527dc809142	surface layout estimation using multiple segmentation methods and 3d reasoning		In this paper we present a novel algorithm to estimate the surface layout of an indoor scene, which can serve as a visual cue for many different applications, e.g. 3D tracking, or localization in visual odometry. The main contribution of this work lies in combining multiple superpixel segmentation methods in order to obtain semantically meaningful regions. For each segmentation method, we combine 3D reasoning with semantic reasoning to generate multiple surface layout label hypotheses for each pixel. We then get the final label for each pixel within a Markov Random Field (MRF) by combining all hypothesis and by enforcing spatial consistency between neighboring pixels. Experimental results on complex indoor scenes show that our proposed method outperforms state-of-the-art methods.	algorithm;experiment;image segmentation;markov chain;markov random field;pixel;point cloud;scene graph;visual odometry	Michael Hödlmoser;Branislav Micusík	2013		10.1007/978-3-642-38628-2_5	computer vision;machine learning;pattern recognition;mathematics	Vision	45.15656220871747	-50.42778937087522	130392
f0c35727615b51a183ca7770645909fc986125c8	depth-based detection of standing-pigs in moving noise environments	agriculture it;computer vision;depth information;foreground detection;moving noise	"""In a surveillance camera environment, the detection of standing-pigs in real-time is an important issue towards the final goal of 24-h tracking of individual pigs. In this study, we focus on depth-based detection of standing-pigs with """"moving noises"""", which appear every night in a commercial pig farm, but have not been reported yet. We first apply a spatiotemporal interpolation technique to remove the moving noises occurring in the depth images. Then, we detect the standing-pigs by utilizing the undefined depth values around them. Our experimental results show that this method is effective for detecting standing-pigs at night, in terms of both cost-effectiveness (using a low-cost Kinect depth sensor) and accuracy (i.e., 94.47%), even with severe moving noises occluding up to half of an input depth image. Furthermore, without any time-consuming technique, the proposed method can be executed in real-time."""	background subtraction;closed-circuit television;execution;glossary of computer graphics;ground truth;interpolation imputation technique;kinect;obstruction;preprocessor;range imaging;real-time clock;real-time computing;sensor;subtraction technique;tracking system;undefined behavior	Jinseong Kim;Yeonwoo Chung;Younchang Choi;Heegon Kim;Yongwha Chung;Daihee Park;Hakjae Kim	2017		10.3390/s17122757	interpolation;electronic engineering;engineering;foreground detection;computer vision;artificial intelligence	Robotics	44.867068235324616	-42.48254719349341	130485
33d0b723db4850d4d3131d14e577278dbef5e86a	resolution consideration in spatially variant sensors	high resolution;raytracing;complex log mapping;tracking system;spatial resolution humans space technology machine vision retina sensor systems visual system cmos technology computer science image generation;raytracing spatially variant sensors log polar transformations fovea exponentially reduced resolution feature invariance multiresolution analysis receptor density human retina space variant active vision systems visual sensors;field of view;space variant active vision;multiresolution analysis;tracking;active vision	Log polar transformations for space variant systems have been proposed and used in active vision research. The idea is to generate an image with a varying resolution over a wide angle field of view. The fovea is of high resolution and the periphery is of exponentially reduced resolution. The justifications for such a sensor are: (i) it provides high resolution and a wide viewing angle; (ii) feature invariance in the fovea simplifies foveation; (iii) it allows multi-resolution analysis; and (iv) it is cheaper and more efficient to build a variable resolution sensor over a particular field of view rather than a uniform high resolution sensor. The receptor density of the human retina is very high, i.e. of the order of IO8 receptors at the fovea. The question is, what resolution should space variant active vision systems have? Real visual sensors have been implemented but is the resolution produced high enough? This paper investigates the resolution requirements of a space variant sensor by simulation for a tracking system using raytracing.	active vision;expanded memory;image resolution;multiresolution analysis;ray tracing (graphics);requirement;sensor;simulation;tracking system;viewing angle	Fee-Lee Lim;Svetha Venkatesh;Geoff A. W. West	1996	Image Vision Comput.	10.1109/ICPR.1996.546133	multiresolution analysis;ray tracing;computer vision;image resolution;active vision;field of view;tracking system;computer science;tracking;computer graphics (images)	Robotics	52.07266237563363	-40.49976551211589	130501
c93de913f7a1b1ff1509bcb2e6e33a9b190ff0a0	detecting changes in 3-d shape using self-consistency	3d shape;image matching computer vision;application software;image matching;changes detection;contracts;mobile robots;layout;computer vision;self consistency;shape;monitoring;computer vision algorithms;shape computer vision cameras application software layout mobile robots stereo vision contracts monitoring us government;stereo vision;image sets;image matching measure changes detection 3d shape self consistency single value functions image sets computer vision algorithms;cameras;us government;image matching measure;single value functions	"""1 A method for reliably detecting change in the 3-D shape of objects that are well-modeled as single-value functions z f x y = (,) is presented. It uses an estimate of the accuracy of the 3-D models derived from a set of images taken simultaneously. This accuracy estimate is used to distinguish between significant and insignificant changes in 3-D models derived from different image sets. The accuracy of the 3-D model is estimated using a general methodology, called self-consistency, for estimating the accuracy of computer vision algorithms, which does not require prior establishment of """"ground truth"""". A novel image-matching measure based on Minimum Description Length (MDL) theory allows us to estimate the accuracy of individual elements of the 3-D model. Experiments to demonstrate the utility of the procedure are presented."""	3d modeling;algorithm;computer vision;experiment;ground truth;minimum description length;sensor	Yvan G. Leclerc;Quang-Tuan Luong;Pascal Fua;Koji Miyajima	2000		10.1109/CVPR.2000.855846	layout;novikov self-consistency principle;mobile robot;computer vision;application software;simulation;shape;computer science;stereopsis;machine learning	Vision	50.874795656423	-40.443446298247956	130656
fc1dd3fb6c3f666a661f4d653e970dce8b808a92	multiview trajectory mapping using homography with lens distortion correction	lens distortion;signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	We present a trajectory mapping algorithm for a distributed camera setting that is based on statistical homography estimation accounting for the distortion introduced by camera lenses. Unlike traditional approaches based on the direct linear transformation (DLT) algorithm and singular value decomposition (SVD), the planar homography estimation is derived from renormalization. In addition to this, the algorithm explicitly introduces a correction parameter to account for the nonlinear radial lens distortion, thus improving the accuracy of the transformation. We demonstrate the proposed algorithm by generating mosaics of the observed scenes and by registering the spatial locations of moving objects (trajectories) from multiple cameras on the mosaics. Moreover, we objectively compare the transformed trajectories with those obtained by SVD and least mean square (LMS) methods on standard datasets and demonstrate the advantages of the renormalization and the lens distortion correction.		Gabin Kayumbi;Andrea Cavallaro	2008	EURASIP J. Image and Video Processing	10.1155/2008/145715	distortion;homography;computer vision;computer science;archaeology;pattern recognition;biometrics	Vision	53.32115794831452	-49.93258546138368	130800
797b638468a21665d4acf5414e35aad8428bcb71	robotcub implementation of real-time least-square fitting of ellipses	humanoid robot;least square fitting;humanoid robotics;least squares approximations;edfe;robotcub;real time;machine vision edfe robotcub humanoid robotics pattern recognition;topographic map;scattered data;open source platform robotcub least square fitting ellipses pattern recognition machine vision humanoid robotics;robot vision;cognitive skills;humanoid robots;machine vision;ellipses;least square;pattern recognition;hough transform;cognitive robotics laboratories humanoid robots robot vision systems robustness pattern recognition machine vision scattering humans navigation;robot vision humanoid robots least squares approximations;open source platform;direct method;open source;ellipse fitting	This paper presents the implementation of a new algorithm for pattern recognition in machine vision developed in our laboratory applied to the RobotCub humanoid robotics platform simulator. The algorithm is a robust and direct method for the least-square fitting of ellipses to scattered data. RobotCub is an open source platform, born to study the development of neuro-scientific and cognitive skills in human beings, especially in children. By the estimation of the surrounding objects properties (such as dimensions, distances, etc...) a subject can create a topographic map of the environment, in order to navigate through it without colliding with obstacles. In this work we implemented the method of the least-square fitting of ellipses of Maini (EDFE), previously developed in our laboratory, in a robotics context. Moreover, we compared its performance with the hough transform, and others least-square ellipse fittings techniques. We used our system to detect spherical objects, and we applied it to the simulated RobotCub platform. We performed several tests to prove the robustness of the algorithm within the overall system, and finally we present our results.	algorithm;direct method in the calculus of variations;hough transform;icub;machine vision;open-source software;pattern recognition;real-time clock;robotics;robustness (computer science);topography	Nicola Greggio;Luigi Manfredi;Cecilia Laschi;Paolo Dario;Maria Chiara Carrozza	2008	Humanoids 2008 - 8th IEEE-RAS International Conference on Humanoid Robots	10.1109/ICHR.2008.4755964	computer vision;simulation;machine vision;computer science;humanoid robot;artificial intelligence;computer graphics (images)	Robotics	53.18147039312271	-39.48325569150555	130977
b7771386689647fb7d5ff8e533f6a29169846364	an adaptive threshold deep learning method for fire and smoke detection		This paper proposes a novel method for fire and smoke detection using video images. The ViBe method is used to extract a background from the whole video and to update the exact motion areas using frame-by-frame differences. Dynamic and static features extraction are combined to recognize the fire and smoke areas. For static features, we use deep learning to detect most of fire and smoke areas based on a Caffemodel. Another static feature is the degree of irregularity of fire and smoke. An adaptive weighted direction algorithm is further introduced to this paper. To further reduce the false alarm rate and locate the original fire position, every frame image of video is divided into 16×16 grids and the times of smoke and fire occurrences of each part is recorded. All clues are combined to reach a final detection result. Experimental results show that the proposed method in this paper can efficiently detect fire and smoke and reduce the loss and false detection rates.	algorithm;artificial neural network;convolutional neural network;deep learning;feature extraction;the times;vibe	Xuehui Wu;Xiaobo Lu;Henry Leung	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122904	computer vision;machine learning;motion detection;deep learning;smoke;feature extraction;computer science;artificial intelligence;constant false alarm rate	Robotics	40.52496179072532	-46.19767125710629	130984
2a8812f286669c4bc0eca462c6cbd90a4a9fb5a8	improved shadow removal for robust person tracking in surveillance scenarios	surveillance scenarios;chromacity;caviar dataset;foreground pixel removal;robust person tracking;image segmentation;surveillance;shadow detection;discrimination rates;shadow removal;image classification;shadow discrimination rate;shadow area classification;accuracy;object segmentation;gradient informations;object tracking shadow removal robust person tracking surveillance scenarios foreground detection object segmentation shadow detection rate shadow area classification shadow discrimination rate foreground pixel removal caviar dataset;streaming media;image color analysis;foreground detection;pixel;object tracking;shadow detection rate;correlation;data sets;multiple object tracking;tracking image classification image segmentation object detection;shadow detections;pixel image color analysis correlation surveillance accuracy streaming media noise;person tracking;tracking;object detection;noise	Shadow detection and removal is an important step employed after foreground detection, in order to improve the segmentation of objects for tracking. Methods reported in the literature typically have a significant trade-off between the shadow detection rate (classifying true shadow areas as shadows) and the shadow discrimination rate (discrimination between shadows and foreground). We propose a method that is able to achieve good performance in both cases, leading to improved tracking in surveillance scenarios. Chromacity information is first used to create a mask of candidate shadow pixels, followed by employing gradient information to remove foreground pixels that were incorrectly included in the mask. Experiments on the CAVIAR dataset indicate that the proposed method leads to considerable improvements in multiple object tracking precision and accuracy.	experiment;gradient descent;pixel;shadow volume	Andres Sanin;Conrad Sanderson;Brian C. Lovell	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.43	computer vision;contextual image classification;chromaticity;computer science;noise;video tracking;pattern recognition;geometry;accuracy and precision;tracking;image segmentation;correlation;data set;pixel;statistics;computer graphics (images)	Vision	42.19706386491654	-48.15399459555804	131046
1c46a5ab03488417739e61885f42c058525a57f3	simultaneous segmentation and superquadrics fitting in laser-range data	linear programming robots shape three dimensional displays laser modes mobile communication;optical radar laser ranging object detection;shape;three dimensional displays;robots;mobile communication;linear programming;laser modes;object detection application simultaneous segmentation superquadrics fitting laser range data laser scanner mounted onboard ground robotic platforms object shape fitting indoor environments outdoor environments 3 d data velodyne lidar mobile robotics applications autonomous vehicle applications	This paper presents a method for simultaneous segmentation and modeling of objects, detected in range data gathered by a laser scanner mounted onboard ground-robotic platforms. Superquadrics are used as model for both segmentation and object shape fitting. The proposed method, which we name Simultaneous Segmentation and Superquadrics Fitting, relies on a novel global objective function that accounts for the size of the object and the distance of range points, and for partial occlusions. Results on experimental 2-D range data, which are collected from indoor and outdoor environments, are qualitatively and quantitatively analyzed. Results are compared with those from popular and state-of-the-art segmentation methods. Moreover, we present results on 3-D data obtained from an in-house setup and also from a Velodyne LIDAR. This paper finds applications in areas of mobile robotics and autonomous vehicles, namely object detection, segmentation, and modeling.	3d scanner;algorithm;autonomous robot;curve fitting;floor and ceiling functions;functional approach;gradient;image segmentation;loss function;mobile robot;object detection;optimization problem;preprocessor;robotics;shape context;superquadrics	Ricardo Pascoal;Vítor M. F. Santos;Cristiano Premebida;Urbano Nunes	2015	IEEE Transactions on Vehicular Technology	10.1109/TVT.2014.2321899	robot;computer vision;simulation;mobile telephony;shape;computer science;linear programming;mathematics	Robotics	53.425351397314444	-42.38955492922895	131145
04c2c3cb5b089f112997316ae8c653990ed6bc99	the power of the dark side: using cast shadows for visually-guided touching	humanoid robot;touch physiological;manipulators;computer vision touch physiological humanoid robots manipulators control engineering computing virtual reality;virtual reality;computer graphic;computer vision;robot vision systems cameras robot sensing systems humanoid robots humans calibration reflection mirrors computer graphics virtual reality;robot arm;depth perception;humanoid robots;time to contact;virtual reality cast shadow visually guided touch humanoid robot human workspaces robot arm time to contact estimate 3d control low texture surfaces computer graphics;control engineering computing	A humanoid robot needs to be able to operate safely in human workspaces, preferably without extensive calibration. We consider the problem of directing a robot arm towards, across, and away from an unmodeled surface without damaging it. For this task we make use of a powerful resource: the shadow cast by the robot's own body. We show that the cast shadow of the arm on the surface can be detected by a camera and used to derive a time-to-contact estimate. This estimate, when combined with the 2D tracked location of the arm's endpoint in the camera image, is sufficient to allow 3D control relative to the surface. We show that the same method can detect either cast shadows or reflections, allowing the robot to operate correctly over water, mirrors, or other reflective materials. Such scenarios, along with low-texture surfaces, are cases in which stereovision - the more commonly used depth cue in robotics - might fail and is worth augmenting. We draw on the literature of computer graphics and virtual reality to argue that for manipulation, sensitivity to shadows and interreflection will be of similar importance to stereovision as a depth cue when attempting to touch an object. In computer vision, shadows are generally treated as a nuisance, but that doesn't mean roboticists should do the same.	communication endpoint;computer graphics;computer vision;dark side;humanoid robot;r-cast;reflection (computer graphics);robotic arm;robotics;stereopsis;virtual reality;workspace	Paul M. Fitzpatrick;Eduardo Torres-Jara	2004	4th IEEE/RAS International Conference on Humanoid Robots, 2004.	10.1109/ICHR.2004.1442136	computer vision;simulation;self-shadowing;computer science;humanoid robot;artificial intelligence;social robot;arm solution;virtual reality;robot control;computer graphics (images)	Robotics	52.46690269117358	-40.55768708707665	131186
e66a0484c14d18979309e24f776754de7fd64361	video-rate eigenspace methods for position tracking and remote monitoring	eigenvalues and eigenfunctions;learning process;eigenspace image coefficients;visual position determination;run time performance;runtime;off line learning;computer networks;computer vision;klt;computerized monitoring;feedback;karhunen loeve transforms;robot vision;internet;monitoring;compact representation;position control;video rates;image representation;image reconstruction;principal component analysis;position tracking;remote monitoring principal component analysis visual position determination remote visual monitoring planar robot video rates eigenspace image coefficients internet karhunen loeve transform klt off line learning run time performance minimum image reconstruction error image representation position tracking;telerobotics;minimum image reconstruction error;remote visualization;robot vision eigenvalues and eigenfunctions tracking monitoring principal component analysis karhunen loeve transforms learning artificial intelligence image reconstruction image representation position control telerobotics;remote monitoring;learning artificial intelligence;remote monitoring image reconstruction cameras computer vision computer networks robot vision systems computerized monitoring karhunen loeve transforms runtime feedback;visual tracking;karhunen loeve transform;off the shelf;robot vision systems;planar robot;cameras;remote visual monitoring;tracking;eigenvectors	The use of principal component analysis is employed for visual position determination and simultaneously for remote visual monitoring. The position of a simple planar robot is visually tracked at video rates using eigenspace methods. The eigenspace image coefficients are simultaneously sent over the Internet to visually display the robot operation at a remote location. A set of basis eigenvectors are first determined using the Karhunen-Loeve Transform (KLT) using an off-line learning process. Once the learning phase is complete, the run-time performance of the eigenspace methods are shown to be fast enough to operate at video rates using off-the-shelf components. The eigenspace provides a compact representation that can be employed for rapid position determination and to provide minimum image reconstruction error for a given number of basis vectors. The computational speed, accuracy, and latency for position determination are experimentally determined. The experimental results show that the eigenspace methods perform well for position tracking and for remote monitoring.		Derek C. Schuurman;David W. Capson	2002		10.1109/IAI.2002.999887	iterative reconstruction;telerobotics;computer vision;the internet;simulation;speech recognition;eye tracking;eigenvalues and eigenvectors;computer science;feedback;tracking;karhunen–loève theorem;rmon;principal component analysis	Robotics	47.17552939316326	-44.389459086465095	131202
76a13a1048f26f0a8c143e3180ee98303c3a8877	sken: a statistical test for removing outliers in optical flow a 3d reconstruction case	random variables;computer vision;three dimensional displays;probability distribution;pipelines;context;cameras	The 3D reconstruction can be employed in several areas such as markerless augmented reality, manipulation of interactive virtual objects and to deal with the occlusion of virtual objects by real ones. However, many improvements into the 3D reconstruction pipeline in order to increase its efficiency may still be done. In such context, this paper proposes a filter for optimizing a 3D reconstruction pipeline. It is presented the SKen technique, a statistical hypothesis test that classifies the features by checking the smoothness of its trajectory. Although it was not mathematically proven that inliers features performed smooth camera paths, this work shows some evidence of a relationship between smoothness and inliers. By removing features that did not present smooth paths, the quality of the 3D reconstruction was enhanced.	3d reconstruction;approximation algorithm;augmented reality;map projection;optical flow;real-time clock;real-time computing;reprojection error;requirement;run time (program lifecycle phase);synthetic data;virtual reality headset	Samuel Macêdo;Luis Arthur Vasconcelos;Vinicius Cesar;Saulo A. Pessoa;Judith Kelner	2014	2014 International Conference on Computer Vision Theory and Applications (VISAPP)	10.5220/0004748802020209	probability distribution;random variable;computer vision;simulation;computer science;machine learning;pipeline transport;statistics;computer graphics (images)	Vision	51.70479072023993	-48.791576314670976	131350
fed20dc00f069c856b83c53a90f7d02a69b693c2	ground plane detection in 3d scenes for an arbitrary camera roll rotation through “v-disparity” representation		In this paper we propose a fast method for detecting the ground plane in 3D scenes for an arbitrary roll angle rotation of a stereo vision camera. The method is based on the analysis of the disparity map and its “V-disparity” representation. First, the roll angle of the camera is identified from the disparity map. Then, the image is rotated to a zero-roll angle position and the ground plane is detected from the V-disparity map. The proposed method was successfully verified on a simulated 3D scene image sequences as well as on the recorded outdoor stereo video sequences. The foreseen application of the method is the sensory substitution assistive device aiding the visually impaired in the space perception and mobility.	assistive technology;binocular disparity;disparity filter algorithm of weighted network;sensor;sensory substitution;stereopsis;xbox live vision	Piotr Skulimowski;Mateusz Owczarek;Pawel Strumillo	2017	2017 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2017F40	computer science;ground plane;computer stereo vision;stereo camera;perception;stereo cameras;computer graphics (images);stereopsis;computer vision;image segmentation;artificial intelligence;sensory substitution	Vision	49.07353803420072	-41.913495315481676	131479
9cd1facd061e807068a22cf1153a53f5ef006c8d	video-rate localization in multiple maps for wearable augmented reality	vision system;augmented reality cameras robot vision systems simultaneous localization and mapping switches navigation layout machine vision degradation humans;wearable augmented reality;image motion analysis;map building;real time;3d map building;wearable camera;wearable camera video rate parallel camera tracking wearable augmented reality 3d map building wearable vision system visual structure from motion method;computer vision;visualization;video cameras augmented reality computer vision image motion analysis tracking;video cameras;three dimensional displays;robots;simultaneous localization and mapping;spatial locality;augmented reality;robot vision systems;cameras;visual structure from motion method;video rate parallel camera tracking;wearable vision system;tracking	We show how a system for video-rate parallel camera tracking and 3D map-building can be readily extended to allow one or more cameras to work in several maps, separately or simultaneously. The ability to handle several thousand features per map at video-rate, and for the cameras to switch automatically between maps, allows spatially localized AR workcells to be constructed and used with very little intervention from the user of a wearable vision system. The user can explore an environment in a natural way, acquiring local maps in real-time. When revisiting those areas the camera will select the correct local map from store and continue tracking and structural acquisition, while the user views relevant AR constructs registered to that map.	ar (unix);augmented reality;display resolution;expectation propagation;experiment;map;match moving;outline of object recognition;real-time computing;real-time locating system;scalability;time complexity;wearable computer;wearable technology	Robert Oliver Castle;Georg Klein;David W. Murray	2008	2008 12th IEEE International Symposium on Wearable Computers	10.1109/ISWC.2008.4911577	robot;computer vision;augmented reality;simulation;visualization;computer science;tracking;computer graphics (images);simultaneous localization and mapping	Vision	49.10437218353391	-43.309949292309824	131738
529a776101f602cb1d81089a191c57c0a770102a	joint view-identity manifold for target tracking and recognition	object recognition;template matching joint view identity manifold target tracking target recognition multiview multitarget modeling local linear gaussian process latent variable model ll gplvm probabilistic jvim intraclass variability interclass variability 2d target shapes particle filter based atr algorithm sensiac atr database;gaussian processes;image matching;visual databases gaussian processes image matching object recognition particle filtering numerical methods target tracking;manifolds shape target tracking target recognition joints topology interpolation;target tracking;particle filtering numerical methods;visual databases	A new joint view-identity manifold (JVIM) is proposed for multiview shape modeling that is applied to automated target tracking and recognition (ATR). This work improves our recent work where the view and identity manifolds are assumed to be independent for multi-view multi-target modeling. A local linear Gaussian process latent variable model (LL-GPLVM) is used to learn a probabilistic JVIM which can capture both inter-class and intra-class variability of 2D target shapes under arbitrary view point jointly in one coexisted latent space. A particle filter-based ATR algorithm is developed to simultaneously infer the view and identity parameters along JVIM so that target tracking and recognition can be achieved jointly in a seamlessly fashion. The experimental results using SENSIAC ATR database demonstrate the advantages of our method both qualitatively and quantitatively compared with existing methods using template matching or separate view and identity manifolds.	algorithm;automatic target recognition;gaussian process;ll parser;latent variable model;particle filter;spatial variability;template matching	Jiulu Gong;Guoliang Fan;Liangjiang Yu;Joseph P. Havlicek;Derong Chen	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467120	computer vision;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;gaussian process;mathematics;statistics	Vision	46.057265003746025	-51.38338294896123	131747
df14bff261bc5fd1c5b66251bdea8cf26940c834	illumination robust color naming via label propagation	image color analysis image edge detection lighting robustness histograms computer vision green products;object detection computer vision image colour analysis inference mechanisms;inference process color naming label propagation color composition computer vision object intrinsic reflectance	Color composition is an important property for many computer vision tasks like image retrieval and object classification. In this paper we address the problem of inferring the color composition of the intrinsic reflectance of objects, where the shadows and highlights may change the observed color dramatically. We achieve this through color label propagation without recovering the intrinsic reflectance beforehand. Specifically, the color labels are propagated between regions sharing the same reflectance, and the direction of propagation is promoted to be from regions under full illumination and normal view angles to abnormal regions. We detect shadowed and highlighted regions as well as pairs of regions that have similar reflectance. A joint inference process is adopted to trim the inconsistent identities and connections. For evaluation we collect three datasets of images under noticeable highlights and shadows. Experimental results show that our model can effectively describe the color composition of real-world images.	color;computer vision;illumination (image);image retrieval;markov random field;reflection (computer graphics);software propagation	Yuanliu Liu;Zejian Yuan;Badong Chen;Jianru Xue;Nanning Zheng	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.78	color histogram;false color;computer vision;color quantization;hsl and hsv;color normalization;color image;color balance;computer graphics (images)	Vision	43.984735574339176	-52.02716059395047	131894
0f1c4a0d19aadf8c74caecb931087e4cadbabb22	a global-to-local framework for infrared and visible image sequence registration	temporal domain global to local framework infrared image sequence registration visible image sequence registration global registration method global error minimizing approach surveillance applications dynamic contents global registration mutual information sum global homography estimation frame to frame registration per frame local homography temporal consistency public benchmark dataset;image sequences estimation smoothing methods cameras robustness image registration sensors;sensors;infrared imaging image registration image sequences;smoothing methods;estimation;image registration;robustness;cameras;image sequences	Based on the development of image registration, sequence registration can be done by computing the transformations between consecutive frames. To take into account the accumulated error, global registration method is usually employed as a global error minimizing approach. However, in real surveillance applications, the visible sequence and infrared sequence may be taken at different times, or from different viewpoints, and may have different dynamic contents. Therefore, global registration is only an approximate estimation for two sequences, resulting in inferior local contents. In this paper we present a novel integrated global-to-local framework that addresses the problems of dynamic infrared and visible image sequence registration. We propose to maximize the sum of the mutual information of two sequences for the global homography estimation. Then, frame-to-frame registration is performed to estimate the per-frame local homography. Finally, a smoothing strategy is adopted to smooth the local homographies in the temporal domain to enforce temporal consistency. We evaluate our proposed framework by comparing it to the state-of-the art sequence registration algorithm. Our method achieves improved performance on the public benchmark dataset.	approximation algorithm;benchmark (computing);closed-circuit television;expectation–maximization algorithm;experiment;homography (computer vision);image registration;multimodal interaction;mutual information;smoothing;transformation matrix	Michael Ying Yang;Yu Qiang;Bodo Rosenhahn	2015	2015 IEEE Winter Conference on Applications of Computer Vision	10.1109/WACV.2015.57	homography;computer vision;estimation;computer science;sensor;image registration;pattern recognition;statistics;robustness	Vision	50.72950645160401	-49.65207236589314	131976
72bcdc650a1af65bbc486e7fc89dd38f8dfba131	multiresolution parametric region tracking for 2d object replacement in video	image resolution;video signal processing;location estimation;texture mapping;image texture;video signal processing image resolution gradient methods image texture image sequences;gradient methods;target tracking video sequences parameter estimation frequency domain analysis cameras augmented reality image generation tv;multi resolution;efficient estimation;image warping multiresolution parametric region tracking 2d object replacement video sequences gradient based techniques target location object replacement algorithm texture mapping techniques;image warping;image sequences;pose estimation	This paper develops an efficient parametric multiresolution region tracking algorithm which is applied to the task of object replacement in video sequences. The tracker relies on gradient-based techniques to provide efficient estimation of the target location and pose in each frame. The tracking algorithm improves upon similar efficient parametric algorithms by increasing the distance an object can move from frame to frame. Experimental results provide clear evidence of the improved performance over existing region tracking. The parameters that estimate the pose and location of the target are used to initialise the object replacement algorithm. The object replacement uses the pose estimates with texture mapping techniques to perform image warping of an arbitrary sized replacement image. The location estimates are used to accurately insert the replacement image into the current frame.	gradient;image warping;multiresolution analysis;page replacement algorithm;texture mapping	Paul Brasnett;David R. Bull;Cedric Nishan Canagarajah	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1419473	image warping;image texture;texture mapping;computer vision;pose;image resolution;3d pose estimation;image processing;computer science;video tracking;pattern recognition;computer graphics (images)	Robotics	48.13767779385516	-48.229808185669775	131981
bc6caaa873d9e1247dd797a3f1dd3466baba1790	3d morphable model fitting from multiple views	feature extraction curve fitting face recognition;cost function;multiple views;face recognition;shape;three dimensional displays;feature extraction;extracted facial feature sequences 3d morphable model fitting multiple views;solid modeling;mathematical model;facial features;face;model fitting;curve fitting;extracted facial feature sequences;3d morphable model fitting;shape active appearance model facial animation head humans equations fitting rendering computer graphics face recognition facial features	This paper presents a method to fit a 3D Morphable Model to a sequence of extracted facial features. The approach is a direct extension of a single view method. The novelty is presented as a new mathematical derivation of the same method but for multiple views where identity and pose are known. The new fitting method exploits point-to-model correspondences and can deal with occlusion since features are not required to correspond across views. The mathematics is explained in detail and the methods single tunable parameter is empirically determined using a database of scanned heads.	algorithm;automatic acoustic management;curve fitting;ground truth;sensor	Nathan Faggian;Andrew P. Paplinski;Jamie Sherrah	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813340	computer vision;computer science;pattern recognition;computer graphics (images)	Vision	48.26110470279411	-50.39995402448736	132030
1bdfd057bcd034e71fcb10cf71d1b370974403cd	a new hierarchical simultaneous localization and mapping method based on local shape context of environment	silicon;local shape context;robot design;map building;active loop closing;hierarchical simultaneous localization and mapping method;shape recognition;mobile robots;laser ranging;slam robots laser ranging mobile robots shape recognition;robot hierarchical simultaneous localization and mapping method local shape context laser range finder active loop closing method;laser range finder;hierarchical map representation;simultaneous localization and mapping;active loop closing method;robot;active loop closing simultaneous localization and mapping local shape context hierarchical map representation;slam robots;buildings;simultaneous localization and mapping silicon buildings	A new kind of local shape context (LSC) is proposed for hierarchical simultaneous localization and mapping (HSLAM) by taking account of the features of informa­tion from laser range Ander. In the process of HSLAM, LSC is used to incrementally segment the map that has been created into topological node. And at the same time an active loop closing method for HSLAM is pro­posed based on the information of LSC. Experiments are carried out with the robot designed by our lab and the experimental results prove that our method per­forms well for map building in large environments.	closing (morphology);robot;shape context;simultaneous localization and mapping	Ronghua Luo;Huaqing Min;Yan Shao	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580817	robot;mobile robot;computer vision;simulation;computer science;artificial intelligence;silicon;simultaneous localization and mapping	Robotics	51.010782668117464	-38.38819460971361	132277
afc6dbfa40187bbb3191a38a5e848c12eaa6d054	vision based obstacle tracking in urban traffic environments	road traffic;kalman filters;prediction algorithms;kalman filter;computer vision;optical imaging;traffic engineering computing computer vision feature extraction kalman filters natural scenes object tracking road traffic stereo image processing;feature extraction;adaptive optics covariance matrix vehicles prediction algorithms kalman filters optical sensors optical imaging;object tracking;stereo image processing;multiple object tracking vision based obstacle tracking urban traffic environment stereo vision based sensor semantic information extraction natural scene stereo reconstruction process temporal information error minimization advanced object model association mechanism kalman filter based tracking algorithm;traffic engineering computing;optical sensors;vehicles;optical sensor;natural scenes;adaptive optics;covariance matrix	Stereo vision based sensors provide large amounts of data, a fact which is advantageous when trying to extract semantic information about the imaged scene. However, this data is corrupted by errors, caused especially by the uncertainties in the stereo reconstruction process. Temporal information can be used in order to minimize these errors. This paper presents an advanced object model, a novel association mechanism and the design of a Kalman filter based tracking algorithm, for tracking multiple objects, in complex, urban traffic scenarios.	algorithm;analysis of algorithms;computation;correspondence problem;kalman filter;mathematical model;object detection;optical flow;particle filter;sensor;stereopsis	Silviu Bota;Sergiu Nedevschi	2011	2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing	10.1109/ICCP.2011.6047874	computer vision;simulation;geography;remote sensing	Robotics	47.35221790739276	-47.496589076105124	132448
c75b2ce90aa8814627a5e9ad55a2017214dd0067	deformable object segmentation and contour tracking in image sequences using unsupervised networks	static information;unsupervised learning;object recognition;dexterous manipulation;pattern clustering;manipulators;image segmentation;robot hand;spatial features;deformable object segmentation;neural networks;neural nets;object segmentation image sequences image segmentation unsupervised learning robotics and automation robot vision systems cameras tracking clustering algorithms performance evaluation;automatic segmentation;segmentation;color information;unsupervised learning dexterous manipulators image colour analysis image segmentation image sequences neural nets object detection pattern clustering robot vision;deformable objects;robotic hand;neural gas;dexterous manipulators;dynamic information;robot vision;unsupervised network;image color analysis;image colour analysis;contour tracking;pixel;image sequence;clustering approach;neural gas networks deformable object segmentation contour tracking image sequences unsupervised learning approach robotic hand manipulators clustering approach color information spatial features unsupervised network static information dynamic information;neurons;unsupervised learning approach;neural gas networks;neural gas segmentation tracking deformable objects dexterous manipulation;tracking;object detection;image sequences	The paper discusses a novel unsupervised learning approach for tracking deformable objects manipulated by a robotic hand in a series of images collected by a video camera. The object of interest is automatically segmented from the initial frame in the sequence. The segmentation is treated as clustering based on color information and spatial features and an unsupervised network is employed to cluster each pixel of the initial frame. Each pixel from the clustering results is then classified as either object of interest or background and the contour of the object is identified based on this classification. Using static (color) and dynamic (motion between frames) information, the contour is then tracked with an algorithm based on neural gas networks in the sequence of images. Experiments performed under different conditions reveal that the method tracks accurately the test objects even for severe contour deformations, is fast and insensitive to smooth changes in lighting, contrast and background.	algorithm;cluster analysis;color;contour line;neural gas;pixel;robot;unsupervised learning	Ana-Maria Cretu;Emil M. Petriu;Pierre Payeur;Fouad F. Khalil	2010	2010 Canadian Conference on Computer and Robot Vision	10.1109/CRV.2010.43	neural gas;computer vision;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;tracking;image segmentation;segmentation;artificial neural network;pixel	Vision	40.65493440583957	-47.759098847493405	132506
e5813bc47ebb1f434a8fa053c857e53c2c41e004	tracking white road line by particle filter from the video sequence acquired by the camera attached to a walking human body	particles;clafic;matrices;roads;navigation systems;particle filter;principal component analysis;visually impaired;particle filters;video;pca;lane detection;cameras;lane recognition	This paper proposes a method for tracking and recognizing the white line marked in the surface of the road from the video sequence acquired by the camera attached to a walking human, towards the actualization of an automatic navigation system for the visually handicapped. Our proposed method consists of two main modules: (1) Particle Filter based module for tracking the white line, and (2) CLAFIC Method based module for classifying whether the tracked object is the white line. In (1), each particle is a rectangle, and is described by its centroid's coordinates and its orientation. The likelihood of a particle is computed based on the number of white pixels in the rectangle. In (2), in order to obtain the ranges (to be used for the recognition) for the white line's length and width, Principal Component Analysis is applied to the covariance matrix obtained from valid sample particles. At each frame, PCA is applied to the covariance matrix constructed from particles with high likelihood, and if the obtained length and width are within the abovementioned ranges, it is recognized as the white line. Experimental results using real video sequences show the validity of the proposed method.© (2012) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.	particle filter	Shohei Takahashi;Jun Ohya	2012		10.1117/12.907814	computer vision;simulation;particle filter;principal component analysis;computer graphics (images)	Vision	48.55909432019043	-41.61669461424284	132578
374682b0595c3144e05e2c4ef575b4d5f28a3eef	learning to track multiple people in omnidirectional video	high resolution;person specific models;real time;software systems;large data sets;video indexing;multiple people tracking;automatic detection;subspace methods;person specific models omnidirectional video capturing multiple people tracking subspace methods meeting understanding;subspace method;meeting understanding;people tracking;omnidirectional video capturing;everyday life;cameras robustness educational institutions hardware software systems monitoring image resolution video recording face detection photometry	Meetings are a very important part of everyday life for professionals working in universities, companies or governmental institutions. We have designed a physical awareness system called CAMEO (Camera Assisted Meeting Event Observer), a hardware/software system to record and monitor people's activities in meetings. CAMEO captures a high resolution omnidirectional view of the meeting by stitching images coming from almost concentric cameras. Besides recording capability, CAMEO automatically detects people and learns a person-specific facial appearance model (PS-FAM) for each of the participants. The PSFAMs allow more robust/reliable tracking and identification. In this paper, we describe the video-capturing device, photometric/geometric autocalibration process, and the multiple people tracking system. The effectiveness and robustness of the proposed system is demonstrated over several real-time experiments and a large data set of videos.	360-degree video;algorithm;component-based software engineering;distortion;experiment;facial recognition system;fuzzy associative matrix;image resolution;image stitching;microphone;online and offline;pan–tilt–zoom camera;radial (radio);real-time clock;sensor;software system;tracking system	Fernando De la Torre;Carlos Vallespí;Paul E. Rybski;Manuela M. Veloso;Takeo Kanade	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570757	computer vision;simulation;image resolution;computer science;multimedia;software system	Robotics	47.074046440486065	-44.84339015478216	132580
598983ab5296874935d26c15f5019561442d3d2b	motion-blur-free microscopic video shooting based on frame-by-frame intermittent tracking	camera shutter state motion blur free microscopic video shooting motion blur free microscope shooting nonblurred videos moving objects frame by frame intermittent tracking method microscopic high speed tracking system position controls objective lens high frame rate video processing;microscopy cameras image sensors target tracking indexes lenses;video signal processing lenses object tracking	We develop a motion-blur-free microscope for shooting non-blurred videos of moving objects by implementing a frame-by-frame intermittent tracking method with a microscopic high-speed tracking system that controls the position of the objective lens using high-frame-rate video processing. With our tracking method, the control target alternates at several hundred hertz, according to the camera's shutter state. The apparent speed of the target object in the captured images is maintained at zero to suppress motion blur when the shutter is open, and the camera's position returns to its home position when the shutter is closed. Our motion-blur-free microscope can shoot non-blurred 512×512 images at 125 fps with frame-by-frame intermittent tracking for objects moving unidirectionally. Compared with the degradation in video that was recorded without tracking, our method reduces image degradation from motion blur 1/40 times or less without decreasing the exposure time when a 10× objective lens is attached. Its performance is verified by showing the experimental results from several moving scenes in microscopic view.	elegant degradation;experiment;gaussian blur;high-throughput computing;motion compensation;movie projector;piezoelectricity;throughput;tracking system;video processing;visual inspection	Takahiro Ueno;Qingyi Gu;Tadayoshi Aoyama;Takeshi Takaki;Idaku Ishii;Tomohiro Kawahara	2015	2015 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/CoASE.2015.7294185	computer vision;geography;video tracking;shutter speed;optics;computer graphics (images)	Robotics	48.821508140913245	-44.87028379447772	132669
4a591b06e2debc9f97b9a9601a66f7449963a9fc	body-based gender recognition using images from visible and thermal cameras	thermal image;visible light image;gender recognition	Gender information has many useful applications in computer vision systems, such as surveillance systems, counting the number of males and females in a shopping mall, accessing control systems in restricted areas, or any human-computer interaction system. In most previous studies, researchers attempted to recognize gender by using visible light images of the human face or body. However, shadow, illumination, and time of day greatly affect the performance of these methods. To overcome this problem, we propose a new gender recognition method based on the combination of visible light and thermal camera images of the human body. Experimental results, through various kinds of feature extraction and fusion methods, show that our approach is efficient for gender recognition through a comparison of recognition rates with conventional systems.	accessory;attempt;computer vision;control system;experiment;feature extraction;human–computer interaction;illumination (image);large;light, visible	Tien Dat Nguyen;Kang Ryoung Park	2016	Sensors	10.3390/s16020156	computer vision;three-dimensional face recognition;multimedia	Vision	45.144087571252264	-43.47403310821019	132718
4a68b04c3217d18a2f6ea6547746d2bbb5099ec0	adaptive color claddification with gaussian mixture model	classification algorithm;gaussian processes;dichromatic reflectance model;dichromatic reflectance model gaussian mixture model adaptive color classification algorithm robocup color based vision faces;real time;lighting reflectivity machine vision robot vision systems face detection robustness robust stability pixel histograms classification algorithms;color model;robocup;color based vision faces;gaussian mixture model;robot vision;image colour analysis;reflection model;multi robot systems;robot vision gaussian processes image colour analysis multi robot systems;adaptive color classification algorithm	In this paper we present an adaptive color classification algorithm for RoboCup. Color-based vision faces the challenge of perceived color being variant to illumination. We propose a color classification algorithm that is robust and reliable under dynamic lighting conditions. Supported by the dichromatic reflectance model, we use a Gaussian mixture model (GMM) of two components to represent the distribution of a color class of interest in the YUV space. The color model is continuously updated to achieve adaptation. We show experimentally that a GMM with two components can be used as an accurate and complete representation of a dichromatic surface, and that our algorithm is capable of adapting and classifying color classes in real time	algorithm;approximation;color;computer graphics lighting;experiment;google map maker;mixture model	Xiaohu Lu;Hong Zhang	2005	2005 IEEE International Conference on Robotics and Biomimetics - ROBIO	10.1109/ROBIO.2005.246287	color histogram;computer vision;color model;color quantization;color normalization;computer science;machine learning;mixture model;gaussian process;color balance;statistics;computer graphics (images)	Robotics	44.34925462830004	-49.596548043738956	132719
10b055e54fa3f81ea5018a923065fa07d878ae5c	texture overlay onto flexible object with pca of silhouettes and k-means method for search into database		In this paper, we propose a method to overlay an arbitrary texture onto a T-shirt worn by a user. To realize such a system, two phases, an offline phase and an online phase are required. During the offline phase, we use a T-shirt with several markers for training images and establishing a correspondence between the shape of the T-shirt area and the array of markers. During the online phase, we use a T-shirt without any markers for the input image and search for a training image which has the most similar shape to the input image. By using markers from the selected training image, we overlay a texture onto the input image. In this method, we represent the shape of a T-shirt based on its contour so that we can handle occlusion by the hands when the hands cross over the T-shirt. In addition, the computation time is reduced by using PCA and K-means method, which enables our system to work in real time.	computation;k-means clustering;online and offline;principal component analysis;time complexity	Hiroshi Tanaka;Hideo Saito	2009			image texture;computer vision;computer science;computer graphics (images)	Vision	47.96757313847391	-46.04724623678731	132974
0cd00aa32cbd0d05d412eb1a117d1318a95df729	real-time enhancement of dynamic depth videos with non-rigid deformations	bilateral total variation depth enhancement super resolution non rigid deformations registration kalman filtering;three dimensional displays videos heuristic algorithms real time systems image resolution cameras two dimensional displays	We propose a novel approach for enhancing depth videos containing non-rigidly deforming objects. Depth sensors are capable of capturing depth maps in real-time but suffer from high noise levels and low spatial resolutions. While solutions for reconstructing 3D details in static scenes, or scenes with rigid global motions have been recently proposed, handling unconstrained non-rigid deformations in relative complex scenes remains a challenge. Our solution consists in a recursive dynamic multi-frame super-resolution algorithm where the relative local 3D motions between consecutive frames are directly accounted for. We rely on the assumption that these 3D motions can be decoupled into lateral motions and radial displacements. This allows to perform a simple local per-pixel tracking where both depth measurements and deformations are dynamically optimized. The geometric smoothness is subsequently added using a multi-level $L_1$ minimization with a bilateral total variation regularization. The performance of this method is thoroughly evaluated on both real and synthetic data. As compared to alternative approaches, the results show a clear improvement in reconstruction accuracy and in robustness to noise, to relative large non-rigid deformations, and to topological changes. Moreover, the proposed approach, implemented on a CPU, is shown to be computationally efficient and working in real-time.		Kassem Al Ismaeil;Djamila Aouada;Thomas Solignac;Bruno Mirbach;Björn E. Ottersten	2017	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2016.2622698	robustness (computer science);computer science;artificial intelligence;computer vision;total variation denoising;minification;recursion;smoothness;kalman filter;synthetic data;image resolution	Vision	52.53602767516442	-48.050133868350734	133016
50121d605efdc93e276dcb93f06be425e6ddf637	multi-view human body pose estimation with cuda-pso		The authors formulate the body pose estimation as a multi-dimensional nonlinear optimization problem, suitable to be approximately solved by a meta-heuristic, specifically, the particle swarm optimization (PSO). Starting from multi-view video sequences acquired in a studio environment, a full skeletal configuration of the human body is retrieved. They use a generic subdivision-surface body model in 3-D to generate solutions for the optimization problem. PSO then looks for the best match between the silhouettes generated by the projection of the model in a candidate pose and the silhouettes extracted from the original video sequence. The optimization method, in this case PSO, is run in parallel on the Graphics Processing Unit (GPU) and is implemented in Cuda-CTM on the nVidia CUDATM architecture. The authors compare the results obtained by different configurations of the camera setup, fitness function, and PSO neighborhood topologies. DOI: 10.4018/jaras.2012100104 52 International Journal of Adaptive, Resilient and Autonomic Systems, 3(4), 51-65, October-December 2012 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. where the use of tight body suits and markers is not acceptable. Additionally, the increasing availability and affordability of the video cameras makes markerless motion capture an ever more attractive alternative. Modeling the articulated structure of the full human body for the purpose of pose estimation requires a large number of parameters, typically at least 30. The articulated pose estimation problem is therefore usually formulated as a search in a high dimensional parameter space, which is invariably computationally very complex. In this paper, we address the issue of complexity by exploring the parallel nature of the markerless pose estimation problem at hand and searching the corresponding large parameter space using Particle Swarm Optimization (PSO). We exploit the fact that the PSO solution naturally lends itself to a parallel implementation on the state-of-the-art CUDATM architecture, as well as that the multi-view pose estimation, based on silhouette comparison, itself contains a degree of parallelism that can be exploited to design a more efficient solution. This paper is organized as follows. We begin with an overview of the state of the art in body pose estimation and parallel PSO in the Related Work Section. In the Implementation Section we outline the CUDATM architecture and present the PSO algorithm developed for it. The details of our method are presented in the Pose Estimation Algorithm Section. Finally, we report experimental results and conclusion.	autonomic computing;cuda;central processing unit;computational resource;degree of parallelism;desktop computer;fitness function;graphics processing unit;heuristic (computer science);mathematical optimization;motion capture;nonlinear programming;nonlinear system;optimization problem;overhead (computing);parallel computing;particle swarm optimization;phase-shift oscillator;rocchio algorithm;run time (program lifecycle phase);speedup;subdivision surface	Luca Mussi;Spela Ivekovic;Youssef S. G. Nashed;Stefano Cagnoni	2012	IJARAS	10.4018/jaras.2012100104	computer vision;mathematical optimization;simulation;computer science	Vision	52.98048634358573	-47.53652343148835	133256
024ccd341b0a7198691ab5d3ee04d8b26fc03b2b	enhancement of particle filter approach for vehicle tracking via adaptive resampling algorithm	tl motor vehicles aeronautics astronautics;histograms;particle filter degeneracy problem adaptive particle filter enhancement vehicle tracking adaptive resampling algorithm road traffic control surveillance system security system vehicle occlusion problem;computer graphics;surveillance system;color;road traffic;qa mathematics;traffic control;traffic engineering computing adaptive filters computer graphics object tracking particle filtering numerical methods road traffic traffic control;adaptive particle filter enhancement;security system;road traffic control;accuracy;vehicles particle filters color target tracking histograms image color analysis accuracy;adaptive filters;particle filter;resampling vehicle tracking particle filter likelihood;image color analysis;object tracking;secure system;traffic engineering computing;vehicles;vehicle tracking;resampling;particle filters;target tracking;adaptive resampling algorithm;vehicle occlusion problem;likelihood;particle filter degeneracy problem;particle filtering numerical methods	Nowadays, vehicle tracking is a vital approach to assist and improve the road traffic control, surveillance and security systems by having the detail of the captured vehicle information. In past, many tracking techniques have been implemented and suffered from the well known 'occlusion' problems. Increasing the accuracy of the tracking algorithm has caused the computational cost due to the inflexibility to adapt the partial and fully occluded situations. Besides occlusion, appearance of new objects and background noises in the captured videos increase the difficulties of continuously tracking the labelled vehicles. In this paper, an adaptive particle filter approach has been proposed as the tracking algorithm to solve the vehicle occlusion problem. In order to solve the common particle filter degeneracy problem, the proposed particle filter is equipped with the adaptive resampling algorithm which is capable of dealing with various occlusion incidents. The experimental results show that enhancement of the particle filter via resampling algorithm has been robustly tracking the vehicles, and significantly improve the accuracy in tracking the occluded vehicles without compromising the processing time.	algorithmic efficiency;computation;degeneracy (graph theory);particle filter;peterson's algorithm;time complexity;vehicle tracking system;whole earth 'lectronic link	Wei Leong Khong;Wei Yeang Kow;Farrah Wong;Ismail Saad;Kenneth Tze Kin Teo	2011	2011 Third International Conference on Computational Intelligence, Communication Systems and Networks	10.1109/CICSyN.2011.62	computer vision;simulation;particle filter;tracking system;computer science;statistics	Robotics	42.56645006489998	-45.31822230270076	133270
568ed7553c96ae54f3977d5da9d0e142bd9ed086	recognition of pedestrian active events by robust to noises boost algorithm		The active events recognition is the essential part of surveillance systems. In such systems, the statistical training methods are very popular. In this research, the Robust for Noises Boost (RONBoost) algorithm is proposed. It is based on the classifying cascades and was tested on various scenarios of pedestrian behavior in the outdoor scenes. The satisfactory results were achieved not only by the RONBoost algorithm implementation but also the object capture and tracking based on the spatiotemporal filtering and the motion segmentation using the analysis of tensor structures. The error of motion segmentation was decreased to 4 %. The experiments demonstrate the satisfactory results: the deviation of moving regions is less 10 % and the classification error is around 2–5 %. For video sequences with worse luminance conditions, this error is increased up to 7–10 %.	algorithm	Margarita N. Favorskaya;Lakhmi C. Jain	2014		10.1007/978-3-319-18416-6_68	simulation;speech recognition;machine learning	Vision	41.36389506331446	-45.56808537188378	133289
be2c456c18237c6b15c0f0d09b20797b9e7a9bd1	the video collaborative localization of a miner’s lamp based on wireless multimedia sensor networks for underground coal mines	biological patents;underground coal mine;biomedical journals;text mining;europe pubmed central;citation search;citation networks;video collaborative localization;wireless multimedia sensor networks wmsns;research articles;abstracts;期刊论文;open access;life sciences;clinical guidelines;miner s lamp;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Based on wireless multimedia sensor networks (WMSNs) deployed in an underground coal mine, a miner's lamp video collaborative localization algorithm was proposed to locate miners in the scene of insufficient illumination and bifurcated structures of underground tunnels. In bifurcation area, several camera nodes are deployed along the longitudinal direction of tunnels, forming a collaborative cluster in wireless way to monitor and locate miners in underground tunnels. Cap-lamps are regarded as the feature of miners in the scene of insufficient illumination of underground tunnels, which means that miners can be identified by detecting their cap-lamps. A miner's lamp will project mapping points on the imaging plane of collaborative cameras and the coordinates of mapping points are calculated by collaborative cameras. Then, multiple straight lines between the positions of collaborative cameras and their corresponding mapping points are established. To find the three-dimension (3D) coordinate location of the miner's lamp a least square method is proposed to get the optimal intersection of the multiple straight lines. Tests were carried out both in a corridor and a realistic scenario of underground tunnel, which show that the proposed miner's lamp video collaborative localization algorithm has good effectiveness, robustness and localization accuracy in real world conditions of underground tunnels.	algorithm;anthracosis;bifurcation theory;body position;cap theorem;capsule endoscopy;coal;global illumination;intersection of set of elements;least squares;least-squares analysis;multimedia;poor posture;sensor;squatting attack;underground	Kaiming You;Wei Yang;Ruisong Han	2015		10.3390/s151025103	embedded system;text mining;medical research;telecommunications;computer science;bioinformatics;engineering;electrical engineering;data mining;world wide web;computer security	Mobile	49.48828217464572	-41.92365138112797	133327
3ee53fe0b721567153459f676977ffeae5ea5395	efficient disparity computation without maximum disparity for real-time stereo vision	real time;stereo vision	In order to improve the performance of correlation-based disparity computation of stereo vision algorithms, standard methods need to choose in advance the value of the maximum disparity (MD). This value corresponds to the maximum displacement of the projection of a physical point expected between the two images. It generally depends on the motion model, the camera intrinsic parameters and on the depths of the observed scene. In this paper, we show that there is no optimal MD value that minimizes the matching errors in all image regions simultaneously and we propose a novel approach of the disparity computation that does not rely on any a priori MD. A local energy minimization is also proposed for fast refinement of the results. We successfully reduce the matching errors compared to traditional local correlation since our approach uses locally a minimal MD. Furthermore, our approach is simple to implement and the iterative search technique might be integrated into other stereo matching algorithms as well. Finally, our proposal is even faster than the fastest possible implementation of local correlation with integral images [1, 4]. That is because we significantly reduce the number of required correlations which ultimately saves processing time.	algorithm;binocular disparity;camera resectioning;computation;computer stereo vision;displacement mapping;energy minimization;fastest;iterative method;molecular dynamics;real-time transcription;refinement (computing);stereopsis	Christian Unger;Selim Benhimane;Eric Wahl;Nassir Navab	2009		10.5244/C.23.42	computer vision;simulation;computer science;stereopsis;computer graphics (images)	Vision	52.91813477234232	-47.539243881197635	133687
8db42cd675025d296b41adca53b4ae00230b2969	stereo-vision for autonomous industrial inspection robots		Camera-based stereo-vision provides cost-efficient vision capabilities for robotic systems. The objective of this paper is to examine the performance of stereo-vision as means to enable a robotic inspection cell for haptic quality testing with the ability to detect relevant information related to the inspection task. This information comprises the location and 3D representation of a complex object under inspection as well as the location and type of quality features which are subject to the inspection task. Among the challenges is the low-distinctiveness of features in neighboring area, inconsistent lighting, similar colors as well as low intra-class variances impeding the retrieval of quality characteristics. The paper presents the general outline of the vision chain as well as performance analysis of various algorithms for relevant steps in the machine vision chain thus indicating the capabilities and drawbacks of a camera-based stereo-vision for flexible use in complex machine vision tasks.	algorithm;autonomous robot;color;cost efficiency;haptic technology;machine vision;robotic arm;stereopsis	Daniel Frank;Jimmy Chhor;Robert Schmitt	2017	2017 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2017.8324804	task analysis;control engineering;engineering;feature extraction;computer vision;robot;haptic technology;machine vision;artificial intelligence;stereopsis	Robotics	49.188031993483065	-39.57330127798242	133728
2067a5a4e8851bf8083285a33f542229792f1826	integrating stereo vision with a cnn tracker for a person-following robot.		In this paper, we introduce a stereo vision based CNN tracker for a person following robot. The tracker is able to track a person in real-time using an online convolutional neural network. Our approach enables the robot to follow a target under challenging situations such as occlusions, appearance changes, pose changes, crouching, illumination changes or people wearing the same clothes in different environments. The robot follows the target around corners even when it is momentarily unseen by estimating and replicating the local path of the target. We build an extensive dataset for person following robots under challenging situations. We evaluate the proposed system quantitatively by comparing our tracking approach with existing real-time tracking algorithms.	algorithm;artificial neural network;convolutional neural network;map;obstacle avoidance;real-time clock;robotics;social robot;stereopsis	Bao Xin Chen;Raghavender Sahdev;John K. Tsotsos	2017		10.1007/978-3-319-68345-4_27	machine learning;computer vision;artificial intelligence;computer stereo vision;convolutional neural network;robot;computer science;stereo cameras;stereopsis	Robotics	46.78157358715454	-40.73941054592465	133775
d8c760e02965bcfd3b6b3c5f184a1ff91df81fc7	bi-model tracking of object of interest using invariant spatiogram descriptor	bi model algorithm object tracking invariant spatiogram descriptor;electrical capacitance tomography petroleum milling machines surveillance position measurement rotation measurement error correction rna image retrieval;target tracking object detection;target tracking;object detection	In this paper, we proposed, a new approach to track the object of interest using bi-model algorithm. The bi-model comprises a long term model and a short term model to allow continuous tracking of the object of interest. To measure the similarity of the bi-model and the candidate targets, an invariant spatiogram descriptor was proposed. Compared to the original spatiogram the new definition is invariant to rotation without sacrificing the performance. The experiments on our dataset and the publicly available datasets verify the effectiveness of our proposed algorithm.	algorithm;experiment;scale-invariant feature transform	Jun Li;Wei-Yun Yau;Jian-Gang Wang;Wee Ser	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761499	computer vision;simulation;data mining;mathematics	Vision	40.61378106957822	-51.39939731516055	133790
fb7bec199a2f7d3caf9bbfa5b9644c3914980bf0	efficient lane boundary detection with spatial-temporal knowledge filtering	spatial temporal knowledge;structure triangle filter;crossing point filter;lane detection	Lane boundary detection technology has progressed rapidly over the past few decades. However, many challenges that often lead to lane detection unavailability remain to be solved. In this paper, we propose a spatial-temporal knowledge filtering model to detect lane boundaries in videos. To address the challenges of structure variation, large noise and complex illumination, this model incorporates prior spatial-temporal knowledge with lane appearance features to jointly identify lane boundaries. The model first extracts line segments in video frames. Two novel filters-the Crossing Point Filter (CPF) and the Structure Triangle Filter (STF)-are proposed to filter out the noisy line segments. The two filters introduce spatial structure constraints and temporal location constraints into lane detection, which represent the spatial-temporal knowledge about lanes. A straight line or curve model determined by a state machine is used to fit the line segments to finally output the lane boundaries. We collected a challenging realistic traffic scene dataset. The experimental results on this dataset and other standard dataset demonstrate the strength of our method. The proposed method has been successfully applied to our autonomous experimental vehicle.	autonomous robot;coalition for patent fairness;frame (physical object);silo (dataset);state machine;unavailability	Zhixiong Nan;Ping Wei;Linhai Xu;Nanning Zheng	2016		10.3390/s16081276	computer vision;simulation	AI	43.01624946479571	-46.93232582935696	133894
17070594cd0c3602fb914f7d34a643fecbea497a	vision-based analysis of pedestrian traffic data	urban environment;stereo vision pedestrian tracking;statistical traffic flow data;urban planning;image processing;intelligent transport system;information retrieval;economic forecasting;public transport;road traffic;intelligent transportation systems;magnetic sensors;automated highways;traffic control;traffic flow;traffic management;user defined pedestrian events;computer vision;monitoring;traffic congestion;roads;three dimensional displays;image color analysis;pedestrian traffic data;stereo vision pedestrian detection;indexation;stereo image processing;transportation;pedestrian detection;stereo vision;environmental economics;cities and towns;statistical information;traffic engineering computing;robustness;traffic engineering computing automated highways computer vision road traffic stereo image processing;roads intelligent transportation systems robustness economic forecasting environmental economics traffic control cities and towns intelligent sensors monitoring magnetic sensors;vision based analysis;public transport systems;intelligent sensors;cameras;urban planning vision based analysis pedestrian traffic data traffic congestion public transport systems intelligent transportation systems stereo vision pedestrian detection stereo vision pedestrian tracking user defined pedestrian events statistical traffic flow data statistical information traffic management;real time systems	Reducing traffic congestion has become a major issue within urban environments. Traditional approaches, such as increasing road sizes, may prove impossible in certain scenarios, such as city centres, or ineffectual if current predictions of large growth in world traffic volumes hold true. An alternative approach lies with increasing the management efficiency of pre-existing infrastructure and public transport systems through the use of intelligent transportation systems (ITS). In this paper, we focus on the requirement of obtaining robust pedestrian traffic flow data within these areas. We propose the use of a flexible and robust stereo-vision pedestrian detection and tracking approach as a basis for obtaining this information. Given this framework, we propose the use of a pedestrian indexing scheme and a suite of tools, which facilitates the declaration of user-defined pedestrian events or requests for specific statistical traffic flow data. The detection of the required events or the constant flow of statistical information can be incorporated into a variety of ITS solutions for applications in traffic management, public transport systems and urban planning.	cluster analysis;declaration (computer programming);interaction;java hotspot virtual machine;logical connective;network congestion;pedestrian detection;requirement;stereopsis	Philip Kelly;Noel E. O'Connor	2008	2008 International Workshop on Content-Based Multimedia Indexing	10.1109/CBMI.2008.4564938	computer vision;transport;intelligent transportation system;active traffic management;simulation;floating car data;image processing;computer science;stereopsis;economic forecasting;traffic flow;urban planning;public transport;robustness;intelligent sensor	HPC	40.75758834610301	-41.859884315707006	133955
937a822ef174381eff059d088c36439fc3df5c97	multiple target localization in wireless visual sensor networks	wei li weizhang 多目标定位 传感器网络 无线 目标匹配 世界坐标系 定位问题 目标信息 目标位置 multiple target localization in wireless visual sensor networks;target localization statistics wireless visual sensor networks occlusion limited field of view	Target localization is an important service in wireless visual sensor networks (WVSN). Although the problem of single target localization has been intensively studied, few consider the problem of multiple target localization without prior target information in WVSN. In this paper, we first investigate the architecture of WVSN where data transmission is reduced to only target positions. Since target matching is a key issue in the multiple target localization, we propose a statistical method to match corresponding targets to located targets in world coordinates. In addition, we also consider scenarios where occlusion or limited field of view (FOV) occurs. The proposed method utilizes target images to the greatest extent. Our experimental results show that the proposed method obtains a more accurate result in targets localization compared with the camera discard scheme, and saves significant amounts of energy compared with other feature matching schemes.	experiment;field of view in video games;glossary of computer graphics;hidden surface determination;internationalization and localization	Wei Li;Wei Zhang	2013	Frontiers of Computer Science	10.1007/s11704-013-2197-0	computer vision;telecommunications;computer science	Robotics	49.41124607758588	-46.264126206462166	133999
a2bb90f3254f9ea89b5a1f22c4608f5b882c6a53	real-time multi-step view reconstruction for a virtual teleconference system	signal image and speech processing;teleconference;real time;quantum information technology spintronics;real time implementation;3d vision;reconstruction algorithm;virtual;view reconstruction	We propose a real-time multi-step view reconstruction algorithm and we tune its implementation to a virtual teleconference application. Theoretical motivations and practical implementation issues of the algorithm are detailed. The proposed algorithm can be used to reconstruct novel views at arbitrary poses (position and orientation) in a way that is geometrically valid. The algorithm is applied to a virtual teleconference system. In this system, we show that it can provide high-quality nearby virtual views that are comparable with the real perceived view. We experimentally show that, due to the modular approach, a real-time implementation is feasible. Finally, it is proved that it is possible to seamlessly integrate the proposed view reconstruction approach with other parts of the teleconference system. This integration can speed up the virtual view reconstruction.	algorithm;experiment;pose (computer vision);real-time clock;real-time computing;real-time transcription	Bang Jun Lei;Emile A. Hendriks	2002	EURASIP J. Adv. Sig. Proc.	10.1155/S1110865702206071	computer vision;simulation;teleconference;computer science;computer graphics (images)	Visualization	53.42449452177162	-46.617465256057514	134071
fee7c6493cf2659cb590baf0963006f0f4653f13	comparison of a new qualifier method for multiple object tracking in robocup 2d simulation league		In this document two methods for a multiple object tracking problem are tested and compared in a 2D environment with quantisied vision considering the tracking problem as a constraint satisfaction problem as a general approach. The first method is a qualifier method which uses three probabilistic models (identity, distance, and movement direction) to compute the belief of the path of a given object considering the path a Markov process. The second method are particle filters with penalised predictions which expands the belief of a given objects in order to get the best match for it. Each method was tested in two situations. In the first situation the observer was static in a fixed position while the second situation involves a dynamic observer. The methods obtained an almost perfect result of 98 % of correct matches for the first situation and achieved a result of nearly 78 % of correct matches in the second situation.	simulation	Nelson I. González;Leonardo Garrido	2014		10.1007/978-3-319-12027-0_58	simulation	Robotics	53.13706702501692	-39.75670248538517	134109
6e65861352ae30ee61038a7536c3bcb31950f612	incremental object learning and robust tracking of multiple objects from rgb-d point set data	3 d point set registration;tempo spatial data association;robot vision;multiple objects tracking;incremental learning;gaussian mixture models;visual tracking;rgb d point set data	In this paper, we propose a novel model-free approach for tracking multiple objects from RGB-D point set data. This study aims to achieve the robust tracking of arbitrary objects against dynamic interaction cases in real-time. In order to represent an object without prior knowledge, the probability density of each object is represented by Gaussian mixture models (GMM) with a tempo-spatial topological graph (TSTG). A flexible object model is incrementally updated in the pro-posed tracking framework, where each RGB-D point is identified to be involved in each object at each time step. Furthermore, the proposed method allows the creation of robust temporal associations among multiple updated objects during split, complete occlusion, partial occlusion, and multiple contacts dynamic interaction cases. The performance of the method was examined in terms of the tracking accuracy and computational efficiency by various experiments, achieving over 97% accuracy with five frames per second computation time. The limitations of the method were also empirically investigated in terms of the size of the points and the movement speed of objects. Crown Copyright 2013 Published by Elsevier Inc. All rights reserved.	algorithm;computation;crown group;experiment;floating point systems;google map maker;graphics processing unit;level of detail;mixture model;real-time clock;real-time computing;real-time locating system;robot;sampling (signal processing);time complexity;topological graph;workspace	Seong-Yong Koo;Dongheui Lee;Dong-Soo Kwon	2014	J. Visual Communication and Image Representation	10.1016/j.jvcir.2013.03.020	computer vision;eye tracking;computer science;machine learning;pattern recognition;mixture model;mathematics	Vision	50.60916364964353	-46.121503064616334	134191
33cbca0bc6fbf28165b47320e5e10247af5787f5	recursive kernel density estimation for modeling the background and segmenting moving objects	g400 computer science;video surveillance;local texture correlation operator recursive kernel density estimation background moving objects segmenting moving objects video sequence video surveillance traffic monitoring gesture recognition human machine interface density functions mean shift method gaussian distributions thresholding mechanism scene segmentation background subtraction;image segmentation;kernel estimation adaptation models approximation methods density functional theory gaussian distribution computational modeling;video surveillance gaussian distribution gesture recognition image segmentation image sequences object detection;recursive kernel density estimation video segmentation background modeling;gesture recognition;gaussian distribution;object detection;image sequences	Identifying moving objects in a video sequence is a fundamental and critical task in video surveillance, traffic monitoring, and gesture recognition in human-machine interface. In this paper, we present a novel recursive Kernel Density Estimation based background modeling method. First, local maximum in the density functions is recursively approximated using a mean shift method. Second, components and parameters in the mixture Gaussian distributions can be selected adaptively via a proposed thresholding mechanism, and finally converge to a stable background distribution model. In the scene segmentation, foreground is firstly separated by simple background subtraction approach. And then a local texture correlation operator is introduced to fill the vacancies and remove the fractional false foreground regions so as to obtain a better video segmentation quality. Experiments conducted on synthetic and video data demonstrate the superior performance of the proposed algorithms.	approximation algorithm;background subtraction;closed-circuit television;converge;gesture recognition;kernel density estimation;maxima and minima;mean shift;recursion (computer science);synthetic data;thresholding (image processing);user interface;website monitoring	Qingsong Zhu;Ling Shao;Qi Li;Yaoqin Xie	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6637956	normal distribution;computer vision;background subtraction;computer science;machine learning;video tracking;pattern recognition;gesture recognition;image segmentation;statistics	Vision	44.24649976729606	-49.78135628168865	134267
491fbb2272d32eacd241b034d4cc5cb68c4b39a8	vehicle speed detecting app		I hereby declare that the work submitted for assessment is original and my own work, except where acknowledged in the submission. Abstract This report presents the measurement of vehicular speed using a smartphone camera. The speed measurement is accomplished by detecting the position of the vehicle on a camera frame using OpenCV's library LBP cascade classifier, the displacement of the detected vehicle with time is used to compute the speed. Conversion coefficient is determined to map the pixel displacement to actual vehicle distance. The speeds measured are proportional to the ground truth speeds. Acknowledgements The completion of this report has been possible only because of the support received from numerous sources which deserve special mention. I would like to begin by thanking God for the grace to excel through the Master's program. A big thank you to every member of my family: XXXXXXXX are greatly appreciated for financially sponsoring the MSc program. XXXXXXXX are acknowledged for being very supportive during the course of this Msc program. XXXXXXXX deserves an inexpressible gratitude for his benevolence and encouragement. I thank XXXXXXXX and XXXXXXXX my supervisor and second maker respectively for their useful suggestions that have been essential to the completion of this project. XXXXXXXX the Program Leader is also acknowledged for her high sense of duty and support to all Master's students assigned to her. My colleagues in the Master's program and in my Department have been good friends and are appreciated.	cascading classifiers;coefficient;displacement mapping;ground truth;opencv;pixel;sensor;smartphone	Itoro Ikon	2017	CoRR		embedded system;computer vision;simulation	HCI	42.456075276853234	-41.010685000677576	134338
0da829018b040a555a009d7e2c2980111377d99f	local shape context based real-time endpoint body part detection and identification from depth images	shape image edge detection humans context three dimensional displays head foot;local shape context;image motion analysis;time of flight;depth image;video signal processing;edge detection;learning model;interest points;real time;human robot interaction;reference point;endpoint body part;human motion capture;human motion;gesture recognition real time endpoint body part detection local shape context descriptor shape features time of flight sensor human robot interaction edge images video processing;gesture recognition local shape context endpoint body part depth image human motion capture;video signal processing edge detection gesture recognition image motion analysis;high speed;gesture recognition	For many human-robot interaction applications, accurate localization of the human, and in particular the endpoints such as the head, hands and feet, is crucial. In this paper, we propose a new Local Shape Context Descriptor specifically for describing the shape features of the endpoint body parts. The descriptor is computed from edge images obtained from depth data generated by a time-of-flight sensor. The proposed descriptor encodes the distance from a reference point to the nearest edges in uniformly sampled radial directions. Based on this descriptor, a new type of interest point is defined, and a hierarchical algorithm for searching good interest points is developed. The interest points are then classified as head, feet, hands and others based on learned models. The system is computationally efficient, and capable of handling large variations in translation, rotation, scaling and deformation of the body parts. The system is tested using videos containing a variety of motions from a publicly available dataset, and is shown to be capable of detecting and identifying endpoint body parts accurately at very high speed.	algorithm;algorithmic efficiency;communication endpoint;feature (computer vision);gesture recognition;human–robot interaction;image scaling;interest point detection;motion capture;precision and recall;radial (radio);real-time clock;sensor;shape context	Zhenning Li;Dana Kulic	2011	2011 Canadian Conference on Computer and Robot Vision	10.1109/CRV.2011.36	computer vision;time of flight;simulation;edge detection;computer science;gesture recognition;computer graphics (images)	Vision	41.178299729932554	-48.89012774935798	134832
0ca263f3462a3b12ca7bc0600995e3958eac5ecc	unsupervised object pose classification from short video sequences	unsupervised learning;topology;sequences;statistical manifold;image processing;probability density;probability density function;low resolution;hand held;classification;collection;accuracy;object manipulation;video images;euclidean space;algorithms;autonomous navigation;probability density functions;dimensional reduction;cameras	We address the problem of recognizing the pose of an object category from video sequences capturing the object under small camera movements. This scenario is relevant in applications such as robotic object manipulation or autonomous navigation. We introduce a new algorithm where we model an object category as a collection of non parametric probability densities capturing appearance and geometrical variability within a small area of the viewing sphere for different object instances. By regarding the set of frames of the video as realizations of such probability densities, we cast the problem of object pose classification as the one of matching (i.e., comparing information divergence of) probably density functions in testing and training. Our work can be also related to statistical manifold learning. By performing dimensionality reduction on the manifold of learned PDFs, we show that the embedding in the 3D Euclidean space yield meaningful trajectories which can be parameterized by the pose coordinates on the viewing sphere, this enables an unsupervised learning procedure for pose classification. Our experimental results on both synthesized and real world data show promising results toward the goal of accurate and efficient pose classification of object categories from video sequences.	algorithm;autonomous robot;cluster analysis;emoticon;image resolution;instance (computer science);kernel density estimation;mobile device;nonlinear dimensionality reduction;portable document format;pyramid (geometry);spatial variability;statistical manifold;unsupervised learning;window function	Liang Mei;Min Sun;Kevin M. Carter;Alfred O. Hero;Silvio Savarese	2009		10.5244/C.23.89	unsupervised learning;computer vision;probability density function;method;pose;object model;3d pose estimation;image processing;machine learning;pattern recognition;mathematics;statistics	Vision	46.045710587152776	-51.494678433720985	134970
9b59a44f9ce90b1dd4a6b07242f753dd17d3e25d	a versatile object tracking algorithm combining particle filter and generalised hough transform	histograms;global object state object tracking algorithm particle filter generalised hough transform low level observations gradient orientation pixel based description global description back projection map probability density;color;probability hough transforms image filtering object tracking particle filtering numerical methods;image color analysis;colour and edge features object tracking generalised hough transform particle filter;object tracking;transforms;robustness;particle filters;histograms transforms particle filters image color analysis object tracking robustness color	This paper introduces a new object tracking method which combines two algorithms working in parallel, and based on low-level observations (colour and gradient orientation): the Generalised Hough Transform, using a pixel-based description, and the Particle Filter, using a global description. The object model is updated by combining information from a back-projection map computed from the Generalised Hough Transform, providing for every pixel the degree to which it may belong to the object, and from the Particle Filter, providing a probability density on the global object state. The purpose of the proposed tracker is to make the most of the two algorithms, in terms of robustness to appearance variation like scaling, rotation, non-rigid deformation or illumination changes.	algorithm;generalised hough transform;gradient;high- and low-level;image scaling;mathematical optimization;particle filter;pixel	Antoine Tran;Antoine Manzanera	2015	2015 International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2015.7367106	hough transform;computer vision;particle filter;computer science;video tracking;pattern recognition;histogram;mathematics;statistics;robustness;computer graphics (images)	Vision	44.955096854295	-49.324583412290686	135071
4c34770efadc416bf558ba771816e32c7e832043	3-d human pose recovery using nonrigid point set registration and body part tracking of depth data		In this paper, we present a novel approach for recovering a 3-D pose from a single human body depth silhouette using nonrigid point set registration and body part tracking. In our method, a human body depth silhouette is presented as a set of 3-D points and matched to another set of 3-D points using point correspondences. To recognize and maintain body part labels, we initialize the first set of points to corresponding human body parts, resulting in a body part-labeled map. Then, we transform the points to a sequential set of points based on point correspondences determined by nonrigid point set registration. After point registration, we utilize the information from tracked body part labels and registered points to create a human skeleton model. A 3-D human pose gets recovered by mapping joint information from the skeleton model to a 3-D synthetic human model. Quantitative and qualitative evaluation results on synthetic and real data show that complex human poses can be recovered more reliably with lower errors compared to other conventional techniques for 3-D pose recovery.	algorithm;collaborative product development;correspondence problem;effective method;point set registration;synthetic data;synthetic intelligence	Dong-Luong Dinh;Sungyoung Lee;Tae-Seong Kim	2015	Multimedia Systems	10.1007/s00530-015-0497-y	computer vision;computer graphics (images)	Vision	49.397225170390826	-48.85497453506958	135127
36ae9e1dfd9e6be30bb8168a2e99385037cd05a3	adaptive unstructured road detection using close range stereo vision	adaptive unstructured road detection changing light conditions long range road region unstructured road detection algorithm urban highways urban roads traversable regions off road domain unstructured domain road region detection close range stereo vision;roads image color analysis training adaptation models classification algorithms cameras libraries;roads;stereo image processing;traffic engineering computing;visual perception;learning artificial intelligence;near to far learning road detection stereo vision unstructured roads traversibility continuous learning;object detection;visual perception learning artificial intelligence object detection roads stereo image processing traffic engineering computing	Detection of road regions is not a trivial problem especially in unstructured and/or off-road domains since traversable regions of these environments do not have common properties unlike urban roads or highways. In this paper a novel unstructured road detection algorithm that can continuously learn the road region is proposed. The algorithm gathers close-range stereovision data and uses this information to estimate the long-range road region. The experiments show that the algorithm gives satisfactory results even under changing light conditions.	algorithm;experiment;graphics processing unit;point cloud;stereopsis;supervised learning;thresholding (image processing)	Kadri Bugra Ozutemiz;Akif Hacinecipoglu;A. Bugra Koku;E. Ilhan Konukseven	2013	2013 9th Asian Control Conference (ASCC)	10.1109/ASCC.2013.6606352	computer vision;simulation;geography;remote sensing	Robotics	42.89320977350459	-43.386338183352535	135144
5a8bb374cf248ffbafd9580d556a1ba8a72ac466	on pattern selection for laparoscope calibration	eye;surgery;augmented reality;calibration;cameras	Camera calibration is a key requirement for augmented reality in surgery. Calibration of laparoscopes provides two challenges that are not sufficiently addressed in the literature. In the case of stereo laparoscopes the small distance (less than 5mm) between the channels means that the calibration pattern is an order of magnitude more distant than the stereo separation. For laparoscopes in general, if an external tracking system is used, hand-eye calibration is difficult due to the long length of the laparoscope.	augmented reality;camera resectioning;feature detection (computer vision);feature detection (web development);refinement (computing);sensor;template matching;tracking system	Stephen A. Thompson;Yannic Meuer;Philip J. Edwards;João Ramalhinho;Maria Ruxandra Robu;Danail Stoyanov;Sébastien Ourselin;Brian R. Davidson;David J. Hawkes;Matthew J. Clarkson	2017		10.1117/12.2253717	computer vision;augmented reality;calibration;simulation;computer graphics (images)	Vision	52.61316020481421	-44.41804487628953	135164
d1253c019d35bafd92ef2e2e432e527a1f7819be	extracting moving objects from a video by sequential background detection employing a local correlation map	moving object;sequential background detection;filtering;mobile camera;camera motion estimation;video signal processing feature extraction image registration median filters motion estimation;median filter;video signal processing;optical filters;moving object extraction;background extraction;motion vector estimation moving object extraction sequential background detection local correlation map video image sequence background subtraction temporal median filter camera motion estimation registration algorithm background extraction;motion estimation;temporal median filter;local correlation map;camera motion;video image sequence;local correlation map object detection mobile camera background subtraction temporal median filter;motion vector;feature extraction;light intensity;image registration;pixel;image sequence;background subtraction;mobile communication;object detection cameras humans motion estimation roads legged locomotion optical filters image sequences streaming media image motion analysis;humans;correlation;motion vector estimation;registration algorithm;cameras;object detection;median filters	This paper describes a technique for extracting moving objects from a video image sequence taken by a fixed or slowly moving camera by background subtraction. The background subtraction method is effective for extracting moving objects from a video provided by a fixed camera. But the latest background image should be employed for the subtraction in order not to be influenced by the light intensity change. A temporal median filter is proposed in this paper which detects the latest background images sequentially. From a video image stream provided by a slowly moving camera, the camera motion is estimated using a registration algorithm and the temporal median filter is applied to the common image area among a set of successive image frames to extract the background. In order that the camera motion estimation may become more exact effective, local correlation maps are calculated which can exclude outliers in estimated motion vectors. The technique was applied to the video images obtained from a hand-held camera and those taken from a camera set at the front seat of a car, and satisfactory results were obtained.	algorithm;background subtraction;map;median filter;mobile device;motion estimation	Makoto Miyoshi;Joo Kooi Tan;Seiji Ishikawa	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811817	filter;median filter;computer vision;camera auto-calibration;match moving;camera resectioning;background subtraction;mobile telephony;feature extraction;computer science;image registration;video tracking;pattern recognition;motion estimation;image subtraction;optical filter;mathematics;motion field;motion compensation;correlation;pixel;pinhole camera model;computer graphics (images)	Vision	44.66235314396068	-45.25968887614506	135183
93108f1548e8766621565bdb780455023349d2b2	facial expression synthesis based on motion patterns learned from face database	databases;eyebrows;hierarchical clustering;pattern clustering;parameterized synthesis facial expression synthesis motion pattern face database face to face human computer communication facial region hierarchical clustering algorithm;image motion analysis;human computer interaction;face databases clustering algorithms cloning correlation eyebrows facial animation;face database;visual databases face recognition human computer interaction image motion analysis pattern clustering;face to face human computer communication;cloning;hierarchical clustering algorithm;face recognition;facial animation;face modeling;clustering algorithms;face;parameterized synthesis;correlation;facial expression;motion pattern;face to face;facial expression synthesis;motion pattern facial expression synthesis;facial region;visual databases	Facial expression is the core function in face-to-face human-computer communication. In order to improve the accuracy and variety of the synthesized facial expressions, we propose a facial expression synthesis approach based on motion patterns learned from face database. We first define a set of partial facial regions: eyebrow, eye-lid, eyeball, upper lip, bottom lip and corner lip. For each region, we use a hierarchical clustering algorithm to learn the motion patterns from the face database. Then the patterns are used for parameterized synthesis of facial expression on the target face models. The experimental results show the effectiveness of the proposed approach.	algorithm;cluster analysis;hierarchical clustering	Jia Jia;Shen Zhang;Lianhong Cai	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653914	facial recognition system;face;computer vision;speech recognition;computer facial animation;computer science;pattern recognition;cloning;hierarchical clustering;cluster analysis;facial expression;correlation;face hallucination	Vision	39.47700556790927	-49.86896551052739	135263
b7fdcc094ad2d0292ae13cbc3cc03aff406c0bfb	automatic video-based human motion analyzer for consumer surveillance system	surveillance video content analysis semantic tracking realtime human behavior analysis calibration;equipment usage functions;background modeling;video surveillance;image motion analysis;surveillance;surveillance system;event based level automatic video based human motion analyzer consumer surveillance system video analysis techniques automatic low cost video surveillance home entrance control equipment usage functions human behavior semantic analysis monocular surveillance video trajectory estimation human body modeling 3d reconstruction scheme object based level performing trajectory estimation posture classification;video analysis;video analysis techniques;3d reconstruction scheme;surveillance video;scene reconstruction;human behavior;human body model;video surveillance image motion analysis image reconstruction image sequences;content analysis;hidden markov models;trajectory;continuous improvement;image reconstruction;human motion;monocular surveillance video;pixel;event based level;consumer surveillance system;semantic;video content analysis;automatic low cost video surveillance;object based level performing trajectory estimation;humans;near real time;camera calibration;home entrance control;scene understanding;human behavior analysis;calibration;human activity;cameras;tracking;realtime;human behavior semantic analysis;semantic analysis;humans motion analysis video surveillance cameras layout continuous improvement domestic safety safety devices automatic control video sequences;level 1;automatic video based human motion analyzer;posture classification;trajectory estimation;image sequences;human body modeling	With the continuous improvements in video-analysis techniques, automatic low-cost video surveillance gradually emerges for consumer applications. Video surveillance can contribute to the safety of people in the home and ease control of home-entrance and equipment-usage functions. In this paper, we study a flexible framework for semantic analysis of human behavior from a monocular surveillance video, captured by a consumer camera. Successful trajectory estimation and human-body modeling facilitate the semantic analysis of human activities and events in video sequences. An additional contribution is the introduction of a 3-D reconstruction scheme for scene understanding, so that the actions of persons can be analyzed from different views. The total framework consists of four processing levels: (1) a preprocessing level including background modeling and multiple-person detection, (2) an object-based level performing trajectory estimation and posture classification, (3) an event-based level for semantic analysis, and (4) a visualization level including camera calibration and 3-D scene reconstruction. Our proposed framework was evaluated and has shown its good quality (86% accuracy of posture classification and 90% for events) and effectiveness, as it achieves a near real-time performance (6-8 frames/second).	camera resectioning;closed-circuit television;kinesiology;object-based language;poor posture;preprocessor;real-time clock;real-time computing	Weilun Lao;Jungong Han;Peter H. N. de With	2009	IEEE Transactions on Consumer Electronics	10.1109/TCE.2009.5174427	iterative reconstruction;computer vision;calibration;camera resectioning;simulation;content analysis;computer science;trajectory;video tracking;tracking;multimedia;human behavior;pixel	Vision	44.37143554240793	-45.58241578518182	135354
d1e7ba2e7680c791347832b83736d173b62fe8ea	true scaled 6 dof egocentric localisation with monocular wearable systems		In this work we present a novel approach to obtain scaled odometry and map estimates when performing monocular SLAM with wearable cameras. After proving first that the oscillation of the body during walking can be observed in the odometric estimate from a monocular SLAM algorithm, we develop a method to estimate the walking speed from the frequency of this oscillation. Having the real walking speed, a scale factor can be dynamically computed to obtain a true scaled estimate of the map and visual odometry, avoiding scale drift on long term trajectories. Although the algorithm requires the person to be walking in order to estimate the scale, the experiments, carried out in outdoor and indoor environments and with different types of cameras, show that our method is reliable and robust to challenging situations like stops, changes in pace or stairs, and provides a significant improvement with respect to the initial unscaled estimate. It also outperforms state-of-the-art solutions to correct the scale drift in monocular SLAM, giving in addition the absolute scale of the trajectory and the 3D observed scene.	algorithm;experiment;simultaneous localization and mapping;visual odometry;wearable computer	Daniel Gutiérrez-Gómez;Josechu J. Guerrero	2016	Image Vision Comput.	10.1016/j.imavis.2016.05.015	computer vision;simulation	Robotics	53.6582909152978	-40.48524630563115	135443
841fec90f94689ffd1db851a7c031bb6d48eec19	joint region tracking with switching hypothesized measurements	tracking filters image sequences hidden feature removal kalman filters state space methods;kalman filtering;recursive estimation;joint region tracking;state space methods;recursive estimation joint region tracking switching hypothesized measurements model multimodal probability distribution occlusion hypotheses;hidden feature removal;image sequence analysis;dynamical processes;kalman filters;tracking filters;multiple objectives;switching hypothesized measurements model;occlusion hypotheses;probability distribution;target tracking time measurement probability distribution state space methods state estimation history superluminescent diodes switches filters recursive estimation;multimodal probability distribution;image sequences	This paper proposes a switching hypothesized measurements (SHM) model supporting multimodal probability distributions and presents the application of the model in handling potential variability in visual environments when tracking multiple objects jointly. For a set of occlusion hypotheses, a frame is measured once under each hypothesis, resulting in a set of measurements at each time instant. A computationally efficient SHM filter is derived for online joint region tracking. Both occlusion relationships and states of the objects are recursively estimated from the history of hypothesized measurements. The reference image is updated adaptively to deal with appearance changes of the objects. The SHM model is generally applicable to various dynamic processes with multiple alternative measurement methods.	algorithmic efficiency;heart rate variability;multimodal interaction;recursion;super high material cd	Yang Wang;Tele Tan;Kia-Fock Loe	2003		10.1109/ICCV.2003.1238316	kalman filter;computer vision;computer science;control theory;mathematics;statistics	Vision	45.857925939599546	-47.702127085287934	135466
da693b57d997b5dc21adb63ff2505e7b168f02ab	bayesian covariance tracking with adaptive feature selection		Effective appearance models are one important factor for robust object tracking. In this paper, a more elaborate object representation model via a simul- taneous online feature selection and feature fusion algorithm is proposed, in which extended variance ratio is used to select the most discriminative power features, and thereby account for appearance model using region covariance de- scriptor which takes into account feature correlation information during tracking. Fusing all selected features, we get a more discriminative appearance model. Furthermore, our simultaneous online feature selection and feature fusion meth- od is integrated into particle filter framework for robust tracking. Experimental results show that this proposed method is robust in heavy occlusions scenes and is able to handle variations in illumination and scale.	feature selection	Dejun Wang;Lin Li;Wei Liu;Weiping Sun;Shengsheng Yu	2014		10.1007/978-3-319-12436-0_57	computer vision;machine learning;pattern recognition;feature	Vision	42.831359333584565	-49.16895091182062	135588
2634798bbfdfb093c6d70f283d5496d952a2c731	nonlinear refinement of structure from motion reconstruction by taking advantage of a partial knowledge of the environment	optimisation;3d object tracking;three dimensional displays cameras cost function solid modeling robustness barium image reconstruction;initial sfm reconstruction;real sequences;optimization process;motion estimation;motion reconstruction;synthetic sequences;camera localization;nonlinear structure refinement;solid modelling cameras image reconstruction image sequences motion estimation natural scenes object tracking optimisation;outdoor localization application nonlinear structure refinement motion reconstruction camera localization geometric 3d model initial sfm reconstruction optimization process real sequences synthetic sequences 3d object tracking;camera motion;3d model;outdoor localization application;image reconstruction;object tracking;geometric 3d model;structure from motion;cameras;natural scenes;solid modelling;image sequences	We address the challenging issue of camera localization in a partially known environment, i.e. for which a geometric 3D model that covers only a part of the observed scene is available. When this scene is static, both known and unknown parts of the environment provide constraints on the camera motion. This paper proposes a nonlinear refinement process of an initial SfM reconstruction that takes advantage of these two types of constraints. Compare to those that exploit only the model constraints i.e. the known part of the scene, including the unknown part of the environment in the optimization process yields a faster, more accurate and robust refinement. It also presents a much larger convergence basin. This paper will demonstrate these statements on varied synthetic and real sequences for both 3D object tracking and outdoor localization applications.	algorithm;augmented reality;cluster analysis;constraint (mathematics);image segmentation;internationalization and localization;mathematical optimization;nonlinear system;online and offline;ray tracing (graphics);refinement (computing);structure from motion;synthetic intelligence	Mohamed Tamaazousti;Vincent Gay-Bellile;Sylvie Naudet-Collette;Steve Bourgeois;Michel Dhome	2011	CVPR 2011	10.1109/CVPR.2011.5995358	iterative reconstruction;computer vision;structure from motion;simulation;computer science;video tracking;motion estimation;computer graphics (images)	Vision	52.59387779229298	-49.715944988647344	135704
2be78919112f0449ef4ee246373954d7368805fc	integrating visual and range data for road detection	image fusion;image classification;optical radar;roads;image registration;highways visual data range data road detection drivable road surfaces single image lidar data aligned image 3d points ground plane estimation obstacle points apriori road models nonroad appearance models markov random field energy function urban roads;roads image classification image fusion image registration markov processes optical radar;markov processes;graph cuts road detection data fusion markov random field	This paper presents a new method for detecting drivable road surfaces in a single image. The method takes advantage of range and visual information so that reliable results are achieved. Specifically, given LIDAR data and an aligned image, it first makes use of 3D points to estimate the ground plane and determine the horizon. Then, subsets of road and obstacle points are extracted from the 3D points based on the plane and LIDAR properties. The pixels registered to the extracted points are used to build apriori road and non-road appearance models. The road detection problem is further formulated using Markov random field whose energy function is defined based on the learned models. Constraints are also added on the energy function to place high confidence on the pixels that are registered to extracted 3D points. Extensive experiments on urban roads and highways show that our method is robust even in complicated environments.	apriori algorithm;autostereogram;experiment;markov chain;markov random field;mathematical optimization;pixel;sensor	Wenqi Huang;Xiaojin Gong;Jilin Liu	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738852	computer vision;contextual image classification;simulation;computer science;image registration;markov process;image fusion;statistics	Robotics	43.423527345710404	-45.06061926210778	135713
59fea9301d42123857b5f9108bbd9f305fc9b5f6	correlation-based facade parsing using shape grammar	optimisation;probability;facade parsing;probability energy correlation based facade parsing framework shape grammar super pixel based facade parsing framework top down shape grammar splitting bottom up information aggregation machine learning boundary estimation random walk optimization results correlation judging;probability grammars learning artificial intelligence optimisation;shape grammar;grammars;shape grammar facade parsing;grammar correlation shape estimation tiles histograms;learning artificial intelligence	With strong inference of hierarchical and repetitive structures, semantic information has been widely used in dealing with urban scenes. In this paper, we present a super-pixel-based facade parsing framework which combines the top-down shape grammar splitting with bottom-up information aggregation: machine learning forecasts prior classes, super-pixels improve compactness, and boundary estimation divides the splitting into two procedures - raw and fine, providing a reasonable initial guess for the latter to achieve better random walk optimization results. We also put forward the correlation judging between floors for the purpose of compromising freedom degree reduction with style variety and flexibility, which is also introduced as alignment constraint term to extend the probability energy. Experiments show that our method converges fast and achieves the state-of-the-art results for different styles. Further study on understanding and reconstruction is in progress of exploiting these results.	machine learning;mathematical optimization;parsing;pixel;top-down and bottom-up design	Runze Zhang;Ruiling Deng;Xin He;Gang Zeng;Rui Gan;Hongbin Zha	2013	2013 2nd IAPR Asian Conference on Pattern Recognition	10.1109/ACPR.2013.81	natural language processing;parser combinator;l-attributed grammar;computer science;machine learning;pattern recognition;stochastic grammar;grammar-based code;top-down parsing	Vision	45.03068989543177	-51.34142396728407	135741
6386a16f3890f60808bc9f8a358688a77be6eb1c	recovery of volumetric object descriptions from laser rangefinder images	bottom up;laser rangefinder;computer model;differential geometry;three dimensional	This paper describes a representation and computational model for deriving three dimensional, articulated volumetric descriptions of objects from laser rangefinder data. What differentiates this work from other approaches is that it is purely bottom-up, relying on general assumptions cast in terms of differential geometry.		Frank P. Ferrie;Jean Lagarde;Peter Whaite	1990		10.1007/BFb0014887	three-dimensional space;differential geometry;computer vision;top-down and bottom-up design;mathematics;computer graphics (images)	Robotics	53.36136284087933	-51.697325133524075	135918
7783e8810c04e8c52c29845c228513c56ee8ce9c	real time localization, tracking and recognition of vehicle license plate		Real time license plate localization, tracking and recognition are reasonably tackled problems with many successful solutions. Though most of these solutions are plausibly fast and efficient, however, almost all of the existing real time systems either deal with only a single problem at a time; detection, tracking, recognition or they are not efficient enough to work well for low quality surveillance videos. The aim of this paper is to address all three tasks for low quality videos in real time. A novel approach is proposed for efficient localization of license plate in video sequence and slightly adapted existing techniques have been applied for tracking and recognition. The implemented system is intelligent enough to automatically adjust for varying camera distances and diverse lighting conditions.	artificial intelligence;closed-circuit television;gaussian blur;image resolution;streaming media	Azeem Shahzad;Muhammad Fraz;Muhammad A. Elahi;M. Saquib Sarfraz	2011			artificial intelligence;computer vision;license;computer science	AI	42.411880844411286	-46.290829906086834	135967
254591dadb82e2d29bcc59abc109773b5c92f9cc	recent advances in motion interpretation based on image sequences	image sequences layout data mining books natural languages image analysis image sequence analysis crops multispectral imaging wind speed;image sequence analysis;natural languages;layout;data mining;books;wind speed;image sequence;crops;image analysis;multispectral imaging;image sequences	"""ION IN MOTION INTERPRETATION Natural language provides the means to describe a complex development at various levels of abstraction. A hierarchy of abstractions from image sequence data towards natural language notions is employed to organize the references. In the following we assume that only gradual changes occur between consecutive frames of a sequence. Otherwise, just the kind of scene—specific knowledge which we desire to extract from the observations would already be required to bridge the gaps in the input data. Low—Level Symbolic Descriptions A first level of abstraction is the localization of greyvalue variations characteristic enough to be found again in a subsequent frame. Examples are zero-crossings — i.e. locations of large greyvalue gradients (6) —, .heu±'istic """"points of interest"""" (7, 8), corner points (9,10,11), edge segsients (12), or primitive regions including those used for cross-"""	emoticon;gradient;high-level programming language;natural language;point of interest;principle of abstraction	Hans-Hellmut Nagel	1982	ICASSP '82. IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.1982.1171499	wind speed;crop;layout;multispectral image;computer vision;image analysis;computer science;digital image processing;natural language;automatic image annotation;information retrieval;digital image	Robotics	39.22106488791698	-49.99410916526581	135991
b781f83c0b5621aabd07d4730eb3ae3eca8a1401	linear markers for robot navigation with panoramic vision	robot sensing systems;binary sequence;mobile robot;low resolution;robot navigation;mobile robots;image sensors;computer vision;navigation;machine vision;field of view;pattern recognition;robustness;panoramic image;navigation system;robot vision systems navigation robot sensing systems mobile robots robustness image sensors cameras computer vision machine vision pattern recognition;robot vision systems;image sensor;cameras	A vision based navigation system is presented for determining a mobile robot's position and orientation using panoramic imagery. An omni-directional image sensor mounted on the robot is useful in obtaining a 360 field of view, permitting navigational markers from all sides to be viewed simultaneously. A robust marker-based system is presented using vertically positioned linear markers as landmarks. The markers consist of linearly encoded digital patterns, similar to a barcode but distinguishable with less pixels. A set of patterns are orthogonal from one another and are readily recognized with any continous section visible. With a vertically posed panoramic image sensor, these vertically mounted linear markers appear along radial lines. The panoramic image is pre-processed according to edge directions to find candidate regions which are spatially sampled into digital symbols. This extracted binary sequence is examined to determine if it belongs in the marker pattern set. This system is shown to be robust even with the low resolution of a panoramic sensor with 800x800 active pixels. Experiments are shown with synthetic imagery and with three real prototype systems.i	barcode;binary number;bitstream;column (database);distortion;experiment;image resolution;image sensor;mobile robot;optic axis of a crystal;pixel;prototype;pseudorandomness;radial (radio);robotic mapping;sampling (signal processing);synthetic intelligence	Mark Fiala	2004	First Canadian Conference on Computer and Robot Vision, 2004. Proceedings.	10.1109/CCCRV.2004.1301438	mobile robot;computer vision;simulation;machine vision;computer science;image sensor;computer graphics (images)	Robotics	52.10751003757538	-41.790477552091566	136062
f5a179041bd786dbcd5cfc378a372a3bb923132c	randomized decision bush: combining global shape parameters and local scalable descriptors for human body parts recognition	body parts recognition;randomized decision bush;classification;body parts recognition randomized decision bush clustering classification shape parameter scalable descriptor;scalable descriptor;pose estimation decision theory image classification pattern clustering;human shape randomized decision bush rdb global shape parameters local scalable descriptors human body part recognition human poses visual shapes gymnastic actions pose clustering body parts classification;shape training testing accuracy computer vision pattern recognition image recognition;clustering;shape parameter	This paper presents a novel method which combines global shape parameters and scalable local descriptors for accurate body parts recognition from a single depth image in real-time. Human poses are of extremely large variation in aspects of visual shapes, because human can take poses from daily activities to gymnastic actions. In order to cover wide-range of the human poses, the proposed algorithm employs a unified structure which combines pose clustering and body parts classification. We name the proposed method Randomized Decision Bush (RDB). Specifically, global shape parameters which can discriminate coarse level shapes are utilized for pose clustering while scalable local shape descriptors are employed for accurate classification. RDB splits the various human poses into multiple clusters which contain similar shapes of the poses. As a result, it provides robust clustering which enables fine level classification within the cluster. The experimental results show improvements on recognizing body parts due to the pose clustering and classification with scalable local descriptors. Additionally, we significantly reduce the complexity of training a large number of human shapes.	amiga rigid disk block;cluster analysis;generalised hough transform;randomized algorithm;real-time clock;scalability;shape analysis (digital geometry)	ByungIn Yoo;Wonjun Kim;Jae-Joon Han;Changkyu Choi;Du-Sik Park;Junmo Kim	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025312	computer vision;biological classification;computer science;machine learning;pattern recognition;mathematics;cluster analysis;shape parameter;statistics	Vision	40.29132849404606	-50.22081113619606	136098
5b6593a6497868a0d19312952d2b753232414c23	face recognition by 3d registration for the visually impaired using a rgb-d sensor		To help visually impaired people recognize people in their daily life, a 3D face feature registration approach is proposed with a RGB-D sensor. Compared to 2D face recognition methods, 3D data based approaches are more robust to the influence of face orientations and illumination changes. Different from most 3D data based methods, we employ a one-step ICP registration approach that is much less time consuming. The error tolerance of the 3D registration approach is analyzed with various error levels in 3D measurements. The method is tested with a Kinect sensor, by analyzing both the angular and distance errors to recognition performance. A number of other potential benefits in using 3D face data are also discussed, such as RGB image rectification, multiple-view face integration, and facial expression modeling, all useful for social interactions of visually impaired people with others.	angularjs;error-tolerant design;facial recognition system;feature data;ibm notes;illumination (image);image rectification;image registration;interaction;kinect;real-time clock;real-time locating system;sensor	Wei Li;Xudong Li;Martin Goldberg;Zhigang Zhu	2014		10.1007/978-3-319-16199-0_53	computer vision	Vision	51.21678530781911	-43.907655869342406	136270
fd1d35eaec856ed25299dbbe62dc265c511b1fd3	an approach to motion vehicle detection in complex factors over highway surveillance video	eigenvalues and eigenfunctions;environmental factors;fast constrained delaunay triangulation algorithm video based vehicle detection complex factors multi feature eigenvector;video surveillance automated highways eigenvalues and eigenfunctions environmental factors feature extraction interactive systems mesh generation motion estimation principal component analysis support vector machines;background modeling;video surveillance;cdt;support vector machines;intelligent transport system;surveillance;intelligent transportation systems;complex factors;automated highways;vehicle detection;vehicle detection road transportation surveillance vehicles automated highways intelligent transportation systems environmental factors principal component analysis support vector machines support vector machine classification;traffic control;motion estimation;background estimation;surveillance video;eigenvector;real video sequence;motion vehicle detection;its;fast constrained delaunay triangulation algorithm;complex environmental factors motion vehicle detection surveillance video automated traffic surveillance systems its intelligent transportation systems background estimation multi feature extraction constrained delaunay triangulation cdt eigenvector principal component analysis pca svm support vector machine real video sequence;feature extraction;principal component analysis;constrained delaunay triangulation;support vector machine classification;traffic surveillance;svm;video based vehicle detection;vehicles;complex environmental factors;support vector machine;automated traffic surveillance systems;multi feature extraction;road transportation;mesh generation;interactive systems;pca;eigenvectors;environmental factor;model theory;multi feature eigenvector	Automated traffic surveillance systems are widely used in intelligent transportation systems (ITS). However, the accuracy of video-based Vehicle Detection is heavily affected by complex environmental factors such as shadows, rain, illumination and glare. This paper introduces an approach to motion vehicle detection in complex condition over highway surveillance video. The framework is composed of two parts: background estimation and multi-feature extraction. A fast constrained Delaunay triangulation (CDT) algorithm based on constrained-edge priority is presented instead of complicated segmentation algorithms. We present a background block update modeling theory based on triangulation to estimate background self-adaptively. Consequently, we can get the difference between the current frame and the background model. After extracting features in triangular candidates, multi-feature eigenvector is created for each vehicle with Principal Component Analysis (PCA). We design a classifier to classify triangular candidate as a part of a real vehicle or not by Support Vector Machine (SVM). And then, a parallelogram is used to represent the vehicle's shape robustly. Finally, experiments using real video sequence are performed to verify the method proposed for complex environmental factors.	adaptive filter;algorithm;closed-circuit television;constrained delaunay triangulation;eclipse;experiment;feature extraction;principal component analysis;robustness (computer science);support vector machine	Hao Sheng;Chao Li;Qi Wei;Zhang Xiong	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.43	support vector machine;computer vision;simulation;computer science;machine learning	Robotics	42.06001193136376	-44.888450071705485	136298
578447591138d6d59753d0eec995ff8a4c2aee6b	recording gaze trajectory of wheelchair users by a spherical camera		Wheelchairs are widely used in the facilities of rehabilitation. In this paper, we propose a method of recording the gaze trajectory of wheelchair users by using a spherical camera mounted on the wheelchairs. A spherical camera has a full field of view and can observe the entire surrounding scenes. First, the gaze point of a user sitting on a wheelchair is estimated from the corneal reflection image observed by a wearable eye camera. Then, the gaze point is mapped onto the full-view image captured by the spherical camera via feature matching. Since it is not guaranteed that the gaze point in an eye image is a distinctive feature point, the matching of a gaze point between these two images cannot be carried out directly. To cope with this problem, we use a coarse-to-fine approach, in which, first, distinctive feature points are used to estimate the relative orientation between the eye camera and the spherical camera, and then, the estimated relative orientation matrix is used to determine the location of gaze points. The effectiveness of the proposed method is shown by real-world experimental results.	ability to sit question;intention - mental process;interface device component;matching;pose (computer vision);rehabilitation therapy;robot (device);wearable computer;mapped;wheelchair	Shigang Li;Tatsuya Fujiura;Isao Nakanishi	2017	2017 International Conference on Rehabilitation Robotics (ICORR)	10.1109/ICORR.2017.8009368	camera resectioning;gaze;camera auto-calibration;wheelchair;mobile robot;wearable computer;artificial intelligence;trajectory;computer vision;computer science;field of view	Vision	48.90295217732619	-41.45284623122425	136303
d5184fbce6a2fffdd14e058f26ae62c7811b90e5	human tracking in video surveillance		In the last decade, due to the increase in terrorist activities and general social problems, providing security to citizens have become the top most priorities for almost all the nations and for the same, a very close watch is required to be kept in the areas that needs security. Keeping human watch 24x7 is not possible as we all know that humans can easily be distracted and a small distraction in very sensitive and highly secure area can lead to big loses. To overcome this human flaw in the area of monitoring, the concept of making monitoring automatic came into existence. Since, video surveillance has came in the market, researches have been taking place in order to make to more easy, accurate, fast and intelligent. The goal of visual surveillance is not only to put cameras in place of human eyes, but also to accomplish the entire surveillance task as automatically as possible. In one statement we can say that video surveillance is nothing but taking the video, identifying unwanted entities, tracking their actions, understanding their actions and raising an alarm. In this paper, we will be study the phases of the video surveillance system. We will see the 3 main methods of human detection. Further, we will see most salient region method for tracking and in this paper we propose a method of handling occlusion using velocity and direction information. Keywords—Image Separation, Video Surveillance System, Noise Removal, Human Detection, Handling occlusion.	closed-circuit television;entity;flaw hypothesis methodology;humans;velocity (software development)	Helly Patel;Mahesh P. Wankhade	2012		10.1007/978-3-642-31513-8_76	salient;computer security;change detection;optical flow;computer science	Vision	41.54781813178121	-44.141472436181104	136308
7f9ea9736f99114bfdd456f9cd7e728776f20e8d	visual tracking using sparse coding and earth mover's distance		An efficient iterative Earth Moveru0027s Distance (iEMD) algorithm for visual tracking is proposed in this paper. The Earth Moveru0027s Distance (EMD) is used as the similarity measure to search for the optimal template candidates in feature-spatial space in a video sequence. The local sparse representation is used as the appearance model for the iEMD tracker. The maximum-alignment-pooling method is used for constructing a sparse coding histogram which reduces the computational complexity of the EMD optimization. The template update algorithm based on the EMD is also presented. When the camera is mounted on a moving robot, e.g., a flying quadcopter, the camera could experience a sudden and rapid motion leading to large inter-frame movements. To ensure that the tracking algorithm converges, a gyro-aided extension of the iEMD tracker is presented, where synchronized gyroscope information is utilized to compensate for the rotation of the camera. The iEMD algorithmu0027s performance is evaluated using eight publicly available videos from the CVPR 2013 dataset. The performance of the iEMD algorithm is compared with eight state-of-the-art tracking algorithms based on relative percentage overlap. Experimental results show that the iEMD algorithm performs robustly in the presence of illumination variation and deformation. The robustness of this algorithm for large inter-frame displacements is also illustrated.	algorithm;bittorrent tracker;c++;cvpr;coefficient;computational complexity theory;convolutional neural network;discriminant;experiment;gyro;gradient descent;gyroscope;iterative method;laser tracker;mathematical model;mathematical optimization;neural coding;particle filter;robot;similarity measure;sparse approximation;sparse matrix;video tracking	Gang Yao;Ashwin Dani	2018	Front. Robotics and AI	10.3389/frobt.2018.00095	machine learning;robustness (computer science);earth mover's distance;artificial intelligence;computer science;similarity measure;computational complexity theory;computer vision;eye tracking;sparse approximation;active appearance model;histogram	Vision	48.036752488030906	-46.56120621934154	136352
c993e26d70c63efdf20d108fa5b3d71b93369c49	a multi-scale model integrating multiple features for vehicle detection	optimisation;video surveillance;occlusion multiscale model integrating multiple features vehicle detection traffic video surveillance systems traffic images probability model template matching local maximization operation multitemplate method multiscale method pose variance;probability;vehicles vehicle detection image color analysis cameras image edge detection training image resolution;image matching;road traffic;video surveillance image matching object detection optimisation probability road traffic road vehicles;proximity detectors;image analysis;traffic surveillance;optimization;video;object detection;road vehicles	In traffic video surveillance systems, vehicles with various distances from the camera have different sizes, resolutions, and angles in traffic images. The common multi-scale method, which scales one vehicle template or the input image for detecting vehicles with different sizes, may fail to detect vehicles with various distances from the camera due to the change of the resolution and angle. To deal with this problem, we have proposed a multi-scale model including multiple templates with different scales and features. Our method includes two steps: constructing the multi-scale model and its probability model, and detecting vehicles from traffic images. In the first step, the multi-scale model is constructed by using three templates T1, T2, T3 which represent vehicles with the short, medium, and long distance from the camera respectively. Each template contains one or some combination of sketch, texture, flatness, and color. In the second step, the three templates are applied for vehicle detection by using the template matching with local maximization operations. The main innovation of this paper is that the combination of multi-template and multi-scale method is applied to detect vehicles with various distances from the camera. To test our method, we have done several experiments on various traffic conditions. The experimental results show that our method effectively copes with vehicles with various distances from the camera and provides the detailed vehicle information after vehicle detection. Moreover, our method adapts to various weather conditions, slight pose variance, and slight occlusion.	autonomous car;closed-circuit television;expectation–maximization algorithm;experiment;multi-core processor;sensor;template matching;texture mapping	Ye Li;Fei-Yue Wang;Bo Li;Bin Tian;Fenghua Zhu;Gang Xiong;Kunfeng Wang	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728264	computer vision;simulation;geography;computer graphics (images)	Vision	42.90849594516748	-45.34513892846284	136360
28042d2a3d2dcea06911fce0cfed2676ffdebcac	a discriminating feature tracker for vision-based autonomous driving	robot movil;error medida;recursive estimation;automatic driving;poursuite optique;measurement error;image processing;nighttime driving;local roads;highways;uncertainty;autonomous system;shadows;path planning;conduccion automatica;procesamiento imagen;cracks;least squares approximation;mobile robots;conduite automatique;robotics;measurement uncertainty;data mining;traitement image;sistema autonomo;persecucion optica;erreur mesure;100 km h discriminating feature tracker vision based autonomous driving uncertainty road direction local roads highways nonideal road conditions sharp curves shadows cracks wet roads rain dark nighttime driving;100 km h;robot vision;robot mobile;optical tracking;feature extraction;dark;systeme autonome;rain;robotica;nonideal road conditions;robustness;vision based autonomous driving;vision artificielle;robotique;vehicles;feature extraction mobile robots path planning robot vision;road transportation;wet roads;artificial vision;sharp curves;measurement uncertainty rain data mining vehicles road transportation least squares approximation recursive estimation robustness robot vision systems cameras;robot vision systems;road direction;discriminating feature tracker;cameras;moving robot;vision artificial	A new vision-based technique for autonomous driving is described. This approach explicitly addresses and compensates for two forms of uncertainty: uncertainty about changes in road direction and uncertainty in the measurements of the road derived in each image. Autonomous driving has been demonstrated on both local roads and highways at speeds up to 100 km/h. The algorithm has performed well in the presence of non-ideal road conditions including gaps in the lane markers, sharp curves, shadows, cracks in the pavement, and wet roads. It has also performed well in rain, dark, and nighttime driving with headlights. >	autonomous car	Henry Schneiderman;Marilyn Nashman	1994	IEEE Trans. Robotics and Automation	10.1109/70.338531	mobile robot;computer vision;shadow;simulation;uncertainty;image processing;feature extraction;computer science;autonomous system;motion planning;robotics;least squares;statistics;measurement uncertainty;robustness;observational error	Robotics	52.392011692264134	-38.21654047601141	136396
c93265a4596713781bdf2f04e79545781b11aa28	an extensible framework for facial motion tracking	motion estimation emotion recognition face recognition;motion estimation;emotion recognition;face recognition;facial expression facial motion tracking flexible head pose;shape three dimensional displays tracking solid modeling face computational modeling skin	Facial motion tracking is a challenging task because of highly flexible head pose and facial expression. An extensible tracking framework is proposed in this paper. Within the framework, proper models are selected according to requirements and restrictions of the application, and different trackers can be constructed to handle different tasks. Experimental result shows that our tracker outperforms the existing commercial software.	commercial software;eye tracking;requirement	Xiaolu Shen;Xuetao Feng;Jungbae Kim;Hui Zhang;Youngkyoo Hwang;Ji-yeun Kim	2013	2013 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2013.6486898	facial recognition system;computer vision;facial motion capture;speech recognition;computer science;motion estimation;three-dimensional face recognition;face hallucination	Robotics	47.17788983196762	-48.753379680525356	136915
39056feb82e07c02cc47d8b4b256eb5620853d74	a stochastic cost function for stereo vision		The goal of this paper is to present a novel stochastic cost function for binocular stereo vision that delivers statistics about the most probable disparities on the pixel level. We drive these statistics by many independent stochastic processes so that robustness to outliers can be achieved. Each of these stochastic processes may be understood as an individual who is requested to deliver his opinion about the depth. Finally, the idea is to fuse all these individual measurements into one global disparity map. In this paper, we use random walks for this.	binocular disparity;binocular vision;loss function;pixel;stereopsis;stochastic process	Christian Unger;Slobodan Ilic	2014			robustness (computer science);computer science;artificial intelligence;computer vision;random walk;pixel;outlier;stochastic process;stereopsis	Vision	46.24515139619023	-48.832827375537285	136974
8220828c08c78b33ea4ab13a6506310c5f4bf0b0	tracking based structure and motion recovery for augmented video productions	jitter reduction;feature tracking;psi_visics;augmented reality;structure and motion;accurate registration	Augmented Reality (AR) can hardly be called uncharted territory. Much research in this area revealed solutions to the three most prominent challenges of AR: accurate camera state retrieval, resolving occlusions between real and virtual objects and extraction of environment illumination distribution. Solving these three challenges improves the illusion of virtual entities belonging to our reality. This paper demonstrates an elaborated framework that recovers accurate camera states from a video sequence based on feature tracking. Without prior calibration knowledge, it is able to create AR Video products with negligible/invisible jitter or drift of virtual entities starting from general input video sequences. Together with the referenced papers, this work describes a readily implementable and robust AR-System.	augmented reality;entity;motion estimation	Kurt Cornelis;Marc Pollefeys;Luc Van Gool	2001		10.1145/505008.505012	computer vision;augmented reality;simulation;computer science;video tracking;computer graphics (images)	Visualization	51.57161549336061	-45.769937525995715	137041
2fc4804ec2749157760e0d2d4a0eaef83ca06782	online environment mapping	environment maps;euclidean environment map;optimisation;measurement optimization cameras three dimensional displays simultaneous localization and mapping;real time;iterative optimization online environment mapping large scale environments euclidean environment map topological map local registration;local registration;large scale;topological map;robot vision;online environment mapping;iterative optimization;euclidean space;large scale environments;slam robots;slam robots optimisation robot vision	The paper proposes a vision based online mapping of large-scale environments. Our novel approach uses a hybrid representation of a fully metric Euclidean environment map and a topological map. This novel hybrid representation facilitates our scalable online hierarchical bundle adjustment approach. The proposed method achieves scalability by solving the local registration through embedding neighboring keyframes and landmarks into a Euclidean space. The global adjustment is performed on a segmentation of the keyframes and posed as the iterative optimization of the arrangement of keyframes in each segment and the arrangement of rigidly moving segments. The iterative global adjustment is performed concurrently with the local registration of the keyframes in a local map. Thus the map is always locally metric around the current location, and likely to be globally consistent. Loop closures are handled very efficiently benefiting from the topological nature of the map and overcoming the loss of the metric map properties as previous approaches. The effectiveness of the proposed method is demonstrated in real-time on various challenging video sequences.	bundle adjustment;iterative method;key frame;mathematical optimization;real-time clock;reflection mapping;scalability;web mapping	Jongwoo Lim;Jan-Michael Frahm;Marc Pollefeys	2011	CVPR 2011	10.1109/CVPR.2011.5995511	computer vision;mathematical optimization;euclidean space;mathematics;geometry;topological map	Vision	52.44957353535932	-46.56586417538849	137056
894fb62e08d0126386650fa2f1d10abc892e820f	dynamic threshold for morphological change detection algorithm	computer vision;morphological change detection;image processing.;dynamic threshold	Vision-based systems for remote surveillance usually involve change detection algorithms for intruders, obstacles or irregularities detection. Real time applications require simple, fast and reliable algorithms for change detection methodology. Morphological change detection algorithm satisfies these requirements. Threshold is a very important issue for morphological change detection. Threshold is static in existing Morphological Change Detection Algorithm [12]. We are introducing a technique to make Threshold dynamic. The proposed Dynamic Threshold determination technique may also be used in Moving object detection.	algorithm;object detection;requirement	A. N. M. Rezaul Karim;M. G. R. Alam	2009	JCIT		computer vision;feature detection;edge detection;image processing;computer science;theoretical computer science;machine learning;change detection;satisfiability	SE	44.90407396766343	-39.992739005332375	137072
84776cd7f9f36b12c381966b0419c7c384b8cc90	iterative versus voting method to reach consensus given multiple correspondences of two sets		Point Set Registration is the process of finding the correspondence between points of two sets. There are some Point Set Registration applications in which given two sets of points, several correspondences between these points may be deducted. However, the use of different parameters or optimisation strategies makes these correspondences differ from each other. In this paper, we present two different methods to obtain a consensus correspondence given several correspondences between two sets of points. The first one is based on the classical voting strategy. The second one iteratively updates the consensus correspondence given two correspondences: a non-previously explored corre- spondence and the current consensus. In this last method, the computation of the consensus given two correspondences is done through a method recently pub- lished. We compare the voting and iterative methods using an image dataset and validate the runtime and the quality of the consensus correspondence using an existing homography between the considered images.	iterative method	Carlos Francisco Moreno-García;Xavier Cortés;Francesc Serratosa	2015		10.1007/978-3-319-19390-8_60	mathematical optimization;discrete mathematics;data mining;mathematics	Vision	50.69046924941797	-51.13059789529166	137216
18ffe00cb25483dd7607620aa9c331d5857c022d	real-time 3d football ball tracking from multiple cameras			real-time transcription	Jinchang Ren;James Orwell;Graeme A. Jones;Ming Xu	2004		10.5244/C.18.85	computer vision;computer science;artificial intelligence;football	Vision	50.717616748606574	-43.00730897132484	137313
6bd8d53a6f4425ad16c8a878805830cda1ab6863	a rao-blackwellized mcmc algorithm for recovering piecewise planar 3d models from multiple view rgbd images	three dimensional displays solid modeling image segmentation computer vision image reconstruction bayes methods cameras;sun3d dataset rao blackwellized markov chain monte carlo algorithm piecewise planar 3d models rgbd images reconstruction technique superpixel labeling;reconstruction;monte carlo methods image reconstruction image segmentation markov processes;segmentation;point clouds;piecewise planar segmentation point clouds reconstruction;piecewise planar	In this paper, we propose a reconstruction technique that uses 2D regions/superpixels rather than point features. We use pre-segmented RGBD data as input and obtain piecewise planar 3D models of the world. We solve the problem of superpixel labeling within single and multiple views simultaneously by using a Rao-Blackwellized Markov Chain Monte Carlo (MCMC) algorithm. We present our output as a labeled 3D model of the world by integrating out ov er all possible 3D planes in a fully Bayesian fashion. We present our results on the new SUN3D dataset [1].	3d modeling;algorithm;markov chain monte carlo;monte carlo method;semantic role labeling	Natesh Srinivasan;Frank Dellaert	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7026091	computer vision;mathematical optimization;pattern recognition;point cloud;mathematics;segmentation	Robotics	46.13766757841236	-50.60226303058823	137426
381034462b911ea977b4462164a0393e08829c56	simulation of falling rain for robustness testing of video-based surround sensing systems		Recently, optical sensors have become a standard item in modern cars, raising questions with respect to the necessary testing under various ambient effects. In order to achieve a high test coverage of vision-based surround sensing systems, a lot of different environmental conditions need to be tested. Unfortunately, it is by far too time-consuming to build test sets of all relevant environmental conditions by recording real video data. This paper presents a novel approach for ambient-aware virtual prototyping and robustness testing. We propose a method to significantly reduce the needed on-road recordings being used for design and validation of vision-based Advanced Driver Assistance Systems (ADAS) and fully automated driving. Our approach facilitates the generation of comparable test sets by using largely reduced amounts of real on-road recordings and applying computer-generated variations of falling rain to it in a comprehensive virtual prototyping environment. In combination with the simulation of camera properties, which influence the visual effects of falling rain to a great extent, we are able to generate different rain scenarios under a wide variety of parameters. Our approach has been applied to an automotive lane detection system using a series of multiple rain scenarios. We have explored, how falling rain can influence such a system and how such behavior can be detected using simulated rain scenarios.	algorithm;architecture design and assessment system;autonomous car;computer vision;computer-generated holography;correctness (computer science);fault coverage;ground truth;illumination (image);image analysis;robustness testing;sensor;simulation;synthetic intelligence;titan rain;visual effects	Dennis Hospach;Stefan Müller;Wolfgang Rosenstiel;Oliver Bringmann	2016	2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;computer vision;electronic engineering;simulation;telecommunications;engineering;software testing;computational model;measurement;statistics;computer graphics (images)	EDA	44.648072712236626	-41.54079266637449	137460
36c5a29a775bae4a09c6384ff1a3ba3aa5e6ea52	approximately global optimization for robust alignment of generalized shapes	robust alignment;gaussian mixture;measurement;gaussian processes;particle swarm optimization shape alignment point registration matching distance transform;algorithms artificial intelligence image enhancement imaging three dimensional models theoretical pattern recognition automated;image based representation;gray scale gradient maps;continuous contours;particle swarm optimization global optimization robust alignment generalized shape alignment problems gray scale images source shape representation two component gaussian mixture distance map representation image based representation continuous contours unstructured sparse point sets gray scale gradient maps energy function;gray scale images;point registration;gray scale;energy function;particle swarm optimizer;shape;image representation;particle swarm optimization;matching;source shape representation;unstructured sparse point sets;shape alignment;particle swarm optimisation gaussian processes image representation;shape robustness iterative closest point algorithm optimization measurement particle swarm optimization gray scale;robustness;global optimization;optimization;two component gaussian mixture distance map representation;iterative closest point algorithm;distance transform;generalized shape alignment problems;computational efficiency;particle swarm optimisation;high efficiency;empirical evaluation	"""In this paper, we introduce a novel method to solve shape alignment problems. We use gray-scale """"images” to represent source shapes, and propose a novel two-component Gaussian Mixture (GM) distance map representation for target shapes. This asymmetric representation is a flexible image-based representation which is able to represent different kinds of shape data, including continuous contours, unstructured sparse point sets, edge maps, and even gray-scale gradient maps. Using this representation, a new energy function based on a novel two-component Gaussian Mixture distance model is proposed. The new energy function was empirically evaluated to be a more robust shape dissimilarity metric that can be computed efficiently. Such high efficiency is essential for global optimization methods. We adopt and modify one of them, the Particle Swarm Optimization (PSO), to effectively estimate the global optimum of the new energy function. Differently from the original PSO, several new strategies were employed to make the optimization more robust and prevent it from converging prematurely. The overall performance of the proposed framework as well as the properties of each algorithmic component were evaluated and compared with those of some state-of-the-art methods. Extensive experiments and comparison performed on generalized 2D and 3D shape data demonstrate the robustness and effectiveness of the method."""	alignment;convergence (action);distance transform;experiment;global optimization;gradient;grayscale;map;mathematical optimization;normal statistical distribution;particle swarm optimization;racepinephrine;shape context;sparse matrix	Hongsheng Li;Tian Shen;Xiaolei Huang	2011	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2010.169	matching;mathematical optimization;shape;computer science;machine learning;pattern recognition;gaussian process;mathematics;distance transform;particle swarm optimization;grayscale;measurement;robustness;global optimization	Vision	49.552108517680814	-51.701147238882534	137802
5e3788835c391a98b927d1e005e21d02053e1168	3d motion estimation using expansion matching and kl based canonical images	eigenvalues and eigenfunctions;eigenvalues and eigenfunctions motion estimation image sequences image matching karhunen loeve transforms video signal processing parameter estimation principal component analysis;video signal processing;image matching;motion estimation;3d translations 3d motion estimation expansion matching kl based canonical images planar objects eigen normalization scaled orthographic projection model comprehensive temporal description degrees of freedom affine parameters video sequence principal component normalization procedure normalization approach intensity weighted spatial values orientation differences affine transformations quasi planar objects robust estimation 3d rotations;motion estimation image motion analysis video sequences shape image generation robustness streaming media motion analysis lighting image retrieval;karhunen loeve transforms;principal component analysis;parameter estimation;image sequences	This paper describes a novel approach to D mo tion estimation of planar objects based on eigen normalization Expansion Matching EXM and a scaled orthographic projection model Our approach leads to a comprehensive temporal description of all degrees of freedom in D rotations and transla tions The D motion parameters of the objects are approximated by the corresponding a ne parameters The objects in each frame of a video sequence are nor malized to a set of canonical images using principal component normalization procedure The normaliza tion approach here is based on principal components of the intensity weighted spatial values and not on the intensity values as in works such as eigenfaces The canonical images generated di er only in orientation Expansion Matching EXM is then used to nd the di erences in orientation A ne transformations be tween the shapes also are derived The pose of the shape in D space can therefore be estimated Exper iments on video sequences of planar and quasi planar objects show robust estimation of the real D rotations and translations of the objects in motion	approximation algorithm;eigen (c++ library);eigenface;motion estimation;orthographic projection;principal component analysis	Zhiqian Wang;Jezekiel Ben-Arie	1998		10.1109/ICIP.1998.723671	computer vision;quarter-pixel motion;computer science;pattern recognition;motion estimation;mathematics;geometry;block-matching algorithm;estimation theory;motion compensation;statistics;principal component analysis	Vision	48.80662698338595	-50.6792257805933	138096
086a6fed9012d976b5a661ed77ef158aa363cf80	appearance based indexing for relocalisation in real-time visual slam	real time visualization;haar wavelet;indexation;real time systems	Previous work on visual SLAM has shown that indexing on space and scale facilitates the use of feature descriptors for matching in real-time systems and that this can significantly increase robustness. However, the performance gains necessarily diminish as uncertainty about camera position increases. In this paper we address this issue by introducing a further level of indexing based on appearance, using low order Haar wavelet coefficients. This enables fast look up of descriptors even when the camera is lost, hence allowing efficient relocalisation. Results of experiments on a range of real world test cases demonstrate that the method is effective, including single frame relocalisation rates up to 90% using relatively low numbers of descriptor comparisons.	coefficient;experiment;haar wavelet;map;real-time clock;real-time computing;simultaneous localization and mapping;software development;test case	Denis Chekhlov;Walterio W. Mayol-Cuevas;Andrew Calway	2008		10.5244/C.22.37	computer vision;simulation;computer science;computer graphics (images)	Robotics	41.23427096015062	-51.49339387464623	138317
b7cc3c4e4e40a3bd5fbda36f1ca44ef51023806d	1-point rigid motion estimation and segmentation with a rgb-d camera	image segmentation;differential geometry;motion estimation;motion estimation cameras differential geometry image colour analysis image segmentation;conference paper;cameras vectors motion segmentation computer vision motion estimation geometry estimation;image colour analysis;1 point rigid motion estimation real rgb d data synthetic rgb d data rotation translation averaging techniques principal curvature directions sign ambiguity single surface point correspondence euclidean transformation image gradients differential geometry dense depth images color images microsoft kinect rgb d camera 1 point rigid motion segmentation;cameras	RGB-D cameras like Microsoft Kinect that provide color and dense depth images have now become commonplace. We consider the problem of estimation and segmentation of multiple rigid body motions observed by such a camera. On the basis of differential geometry of surfaces and image gradients, we present a method for completely estimating the Euclidean transformation of a rigid body by using just a single surface point correspondence. This is facilitated by two methods of removing the sign ambiguity of principal curvature directions which is the main contribution of the paper. Further, we apply state-of-the-art rotation/translation averaging techniques to achieve refined Euclidean transformation estimates and segmentation. Results using both synthetic and real RGB-D data show the validity of our approach.	estimation theory;image gradient;kinect;motion estimation;synthetic data	Samunda Perera;Nick Barnes	2013	2013 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2013.6691469	differential geometry;computer vision;computer science;motion estimation;mathematics;geometry;image segmentation;scale-space segmentation;computer graphics (images)	Vision	52.74880202826677	-50.30637114568994	138666
b6f10f08eb5dd57cf17d0630f59dbff64d8e4a21	2d and 3d face localization for complex scenes	stochastic processes face recognition optical tracking;gaussian mixture model face tracking face localization stochastic tracker deterministic tracker;face tracking;layout face detection detectors stochastic processes cameras merging information technology grid computing surveillance security;gaussian mixture model;face recognition;stochastic processes;optical tracking	In this paper, we address face tracking of multiple people in complex 3D scenes, using multiple calibrated and synchronized far-field recordings. We localize faces in every camera view and associate them across the different views. To cope with the complexity of 2D face localization introduced by the multitude of people and unconstrained face poses, a combination of stochastic and deterministic trackers, detectors and a Gaussian mixture model for face validation are utilized. Then faces of the same person seen from the different cameras are associated by first finding all possible associations and then choosing the best option by means of a 3D stochastic tracker. The performance of the proposed system is evaluated and is found enhanced compared to existing systems.	color histogram;face detection;kalman filter;mixture model;sensor;tracking system;video processing	Ghassan O. Karame;Andreas Stergiou;Nikos Katsarakis;Panagiotis Papageorgiou;Aristodemos Pnevmatikakis	2007	2007 IEEE Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2007.4425339	facial recognition system;stochastic process;computer vision;facial motion capture;simulation;computer science;machine learning;mixture model;statistics	Vision	43.91193649692003	-48.05180707759765	138717
20f5b475effb8fd0bf26bc72b4490b033ac25129	real time detection of lane markers in urban streets	filtering;bezier splines;road accidents;real time lane marker detection;real time;selective oriented gaussian filters;filters;ransac line fitting;vehicle detection;splines mathematics;computer vision;urban streets;intelligent vehicles;splines mathematics driver information systems filtering theory object detection;robustness;vehicles;curve fitting;frequency 50 hz real time lane marker detection urban streets selective oriented gaussian filters ransac line fitting bezier splines;frequency 50 hz;driver information systems;filtering theory;cameras;object detection;road vehicles	We present a robust and real time approach to lane marker detection in urban streets. It is based on generating a top view of the road, filtering using selective oriented Gaussian filters, using RANSAC line fitting to give initial guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is then followed by a post-processing step. Our algorithm can detect all lanes in still images of the street in various conditions, while operating at a rate of 50 Hz and achieving comparable results to previous techniques.	algorithm;edge detection;line fitting;random sample consensus;sensor;smoothing spline;spline (mathematics);video post-processing	Mohamed Aly	2008	2008 IEEE Intelligent Vehicles Symposium	10.1109/IVS.2008.4621152	filter;computer vision;simulation;computer science;robustness;curve fitting;computer graphics (images)	Vision	44.05374549738972	-43.784301390608576	138761
f609d6da7e9b87b386d4779b00038d03ba7fc92d	occlusion handling based on sub-blobbing in automated video surveillance system	moving object;video surveillance;features;automatic detection;occlusion handling;object tracking	Object tracking with occlusion handling is a challenging problem in automated video surveillance. In particular, occlusion handling and tracking have been often considered as separate modules. This paper proposes a tracking method in the context of video surveillance, where occlusions are automatically detected and handled to solve ambiguities. Hence, the tracking process can continue to track the different moving objects correctly. The proposed approach is based on sub-blobbing, that is, blobs representing moving objects are segmented into sections whenever occlusions occur. These sub-blobs are then treated as blobs with the occluded ones ignored. By doing so, the tracking of objects has become more accurate and less sensitive to occlusions. We have also used a feature-based framework for identifying the tracked objects, where several flexible attributes were involved. Experiments on several videos have clearly demonstrated the success of the proposed method.	closed-circuit television;experiment	Mohammad Omair Alam;Boubakeur Boufama	2011		10.1145/1992896.1992914	computer vision;simulation;feature;computer science;machine learning;video tracking;computer graphics (images)	Vision	42.221726611584906	-46.71944155191921	138882
2a18bdcfcb7352e8d88e1ffe86f28ad7bbdba097	real-time hand tracking under occlusion from an egocentric rgb-d sensor		We present an approach for real-time, robust and accurate hand pose estimation from moving egocentric RGB-D cameras in cluttered real environments. Existing methods typically fail for hand-object interactions in cluttered scenes imaged from egocentric viewpoints-common for virtual or augmented reality applications. Our approach uses two subsequently applied Convolutional Neural Networks (CNNs) to localize the hand and regress 3D joint locations. Hand localization is achieved by using a CNN to estimate the 2D position of the hand center in the input, even in the presence of clutter and occlusions. The localized hand position, together with the corresponding input depth value, is used to generate a normalized cropped image that is fed into a second CNN to regress relative 3D hand joint locations in real time. For added accuracy, robustness and temporal stability, we refine the pose estimates using a kinematic pose tracking energy. To train the CNNs, we introduce a new photorealistic dataset that uses a merged reality approach to capture and synthesize large amounts of annotated data of natural hand interaction in cluttered scenes. Through quantitative and qualitative evaluation, we show that our method is robust to self-occlusion and occlusions by objects, particularly in moving egocentric perspectives.	3d pose estimation;augmented reality;benchmark (computing);clutter;convolutional neural network;glossary of computer graphics;hidden surface determination;interaction;real-time clock;real-time computing	Franziska Mueller;Dushyant Mehta;Oleksandr Sotnychenko;Srinath Sridhar;Dan Casas;Christian Theobalt	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.82	computer vision;simulation	Vision	51.36368432618363	-45.76541410592781	138907
272e9983acd69180b3cd1b70baa645444067010e	an hybrid image processing approach to liptracking independent of head orientation	image processing	"""This paper examines the influence of head orientation in liptracking. There are two main conclusions: First, lip gesture analysis and head movement correction should be processed independently. Second, the measurement of articulatory parameters may be corrupted by head movement if it is performed directly at the pixel level. We thus propose an innovative technique of liptracking which relies on a """"3D active contour"""" model of the lips controlled by articulatory parameters. The 3D model is projected onto the image of a speaking face through a camera model, thus allowing spatial re-orientation of the head. Liptracking is then performed by automatic adjustment of the control parameters, independently of head orientation. The final objective of our study is to apply a pixel-based method to detect head orientation. Nevertheless, we consider that head motion and lip gestures are detected by different processes, whether cognitive (by humans) or computational (by machines). Due to this, we decided to first develop and evaluate orientation-free liptracking through a non video-based head motion detection technique which is here presented."""	3d modeling;active contour model;computation;image processing;pixel	Lionel Revéret;Frederique Garcia;Christian Benoît;Eric Vatikiotis-Bateson	1997			binary image;digital image processing;image processing;hybrid image;motion detection;pattern recognition;active contour model;image texture;computer vision;artificial intelligence;computer science;feature detection (computer vision)	Vision	40.41643354837025	-48.703482020877594	139160
0139167c54bc82e056c8d994ef2e38f3a1a5243b	lane departure identification for advanced driver assistance	image artifact;segmentation approach;video sequence;image segmentation;image processing;plsf;video signal processing;piecewise linear stretching function;vehicles roads transforms image color analysis lighting robustness image edge detection;lane marking detection;distance based departure measure;machine vision driver assistance system hough transform ht lane departure lane detection;euclidean distance transform;hough transform ht;image edge detection;roads;image color analysis;machine vision;contrast level;feature extraction;region of interest;video signal processing driver information systems feature extraction hough transforms image segmentation image sequences road vehicles;transforms;hough transforms;algorithms;robustness;hough transform;ldi;vehicles;lighting;detection and identification systems;video sequence lane departure identification ldi advanced driver assistance road vehicle piecewise linear stretching function plsf contrast level region of interest roi lane marking detection hough transform segmentation approach distance based departure measure euclidean distance transform image artifact;roi;advanced driver assistance;road vehicle;lane departure;driver information systems;lane changing;lane detection;lane departure identification;distance;driver assistance system;road vehicles;image sequences	In this paper, a technique for the identification of the unwanted lane departure of a traveling vehicle on a road is proposed. A piecewise linear stretching function (PLSF) is used to improve the contrast level of the region of interest (ROI). Lane markings on the road are detected by dividing the ROI into two subregions and applying the Hough transform in each subregion independently. This segmentation approach improves the computational time required for lane detection. For lane departure identification, a distance-based departure measure is computed at each frame, and a necessary warning message is issued to the driver when such measure exceeds a threshold. The novelty of the proposed algorithm is the identification of the lane departure only using three lane-related parameters based on the Euclidean distance transform to estimate the departure measure. The use of the Euclidean distance transform in combination with the PLSF keeps the false alarm around 3% and the lane detection rate above 97% under various lighting conditions. Experimental results indicate that the proposed system can detect lane boundaries in the presence of several image artifacts, such as lighting changes, poor lane markings, and occlusions by a vehicle, and it issues an accurate lane departure warning in a short time interval. The proposed technique shows the efficiency with some real video sequences.	algorithm;distance transform;euclidean distance;hough transform;piecewise linear continuation;region of interest;time complexity	Vijay Gaikwad;Shashikant Lokhande	2015	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2014.2347400	hough transform;computer vision;return on investment;simulation;machine vision;image processing;feature extraction;computer science;lighting;image segmentation;distance;robustness;computer graphics (images);region of interest	Vision	43.4375316892131	-45.329917089355995	139185
727bceddf84900ed8f050429986e0c1767ae5c21	adaptive directional bounding box from rgb-d information for improving fall detection		Abstract Fall detection for aging people is still a mainstream research focus for the current aging society. Tools that are simple and inexpensive but have high accuracy rates are needed. RGB-D information retrieved from a home entertainment system was used to detect falls using typical bounding boxes techniques. These techniques have limitations. This research introduced the Adaptive Directional Bounding Box that made use of a comprehensive bounding box and a dynamic state machine in a new way to detect falls. The proposed approach offered a way to track and analyze continuous data streams of the visual images to automatically predict a fall event prior to the fall state in a single-phase instead of the typical two-phases. This can significantly affect the survival or severe injury of the elderly. The proposed method can improve accuracy by 25.5% and the response time by 21.31% on average as compared to existing approaches.	minimum bounding box	Apichet Yajai;Suwanna Rasmequan	2017	J. Visual Communication and Image Representation	10.1016/j.jvcir.2017.08.008	mathematics;finite-state machine;artificial intelligence;computer vision;bounding overwatch;home entertainment;data stream mining;rgb color model;response time;minimum bounding box	Vision	39.935219041252616	-42.359675321786916	139323
da75bc573300e3cd09918043a385f0edefe22680	optimal parameter estimation with homogeneous entities and arbitrary constraints	computational geometry;fundamental matrix;projective geometry;multiple constraints;vanishing point;covariance matrices;synthetic data;parameter estimation	Well known estimation techniques in computational geometry usually deal only with single geometric entities as unknown parameters and do not account for constrained observations within the estimation. The estimation model proposed in this paper is much more general, as it can handle multiple homogeneous vectors as well as multiple constraints. Furthermore, it allows the consistent handling of arbitrary covariance matrices for the observed and the estimated entities. The major novelty is the proper handling of singular observation covariance matrices made possible by additional constraints within the estimation. These properties are of special interest for instance in the calculus of algebraic projective geometry, where singular covariance matrices arise naturally from the non-minimal parameterizations of the entities. The validity of the proposed adjustment model will be demonstrated by the estimation of a fundamental matrix from synthetic data and compared to heteroscedastic regression [?], which is considered as state-ofthe-art estimator for this task. As the latter is unable to simultaneously estimate multiple entities, we will also demonstrate the usefulness and the feasibility of our approach by the constrained estimation of three vanishing points from observed uncertain image line segments.	computational geometry;entity;estimation theory;fundamental matrix (computer vision);linear algebra;synthetic data	Jochen Meidow;Wolfgang Förstner;Christian Beder	2009		10.1007/978-3-642-03798-6_30	estimation of covariance matrices;mathematical optimization;combinatorics;mathematics;statistics	Vision	53.108989202596696	-49.575030343124965	139381
639978b91cb7ac8eb1e2b027f8baee141f187c46	omnidirectional vision based topological navigation	vision omnidirectionnelle;navegacion;maps;robot movil;topology;vision ordenador;omnidirectional vision;analisis escena;analyse scene;image processing;panoramic photography;autonomous system;path planning;real time;localization;topologie;asservissement visuel;procesamiento imagen;psi_visics;localizacion;robotics;autonomous mobile robot;feature matching;photographie panoramique;traitement image;fotografia panoramica;sistema autonomo;wide baseline matching;topologia;computer vision;planification trajectoire;captador medida;navigation;omnidirectional camera;measurement sensor;milieu naturel;capteur mesure;localisation;robot mobile;natural environment;vision omnidireccional;temps reel;systeme autonome;robotica;tiempo real;vision ordinateur;robotique;medio natural;visual servoing;topological maps;moving robot;servomando visual;scene analysis	In this work we present a novel system for autonomous mobile robot navigation. With only an omnidirectional camera as sensor, this system is able to build automatically and robustly accurate topologically organised environment maps of a complex, natural environment. It can localise itself using such a map at each moment, including both at startup (kidnapped robot) or using knowledge of former localisations. The topological nature of the map is similar to the intuitive maps humans use, is memory-efficient and enables fast and simple path planning towards a specified goal. We developed a real-time visual servoing technique to steer the system along the computed path. A key technology making this all possible is the novel fast wide baseline feature matching, which yields an efficient description of the scene, with a focus on man-made environments.	autonomous robot;baseline (configuration management);map;mobile robot;motion planning;omnidirectional camera;real-time clock;robotic mapping;visual servoing	Toon Goedemé;Marnix Nuttin;Tinne Tuytelaars;Luc Van Gool	2006	International Journal of Computer Vision	10.1007/s11263-006-0025-9	computer vision;navigation;simulation;internationalization and localization;image processing;computer science;autonomous system;motion planning;natural environment;robotics;visual servoing;mobile robot navigation	Robotics	52.32141845747551	-38.41162147090858	139492
3a95b958a21dc53b49c8ab25a19206a674c055fd	on-road vehicle tracking using part-based particle filter		In this paper, we propose a part-based particle filter for on-road vehicle tracking. The proposed model takes part-based strategies into account in a particle filter. By introducing a hidden state vehicle center position, vehicle parts particles can be updated efficiently as a whole sharing same motion. With a pre-trained appearance and geometric model, tracker can distinguish parts with rich information from invalid parts to make a more precise prediction. Meanwhile some priori knowledge about the moving pattern of vehicles in well-structured on-road environment is learned, and can be used in the inference of measurement model and motion model to improve tracking performance and efficiency. Experiments were conducted with real data collected in Beijing to examine the performance in different situations on both the advantages and challenges. The Beijing highway dataset for on-road vehicle tracking will be opened to the society. We compare our method with the state-of-the-art approaches. Result demonstrate that the proposed algorithm are able to handle occlusion and aspect ratio change in on-road vehicle tracking problem.	algorithm;geometric modeling;particle filter;relevance;vehicle tracking system	Yongkun Fang;Chao Wang;Huijing Zhao;Hongbin Zha	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206224	computer vision;vehicle tracking system;artificial intelligence;control engineering;geometric modeling;computer science;particle filter;inference	Robotics	46.88947324753581	-40.45347526119629	139707
0c17c42d71eacd2244e43fa55a8ed96607337cca	automatic face reenactment	head motion automatic face reenactment image based facial reenactment system face replacement facial gesture reenactment pipeline image retrieval image matching metric;face reenactment;temporal clustering;cs cv;video signal processing image matching image processing;expression matching;video editing;face shape three dimensional displays target tracking solid modeling;cs gr;facial puppetry;computer science;computer vision and pattern recognition;temporal clustering video editing face reenactment facial puppetry expression matching;graphics	We propose an image-based, facial reenactment system that replaces the face of an actor in an existing target video with the face of a user from a source video, while preserving the original target performance. Our system is fully automatic and does not require a database of source expressions. Instead, it is able to produce convincing reenactment results from a short source video captured with an off-the-shelf camera, such as a webcam, where the user performs arbitrary facial gestures. Our reenactment pipeline is conceived as part image retrieval and part face transfer: The image retrieval is based on temporal clustering of target frames and a novel image matching metric that combines appearance and motion to select candidate frames from the source video, while the face transfer uses a 2D warping strategy that preserves the user's identity. Our system excels in simplicity as it does not rely on a 3D face model, it is robust under head motion and does not require the source and target performance to be similar. We show convincing reenactment results for videos that we recorded ourselves and for low-quality footage taken from the Internet.	cluster analysis;database;image registration;image retrieval;image warping;internet;webcam	Pablo Garrido;Levi Valgaerts;Ole Rehmsen;Thorsten Thormählen;Patrick Pérez;Christian Theobalt	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.537	computer vision;computer science;graphics;multimedia;computer graphics (images)	Vision	48.723552636825424	-47.793507439230595	139832
7fd9271c622cdb322b9de2e91b393d8be3010be7	range data fusion for accurate surface generation from heterogeneous range scanners		In this paper, we present a new method for range data fusion from two heterogeneous range scanners for accurate surface modeling of rough and highly unstructured terrain. First, we present the segmentation of RGB-D images using the new framework of the GMM by employing the convex relaxation technique. After segmentation of RGB-D images, we transform both the range data to a common reference frame using PCA algorithm and apply the ICP algorithm to align both data in the reference frame. Based on a threshold criterion, we fuse the range data in such a way that the coarser regions are obtained from Kinect sensor and finer regions of plane are obtained from the Laser range sensor. After fusion, we apply Delaunay triangulation algorithm to generate the highly accurate surface model of the terrain. Finally, the experimental results show the robustness of the proposed approach.	3d scanner;align (company);delaunay triangulation;genetic algorithm;google map maker;kinect;linear programming relaxation;polygonal modeling;principal component analysis;reference frame (video);seamless3d;sensor	Mahesh Kr. Singh;K. S. Venkatesh;Ashish Dutta	2015	2015 12th International Conference on Informatics in Control, Automation and Robotics (ICINCO)		computer vision;delaunay triangulation;computer science;machine learning;pattern recognition;mixture model	Robotics	53.1464745377864	-43.62121870057942	139859
e1e88064a18749dc83bd0e591d0cf195bae3988b	the adaptive algorithm of the light intensity applied to androsot		The vision system in FIRA’s AndroSot plays an important role. To get precise and robust location of both the ball and robot, we need to make a long time before competition, which the most time consuming step is to sample the color of the robot at different location, for the illumination intensity will lead to the changes of the origin color on robot. A light intensity adaptive algorithm is proposed in this paper, it sets up a look up table of the relation between the illumination intensity and the robot color label, and then achieves the color library interoperability under the different light conditions, we show that the accuracy of the algorithm by comparing the calculating result with the sampling result.	adaptive algorithm	Zhongwen Luo;Ke Wang;Hongjian Sun;Wen Chen	2013		10.1007/978-3-642-40409-2_29	humanoid robot;computer vision;adaptive algorithm;robot;sampling (statistics);machine vision;interoperability;artificial intelligence;lookup table;computer science	Vision	48.41238107959587	-42.905744592697026	139979
155d0b8d92ab01508b774b7623b9c0cb04af37c6	sports camera calibration via synthetic data		Calibrating sports cameras is important for autonomous broadcasting and sports analysis. Here we propose a highly automatic method for calibrating sports cameras from a single image using synthetic data. First, we develop a novel camera pose engine. The camera pose engine has only three significant free parameters so that it can effectively generate a lot of camera poses and corresponding edge (i.e., field marking) images. Then, we learn compact deep features via a siamese network from paired edge image and camera pose and build a feature-pose database. After that, we use a novel two-GAN (generative adversarial network) model to detect field markings in real images. Finally, we query an initial camera pose from the feature-pose database and refine camera poses using truncated distance images. We evaluate our method on both synthetic and real data. Our method not only demonstrates the robustness on the synthetic data but also achieves the state-of-the-art accuracy on a standard soccer dataset and very high performance on a volleyball dataset.	autonomous robot;autostereogram;camera resectioning;interaction;item unique identification;synthetic data;synthetic intelligence	Jianhui Chen;James J. Little	2018	CoRR		pattern recognition;artificial intelligence;robustness (computer science);camera resectioning;computer science;real image;synthetic data	Vision	51.6350893352081	-46.21593339547587	140184
6958553ec2bf57a36ef7dd2d3b484d6e69700703	severity classification of abnormal traffic events at intersections	image processing;support vector machines;maximum likelihood;video signal processing;hidden markov models vehicles accidents trajectory support vector machines conferences image processing;hidden markov model;image classification;video processing;support vector machines methods severity classification abnormal traffic events intersections video processing techniques statistical deviation analysis vehicle motions continuous hidden markov model maximum likelihood k nearest neighborhood;maximum likelihood estimation;accident severity classification;video signal processing hidden markov models image classification maximum likelihood estimation statistical analysis support vector machines traffic information systems;hidden markov models;trajectory;statistical analysis;traffic information systems;accidents;accident detection accident severity classification video based traffic scene analysis hidden markov models;accident detection;video based traffic scene analysis;vehicles;support vector machine;conferences;scene analysis	The purpose of this work is to investigate the severity characteristics of abnormal events at intersections by using video processing techniques and statistical deviation analysis methods. In order to detect the abnormal events, trajectory of normal vehicle motions are clustered and common route models are learned by Continuous Hidden Markov Model. In the second part, the abnormal spatio-temporal deviations are detected by extracting partial vehicle motion observations using Maximum Likelihood. Next, the severity definition and classification is done for abnormal events using k-Nearest Neighborhood and Support Vector Machines methods. The two-class event classifier is built to classify abnormal observations into one of the low or high severe event classes. The results indicate that abnormal events can be detected and represented by likelihood probabilities, and depending on these probabilities, severity analysis can be done successfully.	hidden markov model;k-nearest neighbors algorithm;markov chain;norm (social);support vector machine;video processing	Omer Aköz;M. Elif Karsligil	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116128	support vector machine;computer science;machine learning;pattern recognition;data mining;maximum likelihood;hidden markov model;statistics	Vision	39.80363462237596	-45.804532727826704	140231
307676b91b88a868e8123b7591367ccfacf48299	motion segmentation with weak labeling priors		Motions of organs or extremities are important features for clinical diagnosis. However, tracking and segmentation of complex, quickly changing motion patterns is challenging, certainly in the presence of occlusions. Neither state-of-the-art tracking nor motion segmentation approaches are able to deal with such cases. Thus far, motion capture systems or the like were needed which are complicated to handle and which impact on the movements. We propose a solution based on a single video camera, that is not only far less intrusive, but also a lot cheaper. The limitation of tracking and motion segmentation are overcome by a new approach to integrate prior knowledge in the form of weak labeling into motion segmentation. Using the example of Cerebral Palsy detection, we segment motion patterns of infants into the different body parts by analyzing body movements. Our experimental results show that our approach outperforms current motion segmentation and tracking approaches.	algorithm;experiment;motion capture;overhead (computing)	Hodjat Rahmati;Ralf Dragon;Ole Morten Aamo;Luc Van Gool;Lars Adde	2014		10.1007/978-3-319-11752-2_13	computer vision;prior probability;motion capture;artificial intelligence;optical flow;segmentation;video camera;computer science	Vision	43.697658929187575	-46.71827385628876	140455
b69094fd347ed2cd7ded9e7660300d6b8f00b769	self-organizing, adaptive data fusion for 3d object tracking	data fusion;object tracking;self organization	Data fusion concepts are a necessary basis for utilizing complex networks of sensors. A key feature for a robust data fusion system is adaptivity, both to be fault-tolerant and to run in a self-organizing manner. In this contribution a general framework for adaptive data fusion is established with object tracking as an application. The fusion algorithm of Democratic Integration is presented as one possible robust approach to the fusion task. As an alternative the STAPLE algorithm will be shown, which was previously only used for late classifier fusion. Extensions to apply the STAPLE algorithm for the fusion of probabilities will be introduced. Finally both algorithms will be evaluated on complex, realistic scenes to show their capabilities of self-organization and fault-tolerance.	algorithm;complex network;fault tolerance;organizing (structure);self-organization;sensor	Olaf Kähler;Joachim Denzler	2005			computer vision;video tracking;tracking system;artificial intelligence;sensor fusion;computer science	Robotics	46.387976077554974	-38.38711969627838	140508
9a5f57523ba6b48826ae6a8ffec77d341dcd4778	detection and motion estimation of moving objects based on 3d-warping	driver assistance;object motion estimation;optical scanners cameras driver information systems image segmentation motion estimation;image segmentation;motion segmentation three dimensional displays silicon feature extraction cameras current measurement image segmentation;optical scanners;depth sensor moving object detection moving object motion estimation 3d warping advanced driver assistance system forward collision warning stereo camera time of flight sensor 3d laser scanner;motion estimation;moving object detection;driver assistance moving object detection object motion estimation;driver information systems;cameras	The detection of objects in the surrounding environment is a key functionality for every Advanced Driver Assistance System (ADAS). Otherwise, it is impossible to realize any kind of advanced assistance functionality as e.g. a Forward Collision Warning. Besides static obstacles that limit the drivable area, dynamic objects can even be more dangerous due to their ego-movement. Therefore, a detection and also motion estimation of moving objects in the surrounding is essential for every ADAS. In this contribution we present a novel way to detect moving objects only with 3D-measurements of the current surrounding environment that we called 3D-Warping. But we are not only able to detect but also to provide the direction and speed of moving objects in world coordinates. The method is generic and can be applied on any kind of depth sensor, as e.g. a stereo-camera, time-of-flight, and 3Dlaser scanner.	architecture design and assessment system;glossary of computer graphics;matlab;motion estimation;prototype;stereo camera;structured-light 3d scanner;systems architecture;on-line system	Robert Kastner;Tobias Kühnl;Jannik Fritsch;Christian Goerick	2011	2011 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2011.5940401	computer vision;match moving;simulation;geography;computer graphics (images)	Robotics	45.27797224020966	-41.43977710844498	140579
f1d160add7d1d1d7ffe200a7a2b1275dd3879223	object detection using image reconstruction with pca	pedestrian safety;support vector machines;poison control;injury prevention;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;a priori knowledge;human factors;image reconstruction;principal component analysis;occupational safety;pedestrian detection;safety;detection rate;safety research;accident prevention;violence prevention;bicycle safety;support vector machine;poisoning prevention;falls;ergonomics;suicide prevention;object detection	In this paper, we present an object detection system and its application to pedestrian detection in still images, without assuming any a priori knowledge about the image. The system works as follows: in a first stage a classifier examines each location in the image at different scales. Then in a second stage the system tries to eliminate false detections based on heuristics. The classifier is based on the idea that Principal Component Analysis (PCA) can compress optimally only the kind of images that were used to compute the principal components (PCs), and that any other kind of images will not be compressed well using a few components. Thus the classifier performs separately the PCA from the positive examples and from the negative examples; when it needs to classify a new pattern it projects it into both sets of PCs and compares the reconstructions, assigning the example to the class with the smallest reconstruction error. The system is able to detect frontal and rear views of pedestrians, and usually can also detect side views of pedestrians despite not being trained for this task. Comparisons with other pedestrian detection systems show that our system has better performance in positive detection and in false detection rate. Additionally, we show that the performance of the system can be further improved by combining the classifier based on PCA reconstruction with a conventional classifier using a Support Vector Machine. 2007 Published by Elsevier B.V.	heuristic (computer science);iterative reconstruction;object detection;pedestrian detection;principal component analysis;sensor;statistical classification;support vector machine	Luis Malagón-Borja;Olac Fuentes	2009	Image Vision Comput.	10.1016/j.imavis.2007.03.004	margin classifier;support vector machine;computer vision;simulation;computer science;human factors and ergonomics;machine learning;data mining;computer security	Vision	40.777666632104335	-44.977729593009265	140659
aeb82895a5edcd12562b6f8bc1506f003712aaea	discriminative mean shift tracking with auxiliary particles	mean shift;particle filter;image sequence	We present a new approach towards efficient and robust tracking by incorporating the efficiency of the mean shift algorithm with the robustness of the particle filtering. The mean shift tracking algorithm is robust and effective when the representation of a target is sufficiently discriminative, the target does not jump beyond the bandwidth, and no serious distractions exist. In case of sudden motion, the particle filtering outperforms the mean shift algorithm at the expense of using a large particle set. In our approach, the mean shift algorithm is used as long as it provides reasonable performance. Auxiliary particles are introduced to conquer the distraction and sudden motion problems when such threats are detected. Moreover, discriminative features are selected according to the separation of the foreground and background distributions. We demonstrate the performance of our approach by comparing it with other trackers on challenging image sequences.	algorithm;bittorrent tracker;mean shift;particle filter	Junqiu Wang;Yasushi Yagi	2007		10.1007/978-3-540-76386-4_54	computer vision;particle filter;mean-shift;computer science;machine learning;pattern recognition;mathematics	Vision	43.1254674505176	-48.56727382156287	140823
21ad377aeb6ff099c5a59fa82cec95b0bba177bb	mcmc particle filter-based vehicle tracking method using multiple hypotheses and appearance model	probability;road vehicles markov processes maximum likelihood estimation monte carlo methods object tracking particle filtering numerical methods probability;maximum likelihood estimation;target tracking vehicles radar tracking roads object tracking three dimensional displays visualization;object tracking;markov processes;mcmc particle filter based multiple vehicle tracking method multiple object tracking accuracy markov chain monte carlo particle nlter maximum a posteriori probability estimation method track to multiple hypotheses association method appearance model multiple hypotheses model;monte carlo methods;particle filtering numerical methods;road vehicles	In this study, we propose a multiple vehicle tracking method using multiple hypotheses and the appearance model. The multiple hypotheses are associated with multiple tracks using track-to-multiple hypotheses association method. A target state is estimated using the maximum a posteriori probability estimation method. The posterior probability is proportional to the product of a priori probability and the likelihood that is calculated using similarities of multiple hypotheses and the appearance model. The posterior probability density function is estimated using the Markov chain Monte Carlo particle filter. An optimal posterior target state is determined using a sample with the maximum a posteriori probability. Our experimental results show that the proposed method can improve multiple objects tracking precision as well as multiple object tracking accuracy.	map;markov chain monte carlo;monte carlo method;one-to-many (data model);particle filter;vehicle tracking system	Young-Chul Lim;Dongyoung Kim;Chung-Hee Lee	2013	2013 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2013.6629618	econometrics;markov chain monte carlo;pattern recognition;mathematics;statistics	Vision	45.93178659295648	-48.08718851122319	140844
5f8a9a9607026550d9c416bf1204d68378149763	measuring the absolute distance of a front vehicle from an in-car camera based on monocular vision and instance segmentation				Liqin Huang;Yanan Chen;Zhengjia Fan;Zhifeng Chen	2018	J. Electronic Imaging	10.1117/1.JEI.27.4.043019	artificial intelligence;computer science;computer vision;segmentation;monocular vision	Vision	51.69900608168302	-42.55591771777713	141184
6d7ccca55aa4751df0367f8f0c457a5bd192e504	occlusion detecting window matching scheme for optical flow estimation with discrete optimization		Occlusion detection plays an important role in optical flow estimation and vice versa. We propose a single framework to simultaneously estimate flow and detect occlusion using novel support-weight based window matching. The proposed support-weight provides an effective clue to detect occlusion based on the assumption that the occlusion occupies relatively small portion in the window. By applying a coarse-to-fine approach we successfully address non-small occlusion problems as well. The proposed method also presents reasonable estimation for the flow for the occluded pixels. The energy model with the matching cost and flow regularization cost is optimized by an efficient discrete optimization method. Experiments demonstrate our method improves estimated flow accuracy compared to the method without occlusion detection, particularly on motion boundaries. It also yields highly competitive occlusion detection results, outperforming the previous state-of-the-art methods.	discrete optimization;mathematical optimization;optical flow;sensor	Kyong Joon Lee;Il Dong Yun	2017	Pattern Recognition Letters	10.1016/j.patrec.2017.02.009	computer vision;simulation;mathematics	Vision	47.794751704104364	-50.41433274558495	141242
b93d9102ca2dfda72c9f42e4838baf463adf84a9	pedestrian and vehicle detection and tracking with object-driven vanishing line estimation		To robustly detect people and vehicle on the road in a video sequence is a challenging problem. Most researches focus on detecting or tracking of specific targets only. On the contrary, instead of detecting vehicle or pedestrian individually, an integration framework combining the geometric information is proposed. The camera’s pitch angle is estimated with a novel vanishing line estimator. Not only detecting the vanishing point using line intersection approach, but also the object information from tracker are considered. Specifically, the detected vehicle or pedestrian will cast votes for the hypothesized horizon line. The vanishing line can be estimated even when the scenes are cluttered or crowded, and thus the geometric information can be estimated under challenging circumstance. In turn, such information of scene can help the system refine our detection results through Bayes’ network. Finally, to verify the performance of the system, comprehensive experiments have been conducted with the KITTI dataset. It is quite promising that the state-of-the-art detector, in our case, Regionlet detector, can be improved.		Yi-Ming Chan;Li-Chen Fu;Pei-Yung Hsiao;Shih-Shinh Huang	2016		10.1007/978-3-319-54407-6_29	computer science;computer vision;horizon;artificial intelligence;estimator;pattern recognition;convolutional neural network;line–line intersection;detector;bayes' theorem;vanishing point;pedestrian detection	Robotics	41.82741869401055	-45.610171064043946	141370
14261ffecd904d9aff97d98c70e49ec1d870112f	online learning based multiple pedestrians tracking in thermal imagery for safe driving at night	sensor fusion cameras computer vision dynamic programming graph theory image sequences night vision object tracking pedestrians road accidents;radar tracking computer vision sensors conferences object tracking accidents cameras;object tracking online learning multiple pedestrians tracking thermal imagery safe driving night time poor illumination driving eu 23 countries pedestrian vehicle accidents south korea advanced driver assistance system adas automatic pedestrian detection automatic pedestrian tracking night vision camera thermal energy weber fechner law image scaling search area global optimal solution boosted random ferns feedforward system global association system bipartite graph matching hungarian algorithm dynamic programming min cost max flow network flow computer vision video sequences moving camera moving multiple pedestrians sensor fusion	According to a report, night time and poor illumination driving is overall 2-3 times more dangerous then day time. For example, young people aged 18-24 were killed between 21:00 and 05:59 (the night-time and early morning) on week-days in the EU-23 countries because of road accident in 2010. As the similar pattern with EU, most pedestrian-vehicle accidents occur between 6 p.m. and 8 a.m., and the rate of pedestrian fatalities is highest between 4 a.m. and 6 a.m. in South Korea. Among several factors such as inebriated drivers and pedestrian, drowsiness, decreased visibility is the major cause of pedestrian-vehicle accident at night. To reduce the accident owing to driver's inattention at night, recent advanced driver assistance system (ADAS) has been researching on automatic pedestrian detection and tracking using night vision camera. Therefore, this tutorial focuses on introducing a multiple pedestrians tracking system using a thermal camera that is able to discern thermal energy at night-time. In a pedestrian tracking-by-detection system, multi-pedestrian detection accuracy is essential for post tracking process. Since the temperature difference between the pedestrian and background depends on the season and weather, we therefore first introduce two models for detecting pedestrians according to the season and weather, which are determined using Weber-Fechner's law. Two detection models use the optimal levels of the image scaling and search area instead of image pyramid to reduce the computational cost of image scaling for detecting multiple pedestrians of various sizes. Online learning is appropriate in the case that image frames is obtained sequentially. Theoretically offline learning could obtain global optimal solution while it is not as practical as online learning. Therefore, we introduce some state-of-the-art real-time online learning algorithms with our online learning based on boosted random ferns (BRFs) in detail based on the references as the second topic. Third, for association checking of multiple pedestrians, we explain the advantages and disadvantages of feed-forward system and global association system. Feed-forward system uses only current and past observations, which is called tracklet to estimate the current tracker's state. On contrary, global association system uses future and global information to estimate the current tracker's state in an offline step, for example, bipartite graph matching, Hungarian algorithm, dynamic programming, and min-cost max-flow network flow. Because maintaining the tracker's ID in successive frames is a challenging task owing to overlapping pedestrians in the multiple pedestrians tracking system, we introduce a few association checking algorithms to maintain the tracker's ID. As the feature for association checking, we also explain popular features in computer vision, such as the spatial proximity, velocity orientation and context (shape, size, color). Fourth, we introduce a few evaluation video sequences for pedestrian tracking such as OSli thermal pedestrian database, CVC-09 sequences, KAIST benchmark dataset, and KMliTD dataset. In particular, we are focusing on the KMliTD which contains video sequences involving a moving camera, moving multiple pedestrians, sudden shape deformations, unexpected motion changes, and partial or full occlusions between pedestrians at summer and winter night. Finally, we introduce the evaluation methods to measure the performance of the pedestrian tracking system such as Multiple Object Tracking Precision (MOTP) and Multiple Object Tracking Accuracy (MOTA), and Tracking Distance Error (TDE). The performance comparison among different tracking approaches is also presented when the proposed online tracking method is applied to benchmark data. As the further research in multiple pedestrians tracking, we will guide the fusion of sensors such as Radio Detection and Ranging (RADAR) sensor or Light Detection and Ranging (LIDAR) sensor with a camera for overcome the limitations occurred in a standalone sensor. In addition, with the increasing sensor resolutions, we mention the plan how to develop computationally feasible multi-extended object tracking algorithm.	algorithmic efficiency;architecture design and assessment system;benchmark (computing);computer vision;dynamic programming;flow network;hungarian algorithm;image scaling;machine learning;maxima and minima;maximum flow problem;offline learning;online and offline;pedestrian detection;pyramid (image processing);radar;real-time clock;sensor;tracking system;transparent data encryption;usability;velocity (software development);xbox live vision	Byoung Chul Ko;Joon Young Kwak;Jae-Yeal Nam	2016	2016 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2016.7535367	computer vision;simulation;geography;computer graphics (images)	Vision	42.285042500746535	-45.259683237698646	141418
0b92877d16110ebefcd4b2f02b13474d4e825d0e	environment modeling with stereo vision	image segmentation;mobile robot;confidence measure;image segmentation stereo image processing mobile robots;mobile robots;stereo vision layout mobile robots working environment noise image segmentation robot vision systems cameras data structures position measurement size measurement;stereo image processing;stereo vision;stereo pixels compact surface based environment models stereo vision images stereo camera mobile robot patchlets surface element data structure;data structure	We consider the problem of creating compact surface-based environment models from stereo vision images taken from a stereo-camera equipped mobile robot. The stereo images can be quite complex and correlation stereo suffers from considerable noise at ranges over a few metres. We construct the environment models by segmenting the scene viewed from a stereo camera into rectangular planar surfaces through the use of the patchlets surface element data structure. Patchlets are the projection of the stereo pixels onto detected surfaces in the scene. They have position, orientation, size and sensor-based confidence measures. The confidence measures allow proper weighting of patchlet parameters when aggregating patchlets into larger surfaces.	data structure;mobile robot;pixel;stereo camera;stereopsis;stereoscopy	Don Ray Murray;James J. Little	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389885	computer stereo vision;mobile robot;stereo cameras;stereo camera;computer vision;simulation;data structure;computer science;artificial intelligence;epipolar geometry;computer graphics (images)	Robotics	51.14235717863124	-40.2020708227693	141447
65d05a2b853d8ce6b90b0ecad7e73b8c127d6ae3	monocular pose determination from lines: critical sets and maximum number of solutions	orientation determination;image processing;3d image lines;position recovery;pose determination;three dimensional;computer vision;pose determination 3d image lines position recovery orientation determination computer vision critical sets camera;cameras equations laboratories internet geometrical optics;critical sets;internet;image reconstruction;image reconstruction computer vision image processing;cameras;camera;geometrical optics	A subpart of the following problem is considered. It is assumed that a set of known three-dimensional lines with an unknown pose and orientation are observed with a camera. The problem is to recover the position and orientation of the camera from the observed image lines, assuming that a correspondence has been established between the 2-D and the 3-D lines. It is shown that there exist infinite sets of three-dimensional lines such that no matter how many lines are observed in these sets the solution to the orientation or pose determination problem is not unique. The maximum number of possible solutions is given. These results clearly define the domain of validity of algorithms which solve the orientation or pose determination problem. >		Nassir Navab;Olivier D. Faugeras	1993		10.1109/CVPR.1993.340981	iterative reconstruction;geometrical optics;three-dimensional space;computer vision;the internet;3d pose estimation;image processing;computer science;mathematics;geometry	Vision	53.605688109474805	-50.98101197959931	141667
3faf9fc007af6c2efc93cdfc209c0a96313da862	robust image segmentation for overhead real time motorbike counting	traffic video sequences robust image segmentation overhead real time motorbike counting road traffic images foreground blobs laplacian densities;traffic counting;video signal processing image colour analysis image segmentation image sequences motorcycles real time systems road traffic;real time information;traffic monitoring motorbike segmentation counting;trajectory;bicycles;monitoring transportation logic gates;image analysis;traffic surveillance;width;video;height	Motorbikes are often difficult to detect in overhead road traffic images due to the variability of color, size, shape as well as trajectories. This paper tackles the problem of robust and real time image segmentation for motorbike counting. First of all, we perform background subtraction. Foreground blobs are then refined with Laplacian densities. This fusion enables to achieve a significant robustness to cast shadows. Thus, simple features, such as area, height and width, can be used to discriminate motorbikes from other vehicles. Our real time algorithm achieves interesting performances on multiple real traffic video sequences.	aggregate data;algorithm;background subtraction;closed-circuit television;color;image segmentation;laplacian matrix;overhead (computing);performance;real-time computing;simple features;spatial variability;statistical classification	Yohan Dupuis;Peggy Subirats;Pascal Vasseur	2014	17th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2014.6958183	computer vision;simulation;geography;computer graphics (images)	Vision	41.72031161299994	-45.58280224913789	141890
0adbe218b36c62167aa3b1ad567f3e9d8d1269f0	switchable constraints vs. max-mixture models vs. rrr - a comparison of three approaches to robust pose graph slam	graph theory;inference mechanisms;switches simultaneous localization and mapping trajectory robustness optimization measurement cities and towns;slam robots graph theory inference mechanisms;slam robots;false positive loop closure constraints switchable constraints max mixture models rrr realizing reversing recovering algorithm robust pose graph slam algorithm simultaneous localization and mapping outlier constraints place recognition perceptional aliasing robust robotic mapping inference methods data association failures factor graph based slam back ends	SLAM algorithms that can infer a correct map despite the presence of outliers have recently attracted increasing attention. In the context of SLAM, outlier constraints are typically caused by a failed place recognition due to perceptional aliasing. If not handled correctly, they can have catastrophic effects on the inferred map. Since robust robotic mapping and SLAM are among the key requirements for autonomous long-term operation, inference methods that can cope with such data association failures are a hot topic in current research. Our paper compares three very recently published approaches to robust pose graph SLAM, namely switchable constraints, max-mixture models and the RRR algorithm. All three methods were developed as extensions to existing factor graph-based SLAM back-ends and aim at improving the overall system's robustness to false positive loop closure constraints. Due to the novelty of the three proposed algorithms, no direct comparison has been conducted so far.	algorithm;aliasing;autonomous robot;correspondence problem;factor graph;mixture model;requirement;robotic mapping;simultaneous localization and mapping	Niko Sünderhauf;Peter Protzel	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6631320	computer vision;mathematical optimization;graph theory;machine learning;mathematics	Robotics	51.19631750538546	-45.87658631340289	141931
19dfea55ce8c6999415cb6216de0e5bd108a4f79	a mobile vision system for robust multi-person tracking	vision system;automatic failure detection;visual odometry;detectors;video signal processing;failure detection;camera rig;psi_visics;multiperson tracking;video sequences;mobile robots;layout;cognitive feedback loops;data mining;noise robustness;computer vision;egomotion;business environment;visualization;feedback;trajectory;estimation;video signal processing computer vision feedback image sequences tracking;feedback loop;machine vision;system integration;pedestrian detection;machine vision layout feedback loop object detection mobile robots noise robustness cameras data mining detectors video sequences;mobile communication;continuous visual odometry computation;robustness;experimental evaluation;depth estimation;mobile vision system;pedestrian tracking;cameras;tracking by detection;tracking;object detection;video sequences mobile vision system multiperson tracking continuous visual odometry computation tracking by detection pedestrian tracking egomotion camera rig pedestrian detection depth estimation cognitive feedback loops automatic failure detection;image sequences	We present a mobile vision system for multi-person tracking in busy environments. Specifically, the system integrates continuous visual odometry computation with tracking-by-detection in order to track pedestrians in spite of frequent occlusions and egomotion of the camera rig. To achieve reliable performance under real-world conditions, it has long been advocated to extract and combine as much visual information as possible. We propose a way to closely integrate the vision modules for visual odometry, pedestrian detection, depth estimation, and tracking. The integration naturally leads to several cognitive feedback loops between the modules. Among others, we propose a novel feedback connection from the object detector to visual odometry which utilizes the semantic knowledge of detection to stabilize localization. Feedback loops always carry the danger that erroneous feedback from one module is amplified and causes the entire system to become instable. We therefore incorporate automatic failure detection and recovery, allowing the system to continue when a module becomes unreliable. The approach is experimentally evaluated on several long and difficult video sequences from busy inner-city locations. Our results show that the proposed integration makes it possible to deliver stable tracking performance in scenes of previously infeasible complexity.	commonsense knowledge (artificial intelligence);computation;elegant degradation;experiment;fault tolerance;internationalization and localization;mobile operating system;negative feedback;object detection;pedestrian detection;visual odometry;way to go	Andreas Ess;Bastian Leibe;Konrad Schindler;Luc Van Gool	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587581	layout;mobile robot;computer vision;estimation;detector;simulation;visualization;mobile telephony;machine vision;computer science;visual odometry;trajectory;feedback loop;feedback;tracking;robustness;system integration	Vision	46.95936392562005	-45.204914401912276	142024
0af28c157c047f3aada6f3cb17a933bad67d7c50	embedded key-frame extraction for cg animation by frame decimation	databases;libraries;motion analysis;motion capturing data;image motion analysis;frame decimation;legged locomotion;computer graphics;data engineering;data mining;data encapsulation;character generation animation humans data mining motion analysis data engineering motion detection legged locomotion libraries databases;embedded key frame extraction;cg animation;compact representation;automatic detection;key frame selection;image representation;feature extraction;human motion;character generation;animation;human motion representation embedded key frame extraction computer graphics cg animation frame decimation key frame selection motion capturing data;humans;image representation computer animation data encapsulation feature extraction image motion analysis;computer animation;human motion representation;motion detection	This paper proposes a method for key-frame selection of captured motion data. In many cases, it is desirable to obtain a compact representation of the human motion. Key-framing is often used to express CG animation with a set of frames. In general, the animation is described by a set of curves that give the value of the rotation of all joints in each frame. Our method automatically detects the key-frames in captured motion data by using frame decimation. We decimate less important frames one by one, and then rank them by their importance. Our method has an embedded property, that is, all the frames are ranked by their importance, and thus users can specify any number of keyframes from one data set. We demonstrate the validity of our method in the experimental section by several typical motions such as walking and throwing	clay animation;computer animation;decimation (signal processing);embedded system;framing (world wide web);key frame;kinesiology	Shiyu Li;Masahiro Okuda;Shinichi Takahashi	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521693	computer vision;information engineering;computer science;computer animation;multimedia;computer graphics (images)	Robotics	39.35548423759117	-50.92149806582348	142049
6d41b5a24e18e4ac8e6842076cb64d4286572354	visual positioning systems — an extension to movips	databases;histograms;transformation matrix location based service cost effective indoor positioning method movips mobile visual indoor positioning system visual feature extract georeferenced image collection surf points smartphone dead reckoning approach orientation estimation image filtering distance estimation method image matching;vocabulary;accuracy;visualization;visualization databases estimation feature extraction histograms vocabulary accuracy;estimation;feature extraction;smart phones feature extraction image filtering image matching indoor navigation mobility management mobile radio	Due to the increasing popularity of location-based services, the need for reliable and cost-effective indoor positioning methods is rising. As an alternative to radio-based localization methods, in 2011, we introduced MoVIPS (Mobile Visual Indoor Positioning System), which is based on the idea to extract visual feature points from a query image and compare them to those of previously collected geo-referenced images. The general feasibility of positioning by SURF points on a conventional smartphone was already shown in our previous work. However, the system still faced several shortcomings concerning real-world usage such as request times being too high and distance estimation being unreliable because of the employed estimation method not being rotation invariant. In this paper, three extensions are presented that improve the practical applicability of MoVIPS. To speed up request times, both a dead reckoning approach (based on step counting using the accelerometer) and an orientation estimation (based on the smartphones compass) are introduced to filter relevant images from the database and thus to reduce the amount of images to compare the query image to. Furthermore, the vectors of the SURF points are quantized. For this purpose, clusters are calculated from all SURF points from the database. As a result, each image can be represented by a histogram of cluster frequencies, which can be compared with each other a lot more efficiently. The third extension is an improvement of the distance estimation method, which uses the matched feature points of an image to perform a perspective transformation and to determine the actual position with the aid of the transformation matrix.	dead reckoning;edge case;feature vector;global positioning system;image histogram;indoor positioning system;location-based service;odometry;quantization (signal processing);smartphone;speeded up robust features;transformation matrix	Chadly Marouane;Marco Maier;Sebastian Feld;Martin Werner	2014	2014 International Conference on Indoor Positioning and Indoor Navigation (IPIN)	10.1109/IPIN.2014.7275472	computer vision;simulation;geography;data mining	Robotics	49.98398936703471	-44.41635251970594	142125
b24e1d80fc06198ea575e1de175f074251ad3c5b	semantic hierarchy based reasoning chain algorithm for event detection on an intersection	traffic surveillance system;automatic incident detection;semantic routes;surveillance;road traffic;vehicle reasoning chain;automated highways;event detection;vehicle reasoning chain semantic hierarchy based reasoning chain event detection vision based automatic incident detection traffic surveillance system vehicle tracking algorithm highway monitoring system semantic routes;monitoring system;highway monitoring system;monitoring;event detection traffic control vehicle detection vehicle driving road transportation road accidents monitoring neural networks telecommunication traffic detection algorithms;mathematical models;roads;semantic hierarchy based reasoning chain;heavy traffic;vehicle tracking algorithm;traffic engineering computing;traffic surveillance;incident detection;collision avoidance;vehicle tracking;vision based automatic incident detection;interchanges and intersections;traffic engineering computing automated highways collision avoidance monitoring object detection road traffic surveillance tracking;tracking;object detection	"""In this paper, a method of vision-based automatic incident detection for traffic surveillance systems at crossroad is presented. We have developed a dedicated vehicle tracking algorithm based on the ST-MRF model (S. Kamijo and M. Sakauchi, 2002), and have done a successful work of incident detection for an high-way monitoring system (M. Harada et al., 2004) using the tracking algorithm. Here, we apply incident detection to a crossroad. Since the traffic situation on crossroad is much more complex than high-way road, an efficient method to classify the various vehicles' behaviors is required. Even though there is no white lines specify the routes for vehicles to drive in the central area of a crossroad, the vehicles will still follow some unseen routes. Here we call these unseen routes """"semantic routes"""". Our detection method detects the semantic routes and manages vehicles driving in the same semantic route as a """"vehicle reasoning chain"""", and mainly focuses on the first vehicle of a chain. This method can find an incident vehicle quickly in a heavy traffic situation with less false detects"""	algorithm	Shunsuke Kamijo;Xiaolu Liu	2006		10.1109/ITSC.2006.1706765	simulation;engineering;transport engineering;computer security	Logic	41.03327402223539	-44.3822875203763	142129
ae072e1cb757e14e84de907650936ae015e6054b	video-based driver assistance--from basic functions to applications	driver assistance;multi sensor fusion;image sequence analysis;real time image sequence analysis;image sequence;sensor fusion;lane recognition	Image sequences recorded with cameras mounted in a moving vehicle provide information about the vehicle's environment which has to be analysed in order to really support the driver in actual traffic situations. One type of information is the lane structure surrounding a vehicle. Therefore, we systematically developed and investigated driver assistance functions which make explicit use of the lane structure represented by lane borders and lane markings. With increasing computing power of standard PCs it was possible to realize more complex driver assistance with general purpose hardware. Investigations with a video-based lane departure warning system and a lane change assistant for highways will be discussed in detail. We integrated our lane keeping assistant in some experimental cars and performed systematic experiments in real traffic situations which enables the experience of video-based driver assistance on a high level--the action of a system. This allows us to assess whether a driver assistance system really understands the actual traffic situation which is the basis for reliable systems accepted by the user.	acoustic cryptanalysis;algorithm;device driver;experiment;high-level programming language;integrated circuit;radar;real-time clock;real-time computing;sensor;tracking system	Wilfried Enkelmann	2001	International Journal of Computer Vision	10.1023/A:1013658100226	computer vision;simulation;advanced driver assistance systems;computer science;sensor fusion	Mobile	44.169391178035326	-41.09322124735069	142143
a01a097de521c5f1c35c3322e256b463ac62c746	design of intelligent recognition system based on gait recognition technology in smart transportation	gait recognition;automatic fare collection;smart transportation;infrared sensor	With the rapid development of urban smart transportation, automatic fare collection system becomes more and more important. The related device based on recognition system in the smart transportation is a key equipment of automatic fare ticket system. It is part of the automatic fare collection system and passenger interface. Because of the complexity of the passenger traffic, how the intelligent recognition system can identify the passengers is a formidable challenge. In this paper, we design a sort of intelligent recognition system based on the simplified human gait recognition algorithm. Firstly, we analyze and study the existing identification system. After that we propose the improved and optimized algorithm and design layout of the sensors. And then, according to the motion including action, event and behavior, we propose a simplified method using infrared sensor based on XYT human gait recognition model. This system can effectively reduce the recognition system’s judgment and computation time, and improve the accuracy of judgment. Therefore, the proposed system has a certain application value.	algorithm;algorithmic efficiency;computation;evasion (network security);gait analysis;garbage collection (computer science);left 4 dead 2;sensor;throughput;time complexity	Jiachen Yang;Jianxiong Zhou;Dayong Fan;Haibin Lv	2016	Multimedia Tools and Applications	10.1007/s11042-016-3313-6	embedded system;simulation;computer security	Robotics	41.12593162954252	-43.98068292983958	142164
93d6fb3d23c929bef56192203a3130af8d3bcbf2	adaptive sparse mixture particle filter		We present a novel joint detection and tracking algorithm using raw measurements, in a compressed sensing framework. The sparse vector representing the state space is directly reconstructed, which transforms the nonlinear estimation problem into a linear one through sparse representation. A number of significant grids are obtained based on the sparse vector, indicating the positions of multiple potential targets in the state space. Therefore, the multi-model posterior distribution of the state can be sparsely represented by a number of modes centering around the significant grids at each scan. Consequently, a novel algorithm named sparse mixture particle filter is proposed in this work, which provides a sparse representation of the multi-model posterior distribution by identifying the significant grids. Furthermore, a novel adaptive sparse mixture particle filter algorithm is proposed to tackle the high coherence and high computation burden problems, by constructing a compact dictionary based on the state space with low resolution. The simulation results show that the proposed adaptive sparse mixture particle filter based joint detection and tracking algorithm can successfully detect and track multiple targets, which appear and disappear at different times, as well as track closely spaced targets with similar dynamic model.	clutter;compressed sensing;computation;dictionary;image resolution;mathematical model;multi-model database;nonlinear system;particle filter;peterson's algorithm;simulation;sparse approximation;sparse matrix;state space	Jing Liu;XiaoChao Li	2017	2017 20th International Conference on Information Fusion (Fusion)	10.23919/ICIF.2017.8009621	computer vision;machine learning;compressed sensing;radar tracker;particle filter;state space;nonlinear system;sparse approximation;coherence (physics);posterior probability;artificial intelligence;pattern recognition;mathematics	Vision	46.17196499660932	-47.6747039181892	142321
9070045c1a9564a5f25b42f3facc7edf4c302483	everybody needs somebody: modeling social and grouping behavior on a linear programming multiple people tracker	image edge detection trajectory optimization target tracking force linear programming;tracking system;edge detection;group behavior;force;data association;computer vision;subject detection social behavior grouping behavior linear programming multiple people tracker pedestrian motion global optimization scheme data association problem computer vision tasks;trajectory;image edge detection;linear programming;linear program;global optimization;optimization;people tracking;trajectory optimization;target tracking;object detection;target tracking computer vision linear programming object detection	Multiple people tracking consists in detecting the subjects at each frame and matching these detections to obtain full trajectories. In semi-crowded environments, pedestrians often occlude each other, making tracking a challenging task. Most tracking methods make the assumption that each pedestrian's motion is independent, thereby ignoring the complex and important interaction between subjects. In this paper, we present an approach which includes the interaction between pedestrians in two ways: first, considering social and grouping behavior, and second, using a global optimization scheme to solve the data association problem. Results on three challenging publicly available datasets show our method outperforms state-of-the-art tracking systems.	algorithm;correspondence problem;flow network;global optimization;kalman filter;linear programming;mathematical optimization;persistent data structure;recursion;semiconductor industry;sensor;tracking system	Laura Leal-Taixé;Gerard Pons-Moll;Bodo Rosenhahn	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130233	trajectory optimization;computer vision;simulation;edge detection;tracking system;computer science;linear programming;trajectory;machine learning;mathematics;force;group dynamics	Vision	46.51611689280226	-46.75768767355597	142408
3125df863013fd612b22bf436485d9104cfd3e22	vehicle tracking in outdoor environment based on curvelet domain		Vehicle tracking is a difficult part in intelligent traffic system. The images of vehicles on the streets, picked up from cameras, are usually in occlusion because of effecting outdoor environment such as lack light, weather, etc. Therefore, vehicle tracking is a challenging problem. This paper proposed a method for vehicle tracking in an outdoor environment. We use curvelet transform combined with object deformation of contour. The light of background may change from this frame to the other frame. The proposed algorithm has significantly improves the edge accuracy and reduces the wrong position of objects between the frames. For demonstrating the superiority of the proposed method, we have compared the results with the other methods.	curvelet;vehicle tracking system	Thanh Binh Nguyen	2014		10.1007/978-3-319-15392-6_34	computer vision	Robotics	42.54809616126532	-45.27789415068441	142427
987a8671c196dc2743028369019df2da1a80a11c	prior-based facade rectification for ar in urban environment	cameras image segmentation yttrium bayes methods image edge detection histograms buildings;support vector machines augmented reality bayes methods image processing inference mechanisms minimisation;3d stereo scene analysis;greedy rectangular min cut technique facade rectification augmented reality ar urban environment bayesian inference manhattan direction camera coordinate system support vector machine svm based procedure;augmented reality	We present a method for automatic facade rectification and detection in the Manhattan world scenario. A Bayesian inference approach is proposed to recover the Manhattan directions in camera coordinate system, based on a prior we derived from the analysis of urban datasets. In addition, a SVM-based procedure is used to identify right-angle corners in the rectified images. These corners are clustered in facade regions using a greedy rectangular min-cut technique. Experiments on a standard dataset show that our algorithm performs better or as well as state-of-the-art techniques while being much faster.	computation;experiment;frame language;greedy algorithm;image rectification;maximum cut;minimum cut;rectifier (neural networks);triplet state	Antoine Fond;Marie-Odile Berger;Gilles Simon	2015	2015 IEEE International Symposium on Mixed and Augmented Reality Workshops	10.1109/ISMARW.2015.25	computer vision;augmented reality;computer science;machine learning;computer graphics (images)	Vision	46.4128830549259	-50.31937075492314	142463
35a97567918351b07efd7bebb88d243a3ee26a26	robust vehicle edge detection by cross filter method	detectors;detectors decision support systems image edge detection abstracts vehicles;prewitt detector;filtering process robust vehicle edge detection cross filter method visual surveillance vehicle tracking vehicle identification traffic incident detection traffic control traffic management edge locations geometrical shape changes pixel value two phase filtering approach vehicle images classical edge detectors canny detector prewitt detector roberts detector sobel detector spatial relationship intensity change;edge detection;vehicle images;cross filter;image edge detection;abstracts;decision support systems;traffic engineering computing edge detection image colour analysis image filtering road vehicles surveillance;vehicles;roberts detector;sobel detector;sobel detector edge detection vehicle images cross filter canny detector prewitt detector roberts detector;canny detector	In visual surveillance, vehicle tracking and identification is very popular and applied in many applications such as traffic incident detection, traffic control and management. Edge detection is the key to the success of vehicle tracking and identification. Edge detection is to identify edge locations or geometrical shape changes in term of pixel value along a boundary of two regions in an image. This paper aims to investigate different edge detection methods and introduce a Cross Filter (CF) method, with a two-phase filtering approach, for vehicle images in a given database. First, four classical edge detectors namely the Canny detector, Prewitt detector, Roberts detector and Sobel detector are tested on the vehicle images. The Canny detected image is found to offer the best performance in Phase 1. In Phase 2, the robust CF, based on a spatial relationship of intensity change on edges, is applied on the Canny detected image as a second filtering process. Visual and numerical comparisons among the classical edge detectors and CF detector are also given. The average DSR of the proposed CF method on 10 vehicle images is 95.57%.	ansi escape code;canny edge detector;deriche edge detector;edge detection;mask (computing);numerical analysis;pixel;prewitt operator;resultant;roberts cross;sensor;sobel operator;two-phase locking;vehicle tracking system	Katy Po Ki Tang;Henry Y. T. Ngan	2014	2014 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)	10.1109/AIPR.2014.7041898	computer vision;electronic engineering;simulation;edge detection;image gradient;engineering;deriche edge detector;canny edge detector	Vision	43.42031080593987	-44.67996214536597	142626
89d6f7846338eff1cdcd32d8f1c19f7ddb280606	camera motion estimation by geometric aic for factorization with missing data	structure from motion missingdata geometric aic affine space low rank matrix;motion estimation;affine space camera motion estimation geometric aic factorization missing data estimation feature point tracking computer vision feature point trajectories;motion estimation cameras computer vision;computer vision;cameras trajectory transmission line matrix methods computer vision streaming media tracking image reconstruction;cameras	Estimating missing data of feature point tracking is one of the important task in computer vision. For this, many methods have been proposed. Many of them fit an affine space to feature point trajectories, or factorize the input data with missing data into a low-rank matrix. We focus on the fact that the dimension of an affine space and the rank of a factorized matrix varies according to camera motions. We propose a method for estimating camera motions by using the geometric AIC and optimally estimate missing data according to camera motions.	akaike information criterion;computer vision;missing data;motion estimation	Ryosuke Ikeuchi;Yasuyuki Sugaya	2013	2013 2nd IAPR Asian Conference on Pattern Recognition	10.1109/ACPR.2013.134	computer vision;mathematical optimization;camera matrix;mathematics;geometry;motion field	Vision	52.85666386218659	-50.078063058268434	142671
88b83859a7e396b21fd05c8b621bb6a0afa3698e	feature detection based on epipolar constrains	image features;detectors;feature detection;affine invariant feature detection;imu;search space;image matching;epipolar image;inertial measure unit;geometry;computational geometry;image matching epipolar constrains affine invariant feature detection imu inertial measure unit aerotriangulation epipolar geometry harris detector harris epipolar;inertial measurement unit;epipolar geometry;accuracy;feature extraction detectors geometry transforms cameras vehicles accuracy;harris detector;feature extraction;affine transforms;transforms;epipolar image affine invariant feature detection;vehicles;affine invariant;exterior orientation;harris epipolar;epipolar constrains;cameras;invariant feature;aerotriangulation;image matching affine transforms computational geometry feature extraction	Affine invariant features detection under large viewpoint change is a challenge problem in photography recent years. With wide application of IMU (inertial measure unit), the elements of exterior orientation can be obtained before aerotriangulation, and then epipolar geometry can be obtained. Besides, in many applications, it is easy to get epipolar geometry, but it is hard to build matching images. Hence the method is proposed to detect fully affine invariant features using Harris detector and match to each other quickly and precisely in the epipolar image, we named it Harris-Epipolar. Feature detection based on epipolar constrains takes full advantage of the characteristics of easy access to the elements of exterior orientation in current conditions and reduces the search space sharply during image matching. It is greatly improved the matching efficiency and accuracy.	accessibility;camera resectioning;distortion;epipolar geometry;feature detection (computer vision);feature detection (web development);harris affine region detector;image registration;simulation	Bingxuan Guo;Qiaohui Feng;Linhui Li;Zhichao Zhang	2010	2010 18th International Conference on Geoinformatics	10.1109/GEOINFORMATICS.2010.5568135	computer vision;topology;mathematics;geometry;epipolar geometry	Vision	50.861459081088434	-48.056756515677904	142843
bf19f8669795669c27b675d6005d25e527077f0a	object tracking based on particle filter with multi-scale mode	scale estimation particle filter based tracker multiscale mode scale changeable object tracking problem tracker deficiency many to one searching strategy tracking test video clips object scale change tracking precision tracking efficiency precise location;computer vision;multi scale mode;particle filter;object tracking;visual tracking;estimation error vectors search problems convergence object tracking histograms;multi scale mode computer vision visual tracking particle filter;particle filtering numerical methods object tracking;particle filtering numerical methods	"""In this paper, we propose a particle filter based tracker using multiscale mode to solve the scale changeable object tracking problem. Firstly, we discuss the traditional tracker's deficiency in dealing with scale changeable object and propose the multiscale mode. Then, we design our tracker under the particle filter framework using the proposed mode and """"many to one"""" searching strategy. Finally, we compare our tracker with some existing trackers by tracking test on some video clips, which contain three categories of object scale change. Simulation results indicate that the proposed tracker distinctly improves both the tracking precision and efficiency. Compared with the traditional particle filter tracker, our tracker needs fewer particles and obtains more precise location and scale estimation."""	angular defect;particle filter;simulation;software bug;video clip	Wangsheng Yu;Xiaohua Tian;Zhiqiang Hou;Guojian Wei;Wu Li	2013	2013 Seventh International Conference on Image and Graphics	10.1109/ICIG.2013.79	computer vision;simulation;particle filter;tracking system;eye tracking;computer science;video tracking;control theory	Robotics	44.66358107964979	-47.79658942857707	142913
57f3a6ce22c050ca78146b6cc5475c46789647e3	virtual sensors determined through machine learning		We propose a method that increases the capability of a conventional sensor, transforming it into an enhanced virtual sensor. This paper focuses on a virtual thermal Infrared Radiation (IR) sensor based on a conventional visual (RGB) sensor. The estimation of thermal IR images can enhance the ability of terrain classification, which is crucial for autonomous navigation of rovers. The estimate in IR from visual band has inherent limitations, as these are different bands, yet correlations between visual RGB and thermal IR images exist, as different terrains, which visually may appear different, also have different thermal inertia. This paper describes the developed deep learning-based algorithm that estimates thermal IR images from RGB images of terrains, providing the feasibility of the idea with average 1.21 error [degree Celsius].	algorithm;autonomous robot;deep learning;machine learning;sensor	Yumi Iwashita;Adrian Stoica;Kazuto Nakashima;Ryo Kurazume;Jim Torresen	2018	2018 World Automation Congress (WAC)	10.23919/WAC.2018.8430480	control engineering;artificial intelligence;computer vision;degree celsius;deep learning;infrared;rgb color model;engineering	HCI	46.96523369177321	-39.29013264777721	142995
46402562492f08274c62405f08f0c12f1fa246e8	an icp variant using a point-to-line metric	optimisation;algorithmic optimization;convergence;point to line metric;algorithmic optimization point to line metric iterative closest corresponding point variant finite number iterative dual correspondences;iterative algorithms;nonlinear control systems;localization;surface fitting;idc;metric;usa councils;indexing terms;robots iterative methods optimisation;mbicp;point to line;iterative methods;point to line scan matching localization icp idc mbicp metric;iterative dual correspondences;scan matching;robots;iterative closest corresponding point variant;robustness;source code;iterative closest point algorithm surface fitting robots robotics and automation usa councils iterative algorithms iterative methods robustness convergence nonlinear control systems;iterative closest point algorithm;icp;finite number;robotics and automation	This paper describes PLICP, an ICP (iterative closest/corresponding point) variant that uses a point-to-line metric, and an exact closed-form for minimizing such metric. The resulting algorithm has some interesting properties: it converges quadratically, and in a finite number of steps. The method is validated against vanilla ICP, IDC (iterative dual correspondences), and MBICP (Metric-Based ICP) by reproducing the experiments performed in Minguez et al. (2006). The experiments suggest that PLICP is more precise, and requires less iterations. However, it is less robust to very large initial displacement errors. The last part of the paper is devoted to purely algorithmic optimization of the correspondence search; this allows for a significant speed-up of the computation. The source code is available for download.	algorithm;computation;data center;displacement mapping;download;experiment;iteration;mathematical optimization;rate of convergence	Andrea Censi	2008	2008 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2008.4543181	robot;mathematical optimization;combinatorics;index term;internationalization and localization;convergence;finite set;metric;computer science;mathematics;geometry;iterative method;robustness;source code	Robotics	50.83171064192213	-51.23704614973795	142999
444b714987512a11346811e04cd36a72aa8b4d5f	particle filtering with rendered models: a two pass approach to multi-object 3d tracking with the gpu	filtering;opengl implementation;paper;concurrent computing;3d model reconstructions;multicamera scenarios particle filtering two pass approach 3d multiobject 3d tracking gpu appearance based particle filters 3d model reconstructions opengl implementation off screen framebuffers programmable vertex shaders;two pass approach;gpu;off screen framebuffers;state estimation;tracking particle filtering numerical methods rendering computer graphics;appearance based particle filters;computer vision;multicamera scenarios;computational modeling;3d model;image generation;particle filter;three dimensional displays;filtering particle tracking rendering computer graphics particle filters target tracking graphics optimization methods concurrent computing state estimation humans;solid modeling;object tracking;graphics processors;particle filtering;parallel computer;nvidia;nvidia geforce 8800 gt;humans;opengl;computer science;particle tracking;particle filters;target tracking;rendering computer graphics;3d multiobject 3d tracking;programmable vertex shaders;3d reconstruction;cameras;graphics;tracking;particle filtering numerical methods;normalized cross correlation;optimization methods;rendering	We describe a new approach to vision-based 3D object tracking, using appearance-based particle filters to follow 3D model reconstructions. This method is targeted towards modern graphics processors, which are optimized for 3D reconstruction and are capable of highly parallel computation. We discuss an OpenGL implementation of this approach, which uses two rendering passes to update the particle filter weights. In the first pass, the system renders the previous object state estimates to an off-screen framebuffer. In the second pass, the system uses a programmable vertex shader to compute the mean normalized cross-correlation between each sample and the subsequent video frame. The particle filters are updated using the correlation scores and provide a full 3D track of the objects. We provide examples for tracking human heads in both single and multi-camera scenarios.	3d reconstruction;central processing unit;computation;cross-correlation;framebuffer;graphics processing unit;opengl;parallel computing;particle filter;polygonal modeling;rendering (computer graphics);shader	Erik Murphy-Chutorian;Mohan Manubhai Trivedi	2008	2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2008.4563102	computer vision;simulation;concurrent computing;particle filter;computer science;statistics;computer graphics (images)	Vision	47.04732297297997	-46.177072545057875	143094
611c44a1f7b8e953b7376b97049d614df6dce430	vanishing point-based line sampling for real-time people localization	image sampling;multicamera;video surveillance;people localization;real time;geometry;2 d 3 d line sampling;surveillance applications vanishing point based line sampling real time multicamera people localization method image foregrounds vertical line samples 3d scene human locations geometric refinement filtering procedures 3d line samples crowed scenes serious occlusion;vanishing point;image reconstruction;vanishing point 2 d 3 d line sampling multicamera people localization real time;cameras image color analysis image reconstruction target tracking real time systems humans image segmentation;article;video surveillance filtering theory geometry image reconstruction image sampling;filtering theory	In this paper, we propose a real-time multicamera people localization method based on line sampling of image foregrounds. For each view, these line samples are originated from the vanishing point of lines perpendicular to the ground plane. With these line samples, vertical line samples in the 3-D scene can be reconstructed for potential human locations. After some efficient geometric refinement and filtering procedures, the remaining qualified 3-D line samples are clustered and integrated for the identification of locations and heights of people in the scene. Both indoor and outdoor scenarios are examined to demonstrate the effectiveness of our approach in handling serious occlusion in crowed scenes. The average localization error of less than 15 cm for average viewing distance of 15m suggests that our method can be applied to a broad range of surveillance applications that require the real-time computation of localization without using special hardware for acceleration.	computation;filter (signal processing);real-time clock;real-time locating system;refinement (computing);sampling (signal processing);vanishing point;vertical bar	Kuo-Hua Lo;Jen-Hui Chuang	2013	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2013.2242592	iterative reconstruction;computer vision;simulation;vanishing point;mathematics;computer graphics (images)	Vision	49.31376370040117	-46.954723422237436	143164
44fbf01a8fd5b0b194cad4cff4ad84afa301ff76	estimating gaze direction of vehicle drivers using a smartphone camera	vehicle drivers;svm gaze classifier;safety enhancement;driver gaze direction estimation;vehicles cameras training monitoring face estimation accuracy;support vector machines;training;smart phones;capture geometry driver gaze direction estimation vehicle drivers smartphone camera driver assistance methods mobile devices safety enhancement driver attention indicator coarse head pose direction feature descriptor svm gaze classifier;gaze tracking;image classification;driver assistance methods;accuracy;support vector machines cameras driver information systems gaze tracking image classification pose estimation smart phones;estimation;monitoring;driver attention indicator;face;feature descriptor;coarse head pose direction;vehicles;capture geometry;driver information systems;mobile devices;cameras;smartphone camera;pose estimation	Many automated driver monitoring technologies have been proposed to enhance vehicle and road safety. Most existing solutions involve the use of specialized embedded hardware, primarily in high-end automobiles. This paper explores driver assistance methods that can be implemented on mobile devices such as a consumer smartphone, thus offering a level of safety enhancement that is more widely accessible. Specifically, the paper focuses on estimating driver gaze direction as an indicator of driver attention. Input video frames from a smartphone camera facing the driver are first processed through a coarse head pose direction. Next, the locations and scales of face parts, namely mouth, eyes, and nose, define a feature descriptor that is supplied to an SVM gaze classifier which outputs one of 8 common driver gaze directions. A key novel aspect is an in-situ approach for gathering training data that improves generalization performance across drivers, vehicles, smartphones, and capture geometry. Experimental results show that a high accuracy of gaze direction estimation is achieved for four scenarios with different drivers, vehicles, smartphones and camera locations.	algorithmic efficiency;categorization;computer vision;embedded system;feature model;global positioning system;haptic technology;machine learning;mobile device;real-time transcription;sensor;smartphone;visual descriptor	Meng-Che Chuang;Raja Bala;Edgar A. Bernal;Peter Paul;Aaron Burry	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2014.30	face;embedded system;support vector machine;computer vision;estimation;contextual image classification;simulation;pose;computer science;mobile device;mathematics;accuracy and precision;statistics	Vision	39.4238124827752	-43.78586034576365	143436
bfc279913086ef9017ef553939d75fae8dacbce2	online dense local 3d world reconstruction from stereo image sequences	image features;humanoid robot;image reconstruction stereo image processing image sequences cameras stereo vision humanoid robots surface reconstruction robot vision systems mobile robots tracking;path planning;robot navigation;motion estimation;motion capture;camera motion;navigation;navigation humanoid robots path planning robot vision stereo image processing image reconstruction image sequences motion estimation feature extraction;robot vision;humanoid robot 3d reconstruction stereo image sequence visual odometory;humanoid robots;feature extraction;image reconstruction;image sequence;stereo image processing;depth map;humanoid robot online dense local 3d world reconstruction stereo image sequences robot navigation stereo depth map calculation time sequential images 6dof camera motion estimation ransac motion capture visual odometry;3d reconstruction;image sequences	This paper describes an online 3D reconstruction system from stereo image sequences to obtain a dense local world model for robot navigation. The proposed method consists of three components: 1) stereo depth map calculation, 2) correspondence calculation in time sequential images by tracking raw image features, 3) 6DOF camera motion estimation by RANSAC and integrate depth map into 3D reconstructed model. We examined and evaluated our method in a motion capture environment for comparison. Finally experimental results of a humanoid robot H7 are denoted.	3d reconstruction;depth map;depth perception;humanoid robot;motion capture;motion estimation;random sample consensus;raw image format;robotic mapping	Satoshi Kagami;Yutaka Takaoka;Yusuke Kida;Koichi Nishiwaki;Takeo Kanade	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545432	computer vision;simulation;computer science;humanoid robot;artificial intelligence;computer graphics (images)	Robotics	51.855138250636415	-43.822161740095396	143792
656ef752b363a24f84cc1aeba91e4fa3d5dd66ba	robust open-set face recognition for small-scale convenience applications	confounding factor;customer service;face recognition;face modeling;distance metric	In this paper, a robust real-world video based open-set face recognition system is presented. This system is designed for general small-scale convenience applications, which can be used for providing customized services. In the developed prototype, the system identifies a person in question and conveys customized information according to the identity. Since it does not require any cooperation of the users, the robustness of the system can be easily affected by the confounding factors. To overcome the pose problem, we generated frontal view faces with a tracked 2D face model. We also employed a distance metric to assess the quality of face model tracking. A local appearance-based face representation was used to make the system robust against local appearance variations. We evaluated the system’s performance on a face database which was collected in front of an office. The experimental results on this database show that the developed system is able to operate robustly under real-world conditions.	facial motion capture;facial recognition system;laptop;prototype;real-time computing;real-time locating system;robustification;scalability	Hua Gao;Hazim Kemal Ekenel;Rainer Stiefelhagen	2010		10.1007/978-3-642-15986-2_40	computer vision;face detection;simulation;engineering;communication	Vision	41.122004989813114	-48.94030979876566	143835
80fd20d0b95210c124e3d33762d5ee00a2b26bdf	monocular camera fall detection system exploiting 3d measures: a semi-supervised learning approach	image motion analysis;fall detection;self calibration;semisupervised learning	Falls have been reported as the leading cause of injury-related visits to emergency departments and the primary etiology of accidental deaths in elderly. The system presented in this article addresses the fall detection problem through visual cues. The proposed methodology utilize a fast, real-time background subtraction algorithm based on motion information in the scene and capable to operate properly in dynamically changing visual conditions, in order to detect the foreground object and, at the same time, it exploits 3D space’s measures, through automatic camera calibration, to increase the robustness of fall detection algorithm which is based on semi-supervised learning. The above system uses a single monocular camera and is characterized by minimal computational cost and memory requirements that make it suitable for real-time large scale implementations.	algorithm;algorithmic efficiency;artificial intelligence;background subtraction;camera resectioning;error-tolerant design;real-time clock;real-time transcription;requirement;semi-supervised learning;semiconductor industry;supervised learning	Konstantinos Makantasis;Eftychios Protopapadakis;Anastasios D. Doulamis;Lazaros Grammatikopoulos;Christos Stentoumis	2012		10.1007/978-3-642-33885-4_9	computer vision;simulation;machine learning;computer graphics (images)	Vision	40.15477913533262	-44.85532166449276	143938
321141b90aa1f1a7ddb19a97e7cd1d8a5fcadc2e	high-precision bicycle detection on single side-view image based on the geometric relationship	ellipse detection;triangle detection;geometry;computer vision;algorithm;bicycle	Improving the safety of transportation systems attracts lots of attention. Researchers introduced their methods to detect and analyze the vehicle and the pedestrian on the road to accomplish this goal. However, the bicycle is also a significant factor of the safety on a road. In this paper, a bicycle detector for side-view image is proposed based on the observation that a bicycle consists of two wheels in the form of ellipse shapes and a frame in the form of two triangles. Through the proposed triangle detection algorithm, the bicycle model and the geometric constraints on the relationship between the triangles and ellipses, the computation is fast according to the sample implementation and the evaluation of the reduced data amount. Besides, the training process is unnecessary and only single image is required for our algorithm. The experimental results are also given in this paper to show the practicability and the performance of the proposed bicycle model and bicycle detection algorithm.		Yen-Bor Lin;Chung-Ping Young	2017	Pattern Recognition	10.1016/j.patcog.2016.10.012	computer vision;simulation;computer science;geometry;algorithm	Vision	43.02694508175841	-44.448587016786554	144167
33f81815dc3d4bbed26ebd30e4551d1a029ea090	minimum cost multi-way data association for optimizing multitarget tracking of interacting objects	l41513;decomposition;time measurement;radar tracking;video sequences;binary integer programming;data association;visualization;trajectory;linear programming;lagrange dual relaxation;target tracking;target tracking time measurement linear programming radar tracking trajectory visualization video sequences	This paper presents a general formulation for a minimum cost data association problem which associates data features via one-to-one, m-to-one and one-to-n links with minimum total cost of the links. A motivating example is a problem of tracking multiple interacting nanoparticles imaged on video frames, where particles can aggregate into one particle or a particle can be split into multiple particles. Many existing multitarget tracking methods are capable of tracking non-interacting targets or tracking interacting targets of restricted degrees of interactions. The proposed formulation solves a multitarget tracking problem for general degrees of inter-object interactions. The formulation is in the form of a binary integer programming problem. We propose a polynomial time solution approach that can obtain a good relaxation solution of the binary integer programming, so the approach can be applied for multitarget tracking problems of a moderate size (for hundreds of targets over tens of time frames). The resulting solution is always integral and obtains a better duality gap than the simple linear relaxation solution of the corresponding problem. The proposed method was validated through applications to simulated multitarget tracking problems and a real multitarget tracking problem.	aggregate data;correspondence problem;dual;duality gap;electron microscopy;frame (physical object);integer (number);integer programming;interaction;lagrange multiplier;lagrangian relaxation;linear programming relaxation;mental association;numerous;one-to-one (data model);optimizing compiler;physical object;polynomial;simulation;small;time complexity;transcutaneous electric nerve stimulation	Chiwoo Park;Taylor J. Woehl;James E. Evans;Nigel D. Browning	2015	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2014.2346202	mathematical optimization;radar tracker;simulation;visualization;computer science;linear programming;trajectory;control theory;mathematics;decomposition;time	Vision	50.56386311122978	-49.23830281309591	144266
2291fdca4037a97daaaaf9accc02e69e2150e8bf	stereo vision based on-road vehicle detection under illumination changing conditions using self quotient image		Today the many of automotive research groups study how to reduce vehicle accidents. For this reason, they have been developing the advanced driver assistance system (ADAS). In ADAS, the various sensors are used for recognizing the driving situations. For example, there are supersonic wave sensors and radar sensors and so on. In particular, in computer vision research groups, the vision sensors (ex. CCD, IR) are used for this. But it has some difficult problems because the vehicles are mainly driven in outdoors. The images captured by outdoors have various illumination conditions due to weather. It makes difficulty to detecting vehicles in images. In this paper, we introduce the vehicle detection method when the input images of system have illumination changes. We use the self quotient image (SQI) algorithm for illumination equalization. But SQI algorithm produces many false positive results. So we eliminate the false-positive results using stereo vision technique. In main section, we explain this method in detail. And we prove the proposed method has superior performance than existing systems using experiments.	algorithm;architecture design and assessment system;charge-coupled device;computer vision;experiment;image sensor;radar;stereopsis	Jonghwan Kim;Chung-Hee Lee;Young-Chul Lim	2012			computer stereo vision;computer vision;computer graphics (images)	Robotics	42.040516296275854	-44.445423391295456	144332
7c86860f1bc3150eb853a72148e3babe8154532c	anti-occlusion particle filter object-tracking method based on feature fusion		A new anti-occlusion particle filter object-tracking method based on feature fusion is proposed in this study. Colour and local binary pattern features are extracted and additively fused with a deterministic coefficient, which is calculated based on the difference between the object features and the background. An integral cumulative histogram is proposed to reduce the computational cost of feature extraction. A new occlusion determination method is proposed, and corresponding tracking strategies are also put forward for various occlusion conditions; in the case of partial occlusion, block tracking is carried out, and in the case of serious occlusion, the least-square method is used to predict the object position. Context Aware Vision using Image-based Active Recognition (CAVIAR) and Video Image Retrieval and Analysis Tool (VIRAT) video libraries are used to validate the method. The experimental results show that the proposed method can describe an object effectively and improve tracking stability and robustness under the occlusion conditions.	filter object;particle filter	Ruohong Huan;Shenglin Bao;Chu Wang;Yun Pan	2018	IET Image Processing	10.1049/iet-ipr.2017.1068	computer vision;artificial intelligence;robustness (computer science);local binary patterns;occlusion;video tracking;feature extraction;particle filter;mathematics;pattern recognition;image retrieval;histogram	Robotics	42.148496084285746	-49.44790965036841	144390
4ec202c99ea415456cf87e93539e04f030ddf360	scene wireframes sketching for unmanned aerial vehicles		Abstract This paper introduces novel insights to improve the state-of-the-art line-based unsupervised observation and abstraction models of man-made environments. The increasing use of autonomous UAVs inside buildings and around human-made structures demands new accurate and comprehensive representation of their operation environments. Most of the 3D scene abstraction methods use invariant feature point matching, nevertheless some sparse 3D point clouds do not concisely represent the structure of the environment. The presented approach is based on observation and representation models using the straight line segments. The goal of the work is a complete method based on the matching of lines, that provides a complementary approach to state-of-the-art methods when facing 3D scene representation of poor texture environments for future autonomous UAV. Oppositely to other recently published methods obtaining 3D line abstractions, the proposed method features 3D segment abstraction in the absence of a previously generated point based reconstruction. Another advantage is the ability to group the resulting 3D lines according to different planes, for exploiting coplanar line intersections. These intersections are used like feature points in the reconstruction process. It has been proved that this method exclusively based on lines can obtain spatial information in the adverse situations when a SIFT-like SfM pipeline fails to generate a dense point cloud.	aerial photography;unmanned aerial vehicle;website wireframe	Roi Santos;Xose Manuel Pardo;Xosé Ramón Fdez-Vidal	2019	Pattern Recognition	10.1016/j.patcog.2018.09.017	point set registration;line (geometry);point cloud;spatial analysis;computer vision;complete method;invariant (mathematics);pattern recognition;abstraction;artificial intelligence;mathematics	Vision	50.279520316258555	-43.95388668457323	144656
6db67c1b97a8458b60e8826e8166ceaf1c826050	errorcheck: a new method for controlling the accuracy of pose estimates	covariance analysis;pose error covariance pose estimates errorcheck computer vision pose refinement;pose estimation computer vision covariance analysis;computer vision;image edge detection accuracy valves training estimation robustness;pose estimation	In this paper, we present ErrorCheck, which is a new method for controlling the accuracy of a computer vision based pose refinement method. ErrorCheck consists of two parts: a method for validating the robustness of a pose refinement method towards false correspondences and a method for controlling the accuracy of a validated pose refinement method. ErrorCheck uses a theoretical estimate of the pose error covariance both for validating robustness and controlling the accuracy. We illustrate the first usage of ErrorCheck by applying it to state-of-the-art methods for pose refinement and some variations of these methods that turn out to be necessary to introduce.	best, worst and average case;computer vision;refinement (computing)	Preben H. S. Holm;Henrik Gordon Petersen	2011	The 5th International Conference on Automation, Robotics and Applications	10.1109/ICARA.2011.6144900	computer vision;pose;3d pose estimation;analysis of covariance;computer science;machine learning;pattern recognition	Robotics	48.57487523462544	-48.947411072041305	144759
8009957eb8a61034e5a51823824ac6497f9880f9	skin color detection for face localization in human-machine communications	biometrics access control;computer graphics;skin color detection;face detection skin man machine systems user interfaces computer interfaces navigation computer graphics layout cameras tracking;computer vision;skin color;computer vision user interface design 3d graphics scene camera viewpoint human machine communication performance face localization module head pose head movement tracking skin color detection human computer interface;face recognition;biometrics access control image colour analysis user interface management systems computer vision computer graphics tracking face recognition;image colour analysis;user interface design;user interface management systems;3d graphics;tracking	This paper presents the proposed user interface design for computers whereby users can navigate in 3D graphics scene and change camera viewpoint via head movement. This human-machine communication relies very much on the performance of its face localization module, which must determine head pose and track head movement. We have employed the skin color detection approach to face localization. The approach is studied and presented in this paper. The experimental results show that our chosen methodology, is very effective. Furthermore, we demonstrate that skin color detection approach can cope of the variations of skin color and lighting condition.	3d computer graphics;emoticon;face detection;thresholding (image processing);user interface design	Douglas Chai;Son Lam Phung;Abdesselam Bouzerdoum	2001		10.1109/ISSPA.2001.949848	user interface design;facial recognition system;computer vision;face detection;object-class detection;computer science;tracking;multimedia;computer graphics;3d computer graphics;computer graphics (images)	Vision	46.677532326257676	-43.5479795588348	144943
47618de05967e20b3eec3b3c9e563e08f6da51bc	infrared depth camera system for real-time european lobster behavior analysis.		European lobster is a highly treasured seafood, but aquaculture production based on traditional communal rearing practices has proved challenging for this species due to its inherent agonistic behavior. This paper presents a novel computer vision system that is designed for analysis of lobster behavior and can serve as a tool to assist selection of breeding stock in a prospective selective breeding program for European lobster. The automated tracking system provides large quantities of behavioral data for boldness and aggressiveness analysis, and the infrared light source causes less disturbance to the nocturnal animal under observance. In addition, because the object is recognized based on depth information instead of color or grayscale pattern recognition, there are no restrictions on the selection of color or material for the substrate in the experimental setup. This paper also contributes towards diminishing tracking error caused by water surface reflection and robust body orientation estimation in case of inaccurate body segmentation. We tested ten European lobsters sized between 25-30 cm to demonstrate the performance and effectiveness of our proposed algorithm.	algorithm;computer vision;cooperative breeding;grayscale;information theory;pattern recognition;prospective search;real-time transcription;substrate (electronics);tracking system	Sheng Yan;Jo Arve Alfredsen	2018		10.5220/0006723605960602	infrared;computer vision;computer science;artificial intelligence	Vision	45.84345540764692	-42.3930446182824	145107
cc7a101d76e972b98a685fd4655a9e76d7c27461	information fusion for uncertainty determination in video and infrared cameras system	uncertainty;measurement uncertainty;infrared observations;uncertainty estimation;decision process;information fusion;infrared;video and infrared cameras	In the paper I present a new method for measuring uncertainty based on multiple information fusion. A system composed of video and infrared cameras is used. From this system, the features of the observed objects are extracted, separate for the video and infrared cameras. Based on these characteristics, the uncertainty value for video and infrared observations are determined. Three information fusion methods are applied, and for each of them the aggregated uncertainty measure is determined. Finally, a global uncertainty estimation is obtained from the previous results, which can be used for the labeling decision process of recognizing an object.		Cornel Barna	2008		10.1007/978-3-540-85567-5_76	computer vision;simulation;geography;remote sensing	Vision	45.48752988911256	-46.3862613839482	145611
e61e36b42303941575e0376954e17c6c1731db3f	study on forest fires recognition and moving target tracking in video surveillance system	image recognition;video surveillance;ptz forest fires recognition moving target tracking;surveillance system;laoshan forest fire video surveillance system moving target tracking forest fires recognition human factors extremely complex natural factors passive prevention manpower resources material resources active management model automatic monitoring automatic alarming image recognition technology forest fire prevention analogous system disaster prevention;satisfiability;video surveillance image recognition target tracking;human factors;forest fire;digital video;target tracking;fires target tracking humans image recognition monitoring head skin	The forest fires are caused by extremely complex natural factors and human factors. How to convert from passive prevention taxed a lot of manpower and material resources into the active management model with automatic monitoring and alarming, it has been a technical problem for forest fires department. In digital video surveillance system, we used image recognition technology to track suspicious moving target and alarm in period of forest fire prevention, to recognize forest fire and alarm in period of forest fire monitoring. We selected two places and do some test in Laoshan forest fire video surveillance system for testing, the result of test is satisfied. It can be generally used in the analogous system for preventing disaster.	closed-circuit television;computer vision;digital video;human factors and ergonomics;the forest	Xiao-Yun Xiong;Bing Wang	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023697	computer vision;simulation;computer science;human factors and ergonomics;remote sensing;satisfiability	Robotics	43.46860401552242	-42.85528052717397	145939
12ce65bafd9fd535d9c3f9a725d144d898ec2d79	informed visual search: combining attention and object recognition	object recognition;image databases;mobile robot;technology;data collection;mobile robots;information visualization;teknikvetenskap;usa councils;human robot interaction;footwear;attention mechanism;training data;informed visual searching;engineering and technology;robot vision mobile robots object recognition;teknik och teknologier;robot vision;image search;humans;attention mechanism informed visual searching sequential object recognition problem mobile robot;object recognition robot vision systems mobile robots humans footwear training data cameras robotics and automation usa councils image databases;robot vision systems;robotics and automation;cameras;sequential object recognition problem	This paper studies the sequential object recognition problem faced by a mobile robot searching for specific objects within a cluttered environment. In contrast to current state-of-the-art object recognition solutions which are evaluated on databases of static images, the system described in this paper employs an active strategy based on identifying potential objects using an attention mechanism and planning to obtain images of these objects from numerous viewpoints. We demonstrate the use of a bag-of-features technique for ranking potential objects, and show that this measure outperforms geometric matching for invariance across viewpoints. Our system implements informed visual search by prioritising map locations and re-examining promising locations first. Experimental results demonstrate that our system is a highly competent object recognition system that is capable of locating numerous challenging objects amongst distractors.	database;mobile robot;outline of object recognition	Per-Erik Forssén;David Meger;Kevin Lai;Scott Helmer;James J. Little;David G. Lowe	2008	2008 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2008.4543325	mobile robot;computer vision;simulation;information visualization;computer science;engineering;artificial intelligence;object-oriented design;3d single-object recognition	Robotics	48.05879053119867	-39.247133894759685	145994
42f4e2faa11b8f1cf8ee766b6e9eb5488b1aec58	integrated edge and corner detection	gaussian corner detection edge detection;robotics corner detection feature matching gradient operator edge detection corner detector computer vision;corner detection;image edge detection detectors computer vision layout application software navigation robot vision systems intelligent systems intelligent structures anisotropic magnetoresistance;edge detection computer vision;edge detection;gaussian;robotics;feature matching;finite element;real time computer vision;computer vision;corner detector;gradient operator	Corner detection is used in many computer vision applications that require fast and efficient feature matching. For tasks such as robot localisation and navigation, the use of corners for matching is preferred over edges or other, larger, features. In recent years finite-element based methods have been used to develop gradient operators for edge detection that have improved angular accuracy over standard techniques. We extend this work to corner detection, enabling edge and corner detection to be integrated. We demonstrate that accuracy is comparable to well-known existing corner detectors, and that significantly reduced computation time can be achieved, making the approach appropriate for real-time computer vision and robotics.	analysis of algorithms;angularjs;computation;computer vision;corner detection;edge detection;gaussian blur;gradient;real-time clock;robotics;sensor;smoothing;time complexity;video post-processing	Sonya A. Coleman;Bryan W. Scotney;Dermot Kerr	2007	14th International Conference on Image Analysis and Processing (ICIAP 2007)	10.1109/ICIAP.2007.80	corner detection;computer vision;feature detection;scale space;simulation;object-class detection;edge detection;blob detection;computer science;machine learning;finite element method;deriche edge detector;gaussian;canny edge detector;robotics;gradient;interest point detection	Vision	48.54651103691687	-46.7263609656888	146029
7d19cd007bffabefa59de061b303da464711af22	accurate estimation of human body orientation from rgb-d sensors	belief networks;clutter;image motion analysis;image motion analysis belief networks clutter computer vision feature extraction image colour analysis image denoising;rgb d;human body orientation estimation;computer vision;dbns;image colour analysis;feature extraction;期刊论文;superpixel dbns human body orientation estimation rgb d;estimation feature extraction sensors histograms data mining geometry noise measurement;image denoising;superpixel;actigraphy algorithms artificial intelligence computer peripherals computer simulation humans image enhancement imaging three dimensional orientation pattern recognition automated posture transducers video games whole body imaging;cluttered environment human body orientation estimation rgb d sensors human behavior analysis computer vision body poses body appearances rgb d based orientation estimation dbns dynamic bayesian network system depth data noise reduction rgb d superpixels motion cue extraction method static cue extraction method partial occlusions illumination change	Accurate estimation of human body orientation can significantly enhance the analysis of human behavior, which is a fundamental task in the field of computer vision. However, existing orientation estimation methods cannot handle the various body poses and appearances. In this paper, we propose an innovative RGB-D-based orientation estimation method to address these challenges. By utilizing the RGB-D information, which can be real time acquired by RGB-D sensors, our method is robust to cluttered environment, illumination change and partial occlusions. Specifically, efficient static and motion cue extraction methods are proposed based on the RGB-D superpixels to reduce the noise of depth data. Since it is hard to discriminate all the 360 ° orientation using static cues or motion cues independently, we propose to utilize a dynamic Bayesian network system (DBNS) to effectively employ the complementary nature of both static and motion cues. In order to verify our proposed method, we build a RGB-D-based human body orientation dataset that covers a wide diversity of poses and appearances. Our intensive experimental evaluations on this dataset demonstrate the effectiveness and efficiency of the proposed method.	cns disorder;computer vision;dynamic bayesian network;evaluation;extraction;information;performance;shortest seek first;silo (dataset);web search engine;sensor (device)	Wu Liu;Yongdong Zhang;Sheng Tang;Jinhui Tang;Richang Hong;Jintao Li	2013	IEEE Transactions on Cybernetics	10.1109/TCYB.2013.2272636	computer vision;feature extraction;computer science;pattern recognition;clutter;computer graphics (images)	Vision	43.919695880083246	-46.889710511490286	146051
b7b5fd3e2cfc39967e389b974c1cb418b2bf1b8f	heuristic search for structural constraints in data association		The research on multi-object tracking (MOT) is essentially to solve for the data association assignment, the core of which is to design the association cost as discriminative as possible. Generally speaking, the match ambiguities caused by similar appearances of objects and the moving cameras make the data association perplexing and challenging. In this paper, we propose a new heuristic method to search for structural constraints (HSSC) of multiple targets when solving the problem of online multi-object tracking. We believe that the internal structure among multiple targets in the adjacent frames could remain constant and stable even though the video sequences are captured by a moving camera. As a result, the structural constraints are able to cut down the match ambiguities caused by the moving cameras as well as similar appearances of the tracked objects. The proposed heuristic method aims to obtain a maximum match set under the minimum structural cost for each available match pair, which can be integrated with the raw association costs and make them more elaborate and discriminative compared with other approaches. In addition, this paper presents a new method to recover missing targets by minimizing the cost function generated from both motion and structure cues. Our online multi-object tracking (MOT) algorithm based on HSSC has achieved the multi-object tracking accuracy (MOTA) of 25.0 on the public dataset 2DMOT2015 [1].	algorithm;correspondence problem;heuristic;loss function;mathematical model	Xiao Zhou;Peilin Jiang;Fei Wang	2017	CoRR		pattern recognition;discriminative model;machine learning;computer science;artificial intelligence;heuristic	Vision	48.053212250709414	-49.76549266347167	146133
8b1e94ef8fd7fd8bfa10a69bb4ebdff2daf75496	multi-target tracking using hybrid particle filtering	mode stratification;sample size;kalman filter control loop;target tracking entropy filtration image sequences kalman filters monte carlo methods;visual access control;posterior distribution approximation;sequential monte carlo method;sequential monte carlo filtering;kalman filters;head tracking;kalman filter;statistical significance;local relative entropy hybrid particle filtering multi target tracking sequential monte carlo filtering visual access control posterior target distribution kalman filter control loop posterior distribution approximation mode stratification;relative entropy;posterior distribution;particle filter;multi target tracking;stratification;particle tracking target tracking face detection portals filtering monte carlo methods access control entropy face recognition chemicals;entropy;access control;target tracking;filtration;posterior target distribution;local relative entropy;monte carlo methods;hybrid particle filtering;sequential monte carlo;image sequences	We address the problem of multi-target tracking based on sequential Monte Carlo filtering for a visual access control application. Sequential Monte Carlo methods are very suitable for approximating posterior distributions for single target tracking applications. However, tracking multiple targets is more difficult and critically depends on the ability to represent all statistically significant modes with a sufficient number of samples. Even when tracking a single target, controlling the effective sample size of the particle set only crudely estimates how well it approximates the posterior target distribution. In contrast, previous work demonstrates that using a Kalman filter control loop, which monitors the performance of the particle filter, can dramatically improve posterior distribution approximation in a dynamic fashion. This paper extends this principle to multi-target tracking by introducing a technique called mode stratification. In addition, a method to automatically augment and delete the number of modes using local relative entropy measures is introduced. Experiments applying the proposed technique for visual head tracking in an access control application illustrate the effectiveness of the method	access control;approximation;control system;kalman filter;kullback–leibler divergence;monte carlo method;motion capture;particle filter	Jens Rittscher;Nils Krahnstoever;Luis Galup	2005	2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05) - Volume 1	10.1109/ACVMOT.2005.81	kalman filter;econometrics;mathematical optimization;particle filter;computer science;mathematics;statistics	Vision	45.06071827541458	-48.718025541252096	146137
04b22444e540ec0043ca9c01362bdac3579779ec	abdominal palpation haptic device for colonoscopy simulation using pneumatic control	bladder pressure abdominal palpation haptic device colonoscopy simulation pneumatic control gastroenterology assistants team interaction skills haptic feedback forces user interactions sphygmomanometer bladder haptic interface fuzzy controller;fuzzy controller;medical simulation;hysteresis;haptic device;real time;gastroenterology;journal article;force;data model;haptic rendering medical simulation force feedback system design and analysis;haptic interfaces hysteresis force bladder colonoscopy gastroenterology data models;force feedback;haptic rendering;system design;bladder;dynamic response;colonoscopy;haptic feedback;haptic interfaces;user interaction;haptic interaction;data models;steady state;system design and analysis;haptic interface	In this paper, we describe the development of a haptic device to be used in a simulator aiming to train the skills of gastroenterology assistants in abdominal palpation during colonoscopy, as well as to train team interaction skills for the colonoscopy team. To understand the haptic feedback forces to be simulated by the haptic device, we conducted an experiment with five participants of varying BMI. The applied forces and displacements were measured and hysteresis modeling was used to characterize the experimental data. These models were used to determine the haptic feedback forces required to simulate a BMI case in response to the real-time user interactions. The pneumatic haptic device consisted of a sphygmomanometer bladder as the haptic interface and a fuzzy controller to regulate the bladder pressure. The haptic device showed good steady state and dynamic response was adequate for simulating haptic interactions. Tracking accuracy averaged 94.2 percent within 300 ms of the reference input while the user was actively applying abdominal palpation and minor repositioning.	aortic aneurysm, abdominal;arabic numeral 0;bladder tissue;brain–computer interface;controllers;distraction - pain management method;flow rate;haptic device component;haptic technology;hysteresis;interaction;interactive media;interactivity;kilopascal;large;muscle;palpation;pneumatic artificial muscles;pokeweed mitogens;pulse-width modulation;real-time transcription;repositioning (procedure);resultant;silicones;simulation;sphygmomanometers;steady state;training techniques;urinary bladder calculi (disorder)	Mario Cheng;Welber Marinovic;Marcus Watson;Sébastien Ourselin;Josh Passenger;Hans de Visser;Olivier Salvado;Stephan Riek	2012	IEEE Transactions on Haptics	10.1109/TOH.2011.66	control engineering;medical simulation;embedded system;simulation;computer science;engineering;artificial intelligence;haptic technology	Robotics	40.22089168285845	-38.105186033049826	146382
612be5ad60c3354f42cd3a093b5f448095f6de10	a robust framework for face contour detection from clutter background		When applying Chan–Vese (C–V) model to segment a face from an image, the result is always influenced by the initial position, especially in a real scenarios with clutter background. An improved skin tone detection model is proposed based on the Gaussian function, which could generate an accurate initial contour for C–V model. As the single Gaussian model (SGM) only uses the prior information to determine the likelihood of a pixel, it lacks adaptability for an image with lighting and poses variances. A more adaptive SGM (ASGM) is proposed in this paper that, by updating the model parameters according to the input image, could provide a more stable approximation of face region. And then, we apply the estimated initial contour to C–V model for precise face segmentation. Tests conducted on a public face dataset (220 images with pose and illumination changes) have shown that the accuracy of skin region detected by ASGM is at a ratio higher than 99%, which lays a good foundation for further segmentation by the C–V model. Extensive experiments have validated effectiveness and efficiency of our system in face segmentation.	approximation;chan's algorithm;clutter;experiment;illumination (image);pixel;second generation multiplex;v-model	Xun Gong;Xinxin Li;Lin Feng;Ran Xia	2012	Int. J. Machine Learning & Cybernetics	10.1007/s13042-011-0044-x	computer vision;simulation;speech recognition;geography	Vision	43.7479609784372	-49.344353691363935	146514
54f009886509ef2f433b78f9baa8f8b055f96327	aligning windows of live video from an imprecise pan-tilt-zoom robotic camera into a remote panoramic display	optical distortion;frames per second;spatial context;pan tilt zoom robotic camera;video streaming;robot vision systems cameras displays bandwidth animal behavior optical distortion delay image registration video sharing pixel;video signal processing;image alignment;pan tilt zoom;optimal estimation;selective sampling;animal behavior;image sensors;area of interest;video signal processing image sensors robots;image alignment algorithms;image registration;image variance density;displays;pixel;robots;image variance density remote panoramic display pan tilt zoom robotic camera human observers video stream image alignment algorithms spherical projection;video sharing;bandwidth;remote panoramic display;human observers;video stream;robot vision systems;cameras;spherical projection	A pan-tilt-zoom robotic camera can provide detailed live video of selected areas of interest within a large potential viewing field. To provide spatial context for human observers, it is desirable to insert the resulting live video into a large spherical panoramic display representing the entire viewing field. Accurate alignment of the video stream within the panoramic display is difficult due to small errors in the robot pan-tilt values and image distortion due to nonlinear projection. Existing image alignment algorithms cannot keep up with rapid changes in camera position. In this paper, we present a constant-time image alignment algorithm based on spherical projection and projection-invariant selective sampling that accurately registers paired images at 25 frames per second on a standard PC. Experiments suggest that the new alignment algorithm is faster than previous algorithms by a factor four or more. In a companion paper, we present a new calibration algorithm based on image variance density that optimally estimates camera pan-tilt parameters	algorithm;align (company);computation;data structure;digital image processing;distortion;image registration;laptop;map projection;microsoft windows;nonlinear system;pan–tilt–zoom camera;requirement;robot;sampling (signal processing);streaming media;television;time complexity	Ni Qin;Dezhen Song;Kenneth Y. Goldberg	2006	Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.	10.1109/ROBOT.2006.1642226	optimal estimation;robot;computer vision;simulation;computer science;image registration;artificial intelligence;spatial contextual awareness;image sensor;frame rate;map projection;bandwidth;pixel;computer graphics (images)	Robotics	49.60477674205982	-43.49233393142067	146617
787fb4dcc9989e0ff7cf990f18f6c96d0aec61bf	manismc: a new method using manifold modeling and sequential monte carlo sampler for boosting navigated bronchoscopy	sequential monte carlo sampler;bronchoscopic navigation;additional position sensor;bronchoscope motion;smc sampler;manifold modeling;new bronchoscope motion tracking;bronchoscopic video sequence;bronchoscopic scene;smc sampling;navigated bronchoscopy;bronchoscopic scene identification;new method	This paper presents a new bronchoscope motion tracking method that utilizes manifold modeling and sequential Monte Carlo (SMC) sampler to boost navigated bronchoscopy. Our strategy to estimate the bronchoscope motions comprises two main stages: (1) bronchoscopic scene identification and (2) SMC sampling. We extend a spatial local and global regressive mapping (LGRM) method to Spatial-LGRM to learn bronchoscopic video sequences and construct their manifolds. By these manifolds, we can classify bronchoscopic scenes to bronchial branches where a bronchoscope is located. Next, we employ a SMC sampler based on a selective image similarity measure to integrate estimates of stage (1) to refine positions and orientations of a bronchoscope. Our proposed method was validated on patient datasets. Experimental results demonstrate the effectiveness and robustness of our method for bronchoscopic navigation without an additional position sensor.	bronchoscopes;bronchoscopy;contraceptives, oral, sequential;estimated;frame (physical object);frame (video);monte carlo method;motion estimation;navigation;nonlinear dimensionality reduction;particle filter;patients;sampling (signal processing);sampling - surgical action;similarity measure;manifold	Xióngbiao Luó;Takayuki Kitasaka;Kensaku Mori	2011	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-23626-6_31	computer vision;simulation;computer science;computer graphics (images)	Vision	46.890226392022186	-47.60189541296593	146736
be6bc82507ffefa83f012e1a1650ee0c299e33c6	3d active appearance model alignment using intensity and range data	rgbd;active appearance model;human robot interaction;face alignment;3daam;head pose estimation	Active Appearance Models (AAMs) are widely used to match a shape and appearance model to an image. This paper extends the commonly used 2D shape model to 3D, and introduces an effective method for integrating alignment to RGB and 3D range images. The use of a three dimensional model allows accurate estimation of head orientation, shape and position. Existing approaches combining range and intensity data use a manually tuned weighting function to balance 2D and 3D alignments. We develop a method to guide the alignment based on the observed image properties and the sensor characteristics. Our approach is experimentally validated using two different sets of depth and RGB cameras. In our experiments we achieve stable alignment under wide angular head rotations of up to 80^o with a maximum improvement of 26% compared to the 3D AAM using intensity image and 30% improvement over the state-of-the-art 3DMM methods in terms of 3D head pose estimation.	active appearance model	Andreas Dopfer;Hao-Hsueh Wang;Chieh-Chih Wang	2014	Robotics and Autonomous Systems	10.1016/j.robot.2013.11.002	active shape model;human–robot interaction;computer vision;active appearance model;simulation;computer science;artificial intelligence;computer graphics (images)	Robotics	48.10350082642044	-48.794467086141246	146850
d7d375532982f12867f5b22e51db6e484db8ede3	egosampling: wide view hyperlapse from egocentric videos	videos;cameras;three-dimensional displays;legged locomotion;optical imaging;head;electronic mail	The possibility of sharing one’s point of view makes the use of wearable cameras compelling. These videos are often long, boring, and coupled with extreme shaking, as the camera is worn on a moving person. Fast-forwarding (i.e., frame sampling) is a natural choice for quick video browsing. However, this accentuates the shake caused by natural head motion in an egocentric video, making the fast-forwarded video useless. We propose EgoSampling, an adaptive frame sampling that gives stable, fast-forwarded, hyperlapse videos. Adaptive frame sampling is formulated as an energy minimization problem, whose optimal solution can be found in polynomial time. We further turn the camera shake from a drawback into a feature, enabling the increase in field of view of the output video. This is obtained when each output frame is mosaiced from several input frames. The proposed technique also enables the generation of a single hyperlapse video from multiple egocentric videos, allowing even faster video consumption.	3d reconstruction;computation;digital camera;energy minimization;fast forward;optical flow;point of view (computer hardware company);rendering (computer graphics);sampling (signal processing);shake;time complexity;wearable computer	Tavi Halperin;Yair Poleg;Chetan Arora;Shmuel Peleg	2018	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2017.2651051	shake;time complexity;optical imaging;computer vision;wearable computer;sampling (statistics);artificial intelligence;video tracking;computer science;video browsing;field of view	Vision	49.015438389716124	-44.016812410921254	147103
9b1e6410ea3a4892c3eb9dca14e933446bfb8d70	a new robust 3d motion estimation under perspective projection	nonlinear optics;image motion analysis;image coordinate normalization;perspective projection;motion estimation;layout;initial guess algorithm;camera motion;motion parameters;nonlinear systems;image sequences 3d camera motion estimation perspective projection optical flow image pair overdetermined nonlinear equations motion parameters initial guess algorithm image coordinate normalization;image sequence;robustness motion estimation cameras nonlinear equations nonlinear systems layout parameter estimation telecommunications nonlinear optics image motion analysis;robustness;nonlinear equations;optical flow;parameter estimation;nonlinear system;image pair;nonlinear equations motion estimation image sequences;cameras;3d camera motion estimation;overdetermined nonlinear equations;telecommunications;image sequences	In this paper, we present a new 3D camera motion estimation technique using optical flow froma pair of images taken under a perspective projection. The problem formulation leads to the solution of overdetermined nonlinear system of equations w.r.t. the motion parameters. By employing an efficient initial guess algorithm which uses a weak perspective projection and an image coordinate normalization technique, the nonlinear solution can be obtained robustly and accurately. The proposed method has been tested on both several synthetic and real image sequences. The results show that the performance of the proposed algorithm is quite superior to the conventional ones even under more general and noisy situations.	3d projection;algorithm;motion estimation;nonlinear system;optical flow;synthetic intelligence	Hye Ri Cho;Kyoung Mu Lee;Sang Uk Lee	2001		10.1109/ICIP.2001.958205	layout;nonlinear optics;computer vision;mathematical optimization;perspective;nonlinear system;computer science;motion estimation;optical flow;control theory;mathematics;estimation theory;motion field;robustness	Vision	52.764772313185375	-50.217127390474815	147126
194a5a04c606b46b4155322a7778ba02a032caf3	a computer vision system to detect diving cases in soccer		Recently, motion analysis systems have been getting a lot of attention due to their potential in human motion analysis, which has a wide range of applications. One of these applications is analyzing tackle scenes in soccer games. In a tackle scene, players occasionally tend to deceive the referee by intentionally falling to get a free or penalty kick. In this paper, we propose a system to program human body tracking in order to analyze tackle scenes in soccer games. The main idea behind this system is to determine whether the falling player in the tackle scene is attempting to deceive the referee (diving) or not. In this system, the tackle scene goes through five main stages of processing; identification of the falling player, extraction of tracking points, motion tracking, features extraction and scene classification. The tracking component is implemented using Kanade-Lucas-Tomasi optical flow with the aid of pyramid levels and forward-backward error algorithm, while the classification is carried out using Weka software with Naive Bayes tree (NB tree) classifier. The proposed system is implemented and its performance is experimentally tested. The results show a potential to detect diving cases (deceiving in falling), with a classification accuracy of 84%.	algorithm;computer vision;earthbound;experiment;kanade–lucas–tomasi feature tracker;kinesiology;naive bayes classifier;optical flow;pyramid (geometry);statistical classification;tomasi–kanade factorization;tracking system;video tracking;weka	Hana' Al-Theiabat;Inad A. Aljarrah	2018	2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2018.8364457	motion analysis;naive bayes classifier;machine vision;computer vision;software;match moving;optical flow;computer science;artificial intelligence	Robotics	39.50764862348126	-45.123938907396855	147250
4e53d3841703d4c774a8a5aafddf622f2e9e28e4	semantic octree: unifying recognition, reconstruction and representation via an octree constrained higher order mrf	octrees image recognition image reconstruction image representation image resolution markov processes;semantics;octrees three dimensional displays semantics labeling cameras image reconstruction computational modeling;computational modeling;three dimensional displays;image reconstruction;octree data structure semantic octree constrained higher order mrf computer vision community multiresolution image labelling problems holistic scene understanding robotics graphics communities 3d representations image recognition image reconstruction image representation 3d volume hierarchical robust markov random field object class labels kitti vision benchmark suite surface reconstruction data compression dynamic updates;cameras;labeling;octrees	On the one hand, mainly within the computer vision community, multi-resolution image labelling problems with pixel, super-pixel and object levels, have made great progress towards the modelling of holistic scene understanding. On the other hand, mainly within the robotics and graphics communities, multi-resolution 3D representations of the world have matured to be efficient and accurate. In this paper we bring together the two hands and move towards the new direction of unified recognition, reconstruction and representation. We tackle the problem by embedding an octree into a hierarchical robust PN Markov Random Field. This allows us to jointly infer the multi-resolution 3D volume along with the object-class labels, all within the constraints of an octree data-structure. The octree representation is chosen as this data-structure is efficient for further processing such as dynamic updates, data compression, and surface reconstruction. We perform experiments in inferring our semantic octree on the The kitti Vision Benchmark Suite in order to demonstrate its efficacy.	benchmark (computing);computer vision;data compression;data structure;experiment;graphics;holism;markov chain;markov random field;octree;pixel;robotics	Sunando Sengupta;Paul Sturgess	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139442	iterative reconstruction;computer vision;labeling theory;computer science;machine learning;semantics;sparse voxel octree;computational model;octree;computer graphics (images)	Robotics	49.785159176759265	-47.540458094324975	147362
ac21641f23567165fdf7b63227049bae54fdaff1	monocular heading estimation in non-stationary urban environment	estimation vectors training robustness adaptive optics optical imaging cameras;video signal processing;training;motion estimation;video signal processing cameras image sequences learning artificial intelligence motion estimation navigation regression analysis robot vision safety;navigation;optical imaging;lvm monocular heading estimation nonstationary urban environment visual cue human navigation robot automotive safety focus of expansion unstructured environment dynamic environment robust learning framework optical flow latent variable model outlier motion visual field of view bypass classical camera calibration monocular video footage foe numerical method regression mapping;robot vision;vectors;estimation;safety;robustness;regression analysis;learning artificial intelligence;cameras;adaptive optics;image sequences	Estimating heading information reliably from visual cues only is an important goal in human navigation research as well as in application areas ranging from robotics to automotive safety. The focus of expansion (FoE) is deemed to be important for this task. Yet, dynamic and unstructured environments like urban areas still pose an algorithmic challenge. We extend a robust learning framework that operates on optical flow and has at center stage a continuous Latent Variable Model (LVM) [1]. It accounts for missing measurements, erroneous correspondences and independent outlier motion in the visual field of view. The approach bypasses classical camera calibration through learning stages, that only require monocular video footage and corresponding platform motion information. To estimate the FoE we present both a numerical method acting on inferred optical flow fields and regression mapping, e.g. Gaussian-Process regression. We also present results for mapping to velocity, yaw, and even pitch and roll. Performance is demonstrated for car data recorded in non-stationary, urban environments.	camera resectioning;course (navigation);ground truth;kriging;lvm;latent variable model;microsoft outlook for mac;nonlinear system;numerical method;online and offline;optical flow;pitch (music);robotics;stationary process;velocity (software development);yaws	Christian Herdtweck;Cristóbal Curio	2012	2012 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2012.6343057	computer vision;simulation;geography;computer graphics (images)	Robotics	47.13636035629139	-47.24112234152787	147428
423aa4c1abe53d5e4d205fe7b558f4296fbeef4f	mirrobot: fast detection of body-parts in the scale-space of range images with an application to human motion copying	face detector;humanoid robot;android robot;grey level imaging sensor;hand detector;motion detection human robot interaction cognitive robotics robot sensing systems detectors face detection image sensors humanoid robots robustness head;color imaging sensor;motion estimation;upper torso movement;imitation;range imaging sensor;body part detection;mimicking;scale space;humanoid robots;human motion capture;human kinematics estimator;range image;human motion;human body;human body parts detection;object detection humanoid robots motion estimation;end point extraction;humanoid robots body part detection range imaging sensor scale space imitation mimicking human motion capture;mirrobot;upper torso movement mirrobot scale space range image human motion copying human body parts detection end point extraction human kinematics estimator face detector hand detector grey level imaging sensor color imaging sensor android robot;image sensor;object detection;human motion copying	In this paper we present a novel approach to fast detection of human body-parts that relates the idea of scale-space to range images. The resulting method is a pre-detector that extracts end-points at different scales that may correspond to the human's head or hands. The method is aimed to initialize more sophisticated human kinematics estimators and can be enhanced with known face or hand detectors based on color or grey-level imaging sensors. We apply our approach to the human motion copying problem (mimicking) and report on a set-up shown on two fairs in Germany in 2006. In the set-up an android robot is used to mimic observed human upper torso movements. We conclude by discussing results and point to further work	android (robot);kinesiology;scale space;sensor	Jens Kubacki;Ulrich Reiser	2006	ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2006.314447	computer vision;simulation;computer science;humanoid robot;artificial intelligence	Robotics	47.49200844474562	-40.96645880983823	147537
a827ca9496380011096a2dd58c6185d094dce7de	dissecting scale from pose estimation in visual odometry			3d pose estimation;visual odometry	Rong Yuan;Hongyi Fan;Benjamin B. Kimia	2017			artificial intelligence;pattern recognition;computer vision;computer science;visual odometry;pose	Robotics	51.94944318056408	-43.13084216213856	147579
cbeabf342f888a9ce931045a8d4c56ddf693bd2b	appearance based multiple agent tracking under complex occlusions	field of view;color space	Agents entering the field of view can undergo two different forms of occlusions, either caused by crowding or due to obstructions by background objects at finite distances from the camera. This work aims at identifying the nature of occlusions encountered in multi-agent tracking by using a set of qualitative primitives derived on the basis of the Persistence Hypothesis - objects continue to exist even when hidden from view. We construct predicates describing a comprehensive set of possible occlusion primitives including entry/exit, partial or complete occlusions by background objects, crowding and algorithm failures resulting from track loss. Instantiation of these primitives followed by selective agent feature updates enables us to develop an effective scheme for tracking multiple agents in relatively unconstrained environments. The agents are primarily detected as foreground blobs and are characterized by their centroid trajectory and a non-parametric appearance model learned over the associated pixel co-ordinate and color space. The agents are tracked through a three stage process of motion based prediction, agent-blob association with occlusion primitive identification and appearance model aided agent localization for the occluded ones. The occluded agents are localized within associated foreground regions by a process of iterative foreground pixel assignment to agents followed by their centroid update. Satisfactory tracking performance is observed by employing the proposed algorithm on a traffic video sequence containing complex multi-agent interactions.		Prithwijit Guha;Amitabha Mukerjee;K. S. Venkatesh	2006		10.1007/11801603_63	persistence;center of mass;computer vision;simulation;systems modeling;internationalization and localization;color image;field of view;system identification;occultation;computer science;artificial intelligence;trajectory;iterative and incremental development;tracking;color space;pixel	Vision	46.43126171818883	-47.36568730427276	147710
4540941c784f673911ad0745b089c0fb0e1d82a8	efficient vanishing point estimation for unstructured road scenes	tensile stress;scenes;training;unstructured;efficient;point;estimation;image edge detection;roads;image color analysis;vanishing;estimation error;road	Vanishing point estimation is an essential and demanding task in vision-based road detection. One of the main limitations of the existing approaches for vanishing point estimation is computation efficiency, which hampers their real-time applications. This paper presents an efficient method for finding the vanishing point in unstructured road scenes. Color tensors are applied on the input image to find texture orientations and color edges. We propose new strategies to select optimized sets of vanishing point candidates and voters and to define the voting function. The proposed method is evaluated on a benchmark dataset of 2000 images of unmarked pedestrian lanes. The experimental results show that it achieves accuracy comparable with other state-of-the-art methods but with significantly reduced computation time.	benchmark (computing);computation;orientation (graph theory);real-time clock;time complexity;vanishing point	Linh Nguyen;Son Lam Phung;Abdesselam Bouzerdoum	2016	2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2016.7797037	computer vision;point;mathematical optimization;estimation;simulation;mathematics;stress;statistics	Robotics	42.93746729445512	-45.27899944864211	147767
7996a71a86693a721e45ff9482d286a310ada3ad	multi-pn-learning for tracking applications	detectors;interpolation;object tracking computer vision learning artificial intelligence;accuracy;streaming media;computer vision multi pn learning tracking applications multitarget tracker video stream temporal disappearance general single target tracker knowledge learning;target tracking;benchmark testing detectors target tracking streaming media accuracy interpolation;benchmark testing	We present a general multi-target tracker able to simultaneously track, learn, and distinguish arbitrary objects in a single video stream and recognize them again after a temporal disappearance (reentering). We show how this tracker can be created as an extension of a general single-target tracker. Furthermore, we provide evidence that dissimilarities of tracked objects and relations in the learned knowledge can be exploited to improve individual tracking results.	streaming media	Jasper van de Ven;Arne Kreutzmann;Sascha Schrader;Frank Dylla	2014	2014 13th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2014.7064407	benchmark;computer vision;detector;simulation;tracking system;interpolation;computer science;machine learning;mathematics;accuracy and precision;statistics	Robotics	40.29245842519623	-48.08234370515354	148021
1f1ff163ac18f1d0eecc3d08a8eb3ea8bc6fb504	polynomial based approach in analysis and detection of surgeon's motions	decision tree;decision trees human motion recognition scrub nurse robot kohonen map adaboost;wrist;scrub nurse robot;motion estimation;polynomials;medical robotics;right handed;polynomials motion detection surges motion analysis surgery wrist laparoscopes algorithm design and analysis detection algorithms humans;scrub nurse robot polynomial based approach surgeon motions right hand wrist trajectory laparoscopic operation analysis detection algorithms kohonen maps boosted decision trees human motion recognition;self organising feature maps;human motion;robots;detection algorithm;adaboost;surgery;surgery gesture recognition medical robotics motion estimation polynomials self organising feature maps;kohonen map;humans;human motion recognition;point of view;switches;decision trees;gesture recognition;robot kinematics	In the paper a problem of analyzing surgeon's right hand wrist trajectory during laparoscopic operation is considered. Based on the results of the analysis detection algorithms to recognize six motions are developed. The motions can be considered as primitives of a surgery from the human scrub nurse point of view. In the analysis, to represent motions the third order polynomials are used. The two proposed algorithms are based on Kohonen maps and boosted decision trees. The performance of the algorithms is tested on surgical operation data.	algorithm;approximation;data scrubbing;decision tree;gradient boosting;language primitive;polynomial;robot;self-organizing map;sensor;teuvo kohonen	Janusz Jakubiak;Sven Nomm;Juri Vain;Fujio Miyawaki	2008	2008 10th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2008.4795588	computer vision;simulation;computer science;engineering;artificial intelligence;machine learning;decision tree;gesture recognition	Robotics	48.22138847615192	-38.692026810809224	148190
55572443f3474b71bc9f3a02c122b6c574749817	multiple plane tracking using unscented kalman filter	visual servoing convex programming kalman filters object tracking robot vision slam robots stability;visual slam;convex programming;kalman filters;convex optimization;convex optimization plane tracking unscented kalman filter visual servoing visual slam planar feature robotics literature homography computer vision;computer vision;stability;convex functions;planar feature;robotics literature;robot vision;estimation;homography;object tracking;plane tracking;robustness;global optimization;visual servoing;unscented kalman filter;slam robots;robustness kalman filters cameras noise estimation equations convex functions;cameras;noise	An important pre-requisite for many tasks like Visual Servoing and visual SLAM is the task of tracking the underlying features. The use of planar features for these purposes has gained importance recently. Complementing current planar tracking works in the robotics literature, which use multiple features, we formulate the tracking problem using multiple planes. Inspired by the maturity in understanding of geometric quantities like the homography in computer vision, we develop a system based on the Unscented Kalman Filter (UKF) that localizes the camera and estimates the plane parameters of a scene, using homographies as measurement. Homographies are estimated using tracked feature points. We show that this framework provides significant robustness and stability to the system under significant changes of illumination, occlusion etc. Finally, we also propose a Convex optimization based solution for the initialization of this system, which is capable of producing globally optimal estimates, and is a useful algorithm in its own right. Several synthetic and real results are presented to demonstrate the efficacy of our approach.	algorithm;capability maturity model;computer vision;convex optimization;homography (computer vision);kalman filter;mathematical optimization;maxima and minima;real-time clock;robotics;simultaneous localization and mapping;synthetic intelligence;visual servoing	Visesh Chari;C. V. Jawahar	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5650847	kalman filter;computer vision;convex optimization;simulation;control theory;mathematics	Robotics	50.80704401661708	-46.65386273319614	148278
3b325d95e39a9c4afbf417bf16fbdf8f6aa2904d	a hhmm-based approach for robust fall detection	fall person detection;pervasive home monitoring systems;image motion analysis;image formation;video signal processing;video sequences;image metric rectification;hidden markov models;automatic detection;hierarchical hidden markov model;robust method;home monitoring;robustness hidden markov models video sequences computerized monitoring tracking joining processes humans heart image motion analysis image sequence analysis;video signal processing hidden markov models image motion analysis image sequences object detection tracking;image formation fall person detection video sequences pervasive home monitoring systems motion modelling hierarchical hidden markov model person tracking image metric rectification;person tracking;motion modelling;tracking;object detection;image sequences	"""Automatic detection of a falling person in video sequences is an important part of future pervasive home monitoring systems. We propose here a robust method to achieve this goal. Motion is modeled by a hierarchical hidden Markov model (HHMM) whose first layer states are related to the orientation of the tracked person. Finding a consistent way for robustly linking the observation vector to the human poses is the heart of our contribution. In that sense, we carefully study the relationship between angles in the 3D world and their projection onto the image plane. After performing an initial image metric rectification, we derive theoretical properties making it possible to bound the error angle introduced by the image formation process for a standing posture. This allows us to confidently identify other poses as """"non-standing"""" ones, and thus to robustly analyze pose sequences against a given motion model. Several results illustrate the efficiency of the algorithm by pointing out its ability to accurately recognize a person falling down from another walking or sitting, as well as its capacity to run in an unspecified configuration"""	activity recognition;algorithm;apache axis;computation;concatenation;crystal structure;hierarchical hidden markov model;hough transform;image formation;image plane;image rectification;interaction;markov chain;optic axis of a crystal;poor posture;preprocessor;rectifier (neural networks);sensor	Nicolas Thome;Serge Miguet	2006	2006 9th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2006.345146	computer vision;simulation;computer science;machine learning;tracking;image formation;hidden markov model	Robotics	44.60729830332951	-45.19053410339198	148308
55fa8a57993e057edf0418dd469d306b0a6dce3e	a big bang–big crunch type-2 fuzzy logic system for machine-vision-based event detection and summarization in real-world ambient-assisted living	sensors;q science general;fuzzy logic;qa75 electronic computers computer science;event summarization interval type 2 fuzzy logic systems 3d machine vision;graphical user interfaces;three dimensional displays;feature extraction;cameras;sensors three dimensional displays graphical user interfaces feature extraction fuzzy logic cameras	The area of ambient-assisted living (AAL) focuses on developing new technologies, which can improve the quality of life and care provided to elderly and disabled people. In this paper, we propose a novel system based on 3-D RGB-D vision sensors and interval type-2 fuzzy-logic-based systems (IT2FLSs) employing the big bang-big crunch algorithm for the real-time automatic detection and summarization of important events and human behaviors from the large-scale data. We will present several real-world experiments, which were conducted for AAL-related behaviors with various users. It will be shown that the proposed BB-BC IT2FLSs outperform the type-1 fuzzy logic system counterparts as well as other conventional nonfuzzy methods, and the performance improves when the number of subjects increases.	atm adaptation layer;algorithm;bang file;experiment;fuzzy logic;image sensor;machine vision;real-time clock;the quality of life	Bo Yao;Hani Hagras;Daniyal M. Al-Ghazzawi;Mohammed J. Alhaddad	2015	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2016.2514366	fuzzy logic;computer vision;fuzzy electronics;feature extraction;computer science;sensor;artificial intelligence;machine learning;data mining;graphical user interface	Robotics	39.938063650870156	-42.27825397281385	148313
789a2ba0b2fa79d9cf9ba81e84959abb8f154ae9	nonlinear robust velocity estimation of vehicles from a snowfall traffic scene	estimation theory;object recognition;snow;road traffic;anisotropic diffusion;layout noise robustness image recognition road vehicles image sequences vehicle detection working environment noise noise shaping brightness snow;objective function;smoothing methods;estimation theory image sequences road traffic road vehicles snow object detection object recognition smoothing methods;region of interest;image sequence;weather condition;velocity estimation;object detection;low contrast images nonlinear robust velocity estimation snowfall traffic scene moving vehicles road traffic image sequences misty weather conditions snowy weather conditions discontinuities occlusions noise brightness vehicles recognition two stage method nonlinear smoothing anisotropic diffusion method objective function minimization photometric variations fitting residual error nonlinear robust function discontinuity regions;road vehicles;image sequences	This paper discusses a novel robust velocity estimation method for moving vehicles in road traffic image sequences under snowy and misty weather conditions. In such environments, it is difficult to detect the velocity of vehicles because of discontinuities, occlusions, noise, and brightness variations. It should also be noted that falling snow results in indefinite shapes and motion, which occludes the region of interest. In order to estimate and recognize moving vehicles in images, a two-stage method has been proposed. Firstly, a non-linear smoothing is applied based on an anisotropic diffusion method. Secondly, a minimization of an objective function with movements and photometric variations is carried out. The fitting residual error in the function is reduced by a nonlinear robust function in discontinuity regions. Thus the velocity of moving vehicles can be estimated, stably and robustly, even in low contrast images. To verify this, a simple recognition experiment is performed to estimate the number of moving vehicles in images. The high recognition rate shows the validity and power of the proposed scheme.	anisotropic diffusion;loss function;nonlinear system;optimization problem;reflections of signals on conducting lines;region of interest;smoothing;velocity (software development)	Hidetomo Sakaino	2002		10.1109/ICPR.2002.1047400	computer vision;snow;simulation;computer science;cognitive neuroscience of visual object recognition;estimation theory;anisotropic diffusion;region of interest	Vision	43.56571835718154	-45.61112220840747	148319
7954a6ad9fdd35a6374ab8f38b27f41244265f47	3d wire-frame integration from image sequences	3d representation;geometric matching;edge-vertex decomposition;image sequence;structure from motion;wire-frame integration	procedure for integrating 3D positional information for isolated feature-points, extracted from a sequence of images, has been described in a previous paper. The following is a brief outline of the method: When integrating visual features into 3D for a Structure From Motion algorithm, the connectivity and relationships of features are an important adjunct to any quantitative 3D geometry. This paper describes a vision system which aims to perceive and refine this topology, in conjunction with geometry, using edges and vertices extracted from a sequence of monocular images of an unconstrained scene. Rules which tackle the practical difficulties of imperfect image processing and of obscuration features are defined. Results are shown for a rotating view of a polyhedral object, and for an outdoor scene viewed from a moving vehicle.	algorithm;image processing;polyhedron;structure from motion	Mike Stephens;Christopher G. Harris	1988		10.5244/C.2.25	computer vision;simulation;computer science;machine learning;geometry	Vision	49.94470850767581	-48.05509431812386	148346
caf98216bfc897edabe17e49c5f39cfd2c085b63	robust traffic state estimation on smart cameras	video signal processing cameras image sequences intelligent sensors state estimation statistical analysis traffic engineering computing;video signal processing;stationary traffic robust traffic state estimation video based traffic state detection motorways camera calibration parameters lane markings mean traffic speed kanade lucas tomasi optical flow method klt optical flow method robust outlier detection embedded smart camera road conditions illumination conditions detection rate;state estimation;statistical analysis;vectors;estimation;roads;streaming media;smart cameras;traffic engineering computing;vehicles;traffic state detection;vehicles smart cameras roads vectors streaming media cameras estimation;embedded computer vision;intelligent sensors;cameras;smart cameras traffic state detection embedded computer vision;image sequences	This paper presents a novel method for video-based traffic state detection on motorways performed on smart cameras. Camera calibration parameters are obtained from the known length of lane markings. Mean traffic speed is estimated from Kanade-Lucas-Tomasi (KLT) optical flow method using a robust outlier detection. Traffic density is estimated using a robust statistical counting method. Our method has been implemented on an embedded smart camera and evaluated under different road and illumination conditions. It achieves a detection rate of more than 95% for stationary traffic.	anomaly detection;camera resectioning;embedded system;kanade–lucas–tomasi feature tracker;optical flow;prototype;robustness (computer science);smart camera;stationary process;tomasi–kanade factorization	Felix Pletzer;Roland Tusch;László Böszörményi;Bernhard Rinner	2012	2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance	10.1109/AVSS.2012.63	smart camera;embedded system;computer vision;estimation;simulation;statistics;intelligent sensor	Robotics	40.37331131800811	-45.86243101821763	148562
48e7b81d6b6dabd08b9a502dbf36dbae2878efe0	real-time multi-view object tracking in mediated environments	real time tracking;location estimation;real time;multiple objectives;object tracking;expectation maximization algorithm;mixture of gaussians	In this paper, we present a robust approach to real-time tracking of multiple objects in mediated environments using a set of calibrated color and IR cameras. Challenges addressed in this paper include robust object tracking in the presence of color projections on the ground plane and partial/complete occlusions. To improve tracking in such complex environment, false candidates introduced by ground plane projection or mismatching of objects between views are removed by using the epipolar constraint and the planar homography. A mixture of Gaussian is learned using the expectation-maximization algorithm for each target to further refine the 3D location estimates. Experimental results demonstrate that the proposed approach is capable of robust and accurate 3D object tracking in a complex environment with a great amount of visual projections and partial/complete occlusions.	real-time transcription	Huan Jin;Gang Qian;David Birchfield	2008		10.1007/978-3-540-77409-9_22	computer vision;simulation;expectation–maximization algorithm;computer science;machine learning;video tracking;mixture model	Robotics	45.50168303634446	-47.713438908865534	148814
1a0c3266734f44200d40dae7a35d96bec503be01	auto-focusing technique in a projector-camera system	focusing;histograms;image resolution;layout lenses focusing cameras control systems robotics and automation intelligent robots infrared sensors automatic control robot control;blur map;region of interest autofocusing technique projector camera system 3d vision local blur information robust local blur estimator;data mining;image resolution cameras focusing;projector camera;blur map auto focusing projector camera blur estimation;distance measurement;auto focusing;estimation;region of interest;lenses;blur estimation;dc motors;3d vision;cameras	Projector-camera system has been researched for years and it has become more and more popular in 3D vision and many other applications. However, with pre-determined position of projector's lens, projector based researches were not able to deal with the scenes that have objects with different distances from the projector. In this paper, we present a projector autofocusing technique based on local blur information of the image that can overcome above limitation. The algorithm is implemented on projector-camera system, in order to focus the pattern which is projected by projector on all objects in the scene sequentially. The proposed algorithm first obtains a blur-map of the scene on the image by using a robust local blur estimator, and then the region of interest is decided by thresholding the obtained blur-map. Since the main light source is provided by projector, the proposed auto-focusing algorithm achieves a good performance with different light conditions.	algorithm;box blur;gaussian blur;nvidia 3d vision;region of interest;thresholding (image processing);video projector	Lam Bui Quang;DaeSik Kim;Sukhan Lee	2008	2008 10th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2008.4795821	computer vision;estimation;image resolution;computer science;dc motor;histogram;lens;mathematics;statistics;computer graphics (images);region of interest	Vision	49.57717215427115	-40.67382955947789	148938
49ac9738b551d0f8d9c64d5b6e8b08c69e3b0421	3d pictorial structures for multiple view articulated pose estimation	pose estimation object tracking;three dimensional displays estimation joints cameras tree graphs dynamic programming;3d pictorial structures 2d part detector computational tractability professional football game multiple view data intersection constraints joint angle skeleton 2d pose estimation pictorial structures framework multiple calibrated views 3d pose multiple view articulated pose estimation;datorsystem;computer systems;pictorial structures;multiple view 3d reconstruction pictorial structures part based models human pose estimation motion capture;multiple view 3d reconstruction;motion capture;engineering and technology;teknik och teknologier;part based models;object tracking;human pose estimation;pose estimation	We consider the problem of automatically estimating the 3D pose of humans from images, taken from multiple calibrated views. We show that it is possible and tractable to extend the pictorial structures framework, popular for 2D pose estimation, to 3D. We discuss how to use this framework to impose view, skeleton, joint angle and intersection constraints in 3D. The 3D pictorial structures are evaluated on multiple view data from a professional football game. The evaluation is focused on computational tractability, but we also demonstrate how a simple 2D part detector can be plugged into the framework.	3d pose estimation;algorithm;branch and bound;cobham's thesis;convolution;discretization;graphics processing unit;image;maxima and minima;sensor;steiner tree problem	Magnus Burenius;Josephine Sullivan;Stefan Carlsson	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.464	computer vision;motion capture;simulation;pose;3d pose estimation;computer science;video tracking;articulated body pose estimation;computer graphics (images)	Vision	52.99663658259734	-47.72996036452466	149034
9f08bb5de7df3d9552053ff1f4274be5aa7b74a4	a rule-based machine vision system for fire detection in aircraft dry bays and engine compartments	standard deviation;rule based;first order;fire detection;machine vision;charged couple device;computer analysis;high power;image sensor;rule based expert system;expert system	In this paper, a rule-based machine vision approach is applied to detect and categorize hydrocarbon fires in aircraft dry bays and engine compartments. Images for computer analysis are provided by charge-coupled device imaging sensors placed inside dry bays and engine compartments. Using a set of heuristics based on statistical measures derived from the histogram and image subtraction analyses of successive image frames, we showed that it is possible to detect and categorize life-threatening fires from non-fire/non-lethal events accurately in sub-millisecond response time. Specifically, the median, standard deviation, and first-order moment statistical measures of the histogram data of each image frame are used to confirm the presence or absence of fire. Concurrently, another set of mean, median, and standard deviation statistical measures from the image subtraction of two successive frames are used to determine the growth and subsequently reaffirm the existence of a fire. This approach is also tested for false alarms such as those due to flashlights and high-power halogen lights.	logic programming;machine vision	Simon Y. Foo	1996	Knowl.-Based Syst.	10.1016/S0950-7051(96)00005-6	computer vision;simulation;machine vision;computer science;artificial intelligence;machine learning;first-order logic;image sensor;charge-coupled device;standard deviation;expert system	Vision	41.25812601978613	-40.31732686073161	149055
c846051474f2cdd0a10f624e07e03584cb246f73	transform clustering for model-image feature correspondence	hough transform;image features	"""En this paprr we present a novel technique for establishing a rohusl a n d arrurat~ rorr~spondence between a 3d model and a 2d image. \Vt. present a transform clustering approach to i so l a t~ the transformation that maps the mad01 f ~ a t t ~ r e n to the image featur~s . Tt is shown that this transform clustering techniqu~ alleviates the prohIerns with using the traditional Hough transform techniqr~es used 'by previous researchen. dV'r dcmonstra t~ the ~ffectiveness of our approach irk ~st imat ing the position and and pose ofan autonomous rnohilp rohot navigating in an outdoor urban environn~ent. We prespnt experimental results of test in^ this approach using a model of an airport scone. Thc task of establishing a reliable and acri~rate correspondence between an image of x scene and a stored model of it ocrl~ra in a large n u m b ~ r of computer vision problems. Autonomous navigation of a mobile robot given n priori modd nf the pnvisonment and modelbased object recognition at0 two ~xarnples of romputcr vision tasks in which t h ~ niodel-image correspondence needs to be addressed. In the contpxt of autonomous navigation, the robot is provided with a preloaded world model of t h e environment. The world modcl could be in direrent forms, such as a Digilal Elemtian Map IDEM), a CAD description, or a flour map. The robot uses a n onboard camcra t o image the environment . Oncc we establish a carr~spondence bei w w n the image and the model, the robot's position and pose can be drtprmined. This position information can bp used by the robot to SZICCPI(SIUIJ~ navigate in its environmmt . In the r o n t ~ x t of model-based o b j ~ c t recognition, WP are given a geometric description of the object to be recognised and an image or the scene in which the object is prcs~n t . The task i s to isolate the object in the scene by using the image. Mod~l-image correspondenc~ are particularly diffiri~lr because the image and the model are usually in diffccsent formats. diRercnt ro-ordinate frames and of different dimensions. h popular approach to solving this problem is t o extract features from the image and search the model description for the corresponding set of features. The type of It=al~rr~e required and the number oS features used dppends on the rnod~l description and what is assumed to be knnwu ahout the scene. For example, in navigating the Thi* m a r c h war nupported bv in part by Army R e a r c h OF fice contract UAAL03-9 1-I;-0050 and in part by Air Force Ofice or Scicntifir Heu~arch (AFSC) contract E49620-8PCO04-4. rohot in an indoor s t r r i r t r~ r~d rnvironm~nt with a g i v ~ n CAD n ~ o d ~ l of the environment, it is common prart ire to ~ I S P l i n ~ SPgrnPntc a5 feat I I ~ W 131. On the ot h ~ r hand, in navigating the robot in an outdoor mountainous l ~ r r a i n givm a D E M nr the ~nvironmcnt, using curves may be a 10~icd choice [9]. TypicaUv, in t h ~ s e prohl~rns t h ~ modrl and tlir camera (rottot) are sppcifipd in two different ro-ordinate systems. O n c ~ we exlract t h ~ rel~vant featurcs from the image and identify the corresponding Fcatures in the model, WP can romputer the transformation 7 that maps Ihc model f~atztr~s into thp image fpatures. The pararnetcrs of this transformation are the rcquirrd position and pose of the camrra [robot ) with respect to the model. Solving for the pxrarneters of 7 , once a set of model-image irature corrtspondences is rstablished, is a very WPII studied problem ['Zj. Therefore, t h ~ crucial txsk to be accomplished is that of establishing a reliable and accurate correspondence. Xoise, occlusions, errors in feature detrction and inaccurate modcl dcscriptions further complicate this cor~espondence problem. Transfarm Cluatering: Previous r~searchers h a v ~ considered the technique o l matching a key model feat u r ~ , such as a long e d ~ e or a set of lines in specific orientations. t o establish an initial transtormation [ l , fi]. Suhwqrtrnt assignmpnts are thrn used to refine this transformation. New assignments arc S C ~ P C ~ P ~ on predictions of a model feature, projected into the image using tlre cnrrent transformat ion. t lowev~r , these techniques assume that it is possible to initially sderi a correct key m o d ~ l Feature. which may not alwavs be possiblc. Some sesearch~rs used the generalized H o u ~ h transform and its related parameter hashing techniques to perform fmnsJom rlr~st~ring to isolate the transformation mapping the modpl f e a t ~ ~ r ~ s into the image featur~s 16, 5, ID]. The g ~ n ~ r a l i z ~ d tlough transform works by first quantizing the n-dimmsional parameter spare in to discrete huckets or bins. Thc param~ters are the components describing 7. From thtgiven image. Sraturcs arc extracted using a feature extractor. Thpn all the possible modcl-image f ~ a t u r e cotrespondences are hypothesized and, for each hypothesis, the parameter vector is computed. For each parameter vector so computed, its n components are quantized a r~d used as indices to vote in one of t h ~ n-dimensional buckets. S~arching for large cIustrts is then accomplished by finding the buckets with a large nurnhprs of entries. Sometimes it is possible that one correspondence may not give explicitly aU the components of the parameter vector. but may only give a rangp of possible va lu~s far ~ a r h component. In this case. entries ate ma& into all the buckets within range. The a d v a n t a ~ r of this approach i s that c l u s t ~ r i n ~ provides a robust criterion b r firlrrting d i d m o d ~ l fcaturr assignrnmts. The cfbcts of miwing or inrotrpcf roature due to occlusion, ahadnws, or Inw rontra5t. arp not bit. The prohlema associat~d with using the H o u ~ h transform approach to transform rlueterinq are that l a r g ~ transform clust~ra may occur randomly. IT thesc clustrra are as l a r g ~ or I a r ~ o r than those due to the correct transrorm. the estirnalinn proc~durr that reFica only on the H o u ~ h transform will tw Prronmus. If t h ~ nllrnher of hucketrr is incrrased, thpn t h ~ po~~ihi!ity of random large c lus t~rs is aIlrviatrd but t h~ ni lmb~r of corn put ations grows rapidly. Grimson [1] summarizes t h ~ s ~ problems with the RFnPralirrd 1io11gh transform, This pappr pr-pnta a rnrthod to r e d u r ~ t h ~ probl ~ m r ~ s o c i a t ~ d with thr tfough transform approach to tmnsiorrn c lus t~t ing hy ilning a partition of thc paramet r r space. whirh is not nrcrssarily uniform. The partition is, in fact, ~nt~l l tgrnl and uses a pnon rnodrl information, Due to tttp gmmrtric constraints irnposd hy the model and thp campra ~eornetry. not all model features may he t.r*tbkr in dl camera positions. Typically orclusions b ~ t w r m the model i ~ a t u r w affect their visibility at rariour po~ir ions. However, since wr know the 3d dwctip tionfi of tho rnodpl f~at r l rm, t h r s ~ ~ ~ o m e t r i l : consttainta can br prr-computed and uard t o partition the pasame tor sparr to reduce I ~ P probability of the ocrurrrnrr of random transform cluatrrs. demonstrate t t ~ c k t i v e n e s s of our approach in ~stirnating t h ~ pmition anrl a n d pose of an autononious rnohilc robot. The robot is assumed to be navigating in an outdoor, urban environm~nt. The 3d description of the lines lt hat constitun* the rooftops of the buildin~s is given m a aorid modrl. Tbp position and pose of the robot are estimated by ~atahlishing a cortespondpncp between the linw pxtractpd from the imagc (image fratures) and the lines that constitute rooftops of the 1)uildin~s (model teat u r ~ s ) . tly rxploiting t he visibility conatrainf s imposed by i h r 3d world niodel and the camera Emmetry, we partition t h p paramrtrr space i n t o into disrinct, non-overlapping w~ i 0 n s callrd Fdge I'esrbrlity Hegtons (EYRs) [7]. In rach of thrsp r~gions , w e also Rtarr thr list of model featuwri that arp visible from within that region. If'e then hypothesite a correspondence b ~ t w w n all pairs of model and i m a g ~ featurm and romptltr I ~ P range of possiblc traasiwrmations for each hypothesis. lt'c vote in all thp r r~ ions in the pararnetpr sparv wlrer~ this ~ransfotmaiinn I R d i d . Alter consider in^ all t hc pair~ngs. u * ~ spIprt t htr ~ ~ i o n s in the parameter space with the I a r g numbers of votes the candirlat~ EV Ks lot position estirnafion, The actual correspondence and position estimation are then pcrfotmcd hy a ronstrairl~d search prncrsfl within thesp EVRs us in^ a i n t ~ r p r ~ t a t i o n t r w search paradigm. PARTITIONING THE PARAMETER SPACE Consider I he world coordinate system OXJ'i! and the robot coordinate system O'X7""""Zr shown in F i g u r ~ 1. Generally, thp transformation 7 that transforma 0 4 Y 2 into O'.YfY'%' has six dwltes of freedom: t h r e e rotational and t hrpp translational. Sometimes, depending on the applita! ion, somc or these d q r m of Irm-dorn can be elirniFigure 1: Tbr world and robot co-ordinatr systems nated. Most mohilr robot self-location tmks make the assumptinn rhat t h ~ rohot is on t h ~ gro~tnd (04Y plane), so the Z-translation (the h ~ i ~ h t aT the rohot above the round) is assumed to be known or to he zero. The ramora on tho robot is assi~rned t o have zero roll (rotation about X-axis),.and the tilt a n ~ l r : of the camera, (rotation about the Y i s ) is asurnrd to be meuurable. So, thrrc arr rffrrf ivdy t hrw pararnr t r t~ in the transformat ion: two translational (X, Y ) and one rotatianal B [the pan angle of tlir camera. wh i rh is r rotation about the Z-axis). Likrwise. in this paper wp havr only three parametprs of 7: X, Y and 8. Thr parametrr space of tbc trannformatioa is thus the r n t i r ~ CIXY plane and f he r a n p of mbot orientation B is 0 t h r o u ~ h 360 degrw. In this sr r t~on. we bri~flg we dwcribe a method for partitioning thr O.YY plane into rq ions cdled Edge Visibility Hqions ( E V R s ) using thc ~ i w n world model d+ srription. f'nr more details sw [TI. A ~ s o c i a t d with each"""	3d modeling;arp spoofing;apache axis;apple a5;application request routing;artificial intelligence;autonomous robot;carr–benkler wager;cluster analysis;cobham's thesis;computer vision;computer-aided design;emoticon;extended validation certificate;extractor (mathematics);hough transform;information rights management;institute of radio engineers;lu decomposition;map;mobile robot;model f keyboard;outline of object recognition;param;polyhedron;programming paradigm;quantization (signal processing);randomness;reflow soldering;thermal barrier coating;tor messenger;translational research informatics;wp symposium pro;windows nt	Raj Talluri;Jake K. Aggarwal	1992			hough transform;computer vision;computer science;top-hat transform;feature;computer graphics (images)	AI	51.98219775253496	-41.77998145928725	149066
b8d9e2bb5b517f5b307045efd0cc3a9bf4967419	efficient 3d object segmentation from densely sampled light fields with applications to 3d reconstruction	visual hulls;3d object and image segmentation;image based reconstruction;light field processing	Precise object segmentation in image data is a fundamental problem with various applications, including 3D object reconstruction. We present an efficient algorithm to automatically segment a static foreground object from highly cluttered background in light fields. A key insight and contribution of our article is that a significant increase of the available input data can enable the design of novel, highly efficient approaches. In particular, the central idea of our method is to exploit high spatio-angular sampling on the order of thousands of input frames, for example, captured as a hand-held video, such that new structures are revealed due to the increased coherence in the data. We first show how purely local gradient information contained in slices of such a dense light field can be combined with information about the camera trajectory to make efficient estimates of the foreground and background. These estimates are then propagated to textureless regions using edge-aware filtering in the epipolar volume. Finally, we enforce global consistency in a gathering step to derive a precise object segmentation in both 2D and 3D space, which captures fine geometric details even in very cluttered scenes. The design of each of these steps is motivated by efficiency and scalability, allowing us to handle large, real-world video datasets on a standard desktop computer. We demonstrate how the results of our method can be used for considerably improving the speed and quality of image-based 3D reconstruction algorithms, and we compare our results to state-of-the-art segmentation and multiview stereo methods.	3d reconstruction;algorithm;angularjs;desktop computer;epipolar geometry;gradient descent;light field;mobile device;sampling (signal processing);scalability	Kaan Yücer;Alexander Sorkine-Hornung;Oliver Wang;Olga Sorkine-Hornung	2016	ACM Trans. Graph.	10.1145/2876504	computer vision;computer science;theoretical computer science;segmentation-based object categorization;scale-space segmentation;computer graphics (images)	Graphics	53.07147555584133	-47.04197964319239	149109
8bee677c7c7e72f4db7c3dfbd69b38a21a2c8213	occlusion-aware multi-view reconstruction of articulated objects for manipulation	xiaoxia;locally optimized ransac;procrustes analysis;articulated reconstruction;3d reconstruction;computer engineering occlusion aware multi view reconstruction of articulated objects for manipulation clemson university stanley t birchfield huang	The goal of this research is to develop algorithms using multiple views to automatically recover complete 3D models of articulated objects in unstructured environments and thereby enable a robotic system to facilitate further manipulation of those objects. First, an algorithm called Procrustes-Lo-RANSAC (PLR) is presented. Structure-from-motion techniques are used to capture 3D point cloud models of an articulated object in two different configurations. Procrustes analysis, combined with a locally optimized RANSAC sampling strategy, facilitates a straightforward geometric approach to recovering the joint axes, as well as classifying them automatically as either revolute or prismatic. The algorithm does not require prior knowledge of the object, nor does it make any assumptions about the planarity of the object or scene. Second, with such a resulting articulated model, a robotic system is then able to manipulate the object either along its joint axes at a specified grasp point in order to exercise its degrees of freedom or move its end effector to a particular position even if the point is not visible in the current view. This is one of the main advantages of the occlusion-aware approach, because the models capture all sides of the object meaning that the robot has knowledge of parts of the object that are not visible in the current view. Experiments with a PUMA 500 robotic arm demonstrate the effectiveness of the approach on a variety of real-world objects containing both revolute and prismatic joints.	3d modeling;algorithm;planar graph;point cloud;procrustes analysis;public lending right;robot end effector;robotic arm;sampling (signal processing)	Xiaoxia Huang;Ian D. Walker;Stanley T. Birchfield	2014	Robotics and Autonomous Systems	10.1016/j.robot.2013.12.006	3d reconstruction;computer vision;simulation;computer science	Robotics	53.65055179501924	-47.66916601031819	149276
2b5f9a622406458549322d30d9fee46aba385e68	a general framework for fast 3d object detection and localization using an uncalibrated camera	training;indexing scheme 3d object detection 3d object localization uncalibrated camera mobile camera feature based method naive bayes classifiers viewpoint matching feature matching binary descriptors descriptor properties naive classifiers;indexes;three dimensional displays;object detection bayes methods image classification image matching;feature extraction;mobile communication;training indexes feature extraction three dimensional displays object detection mobile communication;object detection	In this paper, we present a real-time approach for 3D object detection using a single, mobile and uncalibrated camera. We develop our algorithm using a feature-based method based on two novel naive Bayes classifiers for viewpoint and feature matching. Our algorithm exploits the specific structure of various binary descriptors in order to boost feature matching by conserving descriptor properties (e.g., rotational and scale invariance, robustness to illumination variations and real-time performance). Unlike state-of-the-art methods, our novel naive classifiers only require a database with a small memory footprint because we store efficiently encoded features. In addition, we also improve the indexing scheme to speed up the matching process. Because our database is built from powerful descriptors, only a few images need to be 'learned' and constructing a database for a new object is highly efficient.	3d computer graphics;algorithm;binary-coded decimal;camera phone;data descriptor;memory footprint;naive bayes classifier;object detection;real-time clock	Andres Solis Montero;Jochen Lang;Robert Laganière	2015	2015 IEEE Winter Conference on Applications of Computer Vision	10.1109/WACV.2015.122	database index;computer vision;mobile telephony;feature extraction;computer science;viola–jones object detection framework;machine learning;pattern recognition	Vision	40.45674062253603	-51.20492566985854	149380
28814a8d6737acec680cef3562d4391ce241d973	time recognition of video clock for live event alert system	time recognition;image segmentation;sequence similarity;low resolution;digit sequence recognition;self consistent;domain knowledge;region of interest;digit transit periodicity detection;digital video;live event alert	In this paper, we propose an algorithm to recognize the time of video clock in broadcast soccer videos. A naive OCR approach to the problem will be confounded by standard image segmentation issues such as merged digits and low-resolution. In contrast, our methods achieve robustness by exploiting the domain knowledge governing clock digit transitions. The time periodicity information enables the reliable recognition of overlaid digit video clock and also facilitates self-consistency checks. A robust time recognition algorithm was presented in our previous papers. However, that algorithm requires the precise ROI (Region Of Interest) as its input. Besides inheriting the existing techniques this paper proposes two new ones to overcome the impreciseness of ROIs. The first one is the extended-ROI digit transit periodicity detection. It can correctly detect the second transit even though the known ROI is not precise. The second one is a Hough-like search for precise second ROI in digit-sequence similarity measure. Experimental results on 70 soccer video clips confirm that our methods can achieve the accurate results.	algorithm;homology (biology);hough transform;image segmentation;optical character recognition;quasiperiodicity;region of interest;robustness (computer science);similarity measure;video clip	Xinguo Yu;Kong-Wah Wan;Hwee Keong Lam;Chong Yee Lee	2010		10.1145/1937728.1937736	computer vision;speech recognition;computer science;communication	Vision	39.68620564823756	-46.67220022486376	149597
f6437d5b53f427fc5b5f31fb86a1c95391dff9f2	fast and robust registration of multiple 3d point clouds	evaluation function;range data;measurement error;3d point cloud;coarse registration robust registration multiple 3d point clouds simultaneous localization and mapping robot slam robot;three dimensional;distance field;data model;robot vision;shape;three dimensional displays;feature extraction;image registration;simultaneous localization and mapping;robustness;point cloud;iterative closest point algorithm;slam robots image registration robot vision;slam robots;iterative closest point algorithm robustness data models three dimensional displays shape cameras feature extraction;cameras;exhaustive search;data models	In this paper, we propose a method for the fast and robust registration of multiple 3D point clouds. This method contributes the simultaneous localization and mapping (SLAM) robot. The problem of point cloud registration can be roughly classified into two. The problem of estimation of the relation from unknown geometry position is coarse registration. Moreover, in case of roughly position is known, the problem of estimation of the relation from known initial position is fine registration. In this study, we solve the coarse registration problem by exhaustive search, and prepare the initial positions for fine registration. To employ exhaustive search, we realize robust matching by using no feature point. The fine registration problem is solved by the ICP algorithm. In this case, reduction of initial positions for the ICP algorithm by non-extremum suppression that uses distance between two range data solves the problem of calculation cost. The distance evaluation function that robust for measurement error tackles the outlier problem. The problem of calculation cost is solved by registering the distance from the last point cloud data beforehand by the distance field. The effectiveness of the proposed method is shown by a actual data.	algorithm;brute-force search;direction finding;distance transform;evaluation function;maxima and minima;point cloud;robustness (computer science);simultaneous localization and mapping;weight function;zero suppression	Hironobu Fukai;Gang Xu	2011	2011 RO-MAN	10.1109/ROMAN.2011.6005295	data modeling;three-dimensional space;computer vision;mathematical optimization;feature extraction;data model;shape;computer science;image registration;artificial intelligence;machine learning;evaluation function;brute-force search;point cloud;distance transform;robustness;observational error;simultaneous localization and mapping	Vision	50.36775205046185	-50.823914790259884	149681
2797125510a64efe6998638a099ed980d72e6f92	improved video-based eye-gaze detection method	biological techniques and instruments;computerised instrumentation;eye;handicapped aids;image sensors;infrared imaging;light reflection;signal detection;corneal reflection light;disabled;eye glass lenses;glint;image difference;light sources;pupil detection;video-based eye-gaze detection	Recently, some video-based eye-gaze detection methods used in eye-slaved support systems for the severely disabled have been studied. In these methods, infrared light was irradiated to an eye, two feature areas (the corneal reflection light and pupil) were detected in the image obtained from a video camera and then the eye-gaze direction was determined by the relative positions between the two. However, there were problems concerning stable pupil detection under various room light conditions. In this paper, methods for precisely detecting the two feature areas are consistently mentioned. First, a pupil detection technique using two light sources and the image difference method is proposed. Second, for users wearing eye glasses, a method for eliminating the images of the light sources reflected in the glass lens is proposed. The effectiveness of these proposed methods is demonstrated by using an imaging board. Finally, the feasibility of implementing hardware for the proposed methods in real time is discussed.	eb-eye;eye tracking;real-time computing;sensor;simulation	Yoshinobu Ebisawa	1998	IEEE Trans. Instrumentation and Measurement	10.1109/19.744648	computer vision;face detection;index term;infrared;eye tracking;image processing;image sensor;lens;glass;biological pest control;optics;head;brightness;physics;detection theory;computer graphics (images)	Vision	46.1804861536227	-43.72678247809154	149862
077cda10b516eb55ae8b8a1315bf8248336934c2	spectral validation of measurements in a vehicle tracking dddas		Vehicle tracking in adverse environments is a challenging problem because of the high number of factors constraining their motion and possibility of frequent occlusion. In such conditions, identification rates drop dramatically. Hyperspectral imaging is known to improve the robustness of target identification by recording extended data in many wavelengths. However, it is impossible to transmit such a high rate data in real time with a conventional full hyperspectral sensor. Thus, we present a persistent ground-based target tracking system, taking advantage of a state-of-the-art, adaptive, multi-modal sensor controlled by Dynamic Data Driven Applications Systems (DDDAS) methodology. This overcomes the data challenge of hyperspectral tracking by only using spectral data as required. Spectral features are inserted in a feature matching algorithm to identify spectrally likely matches and simplify multidimensional assignment algorithm. The sensor is tasked for spectra acquisition by the prior estimates from the Gaussian Sum Filter and foreground mask generated by the background subtraction. Prior information matching the target features is used to tackle false negatives in the background subtraction output. The proposed feature-aided tracking system is evaluated in a challenging scene with a realistic vehicular simulation.	algorithm;background subtraction;correspondence problem;data acquisition;dynamic data driven applications systems;feature model;hidden surface determination;modal logic;modality (human–computer interaction);multimodal interaction;simulation;vehicle tracking system	Burak Uzkent;Matthew J. Hoffman;Anthony Vodacek	2015		10.1016/j.procs.2015.05.358	computer vision;simulation;computer science;data mining	Robotics	43.09217291035527	-45.92024751836597	149889
b65b3e8c95f3f52b191ff08e53fcbef4027a1a74	looking for concepts: unsupervised map construction with unknown sensor configuration	unsupervised learning;object recognition;fuzzy neural nets;unsupervised learning path planning robot vision mobile robots art neural nets fuzzy neural nets feature extraction statistical analysis object recognition markov processes;path planning;prior information;mobile robots;robot vision;statistical analysis;feature extraction;sensor capability unsupervised map construction unknown sensor configuration navigation method landmark definitions;robot sensing systems humans sensor phenomena and characterization signal processing signal processing algorithms cameras data mining robot vision systems navigation bridges;markov processes;art neural nets	The navigation method presented here allows a robot to find its position and its route without prior information about its environment. What is going to be used as landmark is initially unknown, and no specific preprocessing of the sensor signals is done. The robot nevertheless extracts meaningful information from its environment and uses it to build a map. Both landmark definitions and map are continuously adapted. Working without predefined categoriza-tion takes full advantage of the sensor abilities. The result shows that a robot can reliably recognize self-defined landmarks suitable for its sensor capability and create a map of its environment.	autonomous robot;map;odometry;preprocessor;sensor;unsupervised learning	Philip Mächler	1998		10.1109/IROS.1998.724841	unsupervised learning;mobile robot;computer vision;feature extraction;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;motion planning;markov process;mobile robot navigation	Robotics	50.28273306497296	-38.12125259964483	149975
2a29cca2c9584ea2eeab465fb487dde14c941dc2	robust tracking of ellipses at frame rate	reliability;ellipse;real time;cue integration;robustness;tracking	The critical issue in vision-based control of motion is robust tracking at real time. A method is presented that tracks ellipses at field rate using a Pentium PC. Robustness is obtained by integrating gradient information and mode (intensity) values for the detection of edgels along the contour of the ellipse and by using a probabilistic (RANSAC-like, Fischler and Bolles, Commun. ACM 24(6) (1981) 381) method to find the most likely ellipse-shaped object. Detailed experiments document the capabilities and limitations of the approach and the robustness achieved.		Markus Vincze	2001	Pattern Recognition	10.1016/S0031-3203(99)00230-7	computer vision;simulation;computer science;reliability;mathematics;geometry;tracking;ellipse;robustness;computer graphics (images)	Vision	49.139586935067705	-46.04610453597836	149997
e16fd993d4330b8f2dc952199ede846dd3011703	unifying stereo, motion and object recognition via epipolar geometry	object recognition;image matching;1 dimensional;epipolar geometry;stereo matching	In this paper I try to show that through epipolar geometry we can unify the problems of image matching in stereo, motion and object recognition, which have been treated separately. Stereo matching has been known as a 1D search problem. But matching in motion and object recognition have been known as 2D search problems. I show that by recovering epipolar geometry underlying the images, the correspondence search problems in motion and object recognition can also be changed to be 1-dimensional, thus providing a framework to treat all these 3 problems in a uni ed way.	3d modeling;3d single-object recognition;computer stereo vision;epipolar geometry;image registration;outline of object recognition;pattern matching;search problem;the unsolved	Gang Xu	1995		10.1007/3-540-60793-5_81	computer vision;computer science;cognitive neuroscience of visual object recognition;one-dimensional space;fundamental matrix;maximally stable extremal regions;epipolar geometry	Vision	48.046809962795095	-49.511706702225034	150171
0679968bce565fc4ed7776c9179ade6dc86a5f7c	visual quality measures for characterizing planar robot grasps	stability criteria;robot vision intelligent robots manipulator kinematics reliability theory feature extraction real time systems stability criteria;visual quality measures;reliability estimation;intelligent robots;planar robot grasp;feature space;visual quality;manipulator kinematics;reliability theory;actual finger configuration;stability criteria visual quality measures planar robot grasp three finger grip pattern recognition method actual finger configuration barrett hand kinematics feature extraction intelligent robots real time system reliability estimation;robot vision;intelligent robots fingers stability analysis extraterrestrial measurements pattern recognition orbital robotics kinematics informatics laboratories computer science;feature extraction;pattern recognition method;pattern recognition;barrett hand kinematics;real time system;quality measures;robot vision systems;three finger grip;stability criteria feature extraction intelligent robots real time systems reliability estimation;real time systems	This paper presents and analyzes 12 quality measures that characterize robotic grips according to their stability and reliability. The measures are designed to assess three-finger grips of two-dimensional parts performed in a real environment, taking into account both theoretical aspects and unavoidable uncertainties of a grasping action. They build on the existing literature and on physical and mechanical considerations. The measures constitute a feature space that pattern recognition methods can use in order to classify robotic grips according to their quality. Six of the measures depend on the actual finger configuration of the gripper, and they have shown to be critical for better characterization. The kinematics of the Barrett Hand have been used. As a validation step, the measures are merged in two global quality values (with different practical applicability) that can be used to rank feasible candidate grips.	barrett reduction;bespoke;feature vector;hand geometry;pattern recognition;robot end effector	Eris Chinellato;Antonio Morales;Robert B. Fisher;Angel P. Del Pobil	2005	IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)	10.1109/TSMCC.2004.840061	computer vision;simulation;real-time operating system;feature vector;reliability theory;feature extraction;computer science;artificial intelligence;machine learning	Robotics	49.146883254171975	-39.69022475981674	150233
beec5be180c894afe3a9bd929cf9d45c92ffca59	a depth camera based fall recognition system for the elderly	depth image;fall detection;feature extraction;svm	Falls are a great risk for elderly people living alone. Falls can result in serious injuries and in some cases even deaths. It is important to recognize them early and provide assistance. In this paper we present a novel computer vision based fall recognition system which combines depth map with normal color information. With this combination it is possible to achieve better results as depth map reduces many errors and gives more information about the scene. We track and extract motion from the depth as well RGB map and then use Support Vector Machines to classify the falls. Our proposed fall recognition system recognizes and classifies falls from other actions with a very high accuracy (greater than 95%).	assistive technology;computer vision;depth map;support vector machine	Rachit Dubey;Bingbing Ni;Pierre Moulin	2012		10.1007/978-3-642-31298-4_13	support vector machine;computer vision;simulation;feature extraction;computer science	HCI	39.940618857012566	-44.93292953336314	150440
8c0a3cfb411f02380fa11926bd216397b76a33ef	face tracking based on differential harmony search	parameters tuning;dhs based tracking system;dhs based tracker;optimisation face recognition object tracking;computer vision applications;differential harmony search;human face tracking;particle filter;metaheuristic optimisation algorithm;differential harmony search dhs based tracking system multiple visual cues parameters tuning metaheuristic optimisation algorithm optimisation problem particle filter human face tracking computer vision applications dhs based tracker;optimisation problem;multiple visual cues	Owing to its significant roles in computer vision applications, human face tracking has drawn extensive attention in recent years. Most researchers solve face tracking using particle filter, meanshift and their derivatives. Unlike the traditional methods, in this study, face tracking is treated as an optimisation problem and a new meta-heuristic optimisation algorithm, differential harmony search (DHS), is introduced to solve face tracking problems. We compare the speed and accuracy of the proposed method with particle filter, meanshift and improved harmony search. Experimental results show that DHS-based tracker is faster and more accurate and it is easy to handle the parameters tuning. Furthermore, to improve the reliability of tracking, multiple visual cues are applied to DHS-based tracking system and experimental results demonstrate the increased robustness achieved by fusing multiple cues.	harmony search	Ming-Liang Gao;Li-Li Li;Xianming Sun;Dai-Sheng Luo	2015	IET Computer Vision	10.1049/iet-cvi.2014.0035	computer vision;simulation;particle filter;computer science;artificial intelligence;statistics	Vision	45.375038067859904	-47.443499451798374	150871
1597987ce20dfba1eb0e522a12b0cd8ef510a206	measurement and geometric modelling of human spine posture for medical rehabilitation purposes using a wearable monitoring system based on inertial sensors	spine;wearable system;geometric modelling;medical rehabilitation;inertial sensors	This paper presents a mathematical model that can be used to virtually reconstruct the posture of the human spine. By using orientation angles from a wearable monitoring system based on inertial sensors, the model calculates and represents the curvature of the spine. Several hypotheses are taken into consideration to increase the model precision. An estimation of the postures that can be calculated is also presented. A non-invasive solution to identify the human back shape can help reducing the time needed for medical rehabilitation sessions. Moreover, it prevents future problems caused by poor posture.	computer-aided design;mathematical model;poor posture;wearable computer;sensor (device)	Gheorghe-Daniel Voinea;Silviu Butnariu;Gheorghe Mogan	2016		10.3390/s17010003	inertial measurement unit;computer vision;simulation;geometric design;spine;engineering;biological engineering	Mobile	41.18780459225897	-39.21137918631975	151005
6b603b0bfc42df55031e781f62c8fbf8ae52a72c	shape tracking based on switched dynamical models	gaussian mixture;video sequences shape tracking switched dynamical models object tracking multipole models motion parameters shape parameters time varying parameters estimation stochastic models probabilistic mechanism markov chain state estimation algorithm gaussian mixtures propagation multi model framework;video signal processing;gaussian processes;dynamic model;time varying parameter;motion estimation;state estimation;video signal processing tracking motion estimation markov processes gaussian processes image sequences;object tracking;markov processes;stochastic model;shape stochastic processes filters integrated circuit modeling equations yttrium target tracking image analysis gaussian distribution density functional theory;tracking;image sequences;markov chain	Object tracking based on multipole models has been previously advocated as a way to tackle sudden changes of shape or motion parameters. The paper addresses the estimation of time-varying parameters described by a bank of stochastic models switched according to a probabilistic mechanism (Markov chain). A state estimation algorithm is proposed, based on the propagation of Gaussian mixtures in a multi-model framework.	dynamical system	Jorge S. Marques;João Miranda Lemos	1999		10.1109/ICIP.1999.823039	computer vision;markov chain;mathematical optimization;stochastic modelling;machine learning;video tracking;motion estimation;gaussian process;mathematics;tracking;markov process;statistics	Vision	45.849748157532424	-48.290072445566864	151066
b8ff561436e38cc3b996ec92c6ba7f004499b28d	a theoretical consideration of pattern space trajectory for gesture spotting recognition	motion images;image recognition;groupware;gesture trajectory feature pattern space trajectory gesture spotting recognition appearance based feature real time gesture recognition motion images time sequence matching methods;image segmentation;gesture spotting recognition;time sequence matching methods;real time;gesture trajectory feature;inner product;pattern recognition humans image recognition shape speech recognition target recognition target tracking brightness information processing computer interfaces;appearance based feature;computer vision;brightness;shape;target recognition;information processing;pattern recognition;speech recognition;humans;target tracking;pattern space trajectory;computer interfaces;user interfaces;gesture recognition;real time gesture recognition;matching method;image sequences	We propose a new appearance-based feature for realtime gesture recognition from motion images. The feature is the shape of the trajectory caused by gesture, in pattern space defined by inner-product between patterns on frame images. It has three merits, I ) invariant for the target human’s position, size and lie, 2) gesture recognition without interpreting frame image contents, 3) no costly statistical calculation. And it gives us a theoretical guarantee about the effectiveness of several time-sequence matching methods using shapes in the eigenspace or results of position tracking. In this papel; we describe the properties of the gesture trajectory feature, and some experimental results in order to show its applicability to gesture recognition with a theoretical consideration.	gesture recognition;sed	Shigeki Nagaya;Ryuichi Oka;Susumu Seki	1996		10.1109/AFGR.1996.557246	computer vision;speech recognition;computer science;pattern recognition;gesture recognition	Vision	46.701538270245756	-43.74334285505694	151290
77cf6f42581f8030b0fbbd905d2f5b7d24459f47	discriminative sketch-based 3d model retrieval via robust shape matching		We propose a sketch-based 3D shape retrieval system that is substantially more discriminative and robust than existing systems, especially for complex models. The power of our system comes from a combination of a contourbased 2D shape representation and a robust sampling-based shape matching scheme. They are defined over discriminative local features and applicable for partial sketches; robust to noise and distortions in hand drawings; and consistent when strokes are added progressively. Our robust shape matching, however, requires dense sampling and registration and incurs a high computational cost. We thus devise critical acceleration methods to achieve interactive performance: precomputing kNN graphs that record transformations between neighboring contour images and enable fast online shape alignment; pruning sampling and shape registration strategically and hierarchically; and parallelizing shape matching on multi-core platforms or GPUs. We demonstrate the effectiveness of our system through various experiments, comparisons, and user studies.	3d modeling;algorithmic efficiency;automatic parallelization;distortion;experiment;graphics processing unit;multi-core processor;precomputation;sampling (signal processing);shape context;sketch;sketch-based modeling;usability testing	Tianjia Shao;Weiwei Xu;KangKang Yin;Jingdong Wang;Kun Zhou;Baining Guo	2011	Comput. Graph. Forum	10.1111/j.1467-8659.2011.02050.x	active shape model;computer vision;machine learning;pattern recognition;mathematics	Vision	50.996091597145195	-47.582925651443915	151629
e59f3a5cb21dfb528deb699fe25fec35802267f2	probabilistic trajectory estimation based leader following for multi-robot systems	conference paper;trajectory;estimation;multi robot systems;robot vision systems;trajectory control;cameras;robot kinematics	The paper is concerned with the multi-robot leader-following problem in the presence of frequent dropouts in vision detection. In many scenarios, for instance a structured environment, it is inevitable to experience outage of vision detection due to reasons such as the target moving out of view, vision occlusion, motion blurring, etc. The paper proposes a Bayesian trajectory estimation based leader-following approach that can offer accurate path following given intermittent vision observations. The follower robot estimates the trajectory of the leader robot based on the noise-corrupted odometry information of both robots, and inter-robot relative observations based on detection of fiducial markers using an RGBD camera. A linear trajectory-following control method is employed to track a historical pose of the leader robot on the estimated trajectory. Results are obtained based on evaluating the proposed leader-following approach in tests with a zig-zag shaped trajectory and with a trajectory that contains sharp turns.	bayesian programming;downtime;experiment;fiducial marker;odometry;robot;simulation;smoothing;sun outage;trajectory (fluid mechanics);ultra-wideband	Mao Shan;Ying Zou;Mingyang Guan;Changyun Wen;Kwang-Yong Lim;Cheng-Leong Ng;Paul Tan	2016	2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)	10.1109/ICARCV.2016.7838742	computer vision;estimation;simulation;computer science;artificial intelligence;trajectory;control theory;mathematics;robot kinematics;statistics	Robotics	49.25097472821821	-44.86376333544436	151661
629a724d4505ce8be8e443249a2c5197fbfcabe6	an evolutionary framework for stereo correspondence	minimisation;genetic operator;cost function;image matching;geometry genetic algorithms equations robustness cameras vectors layout bridges genetic mutations cost function;epipolar geometry;stereo image processing;genetic algorithms stereo image processing image matching minimisation;genetic algorithm;genetic algorithms;robust epipolar line estimation evolutionary framework stereo correspondence feature correspondence uncalibrated images cost function minimization feature points outliers epipolar geometry recovery genetic algorithm match process	In this paper we propose an evolutionary framework to establish feature correspondence from two uncalibrated images. By minimizing a proposed cost function, we match the feature points, discard the outliers and recover the epipolar geometry in one step. The unifying framework is furnished by genetic algorithm. We also create a new genetic operator to exchange the information between match process and robust epipolar line estimation so that epipolar geometry constraint can be elegantly incorporated into match process. Experiments on synthetic and real images show that this approach is very e ective and fast.	binocular disparity;computation;epipolar geometry;experiment;genetic algorithm;genetic operator;heuristic;loss function;synthetic intelligence;time complexity	Jinxiang Chai;Songde Ma	1998		10.1109/ICPR.1998.711281	computer vision;mathematical optimization;genetic algorithm;computer science;machine learning;mathematics;fundamental matrix;epipolar geometry	Vision	51.6389247268878	-50.981420339970235	151769
7bf32c4164a05718f03797e5dfad928d16a8b7db	surface landmark selection and matching in natural terrain	databases;closest point search structure;robot sensing systems;sensor systems;landmark matches;feature extraction natural scenes image matching;image matching;natural terrain;absolute position estimation position estimation natural terrain landmarks a priori pose information landmark matches closest point search structure;layout;data mining;feature vector;postal services;a priori pose information;feature extraction;position estimation;absolute position estimation;propulsion;space technology;landmarks;read only memory;natural scenes;pose estimation;read only memory layout sensor systems propulsion laboratories space technology postal services robot sensing systems data mining databases	In this paper we present an algorithm for robust absolute position estimation in natural terrain based on landmarks extracted from dense 3-0 surfaces. Our landmarks are constructed by concatenating pose dependent oriented surface points with pose invariant surface signatures into a single feature vector; this definition of landmarks allows a priori pose information to be used to constrain the search for landmark matches. The first step in our algorithm is to extract landmarks from stable and salient surface patches. These landmarks are then stored in a closest point search structure with which landmarks are matched eficiently using available pose constraints and invariant values. Finalb, an iterative pose estimation algorithm, based on least median squares, is wrapped around landmark matching to eliminate outliers and estimate absolute position. To validate our algorithm, we show hundreds of absolute position estimation results from three different natural scenes. These results show that our algorithm can incorporate constraints on position and attitude for eficient landmark matching and match small and dense scene surface patches to large and coarse model surfaces.	aerial photography;algorithm;align (company);autonomous robot;concatenation;feature vector;fred (chatterbot);image resolution;iterative method;nearest neighbor search;technical support;type signature	Andrew E. Johnson	2000		10.1109/CVPR.2000.854868	layout;computer vision;propulsion;pose;feature vector;feature extraction;computer science;machine learning;pattern recognition;space technology;read-only memory	Vision	51.84434590630915	-39.05784293306433	151941
20b8a76e988e796f0f225876a69842f6839e4c98	real-time gender recognition for uncontrolled environment of real-life images		Gender recognition is a challenging task in real life images and surveillance videos due to their relatively low-resolution, under uncontrolled environment and variant viewing angles of human subject. Therefore, in this paper, a system of real-time gender recognition for real life images is proposed. The contribution of this work is fourfold. A skin-color filter is first developed to filter out non-face noises. In order to make the system robust, a mechanism of decision making based on the combination of surrounding face detection, context-regions enhancement and confidence-based weighting assignment is designed. Experimental results obtained by using extensive dataset show that our system is effective and efficient in recognizing genders for uncontrolled environment of real life images.	face detection;handwriting recognition;real life;real-time clock;real-time locating system;real-time transcription;robustification;uncontrolled format string	Duan-Yu Chen;Kuan-Yi Lin	2010			computer science;artificial intelligence;computer vision;multimedia	Vision	45.0996287945858	-43.74930377017721	152032
08c295fc48c6b68fe029e3025c978a8cf054a438	combining appearance models and markov random fields for category level object segmentation	image sampling;markov random fields object segmentation image segmentation shape image classification iterative algorithms object detection image edge detection image sampling data mining;image segmentation;iterative algorithms;visual word occurrences;gibbs sampling;markov random fields;image classification;global constraint;data mining;markov random field;image texture;markov random field models;object segmentation;visualization;label consistency;object localization;computational modeling;shape;long distance;image edge detection;image color analysis;category level object segmentation;image colour analysis;image representation;pascal voc 2007 dataset markov random fields category level object segmentation image classification markov random field models visual word occurrences label consistency local image cues;pixel;random processes;local image cues;pascal voc 2007 dataset;markov processes;random processes image colour analysis image representation image sampling image segmentation image texture markov processes;bag of words;object detection;object model	Object models based on bag-of-words representations can achieve state-of-the-art performance for image classification and object localization tasks. However, as they consider objects as loose collections of local patches they fail to accurately locate object boundaries and are not able to produce accurate object segmentation. On the other hand, Markov random field models used for image segmentation focus on object boundaries but can hardly use the global constraints necessary to deal with object categories whose appearance may vary significantly. In this paper we combine the advantages of both approaches. First, a mechanism based on local regions allows object detection using visual word occurrences and produces a rough image segmentation. Then, a MRF component gives clean boundaries and enforces label consistency, guided by local image cues (color, texture and edge cues) and by long-distance dependencies. Gibbs sampling is used to infer the model. The proposed method successfully segments object categories with highly varying appearances in the presence of cluttered backgrounds and large view point changes. We show that it outperforms published results on the Pascal VOC 2007 dataset.	approximation algorithm;bag-of-words model;benchmark (computing);computer vision;gibbs sampling;image segmentation;markov chain;markov random field;object detection;sampling (signal processing);visual word	Diane Larlus;Frédéric Jurie	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587453	image texture;stochastic process;computer vision;contextual image classification;visualization;object model;gibbs sampling;shape;computer science;bag-of-words model;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;markov process;computational model;pixel	Vision	44.97322613782425	-51.218958652961135	152063
73ea8771af11e78b11a9d39e970fadd87fc0b640	gaze estimation using a webcam for region of interest detection		In this paper, a real-time gaze estimation system using a webcam is proposed, in which variation of head pose is tracked. At first, variation of head position and pose are estimated by using facial features. Then, an iterative iris center detection method is proposed for tracking iris in eye image. Finally, gaze is estimated by using estimated head pose and position, and iris center position. The proposed gaze estimation system is applied to four different applications. Experimental results show that the proposed iterative iris center detection method has a higher accuracy than conventional ones. Also, the proposed gaze estimation system shows about 98 % accuracy using \(640\times 480\) resolution webcam and 42-inch monitor that are 0.75 m apart.	region of interest;webcam	Hong-In Kim;Jin-Bum Kim;J.-E. Lee;T.-Y. Lee;Rae-Hong Park	2016	Signal, Image and Video Processing	10.1007/s11760-015-0837-6	computer vision;simulation;computer graphics (images)	Vision	47.559527864324046	-44.48466514067471	152201
18166432309000d9a5873f989b39c72a682932f5	learning a warped subspace model of faces with images of unknown pose and illumination	cmu;generic model;image based modeling;image registration;face modeling;map estimation;super resolution	In this paper we tackle the problem of learning the appearances of a person’s face from images with both unknown pose and illumination. The unknown, simultaneous change in pose and illumination makes it difficult to learn 3D face models from data without manual labeling and tracking of features. In comparison, imagebased models do not require geometric knowledge of faces but only the statistics of data itself, and therefore are easier to train with images with such variations. We take an image-based approach to the problem and propose a generative model of a warped illumination subspace. Image variations due to illumination change are accounted for by a low-dimensional linear subspace, whereas variations due to pose change are approximated by a geometric warping of images in the subspace. We demonstrate that this model can be efficiently learned via MAP estimation and multiscale registration techniques. With this learned warped subspace we can jointly estimate the pose and the lighting conditions of test images and improve recognition of faces under novel poses and illuminations. We test our algorithm with synthetic faces and real images from the CMU PIE and Yale face databases. The results show improvements in prediction and recognition performance compared to other standard methods.	approximation algorithm;computer vision;database;eigenface;generative model;illumination (image);image registration;lambertian reflectance;nonlinear system;pose (computer vision);signal processing;spatial variability;super-resolution imaging;synthetic intelligence	Jihun Ham;Daniel D. Lee	2008			computer vision;computer science;image registration;pattern recognition;superresolution;computer graphics (images)	Vision	47.32873647693015	-51.204976743617216	152241
3383e7125a2e1d387a5d1ee34fcb412fb4130343	person identification using shadow analysis		We introduce a novel person identification method for a surveillance system of much wider area than conventional systems using CCTV cameras. In the proposed system, we install cameras to rooftops of buildings or a low altitude airship, and identify people by gait features extracted from shadows, which are projected on the ground by the sun in the daytime or lights in the evening. Since conventional systems extract gait features from actual body area, the correct classification ratio is reduced due to the lack of information of body area, in case that images are captured by overhead cameras. On the other hand, the proposed system enables to identify people by gait features which are extracted from shadows projected on the ground, even if images are captured by overhead cameras. In the proposed system, shadow areas projected on the ground are extracted automatically from captured images, and then analyze dynamics of shadow areas by the spherical harmonics. Experiments of person identification using actual outside images revealed that the proposed method showed the best performance than conventional methods, and the results indicate the feasibility of person identification based on shadow analysis.	closed-circuit television;overhead (computing)	Yumi Iwashita;Adrian Stoica;Ryo Kurazume	2010		10.5244/C.24.35	computer vision;simulation	Vision	42.56050912431498	-44.75658705134802	152295
d1314b12e2254a4bd138de37f166136cd28caf3a	video attention deviation estimation using inter-frame visual saliency map analysis	eye;video	A viewer’s visual attention during video playback is the matching of his eye gaze movement to the changing video content over time. If the gaze movement matches the video content (e.g., follow a rolling soccer ball), then the viewer keeps his visual attention. If the gaze location moves from one video object to another, then the viewer shifts his visual attention. A video that causes a viewer to shift his attention often is a “busy” video. Determination of which video content is busy is an important practical problem; a busy video is difficult for encoder to deploy region of interest (ROI)-based bit allocation, and hard for content provider to insert additional overlays like advertisements, making the video even busier. One way to determine the busyness of video content is to conduct eye gaze experiments with a sizable group of test subjects, but this is time-consuming and costineffective. In this paper, we propose an alternative method to determine the busyness of video—formally called video attention deviation (VAD): analyze the spatial visual saliency maps of the video frames across time. We first derive transition probabilities of a Markov model for eye gaze using saliency maps of a number of consecutive frames. We then compute steady state probability of the saccade state in the model—our estimate of VAD. We demonstrate that the computed steady state probability for saccade using saliency map analysis matches that computed using actual gaze traces for a range of videos with different degrees of busyness. Further, our analysis can also be used to segment video into shorter clips of different degrees of busyness by computing the Kullback-Leibler divergence using consecutive motion compensated saliency maps.	digital video;encoder;experiment;kullback–leibler divergence;map analysis;markov chain;markov model;region of interest;steady state;tracing (software);voice activity detection	Yunlong Feng;Gene Cheung;Patrick Le Callet;Yusheng Ji	2012		10.1117/12.907384	computer vision;video;computer science;video tracking;multimedia;physics;computer graphics (images)	Vision	40.1077907006551	-51.82480670702632	152333
32c07520596ac21510177bd5d1d0147f59832275	height restriction barriers detection from traffic scenarios using stereo-vision	yttrium estimation integrated circuits;surf features extraction traffic scenarios height restriction barriers detection trucks driving assistance systems stereo vision intensity information depth information height barriers tracking high quality stereo reconstruction sort sgm algorithm canny edges extraction 2d intensity image 3d information horizontal lines hough transformation filtered edges intensity correlation approach repetitive textural pattern neighboring lines clustering barrier region of interest 3d points;estimation;yttrium;stereo image processing correlation methods driver information systems edge detection feature extraction hough transforms image filtering image reconstruction image texture object detection object tracking pattern clustering road traffic;tracking height restriction barriers intensity information depth information detection height estimation;integrated circuits	Height restriction barriers detection is important usually for trucks' driving assistance systems. In this paper we propose a novel approach that uses stereo-vision and combines intensity and depth information for height barriers detection and tracking. High quality stereo-reconstruction is carried out by the SORT-SGM algorithm. Canny edges are extracted from 2D intensity image and filtered out by the 3D information. Horizontal lines are determined using the Hough transformation on the filtered edges and then validated by an intensity correlation approach taking into consideration that usually height restriction barriers have a repetitive textural pattern. Neighboring lines are clustered together in order to form the barrier region of interest. The height of the barrier is also approximated from the 3D points that belong to the barrier's region of interest. SURF features are extracted for the detected barriers from the intensity image and used for tracking them across frames. The whole height restriction barriers detection system performs real time with high accuracy results.	approximation algorithm;canny edge detector;correspondence problem;hough transform;region of interest;second generation multiplex;stereopsis	Maria-Irina Barbu;Ion Giosan;Tiberiu Marita	2015	2015 IEEE International Conference on Intelligent Computer Communication and Processing (ICCP)	10.1109/ICCP.2015.7312631	computer vision;simulation;geography;computer graphics (images)	Robotics	43.84872759312678	-44.91111592654889	152465
d6c26b6ab6a72799685bb31fe7f7c0acfc3bfbbb	a tracking method for analyzing 3d motions of a human in a marker-free motion capture system	motion capture		motion capture	Sang-Hoon Kim;Chang-Joon Park;Ran-Hee Lee;In-Ho Lee	2003			computer vision;artificial intelligence;motion capture;physics	Vision	51.15321720269419	-43.02318626958483	152467
34a5ea12c3323c59ba9d04e64c6a2d8f85226d58	fast place recognition with plane-based maps	graph theory;path planning;vectors three dimensional displays visualization feature extraction trajectory simultaneous localization and mapping;mobile robots;trees mathematics;trees mathematics graph theory mobile robots path planning slam robots;slam robots;mobile robots fast place recognition plane based maps indoor environments planar region extraction hand held rgb d sensor pbmap 3d planar patches subgraphs interpretation tree	This paper presents a new method for recognizing places in indoor environments based on the extraction of planar regions from range data provided by a hand-held RGB-D sensor. We propose to build a plane-based map (PbMap) consisting of a set of 3D planar patches described by simple geometric features (normal vector, centroid, area, etc.). This world representation is organized as a graph where the nodes represent the planar patches and the edges connect planes that are close by. This map structure permits to efficiently select subgraphs representing the local neighborhood of observed planes, that will be compared with other subgraphs corresponding to local neighborhoods of planes acquired previously. To find a candidate match between two subgraphs we employ an interpretation tree that permits working with partially observed and missing planes. The candidates from the interpretation tree are further checked out by a rigid registration test, which also gives us the relative pose between the matched places. The experimental results indicate that the proposed approach is an efficient way to solve this problem, working satisfactorily even when there are substantial changes in the scene (lifelong maps).	graph (discrete mathematics);map;mobile device;normal (geometry);planar (computer graphics)	Eduardo Fernández-Moral;Walterio W. Mayol-Cuevas;Vicente Arévalo;Javier González	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630951	mobile robot;computer vision;simulation;computer science;artificial intelligence;graph theory;machine learning;planar straight-line graph;mathematics;motion planning	Robotics	51.14369213160932	-38.59819155558368	152520
2aef22de745253283b87c38e4ce0ad7833deb10d	position estimation based object tracking across multiple cameras	kalman filters;kalman filter;cameras vehicles road transportation equations image reconstruction predictive models object detection probability distribution gaussian distribution position measurement;kalman filter position estimation object tracking multiple cameras;tracking cameras kalman filters object detection;object tracking;position estimation;multiple cameras;cameras;tracking;object detection	An object tracking method based on position estimation across multiple cameras is proposed in this paper. For an object which is exiting a former view and entering in a latter, Kalman filter is used to predict the position in the next frame, and the reference region where the object will be observed in the latter view is obtained through a camera model. The object detected in the reference region is the correspondent object and the experiments show its validity.	experiment;kalman filter	Yingna Deng;Hong Zhu;Gang Li;Ruirui Ji	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.450	kalman filter;computer vision;simulation;computer science;machine learning;control theory	Robotics	46.19951006420629	-46.91113015495793	152690
14bca43a25fdebb1612fb6fb54225cd3052bd94f	bayesian kernel tracking	bayes estimation;changement aspect;nonparametric density estimation;pistage;deteccion blanco;probability density;estimacion densidad;bayesian approach;real time tracking;maximum likelihood;articulation;estimation densite;estimation non parametrique;maximum vraisemblance;rastreo;maximum likelihood parameter estimation;parameterization;outlier;parametrizacion;articulacion;detection cible;observacion aberrante;non parametric estimation;density estimation;estimacion bayes;maximum likelihood estimate;object tracking;poursuite cible;observation aberrante;estimacion no parametrica;appearance change;joint;iterative solution;target tracking;target detection;joint domain density;parametrisation;maxima verosimilitud;tracking;estimation bayes	We present a Bayesian approach to real-time object tracking using nonparametric density estimation. The target model and candidates are represented by probability densities in the joint spatial-intensity domain. The new location and appearance of the target are jointly derived by computing the maximum likelihood estimate of the parameter vector that characterizes the transformation from the candidate to the model. This probabilistic formulation accommodates variations in the target appearance, while being robust to outliers represented by partial occlusions. In this paper we analyze the simplest parameterization represented by translation in both domains and present a gradient-based iterative solution. Various tracking sequences demonstrate the superior behavior of the method.	bayesian network;gradient;iterative method;kernel (operating system);real-time clock	Dorin Comaniciu	2002		10.1007/3-540-45783-6_53	econometrics;pattern recognition;mathematics;maximum likelihood;statistics	Vision	46.38641382448581	-50.581397477506044	152760
3245b047c145272e16b097ba21939c80ffead168	real-time multiple objects tracking with occlusion handling in dynamic scenes	image segmentation;real time;image sequences image segmentation target tracking real time systems object detection;prior knowledge;changing background real time multiple objects tracking occlusion handling dynamic scenes video sequences;target tracking;multiple object tracking;object detection;layout pattern recognition shape cameras bayesian methods object detection object segmentation computer vision automatic control automation;dynamic scenes;real time systems;image sequences	This work presents a real-time system for multiple objects tracking in dynamic scenes. A unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a prior knowledge about the shape or motion of objects. The system produces good segment and tracking results at a frame rate of 15-20 fps for image size of 320 /spl times/ 240, as demonstrated by extensive experiments performed using video sequences under different conditions indoor and outdoor with long-duration and complete occlusions in changing background.	experiment;hidden surface determination;image resolution;real-time computing;real-time locating system	Tao Yang;Stan Z. Li;Quan Pan;Jing Li	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.292	computer vision;simulation;computer science;video tracking;image segmentation;computer graphics (images)	Vision	44.627183992703536	-46.86042548138211	152800
